# 9.10 — Artifact Normalization for Downstream Automation

In early 2025, a financial services company had 40,000 human review artifacts sitting in a database. Rich artifacts — reviewer comments explaining why outputs were flagged, highlighted text spans showing where hallucinations occurred, timestamps and session IDs linking each review to production logs. The Trust and Safety team read these artifacts weekly and wrote narrative summaries. Engineering used the summaries to prioritize prompt fixes. It worked, but it didn't scale. A single prompt fix took three weeks: one week for reviewers to accumulate enough examples, one week for the Trust and Safety team to read and synthesize them, one week for Engineering to write and test the patch. By the time the fix deployed, the model had generated 200,000 more outputs with the same flaw.

Engineering proposed automating the pipeline. Read the artifacts, cluster them by issue type, generate candidate prompt patches, route them to Engineering for approval. The Trust and Safety team agreed. Engineering built the pipeline in two months. It failed on day one. Why? Because the artifacts weren't normalized. One reviewer wrote "output contains false claim about interest rates." Another wrote "hallucinated APR." A third wrote "incorrect rate calc." All three were describing the same issue — the model was generating fictional interest rate numbers for loan products — but the artifacts used different vocabulary, different levels of specificity, different structures. The clustering algorithm saw them as three separate issues. The prompt patch generator couldn't identify the common pattern. The pipeline produced 47 candidate patches, 45 of which were duplicates or nonsense. Engineering shut it down and went back to manual synthesis.

Artifacts are rich by design. Reviewers write in natural language, describe edge cases, add context. But downstream automation needs consistency. If you want machines to read artifacts, cluster them, route them, and act on them, you need normalization — a layer between human expression and machine consumption that preserves meaning while imposing structure.

## The Normalization Problem

Human reviewers don't speak in controlled vocabularies. They write what makes sense in the moment, using whatever words feel right. One reviewer says "the output is biased against older adults." Another says "ageist language." A third says "age discrimination in tone." They're describing the same issue with three different phrasings. A human reading all three immediately recognizes the pattern. A machine sees three distinct strings.

Reviewers describe the same concept at different levels of abstraction. One says "the model recommended a medication not approved for this condition." Another says "off-label drug use without disclaimer." A third says "medical misinformation — regulatory violation." Same root issue, three different frames. A human knows these are all variations of "inappropriate medical advice." A machine doesn't.

Reviewers include irrelevant details in free-text comments. "This output is unsafe — the model suggested a dangerous chemical reaction. Also the response was slow to load, maybe a latency issue?" The comment contains two issues: a safety violation and a performance observation. The safety violation is critical and needs to route to your prompt patching system. The performance observation is useful but belongs in your infrastructure monitoring workflow, not your safety remediation pipeline. A human can separate them. A machine without normalization routes the entire comment to both systems, creating noise.

Reviewers use inconsistent severity language. One writes "minor issue." Another writes "low priority." A third writes "not urgent." Are these the same severity level? Close enough that a human would group them, different enough that keyword matching fails. If your automation routes "high severity" issues to an immediate-fix queue and "low severity" issues to a backlog, you need to know whether "minor issue" maps to low severity or gets dropped entirely.

## Controlled Vocabularies and Taxonomies

The first layer of normalization is a **controlled vocabulary** for common concepts. When a reviewer writes a comment, they also tag it with structured categories from a predefined list. The comment might say "output contains ageist language in the second paragraph," but the tags say: Issue Type: Bias, Bias Category: Age, Severity: Moderate, Location: Output Text. The tags are the normalized version. The comment is the human-readable explanation.

The controlled vocabulary is task-specific. For safety review, you have Issue Type options like Bias, Misinformation, Hate Speech, Self-Harm, Dangerous Instructions. Each Issue Type expands into subcategories. Bias has subcategories for Age, Gender, Race, Disability, Socioeconomic Status. Misinformation has subcategories for Medical, Financial, Political, Scientific. The taxonomy is deep enough to capture meaningful distinctions, shallow enough that reviewers can navigate it in under ten seconds.

The vocabulary evolves with your product. When you launch in a new domain, you add domain-specific categories. A healthcare product adds Medical Misinformation subcategories for Drug Interactions, Off-Label Use, Contraindications. A legal product adds categories for Jurisdictional Errors, Statutory Misinterpretation, Case Law Hallucination. You version the vocabulary — every review record stores the taxonomy version it used, so historical reviews remain parseable even after the vocabulary changes.

The taxonomy is not a replacement for free-text comments. It's a parallel structure. Reviewers write what they see in natural language, then tag it with the closest match from the taxonomy. The comment preserves nuance and edge cases. The tags enable aggregation and automation. If 200 reviews in a week tag Issue Type: Misinformation, Subcategory: Medical, Severity: High, your automation routes them to the Medical Misinformation remediation workflow without reading a single comment. The comments are still there for human analysis, but the tags make machine routing possible.

## Normalization at Write Time vs Read Time

You can normalize at write time or read time. **Write-time normalization** happens in the review interface. The reviewer writes a comment, selects tags from dropdowns or autocomplete fields, and submits. The tags are stored alongside the comment as structured fields. Downstream systems read the tags directly. No parsing, no inference, no ambiguity. The schema guarantees that every review has Issue Type, Severity, and Location tags, because the interface requires them before submission.

Write-time normalization is reliable but constrains the reviewer. If the taxonomy doesn't have a category that matches what the reviewer sees, they either pick the closest approximation — which loses precision — or write a detailed comment and leave the tags generic. A reviewer flags an output for "sarcasm that could be misinterpreted as sincere advice in a high-stakes domain." The taxonomy has Tone Issue as a category, but it doesn't have a Sarcasm Misinterpretation subcategory. The reviewer tags it Tone Issue, writes the full explanation in the comment, and hopes someone reads it. The tag is normalized, but it doesn't capture the actual problem.

**Read-time normalization** happens after the review is submitted. The reviewer writes a free-text comment with optional tags. Your normalization pipeline reads the comment, extracts key concepts using NLP, maps them to your taxonomy, and writes the normalized tags as derived fields. The original comment is unchanged. The derived tags enable automation. The reviewer has full expressive freedom.

Read-time normalization is flexible but error-prone. Your NLP model might misclassify "the model refused to answer a legitimate medical question" as a Medical Misinformation issue when it's actually a Refusal Calibration issue. Your keyword extraction might miss domain-specific jargon — a reviewer writes "output hallucinates Coriolis effect magnitude," and your system doesn't recognize "Coriolis effect" as a physics concept, so it tags the issue as General Misinformation instead of Scientific Misinformation. You need a validation loop where humans spot-check the derived tags, correct errors, and retrain the normalization model.

Most teams use a hybrid. Required tags at write time for the most critical dimensions — Issue Type, Severity, Action Required. Optional tags at write time for subcategories and location. Read-time normalization for everything else — extracting mentioned entities, inferring root cause categories, linking to historical issues. The required tags ensure downstream routing works even if the NLP fails. The read-time normalization extracts additional signal without burdening the reviewer.

## Entity Extraction and Linking

Beyond categorical tags, you extract **entities** from reviewer comments. A reviewer writes "the model incorrectly stated that Metformin treats hypertension." Your normalization pipeline extracts: Drug Name: Metformin, Medical Condition: Hypertension, Error Type: Incorrect Indication. You link those entities to your knowledge graph. Metformin links to your drug database, which lists its actual FDA-approved indications. Hypertension links to your medical conditions ontology. The extracted entities become queryable fields. You can run a query: "all reviews mentioning Drug Name: Metformin with Error Type: Incorrect Indication" and find every case where the model hallucinated uses for that specific drug.

Entity extraction works best for domains with well-defined vocabularies. Medical terms, drug names, legal statutes, financial instruments, geographic locations, product SKUs. Your NLP pipeline uses named entity recognition models fine-tuned on domain-specific corpora. For medical review, you use a BioBERT-based NER model trained on clinical notes. For legal review, you use a model trained on case law and statutory text. The models extract entities with 85-95% precision, depending on domain complexity.

You validate extracted entities against authoritative sources. If the NLP extracts "Metformin" as a drug name, you check it against your drug database. If it's present, you link the review to the drug's canonical ID. If it's not present — maybe the reviewer misspelled it as "Metforman" — you flag the review for human correction. Validated entities are reliable. Unvalidated entities are hints.

Entity linking enables cross-review analysis. You see that 40 reviews in the past month mentioned "Metformin" with "Incorrect Indication" errors. You query your production logs and find that all 40 came from prompts that included the phrase "list all uses for." The model is overgeneralizing from its training data when it sees "list all uses" — it generates plausible-sounding but incorrect indications. Your Prompt Engineering team adds a guardrail: when the task involves listing medical uses, the system prompt includes a strict instruction to only state FDA-approved indications and cite sources. The fix came from entity linking, not from reading 40 individual comments.

## Severity and Priority Normalization

Reviewers describe severity inconsistently. "This is bad." "Critical issue." "Needs attention." "Urgent." "High priority." All of these might mean the same thing, or they might span three different severity levels. Your normalization layer maps them to a canonical severity scale: Low, Moderate, High, Critical. The mapping can happen at write time — the review interface shows four severity buttons, and the reviewer picks one. Or it can happen at read time — your NLP reads the comment, identifies severity language, and assigns a normalized severity level.

Read-time severity normalization uses keyword rules and contextual models. If the comment contains "critical," "dangerous," "immediate harm," "safety risk," or "violates policy," the system assigns Critical or High severity. If it contains "minor issue," "cosmetic," "low priority," or "suggestion," the system assigns Low severity. If severity language is absent, the system uses a classifier trained on historical reviews where humans labeled severity. The classifier looks at the Issue Type, the length of the comment, the presence of certain entities, and the reviewer's past severity patterns.

Priority normalization is separate from severity. Severity describes impact. Priority describes urgency. A Critical severity issue might be low priority if it only affects 0.01% of queries. A Moderate severity issue might be high priority if it affects 30% of queries in your fastest-growing market. Your normalization layer computes priority from severity plus volume plus business context. If a reviewer flags an issue as Critical and your logs show 500 occurrences in the past 24 hours, the normalized priority is Immediate. If a reviewer flags an issue as Moderate and your logs show 3 occurrences in the past month, the normalized priority is Backlog.

Priority normalization drives routing. Immediate priority issues go to an on-call escalation queue. High priority issues go to the active sprint. Medium priority issues go to the next sprint's planning backlog. Low priority issues go to a research queue for long-term product improvements. The routing is automatic because priority is normalized and machine-readable.

## Text Span Normalization

Some review interfaces let reviewers highlight text spans — the specific words or sentences in the output that triggered the flag. The reviewer highlights "You should stop taking your medication immediately" in a health-related output and tags it as Dangerous Medical Advice. The highlight is stored as a start offset and end offset in the output text, plus a category tag.

Text spans are already semi-structured, but they need normalization for cross-review aggregation. Two reviewers might highlight overlapping but non-identical spans. Reviewer A highlights "stop taking your medication immediately." Reviewer B highlights "You should stop taking your medication." Both are flagging the same dangerous advice, but the spans differ by a few characters. Your normalization layer clusters overlapping spans and treats them as a single highlighted region.

You normalize span categories the same way you normalize issue categories. Reviewers might tag a span as "unsafe advice," "medical risk," or "dangerous instruction." Your normalization maps all three to a canonical category: Dangerous Medical Advice. The canonical category is what your remediation pipeline reads. If 15 reviews highlight spans tagged as Dangerous Medical Advice in outputs generated from the same prompt template, your system flags that template for immediate revision.

Text span data is powerful for prompt engineering. You export all highlighted spans for a given issue category, analyze the common patterns — specific phrases, sentence structures, tonal cues — and write guardrails that detect those patterns at generation time. A model that frequently generates outputs with highlighted spans containing "stop taking your medication" gets a guardrail that blocks any output containing cessation advice for prescribed medications unless explicitly caveated with "consult your doctor first."

## Artifact Schema Enforcement

Normalization only works if the artifact schema is consistent. Every review for a given task type must store artifacts in the same structure. If one reviewer writes comments in a field called "notes" and another writes them in a field called "explanation," your normalization pipeline breaks. If one review stores highlighted spans as an array of offset pairs and another stores them as a comma-separated string, your span clustering logic fails.

You enforce schema consistency at the database level. The review table has a fixed schema for artifact fields. Comment text goes in a text column, not a varchar. Highlighted spans go in a JSON array column with a defined structure: each element is an object with start-offset integer, end-offset integer, and category string. Tags go in a normalized tags table with foreign keys to a controlled vocabulary table. If a reviewer or a review interface tries to write data that doesn't match the schema, the write fails with a validation error.

You version the artifact schema just like you version the label schema. When you add a new artifact field — say, a confidence score indicating how certain the reviewer is about their judgment — you increment the schema version. Historical reviews don't have the confidence field, so your normalization pipeline checks the schema version and skips confidence-based logic for older reviews.

## Feedback Loops for Normalization Quality

Your normalization pipeline will make mistakes. An NLP model misclassifies an issue. A keyword rule maps a severity term incorrectly. An entity extraction model hallucinates a drug name that doesn't exist. You need a feedback loop where humans review the normalized output, flag errors, and retrain the normalization logic.

You sample 5% of reviews daily and show them to a quality analyst alongside their normalized tags. The analyst sees: Original Comment: "model suggests aspirin for heart attack prevention without checking patient history," Normalized Tags: Issue Type: Medical Misinformation, Subcategory: Drug Interaction, Severity: High. The analyst confirms the tags are correct, or corrects them. If they correct, the correction goes into a training set for the next iteration of the normalization model.

You track normalization error rate by category. If your NLP consistently misclassifies Refusal Calibration issues as Policy Violations, you add more labeled examples of Refusal Calibration to the training set and retrain. If entity extraction frequently hallucinates drug names, you add a stricter validation rule that requires extracted drug names to match your authoritative drug database with 90% string similarity or higher.

You monitor downstream impact. If your prompt patching pipeline generates an unusually high number of candidate patches that Engineering rejects, you trace back to the normalized tags. Maybe your normalization is over-clustering — mapping too many distinct issues to the same category, so your patching logic thinks it's seeing a widespread pattern when it's actually seeing ten unrelated edge cases. You tighten the clustering thresholds and re-normalize the past week's reviews.

## The Normalized Artifact as API Contract

Once artifacts are normalized, they become an API contract for downstream systems. Your active learning pipeline reads normalized tags and routes uncertain examples to the right specialist reviewers. Your prompt patching pipeline reads normalized Issue Type and Severity, clusters reviews by pattern, and generates candidate guardrails. Your policy review workflow reads normalized policy violation tags and escalates to the legal team. Your training data pipeline reads normalized quality ratings and uses them to filter examples for fine-tuning.

Every consumer knows exactly what fields to read, what values to expect, and what version of the schema to parse. If a downstream team wants to build a new automation on top of review artifacts, they read the normalization schema documentation, write their consumer logic, and deploy. They don't need to parse free-text comments or guess at what "high priority" means. The normalized artifact is their interface.

This contract is strict. If you change the normalization logic — say, you split the Medical Misinformation category into three subcategories — you version the change, announce it to all downstream consumers, and give them a migration window. Breaking the contract without notice breaks every consumer.

Normalization is the unglamorous infrastructure that makes everything else possible. Without it, you have rich data and manual workflows. With it, you have automation, insights, and fixes that deploy in hours instead of weeks. Build it carefully. Version it strictly. Treat it as production infrastructure, because it is. The next step is using those normalized artifacts to drive product fixes — not just analysis, but actual prompt patches and runtime controls.

