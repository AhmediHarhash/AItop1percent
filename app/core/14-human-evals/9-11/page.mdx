# 9.11 — From Review Artifacts to Product Fixes: Prompt Patches and Blocked Actions

The alert fires at 2:47pm on a Thursday. Sixteen human reviews in the past hour flagged outputs as containing dangerous medical advice, all from the same prompt template. The normalized artifacts show: Issue Type: Dangerous Medical Advice, Subcategory: Medication Cessation, Severity: Critical, Common Pattern: outputs contain phrase "stop taking" followed by medication name. The Trust and Safety lead sees the alert, reads three sample artifacts, and confirms the pattern. The model is telling users to stop taking prescribed medications without caveats or disclaimers. This is a patient safety issue. It needs to be fixed now, not next sprint.

Engineering gets the alert. They query the production logs, find all instances of the flagged prompt template from the past 24 hours, and see 340 outputs containing the same dangerous pattern. They pull the template's current version, analyze the system prompt, and spot the issue: the template includes an instruction to "provide direct, actionable advice," but it doesn't include a medical disclaimer guardrail. The model interprets "direct" as "no hedging," so when a user asks about medication side effects, the model says "stop taking it" instead of "consult your doctor if you experience these symptoms."

Engineering writes a prompt patch. They add a guardrail to the system prompt: "If the user's query involves prescribed medications, never advise stopping or changing medication without explicitly instructing the user to consult their healthcare provider first." They test the patch on the 340 flagged examples. The model now generates safe responses — it still provides information about side effects, but it frames cessation advice as "discuss this with your doctor" instead of "stop taking the medication." The patch deploys to production at 4:15pm. Total time from alert to fix: 88 minutes. This is only possible because the review artifacts were normalized, the pattern was automatically detected, and the fix pipeline was built to act on artifact data without waiting for a human to synthesize a report.

## The Artifact-to-Fix Pipeline

Human review artifacts are not just records of past judgments. They are signals that drive automated product improvements. The pipeline works like this: reviewers flag issues, artifacts are normalized and stored, a pattern detection system clusters artifacts by common characteristics, candidate fixes are generated based on the cluster's normalized tags and text patterns, fixes are tested against historical examples, approved fixes deploy to production, and post-deployment monitoring confirms the issue is resolved.

This pipeline runs continuously. Every hour, new review artifacts come in. Every hour, the pattern detector checks for clusters that cross a severity or volume threshold. Every hour, candidate fixes are generated for actionable patterns. Some fixes deploy automatically if they meet safety and test coverage criteria. Others route to human approval queues. But the pipeline never stops — it's a closed loop from human judgment to automated remediation.

The pipeline only works if artifacts contain enough structured data to drive automation. Free-text comments alone aren't enough. You need normalized Issue Type, Severity, and Common Pattern tags. You need extracted entities — the medication names, the policy clauses, the text spans that triggered the flag. You need linkage to production logs so you can trace flagged outputs back to the prompts and model versions that generated them. If any of these are missing, the pipeline degrades to manual synthesis and slow fixes.

## Prompt Patches from Clustered Artifacts

The most common fix type is a **prompt patch** — a modification to the system prompt or few-shot examples that prevents the flagged behavior from recurring. Prompt patches are generated by analyzing clustered artifacts for common linguistic or conceptual patterns, writing a guardrail or refinement instruction that addresses the pattern, and testing the patched prompt against historical examples to confirm it eliminates the issue without regressing other behaviors.

A cluster of 40 review artifacts tagged Issue Type: Bias, Subcategory: Gender, Severity: Moderate shows a common pattern: outputs use male pronouns when describing hypothetical professionals in leadership roles. The pattern detector extracts the normalized tags, groups the artifacts, and generates a candidate patch: "When describing hypothetical individuals, use gender-neutral language or alternate between pronouns. Do not default to male pronouns for leadership roles or female pronouns for caregiving roles."

The patch is tested against the 40 flagged examples. The model regenerates responses. The test framework checks whether the new outputs still contain the gendered language pattern. 38 out of 40 now use gender-neutral phrasing. Two still default to male pronouns, but in different contexts — those get flagged as a secondary issue for deeper investigation. The patch is marked as 95% effective, routed to a human reviewer for approval, and deployed within two hours.

Prompt patches are version-controlled. Each patch has a unique ID, a creation timestamp, the cluster of artifacts that triggered it, the test results, the approval record, and the deployment timestamp. If a patch causes a regression — users report that the model's responses became less natural or started refusing legitimate queries — you can trace the regression back to the specific patch, review its test coverage, and decide whether to roll it back or refine it.

## Blocked Actions from High-Severity Artifacts

Some artifacts don't just trigger prompt patches — they trigger **blocked actions** at runtime. A blocked action is a hard constraint added to the model's output validation layer. If the model generates an output that matches a blocked pattern, the output is rejected before it reaches the user, and a fallback response is shown instead.

A cluster of review artifacts tagged Issue Type: Dangerous Instructions, Subcategory: Chemical Synthesis, Severity: Critical shows that the model is generating step-by-step instructions for synthesizing controlled substances when users ask chemistry questions. The artifacts include the exact prompts and outputs. The pattern detector identifies the common elements: queries contain terms like "synthesize," "make," or "prepare" followed by the name of a restricted chemical, and outputs contain numbered steps with specific reagents and procedures.

Engineering doesn't patch the prompt — the risk is too high that a prompt-level guardrail will miss edge cases. Instead, they add a blocked action. The output validation layer now scans every model response for the presence of restricted chemical names cross-referenced with procedural language like "step 1," "add," "heat," "distill." If both are present, the output is blocked. The user receives a fallback: "I can provide general information about chemistry concepts, but I can't provide synthesis instructions for controlled substances."

Blocked actions are stricter than prompt patches. A prompt patch guides the model's generation. A blocked action stops outputs from reaching users even if the model generates them. Blocked actions are used for high-severity, low-tolerance issues: dangerous instructions, illegal advice, severe policy violations, egregious misinformation in regulated domains. If the cost of a single false negative — one bad output getting through — is higher than the cost of many false positives — safe outputs being blocked — you use a blocked action instead of a prompt patch.

Blocked actions are tested rigorously before deployment. You run them against a sample of 10,000 historical outputs to measure false positive rate. If the blocked action triggers on 3% of safe outputs, you refine the pattern — maybe you're detecting "step 1" in outputs that are listing general procedures, not dangerous synthesis instructions. You tighten the rule to require both restricted chemical names and procedural language in the same sentence or within a 50-word span. The false positive rate drops to 0.2%. That's acceptable. You deploy the block.

## Entity-Linked Fixes for Specific Knowledge Gaps

When normalized artifacts include extracted entities — drug names, legal statutes, product SKUs, geographic locations — you can generate **entity-linked fixes** that address specific knowledge gaps. A cluster of 25 review artifacts tagged Issue Type: Misinformation, Subcategory: Financial, Severity: High all mention the entity "Roth IRA contribution limit" and show that the model is citing an outdated limit from 2023 instead of the 2026 limit.

The fix isn't a prompt patch — the model's training data is stale, and a prompt-level instruction won't override learned facts. The fix is a **knowledge injection**. Your system prompt includes a dynamic section that pulls current, authoritative facts from a structured knowledge base. When the model generates a response involving Roth IRA contribution limits, the system prompt includes: "For reference, the 2026 Roth IRA contribution limit is 7,000 dollars for individuals under 50 and 8,000 dollars for individuals 50 and older."

Knowledge injections are triggered by entity mentions in the user query. If the query contains "Roth IRA" or "IRA contribution," the system fetches the relevant facts from the knowledge base and injects them into the prompt. The model still generates the response, but it has access to current, correct information. This approach scales: you maintain a knowledge base of high-risk entities — financial figures, drug dosages, regulatory deadlines — and inject them dynamically rather than trying to keep the model's training data perpetually fresh.

Entity-linked fixes require a structured knowledge base and an entity recognition layer at inference time. The user query is parsed for entities, matched against the knowledge base, and relevant facts are injected before the prompt is sent to the model. The injection is invisible to the user. They ask "What's the Roth IRA limit for 2026?" and the model responds with the correct, current number because the system prompt included it.

## Refusal Calibration from Artifact Patterns

Some artifact clusters reveal **refusal calibration issues** — cases where the model refuses legitimate queries or allows dangerous ones. A cluster of artifacts tagged Issue Type: Refusal Error, Subcategory: Over-Refusal, Severity: Moderate shows that the model is refusing to answer basic legal questions because the system prompt includes a blanket instruction: "Do not provide legal advice." The reviewers note that users are asking factual questions like "What is the statute of limitations for breach of contract in California?" and the model responds "I can't provide legal advice."

The fix is a refusal calibration patch. Engineering rewrites the refusal instruction to distinguish between factual legal information and personalized legal advice: "You may provide general information about laws, statutes, and legal concepts. Do not provide advice on specific legal actions a user should take in their personal situation. If a user asks what they should do in a legal dispute, direct them to consult a licensed attorney."

The patch is tested against 100 historical queries that were over-refused. The model now answers 92 of them appropriately, providing factual legal information without crossing into personalized advice. Eight queries are still refused, but those are edge cases where the user explicitly asked "what should I do" — the refusal is correct. The patch deploys, and over-refusal rates drop by 80%.

Refusal calibration works in both directions. Some artifact clusters show **under-refusal** — the model is answering queries it should decline. A cluster of artifacts tagged Issue Type: Policy Violation, Subcategory: Adult Content, Severity: High shows that the model is generating sexually explicit content in response to creative writing prompts that bypass the existing content policy filters. Engineering tightens the refusal logic, adds pattern detection for euphemistic phrasing that users employ to evade filters, and deploys. The under-refusal rate drops to near zero.

## Fix Verification and Rollback Criteria

Every fix that deploys to production is monitored for effectiveness and regressions. You define **verification metrics** before deployment. For a prompt patch targeting gendered language bias, the metric is: gendered pronoun usage in professional role descriptions should drop by at least 70% without increasing refusal rates. For a blocked action targeting dangerous chemical synthesis instructions, the metric is: zero outputs containing restricted chemical names plus procedural language should reach users, with false positive rate below 0.5%.

You monitor these metrics for 48 hours post-deployment. If the fix meets its target and causes no regressions, it's marked as verified and becomes permanent. If the fix underperforms — the issue persists at 50% of the pre-fix rate instead of the targeted 10% — you investigate. Maybe the pattern detection was too narrow and missed variants of the issue. You refine the fix and redeploy.

If the fix causes a regression — user satisfaction scores drop, refusal rates spike, or a new category of errors appears — you have rollback criteria. A regression is defined as: any user-facing quality metric drops by more than 5%, or any safety metric degrades, or false positive rate exceeds the acceptable threshold. If rollback criteria are met, the fix is reverted automatically within 15 minutes. The artifact cluster is re-analyzed, the fix is redesigned, and a new candidate is tested more thoroughly before the next deployment attempt.

Rollback is automatic for high-severity fixes. If you deploy a blocked action and it causes a 10% spike in refusals of legitimate queries, the system reverts the block within one monitoring cycle. Rollback is manual for low-severity fixes. If a prompt patch causes a slight drop in response naturalness but eliminates a bias issue, a human decides whether the tradeoff is acceptable or whether the patch needs refinement.

## Cross-Fix Interaction Testing

Fixes don't deploy in isolation. Your production system has dozens or hundreds of active prompt patches, blocked actions, and knowledge injections. A new fix might interact with an existing one in unexpected ways. You test for **cross-fix interactions** before deploying.

A candidate prompt patch adds an instruction: "When discussing political topics, present multiple perspectives without favoring any particular ideology." You test it against your existing corpus of 200 prompt patches. One existing patch says: "Refuse to generate content that promotes political extremism." The new patch and the existing patch might conflict — the model might interpret "present multiple perspectives" as requiring it to articulate extremist views, which the existing patch forbids. Your interaction test runs both patches together on 500 political queries and flags 12 cases where the model generates a refusal when it should have provided a balanced response.

You resolve the conflict by refining the new patch: "When discussing political topics, present multiple mainstream perspectives without favoring any particular ideology. Do not articulate or promote extremist views." The refined patch is tested again, this time alongside all 200 existing patches. No conflicts detected. It deploys.

Cross-fix interaction testing is automated. Every new candidate fix is run through a compatibility suite that executes it alongside every active fix on a regression test set of 10,000 queries. Conflicts are flagged by looking for increases in refusal rates, drops in answer quality scores, or outputs that violate safety policies. Flagged conflicts go to human review. Most are resolved by refining the new fix's wording to be more specific and avoid overlap with existing logic.

## The Fix Audit Trail

Every fix that deploys to production has an audit trail. The trail includes the original artifact cluster that triggered the fix, the normalized tags and entities that defined the pattern, the candidate fix text, the test results showing effectiveness and false positive rates, the human approval record if applicable, the deployment timestamp, the verification metrics, and any rollback events.

The audit trail enables compliance and accountability. If regulators ask how you handled a specific category of safety issue, you show them the artifact cluster, the fix that was deployed, and the post-deployment data proving the issue was resolved. If a user complains that the model refused a legitimate query, you trace it to a specific blocked action, review its test coverage, and determine whether the block was overly broad.

The audit trail also enables continuous improvement. Every quarter, you review all deployed fixes and their long-term impact. Fixes that consistently perform well become candidates for integration into the base system prompt — they graduate from dynamic patches to core behavior. Fixes that required frequent refinement or caused multiple regressions are analyzed for root causes. Maybe the underlying issue isn't fixable with prompt-level logic and needs a model retraining or a product redesign.

The artifact-to-fix pipeline transforms human review from a post-hoc quality check into a real-time product improvement engine. Reviewers don't just label data — they generate the signals that directly drive safer, more accurate, more reliable outputs. The faster and more reliably you can move from flagged artifact to deployed fix, the less damage any individual issue can cause. Build the pipeline. Test it rigorously. Treat it as critical infrastructure, because every minute a known issue remains unfixed is a minute your users are exposed to outputs you've already identified as flawed.

The final architectural decision is whether to integrate review workflows into ML pipelines in real-time or in batches — each pattern has different latency, cost, and complexity tradeoffs.

