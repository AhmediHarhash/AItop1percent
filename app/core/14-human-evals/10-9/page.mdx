# 10.9 — Reporting for Stakeholders: Executive vs Operational Views

Most teams build one review operations dashboard and share it with everyone. Engineering stares at aggregate throughput numbers that tell them nothing about data quality. Executives squint at reviewer-level agreement metrics they don't need and can't interpret. Review leads refresh hourly backlog counts that executives will never look at. The single-dashboard approach fails because different stakeholders have different jobs, different decision horizons, and different information needs. A metric that drives an operational decision today is noise in a strategic report next quarter. A trend that informs budget allocation next year is irrelevant to someone routing cases this afternoon.

Effective reporting requires building multiple views from the same underlying data. The operational view is granular, real-time, and action-oriented. The executive view is aggregated, trend-focused, and cost-oriented. The engineering view is segmented by task and model, with deep dives into quality correlations. Each view surfaces the metrics the viewer needs to make their decisions and hides everything else. The goal is not to show all the data — it's to show the right data to the right person at the right time.

## What Executives Need to Know

Executives care about three things: cost, scale, and risk. They need to know whether the review operation is growing in line with product growth, whether cost per review is stable or increasing, and whether quality issues are creating user-facing or regulatory risk. They do not need to know that reviewer 47 had a slow Tuesday or that agreement on task type B dropped two points last week. Those details are operational noise at the executive level.

The executive dashboard shows month-over-month and quarter-over-quarter trends for total review volume, total review cost, cost per review, reviewer headcount, and aggregate quality metrics like overall agreement and production error rate. It highlights outliers — months where cost spiked, quarters where throughput didn't scale with headcount, periods where agreement dropped below acceptable thresholds. It includes annotations for major events: guideline updates, model releases, reviewer hiring surges, regulatory changes.

The most important executive metric is cost per review over time. If cost per review is flat or declining as volume scales, the operation is efficient. If cost per review is increasing, something is broken. Maybe case complexity is rising and you're not adjusting reviewer pay or tooling. Maybe reviewer productivity is declining and you're not catching it. Maybe you're hiring faster than you can train, and new reviewers are dragging down throughput. Rising cost per review is an early warning that the operation won't scale profitably.

The second key executive metric is review coverage as a percentage of production output. If your product generates 500,000 AI outputs per week and your reviewers evaluate 10,000 of them, you're covering 2%. If coverage drops from 2% to 1.5% while production volume doubles, you're falling behind. Executives need to know whether you're keeping pace with product growth or losing ground. Falling coverage means you're either under-resourced or under-tooled. Either way, it's a strategic problem that requires budget or prioritization decisions.

The third key metric is the lag between review and action. How long does it take for a review finding to result in a model update, a guideline change, or a policy intervention? If reviewers flag a dangerous failure mode and it takes eight weeks to fix it in production, the review operation is providing historical data, not actionable intelligence. Executives care about cycle time because slow feedback loops mean slow learning. A fast-moving competitor with a two-week review-to-action cycle will outlearn you. Tracking lag makes the feedback loop visible and creates accountability for closing it.

## What Operational Leads Need to Know

Review leads manage day-to-day operations. They need real-time or near-real-time data to make tactical decisions: which cases to prioritize, which reviewers to assign to which tasks, when to escalate an unclear case, when to pause and recalibrate the team. The operational dashboard is a control panel, not a report. It updates continuously and surfaces problems the moment they occur.

The operational dashboard shows current backlog size, current reviewer availability, current throughput, and current agreement on cases reviewed in the past hour or the past shift. It highlights reviewers who have been idle for more than a set threshold — say, ten minutes — because that might mean they're stuck on a case and need help. It highlights cases that have been in the queue for more than a set time threshold because those represent bottlenecks. It shows which task types are accumulating backlog and which are clearing smoothly.

The operational view segments by task type, by reviewer, and by shift. The review lead needs to see that task type A has a 300-case backlog while task type B has a 20-case backlog, because that tells them to reassign reviewers. They need to see that reviewer X is averaging 8 minutes per case while the cohort median is 4 minutes, because that tells them to check in and see if the reviewer is confused or handling disproportionately hard cases. They need to see that the evening shift has 15% lower agreement than the day shift, because that tells them to provide better support for evening reviewers.

The operational dashboard includes alerts and notifications. If a reviewer has been working on a single case for more than 15 minutes, the system flags it. If backlog for a specific task crosses a threshold, the system alerts the lead. If agreement on cases reviewed in the past two hours drops below a threshold, the system notifies the lead to investigate. These alerts allow the lead to intervene proactively rather than discovering problems retrospectively in a weekly report.

Operational dashboards refresh frequently — every minute for high-volume operations, every five minutes for lower-volume teams. The data is fresh enough to reflect current state but not so fresh that natural variance creates false alarms. A one-minute backlog spike is noise. A ten-minute backlog spike is a signal. The refresh rate balances responsiveness with stability.

## What Engineers Need to Know

Engineers integrating review feedback into model training or evaluation pipelines need a different view. They care about data quality, labeling consistency, edge case coverage, and the correlation between review findings and production outcomes. They need to understand which types of errors reviewers catch, which types they miss, and whether the review process is biased toward certain error modes.

The engineering dashboard segments performance by task type, by model version, and by data source. It shows agreement, precision, recall, and label distribution for each segment. If reviewers are evaluating outputs from GPT-5.2 and Claude Opus 4.5 side by side, the engineer needs to see whether agreement differs by model. If Claude outputs have 92% agreement and GPT outputs have 84%, either the GPT outputs are more ambiguous or the reviewers are less calibrated on GPT's output style. Either way, it's actionable information.

The engineering view includes correlations between review metrics and production metrics. If reviewer-flagged error rate increases by 3% in week one and production user-reported error rate increases by 2% in week two, the correlation suggests that reviews are a leading indicator of production issues. If review error rate increases but production error rate stays flat, reviews might be overcorrecting or catching issues that users don't care about. The correlation tells the engineer whether to trust the review signal.

Engineers also need access to raw review data, not just aggregates. They need to download the cases where reviewers disagreed, the cases where reviewers skipped, and the cases where reviewers flagged errors. This data feeds back into training pipelines, eval datasets, and red-teaming exercises. The engineering dashboard provides export functionality — not just charts, but CSV or JSON exports of the underlying review labels, timestamps, and metadata.

The engineering dashboard updates weekly or daily, depending on how frequently models are retrained. If you retrain weekly, you need weekly review summaries to inform the next training run. If you retrain monthly, weekly updates are sufficient for trending but monthly summaries drive decisions. The cadence matches the decision cycle.

## Balancing Detail and Clarity

The biggest mistake in stakeholder reporting is showing too much. A dashboard with 40 metrics and 15 charts overwhelms the viewer. They don't know what to look at first. They don't know which metrics matter and which are informational. They skim, miss the important signals, and stop checking the dashboard. Effective reporting requires brutal prioritization. Show the three to five metrics that drive decisions. Hide everything else unless the viewer explicitly asks for it.

For executives, the three core metrics are cost per review, review coverage as a percentage of production volume, and production error rate. Everything else is supporting detail. If an executive wants to drill down into why cost per review increased, they can click through to a more detailed view. But the default view shows the headline number and the trend. That's enough to make a budget decision or a hiring decision.

For operational leads, the five core metrics are current backlog size, current throughput, current reviewer availability, current agreement on recent cases, and alerts for stuck reviewers or stuck cases. Everything else is noise during an active shift. If the lead wants to understand why agreement dropped, they can drill into reviewer-level performance. But the default view shows the operational state and the immediate problems.

For engineers, the core metrics are agreement by task type, precision and recall by model version, and the correlation between review error rate and production error rate. Everything else is exploratory. If the engineer wants to understand why precision is low on a specific task, they can drill into case-level data. But the default view shows the quality trends and the model-level segmentation.

The detail exists — it's just hidden behind drill-downs, filters, and exports. The viewer starts with the high-level summary. If they need more, they click. If they don't, they're not overwhelmed. This layered approach respects the viewer's time and attention.

## Frequency and Cadence for Each Audience

Different stakeholders review metrics on different cadences. Operational leads check dashboards multiple times per day. Engineers check weekly. Executives check monthly or quarterly. If you send daily emails to executives, they'll ignore them. If you send monthly summaries to operational leads, they're useless. Match the reporting cadence to the decision cycle.

Operational leads get real-time dashboards that they check on-demand. They don't need scheduled reports — they need live visibility. If something breaks, they check the dashboard. If everything is smooth, they check less often. The dashboard is always available, always current. Supplement the live dashboard with end-of-shift summaries: total cases reviewed, average agreement, any alerts that fired, any cases that needed escalation. The summary gives the lead a record of the shift without requiring them to remember what the dashboard said six hours ago.

Engineers get weekly summaries delivered via email or Slack. The summary includes the top-line metrics — agreement, review volume, error rate — and links to the full dashboard for drill-down. The email is short — five bullet points and two charts. It takes 60 seconds to read. If something looks anomalous, the engineer clicks through to investigate. If everything looks normal, they file it and move on. The weekly cadence keeps the review operation top-of-mind without becoming a daily distraction.

Executives get monthly or quarterly reports, depending on the maturity of the operation. Early-stage operations with high volatility warrant monthly check-ins. Mature operations with stable metrics can report quarterly. The report is a slide deck, not a dashboard. It tells a story: here's where we were last quarter, here's where we are now, here's the trend, here's what we're doing about it. It includes cost trends, volume trends, quality trends, and headcount trends. It highlights wins — we reduced cost per review by 12% — and flags risks — agreement has declined three points and we're investigating. The deck is designed for a 15-minute conversation, not a deep-dive analysis.

Adjust cadence when something breaks. If a critical quality issue emerges, escalate to daily updates for executives until it's resolved. If a new model release causes a spike in review volume, increase engineering updates to daily for the first week. Once the situation stabilizes, return to the normal cadence. The ability to flex reporting frequency up and down keeps stakeholders informed during crises without drowning them in updates during steady state.

## Narrative Reporting vs Pure Data

Numbers without context are ambiguous. If cost per review increased from 2.10 dollars to 2.40 dollars month-over-month, is that bad? Maybe. Or maybe you intentionally shifted to more complex cases that require more expertise and time. Or maybe you hired a new cohort of reviewers who are still ramping. Or maybe a third-party annotation service raised prices. The number alone doesn't tell you whether to be concerned. The narrative does.

Effective stakeholder reporting pairs every key metric with a one-sentence explanation. Cost per review increased 14% due to a shift toward higher-complexity medical case reviews that require specialized domain expertise. That sentence tells the executive whether the increase is expected and justified or whether it's a problem. Review volume increased 22% but headcount only increased 10%, resulting in higher throughput per reviewer due to improved tooling. That sentence tells the executive that efficiency is improving, not that the team is overworked.

Narrative reporting also surfaces actions taken. If agreement dropped from 91% to 87%, don't just report the drop — report what you're doing about it. We identified the drop as driven by new reviewers in the June cohort and have scheduled additional calibration sessions for that group. Expected recovery to 90% by end of next month. The narrative turns a problem metric into a managed risk. The executive knows you're aware of the issue and addressing it.

For operational and engineering audiences, narrative can be briefer. The weekly engineering summary might say: Agreement on summarization tasks dropped 4 points; root cause was ambiguous guideline language for edge case handling; guidelines updated on Friday. That's enough context for the engineer to understand the drop, the fix, and the expected trajectory. They don't need a paragraph — they need the facts.

Include narrative in charts via annotations. If a chart shows a sudden spike in review volume in mid-March, annotate the chart: new model release on March 12 increased edge case volume by 30%. The viewer sees the spike, reads the annotation, and understands the cause without needing to ask. Annotations make charts self-documenting and reduce the number of follow-up questions.

## Building Trust Through Transparency and Honesty

Stakeholder reports are not PR documents. If something is broken, say so. If you don't know the root cause yet, say that too. Executives, engineers, and operational leads all work with incomplete information and complex systems. They understand that things break. What they can't tolerate is being misled or kept in the dark. A report that only surfaces good news loses credibility immediately.

If a metric is trending in the wrong direction, surface it prominently. Don't bury it on page five or omit it from the summary. Show the trend, explain what you know about the cause, and describe what you're doing to investigate or fix it. If you don't know the cause yet, say: Agreement dropped 6 points over two weeks; investigation in progress; will report findings in Friday's update. That's honest and actionable. It sets expectations and buys you time to diagnose properly.

If a metric looks good but you have concerns about its validity, say so. Throughput increased 18% this month, but we suspect this is due to reviewers skipping more complex cases; investigating case mix and skip rate. That kind of caveat prevents stakeholders from making decisions based on misleading data. It also signals that you're thinking critically about the metrics, not just reporting them mechanically.

Transparency also means surfacing uncertainty. If you're testing a new routing algorithm and it's too early to know if it's working, say that. New routing algorithm deployed on Monday; early data shows 5% improvement in throughput but sample size is still small; will have confident results in two weeks. Stakeholders respect honesty about what you know and what you don't. They lose trust when you present tentative findings as definitive conclusions.

## Customizing Reports for Remote and Distributed Teams

When stakeholders are distributed across time zones and regions, live dashboards and synchronous reporting don't work well. A review lead in Singapore can't attend a 9 a.m. Pacific standup. An executive in London can't wait for a U.S.-based team to compile a report during their business hours. Distributed reporting requires asynchronous, self-service access to metrics.

Build dashboards that are accessible 24/7 from anywhere. Use cloud-based tools that don't require VPN or on-premise access. Ensure that dashboards load quickly even on slower international connections. Provide both a web view and an exportable summary so stakeholders can download data for offline review if needed.

For asynchronous updates, use written summaries delivered via email or Slack. A daily operational summary posted at the end of the U.S. workday is available for Asia-Pacific leads to read when they start their morning. A weekly engineering summary posted on Monday morning is available for European engineers to read Monday afternoon and for U.S. engineers to read Monday morning. The written format allows each stakeholder to consume the update when it fits their schedule.

Include time-zone-aware annotations in shared dashboards. If a metric shows a sudden drop at 3 p.m. Pacific, annotate it with the UTC time as well. A London-based stakeholder reading the chart won't have to convert time zones mentally to understand when the event occurred. Small details like this reduce friction and make the data more accessible to global teams.

The next subchapter covers metrics gaming and perverse incentives — how reviewers and teams optimize for the wrong metrics when incentives misalign, and how to design metrics and compensation structures that encourage quality over gaming.

