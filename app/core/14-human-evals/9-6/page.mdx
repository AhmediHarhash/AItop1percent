# 9.6 — Versioning Review Data with Model Versions

Model versions change. Ground truth evolves. Review data generated under one model version has different meaning than review data generated under another. If you don't version your review data alongside your model versions, you lose the ability to debug degradations, reproduce training runs, and understand how model behavior changed over time.

When you train a new model and performance drops, the first question is always: what changed? If your review data isn't versioned, you can't answer that. You don't know which corrections came from which model version. You don't know if the training set shifted. You don't know if the new failures are novel or if they're repeats from an earlier model generation. You're debugging blind.

Versioning review data means tagging every correction, every review case, and every quality measurement with the model version that produced the original output. It means tracking which model version was active when each piece of review data was created. It means being able to reconstruct the complete review history for any model version and compare it to any other version. It's version control for your human feedback loop.

## Why Model Version Matters for Review Data

The same input sent to two different model versions produces two different outputs. If a reviewer corrects the output from version 12, that correction is tied to version 12's behavior. If version 13 already fixed that mistake, the correction is obsolete. If version 13 made a different mistake on the same input, you need a new correction for version 13.

Review data without model versioning is ambiguous. You have a correction, but you don't know which model it corrects. You have a quality score, but you don't know which model it scores. You have a set of flagged edge cases, but you don't know if they're edge cases for your current model or for a model you deprecated six months ago.

Versioning makes everything precise. A correction tagged with model version 12 tells you that version 12 produced the wrong output, and here's the right output. A quality score tagged with version 15 tells you that version 15 achieved 91% accuracy on reviewed cases. A flagged edge case tagged with version 18 tells you that version 18 still struggles with this pattern. You can compare version 18's edge cases to version 12's edge cases and see which problems you solved and which ones persist.

Versioning also lets you filter review data by recency. If your ground truth policies changed in January 2026, review data from December 2025 reflects old policies. If you tag review data with both model version and timestamp, you can filter to only recent data from recent models. That prevents stale corrections from polluting your training set or your quality metrics.

Model versioning is especially critical for rollback scenarios. If version 20 performs worse than version 19, you roll back to version 19. Now you need to know which review data was generated under version 20 so you can exclude it from version 19's quality reports. Otherwise your quality dashboards mix data from two different models and show nonsense.

## Tagging Corrections with Model Version

Every time a correction is created, you tag it with the model version that produced the original output. That tag never changes. Even if the correction is later updated or disputed, the model version tag stays the same because it records which model's behavior the correction was responding to.

The tag is usually a model version identifier. That could be a semantic version number like 2.3.1, a Git commit hash, a deployment timestamp, or a UUID. The identifier needs to be unique and traceable back to the exact model weights and configuration that were active in production when the output was generated. If you deploy multiple times per day, timestamp-based identifiers work well. If you deploy weekly, semantic versions work well.

The model version is captured at output generation time, not at review time. When your production system generates an output, it knows which model version it used. It writes that version identifier to the output record. When that output later gets reviewed, the version identifier flows into the review record. The reviewer never has to manually specify the version. It's already there.

If an output was generated by a routing system that used multiple models, you tag the correction with the specific model that generated that output. If your router sent 80% of traffic to GPT-5.2 and 20% to Claude Opus 4.5, corrections for GPT-5.2 outputs get tagged with GPT-5.2 version info, and corrections for Claude Opus outputs get tagged with Claude Opus version info. You need to track which router version made the decision, which model it routed to, and which version of that model was active.

Some teams also tag corrections with the prompt version. If your prompt is versioned separately from your model, a correction might be responding to a specific combination of model version 18 and prompt version 5. Tagging both lets you analyze whether a failure was caused by the model, the prompt, or the interaction between them. If you later update the prompt but keep the model, you can compare corrections across prompt versions to see if the new prompt fixed old failure modes or introduced new ones.

Tagging happens automatically in the data pipeline. The production system writes outputs to a database with model version metadata. The review system reads from that database and inherits the metadata. The correction propagation pipeline copies the metadata into the training data schema. Nobody manually tags anything. The version flows through the entire system as structured metadata.

## Versioned Training Sets and Reproducibility

Training sets should be versioned alongside model versions so you can reproduce any training run and understand exactly what data produced which model.

When you train model version 20, you're using a specific snapshot of your training data. That snapshot includes your original labeled dataset plus all corrections propagated up to the moment the training run started. If you don't version that snapshot, you can't reproduce the training run later. If version 20 performs well and you want to retrain it six months later, you need the exact same data. If you've been continuously adding corrections to the training set, the current training set is different from what you used in the original run. Without versioning, you can't go back.

The simplest versioning strategy is snapshot-based. Every time you start a training run, you take a snapshot of the training data and tag it with the model version you're about to train. The snapshot is immutable. Even if new corrections are added to the live training set later, the snapshot doesn't change. When version 20 is trained, you create a snapshot called training-data-v20. When version 21 is trained, you create training-data-v21. If you need to reproduce version 20, you use training-data-v20.

A more sophisticated strategy is delta-based. You store the original training set as version 0. Each subsequent version is stored as a delta: the set of corrections and updates added since the previous version. Training data version 20 is the base set plus deltas 1 through 20. This saves storage space if your correction volume is high and your base set is large. It also makes it easy to see what changed between versions. Delta 18 to delta 20 shows you exactly which corrections were added in versions 19 and 20.

Both strategies support reproducibility. If you need to retrain model version 15 because you want to apply a bugfix or a new hyperparameter, you pull training-data-v15 or you reconstruct it from the base set plus deltas 1 through 15. You run the training job with those exact examples. You get a model that's comparable to the original version 15.

Versioned training sets also let you debug performance changes. If version 20 performs worse than version 19, you diff training-data-v20 and training-data-v19. You see which corrections were added. You analyze whether those corrections introduced new edge cases, shifted the label distribution, or included low-quality annotations. You might find that version 20 included 500 corrections from a new reviewer who hadn't been properly calibrated. That explains the regression. You exclude those corrections, retrain, and get version 21 with better performance.

Versioning training data is especially important if you're doing continuous training. New corrections flow in daily. The model retrains weekly. Without versioning, you don't know which corrections were in which training run. With versioning, every model version has a corresponding training data version, and you can trace every performance change back to a specific set of data updates.

## Tracking Ground Truth Evolution Over Time

Ground truth changes. Policies change. Domain knowledge evolves. Review data captures that evolution, but only if you version it.

In March 2025, your legal review team might have labeled a contract clause as low-risk. In August 2025, a regulatory change made that same clause high-risk. The March correction is now wrong by August standards. If you train a model in September 2025 on unversioned data, you might include both the March correction (low-risk) and the August correction (high-risk) for similar clauses. The model sees contradictory labels and learns noise.

If your review data is versioned, you can filter by date and model version. You can exclude corrections from before the regulatory change. You can train only on corrections that reflect current ground truth. You can also track how many corrections were invalidated by the policy change, which tells you how much your ground truth shifted.

Versioning also lets you measure ground truth stability. If 90% of corrections from model version 10 are still valid in model version 20, your ground truth is stable. If only 40% are still valid, your ground truth is shifting rapidly and you need to retrain frequently. Some domains have stable ground truth. Contract risk assessment in a stable regulatory environment stays consistent for years. Other domains have fluid ground truth. Content moderation policies shift every quarter. Social media trends change every month. Knowing your ground truth stability tells you how often to retrain and how much historical data to keep.

You track evolution by comparing versioned review datasets. You take all corrections from model version 10. You re-run them through your current model (version 20) and see how many the current model gets right. If version 20 gets 95% of version 10's corrections right, it learned well from historical feedback. If it gets only 60% right, either ground truth shifted or the model regressed.

You also track evolution by periodically re-reviewing old cases. Every quarter, you sample 200 cases that were reviewed six months ago. You send them to current reviewers without showing the old corrections. You see how often current reviewers agree with old reviewers. High agreement means ground truth is stable. Low agreement means it's shifting. If agreement is low, you investigate. Did policies change? Did reviewer training change? Did the domain itself change? The versioned review data gives you the ability to ask and answer these questions.

## Version Metadata for Debugging and Analysis

Model version tags enable debugging workflows that are impossible without versioning. When a production issue surfaces, you need to know which model version caused it, which review data that version was trained on, and whether reviewers have already flagged similar failures.

If users report that model version 22 is generating offensive outputs, you pull all review cases for version 22. You filter to cases flagged for offensive content. You see that version 22 has 3x more offensive content flags than version 21. You diff the training data. You find that version 22 included 1,200 corrections from a batch that wasn't properly filtered for toxicity. You exclude that batch, retrain, and deploy version 23. The offensive output rate drops back to normal.

None of that is possible without version metadata. You need to know which review cases correspond to which model version. You need to be able to filter review data by version. You need to be able to compare version 22's review data to version 21's. Versioning makes all of that straightforward.

Version metadata also powers performance analysis. You can plot quality metrics by model version. You see that version 15 had 89% accuracy on reviewed cases, version 16 had 87%, version 17 had 91%. You correlate version 17's improvement with the fact that version 17 included 3,000 corrections from a major active learning push. You see that active learning worked. You invest more in it.

You can also analyze failure mode evolution. You tag review cases with failure mode labels: hallucination, refusal, formatting error, factual error. You track how often each failure mode appears by model version. You see that hallucinations decreased from 5% in version 12 to 2% in version 20, but refusals increased from 1% to 4%. That tells you the model got more conservative. You adjust your training strategy to balance safety and helpfulness.

Version metadata makes all of this queryable. You run SQL queries or analytics dashboards that slice review data by model version, by timestamp, by failure mode, by reviewer. You see patterns. You make decisions. You improve the model. Without versioning, you're guessing.

## Schema Design for Versioned Review Data

Your review database schema needs version fields on every record that ties back to a model output. At minimum, you store model version, timestamp, and correction ID. For complex systems, you also store prompt version, routing decision metadata, deployment environment, and region.

A typical versioned correction record includes the input, the model's original output, the reviewer's corrected output, the model version that generated the original output, the timestamp when the output was generated, the timestamp when the correction was made, the reviewer ID, the confidence score, and any flags or metadata. That's enough to fully reconstruct what the model did, what the reviewer changed, and when.

If you're using model routing, you add routing metadata. Which router version made the decision? Which models were available? Which model was selected and why? If the router chose GPT-5.2 over Claude Opus 4.5 because of latency preferences, that context matters. A correction for a GPT-5.2 output might not apply to a Claude Opus output even if the input is the same.

If you're running multi-region deployments, you add region metadata. A correction from your US deployment might not apply to your EU deployment if the two regions use different models or different prompts. Versioning by region lets you track performance separately and train region-specific models if needed.

The schema should also support version ranges. Some corrections apply to a range of model versions. If a correction was made for model version 12 and versions 13 through 18 all have the same behavior on that input, the correction is valid for the range 12 to 18. When version 19 fixes the issue, the correction becomes obsolete for version 19 and later. Storing version ranges prevents you from propagating obsolete corrections to training data for new models.

Schema design also needs to handle version deprecation. When you retire a model version, you mark all associated review data as deprecated. Deprecated data doesn't propagate to training. It doesn't appear in live quality dashboards. But it's still queryable for historical analysis. You might want to compare your current model's performance to a model from two years ago. Deprecated data makes that possible.

The key principle is that version metadata is first-class data, not a side note in a comments field. It's structured, queryable, and indexed. You can filter, aggregate, and join on version fields just like you would on timestamps or user IDs. That makes versioning operationally useful instead of just conceptually important.

The next subchapter covers review triggers from model confidence — how to automatically send outputs to review when the model is uncertain, how to set confidence thresholds that balance review load and quality, and how to use model scores to prioritize which cases reviewers see first.

