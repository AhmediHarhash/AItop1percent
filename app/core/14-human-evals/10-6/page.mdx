# 10.6 — Cost Metrics: Per-Item and Per-Decision Costs

The finance team asks a simple question: how much does each review cost? The operations team gives a vague answer: it depends on complexity, reviewer tier, tooling overhead, and whether you count escalations. Finance pushes back: we need a number. Operations provides an average: twelve dollars per review. Finance builds a forecast based on that number. Six months later, the forecast is wrong by 40%, the review budget is overspent, and nobody understands why. The problem is not the math — the problem is treating cost as a single number when it is actually a distribution shaped by case complexity, reviewer efficiency, tooling infrastructure, and operational overhead.

Review cost metrics determine whether a human-in-the-loop AI system is financially viable. A system that costs three dollars per decision scales to millions of decisions. A system that costs sixty dollars per decision scales to thousands. A system that costs three dollars on average but sixty dollars at the 90th percentile scales to millions of low-complexity cases but becomes unaffordable the moment the product attracts higher-complexity use cases. The cost structure determines the business model. You cannot build the right product without understanding the real cost distribution.

## Direct Cost: Reviewer Compensation per Hour

The most visible cost is reviewer compensation. A reviewer earning 25 dollars per hour who completes ten reviews per hour costs 2.50 dollars per review in direct labor. A reviewer earning 60 dollars per hour who completes three reviews per hour costs 20 dollars per review. The per-review cost depends on both the hourly rate and the throughput, and both variables change with reviewer skill level and case complexity.

Fully loaded cost includes benefits, payroll taxes, and overhead. A reviewer with a 25-dollar hourly rate has a fully loaded cost of 35 to 40 dollars per hour once you include health insurance, retirement contributions, employer-side payroll taxes, and workspace costs. The 2.50-dollar-per-review direct cost becomes 3.50 to 4.00 dollars per review fully loaded. Most teams calculate cost using hourly rate and forget the overhead. When finance audits the program, the real cost is 40% higher than the reported cost. The budget breaks.

A customer support review team estimated per-review cost at 3 dollars based on hourly wages. When finance calculated fully loaded cost including benefits and infrastructure, the real cost was 4.80 dollars per review. The team was processing 200,000 reviews per month. The difference between the estimated cost and the real cost was 360,000 dollars per month. The product was priced based on the 3-dollar estimate. The margin disappeared. The team had to renegotiate pricing, cut scope, or absorb the loss. The mistake was not the wage calculation — it was failing to account for the full cost structure from the beginning.

## Cost by Review Tier: Simple vs Complex Cases

Aggregating cost across all review types hides the cost distribution. A Tier 1 review — simple binary decision, clear rubric, low cognitive load — takes two minutes and costs 1.50 dollars. A Tier 3 review — deep expertise required, cross-referencing external systems, written justification, audit trail — takes thirty minutes and costs 25 dollars. If 80% of reviews are Tier 1 and 20% are Tier 3, the average cost is 6.20 dollars per review. But the product economics are completely different for the two tiers.

A content moderation platform priced their service at 8 dollars per review based on average cost. They attracted a customer segment that submitted mostly Tier 3 cases — nuanced political content requiring cultural context, deep policy interpretation, and senior reviewer judgment. The average cost for that customer was 22 dollars per review. The platform lost 14 dollars on every review. They were growing revenue while losing money on every transaction. Six months later, they introduced tiered pricing: 2 dollars for Tier 1 reviews, 10 dollars for Tier 2 reviews, 30 dollars for Tier 3 reviews. Customers with mostly Tier 1 cases paid less. Customers with mostly Tier 3 cases paid more. The pricing matched the cost structure, and the product became profitable.

Cost distribution also determines capacity planning. If you staff for average cost, you will under-staff for high-complexity periods and over-staff for low-complexity periods. A legal review system had an average review cost of 15 dollars, but the cost spiked to 40 dollars during contract negotiation season when every clause required deep analysis. The team staffed for the 15-dollar average and ran out of capacity during peak season. They either delayed reviews — violating SLAs — or paid overtime — driving cost above 50 dollars per review. The next year, they staffed for the 75th percentile cost instead of the median. They had excess capacity during low-complexity periods, but they absorbed it by assigning reviewers to training, rubric refinement, and quality audits. The cost structure stabilized, and the SLAs held during peak season.

## Escalation Cost: The Hidden Multiplier

Escalated reviews cost more than first-pass reviews because they require senior reviewers, involve longer review time, and often require coordination across multiple people. A first-pass review that costs 3 dollars escalates to a senior reviewer who spends fifteen minutes and costs 12 dollars, and then escalates again to a committee of three people who spend twenty minutes each and cost 45 dollars. The total cost of that single decision is 60 dollars. If escalation rate is 15%, and escalated cases cost ten times more than first-pass cases, the escalation cost dominates the total program cost even though it represents a minority of volume.

A healthcare AI review system had a first-pass review cost of 5 dollars and an escalation cost of 40 dollars. Escalation rate was 12%. The finance team calculated cost as 5 dollars times total volume. The real cost was 5 dollars times 88% of volume, plus 40 dollars times 12% of volume. The blended cost was 9.20 dollars per review — 84% higher than the reported cost. The budget was based on the 5-dollar number. The program overspent by 1.2 million dollars in its first year. The finance team demanded an explanation. The operations team showed the escalation data. Finance had never been told that 12% of reviews required a second tier. The cost model was corrected, the budget was adjusted, and the product pricing was revised.

Escalation rate is not static. It increases when case complexity increases, when junior reviewers are undertrained, or when the rubric is ambiguous. A 12% escalation rate can become 20% overnight if the product launches a new feature that introduces edge cases the rubric does not cover. A cost model that assumes fixed escalation rate will break the moment the product evolves. The cost forecast must model escalation rate as a variable driven by case mix, reviewer skill distribution, and rubric coverage.

## Tooling and Infrastructure Cost: The Per-Review Overhead

Review tooling has a fixed cost — software licenses, infrastructure hosting, integration maintenance — and a variable cost — API calls for data retrieval, storage for audit logs, compute for real-time scoring. The fixed cost is easy to allocate: divide annual tooling cost by annual review volume. The variable cost is harder because it depends on which features the review requires.

A fraud review system used an external API to pull transaction history for each review. The API cost 0.30 dollars per call. Every review required one API call. The per-review infrastructure cost was 0.30 dollars. When the team added a second API to pull credit bureau data for high-risk reviews, the variable cost increased to 0.30 dollars for standard reviews and 0.80 dollars for high-risk reviews. The cost structure became bimodal. The average infrastructure cost was 0.45 dollars, but that average was meaningless — half the reviews cost 0.30 dollars, half cost 0.80 dollars. The finance model needed to track two cost tiers, not one average.

Fixed infrastructure costs scale with total review volume, but they do not scale linearly. A review system that processes 10,000 reviews per month might pay 2,000 dollars per month for tooling — 0.20 dollars per review. The same system processing 100,000 reviews per month might pay 8,000 dollars per month for tooling — 0.08 dollars per review. The per-review infrastructure cost decreases with scale. A financial model that assumes constant per-review cost will overestimate costs at high volume and underestimate costs at low volume. The cost curve must reflect the step-function nature of infrastructure pricing.

## Opportunity Cost: What Reviewers Are Not Doing

The cost of a review is not just the time the reviewer spends reviewing. It is also the time they are not spending on other work. A senior engineer who spends ten hours per week reviewing model outputs is not spending ten hours per week building features, improving infrastructure, or mentoring junior engineers. The opportunity cost is the value of the foregone work.

A machine learning team at a legal AI startup assigned two senior engineers to review fine-tuning outputs every week. Each engineer spent eight hours per week on review. The direct cost was their hourly rate times eight hours — roughly 1,200 dollars per week in fully loaded cost. The opportunity cost was harder to quantify. Those engineers were not shipping the retrieval improvement project, not building the evaluation pipeline, not training junior engineers. The delayed projects cost the company six weeks of lost product velocity and one lost customer who needed the retrieval feature to sign. The review program cost 1,200 dollars per week in direct cost and tens of thousands of dollars in opportunity cost.

Opportunity cost is highest when high-skill people do low-skill work. A senior data scientist reviewing annotation quality could be building the next-generation evaluation model. A senior legal reviewer checking basic contract clauses could be training junior reviewers or refining rubrics. The cost of misallocated talent is not visible in the budget, but it shows up in hiring needs, project delays, and attrition. The most expensive review program is the one that burns out your most valuable people on tasks that could be handled by others.

## Cost Efficiency: Cost per Correct Decision

Cost per review is the wrong denominator if the reviews are wrong. A system that costs 2 dollars per review but has a 15% error rate costs 2.35 dollars per correct decision. A system that costs 4 dollars per review but has a 3% error rate costs 4.12 dollars per correct decision. The second system is more expensive per review but cheaper per correct decision. The business pays for correct decisions, not for reviews.

Error correction cost amplifies the cost of low-accuracy review. A customer support review that approves an incorrect response costs 3 dollars in review time plus 20 dollars in customer service escalation when the customer complains. The total cost is 23 dollars. A more careful review that costs 5 dollars but catches the error before the response is sent saves the 20-dollar escalation. The higher-cost review process is cheaper in total cost of ownership.

A content moderation platform tracked cost per review at 1.80 dollars and celebrated it as industry-leading efficiency. A financial audit revealed that 8% of reviews were incorrect, leading to either wrongful content takedowns or policy violations that required escalated manual review. Each incorrect decision cost an average of 15 dollars to correct — re-review, appeal handling, and policy team time. The true cost per decision was 1.80 dollars plus 8% times 15 dollars, which equals 3.00 dollars per decision. The platform was not efficient — it was externalizing cost to downstream correction processes. They invested in rubric clarity and reviewer training. Error rate dropped to 3%. Cost per review increased to 2.10 dollars due to longer review time, but cost per correct decision dropped to 2.55 dollars. Total program cost decreased even though per-review cost increased.

## Cost Trends Over Time: Learning Curves and Reviewer Maturity

Review cost is not constant. It decreases as reviewers gain experience, as rubrics improve, and as tooling reduces friction. A new reviewer takes eight minutes per review in their first week and four minutes per review after three months. The cost per review drops by 50% as the reviewer climbs the learning curve. A mature review program has lower cost than a new program because the team has refined processes, trained reviewers, and optimized tooling.

A legal review team tracked cost per review over the first year of a new contract review product. Month one cost per review was 18 dollars. Reviewers were learning the rubric, the interface was clunky, and escalation rate was 25%. Month six cost per review was 11 dollars. Reviewers were faster, the rubric had been clarified, the interface had been streamlined, and escalation rate was 12%. Month twelve cost per review was 8 dollars. The team had built shortcuts for common clause types, trained junior reviewers to handle Tier 1 cases independently, and automated the most repetitive data lookups. The cost curve followed a classic learning curve — steep improvement in the first six months, slower improvement afterward.

Cost trends also reveal when a review process is becoming unsustainable. If cost per review increases month over month despite stable case complexity, the team is either facing reviewer burnout, tooling degradation, or scope creep. A customer support review team saw cost per review increase from 4 dollars to 7 dollars over four months. The operations lead investigated and discovered that product had added three new review criteria without adjusting the rubric, the reviewer training, or the time estimates. Reviewers were spending more time per case because they were uncertain how to apply the new criteria. The cost trend was an early warning that the review process was breaking under scope expansion. The team paused new criteria, retrained reviewers on the existing rubric, and brought cost back to baseline before adding new requirements.

## Cost Benchmarking: Internal and External Comparisons

Cost per review has meaning only in comparison to a reference point. Is 5 dollars per review expensive or cheap? The answer depends on the complexity of the task, the accuracy requirements, the industry, and the alternative cost of automation or elimination. Internal benchmarking compares cost across similar review types within the same organization. External benchmarking compares cost to industry standards or vendor pricing.

Internal benchmarking reveals inefficiencies. A healthcare company ran three different human review programs for clinical notes, billing codes, and medication recommendations. Clinical note review cost 6 dollars per review. Billing code review cost 9 dollars per review. Medication review cost 22 dollars per review. The cost differences were justified by complexity — medication reviews required pharmacist-level expertise. But when the operations team compared reviewer throughput, they found that billing code reviewers were reviewing only four cases per hour while clinical note reviewers were reviewing ten cases per hour for similarly complex tasks. The billing code review process had inefficiencies — poor tooling, unclear rubric, high escalation rate. The team applied the same process improvements they had used for clinical note review. Billing code review cost dropped to 6 dollars per review without sacrificing accuracy.

External benchmarking reveals whether a review process is competitive. A fraud detection company built an in-house review team at a cost of 8 dollars per review. A vendor offered the same service at 5 dollars per review. The company ran a cost breakdown and discovered that the vendor's lower cost came from offshore labor and lower accuracy standards. The vendor's error rate was 12%, compared to the in-house team's 4% error rate. When the company calculated cost per correct decision, the in-house team cost 8.33 dollars per correct decision, and the vendor cost 5.68 dollars per correct decision. The vendor was cheaper even after adjusting for accuracy. But the company valued control, data privacy, and the ability to iterate on the rubric quickly. They kept the in-house team and focused on driving cost down through tooling improvements rather than outsourcing. The external benchmark informed the decision, but it did not dictate it.

## Unit Economics and Scalability: When Review Cost Breaks the Business Model

Review cost determines the ceiling on product scale. If your product generates 10 dollars of revenue per transaction and review costs 8 dollars per transaction, your gross margin is 20%. You can scale the product, but you cannot afford sales, marketing, or product development on a 20% margin. If review cost is 12 dollars per transaction, the product loses money on every transaction. Scale makes the problem worse, not better.

A contract automation startup generated 50 dollars per contract in revenue. Legal review cost 35 dollars per contract. Gross margin was 30%. The company raised venture funding, scaled to 10,000 contracts per month, and burned through their runway. The cost structure did not improve with scale because each contract still required individual human review. The founders realized too late that the business model only worked if they could reduce review cost to under 10 dollars per contract through automation, self-service tooling, or tiered pricing. They pivoted to a model where only high-value contracts received full human review, and low-value contracts received automated review with optional human escalation. Gross margin improved to 60%, and the business became viable.

Cost per review sets the addressable market. A hiring AI platform that costs 100 dollars per candidate review can sell to enterprises hiring for executive roles. It cannot sell to companies hiring for entry-level roles where the cost per hire is 200 dollars. A content moderation platform that costs 0.50 dollars per review can sell to social media platforms processing millions of reports per day. It cannot sell to niche community forums processing hundreds of reports per day unless the per-review cost drops below 0.10 dollars. The cost structure determines who can afford the product.

The next subchapter covers the metrics that reveal when review operations are optimized — quality-adjusted throughput, cost per correct decision, reviewer utilization, and the aggregate operational health score that product and finance teams use to evaluate the review system's performance.

