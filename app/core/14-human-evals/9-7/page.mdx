# 9.7 — Review Triggers from Model Confidence

What should trigger human review? The naive answer is random sampling. The right answer is model confidence. When the model is uncertain, review the output. When the model is confident, skip review. This keeps review load manageable while catching the outputs most likely to be wrong.

Most teams start with random sampling. Every hour, sample 50 outputs and send them to review. It's simple. It's unbiased. It gives you a random view of model performance. But it's also wasteful. You review outputs the model got right with high confidence. You miss outputs the model got wrong but didn't happen to sample. You spend reviewer time on cases that teach you nothing new.

Confidence-based triggering does better. The model scores every output with a confidence estimate. Outputs below a confidence threshold go to review. Outputs above the threshold skip review unless randomly sampled for calibration. You review the uncertain cases — the ones the model itself tells you might be wrong. Your review queue becomes a targeted set of edge cases instead of a random grab bag. Your reviewers spend their time on the cases that matter.

This is active sampling applied to review. The model participates in deciding what gets reviewed. The result is higher yield per review hour, faster identification of failure modes, and tighter feedback loops for model improvement.

## Confidence Signals from Model Outputs

Models produce multiple signals you can use as confidence proxies. The most direct signal is the probability distribution over output tokens. If the model assigns 95% probability to the top token at every generation step, it's confident. If it assigns 40% to the top token and 35% to the second token, it's uncertain. You can aggregate per-token probabilities into a sequence-level confidence score. Low scores trigger review.

For classification tasks, the model outputs a probability distribution over classes. If the top class has 98% probability, the model is confident. If the top class has 52% probability and the second class has 48%, the model is uncertain. You set a threshold. Predictions where the top class probability is below 85% go to review. Predictions above 85% are auto-approved unless flagged by other criteria.

For generation tasks, confidence is harder to measure but still possible. You can use the model's log probability of the generated sequence. You can use the entropy of the token distribution at each step. You can use the number of high-probability alternative sequences. Some teams run the model multiple times with temperature sampling and measure the diversity of outputs. High diversity means uncertainty. Low diversity means confidence.

For retrieval-augmented systems, you have additional signals. If the retrieval system returned no documents above a relevance threshold, the model is working without grounding and uncertainty is high. If the top retrieved document has a relevance score of 0.95, the model has strong grounding and uncertainty is low. If the top three documents contradict each other, uncertainty is high even if individual relevance scores are high.

You can also use external signals. If the input contains out-of-vocabulary terms, uncertainty is higher. If the input is much longer than typical training examples, uncertainty is higher. If the input is from a new user segment or geographic region, uncertainty is higher. These signals don't come from the model's internal probabilities, but they correlate with failure risk.

Most production systems combine multiple signals into a composite confidence score. You weight log probability at 40%, retrieval quality at 30%, input novelty at 20%, and output length at 10%. You compute a score between 0 and 1 for each output. Scores below 0.7 trigger review. Scores above 0.9 skip review. Scores between 0.7 and 0.9 are sampled at 10% for calibration.

The key is that confidence estimation is automatic. The model computes it for every output as part of the inference process. No human decides what to review. The system decides based on the model's own uncertainty. That makes review triggering scalable and consistent.

## Setting Thresholds that Balance Load and Quality

The confidence threshold determines how much goes to review. Set it too high and you review everything. Set it too low and you miss failures. The right threshold balances review capacity, quality requirements, and failure cost.

Start by analyzing your model's confidence distribution. Pull a week of production outputs. Compute confidence scores for all of them. Plot the distribution. You might see that 60% of outputs have confidence above 0.9, 30% are between 0.7 and 0.9, and 10% are below 0.7. Now sample outputs from each range and manually review them. You find that outputs above 0.9 have a 2% error rate. Outputs between 0.7 and 0.9 have a 15% error rate. Outputs below 0.7 have a 45% error rate.

If you can afford to review 10% of outputs, you set the threshold at 0.7. That captures the high-error cases. If you can only review 5%, you set the threshold at 0.6. That captures the very highest-error cases. If you can review 15%, you set the threshold at 0.75 and catch more of the medium-error cases too.

The threshold isn't static. As your model improves, the confidence distribution shifts. What used to be a 0.7 confidence case might become a 0.8 confidence case after retraining. You recalibrate thresholds monthly or quarterly. You re-run the confidence-versus-error analysis. You adjust the threshold to maintain your target review volume while maximizing error capture.

You also need separate thresholds for different use cases or risk tiers. High-risk outputs might go to review if confidence is below 0.95. Medium-risk outputs go to review if confidence is below 0.8. Low-risk outputs go to review if confidence is below 0.6. The threshold reflects the cost of failure. If a wrong output means legal liability, you review aggressively. If it means a suboptimal recommendation, you review conservatively.

Some teams use dynamic thresholds that adapt based on review queue load. If the review queue is empty, the system lowers the threshold and sends more outputs to review. If the review queue is overflowing, the system raises the threshold and sends only the most uncertain outputs. This keeps reviewers working at steady capacity without idle time or overload.

Threshold tuning is an ongoing process. You track two metrics: review yield (percentage of reviewed outputs that needed correction) and miss rate (percentage of unreviewed outputs that were later found to be wrong). You want high yield and low miss rate. If yield is low, your threshold is too high and you're reviewing too much. If miss rate is high, your threshold is too low and you're missing errors. You adjust the threshold to optimize the trade-off.

## Prioritizing Review Queues by Uncertainty

Not all uncertain outputs are equally important. You prioritize the review queue so reviewers see the highest-value cases first.

The simplest prioritization is by confidence score. The lowest-confidence outputs go to the top of the queue. Reviewers work through the queue in order. The cases the model is most uncertain about get reviewed first. This maximizes error capture per review hour because low-confidence outputs have the highest error rates.

A better prioritization combines confidence with business impact. An output with 0.65 confidence that's going to a high-value customer is more important than an output with 0.60 confidence going to a free-tier user. You compute a priority score: confidence score times impact weight. Low confidence and high impact results in high priority. High confidence or low impact results in low priority. Reviewers see high-priority cases first.

You can also prioritize by failure mode. If your model has a known weakness with medical terminology, and an output contains medical terms and has confidence below 0.75, that case gets higher priority than a generic low-confidence case. You tag outputs with domain-specific metadata and boost priority when the output falls into a known failure category.

Some teams prioritize by recency. Outputs generated in the last hour get higher priority than outputs generated yesterday. This keeps review close to real time and makes the feedback loop faster. If a reviewer corrects an output from 30 minutes ago, that correction can propagate to training quickly and fix the issue before it affects too many users.

Other teams prioritize by user segment. Outputs going to enterprise customers get higher priority than outputs going to free users. Outputs from beta users testing a new feature get higher priority than outputs from stable features. You align review priorities with business priorities.

The queue itself needs to support priority-based assignment. When a reviewer opens the queue, they get the highest-priority case that matches their skill set. High-priority cases don't sit unreviewed while reviewers work through low-priority cases. The queue dynamically reorders based on priority scores that update as new outputs arrive.

Priority decay prevents starvation. A low-priority case that sits in the queue for 24 hours gets a priority boost. After 48 hours, it gets another boost. Eventually, even low-priority cases get reviewed. This prevents edge cases from languishing forever just because they're not urgent.

## Confidence Calibration and Threshold Accuracy

Model confidence scores are only useful if they're calibrated. A model that outputs 0.9 confidence should be right 90% of the time. If it's only right 70% of the time, the confidence score is miscalibrated and you can't trust it for review triggering.

Calibration is checked by bucketing outputs by confidence score and measuring accuracy within each bucket. Pull 10,000 outputs. Bucket them into confidence ranges: 0.0 to 0.1, 0.1 to 0.2, up to 0.9 to 1.0. For each bucket, compute the accuracy. If the 0.8 to 0.9 bucket has 85% accuracy, the model is well-calibrated in that range. If it has 60% accuracy, the model is overconfident in that range.

Miscalibration is common. Large language models are often overconfident. They assign high probabilities to wrong answers. Smaller models fine-tuned on narrow tasks are often underconfident. They assign low probabilities to correct answers because they were trained to be cautious. Either way, you need to recalibrate.

The simplest recalibration method is temperature scaling. You add a temperature parameter to the model's output logits. You tune the temperature on a validation set to minimize calibration error. A temperature greater than 1 makes the model less confident. A temperature less than 1 makes it more confident. After temperature scaling, you re-bucket and re-check calibration. Usually one round of temperature tuning is enough.

A more sophisticated method is Platt scaling or isotonic regression. You train a small calibration model that takes the model's raw confidence score as input and outputs a calibrated probability. The calibration model learns the relationship between raw scores and actual accuracy. You apply the calibration model to every output before using the confidence score for review triggering. This corrects non-linear miscalibration that temperature scaling can't fix.

Calibration needs to be checked and updated regularly. As you retrain your model, calibration drifts. A model that was well-calibrated in version 15 might be miscalibrated in version 18. You re-run calibration analysis after every major model update. You retrain the calibration model if needed. You ensure that confidence scores remain trustworthy.

Well-calibrated confidence enables better threshold tuning. If you know that 0.8 confidence really means 80% accuracy, you can set precise thresholds based on your error tolerance. If your quality bar is 95% accuracy, you set the review threshold at 0.95 confidence. If miscalibration is high, you can't do that. You have to set conservative thresholds and review more than necessary because you don't trust the confidence scores.

## Fallback Rules When Confidence is Unavailable

Some models don't expose confidence scores. Some outputs are generated by rule-based systems or non-probabilistic methods. You need fallback rules for when confidence isn't available.

The most common fallback is heuristic-based triggering. You define rules based on output characteristics. If the output contains a disclaimer phrase like "I'm not certain," send it to review. If the output is unusually short or unusually long compared to typical outputs for that input type, send it to review. If the output contains profanity or sensitive terms, send it to review. These rules don't require model confidence. They're based on the output itself.

Another fallback is input-based triggering. If the input contains rare words, send the output to review regardless of confidence. If the input is in a language the model wasn't primarily trained on, send it to review. If the input matches a known edge case pattern, send it to review. You're using input characteristics as a proxy for model uncertainty.

You can also use temporal triggering. The first 500 outputs from a newly deployed model go to review regardless of confidence. After 500 outputs with acceptable quality, you switch to confidence-based triggering. This catches deployment issues early before they affect many users.

Some teams use random sampling as a permanent fallback. Even if every output has high confidence, 1% of outputs are randomly sampled and sent to review. This catches cases where the model is confidently wrong — a failure mode that confidence scores miss. Random sampling also provides a baseline for measuring whether confidence-based triggering is actually more efficient than random sampling.

If you're using multiple models in a routing setup, you can use model agreement as a confidence proxy. Send the same input to two models. If they produce similar outputs, confidence is high. If they produce different outputs, confidence is low and the case goes to review. This works even if neither model exposes internal confidence scores. Agreement is an external signal of uncertainty.

The fallback rules should be conservative. If you're unsure whether an output needs review, send it to review. The cost of over-reviewing is wasted reviewer time. The cost of under-reviewing is wrong outputs reaching users. In most domains, the latter is more expensive.

## Adaptive Thresholds Based on Model Performance

The confidence threshold should adapt as model performance changes. When the model is performing well, you can raise the threshold and review less. When performance degrades, you lower the threshold and review more.

Performance is tracked by monitoring review correction rates. If 95% of reviewed outputs need no correction, the model is performing well and the threshold might be too low. You're reviewing outputs the model is getting right. You raise the threshold from 0.75 to 0.80 and review 20% less. If only 60% of reviewed outputs need no correction, the model is struggling and the threshold might be too high. You're missing errors. You lower the threshold from 0.75 to 0.70 and review 15% more.

Some teams use automated threshold adjustment. A monitoring job runs daily. It computes the correction rate from yesterday's reviews. If the correction rate is below 10% for three consecutive days, it raises the threshold by 0.05. If the correction rate is above 30% for three consecutive days, it lowers the threshold by 0.05. Thresholds adapt automatically to model performance without manual intervention.

Adaptive thresholds also respond to traffic patterns. During a traffic spike, you might raise the threshold to keep review load manageable. During low traffic, you might lower the threshold to make use of idle reviewer capacity. The system balances quality, load, and capacity dynamically.

Thresholds can also adapt by domain or feature. If you launch a new feature and early review data shows a 40% correction rate, you set a low confidence threshold for that feature until performance improves. If an established feature has a 5% correction rate, you set a high threshold and review minimally. Different parts of your system get different thresholds based on their maturity and reliability.

Adaptation works only if you're tracking performance in real time. You need dashboards that show correction rates by model version, by feature, by confidence range. You need alerts that fire when correction rates spike. You need the ability to adjust thresholds quickly without redeploying code. Adaptive thresholds turn review from a static sampling strategy into a dynamic feedback system.

The next subchapter covers ground truth updates from production review — how corrections from production cases become the most valuable training data, how to propagate those corrections back into your training pipeline, and how to use production review to continuously refine what "correct" means in your domain.

