# 10.5 — Reviewer Performance Metrics: Individual and Team

How do you measure reviewer performance fairly? The question feels simple until you realize that the reviewers handling the hardest cases will have lower throughput and higher error rates than the reviewers handling the easiest cases. The reviewer who asks clarifying questions and catches edge cases will appear slower than the reviewer who pattern-matches and clicks through. The reviewer who escalates ambiguous cases responsibly will show more "indecisive" behavior than the reviewer who guesses and moves on. If you measure only speed and volume, you will reward the wrong behaviors and burn out your best people.

Reviewer performance metrics determine who gets promoted, who gets coached, who gets additional training, and who gets managed out. Get the metrics wrong and you will optimize for speed over accuracy, volume over judgment, and conformity over critical thinking. The ops team will hit their throughput targets while the quality of the review system quietly degrades. The best reviewers will leave because they see their careful work penalized while careless work is rewarded. The metrics must measure what actually matters — not just what is easy to count.

## Throughput Metrics: Items Reviewed per Hour

Throughput is the most visible performance metric and the most dangerous. Items reviewed per hour, cases closed per day, decisions made per shift — these numbers are easy to track, easy to compare across reviewers, and easy to turn into a leaderboard. They are also easy to game. A reviewer who wants to maximize throughput will skip hard cases, avoid escalations, rush through ambiguity, and optimize for speed at the expense of accuracy.

A content moderation team measured reviewers purely on items per hour. The top performer reviewed 120 items per hour. The median reviewer reviewed 80 items per hour. Management celebrated the top performer and used them as a benchmark. Three months later, a quality audit revealed that the top performer had a 22% error rate — they were approving content that violated policy and rejecting content that did not. The median reviewers had a 6% error rate. The top performer was not the best reviewer. They were the fastest guesser. The throughput metric had created a perverse incentive, and management had amplified it by holding the wrong person up as a model.

Throughput is a valid metric only when **normalized by task complexity**. A reviewer handling Tier 1 cases — simple, low-ambiguity, high-volume decisions — should have higher throughput than a reviewer handling Tier 3 cases — complex, high-stakes, low-volume decisions. Comparing their raw throughput is comparing apples to engine blocks. The fair comparison is throughput within the same complexity tier, or throughput adjusted by a complexity weight assigned to each case type.

A legal review system assigned complexity scores to every contract clause type. A standard confidentiality clause was scored 1.0. A non-compete clause was scored 2.5. A liability limitation clause was scored 4.0. A reviewer who handled ten standard clauses in an hour had a complexity-adjusted throughput of 10. A reviewer who handled three liability clauses in an hour had a complexity-adjusted throughput of 12. The raw item count made the first reviewer look more productive. The complexity-adjusted throughput revealed that the second reviewer was actually more efficient. The metric shifted from punishing reviewers who took hard cases to rewarding them.

## Accuracy Metrics: Agreement with Ground Truth and Calibration

Accuracy is harder to measure than throughput because it requires ground truth. For some review tasks, ground truth exists — a senior reviewer re-reviews a sample of cases, a domain expert provides the correct answer, or the system tracks whether the decision led to a downstream error. For other review tasks, ground truth is subjective, and accuracy becomes a measure of agreement with a consensus standard rather than correctness in an objective sense.

**Spot-check accuracy** measures how often a reviewer's decision matches a ground truth re-review. A random sample of 5% to 10% of each reviewer's decisions is sent to a senior reviewer or a quality assurance team. The spot-check compares the original decision to the ground truth decision and calculates agreement rate. An agreement rate of 95% means the reviewer is making the same call as ground truth 95% of the time. An agreement rate of 80% means they are missing edge cases, misinterpreting rubrics, or applying inconsistent judgment. Spot-check accuracy is the gold standard for measuring reviewer quality, but it is expensive — it requires dedicated QA capacity and delays feedback.

**Calibration accuracy** measures how often a reviewer's decision matches the consensus of their peers on a shared set of calibration cases. Every week, the team reviews ten to twenty calibration cases together, discusses the correct decision, and records the consensus. Individual reviewers' past decisions on those same cases are compared to the consensus. High calibration accuracy means the reviewer is aligned with team standards. Low calibration accuracy means they are interpreting the rubric differently, applying stricter or looser standards, or missing training on specific edge cases. Calibration accuracy is cheaper than spot-check accuracy and provides faster feedback, but it measures alignment rather than objective correctness.

A healthcare AI company used calibration sessions to measure reviewer accuracy. Every Monday, the review team discussed five clinical note cases, debated edge cases, and reached consensus on the correct decisions. The system compared each reviewer's historical decisions on similar cases to the consensus. One reviewer consistently scored low on calibration accuracy — not because they were careless, but because they were applying stricter safety standards than the consensus. The team discussed whether the stricter standard was appropriate. They decided it was for high-risk cases but not for low-risk cases. The reviewer adjusted their approach for low-risk cases, and their calibration accuracy improved. The metric surfaced a philosophical disagreement about risk tolerance that would not have appeared in a throughput metric.

## Consistency Metrics: Intra-Reviewer Agreement Over Time

Consistency measures whether a reviewer makes the same decision on similar cases over time. A reviewer who rates a contract clause as high-risk on Monday and low-risk on Wednesday for identical language is inconsistent. A reviewer who escalates a content moderation case in the morning and approves an identical case in the afternoon is inconsistent. Inconsistency is a sign of unclear rubrics, insufficient training, fatigue, or cognitive overload.

**Intra-reviewer agreement** is calculated by identifying pairs of cases that are similar or identical and checking whether the reviewer made the same decision on both. If the reviewer handled 200 cases in a week, and 30 of those cases had near-duplicates, intra-reviewer agreement is the percentage of duplicate pairs where the decision matched. High intra-reviewer agreement means the reviewer applies the rubric reliably. Low intra-reviewer agreement means their decisions drift based on time of day, fatigue, or context they are unconsciously importing from previous cases.

A financial fraud review team tracked intra-reviewer agreement and found that one reviewer had 78% consistency — significantly lower than the team average of 94%. The ops lead investigated and discovered that the reviewer was working split shifts — four hours in the morning, four hours in the evening. The evening decisions were more conservative than the morning decisions. The reviewer was cognitively fresh in the morning and risk-averse in the evening after a full day of other work. The team adjusted the schedule to consolidate the shift. Consistency improved to 92%. The metric revealed a scheduling problem, not a skill problem.

Consistency also surfaces rubric ambiguity. If multiple reviewers have low intra-reviewer agreement on the same case type, the problem is not the reviewers — the problem is the rubric. A content moderation team found that five different reviewers had inconsistent decisions on political satire content. Sometimes they classified it as acceptable commentary, sometimes as misinformation. The rubric said "remove false claims" but did not define how to handle satirical exaggeration. The team clarified the rubric to explicitly allow satire with appropriate context markers. Consistency on political content cases improved immediately. The performance metric identified a training gap that the team had not noticed.

## Escalation Rate: When to Elevate and When to Decide

Escalation rate measures how often a reviewer sends a case to a senior reviewer, a specialist, or a committee instead of making a decision themselves. Escalation is necessary when cases exceed the reviewer's expertise, involve unusually high stakes, or fall outside the defined rubric. But excessive escalation clogs the senior review queue, increases latency, and suggests that the reviewer is either under-trained or risk-averse to the point of ineffectiveness.

The ideal escalation rate depends on the reviewer's tier and the case distribution. A junior reviewer handling a mixed queue should escalate 10% to 20% of cases — enough to catch genuine edge cases without overwhelming the escalation queue. A senior reviewer handling pre-escalated cases should have a near-zero escalation rate — they are the escalation destination. A reviewer with a 40% escalation rate is either facing an unusually difficult workload or lacking the training to make decisions independently.

A legal review system tracked escalation rates and found that one reviewer escalated 35% of cases — far above the team median of 12%. The operations lead reviewed the escalated cases and found that most of them were straightforward applications of existing precedent. The reviewer was not missing knowledge — they were missing confidence. The lead paired the reviewer with a mentor for two weeks. The mentor reviewed the same cases in parallel and provided real-time feedback: "This one you can decide, here's why. This one you should escalate, here's why." The reviewer's escalation rate dropped to 14% within a month. The metric identified a confidence gap that additional rubric training would not have fixed.

Escalation rate also surfaces inadequate routing. If every reviewer in a team has a high escalation rate for a specific case type, the routing logic is assigning cases to the wrong tier. A healthcare review system found that all Tier 1 reviewers escalated 60% of cases involving experimental treatments. The case type required clinical expertise that Tier 1 reviewers did not have. The routing logic was updated to send experimental treatment cases directly to Tier 2 reviewers with clinical backgrounds. Escalation rate for that case type dropped to 8%. The metric revealed a routing failure, not a training failure.

## Quality Metrics: Error Types and Severity

Not all errors are equal. A false positive in a content moderation system — blocking content that should have been approved — frustrates a user. A false negative — allowing content that should have been blocked — creates legal risk, brand damage, or user harm. A reviewer who makes ten low-severity errors is less concerning than a reviewer who makes two high-severity errors. Quality metrics must distinguish error types and weight them by impact.

**Error taxonomy** categorizes mistakes by type: false positive, false negative, procedural error, documentation error. Each error type is assigned a severity score based on downstream impact. A false negative that allows fraudulent transaction is severity 5. A false positive that delays a legitimate transaction is severity 2. A documentation error that omits required audit trail information is severity 3. A reviewer's quality score is the weighted sum of errors divided by total cases reviewed. Two reviewers with the same raw error count can have very different quality scores if one makes high-severity errors and the other makes low-severity errors.

A customer support review system tracked errors without distinguishing severity. A reviewer who approved 98% of responses correctly but made two high-severity errors — allowing responses that promised refunds the company does not offer — was ranked higher than a reviewer who approved 96% correctly but made only low-severity tone errors. The error weighting system was inverted. The team introduced severity weighting. High-severity errors counted five times more than low-severity errors. The reviewer with the refund errors moved to the bottom of the quality ranking, received immediate retraining, and stopped making promise-based errors. The severity-weighted quality metric aligned reviewer rankings with actual business risk.

## Feedback Loop Latency: Time from Decision to Feedback

Reviewers improve when they receive feedback quickly. A reviewer who learns they made an error three weeks after the decision cannot connect the feedback to the reasoning they used at the time. A reviewer who learns they made an error within 24 hours can reflect on what they missed and adjust. Feedback loop latency is not a performance metric — it is a system metric that determines how fast performance can improve.

Automated feedback loops are fastest but limited in scope. If the review decision is binary and the downstream system records the outcome, the feedback loop can be instant. A fraud review system shows reviewers whether flagged transactions were confirmed fraud or false positives within 48 hours. A content moderation system shows reviewers whether escalated cases were upheld or overturned by senior review within hours. The feedback is narrow — it tells the reviewer whether they were right or wrong, but not why — but it is fast enough to create a learning loop.

Manual feedback loops are slower but richer. A senior reviewer re-reviews a sample of cases, writes comments on errors, and schedules a one-on-one to discuss. The feedback explains not just what was wrong but why it was wrong and what to look for next time. A legal review team introduced weekly feedback sessions where each reviewer discussed one error case with their manager. The sessions took thirty minutes per reviewer per week. Six weeks later, the team's average accuracy improved from 89% to 94%. The feedback loop latency dropped from three weeks to one week, and the improvement rate accelerated. The time investment in manual feedback paid off in quality gains that automated feedback could not deliver.

## Team-Level Metrics: Aggregate Quality and Load Balance

Individual metrics measure reviewer performance. Team-level metrics measure system performance. A team where every individual hits their throughput and accuracy targets but the queue is still backed up has a capacity problem, not a performance problem. A team where accuracy is high but escalation rate is 40% has a training or routing problem, not a reviewer problem. Team-level metrics reveal whether the system design supports the people working in it.

**Load balance** measures how evenly work is distributed across reviewers. Perfect load balance is not the goal — different reviewers have different skills, experience levels, and availability. But extreme load imbalance signals routing failures or scheduling gaps. A team where one reviewer handles 40% of the weekly volume while three others handle 15% each is not balancing load appropriately. Either the high-volume reviewer is being overworked, or the low-volume reviewers are being underutilized.

A content moderation team found that senior reviewers were handling 60% of total volume despite representing only 30% of headcount. Junior reviewers were handling only 40% of volume. The routing logic was set to "assign to most experienced available reviewer," which meant senior reviewers were always picked first. The queue never reached junior reviewers until senior reviewers were at capacity. The team changed the routing logic to "assign to least experienced qualified reviewer," reserving senior reviewers for escalations and edge cases. Load balance improved. Senior reviewer throughput dropped slightly, but junior reviewer throughput increased significantly. Total team throughput increased by 18% without adding headcount.

**Aggregate accuracy** measures team-wide error rate, weighted by severity and case complexity. A team accuracy of 92% sounds good until you discover that the errors are concentrated in high-stakes cases. A team accuracy of 88% sounds concerning until you discover that the errors are evenly distributed across low-stakes cases and have minimal downstream impact. Aggregate accuracy must be broken down by case type, severity, and error category to be actionable.

## Avoiding Metric Gaming and Perverse Incentives

Every performance metric creates an incentive. The incentive will be gamed if the metric rewards behavior that is measurable but not valuable. Reviewers are not trying to break the system — they are trying to meet the goals the system sets for them. If the system rewards speed over quality, reviewers will optimize for speed. If the system rewards low escalation rates, reviewers will make risky decisions instead of escalating. The metric designer is responsible for anticipating how the metric will be gamed and designing around it.

**Multi-dimensional scorecards** prevent single-metric optimization. A reviewer is evaluated on throughput, accuracy, consistency, escalation appropriateness, and feedback incorporation. No single metric dominates. A reviewer who maximizes throughput but tanks accuracy does not get promoted. A reviewer who maximizes accuracy but has 50% escalation rate does not get a bonus. The scorecard forces balanced performance across all dimensions.

A fraud review team used a single metric: fraud catch rate. Reviewers who flagged the most fraudulent transactions were rewarded. The metric seemed aligned with business goals — catching fraud is the point. But it created a perverse incentive. Reviewers started flagging borderline cases as fraud to increase their catch rate, even when the evidence was weak. False positive rate spiked. Legitimate customers were blocked. The team switched to a multi-dimensional scorecard: fraud catch rate, false positive rate, and decision confidence calibration. Reviewers were rewarded for catching fraud without creating excessive false positives. The gaming stopped. The system optimized for business value instead of a single metric.

The next subchapter covers cost metrics — how to measure the per-item and per-decision cost of human review, how to track cost efficiency, and how to identify when review processes are too expensive to scale.

