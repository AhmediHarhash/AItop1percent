# Chapter 6 — Quality Assurance in Review

Reviewing the reviewers is the meta-problem that determines whether your labels are trustworthy or noise. Quality assurance in human review is not about catching individual mistakes—it is about building systematic oversight that detects patterns, measures consistency, and creates feedback loops that improve performance over time. This chapter covers the full QA stack: audit sampling, blind versus known audits, agreement metrics, error pattern analysis, feedback delivery, performance tiers, corrective action, and external validation.

---

- 6.1 — Reviewing the Reviewers: The Meta-Quality Problem
- 6.2 — Audit Sampling: How Much to Check
- 6.3 — Blind Audits vs Known Audits
- 6.4 — Agreement Metrics: Kappa, Krippendorff, and Beyond
- 6.5 — Error Pattern Analysis: Finding Systematic Issues
- 6.6 — Feedback Loops: From Audits Back to Reviewers
- 6.7 — Performance Tiers and Reviewer Stratification
- 6.8 — Corrective Action Protocols
- 6.9 — Quality Dashboards for Review Operations
- 6.10 — External Audits and Third-Party Validation

---

*You cannot trust labels without trusting the process that created them. Quality assurance is how you earn that trust.*
