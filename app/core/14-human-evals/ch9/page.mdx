# Chapter 9 — Integration with ML Pipelines

Human review does not exist in isolation. It feeds machine learning pipelines, improves model training, triggers retraining cycles, and produces artifacts that downstream systems consume. The integration between your review infrastructure and your ML systems determines whether human judgment scales beyond the moment of review or dies the instant the reviewer clicks submit. This chapter covers the full integration surface: feedback loops, labeling pipelines, active learning, correction propagation, versioning, confidence-based triggers, artifact schemas, and the patterns that connect human review to automated improvement.

---

- 9.1 — Human Review as the Feedback Loop for ML
- 9.2 — Labeling Pipeline Integration
- 9.3 — Active Learning: Models Requesting Human Review
- 9.4 — Human-in-the-Loop Inference Workflows
- 9.5 — Correction Propagation: From Review Back to Training
- 9.6 — Versioning Review Data with Model Versions
- 9.7 — Review Triggers from Model Confidence
- 9.8 — Ground Truth Updates from Production Review
- 9.9 — Labels vs Review Artifacts: The Two-Tier Output Schema
- 9.10 — Artifact Normalization for Downstream Automation
- 9.11 — From Review Artifacts to Product Fixes: Prompt Patches and Blocked Actions
- 9.12 — Real-Time vs Batch Integration Patterns

---

*Human review without ML integration is a sunk cost. Human review with ML integration is an investment that compounds.*
