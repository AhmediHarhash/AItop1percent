# 3.2 — Annotation Surfaces: Forms, Highlights, and Structured Inputs

The annotation surface is the interface element where the reviewer records their judgment. The design of this surface determines speed, accuracy, and data quality. A poorly designed surface forces reviewers to translate their judgment into the structure the form demands. A well-designed surface makes the natural judgment the easiest action. The surface should match the task so closely that the reviewer never thinks about the form — they only think about the judgment.

## Classification Interfaces: Single Select, Multi-Select, Hierarchical

The simplest annotation task is classification: assign one or more labels to an item. The interface for classification depends on cardinality. Single-select classification — one label per item — uses radio buttons or a dropdown. Multi-select classification — multiple labels per item — uses checkboxes or a multi-select dropdown. Hierarchical classification — labels nested in categories — uses a tree or cascading menus.

Single-select radio buttons are fastest for fewer than eight options. The reviewer sees all options at once, clicks once, and moves on. If there are more than eight options, radio buttons create visual clutter. A dropdown reduces clutter but requires two clicks: one to open the dropdown, one to select. For high-volume review, radio buttons with keyboard shortcuts beat dropdowns. Each option gets a single keystroke. The reviewer never touches the mouse.

Multi-select checkboxes are appropriate when reviewers commonly select two or three labels per item. If the average is one label with rare exceptions, multi-select feels heavier than it needs to be. If the average is five or more labels, checkboxes become tedious. The reviewer clicks five times per item. A better design for high-label-count tasks is a searchable multi-select: the reviewer types the first few letters of each label, hits enter, and moves to the next. Typing three letters and hitting enter is faster than clicking a checkbox when the list has 40 options.

Hierarchical classification is necessary when labels have clear parent-child relationships. A content moderation task might classify harmful content into top-level categories like hate speech, violence, or misinformation, and then subcategories like targeted hate, generic slurs, graphic violence, or implied violence. The interface shows the top-level categories first. The reviewer selects one. The interface reveals the subcategories. The reviewer selects one and submits. This reduces cognitive load: the reviewer does not see all 30 subcategories at once, only the 5 to 7 that are relevant after selecting the top-level category.

The mistake teams make is building hierarchical interfaces for flat taxonomies. If labels are not genuinely hierarchical — if subcategories do not depend on parent categories — the tree structure adds unnecessary clicks. A flat list with search is faster.

## Free-Text with Validation

Some tasks require the reviewer to write a justification, a summary, or a note. Free-text inputs are unavoidable for these tasks, but they introduce quality variance. One reviewer writes detailed three-sentence explanations. Another writes five-word fragments. A third writes nothing unless required.

Validation reduces variance. The simplest validation is a minimum character count. If the task requires a justification, the form does not allow submission until the text field contains at least 50 characters. This prevents empty submissions, but it also incentivizes padding. Reviewers write 50 characters of filler because the form requires it, not because the justification needs it.

A better approach is validation based on structure. If the task is explaining why a transaction is fraudulent, the form prompts the reviewer with questions: What pattern triggered suspicion? What context supports this judgment? What alternative explanations were considered? The reviewer fills in answers. The form validates that all questions are answered, not just that the total character count is above a threshold. The data is more structured and more useful for training future models or auditing decisions.

The trade-off is rigidity. Structured prompts work well for tasks where the reasoning structure is consistent. They fail for tasks where reasoning varies widely by case. If forcing structure creates more cognitive load than it saves in data quality, the better choice is free-text with optional structure: the reviewer can answer guided prompts or write freeform, whichever fits the case.

## Span Annotation

Some tasks require the reviewer to highlight specific parts of the input. In a document review task, the reviewer might highlight sentences that contain personally identifiable information. In a content moderation task, the reviewer might highlight the specific phrase that violates policy. Span annotation is more precise than document-level labels, but it is also slower.

The interface for span annotation typically allows the reviewer to click and drag to select text, then assign a label to the selected span. The label appears as a colored highlight or underline. The reviewer can annotate multiple spans per document. The challenge is interaction design: how does the reviewer select overlapping spans, or spans that cross sentence boundaries, or spans where the start and end are not visually clear?

The best span annotation tools support keyboard shortcuts for common actions: select the next sentence, select the next paragraph, clear the current selection. They support undo and redo. They show a list of all annotated spans with the ability to click one to jump to it in the document. They validate that spans do not overlap unless overlap is explicitly allowed.

The decision to use span annotation depends on whether the precision is worth the time cost. If you need to know exactly which sentence contains PII, span annotation is necessary. If you only need to know whether the document contains PII anywhere, document-level binary classification is faster. A healthcare team compared span annotation for identifying PII in clinical notes versus document-level classification. Span annotation took 94 seconds per document on average. Document-level classification took 28 seconds. Span annotation produced data that allowed automated redaction of specific sentences. Document-level classification produced data that allowed flagging documents for manual redaction. The team used span annotation for high-value documents where automated redaction saved time downstream. They used document-level classification for lower-value documents where flagging was sufficient. The mixed approach balanced precision and throughput.

## Document-Level vs Sentence-Level

Some tasks can be performed at multiple levels of granularity. A content moderation task could ask the reviewer to label the entire post, or to label each sentence within the post. Document-level is faster. Sentence-level is more precise. The choice depends on how the data will be used.

If the goal is to decide whether to remove the post, document-level is sufficient. The reviewer labels the post as policy-violating or not. If the goal is to train a model to detect specific types of violations at sentence level, sentence-level annotation is necessary. The reviewer labels each sentence independently.

The cost of sentence-level annotation is proportional to document length. A post with ten sentences takes roughly ten times as long to annotate at sentence level as at document level. For long documents, sentence-level annotation becomes impractical. A legal document review task initially asked reviewers to label every sentence in 50-page contracts. The average time per document was 22 minutes. The team switched to paragraph-level annotation. Average time dropped to 11 minutes. The precision loss was minimal: most relevant content appeared in contiguous paragraphs, not isolated sentences.

The rule is: use the coarsest granularity that produces the precision you need. If document-level labels are sufficient for training and decision-making, do not annotate at sentence level just because more data feels better. The time cost is rarely worth it.

## Structured Data Entry

Some review tasks require the reviewer to enter structured data. In a transaction review task, the reviewer might need to enter the merchant name, transaction amount, and risk score. In a document extraction task, the reviewer might need to extract named entities and enter them into specific fields.

Structured data entry is slower than classification. Each field requires typing or selection. The form must validate that required fields are filled, that numeric fields contain numbers, that date fields contain valid dates. Validation errors frustrate reviewers if they block submission without clear guidance on what is wrong.

The best structured data entry forms validate inline as the reviewer types. If a field requires a date, the form shows a date picker or validates the format as the reviewer types and shows an error immediately if the format is wrong. The reviewer does not wait until submission to discover the error. The form also pre-fills fields when possible. If the model has already extracted a merchant name, the form shows it as the default value. The reviewer confirms or corrects it. Pre-filling reduces typing and speeds throughput.

The mistake teams make is requiring structured data entry when unstructured data would suffice. If the goal is to collect information for a human to read later, free-text is faster. If the goal is to collect information for a system to process automatically, structured data is necessary. If the goal is unclear, start with unstructured and add structure only when you discover you need it.

## The Schema-UI Alignment Problem

The annotation schema defines what data the reviewer should provide. The UI defines how they provide it. Misalignment between schema and UI creates friction. The schema says the reviewer should provide a risk score from 1 to 10. The UI provides a text field. The reviewer types "high risk" because that is more natural than typing "8." The data does not match the schema. Downstream systems break.

Schema-UI alignment means the form controls match the data types the schema expects. If the schema expects a number from 1 to 10, the UI provides a slider or a dropdown with values 1 through 10. The reviewer cannot enter invalid data. If the schema expects a date, the UI provides a date picker. If the schema expects one of five predefined labels, the UI provides radio buttons or a dropdown with exactly those five labels. The reviewer never has to translate their judgment into a format the schema accepts. The UI only allows valid inputs.

The challenge is flexibility. If the schema is strict and the UI enforces it, reviewers cannot handle edge cases. If a transaction does not fit any of the five predefined labels, the reviewer is stuck. The form does not let them submit. They either pick the least-wrong label, or they escalate to a supervisor, or they abandon the review. All three outcomes reduce quality.

The solution is an escape hatch. The form enforces the schema for 95 percent of cases. For the remaining 5 percent, the reviewer can flag the item as not fitting the schema and provide a free-text explanation. The item goes to a queue for schema refinement. If the same edge case appears repeatedly, you add a new label or adjust the schema. The escape hatch prevents reviewers from being blocked while also surfacing evidence that the schema is incomplete.

## Form Validation Without Blocking Flow

Validation is necessary to ensure data quality, but poorly implemented validation destroys flow. The worst pattern is blocking validation: the reviewer fills out a form, clicks submit, and sees an error message listing five fields with invalid data. The reviewer has to scroll back through the form, find each error, fix it, scroll back to submit, and click again. The interruption is cognitively expensive. The reviewer loses context. Frustration builds.

The better pattern is inline validation. As the reviewer fills each field, the form validates in real time. If a field is invalid, the form shows an error message next to the field immediately. The reviewer fixes it before moving on. When they reach the submit button, all fields are valid. Submission succeeds on the first click.

The best pattern is progressive validation with smart defaults. The form pre-fills fields with sensible defaults. The reviewer only changes fields where the default is wrong. The form validates as the reviewer types, but it does not show error messages until the reviewer has finished typing — no error messages while they are mid-word. When the reviewer clicks submit, the form checks for any remaining issues and either submits successfully or highlights the specific fields that need attention, with focus automatically moved to the first invalid field.

The goal is to make submission feel inevitable. The reviewer never wonders if the form will accept their input. The form guides them toward valid data continuously, without making them feel corrected or blocked.

## When Simple Beats Complex

There is a persistent instinct to add features to review interfaces. Reviewers ask for more fields, more options, more flexibility. Product instinct says add the features. The result is a form with 30 fields, half of which are optional, a quarter of which are rarely used, and all of which create visual clutter and cognitive load.

The best review interfaces are ruthlessly minimal. They show only the fields necessary for the current task. Optional fields appear only when the reviewer explicitly requests them. Advanced features are hidden behind a single click or keystroke. The default view is clean, fast, and obvious.

A content moderation platform started with a simple interface: the content, four classification buttons, and a submit button. Over two years, reviewers requested additional features: a notes field, a severity dropdown, a language selector, a flag-for-legal button, a related-items panel. The team added all of them. The interface became cluttered. New reviewer ramp-up time increased from five days to nine days. The team audited feature usage. The notes field was used in 8 percent of reviews. The severity dropdown was used in 12 percent. The language selector was used in 3 percent. The flag-for-legal button was used in 0.4 percent. The related-items panel was opened in 18 percent of reviews.

The team redesigned the interface. The default view showed only the content and the four classification buttons. A single "more options" button revealed the notes field, severity dropdown, and language selector. The flag-for-legal button moved to a context menu. The related-items panel appeared only when explicitly opened. New reviewer ramp-up time dropped back to five days. Throughput increased by 11 percent. Reviewers reported that the interface felt faster and less overwhelming.

The principle is: optimize for the common case. If a feature is used in fewer than 20 percent of reviews, it should not be visible by default. If it is used in fewer than 5 percent of reviews, it should require an explicit action to access. The interface should make the most common path the easiest path.

The next subchapter covers context display — how to show reviewers the information they need to make correct decisions without overwhelming them with everything they might possibly need.

