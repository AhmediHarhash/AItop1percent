# 12.1 â€” Review Infrastructure as a Compliance Requirement

Human review infrastructure is not an operational nice-to-have. It is a compliance requirement in every regulated industry and under every major AI governance framework active in 2026. The EU AI Act, GDPR's right to explanation, HIPAA's audit requirements, SOX controls for financial reporting, and state-level AI transparency laws all demand the same thing: meaningful human oversight, documented review processes, and retrievable evidence that humans made consequential decisions. Your review infrastructure is not just how you catch bad outputs. It is how you prove to regulators that you caught them.

Most teams build review infrastructure for quality alone. They design dashboards for speed, workflows for throughput, and UIs for reviewer convenience. Then Legal arrives with a compliance checklist and discovers the system logs nothing useful, stores no decision rationale, and cannot prove who reviewed what. The infrastructure that worked perfectly well for improving model accuracy cannot answer the question a regulator will ask: show me every human decision made about a specific user's data, why it was made, who made it, and what alternatives were considered. If you cannot answer that question within 24 hours of receiving it, your review infrastructure is non-compliant.

## The Three Compliance Obligations

Every human review system must satisfy three distinct regulatory obligations. These are not best practices. They are legal requirements with real penalties for non-compliance.

**Auditability.** The system must produce a complete, tamper-evident record of every review action. That record must include who performed the review, when, what they saw, what decision they made, and what rationale they provided. It must be retrievable by case ID, by reviewer ID, by date range, and by data subject. Under GDPR, a user can request all processing activity related to their data. Under HIPAA, an auditor can request all access logs for a patient's records. Under the EU AI Act's transparency requirements for high-risk systems, you must be able to explain every automated decision and every human override. If your review infrastructure does not generate this record automatically as part of the normal workflow, you will spend months reconstructing it manually when the audit notice arrives.

**Traceability.** The system must link every production decision back to the human review that approved it. If a model generated an output, a reviewer approved it, and that output caused harm, the audit trail must show exactly what the reviewer saw, what context they had, and what instructions governed their decision. This is not just logging. It is causal tracing. The regulator does not care that someone reviewed 400 cases on Tuesday. They care that Case 8472 was reviewed by User 3391 at 14:22 UTC on March 18, that the reviewer saw Version 2.1 of the guidelines, that the output shown to them matched the output shown to the user, and that the reviewer explicitly marked it as compliant with Policy Section 7.3. If any link in that chain is missing, the traceability fails.

**Reproducibility.** The system must allow an independent auditor to see exactly what the reviewer saw at the time of decision. Not what the model generates today. What it generated then. Not what the guidelines say now. What they said then. If a user contests a decision, if a regulator investigates a complaint, if Legal reviews a potential lawsuit, they need to step into the reviewer's exact context. That means versioned guidelines, snapshotted prompts, preserved model outputs, and timestamped policy documents. A review system that shows live data fails this requirement. The decision was made with historical context. The audit must use historical context.

## The High-Risk AI Designation

The EU AI Act designates certain AI systems as high-risk based on their use case. Credit scoring, hiring, medical diagnosis, legal case assessment, and benefit eligibility all qualify. If your AI system falls into a high-risk category, the Act imposes specific human oversight requirements. The human must be competent, properly trained, and empowered to override the system. The oversight must be meaningful, not a rubber stamp. The system must be designed to facilitate effective oversight, not obscure it with speed incentives or interface friction.

Your review infrastructure is how you prove all of that. The training logs prove competence. The decision rationale fields prove meaningful engagement. The override rate and the override-to-production ratio prove empowerment. The interface design proves facilitation. If your review UI does not collect decision rationale, your infrastructure cannot prove meaningful oversight. If your dashboard optimizes for speed above accuracy, your infrastructure undermines the competence requirement. The regulator does not audit your intent. They audit your system design. If the system makes it easy to rubber-stamp and hard to override, that is what the audit will conclude happens.

## The GDPR Right to Explanation

GDPR Article 22 grants users the right not to be subject to purely automated decision-making with legal or significant effect. The standard interpretation requires meaningful human involvement in decisions that affect rights, opportunities, or access. That involvement must be more than symbolic. A human must review the decision with enough context to understand it, enough authority to change it, and enough time to consider it properly.

Your review infrastructure is the evidence that this happened. The audit trail must show that a human saw the case, had access to the full decision context, spent enough time on it to engage meaningfully, and either affirmed or overrode the automated output. If the median review time is 8 seconds and the case files are 40 pages long, the audit trail proves the opposite: the human did not engage meaningfully. If the reviewer never has access to the user's submitted documentation, only to the model's summary, the human did not have full context. If the override button requires manager approval but the approve button does not, the human is not empowered equally in both directions.

The infrastructure design tells the compliance story. Design it poorly and the audit trail becomes evidence of non-compliance.

## The Retention Paradox

Compliance demands long retention periods. GDPR demands deletion on request. These requirements conflict, and your review infrastructure must resolve the conflict correctly.

The resolution is scoped retention. You retain what you are legally required to retain and delete what you are legally required to delete. Under SOX, financial review decisions must be kept for seven years. Under HIPAA, audit logs must be kept for six years. Under the EU AI Act, high-risk system logs must be kept for the system's operational lifetime or as required by sector-specific regulation, whichever is longer. But under GDPR, personal data must be deleted when the processing purpose ends, unless another legal basis applies.

Your review infrastructure must handle this at the field level. The reviewer ID, the timestamp, the decision outcome, and the policy version are audit metadata. They stay. The user's name, email, submitted documents, and any PII in the case description are personal data. They can be deleted or anonymized when the retention period for the business purpose ends, as long as the audit metadata remains intact. A deletion request that wipes the entire review record violates compliance. A retention policy that keeps PII indefinitely because it is easier than field-level deletion also violates compliance.

The infrastructure must support selective deletion with referential integrity. That means foreign keys, versioned schemas, and deletion workflows that preserve the audit chain while removing the personal data. Most teams discover this requirement during their first GDPR deletion request and spend three months retrofitting the database.

## The Reviewer as a Processor

Under GDPR, your human reviewers are processors of personal data. That means they must be trained on data protection principles, granted access only to the data necessary for their role, and logged every time they access a data subject's information. The review infrastructure must enforce this.

Access control must be role-based and case-based. A reviewer should only see cases assigned to them or to the queue they are authorized to work. They should not be able to search for arbitrary users, browse unassigned cases, or view cases outside their jurisdiction. The system must log every case view, every decision, and every data export. If a reviewer downloads case data for offline work, that download is a processing event and must be logged. If a reviewer shares their screen during training, that access is a disclosure and must be governed by policy.

The infrastructure cannot assume reviewers will follow policy. It must enforce policy through design. That means no global search bars, no export-all buttons, no screen-sharing without watermarks, and no case access without logged justification. The regulator will not accept "we told reviewers not to do that" as a defense. The infrastructure must make policy violations difficult, obvious, and logged.

## The Compliance Handoff

When you design review infrastructure, you are not designing for your users alone. You are designing for the auditor who will examine it two years from now, the regulator who will investigate a complaint, and the legal team that will defend your practices in court. That changes the design requirements.

You need fields for decision rationale even if reviewers hate filling them in. You need tamper-evident logs even if they complicate your database architecture. You need versioned guidelines even if managing versions is a pain. You need scoped access control even if it slows down urgent case escalations. These are not features for your primary users. They are features for your compliance users. And compliance users have veto power. If the infrastructure cannot satisfy an audit, it does not matter how fast or pleasant it is. It will be replaced.

The best review infrastructures design for both from the beginning. They make compliance easy, not grudging. Decision rationale fields are presented as helpful prompts, not mandatory bureaucracy. Access logs are surfaced in dashboards reviewers actually use, not hidden in compliance reports. Retention policies are automated, not manual. The infrastructure treats compliance as a first-class user, and compliance becomes a feature, not a burden.

The next subchapter covers audit trail design in detail: what to log, how to structure it, and how to make it queryable when the regulator calls.

