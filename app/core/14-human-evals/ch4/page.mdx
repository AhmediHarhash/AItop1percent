# Chapter 4 — Calibration and Guideline Management

Calibration is the process of aligning human reviewers with each other and with your ground truth. Without it, ten reviewers will produce ten different answers to the same question, and your labels become noise rather than signal. Guideline management is the process of maintaining the rules that reviewers follow—creating them, updating them, versioning them, rolling them back when they fail. Together, calibration and guideline management form the quality backbone of any review operation. This chapter covers both: how to get reviewers aligned, how to detect when they drift, and how to manage the documents that define your quality standards.

---

- 4.1 — Calibration Is the Foundation of Consistent Review
- 4.2 — Golden Sets for Reviewer Calibration
- 4.3 — Blind Duplicates: Measuring Intra-Rater Reliability
- 4.4 — Calibration Sessions: Aligning Reviewers on Edge Cases
- 4.5 — Drift Detection in Human Judgment
- 4.6 — Recalibration Triggers and Protocols
- 4.7 — Cross-Team Calibration for Multi-Site Operations
- 4.8 — Calibration at Scale: When You Have 200 Reviewers
- 4.9 — The Calibration Overhead Trade-off
- 4.10 — Guideline RFC Process: Propose, Test, Approve, Roll Out
- 4.11 — Breaking Changes: When Guideline Updates Require Retraining
- 4.12 — Migration Plans for Old Labels When Standards Change
- 4.13 — Rollback Procedures for Bad Guideline Updates

---

*Uncalibrated reviewers are not a team. They are individuals whose disagreements accumulate into datasets that cannot be trusted.*
