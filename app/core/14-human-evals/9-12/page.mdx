# 9.12 — Real-Time vs Batch Integration Patterns

Real-time integration means human review happens synchronously during the user interaction. The user submits a query, the model generates a response, a human reviewer evaluates it before the user sees it, and the reviewed response is delivered. Latency measured in seconds or minutes. Batch integration means review happens asynchronously after outputs are logged. The user receives the model's response immediately, and hours or days later, a human reviews a sample of logged outputs, flags issues, and those flags feed back into product improvements. Latency measured in hours or days. The two patterns serve different purposes, cost different amounts, and scale differently.

Most teams default to batch because it's easier to build and cheaper to run. But some use cases require real-time review, and the architecture to support it is fundamentally different. The decision isn't about which pattern is better — it's about which pattern matches your risk tolerance, cost constraints, and user experience requirements.

## Real-Time Review: Synchronous Human-in-the-Loop

Real-time review architecture works like this: the user submits a query, the system generates a model response, the response is routed to a human reviewer before delivery, the reviewer evaluates it against task-specific criteria within a target SLA, the reviewer either approves the response and it's delivered to the user or flags it and a fallback response is sent, and the review artifacts are logged for later analysis.

The critical constraint is **latency**. The user is waiting. If the review takes 30 seconds, the user experiences a 30-second delay between submitting their query and seeing a response. If the review takes five minutes, the user abandons the session. Your SLA for real-time review must be tight enough that users perceive the experience as responsive. For most use cases, that means single-digit seconds. For high-stakes use cases where users expect deliberation — legal document review, medical treatment recommendations — you can push to 30-60 seconds if you show a progress indicator and set expectations.

To meet a tight SLA, you need **dedicated reviewer capacity** and **intelligent routing**. Dedicated capacity means reviewers are online, available, and waiting for review tasks to arrive. You can't rely on reviewers logging in whenever they have free time. You need staffed shifts covering the hours your product operates. Intelligent routing means the system distributes incoming review tasks to the least-busy available reviewer with the right expertise. If a medical query comes in, it routes to a reviewer with clinical training, not a generalist.

Real-time review is expensive. If you're generating 10,000 outputs per hour and every output requires human review at 30 seconds per review, you need 83 full-time-equivalent reviewers online simultaneously. At a loaded cost of 25 dollars per hour per reviewer, you're spending 2,075 dollars per hour, 50,000 dollars per day, 18 million dollars per year. That cost is sustainable only if the value of preventing bad outputs from reaching users exceeds the cost of review.

The use cases that justify real-time review are **high-stakes, low-volume, and intolerant of errors**. Medical diagnosis support where a wrong recommendation could harm a patient. Legal contract drafting where a clause error could expose a client to liability. Financial advice where incorrect information could cost a user tens of thousands of dollars. Sensitive content moderation where a single policy-violating output reaching a minor triggers regulatory penalties. In these domains, the cost of a false negative — one bad output getting through — is higher than the cost of reviewing every output.

## Real-Time Review with Sampling and Escalation

Most teams can't afford to review 100% of outputs in real-time. They use **risk-based sampling** instead. The system generates a response, runs it through automated classifiers that estimate risk, and routes high-risk outputs to real-time human review while delivering low-risk outputs immediately. The user only experiences a delay if their specific query triggered a high-risk flag.

The risk classifier is a lightweight model trained on historical review data. It takes the user query and the generated response as input and predicts the probability that a human reviewer would flag the response as unsafe, policy-violating, or incorrect. If the probability exceeds a threshold — say, 15% — the output is held for human review. If the probability is below threshold, the output is delivered immediately and logged for asynchronous batch review.

The threshold is tuned based on your risk tolerance and budget. A high threshold — only route to review if risk probability exceeds 50% — results in low review volume, low cost, and higher chance of bad outputs slipping through. A low threshold — route to review if risk probability exceeds 5% — results in high review volume, high cost, and very few bad outputs reaching users. You calibrate the threshold by measuring false negative rate against cost and adjusting until you hit your acceptable risk level.

Real-time review with sampling is common in content moderation systems. User-generated content is scanned by automated classifiers. Low-risk content publishes immediately. High-risk content goes to human moderators for review before publishing. The user who posted low-risk content sees it live instantly. The user who posted high-risk content sees a "your content is being reviewed" message and receives a decision within 60 seconds. The delay is acceptable because it only affects a small fraction of users, and those users are the ones most likely to be violating policy anyway.

## Batch Review: Asynchronous Sampling and Feedback

Batch review architecture is simpler and cheaper. The user submits a query, the model generates a response, the response is delivered immediately, and the interaction is logged. Hours or days later, a sampling system selects a subset of logged interactions for review, routes them to human reviewers, and the review artifacts feed into your metrics, training data, and product improvement pipelines. The user never experiences review latency. The reviewer never sees the user. The feedback loop is entirely asynchronous.

The core design choice in batch review is **sampling strategy**. You can't review everything — if you're generating 10 million outputs per day, even reviewing 1% is 100,000 reviews. You need a sampling strategy that maximizes the value of each review.

**Random sampling** gives you unbiased estimates of overall quality metrics. If you randomly sample 10,000 outputs per day and reviewers flag 3% as unsafe, you can estimate that approximately 3% of your 10 million daily outputs are unsafe, with a confidence interval you calculate from the sample size. Random sampling is essential for tracking trends over time — you want to know if your safety flag rate is increasing, stable, or decreasing. But random sampling is inefficient for finding rare, high-severity issues. If Critical safety violations occur in 0.01% of outputs, you'd need to review 100,000 random samples to find ten examples.

**Stratified sampling** ensures you review examples from all important subgroups. You stratify by model version, by user cohort, by query category, by time of day, by geographic region. Within each stratum, you sample randomly. This ensures that you catch issues specific to a particular model version or user segment, even if those segments represent a small fraction of overall traffic. A model version deployed to 5% of users might have a unique failure mode that random sampling would rarely detect. Stratified sampling guarantees you review outputs from that version every day.

**Active learning sampling** uses a model to predict which outputs are most likely to be informative. The model scores every logged output on uncertainty, novelty, or predicted risk. High-scoring outputs are prioritized for review. The logic: if the model is uncertain about an output, a human label is maximally valuable for retraining. If an output looks similar to a thousand others you've already reviewed, the incremental value of another label is low. Active learning sampling increases the efficiency of your review budget — you get more signal per review hour than random sampling provides.

**Error-targeted sampling** prioritizes outputs that automated classifiers flagged as potentially problematic but weren't blocked. These are the borderline cases — risk score just below the threshold for real-time review, or outputs that triggered weak signals but not strong enough to warrant intervention. Reviewing these borderline cases improves your understanding of model behavior in the gray zone between clearly safe and clearly unsafe.

## Latency and Feedback Loop Speed

The primary tradeoff between real-time and batch review is **feedback loop latency**. In real-time review, a flagged output never reaches the user. The harm is prevented before it occurs. In batch review, a flagged output reaches the user, and the harm has already happened. The value of batch review is not in preventing the flagged instance — it's in preventing future instances through model retraining, prompt patches, or product changes.

For low-stakes outputs, batch review latency is acceptable. If the model occasionally generates an awkward phrasing or a minor factual error, it's not worth delaying every user to catch it in real-time. You catch it in batch review, log the issue, and fix it in the next model update. Users experience a small number of low-quality outputs, but the cost savings and user experience improvement from eliminating review latency outweigh the cost of those occasional errors.

For high-stakes outputs, batch review latency is unacceptable. If the model tells a user to take a dangerous action, you can't rely on catching it in batch review three days later and fixing it next sprint. The user might have already acted on the bad advice. High-stakes use cases require either real-time review or real-time automated guardrails that are accurate enough to replace human judgment.

## Hybrid Architectures: Batch Review with Real-Time Escalation

Many production systems use a hybrid architecture. Most outputs are delivered immediately and reviewed asynchronously in batches. A small subset of outputs — those flagged by automated classifiers as high-risk — are held for real-time human review. The user experience is fast for 95% of queries and has a review delay for the 5% that trigger risk flags.

The hybrid architecture requires careful tuning. If your risk classifier has a high false positive rate, you'll route too many safe outputs to real-time review, increasing cost and introducing unnecessary latency for users who asked perfectly benign questions. If your risk classifier has a high false negative rate, you'll miss dangerous outputs and deliver them immediately, defeating the purpose of the real-time review layer.

You tune the classifier by measuring precision and recall on a labeled validation set. Precision is the fraction of outputs routed to real-time review that actually needed review — true positives over true positives plus false positives. Recall is the fraction of dangerous outputs that were correctly routed to review — true positives over true positives plus false negatives. High precision minimizes wasted review time and unnecessary user latency. High recall minimizes dangerous outputs slipping through.

The typical operating point is 70-80% precision and 85-95% recall. You accept that 20-30% of outputs routed to real-time review will turn out to be safe upon human evaluation — that's the cost of catching 85-95% of the dangerous ones. The false positives are annoying but not harmful. The false negatives are the dangerous outputs that got delivered immediately. You tolerate a 5-15% false negative rate because the alternative — reviewing everything in real-time — is prohibitively expensive.

## Batch Review Cadence and Aggregation Windows

In batch review, the **review cadence** determines how quickly you detect and respond to issues. Daily review means you sample and review a batch of outputs every 24 hours. Weekly review means you sample and review every seven days. The cadence you choose depends on how quickly your model behavior can drift and how fast you need to react.

If you deploy model updates multiple times per week, you need daily review to detect regressions before they accumulate. If your model is stable and updates are infrequent, weekly review might be sufficient. If you're operating in a high-risk domain with regulatory scrutiny, you might review multiple times per day — every six hours, or even every hour for critical segments.

The cadence also affects your ability to aggregate and trend. If you review daily, you can plot daily trend lines for quality and safety metrics. If you review weekly, your trend lines are coarser. Daily review gives you faster signals but requires more consistent reviewer availability. Weekly review is easier to staff but slower to detect emerging issues.

## Cost and Scale Tradeoffs

Real-time review scales poorly with volume. If your output volume doubles, your real-time review cost doubles. There's no economy of scale — every additional output requires additional reviewer time. The only way to reduce cost is to tighten sampling thresholds, which increases risk, or invest in better automated classifiers to reduce the fraction of outputs that need human review.

Batch review scales better. If your output volume doubles, you can keep your review sample size constant and accept slightly wider confidence intervals on your quality estimates, or you can increase your sample size sub-linearly — maybe by 50% instead of 100%. Batch review cost grows with the log of output volume, not linearly. You can process billions of outputs per day and still keep review costs manageable by sampling intelligently.

The scale tradeoff is why most high-volume consumer products use batch review with automated guardrails. A chatbot serving 50 million users per day can't afford real-time human review on every output. It uses automated classifiers to block high-risk outputs at generation time, delivers the rest immediately, and batch-reviews a stratified sample of 50,000 outputs per day to monitor quality and train better classifiers. The review budget is fixed even as user volume grows.

## Integration with ML Pipelines

Both real-time and batch review integrate with your ML pipelines, but the integration points differ. Real-time review artifacts are written to your review database immediately after each review decision. Your metrics pipeline reads those artifacts in near-real-time — within minutes — to update dashboards and trigger alerts. If your safety flag rate spikes, you know within an hour.

Batch review artifacts are written at the end of each review session, which might be once per day or once per week. Your metrics pipeline runs on the same cadence — it aggregates the day's or week's review results, computes quality metrics, and publishes them to dashboards. Your active learning pipeline reads the batch review labels, selects the most informative examples, and adds them to your training dataset for the next model retraining cycle.

Real-time review enables **closed-loop control** — you detect an issue, deploy a fix, and measure the fix's impact within the same day. Batch review enables **open-loop monitoring** — you detect an issue, deploy a fix in the next sprint, and measure the fix's impact a week later. Closed-loop control is faster but more expensive. Open-loop monitoring is slower but scales better.

## When to Use Which Pattern

Use **real-time review** when the cost of a single bad output reaching a user exceeds the cost of reviewing every output, when your output volume is low enough that review cost is affordable, when user tolerance for latency is high because the task is inherently deliberative, or when regulatory or safety requirements mandate human oversight before delivery.

Use **batch review** when the cost of bad outputs is low or the harm is delayed enough that you can fix it before significant damage occurs, when your output volume is too high for real-time review to be economically feasible, when user experience requires sub-second response times and any review latency is unacceptable, or when you're optimizing for long-term model improvement through training data collection rather than short-term error prevention.

Use a **hybrid architecture** when you have a mix of high-risk and low-risk outputs that can be distinguished with reasonable accuracy by automated classifiers, when you can tolerate some false positives in the real-time review queue, and when your infrastructure can handle the complexity of dual integration paths.

The pattern you choose shapes your entire review infrastructure — the reviewer staffing model, the database schema, the metrics pipeline, the alerting thresholds, the integration with product workflows. Choose carefully. Test both patterns on a pilot before committing to one at scale. The right pattern for your product today might not be the right pattern a year from now when your volume, risk profile, or regulatory environment changes. Build flexibility into your architecture so you can shift between patterns as your needs evolve.

Real-time and batch review are complementary tools, not mutually exclusive alternatives. The best systems use both — real-time review for the highest-risk outputs, batch review for everything else, and automated guardrails covering the gap. Together, they form a defense-in-depth strategy that balances speed, cost, and safety across the full range of model behaviors.

The integration of human review into ML pipelines — whether real-time or batch, with labels and artifacts, normalized for automation, and driving prompt patches and product fixes — is what transforms AI systems from static models that degrade over time into learning systems that improve continuously. Every review is a signal. Every artifact is a potential fix. Every fixed issue is a step toward a more reliable, safer, and more valuable product. Build the infrastructure to capture those signals, act on them quickly, and measure their impact. The quality of your product is limited by the speed and reliability of your feedback loops. Make them fast. Make them robust. Make them a competitive advantage.

