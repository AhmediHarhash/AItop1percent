# 10.2 — Throughput Metrics: Volume, Velocity, and Capacity

Throughput is not a single number. It is a system of metrics that describe how much work moves through your review operation, how fast it moves, and how much capacity you have to handle future demand. The teams that treat throughput as "tasks per day" miss everything that matters. A review operation that completes 800 tasks per day sounds productive until you realize the queue contains 12,000 waiting tasks and grows by 1,100 tasks per day. The operation is not keeping up — it is falling behind at a rate of 300 tasks per day, which means the backlog doubles every 40 days and the whole system collapses within a quarter.

Elite operations measure throughput across three dimensions: **volume** is the total amount of work completed in a time period, **velocity** is the rate at which work moves through the system, and **capacity** is the maximum throughput the operation can sustain given current staffing and tooling. All three metrics matter. Volume tells you whether you are doing enough work. Velocity tells you whether you are doing it fast enough. Capacity tells you whether you can scale to meet future demand. Optimizing one without the others creates operational debt that eventually forces a crisis.

## Volume Metrics and What They Hide

Volume is the simplest throughput metric: count the tasks completed. Tasks reviewed per day, examples labeled per week, cases adjudicated per month. Volume metrics are easy to compute, easy to communicate, and easy to game. A reviewer who completes 300 tasks per day looks twice as productive as one who completes 150 — until you discover that the first reviewer only works on simple binary classification tasks that take 20 seconds each, while the second reviewer handles complex multi-paragraph annotation tasks that require reading policy documents and consulting with subject matter experts.

Raw volume is almost never a fair comparison. Task difficulty varies by orders of magnitude. Labeling sentiment on single-sentence product reviews is not the same as annotating medical imaging scans for tumor boundaries. Reviewing content moderation flags for spam is not the same as adjudicating intellectual property disputes. A content labeling operation might process 5,000 tasks per day with four reviewers. A legal review operation might process 40 tasks per day with eight reviewers. The second operation is not less productive — it is doing harder work.

The fix is **difficulty-adjusted volume**. Assign each task type a weight based on median completion time or cognitive complexity. Simple binary tasks get weight 1.0. Multi-step structured annotation gets weight 3.5. Expert judgment on ambiguous edge cases gets weight 8.0. Calculate throughput as the sum of difficulty-weighted tasks completed, not raw task count. A reviewer who completes 80 weight-8.0 tasks per week has delivered 640 units of adjusted volume. A reviewer who completes 1,200 weight-1.0 tasks per week has delivered 1,200 units. Now you can compare fairly across task types and identify where capacity is constrained.

Difficulty-adjusted volume also surfaces hidden inefficiencies. If your operation completes 15,000 units of adjusted volume per week and 92% of it comes from weight-1.0 tasks while weight-6.0 and weight-8.0 tasks sit in the queue for weeks, you have a staffing allocation problem. You are over-provisioned for simple work and under-provisioned for complex work. The solution is not to hire more reviewers — it is to retrain or reassign reviewers from low-weight to high-weight queues.

## Velocity Metrics and Backlog Health

Velocity measures how fast tasks move from creation to completion. The most important velocity metric is **net throughput rate**: incoming tasks per day minus completed tasks per day. If 900 tasks arrive daily and you complete 850, your net throughput is negative 50 tasks per day. The backlog grows by 50 tasks every day. In 60 days the backlog has grown by 3,000 tasks. In 120 days it has grown by 6,000 tasks. Long before that point, stakeholders lose confidence, teams route around the review process, and the operation becomes irrelevant.

Net throughput tells you whether your operation is sustainable. Positive net throughput means you are clearing the backlog. Zero net throughput means you are keeping up with demand but not reducing accumulated backlog. Negative net throughput means you are in a death spiral and need to add capacity, reduce incoming volume, or simplify the review process immediately. Most teams do not measure net throughput until the backlog crisis is already underway. By the time they notice, the queue contains 20,000 tasks and recovery requires months of emergency hiring or task abandonment.

The second critical velocity metric is **median time to completion**: the time from task creation to review completion, measured at the 50th percentile. This metric captures system throughput in units that matter to stakeholders. A median time to completion of two hours means half of all review requests finish within two hours. A median time to completion of eight days means half of all requests take more than a week. Stakeholders care about this number because it determines how fast they can iterate. A product team that needs reviewed examples for a model experiment cannot afford an eight-day median — they will skip review and use unvalidated data instead.

Median is the right percentile for most purposes. Mean is distorted by long-tail outliers — a few tasks that sit in the queue for three weeks because they are waiting for expert escalation will pull the mean up even if 90% of tasks complete within hours. The 95th percentile is useful for understanding worst-case latency but does not reflect typical experience. The 50th percentile — median — is the number that represents what most stakeholders encounter most of the time.

The third velocity metric is **queue depth trend**: the size of the waiting queue over time. Plot queue depth daily for the last 90 days. A flat trend means you are keeping up with demand. A downward trend means you are clearing backlog faster than new tasks arrive. An upward trend means you are falling behind. The slope of the trend tells you how fast the problem is growing. A queue that grows by 5% per week doubles in size every 14 weeks. A queue that grows by 15% per week doubles every five weeks. The difference between those growth rates is the difference between "we need to hire two more reviewers next quarter" and "we need to hire six more reviewers this month or the system collapses."

## Capacity Metrics and Scaling Headroom

Capacity metrics answer the question: how much work can this operation handle if we fully utilize available resources? The simplest capacity metric is **theoretical maximum throughput**: the number of tasks the team could complete per day if every reviewer worked at full productivity for a full shift with no downtime, no breaks, and no context switching. This number is always higher than actual throughput because real operations have inefficiencies, variability, and non-review work like calibration, training, and escalations.

Theoretical maximum throughput is useful as an upper bound. If your team of 12 reviewers has a theoretical max of 2,400 tasks per day but completes only 1,600 tasks per day on average, you are operating at 67% capacity utilization. That leaves 33% headroom — which could be inefficiency, could be necessary downtime, or could be non-review work that is essential but not counted in task completion metrics. You cannot know which without deeper instrumentation.

The more useful capacity metric is **sustained maximum throughput**: the highest throughput the team can maintain over a full week without quality degradation, burnout, or SLA breaches. This is the realistic capacity, not the theoretical one. Measure it by running the operation at increasing intensity for one-week sprints and tracking quality metrics alongside throughput. Find the throughput level where quality starts to degrade or reviewers start missing SLAs or escalation rates spike. That level is your sustained maximum. It might be 70% of theoretical max. It might be 85%. It is never 100%.

Sustained maximum throughput is the number you use for capacity planning. If sustained max is 1,800 tasks per day and incoming demand is 1,500 tasks per day, you have 20% headroom — enough to handle spikes, seasonal surges, or temporary reviewer absences without falling behind. If sustained max is 1,800 tasks per day and demand is 1,900 tasks per day, you are over capacity and need to hire, outsource, or reduce scope immediately. Running over sustained max for more than a few days degrades quality, burns out reviewers, and creates the backlog spiral.

The final capacity metric is **utilization rate**: actual throughput divided by sustained maximum throughput. A utilization rate of 60% means you are operating well below capacity and could handle significantly more work with current staffing. A utilization rate of 95% means you are running near maximum and have no buffer for surges or variability. A utilization rate over 100% means you are exceeding sustained capacity — which is possible for short sprints but unsustainable over weeks.

Elite operations target 75-85% utilization. Below 75%, you are over-staffed or under-utilized — capacity is being wasted. Above 85%, you are running too hot — any spike in demand, any reviewer absence, any quality issue that requires rework will push you over capacity and start the backlog spiral. The 75-85% band gives you enough buffer to absorb variability while still running efficiently.

## The Task Mix Problem

Throughput metrics are only meaningful when you account for task mix: the distribution of task types and difficulty levels in your queue. A week where you complete 10,000 tasks but 95% of them are simple binary classifications is not equivalent to a week where you complete 10,000 tasks that are evenly split across binary, multi-class, and structured annotation. The second week delivered far more value even though raw throughput is identical.

Task mix affects throughput in two ways. First, it determines average task completion time. A queue dominated by simple tasks moves faster than a queue dominated by complex ones. Second, it determines which reviewers can work on which tasks. If 80% of your queue is expert-level medical annotation but only 30% of your reviewers have medical expertise, you have a bottleneck — the expert reviewers are over-utilized and the generalist reviewers are idle. Throughput is constrained by the scarcest skill, not by total headcount.

Tracking task mix over time reveals shifts in demand. If simple tasks drop from 60% to 40% of the queue over three months, your operation is getting harder — which means throughput will decline unless you hire more expert reviewers or retrain generalists. If expert-level tasks spike from 10% to 25% of the queue, you need to reassess capacity planning because sustained max throughput is about to drop. Task mix is not a static input — it is a dynamic variable that changes operational constraints.

## Throughput Dashboards for Real-Time Operations

The throughput dashboard that matters for day-to-day operations shows five numbers updated every hour: tasks completed in the last 24 hours, current queue depth, net throughput rate over the last seven days, current utilization rate, and forecasted days until backlog is cleared. These five numbers tell operations leads whether the system is healthy or spiraling.

If tasks completed is trending down, you have a capacity problem. If queue depth is trending up, you are falling behind. If net throughput is negative, you are in a backlog spiral. If utilization is above 90%, you are running too hot and quality is at risk. If forecasted days to clear backlog is increasing, the problem is accelerating. Any one of these signals should trigger immediate investigation. Two or more signals together indicate a crisis that requires intervention within 24 hours.

The dashboard also needs segmentation by task type and reviewer cohort. Aggregate throughput numbers hide the problems. If overall throughput looks healthy but one task type has negative net throughput and a growing queue, you have a staffing allocation issue. If overall utilization is 80% but three reviewers are at 110% while five are at 50%, you have a workload distribution issue. Segmented metrics surface these problems before they become system-wide failures.

Elite operations review the throughput dashboard twice per day — once in the morning to check overnight trends and once in the late afternoon to assess daily performance. Weekly operations meetings include a deeper dive: throughput trends over the last four weeks, capacity utilization by task type, net throughput segmented by complexity tier, and forecasted capacity needs for the next quarter. This cadence catches problems early and enables proactive staffing and process adjustments instead of reactive crisis management.

## When Throughput Optimization Breaks Quality

The most dangerous mistake in review operations is optimizing throughput without monitoring quality. A fintech company wanted to reduce review latency from five days to one day. They added gamification to the review tool — leaderboards showing who completed the most tasks per hour, badges for hitting throughput milestones, and monthly bonuses for the top 20% of reviewers by task count. Throughput doubled in three weeks. Median latency dropped to 18 hours. The leadership team celebrated the improvement.

Two months later the data science team discovered that label accuracy had collapsed from 88% to 61%. Reviewers were rushing through tasks to hit leaderboard targets. They were skipping instructions, ignoring edge cases, and defaulting to the easiest judgment on ambiguous examples. The gamification system rewarded speed, so speed is what they delivered. Quality became irrelevant because it was not measured or rewarded. The company had to throw out 40,000 labeled examples, retrain the entire review team, and rebuild trust with stakeholders who had stopped using review outputs after the third batch of bad labels.

The lesson: throughput and quality are linked. Increasing throughput without quality constraints degrades output. Measuring throughput without measuring quality creates perverse incentives. Every throughput optimization must be paired with a quality check. If you push reviewers to complete 30% more tasks per day, check whether accuracy drops. If you shift work to a lower-cost vendor to increase capacity, check whether inter-rater agreement declines. If you simplify the review rubric to speed up decisions, check whether the simplified labels still contain enough signal to train models effectively.

Elite operations set throughput targets as quality-adjusted metrics: "complete 1,500 tasks per day while maintaining 90% accuracy and 85% inter-rater agreement." The throughput target is conditional on quality constraints. If quality drops below threshold, throughput does not count. This prevents gaming and keeps the operation focused on delivering value instead of hitting arbitrary volume targets.

Throughput metrics — volume, velocity, and capacity — tell you whether your review operation can keep up with demand, clear backlogs, and scale to meet future needs. But throughput is meaningless without quality. The next subchapter covers quality metrics in detail: accuracy, consistency, and agreement, and how to measure whether the work your reviewers produce is trustworthy enough to make decisions on.
