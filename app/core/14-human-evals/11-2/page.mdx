# 11.2 — Redaction Modes and Just-in-Time Reveal

The best security control is not showing the data in the first place. If a reviewer doesn't need to see a full email address, show a hash. If they don't need a full credit card number, show the last four digits. If they don't need to know the user's real name, assign a pseudonym. Redaction isn't about hiding data from authorized users — it's about minimizing exposure so that when something goes wrong, the blast radius is small. The question isn't whether you trust your reviewers. It's whether you trust every device they use, every network they connect to, every person who might glance at their screen, and every screenshot tool running in the background. If the answer is no, redact by default.

## The Principle: Progressive Disclosure

Show the minimum viable context for the review task. Reveal more only when the reviewer explicitly requests it and only for the current task. Expire the reveal after the task is submitted. This is **progressive disclosure** — the same UX principle that keeps complex interfaces learnable, applied to security. A reviewer grading chatbot tone doesn't need to see the user's account email, their billing history, or their prior conversations. They need the chatbot's response and enough context to judge whether the tone matches the query. If the query is "How do I cancel my subscription?" and the response is cheerful and deflective, the tone is wrong. The reviewer can make that judgment without knowing who asked or when.

Progressive disclosure reduces accidental exposure. A reviewer skimming 200 tasks per shift can't memorize all the redacted data they never saw. If their laptop is stolen, the cached data contains pseudonyms and masked fields, not real identities. If they take a screenshot to document a UI bug, the screenshot doesn't contain PII. The control doesn't rely on reviewer discipline — it's enforced by what's rendered. Humans are bad at remembering not to do things. Humans are great at working with the interface in front of them. Design the interface so that working normally is also working securely.

## Redaction Levels: None, Partial, Full

**No redaction** means showing the data exactly as it exists in production. Use this only when the task explicitly requires seeing everything — legal review, fraud investigation, escalated trust-and-safety cases. Even then, limit it to reviewers with elevated clearance, and log every access. No redaction is the exception, not the default.

**Partial redaction** masks specific fields while leaving context intact. Email addresses become first-initial-plus-domain: "j...@gmail.com". Phone numbers show country code and last three digits. Credit cards show type and last four digits. Names become pseudonyms: "User 47291" or randomly assigned handles like "PurpleFox83". Dates are rounded to the month or quarter. Locations are rounded to the city or region. The reviewer sees enough structure to understand the interaction but not enough detail to identify the individual. This is the default for most review tasks.

**Full redaction** replaces entire fields with placeholders. Queries are shown as category labels: "medical question," "financial advice," "legal inquiry." User identifiers are stripped entirely. Outputs are shown with no input context. Use this when the review task is pure output evaluation: "Is this response coherent? Is the tone appropriate? Does it refuse harmful requests?" The reviewer doesn't need to see the query to answer those questions. They're judging the model's behavior in the abstract. Full redaction turns review into a blind test. The reviewer can't introduce bias based on user demographics, query phrasing, or prior interactions.

The right level depends on what judgment you're asking for. If you're asking "Did the model answer the user's question?" you need partial redaction — enough context to map output to input. If you're asking "Is this output factually correct?" you may need no redaction — the reviewer needs to see the query, the sources retrieved, and the output, and compare them for accuracy. If you're asking "Is this output harmful?" you need partial redaction — you need the output and the intent category, but not the user's identity.

## Just-in-Time Reveal: Making Redaction Usable

Partial redaction breaks down when the reviewer genuinely needs more context to make a decision. A query is flagged for containing personal information. The reviewer is judging whether the model's refusal was appropriate. With partial redaction, the reviewer sees: "User asked a question containing financial data. Model refused to answer. Was this refusal correct?" The reviewer doesn't know what the financial data was, so they can't judge. The task becomes unworkable.

Just-in-time reveal solves this. The interface shows the redacted version by default. If the reviewer needs more context, they click "Reveal Full Query." The system logs the reveal — timestamp, task ID, reviewer ID — and shows the full text for the current task only. When the reviewer submits their judgment, the reveal expires. If they navigate to the next task, the reveal expires. The full data is visible only during active decision-making. The reviewer can't reveal 50 tasks, screenshot them all, and close the window. Each reveal is scoped, logged, and time-boxed.

Implementation matters. The reveal must be server-rendered, not client-side. A client-side reveal means the full data was already sent to the browser and hidden with CSS. An attacker inspecting network traffic or browser memory sees the unredacted data regardless of whether the reviewer clicked "Reveal." Server-side reveal means the redacted data is sent initially. When the reviewer clicks "Reveal," the client sends a request to the server, the server logs the reveal, and the server returns the unredacted data for that task only. The full data never touches the client unless explicitly requested. This prevents preloading attacks where an attacker modifies the client to auto-reveal every task on page load.

## Redaction Strategies by Data Type

**Personal identifiers** — names, email addresses, phone numbers, government IDs — replace with pseudonyms or hashes. Consistency matters: the same user should map to the same pseudonym across tasks so reviewers can identify repeat users without knowing their identity. If three tasks all show "User 84729," the reviewer can infer it's the same person and spot behavioral patterns without ever learning their name.

**Freeform text** — user queries, uploaded documents, chat messages — use entity-based redaction. Named-entity recognition identifies people, places, organizations, and dates, then replaces them with category labels or generics. "I visited Dr. Martinez at Stanford Health on July 14th" becomes "I visited DOCTOR at HOSPITAL on DATE." This preserves sentence structure and semantic meaning while removing identifiable details. It's not perfect — NER models miss things, especially slang, misspellings, and domain-specific terms — but it's far better than no redaction. For high-risk data, combine NER with regex patterns for emails, phone numbers, and account IDs.

**Numeric data** — ages, salaries, account balances, transaction amounts — round to ranges or percentiles. An age of 34 becomes "30-39." A salary of 127,000 dollars becomes "100k-150k." A transaction of 4,832 dollars becomes "1k-10k." The reviewer sees the scale without seeing the precise value. If the review task is checking whether the model gave age-appropriate advice, knowing "30-39" is sufficient. If the task is fraud detection and exact amounts matter, use just-in-time reveal.

**Temporal data** — timestamps, session durations, event sequences — round to the hour, day, or week depending on sensitivity. Exact timestamps enable re-identification by correlating with other datasets. If a query timestamp is "2026-01-15 14:32:18" and an access log from another system shows the same timestamp, the user can be identified by intersection. Rounding to the day or hour breaks the correlation. The reviewer sees "mid-January 2026" instead of the exact second.

**Multimedia** — images, audio, video uploaded by users — blur faces, mute voices, strip EXIF metadata. If the review task is judging whether the model correctly described the content, show a blurred or cropped version by default. If the reviewer flags "I can't tell what this is," allow just-in-time reveal of the full media, logged and time-limited.

## Pseudonymization Persistence and Consistency

Pseudonyms must be consistent across review tasks but unlinkable to the real identity without a key. The naive approach is hashing: SHA-256 of the user ID produces a pseudonym. The problem is hash collisions and dictionary attacks. If the user ID space is small, an attacker can precompute all hashes and reverse the mapping. If the hash is unsalted, the same user produces the same pseudonym across all datasets, enabling cross-dataset re-identification.

The correct approach uses keyed pseudonymization. A secret key, stored in a hardware security module or key management service, is combined with the user ID to produce the pseudonym. The pseudonym is deterministic for the same user and key but unlinkable without the key. If the review environment is breached, the attacker sees pseudonyms but can't reverse them to real identities. The key is never present in the review environment — it's used by a separate service that generates pseudonyms when data is prepared for review.

Rotation matters. The pseudonymization key should rotate periodically — quarterly or annually depending on data volume. After rotation, old pseudonyms are meaningless, and users who appeared as "User 12345" in Q1 now appear as "User 89023" in Q2. This limits the time window for correlation attacks. If an attacker steals pseudonyms from one quarter, they can't link them to pseudonyms from another quarter. The trade-off is that reviewers lose cross-quarter consistency — they can't track the same user across rotations. For most review tasks, that's acceptable. For long-term behavioral analysis, you may need longer rotation windows or a two-tier pseudonymization scheme where analysts see persistent pseudonyms but front-line reviewers see ephemeral ones.

## Edge Case: When Reviewers Recognize Users Anyway

Redaction prevents systematic re-identification. It doesn't prevent anecdotal recognition. A reviewer sees a query: "How do I get to LOCATION from LOCATION?" The placeholders hide the city names, but the reviewer lives in a small town and recognizes the phrasing from a local forum. They now know a neighbor used your AI. You can't prevent this with redaction alone. What you can prevent is the reviewer searching for that neighbor's other queries. Task assignment should be randomized and non-searchable. Reviewers see the queue assigned to them. They can't search by pseudonym, date, or query text. If they happen to recognize one task, they can't find others. The blast radius is one interaction, not a profile.

For high-risk environments, add geographic separation. Reviewers in the United States don't review data from users in the United States. They review data from Europe or Asia. Reviewers in Europe review data from the Americas. This reduces the chance of personal recognition. A reviewer in Ohio won't recognize local phrasing from a query submitted in Tokyo. It's not foolproof — expats, travelers, and global communities still create overlap — but it reduces risk, especially in small markets.

## Redaction Doesn't Mean Trustlessness

Redaction is a control, not an accusation. The message to reviewers isn't "we don't trust you." It's "we're protecting you from being responsible for data you don't need to see." If a breach happens and the reviewer never saw PII, they're not liable. If they never saw unredacted medical data, they're not subject to HIPAA audits. Redaction shifts the burden from individual discipline to system design. Reviewers appreciate this once it's explained. They don't want the liability either.

Training should emphasize this framing. Redaction exists because the data is sensitive and the company takes that seriously. Just-in-time reveal exists because sometimes you need context to do your job correctly. The system trusts you to request reveals only when necessary, and the logs exist to demonstrate to regulators that reveal was justified. This aligns the company's compliance needs with the reviewer's desire to do high-quality work. The alternative framing — "we redact because we assume you'll leak data" — breeds resentment and makes reviewers more likely to circumvent controls.

## The Performance Cost of Server-Side Reveal

Redaction and just-in-time reveal add latency. A task that once loaded in 200 milliseconds now takes 400 milliseconds because the server is applying NER, replacing entities, and generating pseudonyms. Reveal requests add a round-trip: the reviewer clicks "Reveal," the client sends a request, the server logs it, retrieves the full data, and returns it. That's another 300 milliseconds. For high-throughput reviewers, this compounds. If a reviewer completes one task every 45 seconds and reveal adds 300 milliseconds per task, that's a 0.7% throughput reduction. Across 1,000 reviewers working 8-hour shifts, that's thousands of dollars per week in lost productivity.

The optimization is caching redacted data after first render. When a task enters the review queue, the system pre-redacts it and stores the redacted version. When the reviewer loads the task, the server serves the cached redacted version — no NER, no pseudonymization, no latency. Reveal still requires a round-trip, but 95% of tasks don't require reveal, so the latency impact is minimal. The trade-off is storage: you're storing both the original data and the redacted version. For text data, the overhead is small. For multimedia, it's significant. You may need to redact on-the-fly for images and videos while caching redacted text.

## Audit Logs Make Redaction Defensible

Redaction without logging is security theater. You need evidence that redaction was applied, that reveals were justified, and that access was scoped. The audit log captures every task load, every reveal request, and every submission. Each log entry includes task ID, reviewer ID, session ID, timestamp, redaction level, and whether reveal was triggered. If a regulator asks "Did reviewers see unredacted PII?" you can answer with precision: "Reviewers saw redacted data for 94% of tasks. Reveal was triggered for 6% of tasks, all logged here, all with justification in the task context."

The log also supports quality analysis. If reviewers are requesting reveal on 40% of tasks, your default redaction level is too aggressive — reviewers can't do their job without seeing more. If reveal is requested on less than 1% of tasks, you may be over-redacting and slowing down workflows unnecessarily. The target is 5-10% reveal rate: enough to indicate that reviewers are using the control when needed, low enough that most tasks are completable with redacted data.

The next subchapter covers watermarking, screenshot blocking, and copy controls — technical measures that prevent reviewers from exfiltrating data even when it's visible on their screen.
