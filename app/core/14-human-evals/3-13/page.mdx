# 3.13 — Model-Assisted Review: AI Draft Decisions and Evidence Highlighting

Model-assisted review can make reviewers faster and more consistent. It can also make them worse. The difference depends entirely on how you implement it.

When a model pre-fills a decision, the reviewer sees a suggested answer before they evaluate the evidence. If the suggestion is right most of the time, the reviewer's job becomes verification. They check the model's work. They approve or override. This is faster than making the decision from scratch. But if the model is wrong, and the reviewer does not catch it, the error becomes the reviewer's error. If the model is subtly wrong in ways that align with the reviewer's own biases, the reviewer may never notice.

Evidence highlighting has the same risk. When the model highlights the most relevant spans in a document or the most important features in an image, the reviewer's attention goes to those highlights first. If the highlights are accurate, this saves time. The reviewer focuses on what matters. But if the highlights are misleading, the reviewer misses information they would have noticed if they had scanned the content themselves.

Model-assisted review is a trade-off. It accelerates good decisions and amplifies bad ones. The key is designing the assistance so that it helps without anchoring, speeds up work without deskilling reviewers, and makes errors more visible instead of hiding them.

## Pre-Filling Decisions: When Suggestions Help and When They Hurt

The most common form of model assistance is pre-filling the decision. The reviewer opens a task and sees that the model has already selected an answer. The reviewer's job is to confirm or change it. This pattern works well when the model is highly accurate and the task is low-stakes. It works poorly when the model is mediocre or the task requires nuanced judgment.

If the model is correct ninety-five percent of the time on a simple binary decision, pre-filling saves time. The reviewer scans the content, confirms the model got it right, and moves to the next task. Their throughput increases. Error rates stay low because the five percent of cases where the model is wrong are usually obvious, and the reviewer catches them.

If the model is correct seventy percent of the time, pre-filling becomes dangerous. The reviewer sees a suggested answer. They check the evidence. About seventy percent of the time, the evidence aligns with the suggestion, so they approve it. The other thirty percent of the time, the evidence contradicts the suggestion, and they override. But they are no longer making independent judgments. They are evaluating the model's judgment. That is a different cognitive task. It requires less effort but also less engagement. The reviewer's attention is anchored to the suggestion. They look for reasons to agree with it rather than evaluating the case neutrally.

Anchoring is the core problem with pre-filled decisions. Once the reviewer sees a suggested answer, their judgment becomes biased toward that answer. Psychological research on anchoring effects shows that even irrelevant anchors influence judgment. A suggested decision that comes from a model trained on the same task is a highly relevant anchor. It will bias the reviewer whether they intend it to or not.

The bias is strongest when the case is ambiguous. If the evidence clearly supports one answer, the reviewer will pick that answer regardless of what the model suggested. If the evidence is borderline, the reviewer is more likely to side with the model's suggestion than they would be to pick that answer independently. This shifts the decision threshold. Cases that the reviewer would have judged as borderline-reject without a suggestion become borderline-approve when the model suggests approval.

One way to reduce anchoring is to hide the model's suggestion until after the reviewer makes their own decision. The reviewer evaluates the case, selects an answer, and submits it. Only then does the interface show the model's suggestion. If the two match, the decision is logged and the reviewer moves on. If they differ, the interface asks the reviewer to reconsider. The reviewer can stick with their original choice or change it. This approach preserves independent judgment while still giving the model a second-pass role.

Another approach is to show the model's confidence alongside its suggestion. If the model is eighty-nine percent confident, show that number. If the model is fifty-two percent confident, show that too. Low confidence suggestions are easier for reviewers to override because the model itself is signaling uncertainty. High confidence suggestions still carry anchoring risk but at least the reviewer knows the model is sure.

The decision to use pre-filled suggestions should depend on model accuracy, task ambiguity, and reviewer experience. Use pre-filling for high-accuracy models on low-ambiguity tasks with experienced reviewers. Avoid pre-filling for moderate-accuracy models on high-ambiguity tasks with novice reviewers. The latter combination produces anchoring-driven errors that you will not catch until you audit the decisions weeks later.

## Showing AI Confidence: Calibrated vs Raw Scores

If you show the model's confidence, the number must mean something reliable. A confidence score that says ninety percent but is actually right sixty percent of the time is worse than no score at all. It misleads the reviewer into trusting predictions they should question.

Most model confidence scores are not well-calibrated. A model outputs a probability, but that probability reflects the model's internal calculation, not the true likelihood that the prediction is correct. A model that outputs zero-point-eight-five might be correct eighty-five percent of the time on some task types and fifty percent of the time on others. The raw score is not a dependable signal.

Calibration is the process of adjusting confidence scores so that they match empirical accuracy. If you calibrate the model and it outputs a confidence of seventy-five percent, you have verified that the model is correct seventy-five percent of the time on cases with that confidence level. Calibrated confidence is trustworthy. Raw confidence is not.

To calibrate confidence, you need a validation set. Run the model on a few thousand examples where you know the correct answer. Bucket predictions by their raw confidence score. For every prediction with a raw confidence between seventy and eighty percent, check the actual accuracy. If the model is correct on sixty-five percent of those cases, the calibrated confidence for that bucket is sixty-five percent, not seventy-five.

Once you have calibration data, you can map raw scores to calibrated scores. When the model outputs a raw confidence of zero-point-eight-two, you look up the calibrated accuracy for that range and show that to the reviewer instead. The reviewer sees a number that corresponds to real-world accuracy, not the model's internal probability distribution.

Some teams skip calibration and show raw scores anyway. This is a mistake. Reviewers quickly learn that the confidence numbers are unreliable, and they stop paying attention to them. Once trust in the confidence score is lost, you cannot easily get it back. If you are going to show confidence, show calibrated confidence. If you cannot calibrate, do not show the score.

Confidence scores work best when they help the reviewer allocate attention. A high-confidence prediction on a straightforward case signals that the reviewer can move quickly. A low-confidence prediction on an ambiguous case signals that the reviewer should slow down and examine the evidence carefully. If the score does not reliably distinguish between easy and hard cases, it adds noise instead of clarity.

## Evidence Highlighting: Showing Retrieval Results and Risk Spans

Evidence highlighting is another common form of model assistance. The model identifies the most relevant parts of the input and highlights them visually. In a text classification task, the model might highlight sentences that influenced its decision. In a document review task, the model might highlight the passages retrieved by a RAG system. In a content moderation task, the model might highlight the specific phrases that triggered a policy violation.

Highlighting works when it draws the reviewer's attention to the right evidence. A content moderator reviewing a long forum post benefits from having the problematic sentences highlighted. They can evaluate those sentences closely and skim the rest. If the highlights are accurate, this saves time without sacrificing quality.

Highlighting fails when it draws the reviewer's attention away from the right evidence. If the model highlights the wrong sentences, the reviewer focuses on irrelevant content and misses the important parts. If the model consistently highlights content that aligns with a particular bias, the reviewer's decisions will reflect that bias.

The risk is subtle. Reviewers do not ignore non-highlighted content entirely. They still read the full input. But they spend more time on highlighted sections. If those sections are misleading, the reviewer's judgment tilts. They give more weight to the highlighted evidence than they would if nothing were highlighted.

One way to test whether highlighting is helping or hurting is to run an A/B test. Half of reviewers see highlighted evidence. Half see plain content with no highlights. Measure accuracy and speed for both groups. If the highlight group is faster and equally accurate, highlighting is working. If the highlight group is faster but less accurate, highlighting is introducing bias. If the highlight group is not faster, the highlights are not salient enough to matter.

Another approach is to let the reviewer control the highlighting. Provide a toggle that turns highlights on or off. Some reviewers prefer to scan the content themselves first, then turn on highlights to verify they did not miss anything. Others prefer to see highlights immediately. Giving reviewers control reduces the anchoring effect because they can choose when to let the model guide their attention.

Highlighting is most valuable for long inputs. A reviewer evaluating a ten-page document benefits from highlights that point to the three most relevant paragraphs. A reviewer evaluating a two-sentence input does not need highlights. They can read both sentences in full.

Highlighting should never replace reading. If reviewers start approving decisions based solely on highlighted spans without reading the surrounding context, the highlights have become a shortcut. Shortcuts lead to errors. Monitor for this behavior. If reviewers are completing tasks faster than it is physically possible to read the full input, they are skipping content. That is a red flag.

## The Anchoring Problem and How to Detect It

Anchoring is insidious. Reviewers do not realize they are biased. They feel like they are making independent judgments. But the data shows otherwise. When you compare decisions made with model suggestions to decisions made without them, the distribution changes. Reviewers become more likely to agree with the model even in cases where independent human judgment would go the other way.

You detect anchoring by measuring agreement rates. Calculate how often reviewers agree with the model's suggestion. If agreement is above ninety-five percent and the model's standalone accuracy is below ninety percent, anchoring is likely. The reviewer is not catching all of the model's errors. They are accepting some bad suggestions because the suggestion biases their judgment.

Another signal is reduced override rates on ambiguous cases. If the model suggests approval on a borderline case, an unanchored reviewer will override about fifty percent of the time. An anchored reviewer will override less often. They need stronger evidence to contradict the suggestion. If you see override rates drop when suggestions are introduced, anchoring is affecting decisions.

A third signal is reduced variance in reviewer decisions. Without suggestions, different reviewers often disagree on ambiguous cases. That disagreement reflects genuine ambiguity. With suggestions, reviewer decisions start to converge. They all agree with the model. The variance drops. This looks like improved consistency, but it is actually bias. The reviewers are not becoming more aligned in their judgment. They are becoming more aligned with the model.

To measure this, run a calibration experiment. Take a set of tasks and have half of your reviewers complete them with model suggestions and half without. Compare the two groups. If the with-suggestions group agrees with the model significantly more than the without-suggestions group would have, anchoring is present.

If you detect anchoring, you have three options. First, remove the suggestions entirely. Go back to unaided review. Second, hide suggestions until after the reviewer makes an initial decision. Third, show suggestions but make them less prominent. Use smaller text, a neutral color, and position them below the primary decision interface instead of above it. Subtle design changes can reduce anchoring without eliminating the speed benefit of suggestions.

## When AI Assistance Hurts: Deskilling and Over-Reliance

The long-term risk of model-assisted review is deskilling. When reviewers rely on model suggestions for months or years, they lose the ability to make independent judgments. They stop building the expertise that comes from wrestling with ambiguous cases. They become validators rather than decision-makers.

Deskilling is hard to measure until it is too late. You notice it when you ask a reviewer to work without model assistance and they struggle. They were never bad at the task. They were just out of practice. They had offloaded the hard thinking to the model.

One way to prevent deskilling is to rotate reviewers between assisted and unassisted review. Every reviewer spends part of their time evaluating cases with no model suggestions. This keeps their judgment sharp. It ensures they can still do the task independently if the model becomes unavailable.

Another way is to reserve the hardest cases for unassisted review. If a case is flagged as high-complexity or high-ambiguity, the model does not provide a suggestion. The reviewer must evaluate it from scratch. This ensures that reviewers continue to practice the most difficult judgment calls, which are the ones most vulnerable to deskilling.

Over-reliance shows up in the data as declining override rates over time. In the first month with model suggestions, reviewers override the model eight percent of the time. Six months later, they override three percent of the time. The model has not improved. The reviewers have become more trusting. They are no longer critically evaluating the suggestions. They are rubber-stamping them.

If override rates drop without a corresponding improvement in model accuracy, intervene. Re-train reviewers on independent judgment. Temporarily remove model suggestions. Introduce spot-checks where a second reviewer evaluates the same case without seeing the first reviewer's decision or the model's suggestion. If the second reviewer frequently disagrees, the first reviewer has become over-reliant.

Model assistance is a tool. Like all tools, it can be used well or poorly. The goal is not to maximize the use of assistance. The goal is to use assistance where it improves outcomes and avoid it where it introduces bias, anchoring, or deskilling. That requires ongoing measurement, careful design, and a willingness to turn off features that look good on paper but hurt performance in practice.

Next, we explore how to display model confidence in ways that inform rather than mislead reviewers, and how to design override interfaces that encourage correction without penalty, in 3.14 — Confidence Visualization and Human Override UX.
