# 1.10 — Mapping Review Needs to System Design

The screen shows three hundred twenty unreviewed cases. The queue has been growing for four days. Half the review team is stuck waiting for cases that require specialist expertise. A quarter of the team is working on cases that should have been auto-resolved by the model. Two reviewers are duplicating work on the same high-priority case because the assignment logic did not account for urgency routing. The operations manager is manually triaging, reassigning work, and answering Slack messages about why cases are not being distributed evenly. The system is running. It is not working. The infrastructure exists, but it was designed for a workflow that does not match the actual review needs.

Most teams design review infrastructure around technical capabilities — what the system can do — instead of operational requirements — what the review process needs. They build task queues, assignment logic, and reviewer interfaces without mapping the specific patterns of their review workflow. The result is infrastructure that handles the easy cases smoothly but collapses when faced with the complexity of real operations: cases that need different types of expertise, cases with time-sensitive deadlines, cases that should not be reviewed at all, and cases that require coordination between multiple reviewers. Mapping review needs to system design means starting with the workflow, identifying the decision points and coordination requirements, and then designing infrastructure that supports those needs instead of forcing the workflow to adapt to generic tooling.

## The Five Review Workflow Patterns

Review workflows fall into five patterns, each with distinct infrastructure requirements. The first pattern is **independent single-pass review**: every case is reviewed exactly once by one reviewer, and reviewers work independently without coordination. This is the simplest pattern. Infrastructure needs are minimal — a task queue that assigns cases to reviewers, prevents duplicate assignment, and tracks completion. A customer support ticket classification system uses this pattern. Each ticket gets reviewed once to determine category. No coordination is needed. The infrastructure is a basic queue with round-robin assignment.

The second pattern is **consensus review**: every case is reviewed by multiple reviewers independently, and a consensus mechanism resolves disagreements. This pattern requires infrastructure that routes the same case to multiple reviewers without showing them each other's judgments until all reviews are complete, aggregates judgments into a consensus decision, and flags cases where reviewers disagree for resolution. A content moderation system uses consensus review for borderline cases. Three reviewers see the same post independently. If two agree, that judgment becomes the decision. If all three disagree, the case escalates to a senior reviewer. The infrastructure must prevent reviewers from seeing each other's work, track which cases have complete consensus data, and route disagreement cases to escalation queues.

The third pattern is **hierarchical escalation**: cases are routed to the least expensive reviewer who can handle them, with automatic escalation when confidence is low or stakes are high. A junior reviewer handles obvious cases. Ambiguous cases escalate to experienced reviewers. High-stakes cases escalate to domain experts or legal review. This pattern requires infrastructure that calculates confidence scores, implements escalation rules based on case attributes and reviewer judgments, and ensures that escalated cases reach the right expert without manual triage. A legal document review system uses hierarchical escalation. A paralegal reviews standard contracts. Non-standard clauses trigger escalation to an attorney. Regulatory filings escalate directly to a compliance specialist. The infrastructure routes based on document type, clause detection, and risk scores.

The fourth pattern is **iterative refinement**: a case is reviewed multiple times in sequence, with each reviewer adding detail or correcting errors from prior reviewers. This pattern requires infrastructure that tracks review version history, shows reviewers what prior reviewers did, and enforces sequencing so that reviewers cannot skip ahead in the pipeline. A medical imaging annotation system uses iterative refinement. A junior annotator draws initial bounding boxes around lesions. A senior radiologist refines the box boundaries and adds diagnostic labels. A subspecialist adds detailed classification. Each reviewer builds on the previous reviewer's work. The infrastructure must present prior annotations, track changes, and prevent reviewers from working on cases that have not completed prior stages.

The fifth pattern is **dynamic routing based on workload and expertise**: cases are assigned based not just on case attributes but on real-time reviewer availability, expertise, and workload balance. This pattern requires infrastructure that maintains a model of reviewer capabilities, tracks current workload and velocity, predicts completion times, and optimizes assignment to balance throughput, quality, and workload distribution. A financial compliance review system uses dynamic routing. Some reviewers are faster at transaction analysis. Others excel at document interpretation. The system routes transaction-heavy cases to transaction specialists and document-heavy cases to document specialists, while ensuring that no reviewer is overloaded and that work is distributed fairly. This requires real-time optimization, not static assignment rules.

## Mapping Pattern to Technical Requirements

Once you identify your workflow pattern, the technical requirements become clear. Independent single-pass review requires a task queue with deduplication, assignment tracking, and basic completion metrics. Build or buy a system that prevents the same case from being assigned to multiple reviewers, tracks which cases are in progress versus complete, and provides throughput dashboards. You do not need consensus logic, escalation rules, or expertise modeling. Keep the infrastructure simple.

Consensus review requires case replication across multiple reviewers, judgment aggregation logic, and disagreement resolution queues. When a case enters the queue, the system creates multiple work items — one for each reviewer who needs to see it — and hides each reviewer's judgment from others until all judgments are collected. The aggregation logic depends on your consensus rule. If you use majority vote, the system selects the judgment that at least two out of three reviewers chose. If you use weighted voting where experienced reviewers have more weight, the system calculates a weighted aggregate. If reviewers disagree and no majority exists, the case routes to an escalation queue. This requires more complex queue logic than single-pass review, but the core requirement is judgment isolation until consensus is reached.

Hierarchical escalation requires confidence scoring, rule-based routing, and multi-queue architecture. Confidence scoring can be model-based — the AI model outputs a confidence score, and cases below a threshold escalate — or reviewer-based — reviewers mark cases as uncertain, which triggers escalation. Rule-based routing defines when escalation happens: escalate if confidence is below seventy percent, escalate if the case involves regulatory risk, escalate if the reviewer is junior and the case type is complex. Multi-queue architecture means you have separate queues for junior reviewers, experienced reviewers, and specialists, and cases flow between queues based on escalation rules. This requires routing logic that evaluates rules in real time and moves cases between queues without losing context or duplicating work.

Iterative refinement requires version control, sequential dependency management, and annotation inheritance. Version control tracks every change to a case as it moves through the pipeline. Sequential dependency management ensures that a case cannot enter the senior radiologist queue until the junior annotator has completed it. Annotation inheritance presents the current state of annotations to each reviewer so they can refine instead of starting from scratch. A system built for independent review cannot support iterative refinement without significant rework — you need pipeline orchestration, not just task assignment.

Dynamic routing based on workload and expertise requires a reviewer capability model, real-time workload tracking, and optimization logic. The capability model captures what each reviewer is good at: reviewer A excels at transaction analysis with ninety-four percent accuracy and completes transaction cases in an average of eight minutes, reviewer B excels at document review with ninety-one percent accuracy and completes document cases in twelve minutes. Real-time workload tracking measures how many cases each reviewer currently has in progress, how long they typically take to complete similar cases, and whether they are approaching capacity limits. Optimization logic assigns the next case to the reviewer who can complete it fastest while maintaining quality and without overloading. This requires predictive modeling and real-time decision-making, not static assignment rules.

## The Cost of Mismatched Infrastructure

When infrastructure does not match workflow, you pay in three currencies: manual coordination overhead, degraded quality, and burned reviewer goodwill. A legal document review team used independent single-pass infrastructure for a workflow that actually required hierarchical escalation. Junior reviewers were assigned complex contracts they lacked expertise to handle. They spent thirty minutes per contract and made errors. Senior reviewers were assigned simple contracts that wasted their expertise. The team averaged twenty-two cases per day per reviewer when they should have averaged thirty-five. The mismatch cost them thirty-seven percent of potential throughput plus the quality cost of junior reviewers handling cases beyond their capability.

A content moderation team used static round-robin assignment for a workflow that needed dynamic routing. Some reviewers were faster than others. The round-robin logic assigned the same number of cases to fast and slow reviewers, which meant fast reviewers finished their daily quota in four hours and sat idle while slow reviewers worked ten-hour days to keep up. The team had capacity but could not use it because the assignment logic did not account for velocity differences. They manually rebalanced workload daily, which required an operations manager spending ninety minutes per day reassigning cases. The infrastructure created a coordination tax that consumed nearly fifteen percent of an FTE.

A financial compliance team used consensus review for every case because they believed quality required multiple reviewers. Ninety-two percent of cases were straightforward and did not need consensus. The consensus infrastructure forced three reviewers to look at every case, which tripled labor cost and tripled latency. Cases that should have been reviewed in fifteen minutes took forty-five minutes because the system waited for all three reviewers to complete their work before finalizing the decision. The team eventually moved to hierarchical escalation where only ambiguous cases went to consensus. Throughput increased by sixty-eight percent and cost per case dropped by sixty-two percent. The infrastructure mismatch had been costing them two-thirds of their potential efficiency.

## Designing for Multiple Patterns Simultaneously

Real workflows rarely fit one pattern cleanly. Most review systems need to support multiple patterns for different case types. A customer support AI system uses independent single-pass review for low-risk cases like billing questions, hierarchical escalation for medium-risk cases like technical troubleshooting, and consensus review for high-risk cases like account termination. The infrastructure must support all three patterns within the same queue system, routing cases to the appropriate pattern based on case attributes.

The design challenge is preventing pattern-specific logic from creating a tangled mess of conditional routing. A healthcare AI company built a review system that supported four different patterns with hard-coded logic for each. The routing code had ninety-seven conditional branches. Every time they added a new case type or changed escalation rules, they risked breaking existing logic. A refactor took six weeks. They rebuilt the system with a declarative routing engine: each case type declared its pattern as metadata, and the routing engine executed the pattern without custom code. Adding a new case type required declaring its pattern and escalation rules in a configuration file, not modifying code.

The key is separating pattern definition from pattern execution. The system should define patterns as reusable templates: single-pass pattern assigns cases to one reviewer from a pool, consensus pattern assigns cases to N reviewers and aggregates judgments using a specified rule, escalation pattern assigns cases to the lowest-capability reviewer who meets confidence thresholds and escalates based on rules. When a new case type is added, you map it to an existing pattern, configure pattern parameters, and the execution engine handles the rest. This keeps the system flexible without requiring custom engineering for every new workflow variant.

## The Reviewer Experience Constraint

Infrastructure that optimizes for operational efficiency but ignores reviewer experience creates throughput at the cost of quality and retention. A review system that maximizes utilization by ensuring reviewers never have idle time sounds efficient. In practice, it burns out reviewers who spend eight hours straight reviewing with no natural breaks. A content moderation team built a system that assigned the next case the instant a reviewer completed the current case. Reviewers felt trapped. They could not take a break without logging out entirely, which made them unavailable for urgent cases. The system optimized for zero idle time and created a work environment that felt oppressive. Reviewer turnover doubled.

Reviewer experience requirements must be explicit constraints in system design. Reviewers need breaks. The system should allow reviewers to mark themselves as needing a break without logging out, and respect that status for at least fifteen minutes before auto-assigning new work. Reviewers need variety. A system that assigns three hundred consecutive cases of the same type to a reviewer optimizes for expertise but creates monotony. Mix case types deliberately to keep work engaging. Reviewers need to feel progress. A queue that shows three hundred twenty pending cases without showing completion metrics makes reviewers feel like their work has no impact. Show daily completion counts, weekly trends, and how the queue size is changing over time so reviewers see the impact of their effort.

Reviewer experience also means respecting cognitive load. A system that assigns the hardest cases in sequence to the most experienced reviewer maximizes quality but destroys focus. Hard cases require deep concentration. Completing five hard cases in a row is exhausting. Intersperse hard cases with moderate cases so reviewers can recover between intense cognitive efforts. A legal document review system routes cases to experienced reviewers using difficulty scoring: seventy percent moderate cases, thirty percent hard cases, with hard cases distributed evenly throughout the day instead of clustered. Reviewers maintain higher accuracy because they are not cognitively depleted by continuous high-difficulty work.

## The Compliance and Auditability Layer

Review infrastructure must support compliance and audit requirements from day one. Building compliance as an afterthought is expensive and often incomplete. A financial services company built a review system without audit logging. Two years later, a regulatory audit required them to prove which human reviewer made which decision on which date and based on what criteria. The system tracked final decisions but not who made them or when. Reconstructing the audit trail required nine weeks of forensic work correlating database timestamps with authentication logs. The reconstruction was incomplete — some decisions predated detailed logging and could not be attributed definitively. The compliance gap cost them a fine and forced a system rebuild.

Audit requirements vary by industry but share common elements: identity and authentication logging that proves who accessed the system and when, decision provenance that traces every judgment to a specific reviewer at a specific time, criteria versioning that shows which version of evaluation criteria was in effect when a decision was made, and change history that records every modification to a case including who made the change and why. These requirements must be designed into the data model, not bolted on later. Every case record includes reviewer ID, timestamp, criteria version, and judgment. Every change to a case creates an immutable history entry. Every access to sensitive data generates an audit log.

The compliance layer also includes access control. Not every reviewer should see every case. Healthcare data requires reviewers to have specific certifications. Financial data requires background checks. Regulated content may require geographic restrictions — only reviewers in certain jurisdictions can review cases involving those jurisdictions. The infrastructure must enforce access control at the case level, not just at the system level. A reviewer logs in successfully but should not be assigned cases they lack certification to review. The queue system filters cases based on reviewer credentials before assignment. This prevents compliance violations that occur when a reviewer is technically authorized to use the system but not authorized to review specific case types.

## The Testing and Validation Problem

Review infrastructure is hard to test because correctness depends on human judgment and workflow complexity that cannot be fully simulated. You can unit test queue assignment logic, but you cannot unit test whether the assignment logic feels fair to reviewers. You can integration test consensus aggregation, but you cannot integration test whether the consensus mechanism resolves disagreements in a way that builds reviewer trust. The validation problem is that infrastructure quality is determined by operational performance under realistic conditions, which means you need production-like testing before production deployment.

The solution is shadow operations. Before deploying new infrastructure, run it in parallel with the existing system. Reviewers continue using the old system for production work. The new system receives the same cases, makes assignment decisions, calculates metrics, and generates reports — but none of its decisions affect production. You validate that the new system produces the same decisions the old system did, or when decisions differ, that the difference is intentional and correct. A customer support AI company spent four weeks running new dynamic routing infrastructure in shadow mode. The shadow system assigned cases differently than the old round-robin system, but validation showed that the new assignments balanced workload better without degrading quality. The shadow period caught three bugs that would have caused assignment failures in production.

Shadow operations also test operational workflows. A content moderation team built consensus review infrastructure with a new escalation queue for disagreement resolution. In shadow mode, they discovered that the escalation queue would have received forty-eight cases per day, overwhelming the two senior reviewers responsible for resolution. The shadow period revealed that their consensus rules were too strict and generated excessive escalations. They adjusted the rules to reduce escalation volume to fifteen cases per day before deploying to production. Without shadow testing, the infrastructure would have launched, created an unmanageable escalation queue, and required emergency fixes under pressure.

## The Instrumentation and Observability Requirement

Review infrastructure without observability is a black box. You know cases enter and decisions emerge, but you do not know what happens in between. When quality degrades or throughput drops, you cannot diagnose root causes. Observability means instrumenting every decision point in the review workflow: when a case enters the queue, when it is assigned to a reviewer, when the reviewer starts work, when they complete work, when the case escalates, when consensus is reached. Each event is logged with enough context to reconstruct the full history of any case and to aggregate patterns across all cases.

The key metrics are throughput, latency, quality, and utilization. Throughput measures cases completed per day per reviewer and across the system. Latency measures time from case entry to completion, time from assignment to start, and time from start to finish. Quality measures inter-rater agreement, accuracy against ground truth where available, and escalation rates as a proxy for ambiguity. Utilization measures the fraction of time reviewers spend actively reviewing versus waiting for cases or dealing with system issues. These metrics must be real-time dashboards, not weekly reports. A queue backlog that doubles in two hours needs to be visible immediately, not discovered in Monday's retrospective.

Observability also means making the system's behavior transparent to reviewers. A reviewer wants to know why they were assigned a particular case, how their performance compares to peers, and whether their judgments are accurate. Transparency builds trust. A legal document review system shows reviewers a summary after each case: you completed this case in eleven minutes, similar cases average nine minutes, your accuracy on this case type is ninety-one percent. The feedback helps reviewers calibrate their effort and improve over time. A system that treats reviewers as interchangeable task executors without feedback creates disengagement. Reviewers want to know that their work matters and that they are improving.

## The Evolution Path from Initial Design

No infrastructure design is perfect on the first deployment. The goal is to design for evolution — to build in a way that allows the system to grow and adapt as you learn what actually matters. A common mistake is over-engineering for flexibility you never use. A startup built a review system with pluggable routing logic, configurable consensus rules, and dynamic workflow definitions. The flexibility cost twelve engineer-weeks to build and added complexity that made the system harder to debug. In two years of operation, they never changed the routing logic, never adjusted consensus rules, and never added new workflow definitions. The flexibility was waste.

The opposite mistake is under-engineering and hardcoding assumptions that break as soon as requirements change. A healthcare AI company built a review system with the assumption that every case took the same amount of time to review. When they added a new case type that took four times longer, the queue management logic failed. Cases that required sixty minutes were assigned to reviewers with fifteen minutes left in their shift, which created incomplete work and handoff overhead. Rebuilding the system to track estimated time per case type took five weeks and required migrating existing queue state.

The balance is designing for the changes you know are coming and being pragmatic about hypothetical flexibility. You know you will add new case types — design a case type registry that new types can plug into without code changes. You know criteria will evolve — version criteria and associate decisions with specific versions. You know reviewer team composition will change — model reviewer capabilities as data, not code. You do not know whether you will need real-time collaboration or multi-language support or integration with a dozen different data sources — do not over-engineer for those possibilities until they become real requirements.

## The Integration Boundary Decision

Review infrastructure must integrate with upstream data sources and downstream consumers. The integration boundary determines how coupled your review system is to the rest of your infrastructure. Tight coupling means the review system directly queries your production database, pushes decisions into your application tables, and shares authentication with your main application. Loose coupling means the review system is a standalone service that receives cases via API, returns decisions via API, and manages its own data and authentication. Tight coupling is faster to build initially but creates fragility. Loose coupling requires more upfront design but produces resilient, maintainable systems.

A customer support AI company built tight coupling. The review interface queried the production support ticket database directly. When a reviewer completed a case, the judgment was written directly into the ticket record. This worked until the production database schema changed. The schema migration broke the review interface because queries assumed the old structure. Fixing the review system required coordinating deployments between the review team and the application team. Every database change became a coordination problem. The tight coupling created deployment dependencies that slowed down both teams.

A content moderation team built loose coupling. Cases entered the review system via a queueing API. The review system stored cases, judgments, and reviewer data in its own database. When a decision was final, the system published an event to a message queue that downstream consumers subscribed to. The loose coupling meant the review system could evolve independently. Database schema changes in the main application did not affect the review system. Deployments were independent. The only contract was the API and event schema, which changed rarely and were versioned explicitly. The loose coupling cost three additional weeks of initial engineering but eliminated ongoing coordination overhead.

The decision depends on your organizational structure and scale. If the same team owns the review system and the main application, tight coupling is acceptable — schema changes are coordinated internally. If different teams own the systems, loose coupling is essential to avoid cross-team dependencies. If your review system serves multiple applications, loose coupling is mandatory — you cannot tightly couple to five different databases and remain sane. The earlier you adopt loose coupling, the less painful the eventual migration.

Mapping review needs to system design is not a one-time exercise. It is a continuous discipline. As workflows evolve, case types change, team composition shifts, and organizational priorities adjust, the infrastructure must adapt. The teams that succeed treat review infrastructure as product infrastructure — something that receives ongoing investment, observability, and evolution — not as internal tooling that is built once and then neglected. The foundation is in place. The next challenge is designing queue architecture that distributes work efficiently, prevents bottlenecks, and adapts to real-time operational dynamics.

