# 5.2 — Horizontal vs Vertical Scaling in Human Review

Horizontal scaling adds more reviewers. Vertical scaling makes each reviewer more effective. The first path multiplies capacity by headcount. The second path multiplies capacity by leverage. Most teams default to horizontal scaling because it's conceptually simple — you need more throughput, you hire more people. But horizontal scaling has hard limits, and past a certain point, vertical scaling delivers better results at lower cost. The teams that scale human review successfully use both strategies deliberately, applying each where it has the most impact.

Horizontal scaling works when the bottleneck is purely volume and the work is standardized. If you have 10,000 cases per week and each reviewer can handle 500 cases, you need 20 reviewers. If volume doubles, you hire 20 more. The math is clean, the onboarding is repeatable, and the coordination overhead is manageable as long as you've designed the infrastructure to support it. Horizontal scaling is the right answer when you have a well-defined task, clear guidelines, and tooling that supports parallel work without coordination penalties.

Vertical scaling works when the bottleneck is complexity, inconsistency, or inefficiency. Instead of adding more people doing the same work, you invest in making each person more capable — through better tooling, better training, better guidelines, or automation that removes low-value work. Vertical scaling is harder to design and slower to implement, but it has no hard scaling ceiling. A team of 10 reviewers with excellent tools and automation can outperform a team of 30 reviewers with poor infrastructure. Vertical scaling is the right answer when coordination overhead is already high, when hiring is difficult or expensive, or when the work requires judgment that doesn't parallelize well.

## The Cost Curves

Horizontal scaling has a linear cost curve — every reviewer you add costs roughly the same in salary, benefits, and onboarding. But the **coordination cost** is superlinear. At 10 reviewers, coordination might consume 15 percent of total team time. At 30 reviewers, it might consume 35 percent. At 60 reviewers, it might consume 50 percent. The marginal value of each new hire declines as the team grows, because more of their time is spent coordinating and less is spent reviewing.

Vertical scaling has a different cost curve. The upfront investment is high — building better tools, designing automation, rewriting guidelines, training reviewers on new workflows. But once that investment is made, the marginal cost of additional throughput is low. If you invest $200,000 in automation that filters out 40 percent of low-risk cases, you've effectively increased your team's capacity by 40 percent without adding a single person. That capacity gain persists as long as the automation works, and it scales with volume — if your review load doubles, the automation filters twice as many cases, with no additional cost.

The break-even point between horizontal and vertical scaling depends on your volume, your growth rate, and your time horizon. If you're reviewing 2,000 cases per week and expect that to stay flat, horizontal scaling is probably fine — hire the people you need, and accept the coordination cost. If you're reviewing 10,000 cases per week and expect that to grow to 50,000 within a year, vertical scaling is essential. You cannot hire your way to 50,000 cases per week without collapsing under coordination overhead. You need leverage.

The mistake most teams make is choosing one strategy and ignoring the other. Pure horizontal scaling hits the wall around 30 to 50 reviewers, where coordination overhead makes further hiring counterproductive. Pure vertical scaling underinvests in people, which creates bottlenecks when volume spikes or when automation fails. The right approach is hybrid: invest in vertical scaling to maximize the capacity of each reviewer, then use horizontal scaling to meet volume that exceeds your vertically-scaled capacity. But vertical comes first — you get the infrastructure right, then you scale headcount into that infrastructure.

## Vertical Scaling Techniques

Vertical scaling has four primary levers: automation, tooling, specialization, and training. Each lever multiplies reviewer capacity in different ways, and the most effective vertical scaling strategies use all four in combination.

**Automation** removes work that doesn't need human judgment. A model that auto-approves cases with confidence above 0.95 eliminates 30 to 50 percent of the queue without touching headcount. A model that auto-escalates cases with specific risk markers ensures that hard cases reach experts immediately instead of bouncing through first-tier reviewers who can't resolve them. Automation doesn't replace reviewers — it removes the work that wastes their time and lets them focus on the cases where human judgment matters.

**Tooling** makes each review decision faster and more accurate. A reviewer who has to switch between three systems, copy-paste data, and search for context might spend 40 percent of their time on logistics and 60 percent on actual judgment. A reviewer with a unified interface that surfaces all context in one view, prefills forms, and autocompletes common entries might spend 10 percent of their time on logistics and 90 percent on judgment. The judgment takes the same time either way, but the total time per case drops by half. Better tooling is a force multiplier.

**Specialization** matches reviewer skill to task complexity. If 70 percent of your cases are straightforward and 30 percent require deep expertise, assigning all cases to highly skilled reviewers wastes their capacity. Instead, create tiers: junior reviewers handle the 70 percent of simple cases, senior reviewers handle the 20 percent of moderate cases, and experts handle the 10 percent of hard cases. Each reviewer works at the top of their capability, which maximizes throughput and reduces burnout.

**Training** increases the percentage of cases each reviewer can handle independently. A new reviewer might escalate 20 percent of cases in their first month. After three months of structured training and calibration, that drops to 8 percent. After six months, it drops to 3 percent. Every percentage point reduction in escalation rate increases team capacity, because escalations create latency and consume expert time. Training is an ongoing investment, not a one-time event. The best review teams have continuous calibration sessions, case studies, and feedback loops that keep reviewers sharp and aligned.

These four levers compound. Automation reduces the queue, tooling makes each case faster, specialization ensures the right person handles each case, and training reduces escalations. A team that invests in all four can handle 3x to 5x the volume of a team that invests in none, with the same headcount. That's vertical scaling.

## When Horizontal Scaling Is the Right Answer

Horizontal scaling is not wrong — it's necessary when volume exceeds your vertically-scaled capacity. But it only works if you've built the infrastructure to support it. Adding 20 reviewers to a team with poor tooling, no specialization, and no automation just creates a larger, slower, more chaotic team. Adding 20 reviewers to a team with strong infrastructure means those 20 people are productive from day one, because the system is designed to absorb them without coordination penalties.

The signal that you're ready for horizontal scaling is that your **queue growth rate exceeds what vertical improvements can handle**. You've automated what you can, you've optimized tooling, you've specialized roles, and you've trained your team — and the queue is still growing. At that point, you need more people. But you need them in a structured way. Horizontal scaling works when you have clear tiers, predictable onboarding, repeatable training, and tooling that supports parallel work without stepping on each other's toes.

The failure mode of horizontal scaling is adding people faster than you can onboard them. If your onboarding takes six weeks and you're hiring five people per month, your first hires are barely trained before the next batch arrives. Your training capacity becomes the bottleneck, and your error rates climb because reviewers are working before they're ready. The fix is to **batch hiring in cohorts** and space cohorts far enough apart that the previous cohort is fully trained before the next one starts. This feels slower, but it's faster in the long run, because you're adding effective reviewers instead of partially-trained ones who need hand-holding.

The second failure mode is hiring without structure. If you add 30 reviewers to a flat team with no tiers, no leads, and no specialization, you've created a coordination nightmare. The fix is to **introduce structure as you scale**: team leads at every 8 to 10 reviewers, specialist tracks for domain-specific cases, tiered escalation paths, and unit-level autonomy. Structure feels like bureaucracy, but it's the only way to prevent chaos.

## The Hybrid Strategy

The teams that scale human review to 100+ reviewers and beyond use a hybrid strategy that combines vertical and horizontal scaling deliberately. They start with vertical scaling — automation, tooling, specialization, training — until they hit diminishing returns. Then they scale horizontally in structured cohorts, with each cohort entering an infrastructure that maximizes their productivity. They continue investing in vertical improvements even as they scale horizontally, because every vertical gain multiplies the output of the entire team.

A real example: a legal AI company in 2025 started with a review team of 12 people handling 3,000 contract reviews per week. They invested six months in vertical scaling — built automation that filtered 35 percent of low-risk cases, redesigned tooling to cut review time from 8 minutes to 4.5 minutes per case, created specialist tracks for domain-specific contract types, and ran monthly calibration sessions. After six months, the same 12 people were handling 7,500 reviews per week. Then they scaled horizontally, hiring 18 more people over nine months. By the end of the year, the team of 30 reviewers was handling 22,000 reviews per week with lower error rates than they had at 12 people. The vertical improvements made the horizontal scaling possible.

The mistake they avoided was scaling horizontally first. If they'd hired 18 people immediately, they would have had 30 reviewers doing inefficient work with high coordination overhead, probably handling 12,000 reviews per week instead of 22,000. The vertical scaling created leverage. The horizontal scaling multiplied that leverage.

This is the playbook: vertical first, then horizontal. Automate, optimize, specialize, train — then hire. And keep investing in vertical improvements even as you grow, because the marginal value of a vertical gain increases as the team gets larger. An automation improvement that saves 2 minutes per case saves 400 hours per week for a team of 10 reviewers. It saves 2,000 hours per week for a team of 50 reviewers. Vertical scaling scales.

## Measuring Scaling Efficiency

You need to measure whether your scaling investments are working. The core metric is **throughput per reviewer per week** — total cases reviewed divided by total reviewers. For a healthy operation, this should stay flat or grow slightly as you scale. If it's declining, you're scaling horizontally without sufficient vertical leverage, and coordination overhead is eating your gains.

The second metric is **cost per case** — total reviewer cost divided by total cases reviewed. This should decline or stay flat as you scale, because vertical investments amortize over volume. If cost per case is rising, you're hiring faster than your infrastructure can support, and you're paying more for the same output.

The third metric is **error rate** — the percentage of cases that require rework, escalation, or downstream correction. This should decline as reviewers gain experience and as vertical improvements reduce ambiguity. If error rate is rising, your scaling is outpacing your training capacity, and you're adding reviewers who aren't ready.

Track these metrics monthly. When throughput per reviewer drops, pause horizontal scaling and invest in vertical improvements. When cost per case rises, audit your onboarding and coordination processes. When error rate climbs, slow hiring and strengthen training. These metrics tell you whether you're scaling efficiently or just scaling.

Scaling human review is not one strategy — it's two strategies used in combination. Vertical scaling creates leverage. Horizontal scaling multiplies that leverage. Miss either one, and you hit a wall. Get both right, and you can scale to hundreds of reviewers without losing quality, speed, or sanity.

Next, we'll explore how tiered review structures separate routine cases from complex ones, and how to design tiers that prevent expert bottlenecks.

