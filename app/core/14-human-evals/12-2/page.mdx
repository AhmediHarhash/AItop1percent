# 12.2 â€” Audit Trail Design: What to Log and Why

In August 2025, a European insurance company received a GDPR complaint from a customer whose claim had been partially automated. The regulator requested all review records for that claim within 14 days. The company's review system logged decisions, timestamps, and reviewer IDs. It did not log what version of the decision guidelines the reviewer had used, what the model's original output was before human edits, or what specific policy section justified the decision. The company could prove a human had reviewed the claim. It could not prove the review was meaningful or compliant. The regulator issued a formal warning and required a third-party audit of the entire review system. The missing logs cost the company 11 months of remediation work and 340,000 euros in audit and legal fees.

Audit trails are not optional. They are the only evidence that your human review process happened the way you claim it did. When a regulator, a plaintiff's attorney, or an internal audit team examines your AI system, they do not accept assurances. They accept logs. If the logs do not exist, the process did not happen. If the logs are incomplete, the process is assumed to be non-compliant. If the logs are inconsistent or reconstructed after the fact, they are evidence of a cover-up. Your audit trail is your defense. Design it badly and you have no defense at all.

## The Core Event Schema

Every review action must generate a structured log entry. That entry is not a line in a text file. It is a structured record with mandatory fields, enforced constraints, and immutable timestamps. The minimum schema includes nine fields.

**Event ID.** A unique, non-reusable identifier for this specific log entry. UUID v4 or equivalent. This field allows you to reference a specific event unambiguously in cross-system analysis or legal filings.

**Timestamp.** The exact moment the event occurred, in UTC, with millisecond precision. Not the time the log was written. Not the time the database transaction committed. The time the user took the action. This timestamp must be sourced from a synchronized, tamper-resistant clock. If your review system runs across multiple servers, clock drift becomes a compliance risk. Use NTP synchronization and log the clock source.

**Reviewer ID.** The unique identifier of the person or system that performed the review. Not their email address, which can change. Not their username, which can be reassigned. A stable internal ID that links to your identity management system. This allows you to trace all actions by a reviewer across time, even if they change roles, names, or email addresses.

**Case ID.** The unique identifier of the item being reviewed. This must link to your production case database, your data warehouse, and any external ticketing or CRM systems. A case ID that exists only in the review system is useless for compliance. The regulator will ask for the case by its production identifier, not by an internal review queue number.

**Action Type.** What the reviewer did. The common action types are: viewed case, approved output, rejected output, edited output, requested escalation, added comment, reassigned case, marked as requiring legal review, marked as sensitive. Do not use free-text action descriptions. Use an enumerated type with a controlled vocabulary. This makes the logs queryable and prevents ambiguity. "Approved" is clear. "Looks good" is not.

**Decision Outcome.** What happened to the case as a result of this action. Approved and sent to production. Rejected and withheld. Escalated to senior review. Sent back to the model for regeneration. Queued for additional human review. This field is distinct from action type. A reviewer can approve an output that does not go to production because it requires secondary approval. The action is approve. The outcome is pending.

**Policy Version.** The version identifier of the decision guidelines, rubric, or policy document the reviewer was instructed to follow at the time of review. Not the current version. The version active when the decision was made. This field is how you prove that the reviewer had access to the correct instructions. If the guidelines change after the decision but before the audit, you need this field to reconstruct what the reviewer was supposed to do.

**Model Output Snapshot.** A hash or reference to the exact model output the reviewer saw. Not the model's current output if you regenerate the case today. The output at decision time. Store the hash in the audit log and the full output in a versioned artifact store. This allows you to prove what the reviewer evaluated without bloating your audit logs with full response text.

**Decision Rationale.** The reviewer's explanation for their decision, captured in their own words. This field is mandatory for any decision that affects a user's access, rights, or benefits. It can be optional for low-risk informational reviews. The rationale does not need to be long. It needs to be specific. "Output was accurate and complied with policy Section 4.2" is sufficient. "Looks fine" is not. This field is what makes the review meaningful in the legal sense. Without it, the review is a checkbox exercise.

These nine fields are the minimum. High-risk systems need more.

## Extended Fields for High-Risk Systems

If your AI system is classified as high-risk under the EU AI Act, or if it processes data governed by HIPAA, SOX, or sector-specific regulations, the audit trail must capture additional context.

**Data Subject Identifier.** A stable, pseudonymized reference to the person whose data is being processed. Not their name. Not their email. A hashed or tokenized ID that links back to your identity system through a secure lookup. This field allows you to respond to GDPR subject access requests, HIPAA audit queries, and right-to-explanation requests without exposing PII in your audit logs.

**Regulatory Basis.** The legal or policy basis for the processing activity. GDPR Article 6(1)(b) for contract performance. GDPR Article 9(2)(h) for health data processing. HIPAA Treatment, Payment, or Operations. This field proves that the processing was lawful at the time it occurred. If the regulatory landscape changes, this field shows what basis you relied on.

**Sensitivity Classification.** The data sensitivity tier of the case. Public, Internal, Confidential, Restricted, or whatever taxonomy your organization uses. This field is necessary for demonstrating that access controls were appropriate. If a Restricted case was reviewed by someone without Restricted clearance, this field makes that violation visible in audit queries.

**Override Indicator.** A boolean flag indicating whether this decision overrode the model's original recommendation. If the model said approve and the reviewer said reject, this field is true. This field is critical for demonstrating meaningful human oversight. If the override rate is zero across thousands of reviews, the audit conclusion will be that oversight is a rubber stamp. If the override rate is 40 percent, the audit conclusion is that the model is not fit for purpose. Both extremes trigger regulatory scrutiny. This field is how you track and explain the override rate.

**Confidence Score.** The model's confidence in its output at decision time. If available, log it. This allows you to analyze whether reviewers are more skeptical of low-confidence outputs and more trusting of high-confidence ones. It also allows you to demonstrate that low-confidence outputs received additional scrutiny, which supports a claim of risk-based review.

**Review Duration.** The time elapsed between case assignment and decision. Measured in seconds. This field proves that reviewers spent adequate time on each case. If the median review time for a complex medical case is 12 seconds, that is evidence of insufficient review. If it is 18 minutes, that supports a claim of thorough oversight. This field must be calculated from actual user activity, not from queue timestamps. A case sitting in a queue for three hours while the reviewer is at lunch is not 3 hours of review time.

**Reviewer Training Version.** The version identifier of the training program the reviewer completed before being authorized to review this case type. If a reviewer makes a decision without current training, that is a compliance violation. This field makes those violations detectable.

## What Not to Log

Audit trails must be comprehensive, but they must not be indiscriminate. Logging too much creates three problems: storage cost, query performance, and privacy exposure. Do not log the full case content in every event. Log a reference to it. Do not log every mouse click and keystroke. Log the decisions. Do not log PII unless you have a retention and deletion plan for it.

The audit trail is not a surveillance system. It is an accountability system. That distinction matters legally. If you log every page view, every hover, every scroll position, and every idle period, you are building a reviewer surveillance database. That creates employee privacy concerns, union issues, and in some jurisdictions, legal violations. Log what is necessary to prove compliance and reconstruct decisions. Do not log what is necessary to micromanage reviewers.

The test is simple. If a regulator or a plaintiff's attorney requests the audit trail, would you be comfortable showing them what you logged? If the answer is no because the logs expose private user data unnecessarily, you logged too much PII. If the answer is no because the logs expose granular reviewer activity that is not decision-relevant, you logged too much surveillance data. The audit trail should be something you can hand over with confidence, not something you need to redact heavily before disclosure.

## Immutability and Tamper Evidence

An audit trail is only credible if it cannot be altered after the fact. That means append-only logs, cryptographic integrity checks, and external timestamping.

**Append-only storage.** Once a log entry is written, it cannot be modified or deleted. This is enforced at the database level, not at the application level. Use a database that supports write-once-read-many semantics, or use an append-only table with no delete or update permissions. If compliance requires deletion of PII, the deletion itself must be logged as a new event. The original log entry is tombstoned or redacted, but the fact of its existence and the reason for its deletion are preserved.

**Cryptographic hashing.** Each log entry includes a hash of the previous entry, creating a blockchain-like chain of custody. If any entry is altered, the chain breaks, and the tampering is detectable. This is standard practice in regulated industries and increasingly required by AI-specific frameworks.

**External timestamping.** Use a trusted timestamping authority to periodically sign the current state of the audit log. This proves that the log existed in its current form at a specific time, making backdating or post-hoc fabrication detectable. This is especially important for high-stakes decisions where the decision date is legally significant.

These measures are not paranoia. They are standard audit requirements. If your audit trail is stored in a regular database where any engineer with production access can edit rows, the logs are legally useless. A competent opposing attorney will argue that the logs could have been altered and should be excluded as evidence. A regulator will assume the logs are unreliable and impose harsher penalties. Immutability is not a nice-to-have. It is what makes logs credible.

## The Query Interface

Audit logs are useless if you cannot query them when the audit arrives. Design the query interface before you design the logging schema.

You must be able to retrieve all events for a specific data subject within minutes. That requires indexing by data subject ID. You must be able to retrieve all actions by a specific reviewer across a date range. That requires indexing by reviewer ID and timestamp. You must be able to retrieve all decisions of a specific type made under a specific policy version. That requires indexing by action type and policy version. If any of these queries take hours to run, you cannot meet audit deadlines.

The query interface must also support filtering by outcome, by sensitivity, by override status, and by review duration. An auditor will ask questions like: show me all rejected cases reviewed by Tier 2 reviewers in March where the model confidence was above 0.9 and the review duration was under 30 seconds. If your system cannot answer that question, the auditor will assume you are hiding something.

Build the query interface as a first-class compliance tool, not as an afterthought. Give it to your Legal and Compliance teams before you give it to an external auditor. Let them run practice audits. If they cannot get the answers they need, the infrastructure is not ready.

## The Retention and Deletion Workflow

The audit trail has two lifecycle stages: active retention and compliant deletion. Active retention means the logs are stored in a queryable, indexed system for as long as they might be needed for audit, litigation, or regulatory response. Compliant deletion means the logs are anonymized, archived, or destroyed according to the retention policy.

Retention periods vary by regulation and by case type. SOX requires seven years for financial data. HIPAA requires six years for health data. The EU AI Act requires retention for the operational lifetime of the system for high-risk use cases. GDPR requires deletion when the processing purpose ends, unless another legal basis applies. Your retention policy must map these requirements to case types and log categories.

Deletion is not a bulk operation. It is a scoped operation. You delete PII but retain metadata. You anonymize data subject identifiers but preserve decision statistics. You redact sensitive case content but keep decision outcomes. The deletion workflow must be automated, logged, and auditable. If you delete logs manually, you will delete the wrong things, skip required redactions, or violate referential integrity.

The deletion event itself is logged. The log entry records what was deleted, why, when, and by whom. This creates an unbroken audit chain even when the underlying data is gone. The regulator can see that a case existed, was reviewed, was decided, and was later deleted in compliance with retention policy. That is a complete audit story.

The next subchapter covers reviewer identity and access control: how to ensure that only authorized people review cases, how to enforce the principle of least privilege, and how to prove it during an audit.

