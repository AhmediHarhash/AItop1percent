# 10.10 — Metrics Gaming and Perverse Incentives

The operations manager noticed the pattern on a Thursday afternoon. Reviewer productivity had increased 22% over the past month. Median review time had dropped from 4.1 minutes to 3.2 minutes per case. Throughput was up. Cost per review was down. The numbers looked great. Then she spot-checked twenty recent reviews. Fifteen of them were incomplete. Reviewers were marking cases as reviewed without reading the full context. They were selecting the first plausible label and moving on. They had figured out that the system measured speed and volume, not accuracy. Agreement rate was still acceptable because they were all cutting the same corners in the same way. The metrics said productivity was soaring. Reality said quality was collapsing.

Metrics gaming is what happens when people optimize for the measure instead of the goal. The goal is high-quality review that improves production AI. The measure is throughput, agreement, or review time. When the measure becomes the target, it stops being a useful measure. Reviewers stop asking "is this label correct?" and start asking "which label gets me through this case fastest?" Teams stop asking "are we catching the errors that matter?" and start asking "are we hitting our agreement target?" The system rewards the wrong behavior, and everyone involved knows it's wrong, but the incentives are too strong to resist.

## How Metrics Become Targets and Targets Become Gamed

The moment you tie compensation, performance reviews, or team status to a metric, that metric becomes a target. If reviewers are paid per case reviewed, they will maximize cases per hour. If they're evaluated on agreement with peers, they will converge on whatever labels their peers choose, even if those labels are wrong. If they're rewarded for low skip rates, they will guess rather than escalate. None of this is malicious. It's rational. People respond to incentives. If the incentive structure rewards speed over accuracy, you get speed. If it rewards conformity over correctness, you get conformity.

The healthcare company that saw productivity spike had introduced a performance bonus tied to throughput. Reviewers in the top quartile for cases reviewed per week earned an extra 500 dollars per month. The company intended to reward efficiency. What they actually rewarded was corner-cutting. Reviewers who took time to read full medical histories and cross-check medication interactions earned less than reviewers who skimmed summaries and rubber-stamped labels. Within three weeks, everyone learned the game. Slow down and you lose money. Speed up and you win. Quality became optional.

The second dynamic that drives gaming is measurement lag. If you measure throughput in real time but measure accuracy only once per quarter, reviewers will optimize for throughput because the feedback is immediate. The damage to accuracy won't show up for months, and by then, the fast reviewers have already collected their bonuses. The lag between the gamed metric and the consequence of gaming creates a window where bad behavior pays off. To prevent gaming, measure the thing you care about at the same cadence as the thing you're incentivizing. If you reward weekly throughput, measure weekly accuracy. If the accuracy measurement can't keep up, don't reward throughput weekly.

## Common Gaming Patterns in Review Operations

Certain gaming patterns appear across nearly every review operation that relies on poorly designed metrics. Recognizing the patterns early lets you redesign the system before the gaming becomes endemic.

The first pattern is label convergence without calibration. When reviewers are evaluated on agreement with peers, they naturally converge toward whatever the group consensus is. If three reviewers label a case as correct and one labels it as incorrect, the fourth reviewer will adjust their future labels to match the majority, even if the majority is wrong. Over time, everyone agrees with everyone else, and agreement metrics look excellent. But if the entire group has drifted away from the true ground truth, high agreement is meaningless. This is why agreement must be measured against gold-standard cases, not just peer-to-peer. Peer agreement tells you reviewers are consistent. Calibration against gold cases tells you they're correct.

The second pattern is skip rate suppression. If reviewers are penalized for skipping cases or escalating unclear cases, they will stop skipping. Instead, they'll guess. A 2% skip rate looks efficient. But if that 2% represents the only cases where reviewers are uncertain, and they're now guessing on the other 8% of ambiguous cases, your error rate just quadrupled. Low skip rate is only good if reviewers are genuinely confident. If they're guessing to avoid the skip penalty, low skip rate is a disaster. Monitor both skip rate and the accuracy of non-skipped cases. If skip rate drops and accuracy drops, reviewers are guessing instead of escalating.

The third pattern is cherry-picking easy cases. If reviewers can self-select which cases to review from a queue, they'll pick the easiest ones. A case that requires reading two sentences and selecting a binary label takes 90 seconds. A case that requires reading ten pages of context and evaluating five dimensions takes eight minutes. If both cases pay the same, reviewers will compete for the easy cases and leave the hard cases in the queue. The hard cases either don't get reviewed or get assigned to whoever has no choice. Monitor case difficulty distribution per reviewer. If high performers are handling disproportionately easy cases, your routing is broken or your reviewers are gaming the queue.

The fourth pattern is time inflation without quality improvement. If reviewers are paid by the hour rather than by the case, the incentive flips. Now they want to go slowly. A case that could be reviewed in three minutes gets stretched to six minutes. Reviewers add unnecessary steps, re-read context they already understood, or take longer breaks between cases. Hourly pay without quality accountability rewards inefficiency. The fix is hybrid compensation — base hourly pay plus bonuses tied to both throughput and accuracy. Reviewers can't game by going too fast or too slow. They have to balance both.

## Designing Metrics That Resist Gaming

No metric is ungameable. But some metrics are harder to game than others. The goal is to design a measurement system where the easiest way to score well is to actually do good work. That requires measuring multiple dimensions simultaneously and ensuring that optimizing one dimension doesn't degrade another.

The simplest anti-gaming structure is a composite score that balances throughput and accuracy. Instead of rewarding reviewers based on cases per hour alone, reward them based on a formula: high-quality cases per hour. A high-quality case is one where the reviewer's label matches calibration gold cases, agrees with peer reviewers on ambiguous cases, and is completed within a reasonable time window. A case reviewed in 30 seconds that turns out to be wrong is worth zero. A case reviewed in eight minutes that's correct is worth one. A case reviewed in three minutes that's correct is worth 1.5 because it's both accurate and efficient.

The composite score prevents speed-only optimization and accuracy-only optimization. Reviewers can't win by rushing through cases incorrectly. They also can't win by spending ten minutes per case achieving perfection while their throughput craters. The optimal strategy is to be both fast and accurate, which is the actual goal. The weights in the formula let you tune the balance. If speed matters more, weight throughput higher. If accuracy matters more, weight quality higher. But both must be present.

The second anti-gaming technique is dynamic thresholds. Instead of setting a fixed agreement target of 90%, set the target as top 50% of the reviewer cohort. Now reviewers can't converge on a low-quality consensus and call it done. They're competing against each other. As the top performers improve, the threshold rises. This creates a continuous improvement dynamic rather than a "good enough" plateau. But be careful — if you use cohort-based ranking without absolute quality floors, you can end up with a race to the bottom where everyone is bad but half of them are ranked "good" because they're less bad than the other half. Combine cohort ranking with absolute minimums. To qualify for a bonus, you must be in the top 50% AND above 88% agreement on gold cases.

The third anti-gaming technique is randomized audits. If reviewers know that 10% of their cases will be audited by a senior reviewer or cross-checked against expert labels, they can't game consistently. They don't know which cases will be audited, so they have to treat every case as if it might be checked. Randomized audits catch both intentional gaming and unintentional sloppiness. The audit rate doesn't need to be high — 10% is often enough to change behavior. The key is unpredictability. If reviewers know that only cases flagged by automated systems get audited, they'll game around the automated flags.

## Incentive Structures That Align With Quality

The way you pay reviewers determines what they optimize for. Pay per case and you get speed. Pay per hour and you get slowness. Pay for agreement and you get conformity. Pay for perfection and you get risk aversion and skyrocketing skip rates. The right incentive structure balances all of these and ties compensation to the actual business goal: improving production AI quality.

The most robust structure is base pay plus tiered performance bonuses. Base pay is hourly or salaried, guaranteeing income regardless of case volume or difficulty. This removes the pressure to rush. Performance bonuses are tied to a composite quality score that includes calibration accuracy, peer agreement on ambiguous cases, throughput relative to task complexity, and production impact. Production impact is the hardest to measure but the most important. If a reviewer's labels are used to retrain a model and production error rate drops, that reviewer's work had impact. If production error rate stays flat or increases, the labels didn't help.

Measuring production impact requires closing the loop between review and deployment. Tag every reviewed case with the reviewer ID. When you retrain a model using reviewed data, track which reviewers contributed labels. After deployment, compare production metrics before and after. If reviewers A, B, and C contributed 80% of the training labels and production quality improved, A, B, and C get impact bonuses. If the model got worse, they don't. This creates a direct line between reviewer work and product outcomes. Reviewers learn that agreement with peers is not enough — the labels have to actually improve the model.

Tiered bonuses reward consistency. A one-time spike in performance might be luck. Sustained high performance over three months is skill. Bonuses should vest over time. If a reviewer hits top-tier performance in January, they get a provisional bonus. If they maintain it through March, the bonus is confirmed and paid. If they drop off, the bonus is reduced. This discourages short-term gaming and encourages long-term quality.

Avoid zero-sum ranking systems where only the top 10% of reviewers get bonuses. Zero-sum systems create perverse incentives to sabotage peers. If helping a colleague improve their accuracy means you lose your ranking, you won't help. Instead, use absolute thresholds where anyone who exceeds the quality bar gets rewarded. If twenty reviewers all hit 94% calibration accuracy and deliver high throughput, all twenty get bonuses. This creates a collaborative culture where high performers mentor newer reviewers instead of hoarding knowledge.

## Monitoring for Gaming in Real Time

Gaming doesn't announce itself. Reviewers who are gaming the system won't tell you. The metrics that are being gamed will look good — that's the point of gaming. You have to monitor second-order signals that reveal gaming before the primary metrics degrade.

The first signal is anomalous throughput. If a reviewer's throughput suddenly increases by 40% without a corresponding change in task mix or tooling, investigate. Either they found a legitimate efficiency or they're cutting corners. Spot-check their recent cases. Are they complete? Are they accurate? Are they consistent with how they labeled similar cases last month? A sudden productivity spike is a red flag until proven otherwise.

The second signal is convergence without calibration drift. If team-wide peer agreement increases from 88% to 94% over four weeks but calibration accuracy against gold cases stays flat or declines, the team is converging on the wrong answer. They're agreeing with each other more, but they're not getting closer to ground truth. This pattern indicates either groupthink or coordination. Re-baseline with fresh gold cases and retrain the team.

The third signal is bimodal case difficulty distribution. If some reviewers handle 90% easy cases and others handle 90% hard cases, either routing is broken or reviewers are gaming the queue. Check whether case assignment is random or self-selected. If it's self-selected, switch to managed routing. If it's already managed, audit the routing logic to see if it's systematically biased. Bimodal distributions are never accidental.

The fourth signal is low variance in review time within individual reviewers. Humans are variable. A reviewer handling twenty cases will naturally take 3 minutes on some, 5 minutes on others, and 7 minutes on a few hard ones. If a reviewer's last fifty cases all took between 2.8 and 3.2 minutes, they're not adapting to case complexity — they're spending a fixed amount of time regardless of difficulty. That suggests they're optimizing for a time target rather than for correctness. Variance within a reasonable range is healthy. No variance is suspicious.

## The Skip Rate Paradox

Skip rate is one of the most misunderstood metrics in review operations. A low skip rate looks efficient. A high skip rate looks like reviewers are avoiding work. Neither interpretation is correct without context. The right skip rate depends on the ambiguity of the task and the clarity of the guidelines. For a well-defined binary classification task with clear guidelines, a 2% skip rate is reasonable. For a subjective evaluation task with edge cases and unclear boundaries, a 15% skip rate might be healthy.

The paradox is that penalizing skip rate encourages guessing, which degrades quality silently. If a reviewer encounters a case they're 60% confident about and the guidelines say to skip anything below 80% confidence, they should skip. If you penalize skips, they'll label it anyway. Now you have a label with 60% confidence mixed into your dataset as if it's ground truth. The model trains on noisy data. The error propagates. A few months later, production quality is worse and no one knows why.

The solution is to separate skips into categories. A skip due to "unclear guidelines" is actionable — it means the guidelines need updating. A skip due to "insufficient context in the data" is also actionable — it means the upstream data pipeline is broken. A skip due to "edge case not covered by current policy" is valuable signal — it tells you where to extend the guidelines. Don't penalize these skips. Reward them. They're free research into where your system is underspecified.

The only skip type worth penalizing is "didn't want to think about this case" — a lazy skip. But you can't identify lazy skips from the skip rate alone. You have to read the skip justification. If a reviewer writes "not sure" for every skip, that's lazy. If they write "the model's output cites source A but the linked URL goes to source B, and the guidelines don't specify how to handle citation-target mismatches," that's a valuable skip. Require skip justifications. Review them weekly. Penalize vague skips. Reward specific, well-justified skips. Over time, reviewers learn that thoughtful skips are acceptable and lazy skips aren't.

## When High Agreement Masks Low Quality

Agreement is a consistency metric, not an accuracy metric. If ten reviewers all label the same case incorrectly in the same way, agreement is 100%. If they all label it correctly but with minor variations in how they describe the reasoning, agreement might be 70%. The metric tells you whether reviewers are aligned. It doesn't tell you whether they're right.

This is how teams end up with 92% agreement and 68% accuracy. The reviewers have converged on a shared interpretation of the guidelines. The interpretation is wrong, but it's consistent. Everyone applies the same incorrect logic. Agreement looks great. Production suffers. The only way to catch this is to measure accuracy against an external ground truth — not against peer reviewers, but against expert labels, user feedback, or verifiable facts.

A financial services company ran into this in late 2025. Reviewers were evaluating whether AI-generated contract summaries were accurate. Agreement was 89%. Three months after deploying a model trained on the reviewed data, legal discovered that 14% of the summaries omitted key clauses. The reviewers had all agreed that a summary was acceptable if it captured the main intent of the contract, even if it dropped minor clauses. The guidelines said "accurate summary" but didn't define what counted as minor versus major. All the reviewers made the same judgment call. They agreed with each other. They were all wrong.

The fix was adding a gold-standard dataset of expert-labeled contracts with clause-by-clause annotations. Reviewers were calibrated against the expert labels, not against each other. Agreement dropped to 76% initially because reviewers had to unlearn their shared misconception. Over six weeks, agreement climbed back to 88%, but now it was 88% agreement with the expert standard, not 88% agreement with each other. The distinction mattered.

## The Cost of Over-Measurement and Metric Fatigue

There's a point where measuring too many things becomes counterproductive. If reviewers are tracked on 25 different metrics — throughput, agreement, skip rate, calibration accuracy, median time, variance in time, case difficulty distribution, peer ranking, production impact, escalation rate, comment quality, guideline adherence, task-type-specific accuracy, shift performance, and on and on — they can't optimize for all of them simultaneously. They freeze. They don't know what to prioritize. They either ignore the metrics entirely or they game the ones that are easiest to game.

Metric fatigue is real. When dashboards become cluttered with scores, percentiles, trends, and rankings, reviewers stop looking at them. When performance reviews reference twelve metrics with equal weight, reviewers can't extract actionable feedback. The solution is radical simplification. Pick the three metrics that matter most for your operation. For most teams, that's calibration accuracy, throughput adjusted for task complexity, and production impact. Everything else is supporting detail.

Communicate the three core metrics clearly. Reviewers should know exactly how their performance is measured and exactly what they need to do to improve. "Your calibration accuracy is 86%; target is 90%. Focus on the edge cases in category X where you're missing nuance" is actionable. "Your composite quality score is in the 58th percentile across 14 dimensions" is meaningless. The reviewer has no idea what to fix.

Reduce reporting frequency for non-core metrics. If median review time and variance are informational but not tied to compensation, report them monthly, not daily. Reviewers don't need to stress about them every shift. If skip rate is a secondary metric, report it weekly with context, not hourly as a live number. The constant visibility of marginal metrics creates anxiety and distracts from the metrics that actually matter. Measure everything in the background for your own operational insight, but only surface the core metrics to the reviewers.

## Resetting Incentives When Gaming Is Widespread

If gaming is already endemic — if half your team is optimizing for the wrong thing and everyone knows it — you can't fix it by tweaking metrics. You have to reset the culture and the incentive structure simultaneously. Announce the reset explicitly. Explain what was being gamed, why it happened, and what's changing. Give reviewers a clean slate. Past gaming doesn't carry forward into the new system. The reset is a fresh start.

Introduce the new metrics and the new incentive structure with a grace period. For the first month, the new metrics are visible but not tied to compensation. Reviewers can see their scores, learn how the system works, and adjust their behavior without financial risk. After the grace period, the new system goes live. This prevents the whiplash of reviewers who were succeeding under the old system suddenly failing under the new one.

Involve reviewers in designing the new metrics. If reviewers feel like metrics are imposed from above without understanding the work, they'll resist. If they help define what good performance looks like, they'll buy in. Run workshops where reviewers and leads co-design the composite quality score. What should the weights be? What edge cases should be handled differently? What behaviors should be rewarded versus penalized? Reviewers have ground-level insights that managers don't. Use them.

After the reset, monitor closely for new gaming patterns. Any metric can be gamed. The question is whether the gaming is obvious and addressable or subtle and damaging. If reviewers find a way to game the new system that improves actual quality as a side effect, that's not gaming — that's alignment. If they find a way to game it that degrades quality, close the loophole fast. Metrics are not set in stone. They're hypotheses about what drives good outcomes. Update them as you learn.

The next chapter covers scaling challenges for review operations — how to grow from ten reviewers to two hundred, how to maintain quality as volume explodes, and how to build infrastructure that scales without collapsing under its own complexity.

