# 8.2 — Majority Vote vs Expert Adjudication

When three reviewers evaluate the same output and two say pass while one says fail, you need a tiebreaker. The two most common approaches are majority vote and expert adjudication. Majority vote takes the label supported by more reviewers. Expert adjudication routes the case to a senior reviewer who makes the final call. Both methods work. Neither works in all situations. Choosing the wrong method for a given task type creates either unacceptable cost or unacceptable error rates. The decision depends on task complexity, stakes, reviewer skill distribution, and operational constraints.

## When Majority Vote Works

Majority vote is fast, cheap, and requires no additional infrastructure. If you collect three independent reviews per output, the majority label becomes the ground truth. No escalation. No bottleneck. No senior reviewer spending half their day adjudicating disagreements. For tasks where most cases are clear and disagreements are rare, majority vote keeps the pipeline moving without sacrificing quality.

Majority vote works best when the task is well-defined, the rubric is precise, and reviewers are well-calibrated. A factual accuracy check on structured outputs. A toxicity filter with clear examples of what crosses the line. A formatting validation where the rules are unambiguous. In these cases, disagreement is usually the result of one reviewer making a mistake or misreading the output. The other two reviewers are correct. The majority vote surfaces the correct label without requiring expert intervention.

Majority vote also works when the cost of an individual error is low and you care more about aggregate accuracy than perfect precision on every case. A content recommendation system evaluating whether articles are engaging. A search relevance task judging whether results match user intent. A summarization quality check on news articles. If one output gets mislabeled, it is not a crisis. The system still performs well if 95 percent of labels are correct. Majority vote gets you to 95 percent faster and cheaper than expert adjudication.

The key advantage of majority vote is scalability. You can evaluate 10,000 outputs per week with a pool of 20 reviewers and no bottleneck on senior staff. As volume grows, you add reviewers. You do not need to scale a scarce senior resource. For organizations running continuous evaluation at high throughput, this operational simplicity is decisive.

## When Majority Vote Fails

Majority vote breaks down when disagreements are frequent, when stakes are high, or when the majority is systematically wrong. If 30 percent of cases produce split decisions, majority vote means 30 percent of your labels are decided by tiebreaker logic rather than careful judgment. That is acceptable if the task is low-stakes. It is not acceptable if the task is high-stakes.

A healthcare company used majority vote to evaluate whether AI-generated clinical summaries accurately reflected patient charts. Three nurses reviewed each summary. When disagreement occurred — about 22 percent of cases — the majority vote determined the label. This worked until a post-hoc audit revealed that in 18 percent of disagreement cases, the minority reviewer had been correct. The two majority reviewers had both missed the same subtle factual error or both misinterpreted the same ambiguous phrasing. Majority vote was not surfacing truth — it was amplifying the most common error. The company switched to expert adjudication for all disagreement cases and error rates dropped by half.

Majority vote also fails when the reviewer pool is heterogeneous in skill. If one reviewer is significantly more experienced than the others, their judgment is often more reliable. Majority vote ignores this. A senior reviewer with five years of experience and two junior reviewers with three months of experience all get equal weight. If the senior reviewer disagrees with the juniors, majority vote sides with the juniors. That is not always wrong — sometimes the senior reviewer is overfitting to rare cases and the juniors are correctly applying the rubric to a typical case. But in high-stakes tasks, you want the option to defer to the more experienced judgment.

Finally, majority vote fails on genuinely ambiguous cases where no clear answer exists. If the output sits on a boundary and reasonable people can disagree, majority vote picks a winner arbitrarily. Two reviewers say the tone is acceptable, one says it is too informal. Majority vote labels it acceptable. But the disagreement was meaningful — the tone was borderline and context-dependent. Majority vote discarded that nuance. For tasks where ambiguity is common and consequential, expert adjudication preserves the nuance and allows for more context-aware decisions.

## When Expert Adjudication Is Necessary

Expert adjudication routes disagreement cases to a senior reviewer who has more experience, more training, or more domain expertise than the primary reviewer pool. The adjudicator re-reviews the output, considers the reasoning from the original reviewers if available, and makes a final decision. This decision becomes ground truth. Expert adjudication is slower and more expensive than majority vote, but it produces higher-quality labels in high-stakes or high-ambiguity scenarios.

Expert adjudication is necessary when errors are costly. A content moderation system deciding whether to remove content that could incite violence. A financial compliance system evaluating whether a transaction summary meets regulatory disclosure standards. A medical coding system determining whether an AI-generated diagnosis code is appropriate for billing. In these cases, a wrong label can result in legal liability, financial loss, or patient harm. Majority vote is not sufficient. You need a human with deep expertise making the final call.

Expert adjudication is also necessary when the task requires domain knowledge that most reviewers do not have. A legal document review where one reviewer is a licensed attorney and the others are paralegals. A radiology report evaluation where one reviewer is a board-certified radiologist and the others are medical scribes. The domain expert sees patterns, catches errors, and understands context that generalist reviewers miss. When disagreement occurs, the domain expert's judgment is more reliable than a majority vote among generalists.

A financial services company evaluating AI-generated investment summaries routed all disagreement cases to a senior financial analyst. The primary review pool consisted of junior analysts and contractors with finance backgrounds but less experience. When disagreement occurred — typically on cases involving complex instruments or ambiguous regulatory language — the senior analyst re-reviewed the output and made the final determination. This adjudication layer caught 14 percent of cases where majority vote would have labeled an incorrect summary as correct. The cost was one senior analyst spending six hours per week on adjudication. The benefit was near-zero risk of publishing a financially misleading summary.

## Hybrid Approaches: Routing Rules and Escalation Criteria

Most mature systems do not choose between majority vote and expert adjudication globally. They use routing rules. Simple cases get majority vote. Complex or high-stakes cases get expert adjudication. The challenge is defining the boundary.

One common pattern is to use majority vote by default and escalate to expert adjudication only when certain conditions are met. For example, escalate if the disagreement involves a critical criterion. Escalate if the output is flagged as high-risk by the model. Escalate if one of the reviewers explicitly marks the case as requiring senior review. Escalate if the disagreement is a two-one split on a high-stakes label. These rules automate the decision of when to invoke the more expensive but more reliable process.

A content moderation platform used a two-tier system. Standard content — posts that were clearly safe or clearly violating — went through three-reviewer majority vote. Borderline content — posts that triggered certain keywords, involved sensitive topics, or generated reviewer disagreement — went to expert adjudication by a trust and safety specialist. The routing rules were automated. If a post mentioned self-harm, violence, or political figures and produced any reviewer disagreement, it escalated. Otherwise, majority vote applied. This kept 78 percent of cases on the fast path while ensuring the 22 percent of high-stakes cases received expert attention.

Another hybrid approach is confidence-weighted majority vote. Instead of treating all reviewers equally, you weight votes by reviewer track record. A reviewer with 95 percent historical agreement with adjudicators gets more weight than a reviewer with 80 percent historical agreement. This approach sits between pure majority vote and full expert adjudication. It is cheaper than adjudication but more accurate than unweighted majority vote. Implementation requires tracking per-reviewer accuracy over time, which adds operational complexity but can be automated with the right tooling.

## The Cost-Quality Tradeoff

The fundamental tension is cost versus quality. Majority vote is cheap and scales easily but produces lower-quality labels on ambiguous or high-stakes cases. Expert adjudication produces higher-quality labels but is expensive and does not scale linearly. The right choice depends on how much quality you need and how much you are willing to pay for it.

For a task where 90 percent accuracy is sufficient, majority vote is appropriate. For a task where 99 percent accuracy is required, expert adjudication is worth the cost. The mistake is treating all tasks the same. Most systems have a mix of low-stakes and high-stakes evaluation needs. Apply majority vote to the low-stakes work and reserve expert adjudication for the cases that matter most.

A legal tech company running contract clause extraction calculated the cost and quality difference between the two approaches. Majority vote cost 12 dollars per 100 reviewed outputs and produced 91 percent accuracy on ground truth. Expert adjudication cost 47 dollars per 100 reviewed outputs and produced 97.5 percent accuracy. For low-value contracts — NDAs, standard vendor agreements — they used majority vote. For high-value contracts — M&A agreements, IP licenses — they used expert adjudication. The blended cost was 21 dollars per 100 outputs and accuracy on high-value contracts was 97.5 percent where it mattered. If they had used majority vote everywhere, they would have saved 9 dollars per 100 outputs but missed critical errors in high-stakes documents. If they had used expert adjudication everywhere, they would have spent 26 dollars per 100 outputs more for marginal quality gains on low-stakes work. The hybrid approach optimized for value.

## Operationalizing the Decision

Choosing between majority vote and expert adjudication is not a one-time decision. It evolves with the system. Early in a project, when rubrics are immature and reviewer calibration is low, expert adjudication is necessary for most disagreements. As rubrics stabilize and reviewers gain experience, more cases can shift to majority vote. As stakes increase or new failure modes emerge, some cases shift back to expert adjudication.

The decision should be explicit and documented. Define which task types use which method. Define escalation criteria. Make the rules visible to reviewers so they understand when their judgment will be final and when it will be reviewed by an adjudicator. This transparency reduces frustration and helps reviewers understand the value of their role in the system.

Track the outcomes of both methods over time. Measure how often majority vote labels are later overturned by post-hoc audits. Measure how often expert adjudication changes the label that majority vote would have chosen. If majority vote is almost always correct, you are over-investing in adjudication. If expert adjudication frequently reverses majority vote, the primary reviewer pool needs better training or the task needs stricter escalation rules.

The next subchapter covers what happens when disagreement cannot be resolved at the reviewer level and must be escalated further.

