# Chapter 10 — Metrics and Analytics for Review Operations

What gets measured gets managed. But in human review operations, most teams measure the wrong things—vanity metrics that look impressive in slide decks but fail to predict operational health, quality outcomes, or cost efficiency. The right metrics create visibility into every layer of your review operation: how fast items move through queues, how accurate decisions are, how much each review costs, and where problems are emerging before they become crises. This chapter covers the metrics that actually matter for human review, from throughput and quality to cost and performance—and how to avoid the perverse incentives that corrupt measurement systems over time.

---

- 10.1 — The Metrics That Actually Matter for Human Review
- 10.2 — Throughput Metrics: Volume, Velocity, and Capacity
- 10.3 — Quality Metrics: Accuracy, Consistency, and Agreement
- 10.4 — Latency Metrics: Time-to-Review and SLA Compliance
- 10.5 — Reviewer Performance Metrics: Individual and Team
- 10.6 — Cost Metrics: Per-Item and Per-Decision Costs
- 10.7 — Trend Analysis and Early Warning Systems
- 10.8 — Cohort Analysis for Reviewer Performance
- 10.9 — Reporting for Stakeholders: Executive vs Operational Views
- 10.10 — Metrics Gaming and Perverse Incentives

---

*The dashboard that shows you everything is showing you nothing. The dashboard that shows you the three numbers that predict tomorrow's problems is worth ten engineers.*

