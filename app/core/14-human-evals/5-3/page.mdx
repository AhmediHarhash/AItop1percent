# 5.3 — Tiered Review: First-Pass, Expert, and Escalation

Tiered review is the foundational structure for scaling human judgment past 15 reviewers. The concept is simple: not every case requires the same level of expertise, so you route cases to the appropriate tier based on complexity, risk, and confidence. Simple cases go to first-pass reviewers. Complex cases go to experts. Ambiguous cases escalate through a defined path. When designed correctly, tiered review prevents expert bottlenecks, reduces cost per case, and maintains quality as volume grows. When designed incorrectly, it creates latency, inconsistency, and a queue of escalated cases that no one can clear.

The structure typically has three tiers. **First-pass reviewers** handle the 60 to 80 percent of cases that follow clear guidelines and require minimal judgment. **Senior reviewers** handle the 15 to 30 percent of cases that require deeper context, domain knowledge, or interpretation of ambiguous guidelines. **Experts** handle the 5 to 10 percent of cases that require specialized knowledge, high-stakes judgment, or decisions that set precedent for future cases. The exact percentages depend on your domain, but the principle is universal: the majority of cases should be resolved at the first tier, and only the cases that truly need escalation should reach experts.

The failure mode of tiered review is designing tiers based on seniority instead of task complexity. If your first-pass tier handles "easy" cases and your expert tier handles "hard" cases, but you haven't defined what makes a case easy or hard, every reviewer interprets those boundaries differently. Some first-pass reviewers escalate anything they're unsure about, which floods the expert queue. Some first-pass reviewers make judgment calls on complex cases because they don't want to slow down, which introduces error. The tiers become meaningless.

The fix is to define tier boundaries based on observable case characteristics, not on subjective assessments of difficulty. A case goes to first-pass if it meets criteria A, B, and C. A case goes to senior review if it meets criteria D or E. A case goes to expert review if it meets criteria F. The criteria must be specific enough that a model or a routing system can apply them automatically, with human override only when the routing is clearly wrong. If your tier boundaries require human judgment to apply, your tiers are poorly defined.

## Designing Tier Boundaries

The first step in designing tiered review is to analyze your case distribution. Pull a random sample of 500 to 1,000 cases and classify them by complexity. What percentage of cases can be resolved by applying a guideline with no interpretation required? What percentage require some judgment but no specialized knowledge? What percentage require deep domain expertise or high-stakes decision-making? This gives you your natural tier distribution.

If 75 percent of cases are straightforward, your first-pass tier should be designed to handle 75 percent of volume. If 5 percent of cases require deep expertise, your expert tier should handle 5 percent of volume. The tier sizes should match the case distribution, not the other way around. If you design a system where 50 percent of cases escalate to experts, you've designed a bottleneck.

The second step is to define routing rules. For each tier, specify the case characteristics that belong there. First-pass cases might be defined as: model confidence above 0.90, no policy flags, output length under 500 tokens, no sensitive entities detected. Senior review cases might be defined as: model confidence between 0.75 and 0.90, or policy flag present, or output contains medical advice. Expert review cases might be defined as: legal content, financial advice, model confidence below 0.75, or precedent-setting edge case. These rules are not perfect, but they're specific enough to route 90 percent of cases correctly without human intervention.

The third step is to define escalation triggers. Even within a tier, some cases will be ambiguous. First-pass reviewers need a clear list of conditions that trigger escalation: guideline ambiguity, conflicting policy requirements, case not covered by training, or personal discomfort with the decision. Escalation triggers should be narrow enough that they don't flood the next tier, but broad enough that reviewers feel safe escalating when they're unsure. A first-pass reviewer who's afraid to escalate will make bad calls. A first-pass reviewer who escalates constantly will clog the system. The escalation triggers set the boundaries.

The fourth step is to set escalation quotas. For a healthy first-pass tier, 5 to 10 percent of cases should escalate. If escalation rate is below 5 percent, reviewers might be making judgment calls they shouldn't. If escalation rate is above 10 percent, the tier boundaries are poorly defined or the training is insufficient. Track escalation rate per reviewer and per case type. High escalation rates in specific areas tell you where guidelines need clarification or where routing rules need adjustment.

## Preventing Expert Bottlenecks

The most common failure in tiered review is the expert bottleneck. Experts can handle complex cases, but they can't handle volume. If your expert tier receives 15 percent of cases instead of 5 percent, the expert queue grows faster than the experts can clear it, and the entire system slows down. First-pass reviewers wait hours or days for escalation responses, which creates latency penalties and frustrates the team. The fix is to reduce the inflow to the expert tier, not to hire more experts.

Reducing expert inflow has three approaches. The first approach is to **improve first-pass training** so that fewer cases require escalation. If 8 percent of first-pass escalations are for questions that could be answered by existing guidelines, better training eliminates those escalations. Run calibration sessions, build a knowledge base of past escalation decisions, and create case study libraries that show reviewers how to handle edge cases. Every percentage point reduction in escalation rate frees expert capacity.

The second approach is to **create a senior review tier** that handles cases that are too complex for first-pass but don't require true expertise. Senior reviewers are experienced first-pass reviewers who have been trained on a broader set of cases and given authority to make judgment calls within defined boundaries. They handle the 15 to 25 percent of cases that are ambiguous but not high-stakes. This prevents those cases from reaching experts and keeps expert capacity focused on the 5 percent of cases that no one else can resolve.

The third approach is to **codify expert decisions into guidelines**. Every time an expert makes a judgment call, ask: is this a one-time decision, or is it a precedent that applies to future cases? If it's a precedent, document it, add it to the guidelines, and train reviewers on it. Over time, decisions that used to require expert judgment become routine first-pass decisions. The expert's knowledge scales through documentation, not through their personal capacity.

A well-designed tiered system has experts who are calm, available, and focused on genuinely hard problems — not drowning in a queue of cases that should have been resolved two tiers earlier. If your experts are overwhelmed, the problem is not the experts. The problem is that too many cases are reaching them.

## The Latency Cost of Multi-Tier Review

Tiered review introduces latency. A case that gets routed to first-pass, then escalated to senior review, then escalated again to expert review might take three days to resolve, even if the total review time is only 30 minutes. The time spent waiting in queues and bouncing between tiers dominates the total cycle time. For some use cases, this latency is acceptable. For others, it's a deal-breaker.

The latency cost is highest when tier boundaries are poorly defined and when escalation paths are unclear. A first-pass reviewer who's unsure whether a case should escalate might sit on it for hours, hoping to figure it out, before finally escalating at the end of the day. A senior reviewer who receives an escalation without context might send it back to first-pass for more information, which adds another round trip. A case that should have taken 10 minutes ends up taking two days because it bounced between tiers three times.

The fix is to **make escalation fast and low-friction**. If a first-pass reviewer encounters an escalation trigger, they should be able to escalate immediately with one click, attaching context and routing the case to the right tier. The escalated case should appear at the top of the senior review queue, not at the bottom. The senior reviewer should have all the context they need to make a decision without asking follow-up questions. And if the case needs to escalate again to expert review, that escalation should happen immediately, not after the senior reviewer spends a day trying to figure it out.

The second fix is to **use synchronous escalation for high-priority cases**. If a case is time-sensitive, the first-pass reviewer should be able to ping a senior reviewer or expert in real time, describe the case in Slack or a dedicated escalation channel, and get an answer in minutes. Synchronous escalation introduces coordination overhead, but for high-priority cases, the latency reduction is worth it. Reserve synchronous escalation for cases where speed matters more than process efficiency.

The third fix is to **measure and track escalation latency**. How long does a case spend in each queue? How long does it take for an escalated case to get picked up by the next tier? If escalation latency exceeds 4 hours for routine cases or 30 minutes for high-priority cases, you have a queue management problem. The fix might be staffing, might be tooling, might be process — but you can't fix it if you're not measuring it.

## When to Flatten Tiers

Tiered review adds complexity. For small teams or low-volume operations, the complexity cost exceeds the benefit. If you have 8 reviewers handling 2,000 cases per week, you probably don't need tiers — everyone can handle everything, and the coordination overhead of tier management outweighs the efficiency gain. Tiered review makes sense when you have enough volume and enough specialization that routing cases to the right person creates measurable value.

The breakpoint is usually around 15 to 20 reviewers or 5,000 cases per week. Below that threshold, flat review with peer escalation works fine. Above that threshold, you need structure. But even above the threshold, not every domain benefits from tiers. If your case complexity is uniform — every case requires roughly the same level of judgment — tiers add latency without reducing cost. Tiers work best when case complexity is highly variable, with a long tail of easy cases and a small number of hard cases.

The signal that you need tiers is when your **expert reviewers spend most of their time on cases that don't need expertise**. If your most experienced reviewer is spending 60 percent of their time on routine cases, you're wasting expert capacity. Tiers fix that by routing routine cases to first-pass reviewers and reserving expert time for cases that justify the cost. The signal that tiers aren't working is when **escalation latency exceeds the time it would have taken to just assign all cases to experts**. At that point, the coordination overhead of tiering is higher than the value it creates, and you should flatten.

The decision to introduce tiers or flatten them is not permanent. As your operation scales, tiers become necessary. As your automation improves and eliminates routine cases, tiers might become unnecessary because everything left in the queue requires expertise. The structure should match the work, not the other way around.

## Promotion Between Tiers

A well-designed tiered system has clear promotion criteria. Reviewers start in first-pass, and after demonstrating competence, they're promoted to senior review. Senior reviewers who develop deep expertise in a domain are promoted to expert review. Promotion creates career progression, incentivizes quality, and ensures that your expert tier is staffed by people who have earned the role through demonstrated skill, not through tenure or hiring level.

Promotion criteria should be objective and measurable. A first-pass reviewer is eligible for promotion to senior review after completing 1,000 cases, maintaining an error rate below 3 percent, and passing a calibration assessment. A senior reviewer is eligible for promotion to expert after completing 500 senior cases, handling 5 precedent-setting cases with sign-off from existing experts, and demonstrating mastery of domain-specific knowledge. The criteria should be public, tracked, and applied consistently.

The failure mode is promoting too quickly. A first-pass reviewer who gets promoted after three months might not have seen enough edge cases to handle senior review confidently. A senior reviewer promoted to expert without sufficient domain depth will make inconsistent decisions that erode trust. Promotion should be earned, not automatic. The bar for expert review should be high enough that reaching it feels like an achievement.

The second failure mode is never promoting anyone. If your first-pass reviewers have no path to senior review, your best people will leave for roles that offer growth. If your senior reviewers have no path to expert, they'll stagnate. Career progression is part of retention strategy. A tiered system with clear promotion paths keeps your best reviewers engaged and ensures that expertise develops internally instead of through external hiring.

Tiered review is not a bureaucratic layer — it's a scaling structure that matches reviewer skill to case complexity. When designed well, it prevents bottlenecks, reduces cost, and maintains quality as volume grows. When designed poorly, it creates latency, frustration, and inconsistency. The difference is in the details: clear tier boundaries, fast escalation paths, measurable promotion criteria, and continuous tuning to match the work.

Next, we'll cover sampling strategies for when full review becomes impossible, and how to maintain statistical validity when you can only review a fraction of your output.

