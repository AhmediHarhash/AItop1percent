# 5.5 — Automation-Assisted Review: Letting Models Pre-Filter

The review queue opens at 8 AM. Of the 2,000 cases that came in overnight, 1,400 are straightforward — high confidence, low risk, clear policy compliance. A human reviewer will look at each one, spend 30 seconds confirming what the model already knows, and click approve. That's 700 minutes of human time spent rubber-stamping decisions that don't need human judgment. By noon, the reviewers are bored, fatigued, and making errors on the 600 cases that actually matter. This is the failure mode of manual review at scale: humans spend most of their time on cases where they add no value, and by the time they reach cases where judgment matters, they're too tired to apply it.

Automation-assisted review flips this. A model pre-filters the queue: low-risk, high-confidence cases are auto-approved and never reach a human. High-risk or low-confidence cases are auto-escalated to the appropriate review tier. Ambiguous cases — where the model is uncertain or where policy requires human judgment — land in the human review queue. The result: humans spend 100 percent of their time on cases where their judgment matters, and zero percent on cases where they're just confirming what the model already got right.

The teams that scale human review to 100,000+ outputs per week all use some form of automation-assisted review. The teams that try to scale by hiring more reviewers hit a wall, run out of budget, or burn out their team. Automation is not a replacement for human review — it's a force multiplier that lets humans focus on the 10 to 30 percent of cases where they're essential.

## Auto-Approve Thresholds

Auto-approval is the highest-leverage automation. If a case meets specific safety criteria — confidence above a threshold, no policy flags, output characteristics within bounds — it's approved automatically and removed from the review queue. The human reviewers never see it. Auto-approval reduces queue size, speeds up time-to-user, and eliminates the cognitive load of reviewing cases that are obviously correct.

The challenge is setting the auto-approve threshold. Too conservative, and you auto-approve 10 percent of cases, which barely reduces the queue. Too aggressive, and you auto-approve cases with errors, which erodes trust and creates downstream harm. The right threshold balances error tolerance against review cost. For most systems, auto-approving 40 to 60 percent of cases is feasible without meaningful quality loss, but the exact number depends on your model's calibration, your risk tolerance, and your domain.

The process for setting auto-approve thresholds is empirical. Start by pulling a large sample of outputs — 5,000 to 10,000 cases — that have been human-reviewed. For each case, record the model's confidence score, risk indicators, and policy flags. Then analyze: what percentage of cases with confidence above 0.95 were approved by humans? What percentage of cases with no policy flags were approved? What percentage of cases under 500 tokens were approved? Build a decision tree or logistic regression that predicts human approval based on these features, and use that model to set your auto-approve criteria.

A typical auto-approve rule looks like this: confidence above 0.93, no policy flags, output length under 600 tokens, no sensitive entities, no factual claims requiring verification, and task type in the approved list. A case that meets all criteria is auto-approved. A case that fails any criterion goes to human review. Tune the thresholds by measuring **auto-approve precision** — the percentage of auto-approved cases that would have been approved by a human if they'd reviewed it. Aim for 98 to 99 percent precision. If precision drops below 98 percent, tighten the thresholds.

The second metric is **auto-approve recall** — the percentage of human-approved cases that your auto-approve rule would have caught. Recall measures how much of the queue you're eliminating. A rule with 99 percent precision and 50 percent recall auto-approves half your queue with minimal error. A rule with 99 percent precision and 30 percent recall auto-approves less, but might be appropriate for higher-risk domains. The trade-off is coverage versus safety.

## Auto-Escalate Criteria

Auto-escalation routes high-risk or complex cases directly to the appropriate review tier, bypassing first-pass review. If a case mentions regulated content, contains low-confidence outputs, or triggers a policy flag, it's automatically escalated to senior or expert review. This prevents high-risk cases from sitting in the first-pass queue, where they might be mishandled, and ensures that they reach reviewers with the expertise to evaluate them correctly.

Auto-escalation criteria are typically rule-based, not confidence-based. A case escalates if it contains keywords like "lawsuit," "overdose," "financial advice," or "prescription." It escalates if it mentions entities on a watchlist. It escalates if the model flags it for potential policy violation. It escalates if confidence is below 0.70. These rules are domain-specific, but the principle is universal: if a case carries meaningful risk, it should reach an expert without delay.

The failure mode of auto-escalation is over-escalation. If your rules are too broad, you route 40 percent of cases to expert review, which creates a bottleneck. If your rules are too narrow, high-risk cases slip through to first-pass review, where they're handled incorrectly. The fix is to **tune escalation rules against historical review data**. Pull cases that were escalated by humans in the past, identify the common features, and build rules that capture 90 percent of those cases. Then monitor false-positive escalations — cases that were auto-escalated but didn't actually need expert review — and refine the rules to reduce noise.

Auto-escalation pairs well with tiered review. Cases that meet auto-approve criteria skip review entirely. Cases that meet auto-escalate criteria go directly to expert review. Everything else lands in first-pass review, where it's handled by the majority of your team. The result is a three-way split: auto-approve handles 40 to 60 percent, first-pass handles 30 to 50 percent, expert review handles 5 to 15 percent. Your human review capacity is now focused on the 35 to 65 percent of cases where judgment matters.

## Human-in-the-Loop for Uncertain Cases

The middle ground between auto-approve and manual review is human-in-the-loop for uncertain cases. The model makes a tentative decision, surfaces it to a human with context, and asks for confirmation. The human doesn't review from scratch — they're shown the model's recommendation, the confidence score, and the key factors that influenced the decision. The human's job is to spot-check: does this decision look right? If yes, approve. If no, override and explain why.

Human-in-the-loop reduces review time per case from 2 to 3 minutes to 15 to 30 seconds, because the human is confirming rather than deciding. It also improves model calibration over time, because human overrides become training signal for refining the model. The trade-off is that humans might anchor on the model's recommendation and miss errors they would have caught with a clean-slate review. This anchoring effect is real but manageable — it's smaller than the fatigue effect from reviewing 2,000 cases manually.

The right use case for human-in-the-loop is moderate-risk, moderate-confidence outputs. High-risk outputs should go to full manual review. Low-risk outputs should be auto-approved. The middle tier — cases where the model is 80 to 90 percent confident, or cases with moderate policy implications — benefits from fast human confirmation. The human catches the errors the model missed, but doesn't waste time on cases the model got right.

A practical implementation: the model scores every output on a 0 to 1 risk scale and a 0 to 1 confidence scale. Outputs with risk below 0.3 and confidence above 0.93 are auto-approved. Outputs with risk above 0.7 or confidence below 0.75 go to full manual review. Outputs in the middle range are shown to humans in a streamlined interface: model recommendation, key features, one-click approve or override. The human reviews 300 cases per hour in this mode instead of 30 cases per hour in full manual review. The error rate is only marginally higher, and the throughput gain is 10x.

## Calibrating Automation Thresholds

The hardest part of automation-assisted review is not building the automation — it's calibrating the thresholds so that the automation actually helps instead of creating new problems. Thresholds that are too tight auto-approve nothing and waste the investment. Thresholds that are too loose auto-approve errors and erode trust. The right thresholds are domain-specific, model-specific, and drift over time as your model and your data change.

The calibration process has four steps. First, **measure baseline performance**. Pull 2,000 cases that were human-reviewed, apply your proposed auto-approve and auto-escalate rules, and measure precision and recall. What percentage of auto-approved cases were actually correct? What percentage of auto-escalated cases actually needed escalation? What percentage of the queue did the automation handle?

Second, **test on a shadow deployment**. Run the automation in parallel with human review for two weeks. The humans still review everything, but you log what the automation would have done. Compare automation decisions to human decisions. Measure agreement rate, false positives, false negatives, and edge cases where the automation failed. Use this data to refine thresholds before going live.

Third, **launch with conservative thresholds**. When you first deploy automation, set thresholds that prioritize precision over recall. It's better to auto-approve 30 percent of cases with 99 percent precision than to auto-approve 60 percent of cases with 95 percent precision. Once you've demonstrated that the automation works, you can relax thresholds and increase coverage.

Fourth, **monitor and retune continuously**. Model performance drifts, user behavior shifts, and policy requirements change. What worked in January might fail in June. Track auto-approve precision, auto-escalate precision, and false-negative rate weekly. When precision drops below your threshold, tighten the rules. When you see consistent patterns of missed errors, add new escalation criteria. Calibration is not one-time — it's an ongoing process.

## The Silent Error Problem

The biggest risk in automation-assisted review is the silent error — the case that gets auto-approved incorrectly, and no human ever sees it. If 50 percent of your cases are auto-approved and 2 percent of those contain errors, you're shipping 1 percent of your total output with unreviewed errors. Those errors reach users, cause harm, and you don't know about it until someone complains.

The fix is **continuous audit sampling**. Even for auto-approved cases, review a random sample weekly. Pull 200 to 400 auto-approved cases, review them manually, and measure error rate. If the error rate is within tolerance, your automation is working. If the error rate exceeds tolerance, investigate: is the model drifting? Are users finding new adversarial inputs? Has policy changed? Use the audit sample to detect problems before they become crises.

The second fix is **user feedback loops**. Instrument your system so that users can flag outputs as incorrect, harmful, or inappropriate. Track feedback rate for auto-approved cases versus manually-reviewed cases. If feedback rate for auto-approved cases is higher, your auto-approve thresholds are too loose. User feedback is noisy — many users never report errors — but it's a leading indicator of problems your internal review might miss.

The third fix is **confidence calibration checks**. Periodically verify that your model's confidence scores still correlate with correctness. If the model says 95 percent confidence, are 95 percent of those outputs actually correct? If calibration drifts — which happens when data shifts or when the model is updated — your auto-approve thresholds become invalid. Recalibrate quarterly or after any model change.

## When Automation Goes Wrong

Automation-assisted review fails in three ways. The first failure mode is **over-automation** — auto-approving so much that humans lose oversight and errors accumulate silently. The second failure mode is **under-automation** — being so conservative that automation eliminates only 10 percent of the queue, which doesn't meaningfully reduce review burden. The third failure mode is **automation drift** — thresholds that worked at launch stop working six months later because the model, the data, or the policy changed, and no one noticed.

The fix for over-automation is audit sampling and user feedback, as described above. The fix for under-automation is iterative threshold relaxation — start conservative, measure precision, and gradually increase coverage as confidence grows. The fix for automation drift is continuous monitoring and retuning. None of these fixes are optional. If you deploy automation and walk away, it will eventually fail.

The right mental model for automation-assisted review is that the model is a junior reviewer who handles routine cases under supervision. The model is fast, consistent, and cheap, but it makes mistakes. Your job is to design the supervision system — the audit sampling, the feedback loops, the escalation criteria — so that the model's mistakes are caught before they cause harm. You're not replacing human judgment. You're allocating human judgment to the cases where it matters most.

Automation-assisted review is the only way to scale human oversight past 10,000 outputs per week without losing quality or bankrupting your review budget. Auto-approve handles the easy cases. Auto-escalate routes the hard cases to experts. Human-in-the-loop confirms the uncertain cases. And continuous audit sampling ensures that automation errors don't accumulate silently. The result: a review system that's faster, cheaper, and more reliable than pure manual review — but only if you design, calibrate, and monitor it correctly.

This concludes Chapter 5. We've covered the scaling wall, the trade-offs between horizontal and vertical scaling, the design of tiered review structures, the statistical foundations of sampling, and the automation strategies that let humans focus on judgment instead of rubber-stamping. The next chapter will cover quality management and preventing reviewer drift at scale.

