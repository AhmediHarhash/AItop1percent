# 4.12 — Migration Plans for Old Labels When Standards Change

The team is four weeks into deploying a new toxicity guideline. The guideline is working. Agreement is up, escalations are down, reviewers are confident. Then someone in Product asks the obvious question: what about the 200,000 labels we created under the old guideline? Are they still valid? Do we need to relabel them? Are we training models on data that is now considered wrong? The room goes quiet. Nobody planned for this. The guideline change was treated as a forward-looking fix, not a backward-looking migration. Now the team is facing a quarter-million-dollar relabeling effort, or the risk of training models on deprecated standards, or the complexity of maintaining two parallel label datasets that define toxicity differently.

When a guideline changes, historical labels do not automatically update. They remain frozen in time, tagged with the guideline version that governed them. This creates a decision point: what do you do with labels that were correct under the old standard but may no longer align with the new one? The answer depends on how the labels are used, how severe the guideline change was, and how much inconsistency you can tolerate in your dataset. There is no universal rule. There is only a structured decision process.

## What Happens to Labels Made Under Old Guidelines

Labels do not expire. A label created in January under guideline version 1.4 remains valid as a record of what a reviewer judged at that time under that standard. The label is a historical artifact. The question is not whether the label is valid in an absolute sense. The question is whether the label is still useful for the purpose it is being used for now.

If the labels are being used to train a production model, and the model is expected to align with the current guideline, then labels made under a materially different guideline are suspect. They teach the model a different standard than the one you are now enforcing. If the guideline change was minor — a clarification, a reformatting, a new example that does not change the underlying rule — the labels remain usable. If the guideline change was breaking — a redefined term, a new threshold, a reversed rule — the labels teach the wrong behavior.

If the labels are being used for trend analysis, historical labels are valuable even if they do not match the current guideline. You want to know what reviewers judged as toxic in January, under the January guideline, so you can compare it to what reviewers judge as toxic in June under the June guideline. The inconsistency is the data. Relabeling historical data to match the current guideline erases the ability to measure how your standards have evolved.

If the labels are being used for compliance or audit, the guideline version matters. If a regulator asks how you were moderating content in Q1, you point to the labels created in Q1 under the Q1 guideline. If you have relabeled that data under a later guideline, you no longer have an accurate record of what your moderation standard was at that time. The relabeled data reflects what you think now, not what you did then.

The first step in the migration decision is identifying every downstream use of the labels. Who consumes them? What do they use them for? Will those consumers be affected by the guideline change? This is not an academic question. It is a concrete inventory. You list the model training pipelines, the reporting dashboards, the compliance systems, the A/B test frameworks, the external data delivery contracts. Each one is evaluated separately.

## Relabeling Strategies

If you decide that historical labels must be updated to align with the new guideline, you have four relabeling strategies. The strategies differ in cost, thoroughness, and disruption.

**Full relabeling** means re-reviewing every historical label under the new guideline. You send the entire corpus back through the review pipeline. Reviewers see each example again, apply the new guideline, and generate a new label. The original label is archived with its version tag. The new label replaces it in active datasets. Full relabeling is the most thorough option. It also costs the most. If you have 500,000 historical labels and each review costs $0.50, full relabeling costs $250,000. Full relabeling is justified when the guideline change is fundamental, the historical labels are used in high-stakes systems, and inconsistency is unacceptable.

**Targeted relabeling** means re-reviewing only the subset of labels likely to be affected by the guideline change. If the guideline change redefined toxicity but did not change how you evaluate factual accuracy, you relabel only the toxicity labels. If the guideline change affected only a specific edge case — say, how you handle sarcasm — you filter for examples likely to contain sarcasm and relabel only those. Targeted relabeling requires good metadata. You must be able to identify which labels are in scope. If your labels are not tagged with enough detail to filter effectively, targeted relabeling is not an option.

**Sampling and extrapolation** means relabeling a representative sample of historical labels, measuring the rate of change, and extrapolating the impact. You randomly sample 5,000 labels, relabel them under the new guideline, and calculate that 18 percent of labels changed. You then tag the entire historical corpus as "18 percent suspect under new guideline" and decide whether that level of noise is acceptable for your use case. Sampling is faster and cheaper than full relabeling, but it does not give you clean data. You know the data is noisy. You just know how noisy. This is useful when you need to make a risk-informed decision about whether to relabel fully, but you cannot afford to commit to full relabeling until you understand the scope.

**No relabeling** means accepting that historical labels reflect historical standards and leaving them as-is. The labels are tagged with their guideline version. Downstream systems are updated to account for version differences. Model training pipelines filter out labels from deprecated guideline versions or weight them differently. Reporting dashboards segment metrics by guideline version so that trends are not distorted by comparing labels made under different standards. No relabeling is the cheapest option, but it requires discipline. Every system that consumes labels must understand guideline versioning. If any system ignores version tags and treats all labels as equivalent, you will train models on inconsistent data or produce misleading reports.

## The Cost of Migration

Migration is not free. The direct cost is reviewer time. If you relabel 100,000 examples at $0.50 per label, you spend $50,000. If you relabel 1 million examples, you spend $500,000. These are not hypothetical numbers. Large-scale review operations generate millions of labels per year. A guideline change that requires relabeling two quarters of historical data can cost more than the original labeling effort.

The indirect cost is opportunity cost. Reviewers relabeling historical data are not labeling new data. If your system generates 50,000 new examples per week that need review, and you redirect half your review capacity to relabeling for six weeks, you now have a backlog of 150,000 unlabeled examples. The backlog delays model updates, slows down product launches, and creates pressure to cut corners on quality. Migration is not just expensive in dollars. It is expensive in time, focus, and operational flexibility.

The organizational cost is morale. Reviewers do not enjoy relabeling data they already labeled. It feels like rework. It feels like someone made a mistake and now reviewers are paying for it. If the guideline change was necessary — a compliance requirement, a product shift, a correction of a clear error — reviewers accept it. If the guideline change feels arbitrary or preventable, reviewers resent it. Frequent guideline changes that trigger frequent relabeling efforts erode trust and increase turnover.

The cost is why migration planning happens during the guideline change approval process, not after deployment. The RFC for a breaking guideline change includes a migration plan. The migration plan specifies which labels will be relabeled, what the cost will be, who will fund it, and what timeline is acceptable. If the cost is unacceptable, the guideline change is revised or rejected. You do not discover after deployment that migration is prohibitively expensive. You design the guideline change to minimize migration cost, or you accept the cost upfront as part of the change approval.

## When to Grandfather Old Labels

Grandfathering means accepting that old labels were created under a different standard and will not be updated. The labels remain in the dataset, tagged with their original guideline version, and downstream systems are designed to handle version differences. Grandfathering is the right choice when the cost of relabeling exceeds the value, when the guideline change is forward-looking, and when downstream systems can tolerate mixed-version data.

Grandfathering works when the guideline change is gradual. If you shift from guideline version 1.4 to 1.5, and the changes are incremental, and your dataset spans multiple versions over time, downstream systems are already handling version differences. Adding one more version does not introduce new complexity. Grandfathering breaks down when the guideline change is abrupt and large. If version 2.0 redefines core terms, reverses major rules, and creates incompatibility with everything before it, grandfathering version 1.x labels means maintaining two incompatible datasets indefinitely.

Grandfathering also depends on dataset size and age. If your dataset contains 10 million labels spanning three years, relabeling everything is impractical. You accept that older labels reflect older standards and design systems to account for it. If your dataset contains 100,000 labels spanning six months, relabeling is feasible. The cost is manageable and the benefit of a clean, consistent dataset is high.

The decision to grandfather is explicit and documented. You do not passively allow old labels to accumulate without a plan. You actively decide that old labels will remain as-is, you document the decision, you communicate it to stakeholders, and you ensure that every system consuming labels understands the version distribution and accounts for it. Grandfathering without communication is not a strategy. It is neglect.

## Maintaining Label Provenance and Version

Every label is tagged with the guideline version that was active when the label was created. This is not optional metadata. It is core infrastructure. Without version tags, you cannot segment labels by guideline, you cannot measure the impact of guideline changes, you cannot make informed migration decisions, and you cannot audit how your standards have evolved.

The version tag is immutable. It records the guideline version at label creation time. If the label is later relabeled under a new guideline, the original version is archived and the new version is recorded. The label's history is preserved. You can see that the label was originally created under version 1.4, relabeled under version 2.0, and corrected again under version 2.1. This history is essential for debugging. When a model behaves unexpectedly, you can trace the training data back to the guideline versions that governed it and identify whether the problem is in the labels, the guidelines, or the model itself.

Provenance tracking also includes reviewer ID, timestamp, and review session ID. You can reconstruct exactly when a label was created, by whom, and under what conditions. This is necessary for compliance, for quality investigations, and for calibration analysis. If a reviewer's labels are found to be systematically biased, you can identify all labels they created and decide whether they need to be relabeled. If a guideline version is found to be flawed, you can identify all labels created under that version and decide whether they need to be corrected.

Provenance is stored in the label metadata, not in a separate system. The label and its metadata travel together. If you export labels to a training dataset, the metadata comes with them. If you archive old labels, the metadata is archived with them. Provenance is not an afterthought. It is part of the label from the moment the label is created.

## Handling Multi-Version Training Datasets

When you train a model on labels created under multiple guideline versions, you have three options. First, you filter the training data to include only labels from the current guideline version. This ensures consistency but reduces dataset size. If 80 percent of your labels were created under older guidelines, filtering to the current version cuts your training data by 80 percent. The model may underperform due to insufficient data, even though the data is more consistent.

Second, you include labels from all guideline versions and accept the noise. The model learns an averaged standard that reflects all the guidelines it was trained on. This works when the guideline changes are incremental and the versions are not radically incompatible. It fails when the guideline changes are breaking and the versions teach contradictory behaviors. The model will be confused, inconsistent, and unreliable in edge cases.

Third, you weight labels by guideline version. Labels from the current version receive full weight. Labels from recent deprecated versions receive reduced weight. Labels from old deprecated versions receive minimal weight or are excluded entirely. Weighting allows you to use historical data without letting it dominate the model's behavior. The model is biased toward the current standard but still benefits from the scale of historical data.

The choice depends on data availability, guideline stability, and model performance requirements. If you have abundant data under the current guideline, filter. If you have limited data and stable guidelines, include all versions. If you have limited data and unstable guidelines, weight by version. The decision is not static. As more data accumulates under the current guideline, you shift from weighting to filtering.

## Communicating Version Changes to Downstream Systems

Every system that consumes labels must be notified when a guideline version changes. Model training pipelines need to decide whether to retrain on the new version, filter to a single version, or weight by version. Reporting dashboards need to segment metrics by version to avoid misleading trends. Compliance systems need to track which guideline was active during each audit period. External partners who receive labels under contract need to know that the labels they are receiving may reflect a different standard than previous batches.

The notification is structured. It includes the old version number, the new version number, the date of the change, a summary of what changed, and guidance on how to handle mixed-version datasets. It is delivered through automated alerts, not through email. Systems that depend on labels subscribe to guideline version change events and are notified automatically when a change is deployed.

The next subchapter covers rollback procedures — what happens when a guideline update is deployed and then discovered to be flawed, and how to manage the rollback without losing labels or creating downstream chaos.

