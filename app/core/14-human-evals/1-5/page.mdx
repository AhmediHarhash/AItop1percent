# 1.5 — Review Throughput: Understanding the Human Bandwidth Limit

A single skilled reviewer can evaluate between forty and one hundred twenty outputs per hour, depending on task complexity. A team of ten reviewers can evaluate four hundred to twelve hundred outputs per hour. A model in production generates fifty thousand outputs per day. The math does not work. Even if you dedicate ten reviewers to full-time review, you can cover at most ten percent of production traffic — and only if you accept the quality degradation that comes from sustained high-volume review. The gap between what models produce and what humans can meaningfully review is the fundamental constraint that shapes every decision in human evaluation infrastructure.

This is not a solvable problem in the sense that you can make it go away. You cannot hire enough reviewers to cover all production traffic for any system operating at scale. You cannot speed up reviewers without destroying quality. You cannot automate review without reintroducing the same risks that made you need human review in the first place. The constraint is permanent. The skill is learning to operate inside it — to extract maximum insight from limited review bandwidth, to allocate review capacity to the highest-value cases, and to design systems that remain safe and effective even when you can only review a small fraction of what the model produces.

## The Throughput Ceiling by Task Type

Review throughput is not a single number. It varies wildly by task type, output length, evaluation complexity, and reviewer expertise. A reviewer evaluating sentiment classification — positive, negative, or neutral — can process two hundred outputs per hour with high accuracy. A reviewer evaluating a three-paragraph clinical summary for medical accuracy, completeness, and safety can process twelve outputs per hour. A reviewer evaluating a multi-turn dialogue for coherence, helpfulness, and policy compliance can process twenty-five outputs per hour. The throughput ceiling is determined by the cognitive load required to make a correct judgment.

Short outputs with clear rubrics produce the highest throughput. A customer support AI that generates one-sentence responses can be reviewed at one hundred to one hundred fifty outputs per hour if the rubric is simple: does the response answer the question, is the tone appropriate, does it violate any policies. The reviewer reads the user input, reads the model response, makes a binary or ternary judgment, and moves on. The cognitive load is low because the decision space is constrained. There are only a few things that can go wrong, and they are easy to spot.

Long outputs with subjective rubrics produce the lowest throughput. A legal contract summarization system that generates two-page summaries cannot be reviewed faster than six to ten outputs per hour, even by experienced attorneys. The reviewer must read the original contract, read the summary, verify that all material terms are captured, check for misstatements or omissions, assess whether the summary would mislead a non-expert, and judge overall quality. The cognitive load is high because the decision space is large. Every clause in the contract is a potential point of failure. Every omission could be material or trivial. Every phrasing choice could be clear or misleading. The reviewer must hold multiple criteria in mind simultaneously while processing dense legal language.

Task complexity adds overhead that compounds with output length. A code generation task reviewed for correctness, style, security, and maintainability takes longer than a code generation task reviewed only for correctness. A summarization task reviewed for accuracy, completeness, tone, and readability takes longer than one reviewed only for accuracy. Every additional criterion adds cognitive load. Every additional criterion requires the reviewer to reread the output from a different angle. The throughput ceiling drops as criteria multiply.

## The Accuracy-Throughput Tradeoff

Reviewers can always work faster if you allow them to sacrifice accuracy. The question is how much accuracy you lose for each increment of speed. The tradeoff is not linear. A reviewer who works at eighty percent of their maximum sustainable speed may maintain ninety-five percent accuracy. A reviewer who works at one hundred percent of their maximum speed may drop to eighty-five percent accuracy. A reviewer pushed to one hundred twenty percent of sustainable speed may drop to seventy percent accuracy. The loss accelerates as you push past the throughput ceiling.

A content moderation team tested this tradeoff directly. They asked reviewers to evaluate outputs at three different target speeds: normal pace, twenty-five percent faster, and fifty percent faster. At normal pace, reviewers averaged forty-seven outputs per hour with ninety-two percent agreement with expert ground truth. At twenty-five percent faster, they averaged fifty-nine outputs per hour with eighty-six percent agreement. At fifty percent faster, they averaged seventy outputs per hour with seventy-one percent agreement. The team chose the normal pace. The extra throughput was not worth the accuracy loss, because low-accuracy labels created downstream problems that cost more to fix than the value of the additional volume.

The tradeoff is worse for complex tasks. On simple tasks like sentiment classification, reviewers can increase speed by fifteen to twenty percent without significant accuracy loss because the decisions are so constrained. On complex tasks like medical summarization quality, even a ten percent speed increase causes measurable accuracy degradation. The reviewer has less time to verify each fact, less time to consider edge cases, less time to catch subtle errors. The errors they miss are not random — they are systematically the hardest cases, the ones that require the most attention to catch.

Pushing reviewers past their throughput ceiling does not just reduce accuracy — it increases cognitive fatigue, which reduces accuracy further over time. A reviewer who works at one hundred twenty percent of sustainable speed for one hour may perform acceptably in that hour, but their accuracy in the second and third hours will be significantly worse than if they had worked at eighty percent of sustainable speed. The short-term throughput gain produces a long-term quality collapse.

## Reviewer Experience and the Learning Curve

Throughput increases with experience, but the gains plateau quickly. A new reviewer produces thirty to fifty percent of the throughput of an experienced reviewer in their first week. By week three, they reach seventy to eighty percent. By week eight, they reach ninety to ninety-five percent. After that, throughput gains are marginal. The learning curve is steep at the beginning and flat at the end. The implication: you cannot solve throughput problems by simply waiting for reviewers to get faster. They will get faster, but only up to a ceiling determined by the task itself.

The learning curve varies by task complexity. On simple tasks, new reviewers reach near-expert throughput within two weeks. On complex tasks, it can take two to three months. A healthcare AI company tracked reviewer throughput over six months for clinical summary evaluation. New reviewers started at eight outputs per hour. By month one, they reached eleven outputs per hour. By month three, they reached fourteen outputs per hour. By month six, they reached fifteen outputs per hour. The expert ceiling was sixteen outputs per hour. The plateau started at month three. After that, experience improved accuracy more than throughput.

Experience also changes the error distribution. Novice reviewers make different mistakes than experts. Novices miss subtle errors because they do not yet recognize the patterns. Experts miss obvious errors because they are pattern-matching too aggressively and overlook cases that do not fit the expected pattern. The highest-quality review comes from reviewers in the middle of the experience curve — experienced enough to recognize patterns, not so experienced that they have become overconfident. A well-designed review program cycles reviewers through different task types to prevent overconfidence while maintaining experience-driven throughput gains.

## The Multi-Task Penalty

A reviewer who evaluates a single task type all day is faster than a reviewer who switches between task types. The **multi-task penalty** is the throughput loss caused by context-switching between different rubrics, different output formats, and different evaluation criteria. The penalty is typically fifteen to thirty percent, depending on how different the tasks are. If a reviewer can evaluate forty summarization outputs per hour when focused, they will evaluate twenty-eight to thirty-four outputs per hour if they switch between summarization, sentiment, and structured extraction tasks.

The penalty exists because each task type requires a different mental model. Evaluating summarization quality requires thinking about completeness, accuracy, and readability. Evaluating sentiment requires thinking about tone and emotional valence. Evaluating structured extraction requires thinking about field accuracy and schema compliance. Switching between these models requires mental overhead. The reviewer has to reload the rubric, reload the examples, and reload the pattern recognition heuristics they have built for that task type. This overhead is small per switch — thirty seconds to two minutes — but it compounds across a session.

The multi-task penalty is worse when tasks require different levels of cognitive load. Switching from a simple task like sentiment classification to a complex task like clinical summarization is more disruptive than switching between two similarly complex tasks. The reviewer has to shift gears from fast, shallow processing to slow, deep processing. The transition takes time, and the first few outputs after the switch are reviewed less accurately than they would be if the reviewer had been working on that task type continuously.

The solution is task batching. Assign reviewers to a single task type for at least ninety minutes before switching. If you need a reviewer to cover multiple task types in a day, schedule them in task blocks rather than interleaving tasks. A reviewer who spends ninety minutes on summarization, then ninety minutes on sentiment, then ninety minutes on extraction will be significantly more productive than a reviewer who switches tasks every fifteen minutes. The total time is the same, but the throughput is twenty to thirty percent higher because the multi-task penalty is amortized across fewer switches.

## The Scale Bottleneck

Throughput per reviewer does not scale linearly with team size. A team of ten reviewers does not produce ten times the throughput of one reviewer. It produces seven to eight times the throughput, because coordination overhead, calibration costs, and quality control requirements increase with team size. A team of one hundred reviewers does not produce one hundred times the throughput of one reviewer. It produces fifty to sixty times the throughput, because the overhead compounds further at scale.

The overhead comes from three sources. First, **calibration and quality control**: as the team grows, you need more calibration sessions, more quality checks, and more error correction loops to ensure consistency. A team of five reviewers can calibrate in a single one-hour session every two weeks. A team of fifty reviewers needs multiple calibration sessions covering different sub-teams, plus a layer of senior reviewers who spot-check the work of junior reviewers. This overhead consumes five to ten percent of total reviewer time at small scale, and fifteen to twenty-five percent at large scale.

Second, **program management**: as the team grows, you need more coordination to schedule reviewers, handle escalations, track metrics, and manage the queue. A team of five reviewers can self-coordinate with minimal oversight. A team of fifty reviewers needs dedicated operations staff to manage logistics. The cost of this management layer is proportional to team size, and it comes directly out of available throughput. If ten percent of your reviewer capacity is spent on coordination and management, your effective throughput is ten percent lower than the raw sum of individual reviewer capacity.

Third, **communication and alignment**: as the team grows, the cost of keeping everyone aligned on standards, rubrics, and edge case decisions increases. A team of five reviewers can align through informal conversation. A team of fifty reviewers needs documentation, training materials, recorded calibration sessions, and regular all-hands meetings. The time spent consuming and producing this alignment content is time not spent reviewing outputs. At large scale, this overhead can consume five to ten percent of reviewer time.

The implication: you cannot solve throughput problems by simply hiring more reviewers. Hiring scales throughput sub-linearly, and at some point the overhead of coordination exceeds the value of additional reviewers. The sustainable team size depends on the task complexity and the organizational maturity of your review operations. For simple tasks, you can scale to fifty or one hundred reviewers with acceptable overhead. For complex tasks, the effective ceiling is closer to twenty to thirty reviewers per task type.

## The Queue Management Problem

Review throughput is constrained not just by reviewer capacity, but by how efficiently you feed outputs to reviewers. A reviewer who spends twenty percent of their time waiting for the next output, waiting for a technical issue to resolve, or waiting for clarification on an edge case is delivering twenty percent less throughput than their raw capability. The **queue management problem** is ensuring that reviewers always have work available, always have the context they need, and never waste time on avoidable delays.

The most common queue management failure is the empty queue. A reviewer finishes their assigned batch and has to wait for the system to load the next batch. If that load time is two minutes per batch, and the reviewer completes a batch every thirty minutes, they lose six percent of their time to waiting. Over a full day, that is thirty minutes of wasted capacity per reviewer. Across a team of ten reviewers, that is five hours of wasted capacity per day — equivalent to losing half a reviewer. The fix is preloading: always have the next batch ready before the reviewer finishes the current one.

The second failure is the blocked queue. A reviewer encounters an output they cannot evaluate because critical context is missing, the output is malformed, or the rubric does not cover this case. They flag it for escalation and move on. If these blocked cases accumulate faster than they are resolved, reviewers start seeing the same blocked cases repeatedly, which wastes time. A well-designed queue system removes blocked cases from circulation immediately after the first flag, routes them to a dedicated escalation queue, and only returns them to the review pool after resolution.

The third failure is the imbalanced queue. Different reviewers work at different speeds, have different specializations, or are available at different times. If the queue system does not account for this, some reviewers will be overloaded while others are underutilized. A content moderation team had five reviewers working the day shift and three working the night shift. The queue system distributed work evenly across all reviewers, which meant the night shift reviewers were starved for work while the day shift reviewers were underwater. The fix was a dynamic queue that allocated work based on reviewer availability and real-time throughput.

## The Prioritization Challenge

You cannot review everything, so you must choose what to review. The prioritization decision directly affects throughput utilization. If you prioritize the wrong cases, you waste reviewer capacity on low-value outputs while high-value outputs go unreviewed. If you prioritize randomly, you miss the cases most likely to surface model failures. If you prioritize only the hardest cases, you overload reviewers with cognitively exhausting work and burn them out. The optimal prioritization strategy maximizes the insight gained per hour of reviewer time.

The simplest prioritization strategy is random sampling. Review a random subset of production outputs at a fixed sampling rate — one percent, five percent, ten percent. This strategy is unbiased and gives you a representative view of overall model performance. It is also inefficient, because most production outputs are handled correctly by the model. You spend reviewer time confirming that correct outputs are correct, which provides little new information. Random sampling is the baseline. Every other prioritization strategy should beat it on insight per review.

The second strategy is **risk-based prioritization**: review outputs that the model is most likely to have gotten wrong. This requires a risk score — a signal that predicts failure likelihood. The signal can come from model confidence, from heuristics like unusual output length or unusual input patterns, or from automated metrics like perplexity or semantic similarity to training data. A customer support AI prioritized review of outputs where the model confidence score was below seventy percent. This subset represented twelve percent of production traffic but contained sixty-eight percent of actual failures. Reviewing this subset was five times more efficient than random sampling in terms of failures detected per review.

The third strategy is **diversity-based prioritization**: review outputs that represent different input patterns, edge cases, or rare scenarios. This ensures that your review program covers the full operating space of the model, not just the common cases. A financial services chatbot used clustering to identify twenty distinct input categories, then sampled evenly across categories. This strategy caught failures in rare input types that random sampling would have missed because they represented less than one percent of traffic each.

The fourth strategy is **temporal prioritization**: review recent outputs before old outputs. This ensures that if the model is degrading or encountering new input patterns, you detect it quickly. A news summarization system reviewed only outputs from the past twenty-four hours. This strategy reduced the delay between model degradation and human detection from three weeks to two days, allowing the team to intervene before user complaints accumulated.

Optimal prioritization combines multiple strategies. Review high-risk outputs to catch failures, review diverse outputs to ensure coverage, review recent outputs to detect drift, and review a small random sample to maintain an unbiased performance baseline. The exact mix depends on your risk tolerance, your available review capacity, and the cost of different failure types. A high-stakes system may allocate eighty percent of review capacity to high-risk outputs. A lower-stakes system may allocate only forty percent, preserving more capacity for diversity and baseline sampling.

## Throughput Multipliers: Automation and Hybrid Systems

You can increase effective throughput by using automation to pre-filter outputs before human review. An automated filter that removes obviously correct outputs from the review queue increases the density of meaningful work per review. If ninety percent of production outputs are obviously correct and your automated filter catches eighty-five percent of them with near-perfect precision, reviewers only see twenty-three percent of production traffic — but that subset contains sixty percent of actual failures. Effective throughput per failure detected increases by two point six times.

The challenge is building filters with high precision. A filter with low precision sends bad outputs to the review queue, which is fine — that is what the queue is for. But a filter with low recall removes bad outputs from the queue, which means reviewers never see them, and failures reach production. The safe strategy is high-recall, high-precision filtering for obviously correct outputs, and low-recall, high-precision filtering for obviously incorrect outputs. Everything else goes to human review. The filter reduces volume without introducing blind spots.

A legal contract analysis system used a hybrid approach. They ran every output through an automated check for structural correctness — does the output include all required fields, are all fields formatted correctly, are there any obvious hallucinations like dates in the future or negative dollar amounts. Outputs that failed these checks were flagged for immediate review. Outputs that passed were sent to a secondary check for semantic correctness using a model-as-judge approach. Outputs with high confidence from the judge were marked as likely correct and sent to a low-priority review queue. Outputs with low confidence were marked as uncertain and sent to a high-priority review queue. The system reduced the human review burden by forty-two percent while maintaining the same failure detection rate as full manual review.

Automation does not replace review — it amplifies it. The human reviewers still make the final judgment on every case that matters. But automation ensures that human attention is focused on the cases where human judgment provides the most value. The result is higher throughput, lower cost, and better failure detection — not because reviewers work faster, but because the system delivers higher-value work to them.

## The Throughput-Coverage Tradeoff

High throughput and high coverage are in tension. Throughput is the number of outputs reviewed per hour. Coverage is the percentage of production traffic reviewed. You can maximize throughput by assigning reviewers only the easiest, fastest tasks. You can maximize coverage by sampling broadly across all traffic. You cannot do both simultaneously. The tradeoff is fundamental, and every review program makes an implicit or explicit choice about where to land on the curve.

A high-throughput, low-coverage strategy reviews a small, carefully selected subset of production traffic. This strategy works when you are primarily interested in detecting specific failure modes that appear in predictable patterns. A spam detection system might review only outputs where the model confidence is below fifty percent. This subset represents three percent of traffic but can be reviewed quickly because the task is simple. The strategy maximizes failure detection per hour of review, but it provides no visibility into the ninety-seven percent of traffic that the filter excludes.

A low-throughput, high-coverage strategy reviews a broad, representative sample of production traffic. This strategy works when you need to monitor overall model quality, detect unexpected failure modes, or maintain an unbiased view of performance across diverse inputs. A customer support chatbot might review a random five percent of all conversations, regardless of model confidence or predicted risk. The review is slower because the sample includes both easy and hard cases, but the coverage ensures that no failure pattern is systematically excluded.

The optimal strategy is usually neither extreme. Most systems use a tiered approach: allocate fifty to seventy percent of review capacity to high-risk, high-value cases for maximum failure detection, and allocate thirty to fifty percent to broad sampling for coverage and drift detection. The exact split depends on the maturity of your system. Early in development, coverage matters more because you do not yet know where the model will fail. Late in maturity, targeted high-risk review matters more because you have already characterized the common failure modes and need to detect new ones quickly.

## The Throughput Sustainability Problem

Reviewer throughput degrades over time if the work environment is unsustainable. Burnout, monotony, and poor tooling all reduce throughput — not acutely, but chronically. A reviewer who starts the week at fifty outputs per hour may end the week at forty outputs per hour if the work is mentally exhausting, repetitive, or technically frustrating. The degradation is gradual enough that it is often attributed to individual performance rather than systemic issues. The fix is designing for sustainability: reasonable hours, task variety, adequate breaks, responsive tooling, and respect for cognitive limits.

A content moderation team tracked weekly throughput across six months. In the first two months, reviewers averaged forty-six outputs per hour. By month four, throughput had dropped to thirty-nine outputs per hour. By month six, it was thirty-four outputs per hour. The team assumed reviewers were becoming complacent. Exit interviews revealed the real cause: the work was emotionally draining, the interface was buggy, and there was no variety in task assignments. Reviewers were burning out. The company restructured the program to rotate reviewers across task types, improved the interface, and added mental health support. Throughput recovered to forty-four outputs per hour within eight weeks.

Sustainability is not a moral issue — it is a throughput issue. A burned-out reviewer produces less work at lower quality, then quits and forces you to onboard a replacement who produces even less work for the first several weeks. The cost of unsustainability is permanent productivity loss. The cost of investing in sustainability is modest and recoverable within quarters.

The next subchapter examines the reviewer experience problem — the infrastructure, tooling, and workflow decisions that determine whether reviewers can work effectively or spend their time fighting the system.

