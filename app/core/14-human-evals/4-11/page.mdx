# 4.11 — Breaking Changes: When Guideline Updates Require Retraining

In late 2025, a content moderation team at a social platform deployed a guideline update that clarified when political speech crossed into incitement. The change was small — two paragraphs added, three examples updated. The team announced it in the daily standup, updated the guideline doc, and assumed reviewers would adapt. Within 72 hours, the label distribution shifted by 14 percentage points. Escalations tripled. Reviewers were flagging content as incitement that would have been approved under the old guideline, and approving content that would have been flagged. The guideline change was correct. The deployment was catastrophic. The team had introduced a breaking change without recognizing it, without retraining reviewers, and without managing the transition.

A **breaking change** is a guideline update that fundamentally alters what reviewers should do — not just clarifies what they already understood, but changes the standard itself. It redefines a category, shifts a threshold, introduces a new rule that conflicts with prior practice, or reverses a previous decision. Breaking changes require retraining because reviewers cannot simply read the new text and apply it correctly. Their mental models were built on the old standard. Without explicit retraining, they will misapply the new guideline or continue applying the old one from memory.

## Identifying Breaking vs Non-Breaking Changes

Not every guideline change is breaking. A change that adds examples without changing the underlying rule is non-breaking. A change that fixes a typo, reformats text for clarity, or reorganizes sections without altering meaning is non-breaking. A change that codifies something reviewers were already doing informally is non-breaking. Non-breaking changes can be deployed with a simple announcement. Reviewers can read the update and continue working.

A breaking change is one where reviewers trained on the old guideline will make systematically different decisions than reviewers trained on the new one. The decision is not subjective. You test it. You take a sample of 50 to 100 examples that were labeled under the old guideline, show them to reviewers who know only the new guideline, and measure agreement. If agreement is below 85 percent on cases that were previously unambiguous, the change is breaking. If agreement is high, the change is non-breaking even if the text looks substantially different.

Specific patterns indicate breaking changes. Changing the definition of a core term is breaking. If the guideline previously defined toxicity as "language intended to harm" and the new guideline defines it as "language that causes harm regardless of intent," every reviewer's understanding of toxicity must be rebuilt. Adding a new disqualifying criterion is breaking. If the guideline previously allowed political speech unless it included explicit threats, and the new guideline disallows speech that implicitly encourages violence, reviewers must learn to recognize implicit encouragement — a skill they were not trained on.

Reversing a previous rule is breaking. If the old guideline said "sarcasm is not toxicity" and the new guideline says "sarcasm can be toxicity if it targets a protected class," reviewers who internalized the first rule will not automatically apply the second. Changing scoring thresholds is breaking. If the old guideline required three distinct policy violations to escalate, and the new guideline requires two, reviewers will escalate at different rates. Introducing a new edge case that conflicts with prior practice is breaking. If reviewers were trained to approve a certain pattern and the new guideline forbids it, that pattern must be explicitly retrained.

The rule is simple. If a reviewer could read the new guideline, apply it to examples they previously labeled, and produce different labels without feeling like they made a mistake, the change is breaking. If the reviewer would look at the new guideline and say "this is just a clearer way of saying what I was already doing," the change is non-breaking.

## The Retraining Burden

Retraining is expensive. It takes time, it disrupts throughput, and it requires instructional design effort. For a team of 50 reviewers, a breaking change that requires two hours of retraining consumes 100 person-hours. For a team of 500 reviewers, it consumes 1,000 person-hours. For a team operating across three shifts and six time zones, it requires coordinating retraining sessions, pausing production work, and managing the period where some reviewers are retrained and others are not.

The retraining is not optional. Deploying a breaking change without retraining creates a period where reviewers are working from incompatible mental models. Some are applying the old guideline from memory. Some are trying to apply the new guideline without sufficient understanding. Labels made during this period are inconsistent, unreliable, and potentially worse than labels made under either guideline alone. The transition period becomes a data quality disaster.

Retraining is also not instant. Reviewers need time to internalize the change. They need to see examples, discuss edge cases, make mistakes in a safe environment, and receive correction. A 30-minute video explaining the change is not retraining. A two-hour workshop with live examples, Q&A, and calibration exercises is retraining. The retraining session must cover not just what changed, but why it changed, how to recognize situations where the old and new guidelines diverge, and what to do when uncertain.

The burden of retraining is why breaking changes should be rare. Not every clarification requires retraining. Not every edge case requires a guideline update. Breaking changes are reserved for situations where the old guideline is fundamentally wrong, where product direction has shifted, where compliance requirements have changed, or where accumulated reviewer confusion has made the old guideline unworkable. You do not introduce a breaking change to make the guideline slightly clearer. You introduce a breaking change when the cost of not changing exceeds the cost of retraining.

## Phased Rollout for Breaking Changes

Breaking changes do not roll out to all reviewers simultaneously. They roll out in phases, and each phase includes retraining before deployment. The first phase is a pilot group of 10 to 20 reviewers who are experienced, communicative, and representative of the full reviewer population. The pilot group completes retraining, begins using the new guideline, and provides feedback on whether the retraining was sufficient, whether the guideline is clear, and whether they are encountering edge cases that the guideline does not address.

The pilot runs for one to two weeks. During that time, you monitor the pilot group's labels closely. You compare their label distribution to historical baselines. You review a sample of their work to confirm that they are applying the new guideline correctly. You solicit feedback on what was confusing, what examples were missing, and what additional training would have helped. Based on pilot feedback, you revise the retraining materials, update the guideline if necessary, and prepare for the next phase.

The second phase is a larger cohort — typically 20 to 30 percent of all reviewers. This cohort completes the revised retraining and begins using the new guideline. The second phase reveals scaling issues that the pilot missed. You discover that an explanation that worked for 20 people does not work for 100. You find edge cases that the pilot group did not encounter. You identify bottlenecks in the retraining process — time zones that make live sessions difficult, languages that require translated materials, sub-teams that need domain-specific examples.

The second phase runs for another one to two weeks. You continue monitoring, continue refining, and measure whether labels from the new cohort match expectations. You also compare labels between the cohort using the new guideline and the cohort still using the old guideline. If the two groups produce systematically different labels, that is expected — the change was breaking. If the new group's labels are more consistent, the retraining worked. If the new group's labels are less consistent, the retraining failed and must be revised before full rollout.

The third phase is full rollout. All reviewers complete retraining and transition to the new guideline. By this point, the guideline has been tested, piloted, scaled, and refined based on real production feedback. The retraining materials have been revised multiple times. The risk of catastrophic failure is low. The transition is managed, not chaotic.

## Maintaining Old and New Guidelines During Transition

During the phased rollout, two guidelines are active simultaneously. Reviewers in the pilot and second-phase cohorts use the new guideline. Reviewers not yet retrained continue using the old guideline. This is not a bug. It is a deliberate design choice that prevents forcing reviewers to apply a guideline they have not been trained on.

The review system tracks which guideline each reviewer is using. Labels are tagged with the guideline version. When a reviewer logs in, the system serves the correct guideline for their cohort. The new guideline is not visible to reviewers who have not completed retraining. This prevents reviewers from reading the new guideline, assuming they understand it, and applying it incorrectly without training.

The dual-guideline period is temporary. It lasts for the duration of the phased rollout — typically four to six weeks. Once all reviewers have been retrained, the old guideline is retired. Labels made under the old guideline remain tagged with the old version. Labels made under the new guideline are tagged with the new version. This allows you to analyze the impact of the change, identify any drift, and compare quality metrics before and after the transition.

Maintaining two guidelines creates operational complexity. You must ensure that reviewers are not confused about which guideline applies to them. You must ensure that downstream systems that consume labels understand that labels from the same period may have been created under different standards. You must ensure that any automated systems that depend on labels — model training pipelines, reporting dashboards, compliance audits — account for the guideline version.

The complexity is justified by the alternative. If you deploy a breaking change to all reviewers at once, you create a period where nobody is trained, labels are chaotic, and downstream systems receive garbage data. The dual-guideline period is controlled chaos. The simultaneous deployment is uncontrolled chaos. Controlled chaos is better.

## Communicating Breaking Changes

Reviewers need to know that a change is breaking. The announcement must explicitly state that the guideline has changed in ways that require retraining, that labels made under the old guideline may no longer be correct, and that reviewers must complete the retraining before using the new guideline. The announcement is not phrased as a suggestion. It is phrased as a requirement.

The announcement explains why the change is happening. What problem does the new guideline solve? What failures occurred under the old guideline? What product or compliance requirement necessitated the change? Reviewers are more likely to engage seriously with retraining if they understand the rationale. If the change feels arbitrary, reviewers resist it. If the change feels necessary, reviewers accept it.

The announcement provides a timeline. When does retraining begin? When must it be completed? When does the new guideline take effect? What happens if a reviewer does not complete retraining by the deadline? The timeline is specific. Reviewers know exactly what is expected and when. Ambiguous timelines lead to reviewers assuming they can delay retraining, leading to operational chaos when some reviewers transition and others do not.

The announcement links to the retraining materials. The materials include a summary of changes, side-by-side examples showing how the old and new guidelines differ, a recorded training session, and a calibration exercise. Reviewers are required to complete the calibration exercise before they can use the new guideline. The calibration ensures that reviewers have internalized the change and can apply it correctly.

## Handling Labels Made Under the Old Guideline

When a breaking change is deployed, you have a corpus of historical labels made under the old guideline. Those labels are not automatically wrong. They were correct under the old standard. But they may no longer be correct under the new standard. The decision about what to do with old labels depends on the nature of the change and the downstream use of the labels.

If the breaking change fixes a clear error in the old guideline, old labels are suspect. If the old guideline incorrectly permitted something that the new guideline forbids, labels that approved that thing are wrong. If the old guideline incorrectly forbade something that the new guideline permits, labels that rejected that thing are wrong. These labels may need to be relabeled, or at minimum flagged so that downstream systems know they were created under a deprecated standard.

If the breaking change reflects a product decision rather than correcting an error, old labels are not wrong — they are outdated. The old guideline was right for its time. The new guideline is right now. Labels made under the old guideline can remain in the dataset, tagged with the old version, and used for analysis of how behavior has changed over time. They should not be used to train new models unless the model is being trained to replicate the old standard.

If the breaking change is narrow — affecting only a specific category or edge case — you can selectively relabel. You identify the subset of labels likely to be affected, review them under the new guideline, and update only those that change. This is more surgical than a full relabeling effort and more cost-effective. It requires good instrumentation so that you can filter historical labels by the attributes affected by the guideline change.

The decision is not made lightly. Relabeling is expensive. A corpus of 100,000 labels, where 10 percent are affected by a breaking change, requires reviewing 10,000 examples. If each review takes two minutes, that is 333 person-hours. The cost must be justified by the value of correcting the labels. If the labels are used to train production models, the cost is justified. If the labels are used only for historical analysis, it may not be.

## The Rollback Consideration for Breaking Changes

Rolling back a breaking change is harder than rolling back a non-breaking change. If you deploy a breaking change, retrain reviewers, and then discover that the change was flawed, you cannot simply revert to the old guideline. Reviewers have been retrained. Their mental models have shifted. Reverting the guideline without retraining them back to the old standard creates the same inconsistency problem you were trying to avoid.

The decision to roll back a breaking change requires retraining back to the old guideline, or rolling forward to a corrected new guideline. Both are expensive. Rolling back requires admitting that the retraining effort was wasted. Rolling forward requires additional retraining on top of the initial retraining. The best approach is to avoid deploying a breaking change that needs to be rolled back. This is why breaking changes require more rigorous testing, longer pilot phases, and more conservative approval workflows than non-breaking changes.

If a breaking change must be rolled back, the rollback is announced explicitly. Reviewers are told that the new guideline is being retired, that the old guideline is being reinstated, and that they must re-familiarize themselves with the old standard. A calibration session is conducted to help reviewers transition back. Labels made under the failed new guideline are flagged and reviewed. The rollback is not treated as a quiet revert. It is treated as a major event requiring the same communication and training discipline as the original deployment.

The next subchapter covers migration plans — what to do with labels made under old standards when guidelines change, and how to manage the transition without losing historical data or creating chaos in downstream systems.

