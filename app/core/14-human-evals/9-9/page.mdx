# 9.9 — Labels vs Review Artifacts: The Two-Tier Output Schema

Most teams think human review produces labels. A binary judgment, a severity rating, a category assignment — one structured value that goes into a training dataset or a metrics dashboard. They build review interfaces that collect exactly that, nothing more. Then three months later, Engineering asks why a specific output was flagged as unsafe, and the only record in the database is a boolean True. No context. No reasoning. No evidence. The review happened, the judgment was recorded, and the insight that drove the judgment evaporated the moment the reviewer clicked Submit.

You need two outputs from every review action. The **label** is the structured decision that machines consume — the rating, the category, the pass-fail judgment. The **artifact** is the evidence and reasoning that humans and automation need to understand, reproduce, and act on that decision. Labels drive metrics and training loops. Artifacts drive product improvements, policy refinement, and root cause analysis. If you only capture labels, you get clean numbers and no understanding. If you capture artifacts without structure, you get rich context and no automation. You need both, stored together, with a schema that links them.

## The Label: Structured for Consumption

The label is what your ML pipeline, your metrics system, and your active learning loop need. It's the single source of truth for "what did the reviewer decide." The structure depends on the task. For safety review, it might be a severity enum: Safe, Low Risk, Moderate Risk, High Risk, Critical. For content moderation, it might be a policy violation flag plus category: Violates Policy, Hate Speech. For quality evaluation, it might be a Likert scale rating plus a boolean for whether the output is production-ready.

The label schema must be **consistent across all reviews of the same task type**. Every safety review outputs the same severity field. Every content moderation review outputs the same policy flag and category. If different reviewers record decisions in different formats — one writes "unsafe" in a text field, another selects "High Risk" from a dropdown, a third marks a checkbox labeled "Block This Output" — your aggregation scripts break, your training pipeline can't parse the labels, and your metrics dashboard shows nonsense.

The label schema must be **forward-compatible with automation**. That means using controlled vocabularies, not free text. It means using structured types — enums, integers, booleans, ISO timestamps — not narrative descriptions. It means versioning the schema so that when you add a new severity level or split a category into subcategories, you can still parse historical reviews. The moment your label schema requires a human to interpret it, you've lost the ability to automate downstream consumption.

The label is what you aggregate, trend, and threshold. If 12% of reviews for a specific prompt template return High Risk or Critical, you trigger an alert. If the average quality rating for responses in Spanish drops below 3.8 out of 5, you flag the model for investigation. If 200 reviews in a single day mark outputs as policy violations under the new misinformation category, you escalate to Trust and Safety. These workflows only function if the label is machine-readable and semantically consistent.

## The Artifact: Evidence and Reasoning

The artifact is everything else. It's the highlighted text span that triggered the safety flag. It's the reviewer's written explanation of why the output violates policy. It's the timestamp when the review happened, the prompt that generated the output, the model version, the session ID, the user cohort, the geo-location if relevant. It's the free-text comment from the reviewer describing an edge case they've never seen before. It's the screenshot if the output included a visual element. It's the entire chain of reasoning if the reviewer had to consult a policy document or escalate to a supervisor before deciding.

Artifacts are semi-structured or unstructured. Some fields have rigid schemas — timestamps are ISO 8601, model versions follow semantic versioning, session IDs are UUIDs. Other fields are free text with optional tagging. A reviewer writes: "Output claims the medication treats cancer but the FDA indication is only for diabetes. Factual error, medical domain, high severity." That's an artifact. It's not a label — you can't feed it directly into a training loop. But it's the evidence that explains the label, and it's the signal that tells you this isn't just a hallucination, it's a specific pattern of medical misinformation that your prompt needs to guard against.

Artifacts enable root cause analysis. When you see a spike in safety flags, you don't just know the numbers went up — you can read the reviewer comments, see which text spans were highlighted, identify the common patterns. Maybe 80% of the flags came from outputs generated with a specific few-shot example that introduced a biased framing. Maybe the issue only appears for queries in German when the user is on a mobile device. The label tells you there's a problem. The artifact tells you what the problem is and where it came from.

Artifacts enable policy refinement. A reviewer marks an output as violating the hate speech policy, but in the comments writes: "Borderline case — this is satirical political commentary, not actual hate speech, but the current guidelines don't distinguish." That artifact goes to your Policy team. Over the next month, twelve more reviewers flag similar cases with similar notes. Policy updates the guidelines to explicitly carve out satirical commentary with clear political context. Without the artifacts, you'd just see fluctuating hate speech numbers and assume the model was drifting or the reviewers were inconsistent.

Artifacts enable product fixes. A reviewer flags an output as unsafe because the model generated instructions for synthesizing a controlled substance. The artifact includes the exact prompt, the exact output, the session context showing the user asked a chemistry question that seemed innocuous. That artifact goes to your Prompt Engineering team. They trace it to a missing guardrail in the system prompt — the model was supposed to refuse chemistry questions about controlled substances, but the refusal logic only triggered on exact keyword matches, and this query used a euphemism. The team patches the prompt with a more robust check. That fix came from the artifact, not the label.

## The Schema That Links Them

The two-tier schema stores labels and artifacts together in a single review record. The structure looks like this in prose: every review has a unique ID, a timestamp, a reviewer ID, a task ID linking to the specific prompt-output pair being reviewed, a label object containing the structured decision, and an artifact object containing all supporting context.

The label object has a fixed schema per task type. For safety review: severity enum, policy area enum, action recommendation enum. For quality review: overall rating integer, dimension-specific ratings as a nested object, production-ready boolean. For content moderation: violates-policy boolean, category enum, confidence enum. Every field in the label object is required and machine-typed. If a field is optional, it defaults to a known value — null, or Unknown, or Not Applicable — never missing.

The artifact object has a flexible schema. Some fields are required: reviewer comment text, timestamp, model version, prompt text, output text. Other fields are optional but structured when present: highlighted spans as an array of start-end offsets with associated category tags, escalation flag boolean, supervisor notes text, linked policy document IDs, user feedback if available, session metadata as a nested object. The artifact schema can grow over time — you add new optional fields without breaking consumers that only read the core fields.

The link between label and artifact is bidirectional. The label references the artifact object in the same record. The artifact stores the label decision inline, so you can read a single record and see both the judgment and the reasoning. Downstream systems that only need labels — your active learning loop, your metrics aggregator — read the label object and ignore the artifact. Downstream systems that need full context — your root cause analyzer, your policy review workflow, your prompt patch generator — read the artifact object and use the label as a filter or grouping key.

## Storage and Versioning

You store review records in a database that supports both structured queries and full-text search. The label object goes into typed columns or a JSON field with indexed keys, so you can query "all reviews where severity equals High Risk and policy area equals Medical Misinformation." The artifact object goes into a JSON or text field with full-text indexing, so you can search "all reviewer comments containing the phrase 'sarcasm detection.'"

You version the schema at the task level. Each task type has a schema version number. When you launch a new review task or update an existing one, you increment the schema version and record it in every new review. Historical reviews retain their original schema version. Your consumers check the version number and parse accordingly. If you're reading a version 2 safety review that split the old Unsafe category into three new severity levels, you map the historical version 1 Unsafe labels to the version 2 Moderate Risk equivalent, or you exclude version 1 reviews from trend analysis if the mapping isn't clean.

Schema migrations are explicit, not implicit. You don't silently rewrite old records to match the new schema. You write migration scripts that create version-mapped views or derivative tables, and you document the mapping logic so analysts know what changed and when. If a product decision depends on trend data crossing a schema boundary — comparing safety flag rates before and after a policy update that redefined what "High Risk" means — the analyst needs to know the schema changed, apply the appropriate normalization, and caveat the comparison.

## Artifacts That Trigger Automation

Some artifacts have structured fields that automation can act on directly. A reviewer marks an output as factually incorrect and fills in a structured field pointing to the authoritative source that contradicts the output — a URL to a clinical trial database, a reference to an FDA document, a link to the company's internal knowledge base. Your fact-check automation pipeline reads that field, fetches the authoritative source, and generates a candidate correction that goes into a prompt patch.

A reviewer flags an output as policy-violating and selects a policy clause from a dropdown. Your policy enforcement system reads that clause ID, looks up the corresponding mitigation rule, and adds the rule to the model's output filtering logic. A reviewer marks an output as unsafe for a specific user cohort and tags the demographic or geo metadata that defines the cohort. Your routing logic reads that tag and adds a cohort-specific guardrail to the prompt sent to users matching that profile.

These automation triggers only work if the artifact schema includes the right structured fields. Free-text comments are rich but not directly actionable. A comment like "this violates our political neutrality policy" tells a human what's wrong, but automation needs the policy ID, the violation type, and the recommended action. If your artifact schema doesn't include those fields, the reviewer writes the comment, a human eventually reads it and manually creates a Jira ticket, and the fix happens weeks later. If your artifact schema has a policy-violation object with clause-id, violation-type, and suggested-action fields, automation reads it and routes the finding to the right remediation workflow the same day.

## Labels Without Artifacts Are Blind Metrics

If you only store labels, you can measure trends but not diagnose causes. You see that 8% of outputs were flagged as unsafe in November and 14% in December. You don't know if the model changed, the user base shifted, the policy was updated, or the reviewers interpreted the guidelines differently. You build a dashboard that shows safety flag rates by model version, by geo, by time of day. The dashboard is accurate. It's also useless for making product decisions because you have no idea what any of the flags mean.

You train an active learning loop that prioritizes uncertain examples for review. The loop works — reviewers label the uncertain cases, the model retrains, accuracy improves. But the model keeps making the same category of mistake, just on new examples. Why? Because the labels told the model what was wrong, but not why it was wrong or what pattern to avoid. The artifacts would have revealed the pattern — all the flagged examples used a specific phrasing that triggered a bias, or relied on a knowledge cutoff issue, or misapplied a policy rule. Without artifacts, you fix individual examples and never fix the root cause.

## Artifacts Without Labels Are Unstructured Chaos

If you only store artifacts — free-text reviewer comments, highlighted spans, narrative explanations — you have rich qualitative data and no way to aggregate it. You can't compute metrics. You can't train models. You can't run automated filters. You have a thousand reviewer comments describing a thousand different issues, and every product decision requires a human to read through them all, manually categorize them, and synthesize the findings.

You build a qualitative analysis workflow. Once a month, a team lead reads a sample of reviewer comments, clusters them into themes, and writes a summary report. The report is insightful. It's also late, biased by the sample selection and the team lead's interpretation, and impossible to reproduce or validate. If the CEO asks "what percentage of safety issues are medical misinformation versus political bias," the only answer is "we'll get back to you in two weeks after someone reads 5,000 comments."

## The Two-Tier Schema Is the Minimum

You need both. Labels for automation, aggregation, and metrics. Artifacts for understanding, root cause analysis, and product improvement. The schema that links them is not optional infrastructure — it's the foundation of every downstream workflow that depends on review data. Build it once, version it carefully, enforce it strictly. Every review task outputs both a structured label and a supporting artifact. Every consumer knows which part to read. Every analysis starts with the question: do I need the decision, the reasoning, or both?

The next challenge is normalizing those artifacts so automation can consume them reliably — not just read them, but parse them, route them, and act on them at scale.

