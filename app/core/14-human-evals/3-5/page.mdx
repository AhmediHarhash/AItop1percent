# 3.5 — Decision Trees and Guided Workflows

Why do most content moderation systems fail to enforce policy consistently? Because the policy is 40 pages long, covers 200 edge cases, and expects reviewers to hold the entire decision logic in their heads while judging content in 30 seconds. The reviewer who has read the policy twice and reviewed 500 items still cannot remember whether a post that glorifies past violence but does not incite future violence is a violation under Section 4.2.7 or an exception under Section 4.3.1. They make a guess. Half the team guesses one way. Half guesses the other. Inter-rater agreement is 71 percent. The policy is clear. The reviewers are competent. The interface is the problem.

A decision tree is an interface that guides the reviewer through policy logic step by step. The interface asks a question. The reviewer answers. The interface asks the next question, which depends on the previous answer. After three to seven questions, the interface reaches a conclusion: approve, reject, flag for escalation. The reviewer never needs to remember the policy. The tree embeds the policy. The reviewer just answers questions.

## When to Use Decision Trees vs Open Forms

Decision trees work best for tasks where the decision logic is hierarchical, where the answer to one question determines which question comes next, and where edge cases can be expressed as branching paths. Content moderation is a natural fit: does the post contain violence? Yes. Is the violence graphic? No. Does it glorify violence? Yes. Conclusion: violation under policy section 4.2.3.

Open forms work best for tasks where the decision logic is flat, where all information is needed regardless of prior answers, or where the judgment is holistic and cannot be decomposed into binary questions. An essay quality review task might ask the reviewer to rate clarity, coherence, argument strength, and evidence quality on a scale from 1 to 5. The rating for clarity does not determine which question comes next. All four dimensions are evaluated independently. A decision tree would add no value. An open form is faster.

The test is: can you express the decision logic as a series of if-then rules? If yes, a decision tree might help. If the logic is "read the whole thing and use your judgment," a tree cannot capture it. Do not force holistic judgment into a tree structure. It will produce either a tree so deep that it is slower than freeform judgment, or a tree so shallow that it misses the nuance the reviewer needs.

A legal contract review task initially used an open form: the reviewer read the contract and labeled clauses as standard, negotiable, or high-risk. The team tried replacing it with a decision tree: does the clause involve indemnification? Does it cap liability? Does it reference third parties? The tree had 14 nodes. Reviewers reported that the tree was slower than freeform review and missed important distinctions that did not fit the tree structure. The team reverted to the open form and added structured guidance in the form of a checklist, not a tree. Reviewers used the checklist as a reference while making holistic judgments. Accuracy improved without the rigidity of a tree.

## Building Trees That Match the Policy

A decision tree is only useful if it matches the policy it is meant to enforce. This seems obvious. It is not. Most trees are built by translating policy language into questions, but policy language is written for lawyers or executives, not for decision trees. The result is a tree that technically represents the policy but is unusable in practice.

A financial services team built a decision tree for flagging suspicious transactions. The policy said: "A transaction is suspicious if it is inconsistent with the customer's known transaction history, or if it involves a high-risk jurisdiction, or if it is structured in a manner consistent with money laundering patterns." The team translated this into a tree: Is the transaction inconsistent with history? Is it in a high-risk jurisdiction? Is it structured suspiciously? The problem: each of these questions requires judgment. The reviewer cannot answer them with a simple yes or no. "Inconsistent with history" is not binary. It is a spectrum. The tree forced binary answers to spectrum questions. Reviewers guessed. Accuracy declined.

The team rebuilt the tree with concrete questions. Instead of "Is the transaction inconsistent with history?" the tree asked: "Is the transaction amount more than three times the customer's median transaction amount?" and "Has the customer transacted with this merchant before?" and "Is this transaction type common for this customer?" Each question was binary and answerable from the data the reviewer saw. The tree combined the answers using the same logic the policy specified, but it decomposed the abstract judgment into concrete sub-questions. Accuracy improved by 23 percent.

The principle is: tree questions must be answerable without requiring the reviewer to make the same holistic judgment the tree is meant to assist. If the tree asks "Is this high-risk?" it has not helped. The reviewer still has to judge risk. The tree should ask the specific observable features that define risk, then combine those features into a conclusion.

## Avoiding Tree Sprawl

The temptation when building a decision tree is to handle every edge case. The policy has 40 exceptions, so the tree has 40 branches. The result is a tree with 200 nodes that takes five minutes to navigate. The reviewer spends more time answering tree questions than they would spend reading the policy and making a direct judgment. The tree becomes a burden, not a tool.

The solution is to build the tree for the common case and provide an escape hatch for edge cases. The tree handles 85 percent of reviews that fit the standard decision paths. For the remaining 15 percent that do not fit, the tree includes an "this does not fit the tree" option at every major branch. The reviewer selects it, provides a freeform explanation, and the item escalates to a senior reviewer or a queue for policy refinement.

A content moderation platform started with a tree that attempted to cover every policy exception. The tree had 180 nodes. Average decision time was 74 seconds. Reviewers hated it. The team rebuilt the tree to cover only the ten most common violation types, which represented 89 percent of flagged content. The tree had 31 nodes. Average decision time was 28 seconds. The 11 percent of reviews that did not fit the tree escalated. The escalation queue became a source of policy improvements: recurring escalations indicated missing tree branches or ambiguous policy. Over six months, the team added five branches to the tree based on escalation patterns. The tree covered 94 percent of reviews without sprawl.

The principle is: optimize the tree for volume, not for completeness. A tree that handles 90 percent of cases in 30 seconds is better than a tree that handles 100 percent of cases in 90 seconds.

## The Maintenance Burden

Decision trees are not static. Policy changes. New edge cases emerge. The tree must be updated to reflect the current policy. If the tree is hard to update, it falls out of sync with policy. Reviewers notice inconsistencies. They stop trusting the tree. They bypass it and make direct judgments. The tree becomes dead code.

The maintenance burden is a function of tree complexity and tree tooling. A tree with 200 nodes maintained in a spreadsheet or a code file is effectively unmaintable. A policy change that affects ten branches requires finding and editing ten nodes manually. Mistakes are common. Testing is manual. A tree with 50 nodes maintained in a visual tree editor is easier. The policy owner drags nodes, edits questions, and publishes changes. The tree is tested automatically. Changes deploy within hours.

A social platform with a 140-node content moderation tree initially maintained it as a JSON file edited by engineers. Policy changes required a ticket, a code review, and a deploy. The time from policy change to tree update averaged nine days. Reviewers worked from outdated trees. The team built a no-code tree editor for the policy team. Policy owners edited the tree directly. Changes deployed after automated testing. Time from policy change to tree update dropped to four hours. Reviewer trust in the tree increased. Accuracy improved because the tree reflected current policy.

The principle is: if policy changes more than once per quarter, the tree must be editable by the policy team without engineering involvement. If the tree requires engineering for every update, it will be perpetually out of date.

## Training Reviewers on Trees

A well-designed tree requires minimal training. The reviewer reads the question, answers it, and moves to the next question. But reviewers still need to understand what the tree is doing. If the tree feels like a black box — the reviewer answers questions and the tree spits out a conclusion — the reviewer does not learn the policy. They become dependent on the tree. When they encounter a case that does not fit the tree, they escalate unnecessarily because they never internalized the policy logic.

The better approach is to make the tree a teaching tool. Each node explains why the question matters. When the tree asks "Is the violence graphic?" it includes a tooltip or sidebar that explains what graphic means and why the distinction matters for policy enforcement. When the tree reaches a conclusion, it displays the policy section that justifies the conclusion. The reviewer sees not just the decision but the reasoning. Over time, they internalize the policy. The tree becomes training, not just a decision aid.

A healthcare compliance review team used decision trees with embedded policy explanations. New reviewers saw full explanations for every question. After 50 reviews, explanations collapsed by default but remained accessible on hover. After 200 reviews, explanations were hidden unless explicitly requested. The team measured policy knowledge using quizzes. Reviewers trained with explanatory trees scored 18 percentage points higher than reviewers trained with bare trees. The explanatory tree group also escalated 29 percent less often, indicating they had internalized the policy and could handle edge cases independently.

## Edge Cases That Break the Tree

No tree is perfect. Some cases are genuinely ambiguous. The answer to a tree question is "it depends" or "both" or "neither." The tree offers binary choices. The reviewer is stuck. If the tree forces a choice, the reviewer picks the least-wrong answer. The data is noisy. If the tree requires escalation for every ambiguous case, the escalation queue overflows.

The solution is to allow the reviewer to annotate ambiguity within the tree. The tree asks "Is the violence graphic?" The reviewer sees options: Yes, No, Uncertain. Selecting Uncertain does not escalate immediately. Instead, the tree continues with a default assumption — treat Uncertain as Yes for policy enforcement — and flags the case for later audit. The tree allows forward progress while preserving the signal that the case was ambiguous. Auditors review flagged cases weekly and refine the tree or the policy based on recurring ambiguity.

A transaction fraud detection tree included an Uncertain option at every question. Reviewers used it in 6 percent of cases. The team analyzed the Uncertain cases monthly. Half were cases where the data was incomplete: the tree asked a question the reviewer could not answer because the necessary information was not displayed. The team updated the context display to surface the missing data. Uncertain usage dropped to 3 percent. The remaining cases were genuinely ambiguous: the reviewer had all the data but could argue the answer either way. The team added three new branches to the tree to handle the most common ambiguous patterns. Uncertain usage dropped to 1 percent. The tree improved incrementally based on real reviewer experience.

## A/B Testing Tree Variants

Decision trees are not exempt from empirical testing. If you have two plausible ways to structure a tree, you can A/B test them. Half of reviewers use Tree A. Half use Tree B. You measure decision time, error rate, inter-rater agreement, and reviewer satisfaction. The data tells you which tree is better.

A legal compliance team debated whether a decision tree should start with document type or risk category. Document-type-first logic: identify the document, then assess risk within that document type. Risk-first logic: identify the risk category, then determine if the document type matters. Both structures represented the same policy. The team could not agree. They built both trees and ran a two-week A/B test with 40 reviewers. Tree A started with document type. Tree B started with risk category. Tree A had an average decision time of 48 seconds and an error rate of 5.1 percent. Tree B had an average decision time of 52 seconds and an error rate of 4.3 percent. Tree B was slightly slower but more accurate. The team chose Tree B. The debate was resolved with data, not opinion.

The principle is: tree design is an empirical question. If two structures are plausible, test both. The tree that produces faster or more accurate decisions wins, regardless of which structure feels more intuitive to the team building it.

## When Simpler Is Smarter

The most common mistake in decision tree design is over-engineering. The team builds a beautiful, comprehensive tree that handles 98 percent of cases and guides reviewers through 12 levels of branching logic. The tree is technically impressive. It is also slower than a simple form with five fields. The reviewer would rather answer five questions in parallel than answer 12 questions sequentially. Parallel input is faster than sequential navigation when the questions are independent.

A customer support quality review task initially used a tree: Was the agent polite? Was the response accurate? Was the issue resolved? Did the agent follow protocol? The tree asked questions one at a time. Average review time was 41 seconds. The team replaced the tree with a form showing all four questions at once with radio buttons for each. Average review time dropped to 29 seconds. Accuracy was unchanged. The tree had added no value. The questions were independent. Answering them in parallel was faster than answering them sequentially.

The rule is: use a tree only if the questions are dependent. If the answer to question one determines whether question two is relevant, a tree helps. If all questions are always relevant, a form is faster. Do not use a tree just because it feels more guided. Guidance has a cost. The cost is only justified when the task benefits from sequential logic.

The next subchapter explores inline feedback and real-time coaching — how to help reviewers improve during review, not just in training sessions.

