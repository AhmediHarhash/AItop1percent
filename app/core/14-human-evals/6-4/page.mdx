# 6.4 — Agreement Metrics: Kappa, Krippendorff, and Beyond

In late 2024, a legal tech company spent five months building a training dataset for contract clause extraction. They hired twelve paralegals to label sample contracts, defining categories for indemnification, liability, termination, and dispute resolution. Inter-rater agreement was measured by simple percentage: on average, two reviewers agreed ninety-one percent of the time. The team was confident. They fine-tuned a model on the dataset, deployed it to a pilot customer, and discovered that the model flagged indemnification clauses in only sixty-three percent of cases where human lawyers expected them. The problem was not the model. The problem was that ninety-one percent agreement was an illusion. Most clauses were easy — boilerplate termination language, standard liability caps — and any competent reviewer would label them identically. The hard cases, where judgment actually varied, had been drowned out by the easy majority. Simple agreement percentage had hidden the fact that the reviewers did not actually agree on the cases that mattered.

Agreement metrics exist to quantify how much reviewers align. But not all agreement is meaningful. Two reviewers who both say "yes" ninety percent of the time will agree eighty-one percent of the time by chance alone. Agreement metrics adjust for this. They measure how much reviewers agree beyond what random guessing would produce. The most common metrics are Cohen's kappa for two raters and Krippendorff's alpha for multiple raters. Both are widely used, widely misunderstood, and frequently misinterpreted. Understanding what they measure — and what they do not measure — is essential to using them correctly.

## Cohen's Kappa and Its Limitations

Cohen's kappa measures agreement between two raters on categorical judgments, correcting for chance agreement. The formula compares observed agreement to expected agreement if both raters were guessing randomly based on the marginal distributions of their labels. A kappa of one means perfect agreement. A kappa of zero means agreement is no better than chance. A kappa below zero means agreement is worse than chance, which usually indicates a miscommunication about what the labels mean.

Kappa is appealing because it is simple and widely known. Most statistical software calculates it automatically. Many academic papers report it. It has become the default agreement metric in annotation projects. But it has limitations that make it unreliable in many real-world scenarios.

The first limitation is sensitivity to prevalence. If one category dominates the dataset, kappa can be misleadingly low even when reviewers agree on most cases. Imagine two reviewers labeling ten thousand customer service messages as either "complaint" or "not complaint." Complaints are rare — only two percent of messages. Both reviewers agree on ninety-nine percent of cases. But one reviewer labels 1.8 percent as complaints, and the other labels 2.2 percent. The disagreement is concentrated in a tiny fraction of cases, but because complaints are so rare, kappa drops to 0.85 or lower. The metric punishes the reviewers for the skewed distribution, not for poor agreement.

The second limitation is sensitivity to marginal distributions. Kappa assumes that disagreement is evenly distributed across categories. If one reviewer is systematically more lenient than the other — saying "yes" thirty percent of the time while the other says "yes" twenty percent of the time — kappa will be lower even if they agree on which specific cases are borderline. The metric conflates systematic bias with random disagreement. This makes it hard to diagnose whether reviewers need better guidelines or just need calibration to align their thresholds.

The third limitation is that kappa treats all disagreements as equally bad. If you have four severity categories — low, medium, high, critical — and one reviewer says "medium" while another says "high," that is a one-step disagreement. If one says "low" and the other says "critical," that is a three-step disagreement. Kappa treats both as identical failures. It is a binary metric: agree or disagree. It does not capture the magnitude of disagreement, which often matters more than the fact of disagreement.

The fourth limitation is that kappa requires at least two raters per case. If you have three raters but they each review different subsets of cases, you cannot calculate a single kappa for the group. You must calculate pairwise kappas and aggregate them, which introduces complexity and makes interpretation harder. This is a practical problem in large operations where not every case is reviewed by every rater.

Despite these limitations, kappa remains useful as a rough heuristic. A kappa above 0.8 generally indicates strong agreement. A kappa between 0.6 and 0.8 indicates moderate agreement. A kappa below 0.6 suggests problems that need investigation. But you should never rely on kappa alone. It is a signal, not a verdict.

## Krippendorff's Alpha for Multiple Raters

Krippendorff's alpha generalizes agreement measurement to multiple raters, missing data, and different data types. It handles the scenarios where kappa breaks: cases where not every rater reviews every item, where you have more than two raters, and where the data is ordinal rather than categorical. Alpha is more flexible and more statistically rigorous than kappa. It is also more computationally expensive and less widely known.

Alpha measures the ratio of observed disagreement to expected disagreement under a null hypothesis of random rating. An alpha of one means perfect agreement. An alpha of zero means agreement is no better than random. Unlike kappa, alpha accounts for the magnitude of disagreement when the data is ordinal or interval. If reviewers are rating severity on a one-to-five scale, a disagreement between one and two is treated as less severe than a disagreement between one and five.

Alpha also handles missing data gracefully. If you have five raters but each case is only reviewed by three, alpha uses all available pairwise comparisons to estimate agreement. This makes it practical for large-scale operations where complete overlap is impossible. You do not need to discard data or force every rater to review every case.

The main limitation of alpha is interpretability. Kappa has become familiar through repetition. Practitioners have intuitions about what a kappa of 0.7 means. Alpha does not have the same cultural penetration. Many stakeholders have never heard of it. Explaining it requires more statistical background. This can make it harder to use alpha as a communication tool, even when it is technically superior.

The second limitation is computational cost. Calculating alpha for a dataset with thousands of cases and dozens of raters requires significant processing time, especially for ordinal or interval data where you must compute weighted disagreements. Most annotation platforms do not calculate alpha automatically. You need custom scripts or statistical software. This is a practical barrier for teams without strong data analysis resources.

The third limitation is that alpha, like kappa, does not tell you why agreement is low. It gives you a number. It does not tell you whether disagreement is concentrated in a few difficult cases, whether certain raters are outliers, or whether specific categories are ambiguous. You need additional analysis to diagnose the root cause. Alpha is a summary metric. It is not a diagnostic tool.

## Weighted Agreement for Ordinal Scales

When your data is ordinal — ratings from one to five, severity levels from low to critical, preference rankings — raw agreement metrics treat all disagreements equally. But in most domains, a one-step disagreement is tolerable and a three-step disagreement is not. Weighted agreement metrics account for this by penalizing large disagreements more than small ones.

The most common weighting scheme is quadratic weights, where the penalty for disagreement increases with the square of the distance between ratings. A one-step disagreement has a penalty of one. A two-step disagreement has a penalty of four. A three-step disagreement has a penalty of nine. This penalizes outliers heavily and treats near-misses leniently. Quadratic weighting is appropriate when small disagreements are noise and large disagreements indicate real problems.

Another weighting scheme is linear weights, where the penalty increases linearly with distance. A one-step disagreement has a penalty of one. A two-step disagreement has a penalty of two. A three-step disagreement has a penalty of three. Linear weighting is appropriate when every step matters equally — for example, when rating severity levels where each increment represents a meaningful increase in risk.

Weighted kappa and weighted alpha incorporate these penalties into the agreement calculation. They give partial credit for near-agreement, which is especially important when guidelines allow for subjective judgment within a range. If your guidelines say that a response rated four or five is acceptable, and reviewers consistently choose within that range but not always the same value, weighted metrics will reflect that they are aligned on the substance even if they differ on the precise number.

The challenge with weighted metrics is choosing the weights. Quadratic weights are conventional, but they are not always justified. If you cannot articulate why a two-step disagreement should be four times as bad as a one-step disagreement, quadratic weights are arbitrary. Linear weights are safer when in doubt, because they make fewer assumptions about the relative importance of each step. But the best approach is to define weights based on the task's actual consequences. If a one-step disagreement costs you nothing and a two-step disagreement costs you ten thousand dollars in mislabeled data, use weights that reflect that ratio.

## When Agreement Metrics Mislead

Agreement metrics are summary statistics. They reduce a complex, multidimensional dataset to a single number. This makes them useful for high-level monitoring and for comparing across tasks or teams. But the compression also hides structure. High agreement on average can coexist with severe disagreement on specific subsets of cases. Low agreement overall can mask perfect agreement on ninety-five percent of cases and total chaos on five percent. The metric does not tell you which scenario you are in.

The first failure mode is averaging over heterogeneity. If you have two tasks in your dataset — one where reviewers agree ninety-eight percent of the time, and one where they agree sixty percent of the time — the aggregate kappa might be 0.75. That number makes it look like you have moderate, uniform disagreement. In reality, you have one task that works and one that does not. The aggregate metric conceals the problem. You need to calculate agreement separately for each task to see where the issue lies.

The second failure mode is ignoring systematic bias. Agreement metrics measure consistency, not correctness. Two reviewers can have high agreement and both be wrong. If they misunderstand the guidelines in the same way, they will agree with each other but disagree with ground truth. Kappa will be high. Quality will be low. This is especially dangerous in domains where ground truth is hard to verify. You might operate for months with high inter-rater agreement, never realizing that your entire team has drifted away from the intended standard.

The third failure mode is ignoring outliers. Agreement metrics are averages. If ninety-five percent of reviewers agree and five percent are confused, the metric averages over the confusion. It will show good agreement. But the five percent of confused reviewers are still producing bad labels. If those labels end up in your training set, they introduce noise. The metric did not warn you because it is designed to summarize the group, not to identify individuals.

The fourth failure mode is using agreement as a proxy for quality. Agreement is necessary but not sufficient. If reviewers agree, it means the task is well-defined and the training is working. It does not mean the labels are correct. You still need ground truth validation, expert review, and downstream quality checks. High agreement gives you confidence in consistency. It does not give you confidence in correctness. Teams often confuse the two.

The fifth failure mode is setting arbitrary thresholds. The literature suggests that kappa above 0.8 is "strong agreement" and kappa above 0.6 is "moderate agreement." These thresholds are conventions, not laws of nature. They come from medical diagnosis research in the 1970s, where those levels were found to be clinically useful. They may not apply to your domain. A kappa of 0.6 might be excellent for a highly subjective task like ranking conversational tone. It might be unacceptable for a mechanical task like verifying that a citation matches a source. Context matters. Do not blindly apply thresholds without understanding what they mean for your specific use case.

## Interpreting Agreement Scores in Context

Agreement metrics are most useful when interpreted relative to a baseline. The baseline might be historical: is agreement higher or lower than it was three months ago? The baseline might be cross-task: is agreement on Task A comparable to agreement on Task B? The baseline might be theoretical: is agreement high enough to support the decisions you need to make with the data?

If you are building a training dataset for fine-tuning, you need high agreement. Low agreement means the signal is noisy, and the model will learn noise. A kappa of 0.7 might be acceptable for exploratory data analysis but unacceptable for training. You need to set thresholds based on how the data will be used.

If you are using human review to override model predictions, you can tolerate lower agreement. The goal is not perfect consistency. The goal is to catch egregious errors. Even if reviewers disagree on borderline cases, they will likely agree on the cases where the model failed badly. A kappa of 0.6 might be sufficient because you are using human review as a safety net, not as a precision instrument.

If you are measuring inter-rater agreement to validate a new task or guideline, low agreement is diagnostic information. It tells you that the task is not yet well-defined. Reviewers are interpreting instructions differently. You need to refine the guidelines, add examples, or provide more training. The metric is not a failure. It is a signal that work remains.

Agreement metrics are tools, not answers. They tell you whether reviewers are aligned. They do not tell you whether reviewers are correct, whether the task is well-designed, or whether the data is useful. You need human judgment to interpret the numbers. A kappa of 0.85 might be cause for celebration or cause for concern, depending on the domain, the stakes, and the baseline. The number is the starting point for investigation, not the conclusion.

The next subchapter covers annotator calibration: how to align reviewers at the start of a project, how to recalibrate when drift occurs, and how to maintain consistency across teams and over time.
