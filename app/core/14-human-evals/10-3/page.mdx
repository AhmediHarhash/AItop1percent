# 10.3 — Quality Metrics: Accuracy, Consistency, and Agreement

In August 2025, a healthcare AI company discovered that 14% of their training data was mislabeled. The labels came from a contract review operation that had been running for nine months. The operation completed 3,000 tasks per week, hit all throughput targets, and stayed within budget. No one had checked whether the labels were correct. When the data science team finally ran a quality audit on a random sample of 800 labeled examples, they found that 112 labels were objectively wrong — medications marked as diagnoses, symptoms attributed to the wrong body system, treatment recommendations contradicting clinical guidelines. The team had trained four models on this data. All four models learned the errors and reproduced them in production. The company spent $780,000 on the labeling operation and created an asset that made their models worse.

The mistake was treating throughput as success. The operation delivered volume on schedule and within budget. It did not deliver quality. Quality was assumed but not measured. By the time the errors were discovered, the models were deployed, the mislabeled data was mixed into a 60,000-example dataset, and disentangling good labels from bad labels required re-reviewing everything from scratch. The cost of not measuring quality was ten times the cost of the original labeling operation.

## Accuracy: Measuring Correctness Against Ground Truth

**Accuracy** is the percentage of reviewer judgments that match a known-correct answer. If a reviewer labels 100 examples and 87 of those labels match expert-validated ground truth, accuracy is 87%. Accuracy is the most direct measure of whether reviewers are producing correct outputs. It answers the question: when this person makes a judgment, how often are they right?

Accuracy requires ground truth. You cannot measure accuracy without a set of labeled examples where the correct answer is known and verified. Ground truth comes from three sources: expert review, where domain specialists label examples and their judgments are treated as definitive; consensus labels, where multiple high-quality reviewers independently label the same examples and the majority vote becomes ground truth; or objective verification, where correctness can be confirmed through external data sources like medical records, transaction logs, or regulatory databases.

The challenge is that ground truth is expensive. Expert review costs three to ten times more than standard review. Consensus labeling requires three to five reviewers per example instead of one. Objective verification requires integration with external systems and manual validation. Most teams cannot afford to generate ground truth for every task. The practical solution is to generate ground truth for a **calibration set** — a statistically representative sample of 500 to 2,000 examples that covers the full distribution of task types, edge cases, and difficulty levels — and measure reviewer accuracy against that set.

Accuracy metrics are meaningful only when segmented by task difficulty. A reviewer with 92% accuracy on simple binary classifications and 68% accuracy on ambiguous multi-class edge cases is not a 92% accurate reviewer. They are highly accurate on easy tasks and unreliable on hard ones. If you assign them to a queue that is 70% hard tasks, their effective accuracy is closer to 75%. Segmented accuracy by difficulty tier tells you which reviewers are qualified for which work.

Accuracy also degrades over time. A reviewer who scores 91% accuracy in their first week often drifts to 84% accuracy by month three as they develop shortcuts, forget edge case rules, or stop consulting the rubric. Measuring accuracy once during onboarding is not sufficient. Elite operations measure accuracy continuously through **spot checks**: randomly sample 3-5% of each reviewer's completed tasks every week, compare their labels to ground truth or expert review, and track accuracy trends over time. A reviewer whose accuracy is declining needs retraining. A reviewer whose accuracy is consistently below threshold needs reassignment or removal.

## Consistency: Measuring Stability Over Time

**Consistency** measures whether a reviewer produces the same judgment when shown the same example multiple times. If a reviewer labels an example as positive on Monday and negative on Friday when shown the identical example again, they are inconsistent. Consistency is distinct from accuracy. A reviewer can be consistently wrong — producing the same incorrect judgment every time — or inconsistently right — sometimes producing the correct judgment and sometimes not. Ideal reviewers are both accurate and consistent.

Consistency is measured through **repeat task injection**. Take 50 examples that a reviewer has already labeled, strip the metadata so they do not recognize them as duplicates, and inject them back into their queue two to four weeks later. Compare the new labels to the original labels. If 46 out of 50 labels match, consistency is 92%. If only 38 out of 50 match, consistency is 76% — the reviewer is producing different judgments on the same examples 24% of the time.

Low consistency indicates one of three problems. First, the reviewer may not fully understand the rubric and is making judgments based on intuition or mood instead of criteria. Second, the rubric itself may be ambiguous — examples that seem identical to the reviewer may have subtle differences that the rubric does not clarify, forcing the reviewer to guess. Third, the reviewer may be experiencing cognitive fatigue, attention drift, or burnout, which causes performance to vary based on time of day, workload, or stress level.

Consistency also reveals rubric quality. If every reviewer in a cohort has consistency below 80% on a specific task type, the rubric is broken. The instructions are not clear enough, the criteria are not specific enough, or the edge case handling is missing. A good rubric produces high consistency even across different reviewers. A bad rubric produces inconsistency even within a single reviewer over time. Consistency metrics are a diagnostic for both reviewer performance and rubric design.

## Inter-Rater Agreement: Measuring Alignment Across Reviewers

**Inter-rater agreement** measures how often two or more reviewers produce the same judgment when labeling the same example independently. If three reviewers label 200 examples and all three agree on 168 of them, raw agreement is 84%. Inter-rater agreement tells you whether the review process produces stable outputs or whether different reviewers interpret the rubric differently and produce conflicting judgments.

Agreement is typically measured using **Cohen's kappa** for two reviewers or **Fleiss' kappa** for three or more. Kappa adjusts for chance agreement — the percentage of labels that would match even if reviewers were guessing randomly. A kappa of 0.80 or higher indicates strong agreement. A kappa between 0.60 and 0.80 indicates moderate agreement. A kappa below 0.60 indicates weak agreement and suggests that the review process is not reliable.

Low inter-rater agreement has serious consequences. If two reviewers label the same example differently 30% of the time, the training data you build from their work is contradictory. A model trained on that data learns conflicting signals. If one reviewer marks certain examples as positive and another marks identical examples as negative, the model cannot learn a coherent decision boundary. Low-agreement data produces models with high error rates and unpredictable behavior.

Inter-rater agreement should be measured continuously, not just during calibration. Every week, assign a batch of 50-100 examples to be labeled independently by two or three reviewers. Calculate kappa on those overlapping examples. Track kappa trends over time. A decline in kappa indicates that reviewers are drifting apart — either because they are forgetting the rubric, interpreting edge cases differently, or developing individual shortcuts that diverge from the standard process. When kappa drops below 0.75, the team needs recalibration: group review sessions where reviewers discuss disagreements, clarify rubric language, and realign on edge case handling.

Agreement also varies by task difficulty. Simple binary tasks often achieve kappa above 0.90. Complex multi-dimensional judgments with subjective criteria may achieve kappa of only 0.65 even with expert reviewers. This does not mean the reviewers are bad — it means the task is inherently ambiguous. When agreement is low because the task is hard, the solution is not to retrain reviewers — it is to refine the rubric, add more examples, or accept that certain tasks require expert adjudication instead of crowd consensus.

## The Quality Composite Score

Elite operations do not optimize for accuracy alone, consistency alone, or agreement alone. They optimize for a **composite quality score** that balances all three dimensions. A typical composite formula weights accuracy at 50%, consistency at 25%, and inter-rater agreement at 25%. A reviewer with 90% accuracy, 85% consistency, and kappa of 0.80 with peers has a composite score of 87%. A reviewer with 94% accuracy but 70% consistency and kappa of 0.65 has a composite score of 82% — higher accuracy but lower overall quality because their work is unstable and diverges from team norms.

The composite score serves two purposes. First, it provides a single number for ranking reviewers, assigning high-stakes work, and making staffing decisions. A reviewer with a composite score above 85% is qualified for expert-tier work. A reviewer with a score between 75% and 85% is qualified for standard work but should not handle edge cases. A reviewer with a score below 75% needs retraining or reassignment. Second, the composite score prevents gaming. A reviewer cannot optimize for accuracy by always choosing the safest judgment if that strategy reduces consistency or agreement. The score requires balance across all quality dimensions.

The weights in the composite formula should reflect your priorities. If accuracy is paramount — such as in medical labeling where errors have patient safety consequences — weight accuracy at 60% or 70%. If consistency is critical — such as in content moderation where policy enforcement must be uniform — weight consistency higher. If alignment across a large distributed team is the primary challenge, weight agreement higher. The formula is not universal. It is a policy decision that encodes what quality means for your operation.

## Quality Segmentation by Task Type and Reviewer Tier

Aggregate quality metrics hide critical details. A team with an average accuracy of 88% sounds competent until you segment by task type and discover that accuracy on simple tasks is 96% but accuracy on complex tasks is 71%. The aggregate hides the fact that the team is failing on the work that matters most. Quality metrics must be segmented by task difficulty, task type, and reviewer experience level to be actionable.

Segment quality by **task difficulty tier**. Tier 1 tasks are simple and objective. Tier 2 tasks require judgment but have clear rubric guidance. Tier 3 tasks involve ambiguity, edge cases, or domain expertise. Measure accuracy, consistency, and agreement separately for each tier. Expect accuracy to decline as difficulty increases — 94% on Tier 1, 86% on Tier 2, 78% on Tier 3 is normal. The goal is not to achieve identical quality across all tiers. The goal is to maintain quality above threshold for each tier and route tasks to reviewers qualified for that difficulty level.

Segment quality by **reviewer experience**. New reviewers in their first month should achieve 80% accuracy on Tier 1 tasks. Experienced reviewers with six months of tenure should achieve 90% accuracy on Tier 2 tasks. Expert reviewers with deep domain knowledge should achieve 85% accuracy on Tier 3 tasks. If a new reviewer is achieving only 65% accuracy on Tier 1 tasks after two weeks of training, they are not learning fast enough and may not be a good fit. If an experienced reviewer plateaus at 82% accuracy after six months, they have hit their ceiling and should not be promoted to harder work.

Segment quality by **task type**. A reviewer might be excellent at content moderation but mediocre at legal document review. Another might excel at medical annotation but struggle with financial data labeling. Quality is not a universal trait — it is domain-specific and task-specific. Measure quality separately for each major task type and use those metrics to assign reviewers to the queues where they perform best. A reviewer with 91% accuracy on medical tasks and 74% accuracy on financial tasks should work exclusively on medical tasks. Forcing them into financial work wastes capacity and degrades output quality.

## The Quality Floor and the Performance Improvement Plan

Every review operation needs a **quality floor**: the minimum acceptable composite quality score for continued employment. A typical floor is 75% for new reviewers in their first 90 days and 80% for experienced reviewers. A reviewer who falls below the floor for two consecutive weeks enters a **performance improvement plan**: targeted retraining, daily coaching, and increased spot-check frequency. If quality does not improve to above-floor within four weeks, the reviewer is removed from production work.

This sounds harsh. It is necessary. A reviewer operating at 72% quality produces more harm than value. Their incorrect labels corrupt training data, their inconsistent judgments confuse models, and their low agreement with peers creates noise that other reviewers must clean up. Keeping a low-quality reviewer on the team is expensive and demoralizing. It signals to high-performers that quality does not matter. It forces operations leads to spend time on remediation instead of scaling. The kindest thing you can do for a struggling reviewer is to identify the problem early, give them focused support, and exit them quickly if they cannot improve.

The quality floor also protects the operation from vendor risk. If you outsource review work to a third-party vendor, the contract should include minimum quality SLAs: 85% accuracy, 80% consistency, and kappa of 0.75 or higher. If the vendor falls below those thresholds for two consecutive weeks, you have the right to terminate the contract without penalty. This prevents the vendor from cutting costs by hiring under-qualified reviewers or reducing training investment. Quality SLAs make vendors accountable for outcomes, not just throughput.

## Quality Metrics in Real-Time Dashboards

The quality dashboard that matters for daily operations shows four numbers updated every 24 hours: average accuracy over the last seven days, average consistency over the last seven days, inter-rater kappa over the last seven days, and percentage of reviewers currently below the quality floor. These four numbers tell operations leads whether quality is stable or degrading.

If average accuracy is trending down, the team is drifting or the task mix is getting harder. If consistency is dropping, reviewers are fatigued or the rubric is ambiguous. If kappa is declining, the team is diverging and needs recalibration. If more than 10% of reviewers are below the quality floor, you have a systemic training problem or a hiring problem. Any one of these signals requires investigation. Two or more together indicate a quality crisis that requires immediate intervention.

The dashboard also needs **per-reviewer quality scorecards** that show individual accuracy, consistency, agreement, and composite score, updated weekly. Operations leads review these scorecards every Monday to identify who needs coaching, who is ready for promotion to harder work, and who is trending toward the quality floor. Scorecards make quality visible and create accountability. A reviewer who sees their accuracy drop from 89% to 81% over three weeks knows they need to focus on improvement. A reviewer who sees their kappa rise from 0.72 to 0.82 knows their calibration work is paying off.

Elite operations also publish **quality leaderboards** that rank reviewers by composite score and highlight top performers. Leaderboards create healthy competition and make high quality visible and rewarded. But leaderboards are dangerous if they are the only metric that matters. A reviewer who optimizes for leaderboard position might cherry-pick easy tasks, avoid ambiguous cases, or spend excessive time on each task to maximize accuracy at the cost of throughput. The leaderboard must be balanced with throughput metrics and fairness adjustments for task difficulty to prevent gaming.

## When Quality Metrics Lie

Quality metrics are only as good as the ground truth they are measured against. If ground truth is wrong, accuracy is meaningless. If ground truth is stale — generated two years ago under a different rubric — accuracy against that ground truth does not reflect current quality. If ground truth is unrepresentative — skewed toward easy examples or common cases — accuracy on ground truth overstates performance on real tasks.

A legal tech company measured reviewer accuracy against a ground truth set of 600 labeled contracts. Accuracy was consistently above 90%. The team felt confident. Six months later a client audit discovered that 18% of contract labels in production were incorrect. The problem: the ground truth set was created in 2023 and reflected contract language from that era. By 2026, contract templates had evolved, new clauses had become common, and edge cases that were rare in 2023 were frequent in 2026. The reviewers were accurate against outdated ground truth but inaccurate on current work.

The fix is to refresh ground truth regularly. Every quarter, generate a new calibration set from recent tasks. Retire old ground truth examples that no longer reflect the current task distribution. Keep the calibration set aligned with production reality. Quality metrics measured against stale ground truth are worse than no metrics — they create false confidence and hide real problems.

Quality metrics also lie when ground truth is low quality. If you create consensus-based ground truth by having three junior reviewers vote on each example, the consensus may be wrong. Measuring accuracy against incorrect ground truth rewards reviewers who make the same mistakes as the consensus and penalizes reviewers who are actually more accurate. Ground truth must be created by the best reviewers or by domain experts. Using low-quality annotators to create ground truth is circular — you end up measuring how well reviewers match the mistakes of other reviewers.

## The Integration of Quality and Throughput

The teams that win do not optimize quality or throughput. They optimize both simultaneously. The operational target is not "maximize accuracy" — it is "achieve 88% accuracy while completing 1,500 tasks per day." The goal is not "maximize throughput" — it is "complete as many tasks as possible while maintaining accuracy above 85% and kappa above 0.78."

This requires real-time feedback loops. If quality drops below threshold, throttle throughput until quality recovers. If quality is stable and throughput is below capacity, increase task assignment rates to utilize available capacity. If quality is high but throughput is low, investigate whether reviewers are over-checking their work, whether the rubric is too complex, or whether task routing is inefficient. Quality and throughput are linked — changes in one affect the other, and both must be monitored together to keep the operation healthy.

The ultimate metric is **quality-adjusted throughput**: the number of high-quality tasks completed per week. A reviewer who completes 500 tasks at 92% accuracy delivers 460 quality-adjusted tasks. A reviewer who completes 700 tasks at 76% accuracy delivers 532 quality-adjusted tasks — higher raw throughput but only marginally higher quality-adjusted output. A third reviewer who completes 600 tasks at 88% accuracy delivers 528 quality-adjusted tasks. The second and third reviewers are roughly equivalent in value despite different throughput and quality profiles. Quality-adjusted throughput makes trade-offs explicit and enables fair performance comparisons across reviewers with different speed-quality profiles.

Quality metrics — accuracy, consistency, and inter-rater agreement — tell you whether the work your reviewers produce is trustworthy enough to train models, make product decisions, or satisfy regulatory requirements. Without quality measurement, review operations are theater. With quality measurement, they become the foundation of reliable AI systems. The next subchapter covers latency metrics: how to measure time-to-completion, identify bottlenecks, and ensure review operations meet the speed requirements of the teams that depend on them.
