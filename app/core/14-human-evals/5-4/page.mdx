# 5.4 — Sampling Strategies When 100 Percent Review Is Impossible

Why do most AI companies abandon systematic review within six months of scaling? Because they try to review everything, hit the volume wall, and conclude that review is too expensive to sustain. The assumption that you need to review every output is the mistake. You don't. What you need is statistical confidence that your quality bar is being met, that your error rate is within tolerance, and that your model isn't degrading silently. You can achieve all three by reviewing a sample — if you design the sample correctly, measure coverage, and communicate the limits clearly to stakeholders.

Sampling is not a compromise. It's a deliberate strategy for maintaining quality oversight at scale. A well-designed sample of 500 cases tells you more about model performance than a poorly-reviewed batch of 10,000 cases where reviewers are rushing to meet SLA. The goal is not to catch every error — that's infeasible at scale. The goal is to detect patterns of error at a rate that allows you to intervene before those errors cause material harm. Sampling lets you do that without drowning your review team.

The mistake most teams make is sampling randomly and calling it done. Random sampling is one strategy, but it's not always the right one. Different sampling strategies optimize for different goals: detecting rare failures, measuring performance across subgroups, catching drift, validating high-risk outputs, or balancing cost against coverage. The right sampling strategy depends on what you're trying to learn and what level of confidence you need.

## Random Sampling

Random sampling is the baseline. You select cases uniformly at random from your output distribution, review them, and use the results to estimate overall quality. If you review 500 random cases and find 15 errors, you estimate your error rate at 3 percent, plus or minus a confidence interval that depends on sample size. Random sampling works when your output distribution is relatively uniform and when you care about overall quality more than performance on specific subgroups.

The advantage of random sampling is simplicity and unbiasedness. Every case has an equal chance of being selected, so the sample represents the population without distortion. The disadvantage is that random sampling undersamples rare cases. If 2 percent of your outputs involve medical advice and you sample 500 cases, you'll only see about 10 medical cases. That's not enough to measure quality for that subgroup with any confidence.

The statistical rule for random sampling is that your confidence interval is proportional to one divided by the square root of sample size. A sample of 100 cases gives you a confidence interval of roughly plus or minus 10 percentage points. A sample of 400 cases gives you plus or minus 5 percentage points. A sample of 1,600 cases gives you plus or minus 2.5 percentage points. The cost of tighter confidence grows quadratically — to halve your confidence interval, you need to quadruple your sample size.

For most AI systems, a random sample of 400 to 800 cases per week gives you sufficient confidence to detect changes in overall error rate. If your baseline error rate is 3 percent and it jumps to 5 percent, you'll detect that with 400 cases. If your baseline is 3 percent and it jumps to 3.5 percent, you won't detect that reliably until you have 2,000+ cases. Decide how large a change you need to detect, calculate the required sample size, and commit to reviewing that many cases consistently every week.

## Stratified Sampling

Stratified sampling divides your output into subgroups — by task type, by user segment, by risk level, by language — and samples proportionally or disproportionately from each subgroup. This ensures that rare but important subgroups are adequately represented in your sample, even if they're a small fraction of overall volume. Stratified sampling is essential when different subgroups have different quality requirements or different error distributions.

For example, if 80 percent of your cases are low-risk customer support and 20 percent are high-risk financial advice, a random sample of 500 cases gives you 400 support cases and 100 financial cases. That's enough to measure support quality, but marginal for financial advice. With stratified sampling, you might sample 200 support cases and 300 financial cases, overweighting the high-risk category to ensure sufficient coverage. You then compute overall metrics by weighting each subgroup by its true proportion in the population.

The advantage of stratified sampling is precision for subgroups. You can measure quality for each segment independently and detect issues that would be invisible in a random sample. The disadvantage is complexity — you need to define strata, track volume per stratum, and allocate review capacity accordingly. You also need to decide whether to sample proportionally, where each stratum is sampled in proportion to its population share, or disproportionately, where high-priority strata are oversampled.

Disproportionate stratified sampling is the right choice when some subgroups are more important than others. High-risk cases, low-confidence outputs, new model versions, or sensitive content categories should be oversampled relative to their population share. Low-risk, high-confidence cases can be undersampled. The result is a sample that gives you high confidence on the cases that matter most, even if it gives you less precision on routine cases.

## Risk-Based Sampling

Risk-based sampling prioritizes cases by potential impact. High-risk outputs are reviewed at higher rates than low-risk outputs. This is not statistically representative — your sample is biased toward risk — but that's the point. You're not trying to estimate overall quality. You're trying to catch the failures that cause the most harm.

Risk scoring can be model-driven, rule-driven, or hybrid. A model might assign a risk score based on output characteristics: mentions of regulated topics, low confidence, factual claims, sensitive entities, or policy flags. A rule might assign high risk to any output that mentions financial advice, medical information, or legal guidance. The risk score determines the sampling rate: high-risk outputs are reviewed at 50 to 100 percent, medium-risk at 10 to 20 percent, low-risk at 1 to 5 percent.

The advantage of risk-based sampling is efficiency. You focus review capacity on the cases where errors matter most. The disadvantage is that you lose visibility into low-risk cases. If your model is silently degrading on low-risk outputs, you might not detect it until the degradation spreads to higher-risk cases. The fix is to combine risk-based sampling with periodic random sampling. Most of your review capacity goes to high-risk cases, but you also review a small random sample of low-risk cases to monitor baseline quality.

A real example: a healthcare AI in 2025 used risk-based sampling to prioritize drug interaction warnings and symptom assessment outputs, which were reviewed at 100 percent. General health information outputs were reviewed at 10 percent. Appointment scheduling and administrative outputs were reviewed at 2 percent. This allocated 70 percent of review capacity to the 15 percent of outputs that carried the most risk, while maintaining statistical oversight of the rest. The team caught 94 percent of consequential errors while reviewing only 18 percent of total outputs.

## Confidence-Based Sampling

Confidence-based sampling uses model confidence scores to decide what to review. Outputs with low confidence are reviewed at high rates. Outputs with high confidence are reviewed at low rates. This strategy assumes that the model's confidence correlates with correctness, which is often true but not always. Confidence-based sampling is effective when your model is well-calibrated — when a confidence score of 0.90 actually means the output is correct 90 percent of the time.

The failure mode of confidence-based sampling is overconfidence. If your model assigns high confidence to incorrect outputs — which happens with hallucination, with adversarial inputs, or with distribution shift — you'll undersample exactly the cases where errors occur. The fix is to **validate calibration regularly**. Pull a random sample, measure accuracy per confidence bucket, and verify that 90 percent confidence actually corresponds to 90 percent accuracy. If your model is poorly calibrated, recalibrate it or switch to a different sampling strategy.

When calibration is good, confidence-based sampling is highly efficient. Outputs with confidence above 0.95 can be auto-approved with minimal review. Outputs with confidence between 0.80 and 0.95 are sampled at 10 to 20 percent. Outputs with confidence below 0.80 are reviewed at 100 percent. This focuses human review on the cases where the model is uncertain, which is where human judgment adds the most value.

Confidence-based sampling pairs well with risk-based sampling. You might auto-approve low-risk, high-confidence outputs, review medium-risk or medium-confidence outputs at 20 percent, and review anything high-risk or low-confidence at 100 percent. The combination creates a two-dimensional decision matrix: risk on one axis, confidence on the other, with sampling rates assigned to each cell.

## Statistical Validity and Sample Size

The question every stakeholder asks is: how many cases do you need to review to be confident in your results? The answer depends on three variables: the baseline error rate, the size of change you want to detect, and your desired confidence level. For most practical purposes, 95 percent confidence is standard.

If your baseline error rate is 3 percent and you want to detect a change to 5 percent with 95 percent confidence, you need roughly 800 cases per measurement period. If your baseline error rate is 1 percent and you want to detect a change to 2 percent, you need roughly 2,400 cases. The lower your baseline error rate and the smaller the change you want to detect, the larger your required sample size.

The formula for sample size is available in any statistics textbook, but the intuition is straightforward: smaller error rates require larger samples, and detecting smaller changes requires larger samples. If you're measuring something rare — say, a 0.1 percent rate of catastrophic errors — you need tens of thousands of cases to detect changes reliably. If you're measuring something common — say, a 10 percent rate of minor quality issues — you can detect changes with hundreds of cases.

The mistake most teams make is sampling too little and over-interpreting the results. A sample of 50 cases tells you almost nothing. A sample of 200 cases gives you rough directional signal. A sample of 500 cases gives you enough confidence to act. A sample of 1,000+ cases gives you high confidence for overall metrics and moderate confidence for subgroup analysis. Know your sample size, know your confidence interval, and don't claim precision you don't have.

## Communicating Sampling to Stakeholders

The hardest part of sampling is not the statistics — it's convincing stakeholders that you don't need to review everything. Product wants 100 percent coverage. Legal wants 100 percent coverage. Leadership wants 100 percent coverage. Your job is to explain that 100 percent coverage is not feasible, not necessary, and often counterproductive, because rushing through 10,000 cases produces worse results than carefully reviewing 800.

The way to communicate sampling is to frame it in terms of confidence and risk tolerance. "We review 800 cases per week, which gives us 95 percent confidence that we'll detect any change in error rate larger than 1.5 percentage points. Our current error rate is 2.8 percent. If it rises above 4.3 percent, we'll detect it within one week and intervene. We've chosen this sample size to balance cost, speed, and confidence. Reviewing more cases would give us tighter confidence intervals, but the marginal gain is not worth the cost. Reviewing fewer cases would leave us blind to meaningful changes."

The second part of the communication is acknowledging what sampling cannot do. "Sampling does not catch every error. It detects patterns of error at a population level. If we have one catastrophic failure in 10,000 outputs and we review 800, we might miss it. If we have a systematic issue that affects 5 percent of outputs, we'll catch it. Our sampling strategy is designed for the second case, not the first. For the first case, we rely on risk-based filtering and user reporting."

The third part is showing the trade-off. "If we reviewed 100 percent of outputs, we'd need 30 reviewers instead of 6, and our cost per case would triple. We'd also introduce latency, because 30 reviewers can't stay calibrated as easily as 6. The error rate from reviewer inconsistency would exceed the error rate we're trying to prevent. Sampling lets us maintain high-quality review at scale without collapsing under volume."

Stakeholders who understand statistics will accept this immediately. Stakeholders who don't understand statistics will push back. Your job is to be firm, show the numbers, and demonstrate that sampling works by tracking metrics over time. After six months of consistent sampling with stable quality metrics, the pushback stops.

Sampling is not a way to avoid review — it's a way to do review intelligently at scale. Random sampling gives you unbiased population estimates. Stratified sampling gives you subgroup precision. Risk-based sampling focuses on high-impact cases. Confidence-based sampling leverages model uncertainty. The right strategy depends on your goals, your volume, and your risk tolerance. But every strategy beats the alternative, which is reviewing nothing because reviewing everything is impossible.

Next, we'll cover automation-assisted review, where models pre-filter cases and let humans focus only on the decisions that need judgment.

