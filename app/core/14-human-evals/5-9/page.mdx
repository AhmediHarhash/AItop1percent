# 5.9 — Surge Capacity: Handling Review Spikes

The review queue had been stable for months — 2,000 to 2,500 cases per day, handled comfortably by a team of 18 reviewers. Then a new feature launched, user adoption spiked, and the queue hit 9,000 cases overnight. The team worked overtime, cleared 4,000 cases, and started the next day with 5,000 still pending. By day three, the backlog was 11,000 cases. By day five, the team was burned out, quality was slipping, and escalations were being ignored because no one had time to investigate them. The company had built a review operation optimized for steady state. They had no plan for surge.

Surge capacity is not optional. Every AI system that depends on human review will face demand spikes: a viral post that generates millions of outputs, a product launch that triples usage, a policy change that requires re-reviewing historical data, a security incident that triggers a manual audit of every flagged case in the last 90 days. If your review infrastructure cannot handle a 3x spike in volume without collapsing, you do not have a production system. You have a prototype that works only under ideal conditions.

The mistake most teams make is assuming surge capacity means hiring more people. It does not. Hiring takes weeks, training takes weeks, and by the time new reviewers are productive, the surge is over. Real surge capacity is pre-built. You know where additional capacity will come from, you know how quickly you can activate it, and you know how to maintain quality while volume is elevated. Surge planning is not about reacting. It is about anticipating and preparing.

## Demand Forecasting for Spikes

The first step is predicting when surges will happen. Some surges are predictable: product launches, seasonal traffic increases, marketing campaigns. Others are not: viral content, security incidents, breaking news that drives user behavior changes. You plan differently for each.

For predictable surges, you have weeks of lead time. A product team planning a major launch in Q4 should tell the review operations team in Q3. That gives the review team time to build surge capacity before demand hits. The review team should ask the product team for volume projections: What is expected daily usage growth? What is the expected increase in flagged cases or edge cases? What is the expected duration of elevated volume? Vague answers like "we expect a lot more traffic" are not useful. Demand a number: is it 2x baseline, 5x baseline, 10x baseline? The answer determines whether you need to add 5 reviewers or 50.

For unpredictable surges, you rely on leading indicators. If your review queue is driven by user-generated content, track daily submission volume. If submissions increase 40 percent over two days, expect flagged cases to increase proportionally within the next 48 hours. If your review queue is driven by customer support escalations, track ticket volume and sentiment. A spike in angry tickets predicts a spike in complex escalations that require senior reviewer time. If your review queue is driven by adversarial inputs, track attack attempts. A 3x increase in jailbreak attempts means your red-team review load is about to spike.

Build a dashboard that shows volume trends with 7-day and 30-day moving averages. When daily volume exceeds the 7-day average by 30 percent, activate surge protocols. Do not wait until the backlog is unmanageable. Act when you see the trend, not when you feel the pain.

## Building Surge Capacity: Three Models

Surge capacity comes from three sources: on-call reviewers, cross-trained internal staff, and vendor partners. Each model has different activation time, cost, and quality trade-offs.

**On-call reviewers** are former full-time reviewers who have moved to other roles but remain available for surge work. They are already trained, already calibrated, and already familiar with your rubric. They can start reviewing within hours. The constraint is availability: most people who left the review team did so because they wanted to do something else, and they will not drop everything to come back for a week. You can maintain a pool of 5 to 10 on-call reviewers, but do not expect all of them to be available at once. Treat on-call as a 2x surge buffer, not a 10x buffer.

Compensation for on-call reviewers should be structured as retainer plus per-case pay. Pay a small monthly retainer — $500 to $1,000 — just to keep them in the pool and committed to being available. Then pay per case reviewed during surges, at a rate 30 to 50 percent higher than standard reviewer pay. This structure ensures they are financially motivated to prioritize surge work when it comes up, but it does not cost much when surge capacity is not needed.

**Cross-trained internal staff** are employees in adjacent roles who can do basic review work but are not full-time reviewers. Customer support agents, trust and safety specialists, and product operations staff are common candidates. They do not have the depth of a dedicated reviewer, but they can handle first-pass reviews or low-complexity cases, freeing up dedicated reviewers to focus on edge cases.

Cross-training requires planning. You cannot pull someone from customer support with zero notice and expect them to review cases at 90 percent accuracy. You need to train them in advance. Once per quarter, run a half-day training session where adjacent staff learn the review rubric, practice on test cases, and get calibrated. After training, they are on standby. When a surge hits, you activate them. They log into the review tool, work through low-complexity cases, and escalate anything ambiguous to the dedicated team. They are 60 percent as fast as a trained reviewer, but they are available immediately.

A SaaS company keeps 12 customer support agents cross-trained for review surges. During normal operations, these agents never touch review work. When the review queue exceeds 5,000 cases, the support manager assigns them to review for one or two hours per day until the backlog clears. They handle simple cases: checking for obvious policy violations, flagging outputs that are clearly wrong, approving outputs that are clearly correct. The dedicated review team handles everything in between. This hybrid approach clears 40 percent more volume during surges without hiring.

**Vendor partners** provide the largest capacity but the longest activation time. A vendor can scale from zero to 50 reviewers in two weeks, but those reviewers need training and calibration before they are productive. Vendors are not useful for short-term surges — by the time they are trained, the surge is over. They are useful for sustained elevated demand: a new product launch that permanently raises baseline volume, a regulatory requirement that requires reviewing historical data over six months, or a multi-week marketing campaign that triples traffic.

To make vendor surge capacity viable, you need a pre-negotiated contract with activation terms. The contract should specify: How many reviewers can the vendor activate on 48 hours notice? What is the cost per case during surge periods? What is the training and calibration process? How quickly can reviewers reach 85 percent accuracy? Without these terms pre-negotiated, you will spend the first week of a surge renegotiating the contract while the backlog grows.

One healthcare AI company maintains a standing vendor contract with 20 surge reviewers on standby. The vendor guarantees they can activate 10 reviewers within 48 hours and 20 within one week. The company pays a small monthly standby fee, then pays per case reviewed during surges. The vendor's reviewers are pre-trained on the company's rubric and re-calibrated quarterly. When a surge hits, the vendor is ready. This model is expensive — the standby fee is $8,000 per month — but it has activated three times in the past year, clearing surges that would have otherwise created multi-week backlogs.

## Activation Protocols

Surge capacity does not activate automatically. Someone needs to make the call: we are in surge mode, activate additional capacity. The activation decision should be rule-based, not subjective. Define the threshold in advance: if the review queue exceeds 4,000 cases or if the backlog grows by more than 50 percent in 24 hours, activate surge protocols. Do not wait for the review lead to feel overwhelmed. Use the data to trigger action.

The activation protocol specifies who to activate first and how to allocate work. The standard sequence: activate on-call reviewers within 24 hours, activate cross-trained staff within 48 hours, activate vendor partners within one week. Start with the fastest, highest-quality capacity and escalate as needed.

Work allocation during surges should route easy cases to surge capacity and hard cases to the core team. Use a complexity filter: cases with low ambiguity, high confidence scores, and clear rubric guidance go to on-call or cross-trained reviewers. Cases with high ambiguity, low confidence scores, or frequent escalation history go to the dedicated team. Do not treat all reviewers as interchangeable during surges. Maintain quality by matching case complexity to reviewer skill.

A fraud detection team runs this protocol strictly. When their queue exceeds 3,000 flagged transactions, they activate 6 on-call reviewers who handle straightforward cases: transactions that match known fraud patterns exactly, transactions that are clearly legitimate, and transactions that require only basic identity verification. The core team of 10 reviewers handles the rest: ambiguous transactions, novel fraud patterns, and high-value accounts that require deep investigation. The on-call reviewers clear 50 percent of volume, allowing the core team to focus where their expertise matters.

## Quality During Surges

Quality degrades during surges if you do not protect it deliberately. Reviewers work longer hours, feel pressure to move faster, and make mistakes they would not make under normal conditions. Surge capacity reviewers — especially cross-trained staff — are less accurate than dedicated reviewers. The backlog itself creates pressure: leadership sees 8,000 pending cases and demands faster throughput, even if faster means sloppier.

The quality protections during surges are simple but non-negotiable. First, do not reduce the sampling rate. If you normally audit 10 percent of reviews, keep auditing 10 percent during surges. The temptation is to skip audits because everyone is too busy. Resist it. Audits catch quality drift before it compounds into large-scale errors. If you must adjust sampling, sample more from surge capacity reviewers, not less from everyone.

Second, maintain escalation paths. During surges, some teams disable escalation to keep throughput high. This is catastrophic. Ambiguous cases do not become less ambiguous because the queue is long. If a reviewer is uncertain, they must still escalate. Disabling escalation forces reviewers to guess, and guesses are wrong 40 percent of the time. Keep escalation open, and dedicate at least one senior reviewer to handle escalations full-time during surges.

Third, do not extend reviewer hours indefinitely. A reviewer working 12-hour days for a week will make more mistakes on day six than they did on day one. Cognitive fatigue is real. If the backlog cannot be cleared within normal working hours using surge capacity, accept that some cases will be delayed. Shipping low-quality reviews is worse than shipping them late.

A content moderation company learned this the hard way. During a surge in November 2025, they pushed reviewers to 60-hour weeks for three weeks straight. The backlog cleared, but error rates doubled. They had approved content that violated policy and flagged content that was fine. The errors created user complaints, media coverage, and a six-week cleanup project to re-review 40,000 cases. The cost of the cleanup exceeded the cost of hiring temporary reviewers to handle the surge properly.

## Post-Surge Recovery

After a surge ends, the backlog clears, and volume returns to baseline, most teams immediately return to normal operations. This is a mistake. Surges leave damage: burned-out reviewers, quality drift that has not been caught yet, and process gaps that the surge exposed. Post-surge recovery is the work of identifying and fixing that damage before the next surge hits.

Start with a post-mortem. Gather the review team, the surge capacity reviewers, and leadership. Ask: What went well? What broke? What would we do differently next time? Do not skip this. Surges reveal weaknesses in your infrastructure, and if you do not document those weaknesses, you will hit them again.

Audit quality more heavily in the two weeks after a surge. Sample 20 percent of cases reviewed during the surge, especially cases handled by surge capacity reviewers. Identify systematic errors. If cross-trained staff consistently misinterpreted a rubric criterion, that criterion needs clearer documentation. If on-call reviewers escalated 30 percent of their cases, they need better training or simpler case routing.

Give the core review team time to recover. Do not immediately pile on new projects or demand process improvements. Reviewers who just worked 50-hour weeks for three weeks need breathing room. Schedule lighter workloads for the two weeks following a surge, or rotate reviewers into less cognitively demanding tasks like rubric documentation or test case creation.

Update your surge playbook. Every surge teaches you something about your capacity, your weak points, and your activation speed. Write it down. A good surge playbook includes: volume thresholds for activation, contact information for all surge capacity reviewers, training materials for onboarding cross-trained staff, vendor escalation procedures, and a checklist of actions to take within the first 24 hours of a surge. The playbook is not a document you write once. It is a living document you update after every surge.

## The Economics of Surge Capacity

Maintaining surge capacity is expensive. You are paying retainers to on-call reviewers, paying for quarterly training for cross-trained staff, and paying standby fees to vendors — all for capacity you might use 10 percent of the time. It feels wasteful. It is not.

The cost of not having surge capacity is higher. A three-week backlog delays customer escalations, which increases churn. A quality collapse during a surge creates brand damage that takes months to repair. Burned-out reviewers quit, which increases hiring and training costs. Emergency hiring during a surge is 50 percent more expensive than planned hiring, and emergency hires take twice as long to reach full productivity.

One e-commerce company calculated the ROI of surge capacity. They paid $15,000 per month in retainers, standby fees, and cross-training. Over 18 months, they activated surge capacity four times, each time clearing backlogs that would have otherwise taken three to four weeks to resolve. Each week of backlog was estimated to cost $80,000 in delayed escalations, customer complaints, and reviewer overtime. By clearing backlogs in days instead of weeks, surge capacity saved $960,000 over 18 months. The total cost was $270,000. The ROI was 3.5x.

Surge capacity is insurance. You pay for it continuously, and you hope you never need it. But when you need it, the cost of not having it is catastrophic. The question is not whether to build surge capacity. The question is how much, and which model fits your volume patterns, your budget, and your quality requirements.

In the next chapter, we turn to performance management and quality accountability — how to measure, evaluate, and improve individual reviewer performance at scale.
