# 8.8 — Evidence Attachment Rules: What Can and Cannot Be Cited

The invisible-evidence anti-pattern destroys decision defensibility faster than any other mistake. A reviewer marks a medical summary as unsafe, citing their clinical knowledge. An annotator flags content as policy-violating based on context they saw on a different platform. A senior reviewer overturns a decision because they remember a similar case from three months ago. All three decisions may be correct — but none can be defended in an audit, an appeal, or a legal proceeding because the evidence is locked in the reviewer's mind instead of attached to the decision record. If the evidence that drove a decision cannot be reproduced by someone who was not in the room, the decision is legally and operationally indefensible.

Evidence attachment is not about slowing reviewers down or creating bureaucratic busywork. It is about ensuring that every decision your system makes can be explained six months later when Legal asks why this content was approved, when Trust and Safety investigates a pattern of failures, or when a regulator demands to see the basis for a high-stakes classification. The distinction between valid evidence and prohibited evidence is not intuitive — it requires explicit rules, technical enforcement, and reviewer training.

This subchapter defines what counts as evidence, what does not, how to enforce evidence attachment, and how to preserve evidence so it remains usable long after the decision was made.

## What Counts as Valid Evidence

Valid evidence is anything that can be reproduced, inspected, and verified by someone who was not part of the original decision. The test is simple: if an auditor, an appeal reviewer, or a lawyer asks to see the basis for this decision, can you show them exactly what the original reviewer saw?

The content item itself is always valid evidence. If a reviewer labels a customer support transcript as hostile based on specific phrasing, the transcript is the evidence. If they mark a financial prediction as inaccurate based on a calculation error in the output, the output text is the evidence. The item under review is the primary evidence source — reviewers must be able to cite specific portions of it.

Model metadata attached to the item is valid evidence. If the model returned a confidence score, a source attribution, or a structured field alongside the output, and that metadata was visible to the reviewer at decision time, it can be cited. A reviewer who flags a summary as low-confidence based on a score of 0.34 is using valid evidence as long as that score was part of the item record.

Policy documentation versioned and timestamped is valid evidence. If your review interface surfaces policy guidelines, and those guidelines are stored with version control, a reviewer can cite the policy. "This violates Section 3.2 of the Content Safety Policy version 2.7" is defensible if you can produce version 2.7 and show it was active at the time of the decision. This requires maintaining a policy version history and timestamping which version was displayed to the reviewer.

Annotations or labels from previous review stages on the same item are valid evidence if they are part of the item's audit trail. If an item went through Level 1 review before reaching Level 2, and the Level 2 reviewer references the Level 1 label, that is valid as long as the Level 1 decision is attached to the item record. Cross-stage citations require intact lineage.

Ground truth examples from your golden set are valid evidence if they are formally published and accessible at decision time. If a reviewer says "this item matches the pattern in golden example GT-0482," that is valid only if GT-0482 was accessible in the review interface and stored with the decision. Ad-hoc examples the reviewer remembers are not valid.

Clarifications requested and received during the review are valid evidence if they are logged. If a reviewer escalates an ambiguous case to a senior reviewer and receives a written clarification, that clarification becomes part of the decision record. If the clarification was verbal or over Slack and not logged, it is not defensible evidence.

## What Cannot Be Cited as Evidence

Prohibited evidence is anything that cannot be reproduced or verified. External context not visible in the review interface is prohibited. If a reviewer flags a political opinion as misleading based on news they read yesterday, that is prohibited evidence. The news article was not part of the item record. Another reviewer re-examining the decision cannot see what the original reviewer saw. This is especially dangerous in content moderation and medical review where external knowledge feels necessary — it may improve accuracy, but it destroys defensibility.

Personal expertise or domain knowledge not documented in policy is prohibited. A physician reviewing medical AI outputs cannot cite their clinical training as evidence unless the policy explicitly authorizes clinical judgment and defines how it should be applied. Expertise improves decisions, but only when the framework for applying it is documented. Otherwise, every reviewer applies their expertise differently, and you cannot explain the variance.

Memory of previous items or decisions is prohibited unless those items are formally linked in the system. A reviewer who says "this is similar to case 18473, which we marked unsafe" is using prohibited evidence if case 18473 is not attached to the current item. Similarity is valid reasoning only when the reference item is retrievable.

Information from other platforms, tools, or systems not integrated into the review interface is prohibited. If a reviewer opens a browser tab to check a fact, looks up a term in an external glossary, or consults a colleague's notes in a separate document, that context is invisible to the audit trail. It may be correct, but it is not evidence. If external tools are necessary, they must be integrated into the review workflow so their use is logged.

Verbal instructions or hallway conversations are prohibited. If a manager tells reviewers in a stand-up meeting to start flagging a certain pattern, and that instruction is not written into policy or logged as a rule change, decisions based on it are indefensible. Verbal guidance is common in fast-moving operations — it must be documented immediately or it does not exist.

Time-limited context that expires is prohibited unless explicitly preserved. If your review tool displays a temporary banner about a policy update, and that banner disappears after 48 hours, reviewers cannot cite it after it is gone. Temporary context must either be converted into permanent policy or logged as part of the decision record.

## Chain of Custody for Evidence

Evidence is only defensible if its chain of custody is intact. This means you must prove that the evidence attached to a decision is the same evidence the reviewer saw, unaltered, at the time of the decision. A decision made in March citing version 2.7 of a policy is indefensible if the policy was edited in April and you cannot produce the March version.

Item snapshots must be immutable. When a reviewer makes a decision, the system must capture a snapshot of the item exactly as it appeared in the review interface — the text, the metadata, the timestamps, the formatting. If the item is later edited, corrected, or enriched, the snapshot remains frozen. A healthcare company reviewing AI-generated discharge summaries learned this the hard way when a clinician corrected a formatting error in a summary after it had been reviewed and flagged as unsafe. When the hospital challenged the decision, the company could not prove what the original reviewer had seen — the corrected version looked fine, and the decision looked arbitrary.

Policy versions must be timestamped and stored. Every version of every policy document must be archived with an effective date range. When a decision cites a policy, the system must record which version was active. If version 3.1 replaced version 2.7 on June 15, and a decision was made on June 14, the system must attach version 2.7, not version 3.1. This is trivial with version control but requires deliberate design.

Metadata must be captured at decision time, not reconstructed later. If a model confidence score influenced a decision, the score logged in the decision record must be the score that was displayed to the reviewer. If the model was retrained and confidence scores were recalculated, the recalculated score does not replace the original. A financial services firm faced an audit where regulators questioned why certain low-confidence predictions were approved. The firm's logs showed current confidence scores — recalculated after model improvements — that were higher than the scores reviewers had seen. The firm could not defend the original decisions because the original scores were overwritten.

Access logs must show who saw what and when. If a reviewer claims they did not have access to a certain policy section at decision time, you need logs that show which policy documents were accessible in the interface when the decision was made. Access control is evidence — if a junior reviewer was not supposed to see senior-level guidance, the system must prove they did not.

Changes to evidence after a decision must be tracked separately. If an item is re-reviewed and new evidence is added, the new evidence must not overwrite or modify the original decision's evidence. Each decision gets its own immutable evidence bundle. This allows you to show how understanding of an item evolved over time without destroying the defensibility of earlier decisions.

## Evidence Retention Requirements

Evidence must be retained for as long as the decision may be questioned. Retention periods are driven by legal requirements, regulatory timelines, and business risk. A content moderation decision may be appealed within 30 days. A medical AI review may be audited for seven years under HIPAA. A financial prediction may be subpoenaed a decade later in litigation. Your retention policy must account for the longest plausible timeline.

Regulatory retention floors are non-negotiable. HIPAA requires six years for health records. SOX requires seven years for financial controls. The EU AI Act requires documentation of high-risk AI system decisions for the lifetime of the system. GDPR complicates this with the right to be forgotten, but even under GDPR, you can retain evidence necessary for legal compliance, regulatory obligations, or the establishment, exercise, or defense of legal claims. Your retention policy must define these justifications explicitly.

High-risk decisions require longer retention. If a decision has financial, legal, health, or safety consequences, plan for a minimum of seven years. A medical device company reviewing AI-generated patient risk scores retains all review evidence for ten years because malpractice claims can surface long after an incident. A credit decisioning platform retains review evidence for fifteen years because disputes over loan denials can emerge during bankruptcy proceedings or class action lawsuits.

Deleted or anonymized items still require evidence retention if the decision remains active. If a user requests deletion of their data under GDPR, you may need to delete the content item — but you still need to retain enough evidence to defend the decision. This usually means keeping an anonymized or redacted version of the item, the decision label, the reviewer rationale, and the policy version cited. A social media platform deletes user content on request but retains anonymized snippets of flagged content plus the moderation decision for regulatory compliance.

Evidence storage must be immutable and tamper-evident. Store evidence in append-only logs, write-once storage, or blockchain-backed systems if the risk justifies it. If evidence is stored in a mutable database, implement audit trails that detect and log any modification. A financial AI platform stores review evidence in Amazon S3 with object locking enabled, ensuring that once evidence is written, it cannot be altered or deleted before the retention period expires.

Retention costs must be factored into infrastructure planning. If you review one million items per month, and each decision generates 50 KB of evidence, you are storing 50 TB per month. Over seven years, that is over 4 PB. Compression, deduplication, and tiered storage reduce costs, but retention at scale is not cheap. A customer support AI platform moved evidence older than two years to cold storage, reducing costs by 80 percent while maintaining full retrievability for audits.

Retention expiration must be automated and logged. When the retention period expires, evidence should be automatically deleted or archived to offline storage. The deletion itself must be logged to prove compliance with data minimization requirements. Manual retention management does not scale and creates compliance risk when old data lingers indefinitely.

## Technical Enforcement of Evidence Rules

Reviewers will not follow evidence rules unless the system enforces them. The review interface must make it impossible to submit a decision without valid evidence. This is not about trusting reviewers — it is about removing the opportunity for shortcuts.

Required fields for evidence citation force reviewers to attach proof. If a decision requires a rationale, the interface should require the reviewer to either highlight a specific portion of the item, cite a policy section by ID, or reference a golden example. Free-text rationales like "seems wrong" should be rejected. A legal document review platform requires reviewers to cite at least one policy rule and one sentence from the document being reviewed before the decision can be submitted.

Evidence validation checks prevent prohibited citations. If a reviewer tries to cite external context, the system should flag it. A medical review platform scans rationale text for phrases like "based on my clinical experience" or "I looked this up" and prompts the reviewer to either cite a formal policy or escalate the case for a documented clarification. This does not prevent expertise from influencing decisions — it ensures expertise is applied within a documented framework.

Golden set integration makes valid examples easily accessible. If reviewers are expected to cite similar cases, those cases must be searchable in the review interface. A content moderation tool lets reviewers search the golden set by keyword, category, or policy section and attach matching examples with one click. Citations are automatically logged with the golden example ID and timestamp.

Policy version display shows reviewers which version they are using. The interface should display the active policy version number and effective date prominently. When a reviewer cites a policy, the system automatically attaches the version ID to the decision record. A compliance review platform displays the policy version in a header on every screen and includes a link to view the full policy text, ensuring reviewers know exactly which rules they are applying.

Escalation workflows create documented clarifications. If a reviewer encounters a case that requires judgment beyond written policy, the escalation must be logged. The senior reviewer provides a written clarification, which is attached to both the original decision and added to a clarifications library for future reference. Over time, these clarifications feed back into policy updates. A financial review team logs every escalation and quarterly reviews them to identify gaps in guidance.

Audit trails capture everything that was visible to the reviewer. The system must log not just the decision and the cited evidence, but also what was displayed in the interface at decision time — which policy version was active, which golden examples were suggested, which metadata fields were populated. If a reviewer claims they did not see a certain piece of information, the logs should prove what was or was not shown.

## Evidence Attachment in Multi-Stage Review

When items pass through multiple review stages, evidence accumulates. Each stage adds its own evidence layer, and later stages may reference earlier evidence. Managing this lineage is critical to defensibility.

Upstream evidence must be carried forward. If an item was reviewed at Level 1 and flagged for escalation, the Level 2 reviewer must see the Level 1 decision, the rationale, and any evidence cited. The Level 2 decision may agree, disagree, or refine the Level 1 decision, but it must acknowledge the prior evidence. A compliance platform displays the full decision history in a timeline view, showing what each reviewer saw and decided.

Disagreement between stages must be documented. If Level 2 overturns Level 1, the Level 2 reviewer must explain why. The explanation is evidence — it shows the reasoning that led to a different conclusion. If Level 2 frequently overturns Level 1 without clear rationale, the pipeline has a training or calibration problem.

Consensus mechanisms require all reviewers' evidence. If a decision requires agreement from three reviewers, each reviewer's evidence must be logged separately. If two reviewers cite one policy section and the third cites a different section, that divergence is valuable data. A medical review platform logs each reviewer's cited evidence and flags cases where reviewers used different reasoning to reach the same conclusion.

Final decision evidence is a synthesis. If five reviewers contribute to a decision, the final evidence bundle must include all five sets of citations, the consensus mechanism result, and any tiebreaker reasoning. This is especially important in adversarial cases where you may need to show that multiple independent reviewers reached the same conclusion using valid, documented evidence.

## Training Reviewers on Evidence Discipline

Reviewers do not intuitively understand the difference between valid and prohibited evidence. It requires explicit training and regular reinforcement.

Training must include examples of defensible and indefensible decisions. Show reviewers real cases where decisions were upheld because evidence was strong, and real cases where decisions were overturned because evidence was missing or prohibited. A content moderation team uses anonymized examples from past appeals to teach new reviewers what good evidence looks like.

Calibration exercises must test evidence attachment. During calibration, reviewers should be graded not just on decision accuracy but on evidence quality. A reviewer who makes the right call but cites prohibited evidence fails the calibration. A healthcare review team scores calibration cases on a two-axis rubric: decision correctness and evidence defensibility.

Ongoing feedback highlights evidence gaps. If a reviewer frequently submits decisions with weak or missing evidence, their manager should flag it in one-on-ones. If a team shows patterns of prohibited citations, a refresher training is needed. A financial review operation tracks evidence quality as a performance metric alongside decision accuracy.

Evidence culture must be reinforced from the top. If senior reviewers routinely submit decisions without proper citations, junior reviewers will follow. If leadership treats evidence attachment as bureaucratic overhead, the discipline collapses. A compliance-driven organization makes evidence defensibility a core value, celebrated in team meetings and enforced in performance reviews.

The next subchapter covers legal hold workflows — the process for preserving review artifacts when litigation, investigation, or regulatory inquiry makes every decision record potentially critical evidence in a legal proceeding.
