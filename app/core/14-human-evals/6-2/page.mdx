# 6.2 — Audit Sampling: How Much to Check

Every audit is a sample. You cannot review every reviewer decision without doubling your operational cost and halving your throughput. The question is not whether to sample, but how much to check, which cases to select, and how to know when your sample is large enough to trust the conclusions you draw from it. Get the sampling strategy wrong and you either waste resources auditing cases that teach you nothing, or miss critical errors because your sample was too small or too biased to detect them.

Sampling strategy is where statistics meets operations. The formulas exist. You can calculate the sample size required to detect a given error rate with a given confidence level. But those formulas assume random sampling, stable error rates, and uniform task difficulty — none of which hold in real review operations. Reviewers vary. Tasks vary. Error rates drift. The operational challenge is designing a sampling strategy that balances statistical rigor with practical constraints and adapts to the realities of production systems.

## Statistical Foundations of Sampling

The core statistical question is: if you audit N cases out of a population of M cases, how confident can you be that your sample accurately reflects the population? The answer depends on the error rate you are trying to detect, the confidence level you require, and the margin of error you are willing to accept.

For binary outcomes — a reviewer decision is either correct or incorrect — the formula is straightforward. To detect an error rate of five percent with ninety-five percent confidence and a margin of error of plus or minus two percent, you need approximately 456 samples. To detect a one percent error rate with the same confidence and margin, you need approximately 1,825 samples. The rarer the error, the larger the sample required to reliably detect it.

This creates an immediate operational problem. If you have ten reviewers each producing a thousand decisions per week, auditing 1,825 cases per week per reviewer is not feasible. You would need nearly two full-time auditors per reviewer. The cost is prohibitive. Most organizations settle for smaller samples and accept wider margins of error. They might audit fifty cases per reviewer per week, which gives them a margin of error closer to plus or minus fourteen percent. That is enough to detect gross problems — a reviewer whose error rate is twenty or thirty percent — but not enough to distinguish between a five percent error rate and a ten percent error rate.

The trade-off is explicit. Smaller samples reduce audit cost but increase uncertainty. Larger samples increase confidence but consume more resources. The right balance depends on the stakes. For high-risk tasks — content moderation involving harm, medical annotation, financial fraud detection — you accept higher audit costs because missing errors has severe consequences. For low-risk tasks — ranking preference data, entertainment recommendations — you accept wider margins because the cost of an undetected error is low.

## Sample Size Requirements by Goal

The sample size you need depends on what you are trying to learn from the audit. Different questions require different levels of statistical power.

If the goal is to estimate overall accuracy across the entire review population, you can use a pooled sample. Audit a random selection of cases across all reviewers and calculate the aggregate error rate. This gives you a system-wide metric but tells you nothing about individual reviewer performance. A pooled sample of three hundred to five hundred cases per week is often sufficient to track overall trends and detect sudden quality drops caused by guideline changes, tool issues, or task definition problems.

If the goal is to evaluate individual reviewer performance, you need per-reviewer samples. Each reviewer must receive enough audits to generate a statistically meaningful score. At a minimum, you need thirty to fifty audits per reviewer per evaluation period to avoid scores that are dominated by noise. Better practice is one hundred audits per reviewer per month, which allows you to distinguish between reviewers with five percent error rates and those with fifteen percent error rates with reasonable confidence. This becomes the baseline audit volume for most large operations: one hundred cases per reviewer per month, stratified evenly across the weeks to provide regular feedback.

If the goal is to detect rare but high-severity errors, you need risk-weighted sampling. Random sampling is inefficient for rare events. If serious errors occur in one percent of cases, a random sample of one hundred cases has a thirty-seven percent chance of missing them entirely. Instead, you oversample high-risk cases — decisions involving policy violations, cases escalated by users, cases where the model and the reviewer disagreed, cases flagged by automated heuristics. This concentrates audit resources on the cases most likely to contain consequential errors.

If the goal is to evaluate agreement on difficult or ambiguous cases, you need targeted sampling of edge cases. Random samples overrepresent easy, clear-cut decisions where everyone agrees. They underrepresent the borderline cases where guidelines are vague and reasonable people disagree. To measure how well reviewers handle ambiguity, you deliberately sample from the tail of the distribution — cases that took longest to decide, cases with low model confidence, cases that other reviewers escalated. This tells you whether your guidelines are clear enough and whether training has prepared reviewers for difficult scenarios.

## Stratified Sampling by Reviewer, Task, and Difficulty

Random sampling is unbiased, but it is not always optimal. If you have one hundred reviewers and you randomly sample five hundred cases per week, some reviewers will appear in the sample multiple times and some will not appear at all. Over time, this evens out, but in any given week, you will have incomplete visibility. Stratified sampling ensures that every reviewer is audited, every task type is covered, and the sample reflects the structure of your workload.

The most common stratification is per-reviewer. You allocate a fixed number of audits to each reviewer per period — say, twenty-five per week or one hundred per month — and randomly select cases from each reviewer's work. This ensures every reviewer receives regular feedback and prevents the situation where high-volume reviewers dominate the sample and low-volume reviewers go unaudited for months. The trade-off is that you lose some statistical efficiency. A pooled random sample of the same size would have slightly narrower confidence intervals. But the operational benefit of per-reviewer feedback usually outweighs the statistical cost.

The second stratification is by task type. If reviewers handle multiple kinds of work — some evaluate helpfulness, some check factuality, some label sentiment — you want audits to cover all task types proportionally. Otherwise, you might discover six months into an operation that your factuality audits have been thorough but your sentiment audits have been neglected. Task-stratified sampling ensures that every task gets visibility and that quality metrics are comparable across tasks.

The third stratification is by difficulty. Not all decisions are equally hard. Some cases are clear-cut: the model output is obviously correct or obviously wrong, and any competent reviewer will reach the same conclusion. Other cases are genuinely ambiguous: the guidelines do not clearly specify the correct answer, or the correct answer depends on subjective judgment. If you sample uniformly, your audits will overrepresent easy cases and underrepresent hard ones. You will conclude that reviewers are highly accurate, but that accuracy may collapse on difficult cases that matter most.

Difficulty-stratified sampling requires a proxy for difficulty. You might use review time: cases that took longer than the median are likely harder. You might use model confidence: cases where the model was uncertain are often ambiguous. You might use escalation history: cases that other reviewers have escalated are, by definition, non-obvious. You oversample these difficult cases to ensure that your audit findings reflect reviewer performance where it matters, not just where it is easy.

The challenge with stratification is that it multiplies sample requirements. If you want per-reviewer, per-task, difficulty-stratified audits, and you have ten reviewers, five task types, and three difficulty bins, you now have one hundred and fifty strata. To maintain statistical power within each stratum, you need samples in each one. This quickly becomes unmanageable. The practical solution is to prioritize. Stratify by reviewer first, because individual feedback is essential. Stratify by task second, if task types differ significantly. Use difficulty stratification only for periodic deep dives, not for routine audits.

## The Cost of Over-Sampling Versus Under-Sampling

Audit cost is not just auditor time. It is also the opportunity cost of not reviewing new cases, the latency cost of delayed feedback, and the cultural cost of treating QA as surveillance rather than as support. Over-sampling wastes resources. Under-sampling leaves errors undetected. The optimal sample size is the smallest sample that meets your accuracy and feedback requirements.

Over-sampling manifests as redundant audits. You audit one hundred cases per reviewer per week when fifty would have given you the same conclusions. The extra fifty audits consume auditor time that could have been spent on training, guideline refinement, or deeper analysis of flagged patterns. Reviewers receive feedback more frequently than they can act on it. The marginal value of each additional audit declines. You are paying for precision you do not need.

The cost is compounded if audits are slow to process. If it takes three days to complete and score an audit, then auditing one hundred cases per week per reviewer means feedback arrives nearly a week after the work was done. The reviewer has already completed five hundred more decisions using the same approach. If that approach was flawed, the error has propagated. Faster feedback with smaller samples is often more valuable than delayed feedback with larger samples.

Under-sampling manifests as undetected drift. You audit twenty cases per reviewer per month, conclude that everyone is performing acceptably, and later discover that two reviewers have been ignoring a guideline for six months. The sample size was too small to catch errors that occur in five to ten percent of cases. By the time the problem surfaces, thousands of cases have been mislabeled. If those labels trained a model, the model has learned the error. If they informed a product decision, the decision was based on bad data.

The worst version of under-sampling is sampling that stops entirely. Teams launch review operations with rigorous audits, then scale back as they gain confidence. Audits become quarterly instead of monthly, then semi-annual, then informal. Drift sets in. Guidelines evolve informally. New reviewers learn from peers, not from training. When someone finally runs an audit, they discover that inter-rater agreement has collapsed. The cost of under-sampling is not just the undetected errors. It is the compounded cost of retraining, relabeling, and rebuilding trust in the data.

The optimal approach is adaptive sampling. Start with high audit rates during onboarding and stabilization. As reviewers prove consistent, reduce audit frequency to baseline levels. If a reviewer's audit scores drop, increase audit rates temporarily until they recalibrate. If a new task or guideline is introduced, increase audits for all reviewers until the team stabilizes. Adaptive sampling concentrates resources where risk is highest and scales back when confidence is justified.

## Adaptive Sampling Based on Risk

Adaptive sampling treats audit volume as a variable, not a constant. Instead of auditing every reviewer at the same rate forever, you adjust audit intensity based on observed performance, task risk, and operational changes. This allows you to maintain high confidence where it matters while reducing cost where it does not.

The simplest form of adaptive sampling is performance-triggered. Set a threshold: reviewers with audit scores above ninety-five percent accuracy over the past month drop to a reduced audit rate. Reviewers below ninety percent move to an elevated rate. Reviewers between ninety and ninety-five percent remain at baseline. This concentrates audits on reviewers who need feedback while reducing overhead for consistently high performers.

The second form is tenure-based. New reviewers receive daily audits during their first two weeks, then weekly audits for the next month, then drop to the baseline rate once they demonstrate consistency. This front-loads QA during the highest-risk period — when reviewers are still learning and most likely to make mistakes — and scales back once they have proven competent. Tenure-based sampling reduces overall audit volume without sacrificing quality during onboarding.

The third form is task-risk-based. High-stakes tasks — content moderation involving harm, medical decision support, fraud detection — receive permanently elevated audit rates. Lower-stakes tasks — entertainment preferences, non-sensitive ranking — receive reduced rates. This aligns audit investment with consequence. If a mistake in Task A costs nothing and a mistake in Task B costs thousands of dollars or harms users, Task B should consume a disproportionate share of audit resources.

The fourth form is change-triggered. Any time you update guidelines, change task definitions, or introduce new tools, temporarily increase audit rates across all reviewers to detect whether the change caused confusion or drift. After two weeks, if audit scores remain stable, return to baseline. Change-triggered sampling catches implementation issues early, before they compound into systemic problems.

Adaptive sampling requires infrastructure. You need a system that tracks audit scores over time, calculates per-reviewer accuracy trends, and automatically adjusts sampling rates based on rules you define. Most review platforms do not offer this out of the box. Teams build it themselves or accept fixed sampling rates. The operational investment is worth it at scale, because adaptive sampling can reduce audit volume by thirty to fifty percent without sacrificing confidence.

## When Sampling Fails

Sampling assumes that the cases you audit are representative of the cases you do not audit. This assumption breaks when the sample is biased, when the population is non-stationary, or when reviewers change behavior in response to being audited. These failure modes are common and often invisible until someone investigates why audit scores do not match production outcomes.

The first failure mode is selection bias. If you only audit cases that are easy to audit — short responses, clear guidelines, binary decisions — your audits will overestimate accuracy on hard cases. If you only audit cases flagged by automated heuristics, your audits will miss errors that the heuristics did not catch. If you only audit reviewers who volunteer cases for feedback, you will get a non-representative sample of confident reviewers who already know the guidelines. Every sampling shortcut introduces bias. The solution is to randomize within strata, not to cherry-pick convenient cases.

The second failure mode is temporal drift. Error rates are not constant. They rise when reviewers are fatigued, when guidelines are ambiguous, when tasks change. If you audit Monday's work and assume it represents the whole week, you miss the Friday afternoon slump. If you audit last month's work and use it to evaluate this month's performance, you miss the impact of a guideline change released two weeks ago. The solution is continuous sampling: audit recent work, audit across all days of the week, and track trends rather than point estimates.

The third failure mode is the Hawthorne effect: reviewers change behavior when they know they are being audited. A reviewer who knows that every tenth case is audited will spend extra time on cases nine, ten, and eleven. They will escalate borderline cases they would normally decide. They will follow the letter of the guidelines even when experience suggests a different judgment. The audited cases look great. The unaudited cases do not. The solution is blind audits, which we cover in the next subchapter.

Sampling is the compromise you make to scale QA. Perfect knowledge would require auditing every case, which is operationally impossible. Sampling gives you partial knowledge, and the art is designing the sample so that what you learn is both accurate enough to trust and cheap enough to sustain. The next subchapter examines whether audits should be visible to reviewers or hidden, and how that choice affects both measurement accuracy and team culture.
