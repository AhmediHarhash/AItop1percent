# 8.4 — The Adjudicator Role: Selection and Training

Who should resolve disagreements when reviewers diverge? The obvious answer is the most experienced reviewer. The correct answer is more specific than that. The adjudicator role requires a combination of task expertise, judgment under ambiguity, communication skill, and calibration discipline. Not every senior reviewer has all four. Not every domain expert can adjudicate effectively. Selecting the wrong adjudicators creates bottlenecks, inconsistent ground truth, and reviewer frustration. Selecting the right adjudicators and training them well is one of the highest-leverage decisions you make in a review system.

## What the Adjudicator Role Requires

Adjudication is not just reviewing outputs with more care. It is a distinct skill set. The adjudicator must interpret the rubric in edge cases, make defensible decisions under ambiguity, provide clear rationale that reviewers can learn from, identify when a disagreement reveals a rubric gap, and maintain consistency across hundreds of decisions over time. These are not the same skills required for primary review.

A strong primary reviewer can follow the rubric accurately and flag edge cases. A strong adjudicator can decide what to do with the edge cases the rubric does not cleanly cover. A primary reviewer applies the rules. An adjudicator interprets the rules and, when necessary, clarifies what the rules mean in context. This requires a deeper understanding of the task, more experience with boundary cases, and more comfort with judgment calls that will be scrutinized.

Adjudicators also need communication skill. When they resolve a disagreement, they provide rationale that explains why they chose one interpretation over another. This rationale becomes a teaching tool. If the rationale is vague or defensive, reviewers do not learn. If the rationale is clear and references specific rubric sections or examples, reviewers improve their calibration and future disagreements on similar cases decline. The adjudicator is not just labeling outputs — they are teaching the reviewer pool through their decisions.

Finally, adjudicators need emotional resilience. Disagreements sometimes involve reviewers who feel strongly that their interpretation was correct. The adjudicator will overrule people who have more domain experience, more tenure, or more confidence. If the adjudicator cannot make those calls without hesitation, they will avoid overruling and default to majority vote or escalation when they should decide. Adjudication requires confidence tempered by humility — confidence to make the call, humility to escalate when the case is beyond their ability to resolve.

## Selecting Adjudicators: Experience Is Necessary But Not Sufficient

The first filter is task experience. An adjudicator should have completed hundreds of primary reviews for the task they will adjudicate. They need to know the rubric deeply, have seen the common failure modes, and understand the distribution of outputs the system produces. Adjudicating from theory does not work. You need exposure to real cases.

But experience alone is not enough. Some experienced reviewers are consistent but inflexible. They apply the rubric mechanically and struggle when cases do not fit neatly into the guidelines. Others are thoughtful but inconsistent. They make nuanced judgments that vary depending on their mood, workload, or recent cases they reviewed. Neither type makes a strong adjudicator. You need reviewers who are both experienced and demonstrably consistent across time.

One selection method is to measure historical agreement between the candidate and other reviewers. A reviewer who has 92 percent agreement with the broader pool and whose decisions are rarely overturned in post-hoc audits is a strong candidate. A reviewer who has 82 percent agreement and whose decisions are frequently outliers is not ready. Low agreement does not always mean the reviewer is wrong — sometimes it means they are catching issues others miss. But chronic disagreement without evidence that the reviewer is right means they are applying a different mental model than the rest of the team. That makes them a poor adjudicator because their decisions will not align with the shared understanding of the rubric.

Another selection method is to test judgment under ambiguity. Give the candidate ten cases where reasonable people can disagree. Ask them to make a determination and provide rationale. Evaluate the rationale for clarity, logical consistency, and reference to the rubric. A candidate who writes "this seems wrong" is not ready. A candidate who writes "the rubric defines professional tone as avoiding sarcasm, and while the response is factually correct, the phrase in question reads as sarcastic in context, so I am labeling this as failing the tone criterion" is ready. The adjudicator must be able to articulate why they reached their conclusion in terms the reviewer pool can understand and learn from.

Domain expertise is a third factor, but it depends on the task. For highly technical tasks — legal document review, medical coding, contract clause extraction — domain expertise is necessary. A non-lawyer cannot adjudicate legal interpretation disagreements reliably. A non-clinician cannot adjudicate clinical accuracy disagreements reliably. For less specialized tasks — customer service tone, summarization quality, content relevance — domain expertise matters less than judgment and calibration. A strong generalist adjudicator who knows the rubric well can resolve most disagreements in these domains.

## Training Adjudicators: What Makes Adjudication Different

Adjudicators need training even if they are experienced reviewers. The role is different. The decisions are harder. The rationale must be more precise. The stakes are higher because adjudication sets ground truth.

Training begins with understanding the difference between applying the rubric and interpreting it. Show adjudicators ten cases where the rubric is ambiguous or incomplete. Walk through how to decide in the absence of a clear directive. Teach them to ask: what was the rubric trying to enforce with this criterion? What outcome does this criterion serve? If the rubric is silent on this scenario, what interpretation aligns with the broader goals of the evaluation? These questions let adjudicators make consistent decisions even when the rubric does not provide explicit guidance.

Next, train adjudicators to write rationale. The rationale is not for the adjudicator — it is for the reviewers who disagreed. A good rationale does three things. It states the decision clearly. It explains which part of the rubric supports the decision. It acknowledges the alternate interpretation and explains why it was not chosen. This structure validates the reviewers who disagreed while teaching them why a different interpretation applies in this case.

A financial services company trained adjudicators using a structured rationale template. Every adjudication included: the final label, the criterion in question, the specific rubric section that applied, a one-sentence summary of the reviewers' disagreement, and a two-to-four sentence explanation of why one interpretation was preferred. Adjudicators initially resisted the structure as time-consuming. After three weeks, they realized the structure forced them to think clearly and reduced back-and-forth with reviewers who questioned decisions. Disagreement rates on similar cases dropped because reviewers were learning from the rationale instead of repeating the same mistakes.

Training should also cover escalation criteria. Adjudicators need to know when to decide and when to escalate. Early adjudicators tend to escalate too often because they are afraid of making a wrong call. The training should emphasize that adjudication is about making the best decision possible given the rubric as written, not about finding the perfect decision. If the rubric supports a decision and the adjudicator is confident in their reasoning, they should decide. If the rubric does not support a decision or the adjudicator lacks the expertise to evaluate the case, they should escalate. The line between the two is judgment, but training can clarify the principles that guide that judgment.

## Maintaining Adjudicator Calibration Over Time

Adjudicators drift just like primary reviewers. After six months of resolving disagreements, an adjudicator's mental model may diverge from the rubric or from other adjudicators in the pool. Calibration is not a one-time event. It requires regular check-ins and measurement.

One calibration method is adjudicator-to-adjudicator agreement checks. If you have multiple adjudicators, periodically route the same disagreement cases to two adjudicators independently. Measure how often they reach the same conclusion. Agreement above 90 percent suggests strong calibration. Agreement below 80 percent suggests the adjudicators are interpreting the rubric differently and need recalibration. Use the disagreements to facilitate discussion. Why did one adjudicator label this case as passing while the other labeled it as failing? Which interpretation aligns better with the rubric? The discussion itself improves calibration.

Another method is ground truth spot checks. Every month, pull a random sample of 50 adjudicated cases. Have a senior reviewer or the escalation team re-review them without knowing the adjudicator's decision. Measure how often the senior reviewer agrees with the adjudicator. If agreement is consistently above 95 percent, the adjudicator is well-calibrated. If agreement drops below 90 percent, investigate. Are there systematic errors? Has the adjudicator developed a blind spot? Has the rubric changed and the adjudicator missed the update? The spot check surfaces calibration drift early before it produces hundreds of bad labels.

Adjudicators should also participate in group calibration sessions, not just run them. A quarterly session where adjudicators and senior reviewers review recent edge cases together keeps everyone aligned. The session is not about proving who is right — it is about ensuring that everyone is applying the rubric the same way. If adjudicators attend these sessions, they remain part of the calibration ecosystem instead of drifting into a silo.

## The Adjudicator Workload Problem

Adjudication is cognitively demanding. Each case requires reading the output, understanding the reviewers' disagreement, consulting the rubric, making a judgment, and writing rationale. A strong adjudicator can handle 20 to 40 adjudications per day depending on task complexity. Beyond that, quality degrades. Adjudicators start rushing. Rationale becomes terse. Decisions become inconsistent.

The workload problem becomes acute as the system scales. If disagreement rates are 15 percent and you are reviewing 2,000 outputs per week with three reviewers each, that is 300 disagreement cases per week. If one adjudicator handles all of them, they are overloaded and become a bottleneck. If you add more adjudicators, you need to ensure they stay calibrated with each other or you introduce inconsistency.

The solution is usually a combination of three strategies. First, reduce disagreement rates through better rubrics and better primary reviewer training. If you cut disagreement from 15 percent to 10 percent, adjudication volume drops by a third without hiring anyone. Second, use majority vote for low-stakes cases and reserve adjudication for high-stakes or high-ambiguity cases. This reduces the percentage of disagreements that require adjudication. Third, grow the adjudicator pool slowly and invest in calibration. Adding a second adjudicator doubles capacity but requires regular agreement checks to ensure consistency. Adding a fifth adjudicator without calibration infrastructure creates chaos.

A logistics company grew their adjudicator pool from one to four over eight months. They promoted adjudicators from the primary reviewer pool based on consistency and judgment. Every new adjudicator shadowed an experienced adjudicator for two weeks, reviewing the same cases and comparing rationale. After two weeks, they adjudicated independently but had their decisions spot-checked weekly for the first month. Monthly calibration sessions kept all four adjudicators aligned. Adjudication turnaround time dropped from five days to one day and consistency remained above 92 percent. The growth was slow but sustainable.

## Adjudicators as the Bridge Between Reviewers and Rubric Authors

The adjudicator role is not just operational — it is strategic. Adjudicators see patterns in disagreements that reveal rubric weaknesses. They see cases the rubric does not cover. They see criteria that are consistently misinterpreted. If adjudicators communicate these patterns to the escalation team or rubric authors, the system improves. If adjudicators treat their role as purely executional — label the case, move to the next — those insights evaporate.

Mature systems formalize this feedback loop. Adjudicators submit a monthly summary of the most common disagreement patterns they observed. The escalation team uses this data to prioritize rubric updates. A legal document review system had adjudicators tag every adjudication with a reason code: ambiguous guideline, missing example, novel output type, reviewer error, or legitimate ambiguity. Once a month, the rubric author reviewed all cases tagged as ambiguous guideline or missing example and updated the rubric accordingly. This process turned adjudication from a reactive labeling task into a proactive quality improvement mechanism.

The adjudicator role is one of the most important in a human review system. Selecting the right people, training them thoroughly, and maintaining their calibration over time is not optional. It is the foundation of defensible, consistent ground truth.

