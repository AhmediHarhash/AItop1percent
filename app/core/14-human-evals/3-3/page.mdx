# 3.3 — Context Display: Showing Reviewers What They Need

Reviewers cannot judge quality without context. A sentence flagged as misinformation might be accurate in one domain and false in another. A transaction flagged as fraudulent might be legitimate when you see the customer's purchase history. A response flagged as off-topic might be exactly right when you see the three messages that preceded it. The interface must show the reviewer the context they need to make the correct decision. Too little context produces guesses. Too much context produces paralysis. The right amount of context, displayed at the right moment, produces accuracy.

## The Context Starvation Problem

The most common failure mode in review interfaces is context starvation. The reviewer sees only the item being judged, with no surrounding information. They are asked to label a message as policy-violating without seeing the conversation thread. They are asked to assess the accuracy of a model response without seeing the user prompt. They are asked to flag a transaction as risky without seeing the customer's account history. The task is technically possible — the reviewer can make a judgment — but the judgment is based on incomplete information. Accuracy suffers.

A customer support quality review team evaluated agent responses for tone and helpfulness. The initial interface showed only the agent's response. Reviewers labeled responses as helpful or unhelpful. Inter-rater agreement was 68 percent. The team added the customer's initial message to the interface. Agreement increased to 79 percent. The team added the full conversation thread. Agreement increased to 87 percent. The same reviewers, the same task, the same policy — but accuracy improved by 28 percent when the interface showed the context the reviewers needed.

The cost of context starvation is not always visible in inter-rater agreement. If all reviewers lack the same context, they might agree with each other while all being wrong. A content moderation task asked reviewers to flag medical misinformation. The interface showed only the post text. Reviewers flagged posts that contradicted mainstream medical advice. A post claiming that a specific supplement cured a disease was flagged. Later analysis revealed the post was citing a peer-reviewed clinical trial. The reviewers were right to be skeptical — extraordinary claims require extraordinary evidence — but they lacked the context to verify the claim. The post should have been labeled as needs-verification, not misinformation. The interface did not show links, citations, or sourcing. Context starvation led to over-flagging.

## Progressive Disclosure

The opposite failure mode is context overload. The interface shows everything: the conversation thread, the user profile, the account history, related items, similar cases, policy excerpts, and a panel of reference documentation. The reviewer spends 40 seconds reading context and 10 seconds making the judgment. Throughput drops. Cognitive load increases. Reviewers report feeling overwhelmed.

Progressive disclosure solves this. The interface shows the minimum context necessary to make the judgment by default. Additional context is available on demand, one click or keystroke away. The reviewer starts with the essentials. If the essentials are sufficient, they decide immediately. If they need more, they expand the context panel. The interface does not assume every reviewer needs every piece of context for every review. It assumes the common case is simple and makes the complex case possible.

A fraud detection team designed an interface with progressive disclosure. The default view showed the transaction amount, merchant, and timestamp. A single keystroke revealed the customer's transaction history for the past 30 days. Another keystroke revealed account age, previous flags, and linked accounts. A third keystroke revealed the model's confidence score and the features that contributed most to the flag. The majority of reviews required only the default view. Complex cases required the additional context. Average decision time was 31 seconds. When the team had shown all context by default, average decision time had been 58 seconds. Accuracy was unchanged. Progressive disclosure cut review time in half without sacrificing quality.

The design principle is: show what is necessary, hide what is optional, make hidden context instantly accessible. The reviewer should never have to navigate away from the review to find context. They should never have to remember where to find it. It should be one keystroke or one click away, always in the same place.

## Related Items

Some judgments require seeing other items that share context. A content moderation task judging whether a comment is harassment might need to see other comments the same user posted in the same thread. A transaction review task might need to see other transactions the customer made in the same session. A document review task might need to see other documents in the same contract set.

The interface can show related items in several ways. The simplest is a sidebar panel that lists related items by title or summary. The reviewer clicks one to open it in a modal or side-by-side view. The more sophisticated approach is inline expansion: the reviewer hovers over or clicks a related-items indicator, and the related items appear inline without leaving the current review. The reviewer scans the related items, closes the panel, and continues.

The challenge is relevance. If the interface shows ten related items and only one is useful, the reviewer wastes time scanning nine irrelevant items. The interface must rank related items by relevance. For conversation threads, show the immediate parent and child messages first. For transactions, show other transactions with the same merchant or similar amounts first. For documents, show documents with overlapping parties or similar clauses first. The ranking algorithm does not need to be perfect. It only needs to put the most likely useful items at the top.

A legal document review team used related-items ranking to surface similar clauses across contracts. The initial implementation showed all clauses containing the same keywords. The reviewer saw 40 related clauses, most of which were not relevant. The team refined the ranking to prioritize clauses in contracts with the same counterparty, then clauses in contracts with similar dates, then clauses with high text similarity. The refined ranking showed an average of six related clauses, with the top three relevant in 81 percent of cases. Reviewers stopped ignoring the related-items panel and started using it as a primary decision tool.

## Conversation History

For tasks that judge conversational AI outputs, conversation history is essential context. The reviewer must see the full conversation thread, not just the current turn. The thread shows the user's intent, the history of the conversation, and the context the model had when generating the response. Without the thread, the reviewer cannot judge whether the response is appropriate.

The interface must decide how much history to show by default. Showing the entire thread from the first message works for short conversations. For long conversations, showing the entire thread creates scroll fatigue. The reviewer has to scroll through 20 messages to understand context for the current message. Progressive disclosure applies here: show the current message and the previous two to three turns by default. Provide a "show full thread" option for cases where more history is necessary.

Conversation history should be visually distinct from the item being reviewed. Use alternating background colors, indentation, or speaker labels to make it clear which message is the user and which is the model. The reviewed response should be highlighted or visually emphasized. The reviewer should never have to guess which part of the interface they are judging.

A customer support AI team initially showed conversation history in chronological order, oldest first. Reviewers had to scroll to the bottom to see the current message. The team reversed the order: newest first, with the current message at the top. Review time dropped by 18 percent. Reviewers no longer had to scroll to find the message being judged. Later, the team added an option to toggle between chronological and reverse-chronological order. Most reviewers preferred reverse-chronological, but a minority preferred chronological for long threads. Offering the toggle satisfied both groups.

## User History

Some judgments benefit from seeing the user's history beyond the immediate conversation. A content moderation task judging whether a comment is abusive might need to see the user's previous moderation history. A transaction review task might need to see the customer's account age, previous disputes, and lifetime transaction volume. A response quality review might need to see how often the user has complained about previous responses.

User history is sensitive. Showing a reviewer that a user has been flagged three times for policy violations creates bias. The reviewer might judge the current item more harshly because of the history, even if the current item is borderline or acceptable. The design choice is whether to show history by default, hide it by default, or show it only for certain thresholds.

A content moderation platform tested three designs. The first showed the user's moderation history by default: number of previous flags, number of previous violations, account age. The second hid history by default and allowed the reviewer to request it. The third showed history only if the user had more than two previous violations in the past 90 days. The team measured accuracy against ground truth labels created by senior reviewers who always saw full context. The first design had the highest accuracy overall but showed evidence of bias: users with previous violations were flagged at higher rates even when the current content was borderline. The second design had lower accuracy for repeat offenders: reviewers missed patterns that history would have revealed. The third design balanced accuracy and bias: history appeared when it was most relevant, and reviewers did not over-rely on it for users with clean records.

The lesson is that user history is useful context, but it introduces bias if not carefully managed. The safest approach is to show history only when it crosses a relevance threshold, and to train reviewers to judge the current item on its own merits first, then use history only as a tiebreaker for ambiguous cases.

## Model Inputs and Outputs Side by Side

For tasks judging model-generated content, the reviewer needs to see both the input the model received and the output it produced. A response accuracy task shows the user prompt and the model response. A summarization task shows the source document and the summary. A translation task shows the source text and the translation. The interface must show both, and it must make the relationship between them clear.

The most common design is side-by-side display: input on the left, output on the right. The reviewer scans the input, scans the output, and makes a judgment. This works well for short inputs and outputs. For long inputs or outputs, side-by-side creates scrolling synchronization problems: the reviewer scrolls the output and loses track of the corresponding part of the input.

An alternative design is stacked display: input at the top, output below. The reviewer scrolls down to see the output after reading the input. This avoids synchronization issues but requires more scrolling. A third design is tabbed display: input and output in separate tabs. The reviewer switches between tabs. This works for very long documents but creates memory load: the reviewer must remember details from the input tab while viewing the output tab.

The choice depends on input and output length. For short content — fewer than 500 words each — side-by-side is fastest. For medium content — 500 to 2,000 words — stacked is clearer. For long content — more than 2,000 words — provide both stacked and side-by-side as options and let the reviewer choose.

A legal summarization task initially used side-by-side display for 10-page documents. Reviewers reported difficulty tracking correspondence between sections. The team switched to stacked display with anchor links: the output included clickable references to source sections, and clicking a reference scrolled the input panel to the corresponding section. Reviewers could verify correspondence without manual scrolling. Accuracy improved by 14 percent. Review time increased by 9 percent, but the accuracy gain justified the time cost.

## Hiding Noise While Preserving Signal

Not all context is useful. Some context is noise: information that is technically available but irrelevant to the judgment. Showing noise dilutes signal. The reviewer spends cognitive budget filtering out irrelevant information instead of focusing on the relevant parts.

The interface must decide what to hide. If a transaction review task shows 40 fields of transaction metadata, but only five are relevant to fraud detection, the interface should show the five by default and hide the rest. If a content moderation task shows user profile information including registration date, email verification status, and timezone, but only previous moderation history is relevant, the interface should hide the rest.

The risk of hiding information is that reviewers might need something you hid. The solution is to make hidden information accessible but not visible by default. A "show all details" button or keystroke reveals everything. The reviewer uses it only when the default context is insufficient. Usage analytics tell you whether you hid the right things. If reviewers frequently reveal hidden information, you hid something useful. If they rarely reveal it, you hid noise.

A hiring platform that used reviewers to assess candidate profiles initially showed 18 fields of candidate information. Reviewers reported feeling overwhelmed. The team analyzed which fields reviewers used to make judgments. They found six fields were viewed in more than 80 percent of reviews. The remaining 12 fields were viewed in fewer than 20 percent of reviews. The team redesigned the interface to show the six high-use fields by default and collapsed the rest behind a "show more" button. The button was clicked in 11 percent of reviews. Reviewer satisfaction increased. Decision time decreased by 22 percent. Accuracy was unchanged.

## When More Context Hurts

There is a point where additional context reduces accuracy instead of improving it. This happens when context introduces bias, when context is conflicting, or when the sheer volume of context exceeds the reviewer's working memory.

A hiring evaluation task showed reviewers a candidate's resume, cover letter, and three references. The team hypothesized that adding interview notes would improve accuracy. They tested it: half of reviewers saw interview notes, half did not. The group that saw interview notes had lower inter-rater agreement and lower accuracy against ground truth. The interview notes introduced bias: reviewers weighted interview impressions more heavily than resume content, even though the task was to assess qualifications, not interview performance. The team removed interview notes from the interface.

The principle is: more context is better only if the context is relevant, unbiased, and not overwhelming. Before adding context, test whether it improves decisions. Measure accuracy with and without the context. If accuracy does not improve, the context is noise.

The next subchapter explores keyboard-first design — how to build interfaces where reviewers never need to touch the mouse, and why this matters for throughput and accuracy at scale.

