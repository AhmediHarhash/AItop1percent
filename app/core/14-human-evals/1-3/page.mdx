# 1.3 — When Human Review Is Mandatory vs Optional

Most teams treat human review as something you do when you have time, when stakeholders ask for it, or when production breaks. This is wrong. Human review is mandatory in specific scenarios where the risk of undetected failure exceeds the cost of systematic review. Human review is optional in scenarios where automated evaluation provides sufficient confidence and the consequences of missed errors are tolerable. Human review is unnecessary in scenarios where the task has objective ground truth and automated evaluation covers the entire risk surface. The discipline is knowing which scenario you are in and building the infrastructure to support the mandatory cases before they become crises.

The teams that fail are the teams that treat all human review as optional until a catastrophic failure forces them to review everything reactively. The teams that succeed define upfront when human review is required, build the infrastructure to make that review systematic, and enforce the requirement as a release gate. Human review is not a best practice. It is a risk control. The question is not whether you can afford to do it. The question is whether you can afford not to.

## Mandatory Scenario One: High-Stakes Decisions with Severe Failure Consequences

Human review is mandatory when your AI system makes or influences decisions where errors cause direct harm. Medical diagnosis assistance, legal contract analysis, financial advice, content moderation, hiring recommendations, insurance underwriting — these are domains where a single undetected error can result in injury, legal liability, financial loss, or reputational damage that exceeds the cost of reviewing every output.

In these domains, automated evaluation is necessary but not sufficient. You run automated evals to catch known failure modes and prevent regressions. But you also review a statistically significant sample of production outputs every week with human experts who can identify errors your automated evals missed. The review is not optional. It is not done when convenient. It is scheduled, resourced, and enforced as a mandatory quality gate.

A healthcare AI that provides diagnostic recommendations based on patient symptoms must be reviewed by licensed clinicians. The automated eval suite checks that the model does not hallucinate conditions, that it cites relevant guidelines, that it does not contradict itself across multi-turn conversations. But the automated evals cannot catch the case where the model recommends a correct treatment for the stated symptom but fails to ask about contraindications that would make the treatment dangerous. A human clinician catches that. The review happens on a random sample of five hundred outputs per week. If the error rate exceeds the threshold defined by your risk tolerance, you halt releases until the root cause is fixed.

The cost of this review is high. Five hundred outputs per week at two minutes per output is over sixteen hours of clinician time per week. At a fully-loaded cost of one hundred fifty dollars per hour, that is twenty-four hundred dollars per week, or one hundred twenty-five thousand dollars per year. That cost is unacceptable to teams that think of human review as nice-to-have. That cost is trivial to teams that understand the liability exposure of shipping a diagnostic tool with a ten percent error rate that could have been detected.

## Mandatory Scenario Two: Regulatory Compliance and Auditability Requirements

Human review is mandatory when regulations or contractual obligations require documented evidence of quality validation. The EU AI Act, HIPAA, SOX, financial services regulations, and many enterprise contracts explicitly require human oversight of high-risk AI systems. Automated evaluation alone does not satisfy these requirements. You must document that qualified humans reviewed system outputs, applied defined criteria, and verified compliance with policy.

This is not about whether your automated evals are good enough. This is about whether you can produce an audit trail when a regulator or customer asks how you validated model quality. The audit trail must show who reviewed what, when, using what criteria, and what they found. A spreadsheet someone filled out last year does not count. A documented review process with version-controlled rubrics, timestamped review records, and aggregated findings does.

A financial services AI that provides investment advice must comply with regulations that require human oversight of algorithmic recommendations. You cannot tell the regulator that your automated eval suite achieved ninety-four percent accuracy on your test set. The regulator wants to see evidence that licensed financial advisors reviewed outputs, verified they met suitability standards, and documented any errors. Human review infrastructure provides that evidence. Every review session is logged. Every judgment is recorded. Every finding is timestamped and attributed to a qualified reviewer. When the audit happens, you produce the records.

The cost of not having this infrastructure is that you cannot prove compliance. The regulator assumes you are not compliant. You are fined, your product is pulled from the market, or your customer terminates the contract. The cost of having the infrastructure is that you review a sample of outputs every month, document the findings, and maintain the records. The difference between the two costs is the difference between operating legally and operating in violation.

## Mandatory Scenario Three: New Task Domains Where Ground Truth Is Undefined

Human review is mandatory when you are building an AI system for a task where you do not yet know what good looks like. This is the exploration phase. You do not have labeled data. You do not have automated evals. You do not have a rubric. You have a hypothesis about what the AI should do, and you need humans to review outputs and tell you whether the hypothesis is correct.

In this scenario, human review is not validating quality. Human review is defining quality. You generate a few hundred outputs from your prototype model. You have domain experts review them and answer three questions for each output: Is this output correct? If not, what is wrong with it? What would a correct output look like? The answers to these questions become your ground truth dataset. They also become the basis for your first automated eval suite.

A legal tech company building an AI to summarize discovery documents starts with no ground truth. They do not know what a good summary looks like. They do not know what level of detail is appropriate. They do not know what information is critical to preserve and what can be omitted. They generate summaries for a hundred documents. They have three senior litigators review every summary. The litigators mark which summaries are acceptable, which are too brief, which are too detailed, which omit critical information, and which add information not present in the source. Those reviews define what a good summary is. The company uses those reviews to label the hundred documents as ground truth. They use the patterns in the reviews to build an automated eval that checks for omitted critical information. Human review was mandatory because without it, the company had no way to evaluate the model.

This phase does not last forever. Once you have ground truth and automated evals, human review shifts from defining quality to validating quality. But at the start, human review is the only source of truth. Skipping it means you are building a model with no way to know if it works.

## Optional Scenario One: Investigating Divergence Between Metrics and User Feedback

Human review is optional but highly valuable when your automated eval metrics say the model is performing well, but user feedback says otherwise. Users complain that outputs are too verbose, too terse, too formal, too casual, condescending, confusing, or culturally inappropriate. Your automated evals show no degradation. The divergence indicates that your eval suite is not measuring something users care about.

Human review is how you investigate the gap. You collect examples of outputs that users flagged as problematic. You have reviewers evaluate those examples using your existing rubric. If reviewers agree with users that the outputs are problematic, but your automated evals marked them as correct, you have identified a gap in your eval suite. You update the suite to measure the dimension users care about. If reviewers disagree with users and believe the outputs are correct, you have a user expectation problem, not a model quality problem. You address it with better onboarding, clearer documentation, or a product change.

This review is optional in the sense that you could ignore user feedback and assume your automated evals are comprehensive. That assumption is wrong more often than not. The review is valuable because it aligns your quality measurements with user reality. But it is not mandatory in the way that high-stakes review or regulatory review is mandatory. You run this review when the cost of the divergence exceeds the cost of the investigation. If user churn is increasing and complaints are rising, you run the review immediately. If complaints are low and satisfaction is stable, you defer the review to the next quarterly quality audit.

## Optional Scenario Two: Spot-Checking Automated Eval Accuracy

Human review is optional for validating that your automated evals are measuring what you think they are measuring. Automated evals are code. Code has bugs. An eval that is supposed to check factual accuracy might have a bug that causes it to pass outputs with hallucinated dates. An eval that checks tone might be overfitting to your test set and missing tone errors in production. Human review is how you catch these bugs.

You run this review quarterly or after major eval suite changes. You take a random sample of examples that your automated evals marked as passing. You have humans review them using the same criteria the automated eval is supposed to enforce. You measure agreement between humans and the automated eval. If agreement is above ninety percent, the eval is working. If agreement is below eighty percent, the eval is broken. You debug the eval, fix the bug, and re-validate.

This review is optional because most of the time, your automated evals are working correctly. The value of the review is that it catches the edge cases where the eval is subtly wrong in a way that does not trigger obvious alarms. A factual accuracy eval that passes ninety-five percent of outputs but has a bug that causes it to pass five percent of outputs with hallucinated dates is not obviously broken. It looks like a high-performing eval. Human review surfaces the five percent error rate and triggers the fix.

## Unnecessary Scenario One: Tasks with Objective Ground Truth and Comprehensive Automated Coverage

Human review is unnecessary when the task has a single correct answer, that answer can be verified programmatically, and your automated eval suite has been validated to cover the full range of production inputs. Data extraction tasks, classification tasks with well-defined categories, translation tasks with reference translations — these are tasks where automated evaluation is sufficient after initial validation.

A model that extracts invoice line items into structured fields can be evaluated by comparing extracted values to ground truth. If the model extracts the correct vendor name, invoice number, line item descriptions, quantities, and amounts, the output is correct. If any field is wrong, the output is incorrect. There is no subjective judgment. You do not need a human to decide whether the extraction is good enough. The comparison is binary.

Even in these cases, you need human review during initial development to validate your ground truth and test set. You need humans to verify that your test set is representative of production data, that your ground truth labels are accurate, and that your automated eval is implemented correctly. But once that validation is complete and your automated eval shows stable performance over time, human review adds no value. The automated eval is faster, cheaper, and more consistent than human review.

The mistake is assuming your task is in this category without validating it. Many teams believe their task has objective ground truth when it actually has subjective elements. A summarization task seems objective until you realize that different stakeholders want different levels of detail. A classification task seems objective until you realize that examples near category boundaries are ambiguous. Human review during initial development is what tells you whether your task is truly objective or has hidden subjectivity.

## Unnecessary Scenario Two: Internal Research Prototypes with No User Exposure

Human review is unnecessary for early-stage research prototypes that are not user-facing and not making decisions with real consequences. If you are experimenting with a new prompting technique, a new retrieval strategy, or a new model architecture, and you are evaluating it purely on internal benchmarks, automated evaluation is sufficient. Human review is expensive and slow. Spending human time reviewing outputs from a prototype that will never reach production is waste.

The boundary is user exposure. The moment your system is exposed to real users, even in a limited beta, human review becomes necessary. The moment your system makes decisions that affect real outcomes, even in a pilot, human review becomes mandatory. But for pure research work where the goal is to compare approaches and decide what to build next, automated evaluation on held-out test sets is enough.

The risk is that teams defer human review too long and ship prototypes to users without ever validating quality through human judgment. The prototype performs well on internal benchmarks, leadership approves it for beta launch, users start interacting with it, and then you discover that the model fails in ways your benchmarks did not measure. By the time you realize you need human review, the damage is done. The safe default is to run a small human review of a few hundred outputs before any user-facing launch, even if the prototype performed well on automated evals. That small review catches the gaps your benchmarks missed.

## The Risk-Based Decision Framework

The decision about whether human review is mandatory, optional, or unnecessary is a risk assessment. You estimate the probability of an undetected error, the cost of that error if it occurs, and the cost of human review to detect it. If the expected cost of undetected errors exceeds the cost of human review, human review is mandatory. If the expected cost is lower but non-trivial, human review is optional and you run it when budget and time allow. If the expected cost is negligible, human review is unnecessary.

A medical AI that recommends treatments has a high probability of errors your automated evals miss and a very high cost per error. A single missed drug interaction could kill a patient and expose you to malpractice liability. The expected cost of undetected errors is in the millions. The cost of human review is thousands per week. Human review is mandatory.

A chatbot that answers customer questions about store hours has a low probability of catastrophic errors and a low cost per error. If the chatbot gives the wrong hours, the customer is mildly inconvenienced. The expected cost of undetected errors is low. The cost of human review is still thousands per week. Human review is optional. You run it quarterly to spot-check quality, but you do not review every output.

A model that generates synthetic test data for your internal QA team has negligible user impact. If the data is bad, your QA team catches it during testing. No customer is harmed. The expected cost of undetected errors is effectively zero. Human review is unnecessary. Automated evals that check for format compliance and distribution coverage are sufficient.

The framework clarifies that human review is not a quality ritual you perform because best practices say so. Human review is a cost you incur when the risk justifies it. Teams that review everything waste money. Teams that review nothing take risks they do not understand. The discipline is matching review intensity to risk.

## The Minimum Viable Review for Mandatory Cases

When human review is mandatory, the question is not whether to do it, but how much is enough. You cannot review every output. Even high-stakes systems generate thousands of outputs per day. Reviewing all of them would require a team of fifty reviewers working full-time. The alternative is statistical sampling. You review enough outputs to detect errors at a defined confidence level.

If your risk tolerance is that no more than two percent of outputs can contain critical errors, you calculate the sample size required to detect a two percent error rate with ninety-five percent confidence. The formula depends on your total output volume, but for most production systems, a weekly sample of three hundred to five hundred outputs is sufficient. If you review five hundred outputs and find fifteen critical errors, your estimated error rate is three percent with a confidence interval that exceeds your threshold. You halt releases and investigate.

This minimum viable review is what makes mandatory human review economically feasible. You do not review everything. You review enough to know with statistical confidence that your error rate is acceptable. You build the infrastructure to make that review fast, consistent, and actionable. You enforce the review as a gate. This is the standard for high-stakes AI systems in 2026. Anything less is professional negligence.

The next subchapter describes the anatomy of a human review task and the six components that determine whether review is fast or slow, consistent or chaotic.
