# 4.10 — Guideline RFC Process: Propose, Test, Approve, Roll Out

Guidelines are code for human judgment. They define the standard, shape every decision, and accumulate complexity over time. And like code, they should never be edited directly in production, deployed without testing, or changed by a single person without review. The teams that treat guidelines as living documents maintained in shared folders — editable by anyone, versioned by filename suffixes, deployed by Slack announcement — discover that their review infrastructure collapses under the weight of inconsistency. Reviewers work from different versions. Changes introduce silent regressions. Nobody knows which guideline is canonical.

The solution is not more process for the sake of process. It is applying the same discipline to guideline changes that engineering teams apply to code changes: version control, review workflows, testing, and staged rollout. The guideline RFC process creates a structure where changes are proposed, debated, tested on real data, approved by stakeholders, and deployed incrementally. It transforms guideline management from ad-hoc document editing into a controlled, auditable system.

## Version Control for Guidelines

Every guideline lives in version control. Not in Google Docs, not in Notion, not in a wiki with revision history that nobody checks. In Git, with commit messages, branches, pull requests, and merge history. This is not symbolic. Version control gives you diff views, blame logs, rollback capability, and a complete audit trail of every change ever made to your judgment standards.

The repository structure mirrors your task taxonomy. One directory per task type. One file per major guideline section. Configuration files for rubrics, scoring thresholds, and edge case rules. The guideline repository becomes the source of truth for what reviewers should do, and the deployment pipeline ensures that what reviewers see matches what the repository contains. No divergence, no drift, no mystery versions floating in email attachments.

Version control also enables branching. When someone proposes a guideline change, they create a branch. The proposal exists in isolation until it is reviewed, tested, and approved. Multiple proposed changes can exist simultaneously without interfering with each other. The main branch remains stable. Reviewers always work from main. Proposed changes live in branches until they are ready for production.

## The RFC Template

A guideline change proposal is not a Slack message saying "we should clarify the toxicity section." It is a structured document that forces the proposer to think through the change, justify it, and specify exactly what will be different. The RFC template standardizes this. Every proposal contains the same sections, answers the same questions, and provides the same evidence.

The template starts with the motivation. What problem does this change solve? What confusion exists today? What errors are reviewers making? What product change requires updated guidelines? The motivation section is specific. It cites real examples — reviewer questions, failed calibration sessions, production incidents, user complaints. A vague motivation like "improve clarity" gets rejected. A concrete motivation like "twelve reviewers in the past month marked sarcasm as toxicity because the guideline does not distinguish intent from impact" moves forward.

Next comes the proposed change. The RFC includes before-and-after text. You see the current guideline language, then the proposed replacement, side by side. No ambiguity about what will change. The diff is explicit. If the change affects rubric scoring, the RFC includes the updated rubric. If it introduces new edge case rules, those rules are written out in full. The proposed change is complete and ready to deploy if approved.

The impact assessment follows. Who is affected by this change? How many reviewers will need retraining? Does this change scoring in ways that affect historical labels? Will models trained on old labels need to be reevaluated? Does this change conflict with compliance requirements, legal review, or customer agreements? The impact assessment forces the proposer to think beyond the text change to the operational and systemic consequences.

The testing plan specifies how the change will be validated before rollout. How many historical labels will be re-reviewed under the new guideline? What percentage agreement is required between old and new interpretations? Will a small group of reviewers pilot the change first? What metrics will confirm that the change improves consistency rather than creating new confusion? The testing plan is not optional. A proposal without a testing plan does not move to approval.

Finally, the rollout plan. How will the change be deployed? All reviewers at once or phased by cohort? Will reviewers be required to complete a calibration exercise before using the new guideline? How will the change be communicated? What support will reviewers receive during the transition? The rollout plan ensures that a good change is not sabotaged by poor deployment.

## Stakeholder Review

Guideline changes are not purely operational decisions. They affect product behavior, compliance posture, user experience, and model performance. The RFC review process brings the right stakeholders into the conversation before a change is locked in. The review is structured. Each stakeholder group has a defined role, a clear set of concerns to evaluate, and a deadline to respond.

Product reviews the change for alignment with product intent. Does this change make the system more or less aligned with what users expect? Does it introduce behavior that conflicts with product positioning? If the guideline change affects what the system considers acceptable, Product confirms that the change reflects intended product direction, not reviewer drift.

Legal reviews the change for compliance and risk. Does the updated guideline introduce liability? Does it conflict with contractual obligations, regulatory requirements, or platform policies? Legal does not micromanage language, but they flag any change that shifts what the system permits or forbids in ways that create legal exposure.

Engineering reviews the change for operability. Does the new guideline require changes to the review interface? Does it depend on data that reviewers do not currently see? Does it introduce complexity that will slow review throughput or require new instrumentation? Engineering ensures that the guideline is implementable within the current infrastructure or flags what needs to be built to support it.

Trust and Safety reviews the change for safety implications. Does the updated guideline weaken protections against harm? Does it introduce edge cases that attackers could exploit? Does it change how the system handles high-risk content? Trust and Safety ensures that guideline changes do not degrade safety in pursuit of other goals.

Domain experts review the change for accuracy. If the guideline covers medical content, a clinician reviews it. If it covers legal content, a lawyer reviews it. If it covers content moderation, someone with subject matter expertise in online harm reviews it. Domain experts catch errors that generalists miss — technical inaccuracies, misunderstandings of specialized terminology, rules that sound reasonable but are wrong in practice.

Each stakeholder approves or requests changes. The RFC is not deployed until all required approvals are received. This is not consensus theater. Stakeholders have veto power over changes that create unacceptable risk in their domain. If Legal blocks a change, the change does not happen. If Trust and Safety requires modifications, those modifications are made before approval.

## Testing on Historical Data

Before a guideline change reaches reviewers, it is tested on historical labels. You take a sample of previously labeled data — typically 200 to 500 examples across the full distribution of edge cases — and you have a small group of reviewers re-review those examples using the proposed guideline. Then you compare the new labels to the original labels. You are not looking for perfect agreement. You are looking for acceptable consistency and clear improvement where the guideline was previously ambiguous.

If agreement drops below 85 percent on cases that were previously clear, the guideline introduced new confusion. If agreement improves on cases that were previously disputed, the guideline is working. If new edge cases emerge that the proposed guideline does not cover, the guideline goes back to revision. The testing phase is not a formality. It is the place where well-intentioned changes are discovered to be unworkable in practice.

Testing also reveals hidden dependencies. A guideline change that looks simple in isolation may conflict with other guidelines that reference the same concepts. Testing exposes these conflicts before they reach production. Reviewers using the new guideline encounter situations where two guidelines now contradict each other, or where a clarification in one section creates ambiguity in another. These conflicts are resolved during testing, not discovered in production.

The testing group is not random. You select reviewers who represent the full range of experience levels and interpretation tendencies. Senior reviewers who know the edge cases. New reviewers who are still learning. Reviewers who historically interpret strictly and reviewers who interpret leniently. If the guideline works for all of them, it is robust. If it only works for one group, it is not ready.

## Approval Workflow

The approval workflow is linear and gated. Each stage must complete before the next begins. The RFC is submitted. Stakeholders review in parallel. All required approvals are collected. Testing is conducted. Results are reviewed. If testing passes, the change moves to rollout planning. If testing fails, the change returns to revision or is abandoned.

No shortcuts. No "we will fix it after rollout." No "Legal already approved it verbally so we can skip the formal review." The workflow exists because informal processes fail at scale. When you manage 50 reviewers, you can coordinate changes via Slack. When you manage 500, you cannot. The workflow is the only thing that prevents chaos.

Approval decisions are documented. Who approved, when, and with what conditions. If Legal approved with the condition that a specific disclaimer be added to the review interface, that condition is recorded and tracked. If Product approved with the understanding that a follow-up change will address a separate concern, that follow-up is logged and scheduled. Documentation ensures that approvals are not misunderstood, conditions are not forgotten, and commitments are not lost.

## Phased Rollout

Guideline changes do not deploy to all reviewers simultaneously. They roll out in phases. The first phase is a small pilot group — typically 10 to 20 reviewers who are experienced, communicative, and good at identifying problems. The pilot group uses the new guideline in production for one to two weeks. During that time, you monitor their work closely. You review their labels, solicit feedback, track whether they are encountering edge cases the guideline does not cover, and measure whether their consistency improves.

If the pilot succeeds, the change rolls out to the next cohort — typically 20 to 30 percent of all reviewers. This cohort is large enough to reveal scaling issues that the pilot missed but small enough that you can roll back without massive disruption if something goes wrong. The second phase runs for another one to two weeks. You continue monitoring, continue collecting feedback, and compare metrics between the cohort using the new guideline and the cohort still using the old one.

If the second phase succeeds, the change rolls out to all reviewers. By this point, the guideline has been tested on historical data, piloted with a small group, scaled to a mid-sized cohort, and refined based on real production feedback. The risk of a catastrophic failure is low. The risk of silent drift is low. You have confidence that the change is ready for full deployment.

Each phase includes a calibration session. Before reviewers begin using the new guideline, they complete a calibration exercise that covers the changes, explains the rationale, and walks through updated examples. Reviewers do not discover the change by accident while reviewing. They are trained on it explicitly. The calibration session is recorded and added to onboarding materials so that new reviewers hired during the transition receive the same training.

## Communicating Changes to Reviewers

Reviewers do not read RFC documents. They need a different communication artifact — one that explains what changed, why it changed, and what they should do differently. The change announcement is short, specific, and action-oriented. It opens with the problem the change solves. It summarizes the key updates in plain language. It provides before-and-after examples. It links to the full guideline text and the calibration exercise.

The announcement is delivered through the review interface, not through email. When a reviewer logs in after the change has rolled out to their cohort, they see a notification that requires acknowledgment. They cannot proceed to reviewing until they have read the announcement and confirmed that they understand the change. This is not bureaucratic friction. It is the only way to ensure that reviewers are not working from outdated mental models.

High-impact changes receive additional support. A live Q&A session where reviewers can ask questions. Office hours where they can discuss edge cases. A feedback channel where they can report confusion or conflicts. The goal is not to control interpretation. The goal is to ensure that every reviewer has the information and support they need to apply the guideline consistently.

## Version Tagging and Rollback Capability

Every deployed guideline is tagged with a version number. Reviewers see the version in the review interface. Labels are tagged with the guideline version that was active when the label was created. This means you can always trace a label back to the exact guideline that governed it. If a future change makes old labels suspect, you know which labels were made under which standard.

Version tagging also enables rollback. If a guideline change is deployed and then discovered to be flawed, you roll back to the previous version. The rollback is a deployment event, not a manual edit. You revert the guideline repository to the prior commit, trigger the deployment pipeline, and reviewers see the old guideline again. Labels made under the bad guideline are flagged for review. The bad version is retired and marked as deprecated.

Rollback is rare but essential. It exists for the same reason that software deployments have rollback plans. Not every change works. Not every change can be tested comprehensively before production. When a change fails, you need the ability to undo it cleanly and quickly. Version control and deployment discipline make rollback possible. Without them, you are stuck trying to manually reconstruct what the guideline used to say and hoping everyone remembers what they were doing before the change.

The next subchapter covers breaking changes — guideline updates that are not backward-compatible with prior training and require reviewers to be retrained before they can continue working.

