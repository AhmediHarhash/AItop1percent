# 3.7 — Reviewer Dashboards: Performance, Progress, and Feedback

In October 2025, a content moderation operation with 340 contract reviewers rolled out a new dashboard. Every reviewer could see their accuracy score, updated hourly. They could see how they ranked against their peers. They could see their throughput, their agreement rate with consensus, and their average time per task. The dashboard was beautiful. The engineering team was proud. Within three weeks, accuracy dropped by eleven percentage points across the entire operation. Reviewers started rushing through edge cases to maintain throughput. They started second-guessing their judgment to align with perceived team norms. They started optimizing for metrics instead of quality. The dashboard destroyed the thing it was designed to measure.

The failure wasn't technical. The dashboard worked perfectly. The failure was in understanding what reviewers need to see versus what managers want them to see. Reviewer dashboards are not neutral. They are incentive systems disguised as information displays. Show a reviewer their rank, and they will optimize for rank. Show them their speed, and they will optimize for speed. Show them their error rate without context, and they will either become paralyzed by self-doubt or start gaming the system to look better. What you measure, and how you display it, determines what behavior you get.

## What Reviewers Need to See

Reviewers need three types of information: where they stand, how they're improving, and what they should focus on. These are not the same as throughput metrics, error rates, and peer rankings. Those are management metrics. Reviewers don't need to know that they completed 487 tasks this week. They need to know whether they're on track to finish their assigned queue by end of shift, and whether their judgment quality is stable.

Progress against personal goals is more useful than absolute numbers. Show a reviewer "64 out of 120 tasks completed today, on track to finish by 5 PM" instead of "64 tasks completed." The first tells them whether they need to speed up or if they have time to slow down and focus on quality. The second is just a number. Context makes information actionable.

Accuracy should be shown with calibration context, not as a raw percentage. Telling a reviewer "your accuracy this week is 91 percent" sounds good until they learn the team average is 94 percent. Then it sounds like failure. Better to show "your accuracy is within normal range, and you're improving on medical terminology tasks where you struggled last week." This is harder to compute. It requires tracking per-category performance over time and comparing against meaningful baselines. But it's what reviewers actually need. Raw percentages without context create anxiety, not insight.

Feedback on where to improve should be specific and recent. A dashboard that says "you missed 12 tasks this week" doesn't help. A dashboard that says "yesterday you marked three tasks as policy-violating when the consensus was borderline, review the policy section on contextual exceptions" is actionable. The difference is granularity and timing. Aggregate weekly stats are useful for managers trying to spot trends. Individual reviewers need task-level, category-level, real-time guidance.

Queue depth visibility is underrated. Reviewers make different decisions when they're staring at a 600-task backlog versus a 40-task backlog. If the queue is huge, they'll rush. If it's nearly empty, they'll slow down. Both are rational. Let reviewers see the queue size for their tier and specialty, not just their personal assignments. If the specialist medical queue has 200 tasks and the generalist queue has 30, the medical specialists need to know that. It changes how they prioritize.

## What Managers Need to See

Manager dashboards serve a different function. They're for capacity planning, quality monitoring, and identifying reviewers who need coaching. The mistake most teams make is showing managers the same dashboard reviewers see, just with access to everyone's data. That misses the point. Managers don't need to know that Emily completed 487 tasks this week. They need to know that the team's accuracy on medical content dropped by four percentage points over the last three days, and it correlates with a new task type that launched Monday.

Aggregate trends matter more than individual performance. Show team-level accuracy over time, broken down by task type and difficulty. Show agreement rates between reviewers and with consensus. Show time-per-task distributions, not averages, because averages hide bimodal patterns. If half your reviewers take two minutes per task and half take seven, the average is meaningless. The distribution tells you that you have two populations with different workflows or skill levels.

Outlier detection is where individual metrics become relevant. If a reviewer's accuracy is fifteen percentage points below team average for two consecutive weeks, that's a coaching conversation. If a reviewer's throughput suddenly doubles while accuracy stays stable, that's worth understanding—maybe they figured out a better workflow. If a reviewer's skip rate is three times the norm, something is wrong with their task feed or their understanding of the rubric. Dashboards should highlight these outliers automatically, not force managers to dig through spreadsheets.

Capacity utilization is critical for operations planning. Show how much of your review capacity is being used, broken down by tier and specialty. If your specialist queue is at 95 percent capacity and your generalist queue is at 60 percent, you need to hire specialists or reroute tasks. If capacity is consistently under 70 percent, you're overstaffed or your task pipeline is undersupplying. These aren't reviewer performance issues. They're system design issues. The dashboard should make them visible.

## Real-Time vs End-of-Day

Real-time dashboards sound modern and sophisticated. They're also dangerous if misused. Showing a reviewer their accuracy score updating every ten minutes creates a feedback loop that distorts judgment. The reviewer starts thinking about their score instead of the task in front of them. They second-guess themselves. They align with what they think the system wants instead of applying their own expertise. Real-time feedback feels responsive. It often makes quality worse.

End-of-day summaries are better for most metrics. At the end of a shift, show the reviewer how they did, what they improved on, and what to focus on tomorrow. This gives them time to reflect without the pressure of immediate correction. It also reduces noise. A reviewer who has a bad hour—three tough tasks, two mistakes—doesn't need to see their score drop in real time. By end of day, those three tasks are averaged with fifty good ones, and the signal is clearer.

There are exceptions. Queue progress should be real-time because it affects pacing decisions. If a reviewer is halfway through their shift and only 30 percent through their queue, they need to know now, not at 5 PM. Similarly, if they're ahead of schedule, let them know. Real-time progress tracking helps reviewers manage their own time. Real-time quality scoring does not help. It creates anxiety and distorts behavior.

Critical alerts belong in real-time. If a reviewer marks ten tasks in a row with the same label when historical patterns suggest more variance, flag it immediately. They might have misunderstood a new guideline. They might be tired and defaulting to the easiest judgment. They might have hit a cluster of genuinely similar tasks. Real-time doesn't mean "show them their accuracy score." It means "interrupt them when something looks systematically wrong." The goal is to catch errors before they compound, not to make reviewers nervous.

## Gamification Dangers

Leaderboards, badges, and achievement systems are tempting because they work in consumer products. They're toxic in review systems. The moment you introduce a leaderboard ranking reviewers by throughput, you've told every reviewer that speed matters more than quality. Even if you say accuracy matters, even if you have accuracy metrics on the same dashboard, the leaderboard sends the real message.

Gamification optimizes for the metric you gamify. If you give badges for "100 tasks in one day," you will get reviewers racing to 100 tasks. Quality will suffer. If you give badges for "10 days with zero errors," you will get reviewers who are afraid to take on hard tasks. They'll skip or defer anything ambiguous to protect their streak. The badge didn't incentivize quality. It incentivized risk aversion.

Some teams try to gamify quality by giving points for accuracy, agreement with consensus, or calibration performance. This is marginally better but still flawed. Reviewers will optimize for agreement rather than independent judgment. If the consensus is wrong, a reviewer who applies correct judgment will lose points. Over time, the system punishes independent thinking. You end up with a team that converges on mediocrity.

The only safe use of gamification is for non-judgment behaviors: completing training modules, providing feedback on confusing tasks, contributing to guideline improvements. These are valuable activities that don't directly affect label quality. Rewarding them doesn't distort the core work. Even then, keep rewards symbolic. A badge is fine. A cash bonus tied to badges creates the wrong incentives.

If your organization insists on gamification, make it team-based, not individual. "The medical review team completed 10,000 tasks this month with 96 percent accuracy" is a collective win. It doesn't pit reviewers against each other. It also doesn't reward cutting corners because accuracy is part of the goal. Team-based metrics are less toxic than individual ones, though still risky. The safest approach is no gamification at all.

## Transparency vs Surveillance

Reviewers should have access to their own data. They should be able to see which tasks they labeled, which ones they got wrong, and what the correct labels were. This is transparency. It builds trust and enables self-improvement. Surveillance is when managers can see every task a reviewer touched, how long they spent on it, and whether they paused to look something up. The line between transparency and surveillance is whether the data is used to help reviewers improve or to micromanage them.

Audit trails are necessary for quality control and compliance. If a high-stakes label is disputed, you need to know who made the call and what information they had. If a reviewer is systematically mislabeling a category, you need to identify the pattern and retrain them. These are legitimate uses of detailed data. They become surveillance when managers use the data to question every decision, compare reviewers' time-per-task down to the second, or punish reviewers for taking bathroom breaks.

The test is whether reviewers feel the system is on their side. If reviewers believe the dashboard exists to help them do better work, they'll use it. If they believe it exists to catch them making mistakes or slacking off, they'll resent it. Resentful reviewers do not produce high-quality labels. They produce labels that look good on the dashboard. The difference is critical.

Make the dashboard opt-in for detailed views. Let reviewers decide whether they want to see time-per-task breakdowns or just overall progress. Let them control whether their data is visible to peers or just to managers. Some reviewers want full transparency. Others find it stressful. A one-size-fits-all dashboard assumes all reviewers respond the same way to information. They don't. Configurability reduces stress and increases trust.

## Feedback Delivery Timing

Feedback on individual tasks should happen as close to the task as possible, ideally within 24 hours. If a reviewer makes a mistake on Monday and doesn't hear about it until Friday, the context is gone. They won't remember the task. They won't understand what they got wrong. The feedback becomes trivia, not learning.

Immediate feedback during the task is too much. If a reviewer labels a task and the system immediately says "wrong, the correct answer is X," the reviewer learns to rely on the system instead of their judgment. They stop thinking critically. They wait for correction. This is fine for training exercises. It's destructive in production. Production reviewers need to develop independent judgment, not dependency on real-time correction.

The right window is after the task is submitted but before the end of the day. If consensus is fast, show feedback within an hour. If consensus takes longer, batch it into an end-of-shift summary. The goal is to give reviewers enough time to think independently but not so much time that the task is forgotten.

Positive feedback matters as much as negative feedback. If a reviewer handled a genuinely difficult edge case correctly, tell them. Most feedback systems only surface errors. This creates a perception that the only time anyone notices your work is when you screw up. Over time, reviewers become defensive and risk-averse. Balanced feedback—highlighting both mistakes and successes—keeps morale high and judgment independent.

Aggregate feedback at the end of each week gives reviewers a chance to see progress. "This week you improved your accuracy on legal content from 88 to 93 percent" is motivating. It shows that effort leads to improvement. Weekly feedback also smooths out daily variance. A bad day doesn't define a reviewer. A bad week might indicate a real issue. The cadence matters.

Feedback should be private by default. Some reviewers are comfortable with public recognition. Most are not. If you display individual performance in a shared dashboard, some reviewers will feel exposed and judged. Make public dashboards team-level and aggregate. Make individual dashboards private. Let reviewers choose whether to share their stats with peers. Forced transparency creates social pressure that distorts judgment just as much as gamification does.

---

*Next: 3.8 — Mobile and Remote Review Considerations*
