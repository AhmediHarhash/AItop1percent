# 2.14 — Anti-Patterns: Prompt Smells and How to Refactor Them

A legal tech startup deployed a contract analysis AI in November 2025 that was supposed to extract key clauses and flag risky terms. The system worked beautifully in testing with their carefully curated evaluation set. In production, accuracy collapsed to 43% within the first week. The engineering team was baffled. They had invested six weeks in prompt development, incorporated feedback from legal experts, and tested extensively. What went wrong?

The post-mortem revealed a collection of prompt anti-patterns that testing had hidden. The prompt included contradictory instructions copied from three different legal experts without reconciliation. It contained examples that the team had cherry-picked for testing success but that didn't represent actual contract diversity. Critical instructions were buried at the end of a 3,100-token prompt where the model rarely gave them weight. The prompt had accumulated layers of patches and workarounds until its core logic was incoherent.

No single issue caused the failure. The prompt had degraded through a series of small compromises and untested assumptions, each reasonable in isolation but toxic in combination. The team needed to learn prompt code review practices—ways to identify problematic patterns before they reached production and systematic refactoring approaches to fix them.

## Over-Instruction and Constraint Conflict

Over-instruction happens when you keep adding more and more specific rules without removing obsolete ones. Your prompt tells the model to "be concise" and "provide comprehensive details" and "limit responses to 100 words" and "include relevant context." These instructions conflict. The model will follow whichever instruction receives the strongest attention weight, producing inconsistent behavior across different inputs.

You can spot over-instruction by looking for multiple rules about the same aspect of the output. If you have three separate sentences about response length, you're over-instructing. Consolidate them into one clear constraint. If you have formatting requirements scattered across different sections of your prompt, you're over-instructing. Group formatting rules into one section with clear hierarchy.

Conflicting constraints are even more insidious because they appear in different forms that aren't obviously contradictory. "Always include source citations" conflicts with "keep responses under 50 words" for most complex queries. "Be friendly and conversational" conflicts with "use precise technical terminology" in most domains. When you introduce a new constraint, audit existing constraints for potential conflicts.

Refactor over-instruction by extracting the essential requirement and deleting everything else. Ask: what is the actual outcome I need? Then write the single clearest instruction that produces that outcome. Most over-instruction comes from iterative patches where each new issue prompts adding another rule. Resist this pattern. When you need to modify behavior, refactor the core instruction rather than layering patches.

## Cargo-Cult Patterns and Ritual Instructions

Cargo-cult prompt engineering happens when you include instructions or patterns because you saw them work elsewhere, without understanding why they work or whether they apply to your use case. Common cargo-cult elements include "let's think step by step" in prompts where no reasoning chain is needed, extensive politeness framing in prompts where the model isn't customer-facing, or chain-of-thought prompting in simple classification tasks.

These patterns aren't inherently bad. They're inappropriate when applied without understanding their purpose. "Let's think step by step" helps with complex reasoning tasks because it encourages the model to show its work and catch logical errors. It adds noise in simple lookup or classification tasks where no reasoning chain exists. Politeness framing matters when outputs go directly to users. It wastes tokens in internal processing where no one reads the model's phrasing.

You can identify cargo-cult patterns through ablation testing. Remove the suspected cargo-cult element and measure whether performance changes. If "let's think step by step" has no impact on your classification accuracy, it's cargo cult. If elaborate role-playing ("you are an expert financial analyst with 20 years of experience") doesn't change output quality, it's cargo cult. Test and measure rather than assuming.

Refactor cargo-cult patterns by deleting them and validating that performance holds. Most prompts improve when you remove unnecessary elements. The model has less noise to process and can focus on genuine requirements. If you're afraid to delete something because "it might help," that fear is itself a sign of cargo-cult thinking. Test the deletion. Data defeats superstition.

## Copy-Paste Prompt Evolution

Copy-paste evolution happens when you duplicate an existing prompt to create a new one, modify it slightly, and deploy it without fully adapting it to the new context. You end up with prompts containing instructions for different tasks, references to examples that don't exist in the prompt, or constraints that made sense in the original context but are wrong in the new one.

The legal tech startup's prompt showed classic copy-paste evolution. It included instructions about email tone that made sense when it was copied from a customer service prompt but were irrelevant for contract analysis. It referenced "the customer's inquiry" in several places even though the task was analyzing legal documents, not answering questions. These vestigial instructions didn't break the prompt obviously, but they added noise and confusion.

Spot copy-paste evolution by reading your prompt with fresh eyes. Any instruction that references entities or contexts that aren't part of your task is probably vestigial. Any constraint that seems oddly specific or tangential to your core task likely came from a different context. Any example that doesn't quite fit your domain probably survived from the template you started with.

Refactor copy-paste prompts by rewriting from scratch based on your actual requirements. This sounds drastic, but it's often faster and more effective than trying to patch a copied prompt into correctness. Write down what your task actually needs to accomplish. List the constraints that actually matter. Select examples that actually represent your domain. Build the prompt fresh. Reference the copied prompt for ideas, but don't treat it as a foundation to modify.

## Prompt Drift and Undocumented Assumptions

Prompt drift happens gradually as multiple people modify a prompt over time, each making small changes without understanding the full context. The original author knew why certain instructions were phrased carefully. The second engineer tweaked phrasing to fix an edge case without realizing they changed the meaning. The third engineer added a constraint that subtly conflicts with existing logic. Six months later, no one fully understands why the prompt is structured the way it is.

Undocumented assumptions are closely related. The original prompt author assumed certain input characteristics or made trade-off decisions without documenting them. Future maintainers don't know these assumptions and make changes that violate them. A prompt might be carefully tuned for inputs of 200-500 words. Someone uses it on 50-word inputs and adds instructions to handle that case, breaking the tuning for the original input length range.

Combat prompt drift by treating prompts as code and using code review practices. Every prompt change should go through review where the reviewer asks: does this change conflict with existing logic? Does it maintain the prompt's coherent structure? Is the reason for the change clear? Review catches drift before it accumulates into incoherence.

Document assumptions explicitly in comments or in a companion document. "This prompt assumes input is extracted from customer emails and has already been sanitized for PII. Do not use for raw user input." "This classification is tuned for US English. International English requires different examples." These notes prevent future maintainers from making changes that violate implicit assumptions.

## Example-Instruction Misalignment

Your examples and your instructions should tell the same story. Example-instruction misalignment happens when they contradict each other. Your instructions say "be concise," but your examples are elaborate. Your instructions say "extract only explicit information," but your examples include inferred details. The model receives conflicting signals and behavior becomes unpredictable.

This misalignment often emerges when instructions and examples are developed by different people or at different times. Someone writes careful instructions, then someone else adds examples based on what works in practice rather than what the instructions specify. Or the instructions are updated without updating the examples to match. Either way, the prompt contains internal contradictions.

Spot misalignment by reading your instructions, then reading your examples, and asking whether the examples demonstrate what the instructions prescribe. If you specify structured output format in instructions but your examples show free-form text, that's misalignment. If you specify uncertainty handling in instructions but your examples never show uncertain cases, that's misalignment.

Refactor misalignment by making examples canonical. If your examples represent correct behavior, update instructions to match. If your instructions represent correct requirements, update or replace examples to demonstrate them. Don't try to maintain separate truth in instructions and examples. Pick one as the source of truth and make the other consistent with it.

## Untested Edge Case Handling

Many prompts include elaborate edge case handling logic that has never been tested on actual edge cases. The logic sounds reasonable, but it doesn't work in practice. Your prompt says "if the input is ambiguous between categories A and B, choose category A," but you've never tested this on actually ambiguous inputs. In production, the model ignores this instruction or interprets "ambiguous" differently than you intended.

Untested edge case handling is usually well-intentioned. Engineers anticipate problems and add instructions preemptively. But anticipation isn't validation. Your edge case handling might be wrong, incomplete, or based on edge cases that don't actually occur in production. Without testing, you won't know until production traffic reveals the gaps.

Build test cases specifically for your edge case handling. If your prompt includes logic for handling missing fields, create test inputs with those fields missing and verify the model follows your specified behavior. If your prompt includes disambiguation rules, create ambiguous test cases and verify the model applies your rules correctly. Test the edge cases your prompt claims to handle.

Refactor untested edge case handling by deleting it and waiting for actual edge cases to appear. This sounds reckless, but it's often the right approach. Real production edge cases inform better edge case handling than hypothetical edge cases imagined during development. Start with core functionality. Add edge case handling reactively based on observed failures. Your edge case handling will be simpler, more relevant, and actually tested.

## Prompt Bloat and Feature Creep

Prompts grow over time as teams add features, handle new edge cases, and incorporate feedback. Like codebases, prompts accumulate bloat. What started as a focused 400-token prompt becomes a sprawling 2,800-token behemoth that tries to handle every possible scenario. Performance degrades, maintenance becomes painful, and no one dares refactor because they don't understand what's still needed.

Prompt bloat has real costs beyond token expense. Large prompts take longer to process. They dilute the model's attention across too many instructions. They make changes risky because interactions between different sections become unpredictable. They slow down engineering velocity because understanding and modifying the prompt requires significant cognitive load.

Fight prompt bloat through regular refactoring. Schedule quarterly prompt reviews where you audit what instructions are actually necessary and what's accumulated cruft. Use ablation testing to identify sections that don't affect performance. Archive old versions so you can safely delete instructions knowing you can retrieve them if needed. Treat prompt size as a metric to optimize, not just an implementation detail.

Consider splitting bloated prompts into multiple focused prompts. If your prompt tries to handle five different subtasks, maybe you need five prompts, each optimized for its specific task. This increases operational complexity but decreases prompt complexity. The trade-off is often worth it when individual prompts drop from 2,500 tokens to 400 tokens and become maintainable again.

## Prompt Code Review Checklist

Establish a prompt review checklist that catches common anti-patterns before they reach production. Your checklist should include: Are instructions clear and non-contradictory? Do examples align with instructions? Is edge case handling tested? Are there vestigial instructions from copied templates? Is the prompt focused on a single coherent task? Is token count appropriate for task complexity?

Review for assumptions and document them. What input characteristics does this prompt assume? What trade-offs were made? Why are examples chosen the way they are? Why is this instruction phrased this specific way? These questions surface implicit knowledge and make it explicit for future maintainers.

Check for testability. Can you write automated tests that verify this prompt's behavior? If not, the prompt is probably too vague or too complex. Refactor until the prompt is specific enough that you can test whether it works. Testable prompts are almost always better prompts because testability forces clarity.

Verify that performance is actually measured. Every production prompt should have metrics: accuracy, consistency, error rates, user satisfaction. Review should check that these metrics exist and are being monitored. A prompt without metrics is a prompt without accountability. You can't improve what you don't measure.

## Refactoring Patterns for Prompt Improvement

When you identify a problematic prompt, systematic refactoring is more effective than ad-hoc tweaking. Start by establishing your baseline: current performance metrics, current token count, current error modes. Any refactoring that maintains performance while improving clarity or efficiency is good. Any refactoring that improves performance is better.

Extract reusable components from monolithic prompts. If your prompt contains a 300-token section on output formatting that's identical across multiple prompts, extract it into a shared component. This reduces duplication and makes updates easier. Changes to formatting requirements happen once, not in 15 different prompt files.

Consolidate redundant instructions into single, clear statements. If you say the same thing three different ways, pick the clearest way and delete the others. Consolidation almost always improves performance because it reduces noise and strengthens the signal from the important instruction.

Reorder instructions to put critical constraints first. Models give more weight to early instructions. If your most important requirements are buried in the middle or end of your prompt, move them to the beginning. This reordering often produces measurable accuracy improvements without changing the content at all.

## Testing Your Refactoring

Never deploy a refactored prompt without validation. Run your full test suite against both the original and refactored versions. Compare performance metrics. Check that error modes haven't shifted. Verify token count changes match your expectations. Refactoring should be evidence-based improvement, not hopeful change.

Use A/B testing for significant refactorings in production. Deploy the refactored prompt to a small percentage of traffic and compare results to the original prompt. Monitor for performance differences, error rate changes, and any unexpected behaviors. Gradual rollout catches issues that test suites miss.

Document what changed and why. Your commit message or change log should explain the anti-pattern you fixed, the refactoring approach you used, and the measured impact. This documentation helps your team learn from refactoring and builds institutional knowledge about what works.

Celebrate successful refactoring. When a refactoring reduces token count by 40% while maintaining accuracy, that's worth recognizing. When it improves accuracy by 6%, that's a significant win. Prompt engineering culture should value good refactoring as much as new feature development. Clean, maintainable prompts are as important as functional ones.

## Building Prompt Quality Culture

Anti-patterns persist because teams don't have systems to prevent them. Build prompt quality into your development process. Require prompt reviews before production deployment. Maintain a prompt style guide that documents best practices and common anti-patterns. Share post-mortems when prompts fail so the whole team learns from mistakes.

Create prompt linting tools that catch common issues automatically. Flag prompts over 2,000 tokens for review. Flag prompts with obvious contradictions ("be concise" + "be comprehensive"). Flag prompts with untested edge case handling. Automated checking catches issues that humans miss in review.

Invest in prompt testing infrastructure. Make it easy to run validation sets, easy to add new test cases, easy to compare prompt versions. When testing is low-friction, engineers test more. When testing is painful, corners get cut. Your testing infrastructure is as important as your prompt engineering skills.

Treat prompt engineering as engineering, not art. Prompts are code. They should be reviewed, tested, refactored, and maintained with the same discipline as any other code. Teams that adopt this mindset produce better prompts and avoid the anti-patterns that plague teams who treat prompts as ad-hoc text configuration.

Now that you understand common prompt anti-patterns and how to refactor them, you need to know how to safely handle user data within prompt templates without creating injection vulnerabilities.
