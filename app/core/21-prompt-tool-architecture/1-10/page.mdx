# 1.10 — The Role of Examples: Zero-Shot, Few-Shot, and Many-Shot

A healthcare SaaS company deployed a medical coding assistant in October 2024 using Claude Opus 4 with a carefully crafted zero-shot prompt. The prompt described ICD-10 coding rules in detail across 850 tokens. Their validation set showed 91% accuracy, meeting their launch threshold. They chose zero-shot to avoid the overhead of maintaining example sets across 12,000 procedure types.

By December 2024, they had processed 45,000 real patient encounters. Accuracy had dropped to 84% in production. The failure analysis revealed a clear pattern: rare procedures and ambiguous clinical notes triggered most errors. Their quality team manually created 30 examples covering the most common failure modes and added them to the prompt. Accuracy immediately jumped to 94%, exceeding their original target.

The team had assumed that detailed instructions could replace examples. They learned that examples teach patterns that are difficult to specify in prose. Examples show the model what good judgment looks like in ambiguous situations. The examples also compressed 850 tokens of instructions down to 600 tokens plus examples, reducing both complexity and latency while improving accuracy.

## Zero-Shot Performance as a Baseline

**Zero-shot** prompting provides only task instructions without any examples of inputs and outputs. Modern large language models can perform many tasks zero-shot because they learned similar patterns during training. Claude 4, GPT-4.5, and Gemini 2 can summarize text, extract entities, classify sentiment, and answer questions without ever seeing task-specific examples.

Zero-shot works when your task aligns closely with common training patterns. Summarizing news articles works zero-shot because models saw millions of articles during training. Extracting company names from press releases works zero-shot because this pattern is common in training data.

Zero-shot fails when your task requires domain-specific judgment, uses specialized formats, or involves subtle distinctions. Medical coding is rare in training data. Your company's specific ticket categorization scheme did not appear in any training data. The distinction between a billing error and a feature request in your support queue requires context the model does not have.

You establish zero-shot as a baseline before adding examples. If zero-shot achieves acceptable performance, adding examples might not justify the maintenance cost. If zero-shot performance has clear gaps, examples become your primary tool for closing them.

## Few-Shot Example Selection Strategies

**Few-shot** prompting adds a small number of examples to demonstrate the desired input-output mapping. Few-shot typically means 3-10 examples. This range works across most tasks because modern models generalize well from limited examples.

Example selection matters more than example quantity. Three well-chosen examples often outperform ten mediocre ones. Good examples are diverse, representative, and instructive. They cover the main task variations, represent the actual distribution of inputs you will see, and teach the model something it could not infer from instructions alone.

Diversity means examples should span the task space. If you are classifying support tickets into five categories, include at least one example per category. If inputs vary in length, include both short and long examples. If tone varies, show formal and informal examples.

Representativeness means examples should reflect production data characteristics. If 70% of your inputs are straightforward and 30% are ambiguous, your example set should roughly match that ratio. Examples drawn from an idealized or sanitized dataset teach the model patterns it will not encounter in production.

Instructiveness means examples should teach boundary cases and judgment calls. Include examples where the correct answer is not obvious from instructions alone. Show cases where similar inputs require different outputs due to subtle distinctions. Demonstrate how to handle missing information or ambiguous inputs.

## Many-Shot Learning in Long-Context Models

**Many-shot** prompting uses dozens or hundreds of examples when models have sufficient context windows. Claude Opus 4 supports 200,000 token contexts. GPT-4.5 supports 128,000 tokens. These windows enable example sets that would have been impossible with earlier models.

Many-shot makes sense when task complexity exceeds what you can specify in instructions. Instead of writing rules for every edge case, you show the model 100 examples covering diverse scenarios. The model learns implicit patterns from the example distribution.

The returns to additional examples diminish after a point. The jump from zero-shot to three-shot is usually dramatic. The jump from three-shot to ten-shot is meaningful. The jump from ten-shot to 100-shot is often marginal unless your task has high intrinsic complexity.

You validate whether many-shot justifies its cost by measuring performance gains per example added. If adding examples 30-40 improves accuracy by 2 percentage points but adding examples 60-70 improves it by 0.2 percentage points, you have hit diminishing returns. The optimal point balances performance gains against latency and cost increases from longer prompts.

Many-shot also enables dynamic task specification. Instead of maintaining explicit rules for how to handle regional variations in address formats, you include 50 examples from different countries. The model learns regional patterns implicitly. When new edge cases emerge, you add examples rather than updating rules.

## Example Ordering Effects

Example order influences model behavior more than most teams expect. Models are sensitive to recency, so later examples often carry more weight than earlier ones. Models also learn sequence patterns, so if all your positive examples appear before negative examples, the model may learn that order matters.

Randomizing example order during development helps you discover order dependencies. If performance varies significantly across different random orderings, your examples may be teaching unintended patterns. Stable performance across orderings suggests robust example selection.

Strategic ordering can improve performance when you understand the effect. Placing the most instructive or challenging examples last gives them more weight. Grouping similar examples together helps the model learn category patterns. Alternating between categories prevents sequence bias.

For production systems, you typically want consistent ordering to ensure reproducible behavior. Choose an ordering that performs well across your test set and lock it. Document why examples appear in that order so future maintainers understand the reasoning.

## When Zero-Shot Outperforms Few-Shot

Adding examples can hurt performance in specific scenarios. If your examples are not representative of production data, they teach the model the wrong patterns. If your examples contain subtle errors, the model learns to reproduce those errors. If your examples are too narrowly focused, they reduce the model's ability to generalize.

Zero-shot also wins when task requirements change frequently. Maintaining high-quality examples across rapidly evolving requirements creates ongoing overhead. If your product team ships new features weekly that change task semantics, the cost of updating examples may exceed the benefit.

Some tasks are easier to specify than to demonstrate. Simple transformations like "convert all dates to ISO 8601 format" or "extract all email addresses" work perfectly with instructions alone. Examples add tokens without adding information.

You measure whether examples help by A/B testing. Run the same inputs through zero-shot and few-shot variants. If few-shot does not improve accuracy by at least 3-5 percentage points, the examples are not earning their cost in latency and maintenance.

## Negative Examples and Boundary Specification

**Negative examples** show the model what outputs to avoid. They are particularly valuable for tasks where the boundary between acceptable and unacceptable outputs is subtle. A customer service prompt might include examples of responses that are technically accurate but have the wrong tone.

Negative examples work by showing contrasts. You provide an input, a bad output, and a good output. The model learns to distinguish the patterns that separate them. This is more effective than instructions saying "avoid being robotic" because the model sees concrete instances of what robotic means in your context.

The ratio of positive to negative examples matters. Too many negative examples can confuse the model or make it overly conservative. A typical ratio is 3-4 positive examples for every negative example. The negative examples should represent common failure modes, not random bad outputs.

Boundary cases benefit especially from negative examples. If you are classifying bug reports versus feature requests, show examples near the boundary where the distinction is unclear. One example might be a user describing unexpected behavior that turns out to be intended functionality. Show both how to recognize this case and how to respond appropriately.

## Dynamic Example Selection

Static example sets work for stable tasks. Dynamic selection adapts examples to each input. If you are classifying support tickets, you might retrieve the three most similar historical tickets and use them as examples for the current classification.

Dynamic selection requires infrastructure for similarity search, typically using embedding-based retrieval. You maintain an example database with embeddings. For each new input, you embed it, retrieve the nearest neighbors, and include them in the prompt. This is the foundation of **retrieval-augmented generation** (RAG) applied to example selection.

Dynamic examples provide better coverage than static examples when your input distribution is diverse. A static set of ten examples cannot cover all variations. A dynamic system draws from hundreds or thousands of examples, always providing relevant context for the current input.

The cost is added latency from retrieval and potentially longer prompts if you include many dynamic examples. You need to measure whether the performance gain justifies the added complexity. Dynamic selection makes most sense for high-value tasks where accuracy improvements directly impact revenue or user experience.

## Example Contamination Risks

Example contamination occurs when examples leak information that should not be available during task performance. If you are testing a model's ability to classify medical images, including examples from the test set contaminates the evaluation. The model succeeds not because it learned the task, but because it memorized the examples.

Contamination also appears when examples include information from the future. A fraud detection prompt might include examples where the fraud label is known but would not be available in production until days later. The model learns to use signals that will not exist when you actually deploy it.

You prevent contamination through temporal splitting and strict data hygiene. Training examples must come from periods before your test period. Examples must use only information that would be available at prediction time. If your model makes decisions in real-time but examples include information available only after batch processing, the examples are contaminated.

Contamination is particularly dangerous because it creates false confidence. Your test metrics look excellent, but production performance collapses because the model relied on information it will not have. Regular production monitoring catches this quickly, but prevention through proper example selection is better than detection.

## Example Maintenance and Versioning

Examples require ongoing maintenance as tasks evolve and model capabilities improve. An example set created for Claude 3.5 Sonnet in June 2024 may not be optimal for Claude Opus 4 in January 2026. The newer model might need fewer examples or different examples to achieve the same performance.

Version control for example sets allows rollback when updates degrade performance. Each example set version is tagged with creation date, target model, and performance metrics. When you update examples, you A/B test the new version against the current production version before deploying.

Example drift occurs when production data shifts but example sets stay static. If your customer support tickets gradually shift toward mobile app issues but your examples focus on web platform issues, the model's performance will degrade over time. Regular example refresh cycles prevent this drift.

Automated example mining helps maintain freshness. You identify production cases where the model performed well, validate them manually, and add them to your example pool. This creates a feedback loop where production experience improves the prompt continuously.

## Balancing Example Cost and Coverage

Each example adds latency and token cost. If your task runs thousands of times per day, example overhead compounds quickly. A prompt with ten examples might use 2,000 more tokens than a zero-shot prompt. At scale, this impacts both response time and inference cost.

You optimize this tradeoff by measuring the marginal value of each example. Start with one example and measure performance. Add a second and measure the lift. Continue until the performance gain from the next example is smaller than the cost increase you are willing to accept.

Example compression techniques reduce token overhead. Instead of showing complete examples with all context, show only the essential parts. Instead of verbose natural language examples, use structured formats that convey the same information more efficiently. Instead of including explanation in examples, let the pattern speak for itself.

Some teams maintain multiple example sets for different scenarios. Common cases get a lightweight prompt with 2-3 examples. Edge cases detected through preliminary classification get a heavier prompt with 8-10 examples. This tiered approach balances cost and coverage.

## Cross-Model Example Portability

Examples created for one model family often transfer to others, but with varying effectiveness. Claude and GPT-4 use different training data and architectures. Examples optimized for one may be suboptimal for the other.

Model-agnostic example selection focuses on fundamental task patterns rather than model-specific quirks. Choose examples that teach the task clearly regardless of which model processes them. Avoid examples that exploit known model behaviors like specific phrase sensitivities.

When you switch models, revalidate your example set rather than assuming it will transfer. Run your full test suite with the new model using the existing examples. Measure any performance changes. If performance drops significantly, investigate whether different examples would work better for the new model.

Some teams maintain model-specific example variants when they deploy to multiple model providers. The core examples are the same, but formatting or emphasis might differ. This adds maintenance overhead but ensures optimal performance across providers.

Understanding how to leverage examples effectively reduces prompt complexity while improving output quality. The next topic addresses how to manage complexity through decomposition when single prompts become unwieldy.

# 1.11 — Prompt Decomposition: Breaking Complex Tasks Into Prompt Chains