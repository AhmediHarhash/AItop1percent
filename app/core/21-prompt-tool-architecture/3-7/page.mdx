# 3.7 â€” Vision and Image Prompting: Layout, Annotation, and Multi-Image

An insurance company deployed a claims processing system in June 2024 that used vision models to assess vehicle damage from customer photos. The system worked adequately for single-car accidents but failed catastrophically on multi-vehicle claims. When a customer submitted four photos showing damage to their vehicle and the other driver's vehicle, the system produced a damage estimate of $47,000 for repairs to a Honda Civic. The actual damage totaled $8,200.

Investigation revealed the vision model had analyzed all four images but could not distinguish which vehicle belonged to the claimant. The prompt said "assess damage severity from the provided images" but never specified which vehicle to evaluate. The model had combined damage from both vehicles, added cosmetic issues from old photos the customer included for comparison, and produced a nonsensical estimate. The company processed 340 claims with this system before catching the error, requiring $127,000 in corrections and settlements. The prompt had treated images as interchangeable inputs rather than distinct sources requiring explicit reference.

## Vision Models Need Explicit Task Framing

When you provide an image to a vision model, you must tell it what to look for. The model can see the entire image but has no inherent understanding of which elements matter for your task. A prompt that says "analyze this image" produces generic descriptions. A prompt that says "identify safety violations in this construction site image, focusing on personal protective equipment and fall hazards" produces targeted analysis.

**Task framing** means specifying the visual elements, patterns, or features relevant to your use case. You cannot assume the model will infer intent from context. If you need specific measurements, say so. If certain regions of the image matter more than others, direct attention there. If you need comparative analysis across images, structure the comparison explicitly.

The specificity required increases with task complexity. Simple tasks like "describe the objects in this image" need minimal framing. Complex tasks like "assess manufacturing quality by examining weld consistency, surface finish, and alignment tolerances" require detailed visual criteria.

## Layout Analysis Requires Spatial Reference

Images have spatial structure that text does not. Elements appear in specific locations, relative positions matter, and layout conveys meaning. When you prompt a vision model to analyze layout, you must provide spatial reference frameworks.

**Spatial reference** means giving the model vocabulary to describe locations. Generic terms like "top" and "bottom" work for simple cases. Complex layouts need more precision: "upper left quadrant," "centered in the main content area," "below the header and above the fold." The model can recognize spatial relationships but needs language to express them.

Document layout analysis particularly benefits from spatial framing. A prompt analyzing an invoice might say "locate the total amount, typically found in the bottom right section of the document, and verify it matches the sum of line items in the central table." This prompt combines spatial guidance with task logic.

For multi-region analysis, number or label regions explicitly: "Divide the image into four quadrants labeled A, B, C, D clockwise from upper left. Analyze the equipment visible in each quadrant separately." This structure lets you reference specific regions in follow-up prompts.

## Annotation Instructions Must Specify Format and Detail Level

When you ask a vision model to annotate images with labels, bounding boxes, or descriptions, the output format critically affects usability. Text-based vision models cannot draw on images but can describe annotations in ways that humans or downstream systems can interpret.

**Annotation prompts** specify what to mark, how to describe locations, and what detail level to provide. A prompt that says "identify all vehicles in this parking lot image" is incomplete. A better prompt says "identify all vehicles, providing for each: vehicle type, approximate bounding box coordinates as percentages of image dimensions, and confidence level."

The location description method depends on your use case. Percentage-based coordinates work when downstream systems will process the annotations. Natural language descriptions work when humans will use the output: "a blue sedan in the third parking space from the left." Hybrid approaches combine both for maximum utility.

Detail level varies by application. Low-detail annotations might simply count objects. Medium-detail annotations categorize and locate objects. High-detail annotations include attributes, conditions, and relationships: "a blue 2020 Honda Civic with visible front bumper damage, parked adjacent to a white SUV."

## Multi-Image Comparison Needs Explicit Pairing Logic

When you provide multiple images to a vision model, you must specify how they relate to each other. Are you comparing them? Sequencing them? Looking for changes? Without explicit instructions, the model treats them as independent inputs.

**Pairing logic** structures multi-image analysis. For before-and-after comparisons, label images explicitly: "Image A shows the site before construction, Image B shows current state. Identify changes between A and B, focusing on new structures and landscape modifications." For multi-angle views, specify the relationship: "These three images show the same object from front, side, and top views. Describe the object's complete geometry using all three perspectives."

Comparison tasks require comparative language. Do not ask the model to "analyze these images" when you mean "compare these images and identify differences." Specify what constitutes a meaningful difference. In manufacturing inspection, you might say "flag differences in component placement, orientation, or condition, but ignore lighting variations and camera angle differences."

For large image sets, provide organizational structure. If you submit 20 images from a facility inspection, group them: "Images 1-5 show the north wing, Images 6-12 show the main production floor, Images 13-20 show the south wing. Analyze each section separately, then compare findings across sections."

## Vision Model Limitations Shape Prompt Design

Vision models in January 2026 excel at object recognition, scene understanding, and text extraction but struggle with precise measurements, fine detail in low-resolution images, and certain visual reasoning tasks. You must design prompts that work within these constraints.

Models cannot measure physical dimensions from images without reference objects. If you need measurements, include scale references in the image and prompt: "Using the standard ruler visible in the frame as reference, estimate the dimensions of the component." Even with references, expect approximations rather than precision.

Text in images poses variable challenges. Clear, high-contrast text in standard fonts extracts reliably. Handwriting, stylized fonts, and text on complex backgrounds extract less reliably. Prompts should acknowledge uncertainty: "Extract all visible text from this form, flagging fields where text is unclear or ambiguous."

Small objects, fine details, and subtle defects require high-resolution images. If your use case involves detecting hairline cracks, tiny components, or subtle color variations, prompt design cannot compensate for inadequate image quality. The prompt can say "examine for micro-fractures" but the model needs sufficient resolution to see them.

## Domain-Specific Visual Features Need Explicit Definition

Different domains have specialized visual vocabularies. Medical imaging uses terms like "radiolucent" and "enhancement patterns." Manufacturing inspection references "burrs," "flash," and "parting lines." Architectural analysis describes "fenestration," "massing," and "setbacks." Generic vision prompts produce generic descriptions.

**Domain-specific prompts** incorporate specialized terminology and direct attention to domain-relevant features. A medical imaging prompt might say "analyze this chest X-ray for signs of pneumothorax, cardiomegaly, and pleural effusion, noting any asymmetry or abnormal densities." The model recognizes these terms from training data and focuses on relevant image regions.

When domain terms are highly specialized or ambiguous, define them in the prompt: "Identify instances of cold joint defects, characterized by visible seams in concrete where one pour did not properly bond with an adjacent pour, typically appearing as distinct lines or separations in the surface."

Domain expertise manifests in knowing which visual features matter. An expert examining fabric quality looks for specific defect types in specific patterns. Your prompt should encode this knowledge: "Inspect this fabric image for common defects including slubs, pulls, color variations exceeding 1-2 shades, and pattern misalignment, paying particular attention to selvage areas where defects concentrate."

## Multi-Image Synthesis Requires Compositional Reasoning

Some visual tasks require synthesizing information across multiple images to reach conclusions no single image supports. You might need to track an object across video frames, reconstruct a 3D structure from multiple angles, or identify patterns across a set of specimens.

**Compositional reasoning prompts** guide the model to build understanding incrementally across images. For object tracking, structure the task: "These 10 images are consecutive frames from surveillance footage. Track the individual wearing a red jacket across frames, noting their position and actions in each frame, then summarize their path through the scene."

For 3D reconstruction, make the reasoning explicit: "These four images show the same building from north, south, east, and west. Describe the building's complete geometry, noting features visible from each angle. Reconcile any apparent contradictions in dimensions or features."

Pattern identification across image sets requires aggregation logic: "These 50 images show different instances of the same manufacturing defect type. Analyze them collectively to characterize the common visual features of this defect, note variations in appearance, and propose visual criteria for automated detection."

## Context and Metadata Improve Vision Prompts

Images rarely exist in isolation. They have capture contexts: time, location, camera settings, subject information. Including relevant metadata in your prompt improves model performance on context-dependent tasks.

For time-series images, include timestamps: "This image was captured on January 15, 2026 at 14:30. Compare it to the baseline image from January 1, 2026 at the same time of day, accounting for seasonal lighting differences." For location-based tasks, include geographic context: "This satellite image shows farmland in Iowa during late August, when corn is typically at full maturity."

Subject metadata helps with categorization and assessment tasks: "This X-ray is from a 45-year-old patient with a history of chronic obstructive pulmonary disease. Assess for disease progression compared to typical COPD presentations." The model can reference its training data about typical patterns for the specified demographic and condition.

Camera and sensor metadata matter for technical analysis: "This thermal image uses a temperature scale where white represents 90C and black represents 20C. Identify hotspots exceeding 70C." Without the scale reference, the model cannot interpret relative temperatures.

## Prompt Length and Complexity Scale With Visual Complexity

Simple images need simple prompts. Complex images need detailed instructions. A photo of a single object on a plain background might only need "describe this object in detail." A complex scene with multiple objects, relationships, and regions of interest requires structured prompting that guides attention systematically.

For complex visual analysis, break the task into stages within a single prompt: "First, identify all distinct objects in the scene and their locations. Second, describe the spatial relationships between objects. Third, assess the overall scene purpose or activity. Finally, note any anomalies or unexpected elements."

This staged approach prevents the model from becoming overwhelmed or missing important elements. Each stage has a clear scope, and later stages can reference earlier stage outputs. The model builds a comprehensive understanding incrementally rather than attempting simultaneous analysis of all visual elements.

## Vision Prompts Benefit From Examples and Contrast

When visual tasks involve subjective judgments or domain-specific quality standards, examples help calibrate model responses. You cannot include example images in a text prompt, but you can describe them or reference previously analyzed images.

**Descriptive examples** set standards: "Assess the quality of this weld. A high-quality weld shows consistent bead width, smooth surface, and complete penetration. A low-quality weld exhibits irregular bead width, porosity, or incomplete fusion." The model uses these criteria to evaluate the actual image.

**Contrastive examples** clarify boundaries: "Identify severe damage requiring immediate repair versus moderate damage that can be deferred. Severe damage includes structural deformation, missing components, or safety-critical failures. Moderate damage includes cosmetic issues, minor wear, or non-critical functional degradation." Contrast helps the model distinguish adjacent categories.

For ongoing systems, reference past analyses: "Compare this inspection image to the baseline inspection from June 2025. We previously flagged three areas of concern: the north wall crack, the roof membrane lifting, and the foundation settling. Assess whether these issues have progressed."

## Output Structure Matches Downstream Use

Vision model outputs serve different purposes: human review, automated processing, audit trails, training data generation. The output format you request should match the use case.

For human review, prioritize readability and context: "Provide a structured analysis with sections for overall assessment, specific findings organized by severity, and recommended actions. Use clear language and avoid excessive technical jargon."

For automated processing, prioritize consistency and parsability: "Return findings as a structured list. For each finding, provide: category, location coordinates, severity level, and confidence score. Use standardized category labels from our defect taxonomy."

For audit trails, emphasize transparency and provenance: "Document your analysis process, noting which image regions you examined, what features you looked for, and what evidence supports each conclusion. Flag areas of uncertainty."

The next subchapter covers audio and voice prompt design, where you handle real-time transcription, disfluencies, and voice-specific interaction patterns.
