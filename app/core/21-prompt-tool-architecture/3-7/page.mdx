# 3.7 â€” Vision and Image Prompting: Layout, Annotation, and Multi-Image

An insurance company deployed a claims processing system in June 2024 that used vision models to assess vehicle damage from customer photos. The system worked adequately for single-car accidents but failed catastrophically on multi-vehicle claims. When a customer submitted four photos showing damage to their vehicle and the other driver's vehicle, the system produced a damage estimate of $47,000 for repairs to a Honda Civic. The actual damage totaled $8,200.

Investigation revealed the vision model had analyzed all four images but could not distinguish which vehicle belonged to the claimant. The prompt said "assess damage severity from the provided images" but never specified which vehicle to evaluate. The model had combined damage from both vehicles, added cosmetic issues from old photos the customer included for comparison, and produced a nonsensical estimate. The company processed 340 claims with this system before catching the error, requiring $127,000 in corrections and settlements. The prompt had treated images as interchangeable inputs rather than distinct sources requiring explicit reference.

The company rebuilt their vision prompting system over three months, spending $84,000 on development and testing. The new system required explicit vehicle identification, timestamp validation, damage localization, and multi-image consistency checking. Claims processing accuracy rose from 72 percent to 96 percent. The initial failure cost $127,000 in corrections plus customer trust. The lesson was that vision models see everything you show them, but understanding what matters requires explicit instruction.

## Vision Models Need Explicit Task Framing

When you provide an image to a vision model, you must tell it what to look for. The model can see the entire image but has no inherent understanding of which elements matter for your task. A prompt that says "analyze this image" produces generic descriptions. A prompt that says "identify safety violations in this construction site image, focusing on personal protective equipment and fall hazards" produces targeted analysis.

**Task framing** means specifying the visual elements, patterns, or features relevant to your use case. You cannot assume the model will infer intent from context. If you need specific measurements, say so. If certain regions of the image matter more than others, direct attention there. If you need comparative analysis across images, structure the comparison explicitly.

The specificity required increases with task complexity. Simple tasks like "describe the objects in this image" need minimal framing. Complex tasks like "assess manufacturing quality by examining weld consistency, surface finish, and alignment tolerances" require detailed visual criteria.

For the insurance claims system, proper task framing meant: "Analyze the provided vehicle damage photos. Focus exclusively on the claimant's vehicle, identified by license plate XYZ-1234 visible in image metadata. Assess visible damage to body panels, glass, lights, and structural components. Ignore cosmetic wear unrelated to the incident. Estimate repair costs based on visible damage severity and affected components."

Task framing also includes negative instructions: what not to analyze. "Do not assess damage to other vehicles visible in photos" prevents the kind of cross-vehicle confusion that caused the original failure. "Ignore background elements like parking lots, buildings, or bystanders" focuses attention appropriately.

Vision tasks often have implicit assumptions you must make explicit. When assessing product quality from images, you assume the image shows the actual product, not marketing photos or similar items. Your prompt should verify: "Confirm this image shows the specific product unit being evaluated, not stock photography or different items. If uncertain, flag for manual review."

## Layout Analysis Requires Spatial Reference

Images have spatial structure that text does not. Elements appear in specific locations, relative positions matter, and layout conveys meaning. When you prompt a vision model to analyze layout, you must provide spatial reference frameworks.

**Spatial reference** means giving the model vocabulary to describe locations. Generic terms like "top" and "bottom" work for simple cases. Complex layouts need more precision: "upper left quadrant," "centered in the main content area," "below the header and above the fold." The model can recognize spatial relationships but needs language to express them.

Document layout analysis particularly benefits from spatial framing. A prompt analyzing an invoice might say "locate the total amount, typically found in the bottom right section of the document, and verify it matches the sum of line items in the central table." This prompt combines spatial guidance with task logic.

For multi-region analysis, number or label regions explicitly: "Divide the image into four quadrants labeled A, B, C, D clockwise from upper left. Analyze the equipment visible in each quadrant separately." This structure lets you reference specific regions in follow-up prompts.

The insurance system evolved to use spatial reference for damage assessment: "Analyze damage by vehicle section: front fascia, driver side, passenger side, rear fascia, hood, roof. For each section, note damage location, type, and severity. Use clock positions for curved surfaces: 12 o'clock is top center, 3 o'clock is right edge, et cetera."

Spatial reference becomes critical when multiple similar objects appear in one image. "Identify defects in the circuit board" is ambiguous when the image shows four circuit boards. "Identify defects in the circuit board in the upper left quadrant, marked with serial number S12345" removes ambiguity.

Layout analysis also includes understanding visual hierarchies. In forms and documents, larger text usually indicates headers. Bold text indicates emphasis. Grouped elements indicate relationships. Your prompt can instruct: "Identify the document structure by analyzing text size, weight, and spacing. Extract information according to hierarchy, treating larger text as section headers and grouped items as related fields."

## Annotation Instructions Must Specify Format and Detail Level

When you ask a vision model to annotate images with labels, bounding boxes, or descriptions, the output format critically affects usability. Text-based vision models cannot draw on images but can describe annotations in ways that humans or downstream systems can interpret.

**Annotation prompts** specify what to mark, how to describe locations, and what detail level to provide. A prompt that says "identify all vehicles in this parking lot image" is incomplete. A better prompt says "identify all vehicles, providing for each: vehicle type, approximate bounding box coordinates as percentages of image dimensions, and confidence level."

The location description method depends on your use case. Percentage-based coordinates work when downstream systems will process the annotations. Natural language descriptions work when humans will use the output: "a blue sedan in the third parking space from the left." Hybrid approaches combine both for maximum utility.

Detail level varies by application. Low-detail annotations might simply count objects. Medium-detail annotations categorize and locate objects. High-detail annotations include attributes, conditions, and relationships: "a blue 2020 Honda Civic with visible front bumper damage, parked adjacent to a white SUV."

The insurance system specified annotation format as: "For each area of damage, provide: affected component name, damage type from taxonomy of scratch, dent, crack, break, or tear, severity on scale of minor, moderate, severe, approximate size in inches if measurable, and estimated repair method of repair, replace, or paint only."

This structured format enabled automated cost estimation. The system parsed damage annotations, looked up component costs in a database, and calculated repair estimates. Unstructured annotations like "the front is pretty messed up" would have been useless for downstream processing.

Annotation instructions also handle uncertainty. "If damage severity is ambiguous or details are unclear due to photo quality, mark confidence as low and note specific uncertainty. Do not guess at details you cannot clearly observe." This prevents hallucinated annotations that downstream systems might trust inappropriately.

## Multi-Image Comparison Needs Explicit Pairing Logic

When you provide multiple images to a vision model, you must specify how they relate to each other. Are you comparing them? Sequencing them? Looking for changes? Without explicit instructions, the model treats them as independent inputs.

**Pairing logic** structures multi-image analysis. For before-and-after comparisons, label images explicitly: "Image A shows the site before construction, Image B shows current state. Identify changes between A and B, focusing on new structures and landscape modifications." For multi-angle views, specify the relationship: "These three images show the same object from front, side, and top views. Describe the object's complete geometry using all three perspectives."

Comparison tasks require comparative language. Do not ask the model to "analyze these images" when you mean "compare these images and identify differences." Specify what constitutes a meaningful difference. In manufacturing inspection, you might say "flag differences in component placement, orientation, or condition, but ignore lighting variations and camera angle differences."

For large image sets, provide organizational structure. If you submit 20 images from a facility inspection, group them: "Images 1-5 show the north wing, Images 6-12 show the main production floor, Images 13-20 show the south wing. Analyze each section separately, then compare findings across sections."

The insurance system learned to structure multi-image prompts carefully: "Four images are provided. Images 1-2 show the claimant's vehicle from front and driver side. Images 3-4 show the other vehicle for reference only. Analyze damage only from images 1-2. Use images 3-4 solely to understand accident mechanics if relevant to causation assessment."

Multi-image consistency checking prevents contradictory assessments: "After analyzing all images, verify that damage assessments are consistent across different views. If one image shows severe damage to a component but another image shows the same component intact, flag this inconsistency for human review rather than making assumptions."

Temporal sequencing matters for time-series images. "These images show disease progression over four weeks. Image 1 baseline, Image 2 week one, Image 3 week two, Image 4 week three. Note changes from baseline and week-to-week progression. Distinguish acute changes from gradual trends."

## Vision Model Limitations Shape Prompt Design

Vision models in January 2026 excel at object recognition, scene understanding, and text extraction but struggle with precise measurements, fine detail in low-resolution images, and certain visual reasoning tasks. You must design prompts that work within these constraints.

Models cannot measure physical dimensions from images without reference objects. If you need measurements, include scale references in the image and prompt: "Using the standard ruler visible in the frame as reference, estimate the dimensions of the component." Even with references, expect approximations rather than precision.

Text in images poses variable challenges. Clear, high-contrast text in standard fonts extracts reliably. Handwriting, stylized fonts, and text on complex backgrounds extract less reliably. Prompts should acknowledge uncertainty: "Extract all visible text from this form, flagging fields where text is unclear or ambiguous."

Small objects, fine details, and subtle defects require high-resolution images. If your use case involves detecting hairline cracks, tiny components, or subtle color variations, prompt design cannot compensate for inadequate image quality. The prompt can say "examine for micro-fractures" but the model needs sufficient resolution to see them.

The insurance system established image quality requirements based on model limitations. Photos must be at least 2 megapixels, well-lit, in focus, and showing damage clearly. Images failing quality checks triggered human review rather than automated analysis. This prevented low-quality inputs from producing low-quality assessments.

Occlusion poses another challenge. When objects are partially hidden behind other objects, models may not fully understand what is hidden. Prompt explicitly: "If key components are occluded or not fully visible, note this limitation. Do not infer condition of hidden components. Flag cases where occlusion prevents confident assessment."

Color perception varies with lighting conditions. The same car looks different in sunlight versus shade. Prompt accounting for this: "Assess damage based on structural deformation and component integrity rather than color variations that may result from lighting conditions. Note if lighting quality affects assessment confidence."

## Domain-Specific Visual Features Need Explicit Definition

Different domains have specialized visual vocabularies. Medical imaging uses terms like "radiolucent" and "enhancement patterns." Manufacturing inspection references "burrs," "flash," and "parting lines." Architectural analysis describes "fenestration," "massing," and "setbacks." Generic vision prompts produce generic descriptions.

**Domain-specific prompts** incorporate specialized terminology and direct attention to domain-relevant features. A medical imaging prompt might say "analyze this chest X-ray for signs of pneumothorax, cardiomegaly, and pleural effusion, noting any asymmetry or abnormal densities." The model recognizes these terms from training data and focuses on relevant image regions.

When domain terms are highly specialized or ambiguous, define them in the prompt: "Identify instances of cold joint defects, characterized by visible seams in concrete where one pour did not properly bond with an adjacent pour, typically appearing as distinct lines or separations in the surface."

Domain expertise manifests in knowing which visual features matter. An expert examining fabric quality looks for specific defect types in specific patterns. Your prompt should encode this knowledge: "Inspect this fabric image for common defects including slubs, pulls, color variations exceeding 1-2 shades, and pattern misalignment, paying particular attention to selvage areas where defects concentrate."

The insurance system incorporated auto body domain expertise: "Assess damage using standard collision repair terminology. Identify panel damage as repairable minor, repairable major, or replacement required based on dent depth, crease severity, and metal stretching. Evaluate paint damage separately from structural damage. Note whether damage affects crush zones or safety systems requiring specialized repair."

Domain vocabulary also helps with consistency. When all damage reports use the same terminology, trends become visible. When each report uses different words for the same damage type, pattern recognition fails. Standardized vocabulary in your prompts creates standardized outputs.

Medical imaging prompts require particular care with terminology. Different imaging modalities have different vocabularies. CT scan analysis uses different terms than MRI analysis. Radiologists know these distinctions, but generic vision prompts may not. Specify modality-appropriate terminology in your instructions.

## Multi-Image Synthesis Requires Compositional Reasoning

Some visual tasks require synthesizing information across multiple images to reach conclusions no single image supports. You might need to track an object across video frames, reconstruct a 3D structure from multiple angles, or identify patterns across a set of specimens.

**Compositional reasoning prompts** guide the model to build understanding incrementally across images. For object tracking, structure the task: "These 10 images are consecutive frames from surveillance footage. Track the individual wearing a red jacket across frames, noting their position and actions in each frame, then summarize their path through the scene."

For 3D reconstruction, make the reasoning explicit: "These four images show the same building from north, south, east, and west. Describe the building's complete geometry, noting features visible from each angle. Reconcile any apparent contradictions in dimensions or features."

Pattern identification across image sets requires aggregation logic: "These 50 images show different instances of the same manufacturing defect type. Analyze them collectively to characterize the common visual features of this defect, note variations in appearance, and propose visual criteria for automated detection."

The insurance system used compositional reasoning for complex claims: "These eight images show accident scene from multiple angles. Synthesize information across all images to: reconstruct accident sequence, identify point of impact, assess force and direction, determine which vehicle damage occurred in this incident versus pre-existing damage, and note any inconsistencies requiring investigation."

Compositional reasoning also handles incomplete information. "Image 1 shows front damage but rear is not visible. Image 3 shows rear damage but front is not visible. Combine information from both images to provide complete damage assessment, noting which findings come from which images."

Cross-image validation improves accuracy: "After analyzing all images individually, cross-check findings. If damage visible in one image should be visible in another but is not, investigate whether the discrepancy indicates photo angle, lighting, or potential inconsistency. Flag significant cross-image discrepancies."

## Context and Metadata Improve Vision Prompts

Images rarely exist in isolation. They have capture contexts: time, location, camera settings, subject information. Including relevant metadata in your prompt improves model performance on context-dependent tasks.

For time-series images, include timestamps: "This image was captured on January 15, 2026 at 14:30. Compare it to the baseline image from January 1, 2026 at the same time of day, accounting for seasonal lighting differences." For location-based tasks, include geographic context: "This satellite image shows farmland in Iowa during late August, when corn is typically at full maturity."

Subject metadata helps with categorization and assessment tasks: "This X-ray is from a 45-year-old patient with a history of chronic obstructive pulmonary disease. Assess for disease progression compared to typical COPD presentations." The model can reference its training data about typical patterns for the specified demographic and condition.

Camera and sensor metadata matter for technical analysis: "This thermal image uses a temperature scale where white represents 90C and black represents 20C. Identify hotspots exceeding 70C." Without the scale reference, the model cannot interpret relative temperatures.

The insurance system learned to provide rich metadata: "Vehicle: 2022 Honda Civic, 18,000 miles. Incident date: March 15, 2025. Weather: dry conditions, daylight. Photos taken: March 16, 2025 by professional adjuster using standardized photography protocol. Vehicle location during photos: company lot, consistent lighting. Previous damage history: none on record."

This context enabled better assessment. Knowing the vehicle was new helped distinguish incident damage from age-related wear. Knowing photos were professional meant higher confidence in image quality. Knowing weather was dry meant no environmental factors obscuring damage.

Metadata also prevents inappropriate comparisons. "Do not compare this 10-year-old vehicle to new vehicle standards. Assess damage relative to vehicle age and expected condition. Minor cosmetic issues consistent with normal wear should not be flagged unless clearly incident-related."

## Prompt Length and Complexity Scale With Visual Complexity

Simple images need simple prompts. Complex images need detailed instructions. A photo of a single object on a plain background might only need "describe this object in detail." A complex scene with multiple objects, relationships, and regions of interest requires structured prompting that guides attention systematically.

For complex visual analysis, break the task into stages within a single prompt: "First, identify all distinct objects in the scene and their locations. Second, describe the spatial relationships between objects. Third, assess the overall scene purpose or activity. Finally, note any anomalies or unexpected elements."

This staged approach prevents the model from becoming overwhelmed or missing important elements. Each stage has a clear scope, and later stages can reference earlier stage outputs. The model builds a comprehensive understanding incrementally rather than attempting simultaneous analysis of all visual elements.

The insurance system used staged prompts for complex damage: "Stage 1: Inventory all visible damage to body panels, glass, lights, trim, and wheels. Stage 2: For each damaged component, assess severity and repair method. Stage 3: Identify any damage suggesting underlying structural issues requiring deeper inspection. Stage 4: Synthesize findings into total damage estimate and flag any concerns for manual review."

Staged processing also improved debugging. When estimates seemed wrong, investigators could review stage outputs to identify where analysis went astray. Was the damage inventory incomplete? Was severity assessment inaccurate? Did the synthesis logic fail? Clear stages made problems traceable.

## Vision Prompts Benefit From Examples and Contrast

When visual tasks involve subjective judgments or domain-specific quality standards, examples help calibrate model responses. You cannot include example images in a text prompt, but you can describe them or reference previously analyzed images.

**Descriptive examples** set standards: "Assess the quality of this weld. A high-quality weld shows consistent bead width, smooth surface, and complete penetration. A low-quality weld exhibits irregular bead width, porosity, or incomplete fusion." The model uses these criteria to evaluate the actual image.

**Contrastive examples** clarify boundaries: "Identify severe damage requiring immediate repair versus moderate damage that can be deferred. Severe damage includes structural deformation, missing components, or safety-critical failures. Moderate damage includes cosmetic issues, minor wear, or non-critical functional degradation." Contrast helps the model distinguish adjacent categories.

For ongoing systems, reference past analyses: "Compare this inspection image to the baseline inspection from June 2025. We previously flagged three areas of concern: the north wall crack, the roof membrane lifting, and the foundation settling. Assess whether these issues have progressed."

The insurance system built a damage severity rubric into prompts: "Minor damage: scratches less than 2 inches, small dents without paint damage, chips in non-structural components. Moderate damage: dents 2-6 inches, paint damage with no deformation, cracked trim or lights. Severe damage: panel deformation, structural compromise, broken glass, damage to safety systems."

This rubric provided consistent severity classification across different adjusters and different claim handlers. What one person might call moderate, another might call severe. The rubric created shared standards that produced consistent outputs.

## Output Structure Matches Downstream Use

Vision model outputs serve different purposes: human review, automated processing, audit trails, training data generation. The output format you request should match the use case.

For human review, prioritize readability and context: "Provide a structured analysis with sections for overall assessment, specific findings organized by severity, and recommended actions. Use clear language and avoid excessive technical jargon."

For automated processing, prioritize consistency and parsability: "Return findings as a structured list. For each finding, provide: category, location coordinates, severity level, and confidence score. Use standardized category labels from our defect taxonomy."

For audit trails, emphasize transparency and provenance: "Document your analysis process, noting which image regions you examined, what features you looked for, and what evidence supports each conclusion. Flag areas of uncertainty."

The insurance system optimized output for automated cost estimation: "Return damage assessment as JSON with schema: vehicle_sections array containing section_name, damages array of damage_type, severity, repair_method, estimated_cost_low, estimated_cost_high, confidence. Include notes field for context not captured in structured fields."

This structured output fed directly into claims processing systems. No human parsing required. The system extracted the JSON, validated it, calculated total cost estimates, and routed high-uncertainty claims to human review. The output format enabled full automation for 85 percent of simple claims.

The next subchapter covers audio and voice prompt design, where you handle real-time transcription, disfluencies, and voice-specific interaction patterns.
