# 2.5 — Reflection and Self-Critique Prompts

A legal services platform paid $890,000 to settle three client complaints in November 2025 after their contract analysis system missed critical liability clauses. The system used GPT-4o to extract key terms, obligations, and risk factors from commercial agreements. Accuracy in testing had been 96.4% on standard contracts. The failures involved complex nested clauses where subtle conditional language reversed liability assignments.

The team of nine engineers had focused on improving extraction prompts, adding more examples and refining instructions. Accuracy improved incrementally but plateaued at 97%. The remaining 3% of errors included catastrophic misreadings that exposed clients to significant risk. A single missed indemnification reversal cost one client $420,000.

In December, the team added a reflection stage. After initial extraction, the system prompted: "Review your analysis. Check each extracted term against the original text. Flag any terms where you have less than 90% confidence. List potential alternative interpretations." Accuracy jumped to 98.9%. More importantly, the reflection stage flagged 89% of remaining errors as "uncertain," routing them to human review. Catastrophic undetected errors dropped to near zero.

The breakthrough was recognizing that models often have some internal signal that their answer might be wrong. Reflection surfaces that signal.

## The Mechanics of Model Self-Reflection

Self-reflection prompts ask the model to critique its own previous output. The basic pattern is: generate an answer, then prompt the model to review that answer for errors, gaps, or inconsistencies. This two-stage process often catches mistakes that single-pass generation misses.

The mechanism appears to work through attention reallocation. In initial generation, the model's attention is split between understanding the input and producing output. In reflection, the model's attention focuses entirely on evaluating existing output against the input. This division of cognitive labor can reveal errors that the unified generation process overlooked.

Reflection is distinct from simply asking the model twice. If you prompt "What is the answer?" twice separately, you're sampling twice from the same distribution. If you prompt "What is the answer?" then "Review your answer for errors," the second prompt operates in a different mode. The first is generative, the second is evaluative. These tap different capabilities.

The effectiveness of reflection varies by model sophistication. Claude 3.5 Sonnet, Claude 4 Opus, and GPT-4o show strong reflection capabilities. They can identify errors in their own outputs with reasonable reliability. Smaller models like Llama 3 8B show weaker effects. The model needs sufficient capability to engage in meaningful self-critique.

Reflection works best when errors are detectable through logical analysis rather than requiring external knowledge. If the model made a math error, reflection can catch it by rechecking calculations. If the model hallucinated a fact, reflection might not help because the model still doesn't have the correct information. Reflection improves reasoning errors more than knowledge errors.

## Chain-of-Verification: Structured Reflection Protocols

Chain-of-verification structures reflection into explicit verification steps. After generating an answer, the model generates verification questions about that answer, answers those questions, and then produces a final revised answer based on the verification results.

The pattern looks like: "Question: [original question]. Initial Answer: [model generates answer]. Verification Questions: [model generates 3-5 questions to verify the answer]. Verification Answers: [model answers each verification question]. Final Answer: [model produces revised answer based on verification]."

This approach forces systematic review rather than vague "check your work" instructions. A medical diagnosis system might verify: "Is the symptom set consistent with the proposed diagnosis? Are there alternative diagnoses with similar symptoms? Does the proposed diagnosis account for all reported symptoms? Are there contraindications in the patient history?"

Chain-of-verification increases token usage significantly. A 200-token answer might expand to 800 tokens with verification steps. The cost multiplier is 2-4x depending on verification depth. This is cheaper than self-consistency with 5 samples but more expensive than single-pass generation.

A financial fraud detection platform implemented chain-of-verification in August 2025. Initial fraud assessments generated risk scores with brief explanations. Verification added four questions: "Are the flagged patterns actually unusual for this merchant category? Is the transaction amount consistent with historical patterns? Could legitimate explanations account for the flags? Are there additional risk factors not initially considered?" Final assessments showed 4.2 percentage point accuracy improvement.

The structure helps with auditability. When you need to explain why a system reached a conclusion, chain-of-verification provides the reasoning trace: initial answer, verification questions, verification results, final answer. This transparency is valuable for regulated industries and high-stakes decisions.

## Self-Correction Patterns: Iterative Refinement

Self-correction uses reflection to iteratively improve outputs. The pattern is: generate, critique, revise, and optionally repeat. Each cycle refines the output based on identified weaknesses.

Simple self-correction uses one iteration: "Here is my answer: [answer]. Reviewing for errors: [critique]. Revised answer: [improved answer]." This catches obvious errors without excessive iteration. A technical writing assistant uses this pattern to improve explanation clarity. Initial drafts often contain jargon or assume too much knowledge. The critique identifies these issues, and the revision addresses them.

Multi-iteration refinement repeats the cycle multiple times. "Generate initial answer → Critique → Revise → Critique again → Revise again → Final answer." Diminishing returns typically appear after 2-3 iterations. Each cycle finds fewer improvements while adding latency and cost.

A software documentation generator tested iteration counts in October 2025. One iteration improved quality scores from 7.2 to 8.1 (on a 10-point expert rating scale). Two iterations reached 8.4. Three iterations reached 8.5. Four iterations reached 8.5 again—no improvement over three. The optimal strategy was two iterations for the best quality-cost-latency balance.

Convergence criteria determine when to stop iterating. Options include: fixed iteration count, no significant changes between iterations, critique identifies no remaining issues, quality threshold reached. Fixed counts are simplest. Convergence detection is more adaptive but requires defining "significant change" or "no issues."

Self-correction works particularly well for style and structure issues. "Make this clearer," "ensure logical flow," "check for consistency" are all improvements that reflection can guide. It works less well for factual accuracy because the model can't verify facts it doesn't know.

## When Reflection Helps Versus Adds Noise

Reflection helps most when the model has the capability to detect errors but sometimes fails to apply that capability in initial generation. Complex reasoning tasks, subtle logical errors, formatting issues, internal consistency problems, and unclear explanations all benefit from reflection.

Reflection helps less when the model lacks the underlying capability. If the model doesn't understand a concept, asking it to review its misunderstanding doesn't help. If the model doesn't know a fact, asking it to verify that fact won't surface the correct information. Reflection improves execution of existing capabilities, not acquiring new ones.

Tasks with objective correctness criteria benefit more than subjective tasks. Math problems have right answers that reflection can verify. Creative writing has no single correct answer, so reflection might just second-guess reasonable initial choices. "Is this calculation correct?" is answerable through reflection. "Is this story engaging?" is less so.

A customer service response system tested reflection in December 2024. For factual questions about policies, reflection improved accuracy from 91% to 94%. For empathetic responses to complaints, reflection had no effect or slightly hurt quality by making responses more stilted and self-conscious. The team implemented selective reflection: factual queries got reflection, emotional support queries didn't.

Reflection can add noise when it triggers overcorrection. The model identifies a minor issue and changes something that was fine, introducing new errors. "This phrase might be unclear" leads to a revision that's actually more confusing. Monitoring for this requires comparing initial answers to reflected answers and identifying cases where reflection degraded quality.

## Reflection as Quality Gate: Confidence Filtering

Reflection can serve as a quality gate rather than correction mechanism. Instead of revising the answer, the model evaluates its confidence and flags low-confidence cases for alternative handling.

The pattern is: "Answer: [model generates answer]. Confidence Assessment: Review your answer. On a scale of 0-100, how confident are you this is correct? List specific concerns if confidence is below 80." This produces an answer plus metadata about reliability.

Applications use confidence scores to route decisions. High-confidence answers (above 90) proceed automatically. Medium-confidence answers (70-90) get additional validation. Low-confidence answers (below 70) escalate to human review. This optimizes the human-AI division of labor.

A tax preparation system uses confidence-based routing. The system handles straightforward returns automatically. It flags complex situations involving capital gains, foreign income, or business deductions for accountant review. Reflection-based confidence scoring determines which cases are straightforward versus complex. This reduced accountant review load by 67% while maintaining accuracy.

Confidence calibration matters critically. If the model says "95% confident" but is actually wrong 20% of the time, your routing logic fails. Test whether model-reported confidence correlates with actual accuracy. Well-calibrated models show strong correlation: 95% confidence cases are correct 95% of the time. Poorly calibrated models show weak correlation.

Claude models generally show reasonable confidence calibration. GPT-4o tends toward overconfidence. Gemini 2.0 models vary. Test calibration for your specific use case and model, not just general benchmarks. Confidence patterns differ across domains and prompt styles.

## Iterative Refinement Loops: Multi-Stage Improvement

Iterative refinement uses multiple rounds of generation, critique, and revision to progressively improve outputs. This is most valuable for complex creative or analytical tasks where quality matters more than speed.

A content marketing platform uses three-stage refinement for blog posts. Stage 1: Generate initial draft based on topic and keywords. Stage 2: Critique for engagement, SEO optimization, and brand voice alignment. Stage 3: Revise based on critique. Stage 4: Final review for grammar and formatting. The process takes 8 seconds versus 2 seconds for direct generation but produces content that clients rate 35% higher.

Refinement loops require clear evaluation criteria at each stage. "Improve this" is too vague. "Check for: logical flow, supporting evidence for claims, clear topic sentences, varied sentence structure" gives the model specific dimensions to evaluate. Good critique prompts enumerate what to look for.

Separation of concerns across stages improves results. Stage 1 focuses on content and ideas. Stage 2 focuses on structure and argument. Stage 3 focuses on style and clarity. Each stage has a narrow mandate, preventing the model from trying to optimize everything simultaneously.

A legal brief writing assistant uses four-stage refinement: (1) Generate argument structure, (2) Review for logical soundness and completeness, (3) Revise with improvements, (4) Polish citations and formatting. Each stage has specific quality criteria. The structured approach produces briefs that attorneys rate as "usable with minimal editing" 78% of the time versus 41% for single-pass generation.

Cost-benefit analysis is essential for multi-stage systems. If three stages cost 6x single-pass generation, the quality improvement must justify the cost. For high-value outputs (legal documents, medical reports, executive communications), the math works. For low-value outputs (social media comments, routine emails), it doesn't.

## Implementation Patterns for Production Reflection

Implement reflection as a conditional operation based on task characteristics. Simple queries get direct answers. Complex queries get reflection. Route based on input complexity, domain difficulty, or other signals. A technical support system uses reflection for multi-step troubleshooting but not for simple factual questions.

Structure reflection prompts with specific evaluation criteria. "Review your answer for correctness" is vague. "Check: (1) Are all facts verifiable? (2) Do the numbers add up? (3) Does the conclusion follow from the reasoning? (4) Are there obvious gaps?" gives the model a concrete evaluation framework.

Parse reflection outputs to extract both improved answers and confidence signals. If reflection format is "Issues Found: [list], Confidence: [score], Revised Answer: [text]," extract all three components. Use them for routing, logging, and quality monitoring.

A financial analysis platform structures reflection as JSON: `{"initial_answer": "...", "identified_issues": ["..."], "confidence": 0.85, "final_answer": "..."}`. This makes downstream processing deterministic. The application code uses confidence scores for routing and logs identified issues for quality analysis.

Cache reflection prompts when possible. The reflection instruction itself ("Review this answer for errors in: reasoning, facts, calculations") doesn't change across requests. Use prompt caching to reduce costs. Claude's prompt caching and similar features can cut reflection costs by 30-40%.

## Combining Reflection with Chain-of-Thought

Reflection and chain-of-thought work synergistically. CoT produces explicit reasoning steps. Reflection reviews those steps for errors. The combination catches both reasoning mistakes and conclusion errors.

The pattern is: "Think step by step: [model generates reasoning chain]. Now review your reasoning: [model critiques each step]. Final answer: [model produces conclusion based on reviewed reasoning]." This three-stage approach is powerful for complex reasoning tasks.

A scientific literature review system uses CoT-plus-reflection to assess study quality. CoT stage: "Step 1: Identify study design. Step 2: Assess sample size adequacy. Step 3: Evaluate statistical methods. Step 4: Check for confounding factors. Step 5: Conclude on study quality." Reflection stage: "Review each step. Are there flaws in the analysis? Did I miss important quality indicators?" This catches errors in both the systematic evaluation and the overall assessment.

The cost is substantial. CoT already increases tokens 2-4x over direct answers. Adding reflection increases tokens another 1.5-2x. Total cost can be 6-8x single-pass generation. Use this combination only when accuracy justifies the cost: high-stakes decisions, complex analysis, regulated environments.

Research on mathematical reasoning shows CoT plus reflection outperforms either technique alone by 3-5 percentage points. The reflection catches calculation errors that CoT makes, and CoT provides the structure that makes reflection effective. The synergy is real.

## Measuring Reflection Effectiveness

Measure accuracy with and without reflection on test sets where you have ground truth. This is the direct test: does reflection actually improve your metrics? Compare error rates, error types, and cost per correct answer.

A healthcare documentation system measured reflection impact on clinical note accuracy. Without reflection: 94.2% accuracy, $0.03 per note. With reflection: 97.1% accuracy, $0.08 per note. Error cost per note (weighted by severity): $4.20 without reflection, $1.40 with reflection. Net impact: reflection saves $3.30 per note, justifying the $0.05 additional cost.

Track how often reflection changes answers. If reflection never changes answers, it's not helping. If it always changes answers, it might be overcorrecting. Optimal rates vary by task but typically fall in 10-30% range: reflection identifies issues in some cases but not all.

Analyze whether reflection changes improve or degrade quality. Compare initial versus reflected answers on test cases. Calculate: accuracy of unchanged answers, accuracy of changed answers where reflection improved quality, accuracy of changed answers where reflection degraded quality. This reveals reflection's true value.

Measure latency impact. Reflection adds at least one additional generation pass. For interactive applications, this might push latency beyond user tolerance. A chatbot with 1.5-second response time might become 4 seconds with reflection. Test whether users notice and care.

## When to Skip Reflection

Skip reflection for simple tasks where initial generation is already highly accurate. "What is 2+2?" doesn't need reflection. Classification tasks with 99%+ baseline accuracy won't improve meaningfully from reflection. The cost isn't justified.

Skip reflection for subjective or creative tasks where there's no objective correctness to verify. "Write an engaging story" can't be meaningfully self-evaluated by the model. Reflection might second-guess creative choices that were fine initially.

Skip reflection when latency budgets are tight. Real-time systems, interactive applications, and time-sensitive processing often can't afford the additional generation passes. Use faster techniques like output validation or simpler prompt improvements.

Skip reflection at high volume unless error costs justify it. Processing millions of items daily with reflection costs 2-3x more than direct generation. If error costs are low, the economics don't work. Reserve reflection for subset of high-value or high-risk items.

A spam filtering system processes 10 million emails daily. Direct classification costs $1,000 daily. Reflection would cost $2,500 daily. Error cost is minimal—false positives go to spam folder, false negatives are annoying but not catastrophic. The system skips reflection and invests the $1,500 savings in better training data.

## Future Directions: Models Trained for Self-Critique

Current frontier models show reflection capabilities as an emergent property of scale and training. Future models may be explicitly optimized for self-critique through specialized training approaches. OpenAI's work on process supervision and Anthropic's research on constitutional AI point in this direction.

Models trained specifically for self-evaluation might show better calibration between confidence and accuracy. Current models are imperfectly calibrated—they sometimes express high confidence in wrong answers. Training that penalizes miscalibrated confidence would improve reflection effectiveness.

Specialized reflection models are another possibility. Instead of one model doing both generation and reflection, use separate models optimized for each task. A generator model produces outputs; an evaluator model critiques them. This division of labor might outperform single-model reflection.

Some research explores reward models as reflection mechanisms. The reward model evaluates generation quality, and this signal feeds back into the generation process. This is related to reinforcement learning from human feedback (RLHF) but applied at inference time rather than just training time.

For practitioners, the current state of reflection capabilities is already valuable. Claude 3.5 Sonnet, Claude 4 Opus, GPT-4o, and GPT-4.5 Turbo all support effective reflection patterns. Implement reflection where it provides measurable value, measure results rigorously, and adjust based on your specific use case economics.

The next chapters explore advanced prompt architecture patterns including few-shot learning, prompt chaining, and tool-augmented generation techniques.
