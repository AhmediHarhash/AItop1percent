# 4.4 â€” System Prompt Evolution Across Conversation Turns

A medical documentation assistant used by 120 hospital systems caused a patient safety incident in July 2024 when its static system prompt led to critical information being overlooked. The product helped doctors generate clinical notes by conversing about patient encounters. The system prompt defined the assistant's role and output format at conversation start. Twenty-five turns into a complex case discussion, the doctor mentioned a new medication allergy discovered during the visit. The assistant, still operating under its initial "generate a summary of the encounter" instructions, included the allergy in the narrative but didn't flag it as critical new information requiring special attention. The note went into the EHR without proper allergy alerts. The hospital's safety review revealed the issue: the system prompt never evolved as the conversation revealed high-priority information. It treated turn 25 the same as turn 1.

You need **system prompt evolution** when your AI product's role and constraints should change as conversations progress and new information emerges. A static system prompt written at conversation start can't account for what you'll learn in turn 15. As users provide context, make decisions, or reveal preferences, your assistant's instructions should adapt to reflect the current state, incorporate accumulated knowledge, and adjust priorities based on what's now known.

## The Static System Prompt Trap

Most developers write one system prompt at the beginning of a conversation and reuse it for every turn. This works for simple Q&A where each turn is independent. It fails for conversations where early turns establish context that should inform how later turns are handled.

Consider a code review assistant. At turn 1, your system prompt says "You are a helpful code reviewer. Provide feedback on code quality." At turn 15, you've learned: the user is working on a Python library, they prioritize performance over readability, they're targeting Python 3.11+, and they're particularly concerned about memory usage. Your turn 1 system prompt doesn't capture any of this. Every response regenerates from generic instructions instead of specialized, context-aware instructions.

The LLM can pick up some context from conversation history, but that's inefficient and unreliable. You're burning tokens repeating information and hoping the model infers the pattern. Explicit system prompt updates make expectations clear: "You are reviewing Python 3.11+ code for a performance-critical library. Prioritize memory efficiency over readability. Focus on algorithmic complexity and memory allocations."

Static prompts also miss conversation phase shifts. Turn 1-10 might be exploration: "Help me understand this codebase." Turn 11-20 might be implementation: "Let's write the new feature." Turn 21-30 might be debugging: "This isn't working as expected." Each phase needs different assistant behaviors, constraints, and output formats. One system prompt can't serve all three phases effectively.

The trap is convenience. Static prompts are easy to implement: write once, reuse forever. Dynamic prompts require tracking what's changed, deciding when to update, and managing prompt state across turns. But the complexity investment pays off in conversation quality and task success rates.

## Accumulation vs. Replacement Strategies

When evolving system prompts, you can either accumulate information (append new instructions to existing ones) or replace sections (swap out old instructions for new ones). Both have uses. Accumulation works for additive context: you learn the user's programming language, then their framework, then their deployment target. Each fact adds to the context without invalidating previous facts.

Replacement works for state transitions: you were in "planning mode," now you're in "implementation mode." The planning instructions no longer apply. Replacing them with implementation instructions keeps your prompt focused and token-efficient. Accumulation would create a prompt with both planning and implementation instructions, causing confusion about which mode is active.

A hybrid approach often works best: maintain a core system prompt that never changes (defines fundamental role and capabilities), plus dynamic sections that accumulate or replace based on section type. Your core says "You are a code review assistant." Your context section accumulates: "Tech stack: Python, Django, PostgreSQL." Your mode section replaces: ~~"Mode: Exploration"~~ "Mode: Implementation."

Implement this as a structured prompt builder. Define prompt sections as objects or templates: core_role, accumulated_context, current_mode, user_preferences, constraints. Each section has update rules: core_role is immutable, accumulated_context appends new facts, current_mode replaces on phase transitions. Your prompt builder assembles the current system prompt by combining sections according to their current state.

Track prompt evolution in your conversation state. Store: original system prompt, accumulated context facts, current mode, prompt version number. When debugging conversation issues, you need to know what system prompt was active at turn 15, not just what it is now. Log every system prompt change with timestamp and triggering event.

## Detecting When to Update

Not every turn should trigger system prompt updates. Most turns add to conversation history without changing fundamental context. You need triggers that identify when significant new information has emerged that warrants prompt evolution.

User-declared facts are high-confidence triggers. When the user explicitly states "I'm using Python 3.11," "I prefer detailed explanations," or "This is for a production system with 10M users," extract these facts and update the system prompt. These aren't inferences; they're ground truth that should inform all subsequent turns.

State machine transitions are structural triggers. If you implement conversation state machines (from Chapter 4.1), state changes often warrant prompt updates. Entering a "debugging" state should activate debugging-specific instructions. Completing a "requirements gathering" state should update the prompt with gathered requirements and transition to solution mode instructions.

Tool call results can trigger updates. If you call a tool that retrieves the user's configuration file and it shows they're using React 18 with TypeScript strict mode, that information should be promoted to the system prompt. You don't want the LLM to forget this by turn 50 when it's been pushed out of conversation history by other context.

Inference-based triggers are lower-confidence but still valuable. If the user asks five consecutive questions about async/await patterns, infer they're learning async programming and update the prompt to emphasize educational explanations over terse answers. If they consistently choose option A over option B across multiple decisions, infer a preference and encode it.

Implement trigger evaluation after each turn. Analyze the user's message and assistant's response for trigger patterns. When detected, queue a prompt update for the next turn. Don't update mid-turn; the current response is already generated. Apply updates before processing the next user message.

## Structuring Dynamic Prompt Sections

Break your system prompt into logical sections that can be independently updated. This structure makes evolution manageable and prevents different update mechanisms from interfering with each other.

A typical structure might include: **Role Definition** (static, defines what the assistant is), **Current Task Context** (dynamic, what you're working on right now), **User Profile** (accumulated, facts about the user), **Conversation Phase** (replaced, whether you're planning/implementing/debugging), **Active Constraints** (dynamic, current rules and limitations), **Output Format** (conditional, how responses should be structured).

Role Definition stays constant: "You are an AI code review assistant helping developers improve code quality." This grounds the assistant's purpose across the entire conversation. Even as context evolves, the fundamental role doesn't change.

Current Task Context updates as new information emerges: "The user is implementing OAuth2 authentication for a Django REST API. They need to support both password and token-based flows. The API serves a React frontend and a mobile app." This section accumulates facts about what you're building.

User Profile tracks persistent preferences and environment: "User: Senior Python developer, 5 years experience. Prefers type hints and comprehensive docstrings. Works at a startup with 50K MAU. Deployment target: AWS ECS with PostgreSQL RDS." This section grows as you learn about the user.

Conversation Phase reflects current mode: "Phase: Implementation. User is writing code. Provide specific, actionable code suggestions with examples. Focus on getting to working code quickly rather than extensive planning." This section replaces when you shift from planning to implementation to testing.

Active Constraints capture current requirements: "Constraints: Must maintain backwards compatibility with v1.2 API. Cannot add new dependencies. Must complete by end of week." These might be added, removed, or updated as project constraints evolve.

Output Format adjusts based on current needs: "Output: Provide code blocks with file names and line numbers. After code, explain key decisions. Keep explanations under 100 words unless user requests more detail." This can shift between verbose and terse, structured and freeform.

## Templating and Variable Injection

Implement system prompt evolution with templates and variable substitution rather than string concatenation. Define templates for each section with placeholder variables, then inject current values when building the prompt for each turn.

For example, a Task Context template might be: "Current Task: {task_description}. Tech Stack: {technologies}. Key Requirements: {requirements}. Current Blockers: {blockers}." Your conversation state tracks values for these variables. When task_description gets updated at turn 12 or blockers gets added at turn 20, you update the variables and regenerate the section.

Templates make prompt evolution predictable and testable. You can verify that template logic works correctly independent of variable values. You can test that variable extraction from user messages works correctly independent of template formatting. This separation of concerns prevents prompt update bugs.

Store templates in configuration files, not hardcoded in application logic. This lets you iterate on prompt structure without code deployment. You can A/B test different template structures by routing conversations to different template versions. You can version templates and track which version performed best for which conversation types.

Variable extraction needs robust parsing. When the user says "I'm using React and TypeScript," extract technologies=["React", "TypeScript"]. When they say "We need to launch by March 1st," extract deadline="March 1st" and add it to constraints. Use structured LLM outputs or dedicated extraction prompts to reliably pull variables from natural language.

Validate variable values before injection. If you extract a technology stack that includes an invalid entry, don't inject it. If you extract a constraint that contradicts an existing constraint, flag the conflict and ask for clarification. Bad variable values create bad prompts that confuse subsequent turns.

## Prompt Versioning and Turn Tracking

Maintain a prompt version number that increments with each modification. Store this version alongside the prompt content. When analyzing conversation quality or debugging issues, you need to know exactly what prompt version was active at each turn.

Log prompt updates with detailed metadata: turn number, version number, what changed (which sections were updated, what variables were modified), what triggered the update (user statement, state transition, inference), and the full prompt content after the update. This creates an audit trail for prompt evolution across the conversation.

Consider whether to show prompt updates to users. In some products, transparency is valuable: "I've updated my understanding: you're working with Python 3.11 and Django. I'll tailor my suggestions accordingly." This builds trust and lets users correct misunderstandings. In other products, invisible updates are better; users don't need to think about system mechanics.

Implement prompt rollback capability for error recovery. If a prompt update causes the assistant to behave incorrectly, you should be able to revert to the previous version. This is rare but valuable: a bad variable extraction creates a confusing prompt, the user complains, and you can roll back to the last working version while you fix the extraction logic.

Track prompt size in tokens. System prompts consume context window budget. As you accumulate context and add constraints, your system prompt grows. Monitor token usage and implement compression when prompts exceed a threshold. You might summarize accumulated context or consolidate redundant constraints to keep prompt size manageable.

## Phase Transitions and Mode Switching

Many conversations have distinct phases with different requirements. Phase transitions are natural points for substantial prompt updates. Instead of gradual accumulation, you perform a structured transition that resets certain sections and activates phase-specific instructions.

Define phases explicitly in your conversation design: Exploration, Planning, Implementation, Testing, Debugging, Refinement. Each phase has a template that defines what the assistant should focus on, what output format to use, and what behaviors to prioritize.

When transitioning from Exploration to Planning, you might: keep accumulated context (tech stack, requirements), replace mode-specific instructions (stop asking clarifying questions, start proposing solutions), activate new constraints (solutions must be implementable with current tech stack), change output format (structured plans with tasks and estimates).

Detect phase transitions through explicit user requests ("Let's start building this") or state machine logic (completed requirements gathering, entering solution design state). Don't infer phase transitions from subtle signals; you'll create jarring experiences when the assistant suddenly changes behavior unexpectedly.

Announce phase transitions when appropriate: "Now that we've explored your requirements, I'll shift to planning mode and propose an implementation approach." This prepares the user for a change in conversation style and gives them a chance to object if the timing is wrong.

Implement phase-specific safeguards. In Implementation phase, prevent the assistant from questioning fundamental requirements (that was Exploration phase). In Debugging phase, prevent the assistant from suggesting architectural changes (that's Refinement phase). Phase boundaries enforce focus and prevent conversation drift.

## Context Summarization in System Prompts

As accumulated context grows, raw injection becomes inefficient. You don't need to list all 15 technologies in the tech stack every turn. Summarize accumulated information into concise context that informs behavior without consuming excessive tokens.

Instead of: "User mentioned Python. User mentioned Django. User mentioned PostgreSQL. User mentioned Redis. User mentioned Celery. User mentioned Docker. User mentioned AWS..." write: "Tech Stack: Python/Django backend with PostgreSQL database, Redis cache, and Celery for async tasks. Containerized with Docker, deployed on AWS."

Summarization should preserve decision-relevant information while eliminating redundancy. If the user mentioned they're using Python 3.11.5 specifically, summarize to Python 3.11 unless the minor version is critical for some decision. If they mentioned deployment to AWS three times in different contexts, mention AWS once in the tech stack summary.

Implement summarization at regular intervals. Every 10 turns, or when accumulated context exceeds a token budget, trigger summarization. Use an LLM call with a specialized prompt: "Summarize this accumulated context into a concise technical profile, preserving all information relevant to code generation and technical decision-making."

Test summarization quality by comparing responses generated with full context versus summarized context. Significant divergence indicates your summarization is losing important information. Iterate on summarization prompts until outputs are consistent across both versions.

## Handling Contradictions and Updates

Users change their minds. Requirements evolve. Initial statements get corrected. Your system prompt evolution must handle contradictions gracefully.

When new information contradicts existing prompt content, decide: is this a correction (user misspoke earlier, new info is authoritative) or a change (previous info was correct then, but circumstances changed)? For corrections, replace the old information immediately. For changes, consider whether to keep history of the change or just update to current state.

Corrections should update silently. If the user said "Python 3.10" in turn 2 but "Actually, it's 3.11" in turn 5, just update the tech stack to 3.11. Don't clutter the prompt with "previously said 3.10, now says 3.11." The current truth is 3.11.

Significant changes might warrant acknowledgment: "I've updated my understanding: you're now targeting PostgreSQL instead of MySQL for the database layer. I'll adjust my suggestions accordingly." This confirms the update was processed and gives the user confidence their correction was heard.

Implement conflict detection in your variable extraction logic. If you're about to inject database="PostgreSQL" but your current prompt says database="MySQL," flag this as a potential contradiction. Either ask for clarification or apply a resolution policy (newer information wins, explicit corrections override inferences, etc.).

Log all contradictions and resolutions. These logs reveal prompt quality issues. If you frequently extract contradictory information, your extraction logic is unreliable. If users frequently correct the same variable, your initial extraction prompt needs improvement.

## Testing Dynamic Prompt Systems

Test prompt evolution through multi-turn conversation simulations. Create test conversations where users progressively reveal information, change requirements, or transition between phases. Verify that prompts evolve appropriately at each step.

Assert on prompt content at specific turns: "After turn 5 where user mentions Python, system prompt should contain 'Python' in tech stack section." "After state transition to Implementation phase, system prompt mode should be 'Implementation' and should include implementation-specific instructions." These assertions verify prompt evolution logic works.

Test edge cases: what happens if the user contradicts themselves three times? What if they provide no explicit facts for 30 turns? What if they switch topics completely mid-conversation? Edge cases expose whether your evolution logic is robust or brittle.

Test prompt size growth. Simulate conversations of 50, 100, 200 turns and track system prompt token usage. Does it grow unbounded? Does summarization kick in appropriately? Does the prompt stay within your token budget?

Compare conversation quality with static vs. dynamic prompts. Run the same conversation flow with static prompts and with evolution enabled. Measure task completion rate, user corrections, conversation length. Quantify whether prompt evolution delivers measurable value.

## Prompt Evolution Antipatterns

Avoid **over-updating**: changing the prompt every single turn based on minor signals. This creates instability and inconsistency. The assistant's behavior shouldn't shift noticeably between adjacent turns unless something significant changed.

Avoid **under-updating**: only using the initial prompt and ignoring all new information. This wastes the value of multi-turn conversation and makes your assistant appear to have amnesia.

Avoid **conflicting instructions**: accumulating constraints that contradict each other. "Be concise" and "Provide detailed explanations" can't both be active simultaneously. When adding new instructions, check for conflicts with existing ones.

Avoid **hidden updates**: changing prompt behavior without any tracking or visibility. If you can't explain why the prompt is what it is at turn 25, you have a black-box evolution system that will be impossible to debug or improve.

Avoid **excessive prompt bloat**: letting prompts grow to thousands of tokens through unconstrained accumulation. Monitor size and compress aggressively.

Avoid **losing context on page refresh**: if users can resume conversations later, persist evolved prompts along with conversation history. Don't regenerate prompts from scratch on each session; you'll lose the evolution that occurred in previous sessions.

## Dynamic Prompts as Conversation Infrastructure

System prompt evolution is infrastructure, not a feature users see directly. But it profoundly affects conversation quality. Prompts that adapt to accumulated context enable assistants to become more helpful over time rather than maintaining constant (and therefore increasingly irrelevant) behavior.

The investment in dynamic prompt systems pays off in longer conversations and complex tasks. Simple Q&A doesn't need it. But if your product supports users across 20+ turn conversations working on multi-step projects, static prompts will limit your ceiling.

Start with simple accumulation: extract facts from user messages and append to system prompt context. Add mode switching when you have distinct conversation phases. Add summarization when accumulated context grows too large. Build sophistication incrementally based on observed conversation patterns and failure modes.

Track the relationship between prompt evolution and conversation success. Do conversations with more prompt updates complete tasks more successfully? Do they have fewer user corrections? Do they require fewer clarifying questions? If prompt evolution doesn't correlate with better outcomes, you're either implementing it wrong or your conversation patterns don't benefit from it. Measure to know which.

System prompt evolution transforms static assistants into adaptive ones that improve their fit to user needs as conversations progress, creating experiences that feel increasingly personalized and contextually aware.
