# 9.3 — Prompt and Tool Incident Response Playbooks

A Series A legal tech startup deployed an AI contract analysis feature in September 2025. At 2 AM on a Saturday, their on-call engineer got paged for high error rates. API calls to Claude were failing with 529 errors indicating overload. The engineer had no runbook for AI incidents. They escalated to the CTO who was traveling internationally. By the time the team assembled and identified the issue—a retry loop in their error handling that amplified load 50x—eight hours had passed. They lost three enterprise prospects who experienced the outage during business hour trials. The incident cost $340,000 in lost deals plus two weeks of reputation repair. The root cause was treating AI system failures as generic outages instead of having specific incident response procedures.

Most teams have incident response processes for traditional systems. Database failures, API outages, deployment problems. These processes work because teams understand failure modes and have practiced responses. AI systems have different failure modes. Models start returning low-quality outputs without error codes. Costs spike from unexpected usage patterns. Tool calls fail in ways that look like successes. Generic incident response is too slow.

## Why AI Incidents Look Different from Traditional Outages

Traditional outages have clear signals. Your API returns 500 errors. Your database is unreachable. Your deployment fails health checks. These are binary failures. Systems work or they do not. Incident response focuses on restoring service quickly.

AI incidents are often gradual quality degradations. Your summarization feature starts producing summaries that are slightly too long or miss key details. Your classification system becomes less accurate. Your generation quality drops. All API calls succeed. No errors get logged. Users notice something is wrong but your monitoring shows green.

This creates detection delays. Traditional outages get detected in seconds by health checks. AI quality degradations take hours or days to surface. By the time you realize there is a problem, many users have had bad experiences. Your incident response must account for these detection delays.

AI incidents also have unique root causes. A model provider might deploy a new version with different behavior. Your prompt might interact poorly with new training data. A tool might return unexpected formats that your parsing code mishandles. Token limits might get exceeded in ways that silently truncate outputs. These causes do not map to traditional infrastructure problems.

## Classifying Prompt and Tool Incidents

You need a taxonomy for AI incidents so teams can quickly identify what type of problem they face and apply the right response. Most teams use five categories that cover distinct failure patterns and mitigation strategies.

**Availability incidents** are traditional outages where the model API is unreachable or returning errors. 529 overload errors. Network timeouts. Authentication failures. These incidents prevent your system from working at all. The response is immediate fallback—serve cached responses, disable the feature gracefully, or route to backup models. You escalate to your model provider if the outage is on their end.

**Performance incidents** are when the system works but too slowly. Response times exceed user expectations. Timeouts occur at your application layer even though the API responds eventually. These incidents degrade user experience without complete failure. The response is request optimization—reduce context length, use faster models, implement aggressive timeouts, or temporarily disable features for slow requests.

**Quality incidents** are when outputs degrade without technical errors. Accuracy drops. Hallucination rates increase. Output formats become inconsistent. These are the hardest incidents to detect and diagnose. The response is validation tightening—add stricter output validation, fall back to simpler prompts, increase sampling parameters for stability, or temporarily route to higher-quality models.

**Cost incidents** are when spending spikes unexpectedly. A bug causes retry loops. Usage grows faster than anticipated. A feature gets abused. These incidents do not affect users directly but threaten budgets. The response is immediate rate limiting—cap requests per user, throttle expensive operations, disable experimental features, or switch to cheaper models temporarily.

**Safety incidents** are when the system produces harmful, biased, or non-compliant outputs. Jailbreak attempts succeed. PII leaks through prompts. Outputs violate content policies. These incidents have legal and reputational risk. The response is immediate shutdown—disable the feature until you can add guardrails, implement stricter content filtering, and conduct thorough review.

This taxonomy helps teams triage quickly. When an alert fires, your first question is which category. The answer determines who responds, how urgently, and what actions to take. The legal tech startup's incident was availability caused by accidental amplification. The right response was circuit breaking, not debugging individual failures.

## Building Runbooks for Common Failure Modes

A **runbook** is a step-by-step guide for responding to specific incident types. It tells on-call engineers what to check, what to try, and when to escalate. Runbooks reduce incident response time from hours to minutes because engineers do not have to figure out what to do from scratch.

Your AI system needs runbooks for each failure mode. Start with the most common incidents, then expand coverage over time. A mature AI platform has 15-20 runbooks covering the full range of failure scenarios.

The availability incident runbook starts with checking model provider status. Go to the provider's status page. Check their API health endpoints. Search their community forums for reports. If the provider is down, your options are limited—wait, use cached responses, or failover to a backup provider if you have multi-provider setup. If the provider is healthy, the problem is your system. Check your rate limits. Check for retry loops. Check for authentication issues.

The performance incident runbook focuses on request characteristics. Pull slow request logs and identify common attributes. Are certain prompts consistently slow? Are specific users affected? Did slowness start after a recent deploy? Check context lengths—long contexts are the most common performance killer. Check concurrent request load—are you overwhelming the API? Check for changes to prompt structure that might have increased processing complexity.

The quality incident runbook requires comparing current outputs to baselines. Run your evaluation set and compare metrics to historical results. If accuracy dropped, check if the model version changed. Check if prompt modifications were deployed recently. Check if upstream data sources changed. Look for patterns in failures—does quality degrade for specific input types? For certain users? At certain times of day?

The cost incident runbook identifies spending spikes and their sources. Check cost dashboards for anomalies. Identify which teams or features drove the spike. Pull logs for high-cost requests. Check for retry loops or error handling bugs that repeat requests. Check for new users or use cases generating unexpected load. Check if anyone launched a feature without budget approval.

The safety incident runbook starts with immediate mitigation, then investigation. Disable the feature or add emergency content filtering. Pull logs of problematic outputs and the prompts that generated them. Identify whether the issue is a new jailbreak technique, a prompt logic error, or model behavior change. Engage your security and compliance teams. Document everything for potential legal or regulatory review.

## Detection and Alerting for AI-Specific Problems

You cannot respond to incidents you do not detect. Traditional monitoring catches availability and performance problems. Quality, cost, and safety problems need AI-specific detection.

Quality monitoring requires continuous evaluation. Run a subset of your evaluation set through production prompts every hour. Compare outputs to expected characteristics. If accuracy drops below threshold, alert. If format compliance fails, alert. If semantic similarity to reference outputs decreases, alert. This catches quality degradations hours or days faster than waiting for user complaints.

Cost monitoring requires real-time spending analysis. Calculate costs per prompt execution. Track spending against daily budgets. Alert when spending rate exceeds expected baseline by 50%. Alert when individual requests exceed cost thresholds. Alert when spending per user spikes. These alerts catch cost incidents immediately instead of at the end of the billing cycle.

Safety monitoring requires content classification. Run model outputs through content policy classifiers. Check for PII patterns. Check for toxic content. Check for competitor mentions if relevant. Alert when classifiers flag problematic content. Alert when outputs contain patterns from known jailbreaks. These alerts catch safety issues before they cause major harm.

You also need composite alerts that combine signals. If latency increases and error rates increase simultaneously, that suggests provider issues. If costs spike and quality drops, that might indicate a retry loop on failed validation. If user activity drops after a deployment, that suggests users are encountering problems even if technical metrics look healthy.

Most teams send alerts to Slack channels and PagerDuty for critical issues. Different incident types route to different responders. Availability and performance issues page on-call engineers. Quality issues page the ML team. Cost issues alert finance and engineering managers. Safety issues page security and legal. This routing ensures the right people see problems they can fix.

## Incident Response Team Structure

AI incidents require different expertise than traditional outages. Database problems need database experts. AI problems need people who understand prompts, models, and evaluation. You need a response structure that brings together the right skills.

Most teams have a three-tier escalation structure. Tier 1 is on-call engineers who follow runbooks for common issues. They can restart services, roll back deployments, enable fallbacks, and apply circuit breakers. They handle 60-70% of incidents using documented procedures.

Tier 2 is prompt engineers and ML specialists who handle quality incidents and novel failures. They can debug prompt behavior, analyze evaluation results, adjust model parameters, and implement quick fixes. They get escalated to when runbooks do not resolve the issue or when investigation requires deep prompt system knowledge.

Tier 3 is senior technical leadership who make decisions about disabling features, communicating to users, or accepting temporary workarounds. They get escalated to for major incidents affecting revenue or safety, incidents requiring business decisions, or incidents that might need customer communication.

This structure requires cross-training. On-call engineers need to understand basic prompt debugging even if they are not prompt specialists. Prompt engineers need to understand operational concerns like rate limiting and fallbacks even if they are not SREs. Leadership needs enough technical depth to make informed decisions under pressure.

The legal tech startup had no tiered structure. Their on-call engineer did not know how to debug AI systems. Their prompt specialist was not on call and did not see the incident until Monday. Their CTO made the decision to disable the feature from an airport without full context. A proper structure would have triaged faster and minimized impact.

## Post-Incident Review and Learning

Resolving an incident is not the end of incident response. You need to understand what happened, why it happened, and how to prevent recurrence. This happens through post-incident review, sometimes called postmortems or retrospectives.

Schedule the review within 48 hours while details are fresh. Invite everyone involved in detection, response, and resolution. Include representatives from affected teams. The goal is learning, not blame. You are optimizing for better future response, not punishing people for mistakes.

Document five sections in your incident report. First is timeline—when did the incident start, when was it detected, what actions were taken when, when was it resolved. This reveals detection delays and response efficiency.

Second is root cause—what actually failed and why. For AI incidents, distinguish between proximate causes and underlying causes. Proximate cause might be a model API returning slow responses. Underlying cause might be that you have no timeout configuration or fallback logic.

Third is impact—who was affected, for how long, and what was the business cost. Measure in user impact, revenue loss, support burden, and reputation damage. Quantifying impact helps prioritize prevention efforts.

Fourth is what went well—which parts of your response worked effectively. Maybe your monitoring detected the issue quickly. Maybe your runbook helped the on-call engineer take fast action. Maybe your backup systems prevented worse impact. Understanding what works helps you do more of it.

Fifth is action items—specific, assigned tasks to prevent recurrence. Add monitoring for this failure mode. Create a runbook for this incident type. Fix the bug that caused the problem. Implement the circuit breaker that would have limited impact. Each action item needs an owner and deadline.

Make incident reports accessible to the whole engineering team. Some teams post summaries in team channels. Some maintain a incidents database that engineers can search when facing similar issues. Some do monthly incident review sessions to share learnings. The format matters less than ensuring knowledge spreads.

## Preventing Recurrence Through System Improvements

Post-incident action items fall into three categories. Quick fixes address the immediate cause. Systemic improvements make the class of failure less likely. Detection improvements help you find similar issues faster next time.

Quick fixes are usually code changes. Fix the bug that caused retry loops. Adjust the timeout that was too generous. Correct the prompt that was causing quality issues. These fixes are specific to the incident and usually ship within a week.

Systemic improvements change architecture or process. Add circuit breakers to prevent amplification. Implement multi-provider failover for availability. Create prompt validation that catches problematic changes before production. These improvements take longer but prevent whole categories of incidents.

Detection improvements make failures visible sooner. Add monitoring for the metric that would have caught this issue earlier. Create an alert for the condition that caused this incident. Tighten thresholds on existing alerts that did not fire when they should have. Better detection reduces mean time to detection, the often-dominant factor in incident impact.

You cannot implement every possible improvement. Prioritize based on incident frequency and severity. If you have availability incidents weekly, invest in failover infrastructure. If quality incidents are rare but high impact, invest in better evaluation and gradual rollout. If cost incidents are frequent, invest in budget controls and rate limiting.

Track prevention effectiveness. After implementing circuit breakers, do you see fewer amplification incidents? After adding quality monitoring, are quality incidents detected faster? This data justifies continued investment in reliability and helps you focus on highest-leverage improvements.

## Rehearsing Response Through Incident Drills

Fire departments train with drills. Surgical teams simulate emergencies. On-call engineers should practice incident response. You do not want the first time your team responds to a quality incident to be a real incident with user impact. You want practiced responses that are fast and confident.

Schedule quarterly incident drills. Pick a realistic failure scenario. A model provider has an outage. A prompt change causes quality regression. A cost spike from unexpected usage. Simulate the failure in a staging environment. Page the on-call engineer. Execute the response playbook. Time how long each step takes.

Drills reveal gaps. Maybe your runbook is out of date. Maybe key tools are not accessible to on-call engineers. Maybe escalation paths are unclear. Maybe backup systems do not actually work. You discover these problems in a controlled drill instead of during a real incident.

Drills also build confidence and muscle memory. Engineers practice using dashboards, executing rollbacks, and communicating with stakeholders. When real incidents happen, these actions feel familiar instead of stressful and novel. Response times improve because people know exactly what to do.

Rotate drill scenarios to cover different failure types. Practice availability incidents, quality incidents, cost incidents, and safety incidents. Practice incidents that require escalation. Practice incidents where the usual fix does not work. This variety prepares teams for the unexpected.

After each drill, do a brief retrospective. What went well? What was confusing? What should be changed? Update runbooks based on drill findings. Fix tool access issues. Clarify escalation procedures. The drill investment pays off through better real incident response.

## Communicating During and After Incidents

Incidents have technical and human dimensions. Users are affected. Stakeholders need updates. Teams coordinate response. Communication is as important as technical mitigation.

During incidents, establish a dedicated communication channel. Create a Slack channel or war room for the specific incident. All responders join. All updates get posted there. This prevents information fragmentation where different people have different understandings of the situation.

Assign a communication lead separate from technical responders. This person coordinates updates to stakeholders, writes status posts, and handles escalation. They let technical responders focus on fixing the problem instead of answering questions.

Provide regular updates even when nothing has changed. Every 30 minutes during major incidents, post a status update. Users affected, mitigation steps taken, current status, expected resolution time. Updates manage stakeholder anxiety and demonstrate you are actively working the problem.

After incidents, communicate learnings appropriately. For internal incidents, share post-incident reports with engineering and affected teams. For user-facing incidents, publish status page updates and customer communications explaining what happened and what you are doing to prevent recurrence. For severe incidents, leadership might do direct outreach to affected customers.

The legal tech startup failed at communication. Their prospects experienced an outage with no explanation. No status updates. No acknowledgment. By the time the team was ready to explain what happened, prospects had already decided the platform was unreliable. Better communication during the incident might have saved those relationships.

## Building Organizational Muscle for AI Incidents

Incident response maturity develops over time. Your first AI incidents will be chaotic. People will not know what to do. Runbooks will not exist. Monitoring will miss problems. This is normal. You get better through practice and continuous improvement.

Start by writing your first runbook after your first incident. Document what happened and how you resolved it. Next time you see something similar, you have a starting point. After five incidents, you have five runbooks. After 20 incidents, you have comprehensive coverage.

Invest in monitoring and alerting incrementally. After each incident, add the monitoring that would have caught it earlier. Over time, your detection capabilities compound. You catch more problems faster with less user impact.

Create a culture of blameless learning. Engineers should feel safe reporting mistakes and close calls. Every near-miss is an opportunity to improve before it becomes a real incident. Every incident is an opportunity to make the system more robust.

Measure incident response metrics. Track mean time to detection. Track mean time to resolution. Track incident frequency by category. Track recurrence rates for incident types you have seen before. These metrics show whether you are getting better at responding and preventing.

The legal tech startup rebuilt their incident response program after the expensive outage. They created runbooks for five failure types. They added quality and cost monitoring. They did quarterly drills. They established escalation procedures. Six months later, they detected and resolved a quality incident in 15 minutes with no user impact. That fast response came from systematic preparation, not luck.

Incidents are inevitable in complex systems. AI systems add new failure modes to traditional infrastructure problems. You will have availability incidents, quality incidents, cost incidents, and safety incidents. The question is whether you respond quickly with practiced procedures or slowly with improvised chaos. Runbooks, drills, and systematic learning turn incidents from disasters into learning opportunities.

The next subchapter examines the maturity model for prompt engineering practices, describing the progression from ad-hoc experimentation to engineering discipline and providing assessment criteria for organizational readiness.
