# 3.2 — Designing Prompts for 100K–1M+ Token Context Windows

A Series B fintech company spent $380,000 on a December 2025 sprint to migrate their compliance analysis pipeline to Claude's 200K context window, believing that dumping entire regulatory document sets into single prompts would eliminate their multi-stage retrieval system and cut latency by 70%. The sixteen-person engineering team rewrote their prompt templates to include full text of all relevant regulations—often 150,000 to 180,000 tokens per request—plus case law excerpts, internal policy documents, and the transaction data being analyzed. They launched the new system to their premium tier customers on December 18th.

Within four days, the VP of product was fielding escalations from their three largest enterprise clients. The new system was slower than the old one, not faster. Response times had increased from 12 seconds to 41 seconds. Cost per analysis had tripled from $1.20 to $3.80. Worst of all, output quality had not improved—in several cases, it had degraded. The model was missing regulatory citations that the old system caught reliably. It was sometimes generating vague summaries instead of specific compliance findings. The team had assumed that more context automatically meant better answers, but they had hit diminishing returns and architectural breakdown.

The root cause was context overload. The model was spending compute on processing irrelevant regulations, skimming through redundant policy documents, and attending to background material that added no value to the specific analysis task. The old retrieval system had been selective, surfacing only the three to five most relevant regulatory sections. The new system dumped everything and hoped the model would figure out what mattered. It could not, or at least not efficiently enough to justify the cost and latency penalty.

## Long Context as Capability, Not Universal Solution

Long context windows are a tool for specific problems, not a replacement for good information architecture. When Claude Opus 4 and GPT-4 Turbo launched with 200K token windows, many teams treated them as "throw everything in and let the model sort it out" black boxes. This works for narrow use cases—analyzing a single long document, comparing two complete codebases, or processing an entire conversation history—but it breaks down when you try to use long context as a substitute for retrieval, filtering, and relevance ranking.

The core issue is that **attention is finite**. A model with a 200K context window can process 200K tokens, but it cannot attend equally to all 200K tokens when generating each output token. Transformer architectures distribute attention across the context, and as context length grows, the attention allocated to any single token shrinks. The model can "see" all 200K tokens, but it cannot hold all of them in active working memory simultaneously.

This creates a trade-off. Long context windows allow you to include more information, which increases the chance that relevant information is present. But they also increase the chance that relevant information gets lost in the noise, especially if that information appears in the middle of a long context. You gain recall—more relevant facts are somewhere in the context—but you risk precision—the model might not reliably extract those facts when they matter.

You use long context windows when the task genuinely requires access to large amounts of information that cannot be easily filtered or ranked. You avoid them when you can achieve the same result with selective retrieval and smaller contexts.

## What to Include vs Exclude in Long-Context Prompts

You include information that the model needs to generate a correct, complete, and grounded answer. This is not "everything that might be relevant." It is "everything that is necessary." The distinction matters because every additional token you include increases cost, latency, and the risk of attention dilution.

You include complete documents when the task requires cross-referencing, coherence checking, or full-document synthesis. If your prompt asks the model to summarize a 50,000-token legal contract, you include the full contract because the model needs access to all clauses to identify conflicts and dependencies. If your prompt asks the model to compare two versions of a technical specification and identify all changes, you include both full versions because the model needs complete context to catch subtle edits.

You exclude documents that provide background context but are not directly referenced in the task. If your prompt asks the model to analyze a specific regulatory compliance issue, you include the regulations that apply to that issue. You do not include the entire regulatory code just because it exists. If your prompt asks the model to debug a specific function, you include that function and its direct dependencies. You do not include the entire codebase unless the bug involves cross-module interactions.

You exclude redundant information. If five documents all restate the same policy, you include one document and reference the others by name. The model does not need to read the same policy five times to understand it. Redundancy wastes tokens and increases the chance of lost-in-the-middle errors.

You exclude low-relevance information even if it is topically related. If your prompt asks the model to analyze a contract dispute, you include the contract and relevant case law. You do not include a general primer on contract law unless the dispute involves an obscure legal principle. The model already knows contract law basics. You are paying for tokens that teach it nothing new.

## Diminishing Returns of Context Length

Context length exhibits diminishing marginal returns. The first 10,000 tokens of context often provide the bulk of the value. The next 50,000 tokens add incremental value. The next 100,000 tokens add very little unless the task specifically requires access to that much information.

This happens because most tasks have a natural information boundary. A legal analysis task might require 5,000 tokens of contract text, 3,000 tokens of case law, and 2,000 tokens of internal notes—call it 10,000 tokens total. Adding another 50,000 tokens of tangentially related case law does not improve output quality because the model already has the information it needs. The additional context is noise.

You measure diminishing returns by running experiments where you vary context length and measure output quality. Start with a minimal context that includes only essential information. Measure quality. Add another 10,000 tokens of potentially relevant information. Measure again. Keep adding until quality stops improving. That is your point of diminishing returns.

Most teams find that quality plateaus somewhere between 20,000 and 60,000 tokens for structured analysis tasks. For summarization tasks, quality might plateau at 80,000 tokens. For creative tasks like story writing or brainstorming, quality might keep improving up to 150,000 tokens because the model benefits from diverse examples and background material. The plateau point depends on task type.

Once you hit the plateau, additional context is waste. It costs money, adds latency, and increases the risk of attention errors. You cut it.

## Structuring Massive Inputs for Model Consumption

When you do need to include 100,000-plus tokens of context, you structure it to make the model's job easier. Unstructured dumps of text force the model to infer relationships, identify key sections, and navigate without signposts. Structured inputs provide roadmaps.

You use explicit section markers. Instead of concatenating ten documents into one blob, you separate them with clear delimiters: "Document 1: Employment Contract," "Document 2: Employee Handbook," "Document 3: Severance Policy." The model can then reference specific documents in its reasoning chain. Without markers, the model might generate vague statements like "according to the policy" without specifying which policy.

You use hierarchical structure for nested information. If you are including a 60,000-token regulatory document, you preserve its section numbering and headings. The model can then reference "Section 4.2.1" instead of "somewhere in the middle of the regulations." Hierarchical structure also helps the model navigate. If it needs information about data retention, it can scan section headers instead of reading every sentence.

You front-load critical information. If there is a specific clause, regulation, or code snippet that your task hinges on, you place it near the beginning of the context. This exploits the primacy effect—models tend to attend more strongly to information near the start of the context. You do not bury critical information on line 80,000 of a 120,000-token context.

You use metadata tags to signal importance. If certain documents are authoritative sources while others are supplementary, you label them: "Primary Source: IRS Publication 15," "Background Reference: Company Tax Policy." This helps the model weight information correctly. Without tags, the model treats all documents as equally authoritative.

You compress low-priority information. If you need to include a 40,000-token reference document that is only tangentially relevant, you include a 2,000-token summary instead of the full text. The model gets the key points without wading through details it does not need. You include the full text only if the task requires fine-grained access.

## Cost Implications of Long Contexts

Long context windows are expensive. Input tokens cost less than output tokens, but at 100,000-token context lengths, input costs dominate your bill. If your model charges $3 per million input tokens and you run 10,000 requests per day with 150,000-token contexts, you are spending $4,500 per day on input tokens alone—$135,000 per month.

You calculate long-context costs by multiplying average context length by requests per period by input token price. If you are considering a migration to long-context prompts, you run this calculation before you write any code. Most teams discover that long-context approaches are viable only if they reduce requests or improve outcomes enough to justify the higher per-request cost.

You compare long-context costs to multi-stage retrieval costs. A two-stage system that retrieves 5,000 tokens in stage one and 15,000 tokens in stage two costs less per request than a single-stage system that processes 150,000 tokens. The two-stage system makes two API calls, but the total token cost is lower. You choose the architecture that minimizes cost while meeting quality requirements.

You use caching to reduce repeated context costs. If you are running multiple analyses on the same regulatory document set, you cache the document embeddings or context so you do not re-process the same 100,000 tokens on every request. Some API providers offer prompt caching where repeated context prefixes are billed at reduced rates. You exploit these features when available.

You monitor context length distributions in production. If 80% of your requests use fewer than 30,000 tokens but 20% use 150,000 tokens, you might split your system into two tiers. The high-volume, low-context tier uses a cheaper model or tighter retrieval. The low-volume, high-context tier uses long-context windows. This optimizes cost across your workload mix.

## When Long Context Beats Multi-Stage Retrieval

Long context windows win when the task requires global coherence, cross-referencing, or synthesis across many sections of a document. If your prompt asks the model to identify all conflicting clauses in a 50,000-token contract, long context allows the model to hold the entire contract in memory and check every clause pair. A retrieval-based approach would struggle because you cannot predict which clauses conflict without reading them all.

Long context windows win when retrieval is expensive or brittle. If your retrieval system requires embedding thousands of document chunks, maintaining a vector database, and tuning relevance thresholds, the operational cost might exceed the token cost of just processing the full document. Long context simplifies architecture.

Long context windows win when the information need is unpredictable. If your task is open-ended—"analyze this document and tell me what is interesting"—you cannot pre-filter relevant sections because you do not know what the model will find interesting. You include the full document and let the model explore.

Long context windows win when latency is more important than cost. A single long-context request completes faster than a multi-stage retrieval pipeline with multiple round trips. If your SLA requires sub-three-second responses, long context might be your only option even if it costs more per request.

## When Multi-Stage Retrieval Beats Long Context

Multi-stage retrieval wins when the task has a narrow information need and you can reliably identify relevant sections. If your prompt asks the model to check whether a contract includes a non-compete clause, you retrieve the section on post-employment restrictions and send only that section to the model. You save 95% of the token cost and lose no quality.

Multi-stage retrieval wins when most of your context is irrelevant most of the time. If you are running a Q&A system over a 500,000-token knowledge base and each question needs only 5,000 tokens of context, long context is absurd. You retrieve the relevant 5,000 tokens and ignore the rest.

Multi-stage retrieval wins when your workload is heterogeneous and some requests need far more context than others. You use retrieval to normalize context length across requests. Every request gets the tokens it needs, not a fixed long-context budget.

Multi-stage retrieval wins when you need to compose information from multiple sources that are updated independently. If your system pulls data from a CRM, a knowledge base, and a vector database, you retrieve from each source and compose a context dynamically. Long context does not help because you cannot pre-concatenate sources that change frequently.

## Hybrid Architectures: Retrieval Plus Long Context

The best production systems combine retrieval and long context. You use retrieval to filter down to a manageable set of relevant documents, then you include the full text of those documents in a long-context prompt. This gives you the precision of retrieval and the global coherence of long context.

A typical hybrid flow: user submits a query, your retrieval system identifies the top five relevant documents from a corpus of 500, you include the full text of those five documents (totaling 60,000 tokens) in your prompt, the model generates an answer grounded in those five documents. You did not process all 500 documents, so you saved cost. You did not truncate the five relevant documents to snippets, so you preserved coherence.

You tune the retrieval threshold to balance cost and recall. If you retrieve only the top two documents, you minimize cost but risk missing relevant information. If you retrieve the top twenty documents, you maximize recall but approach the cost of just processing everything. You run experiments to find the sweet spot.

You use retrieval to handle the long tail. Your system has a default mode that uses a 30,000-token context from retrieval. If the model indicates that it needs more information to answer the query, your system retrieves additional documents and makes a second request with a 100,000-token context. Most requests stay in the cheap mode. Difficult queries escalate to the expensive mode.

You use long context to handle edge cases that break retrieval. If retrieval fails to surface the relevant document—because the query uses unexpected terminology or the document is poorly indexed—you fall back to a long-context prompt that includes a broader set of documents. This ensures robustness at the cost of occasional high-token requests.

## Latency Characteristics of Long-Context Prompts

Long-context prompts are slower than short-context prompts, all else equal. The model must process more input tokens before it begins generating output. For a 150,000-token context, this processing phase can add 10 to 30 seconds of latency depending on model and hardware. If your SLA requires sub-five-second responses, long context might be incompatible.

You reduce latency by using prompt caching where available. If you are reusing the same 100,000-token regulatory document across multiple requests, you cache it so the model does not re-process it every time. Cache hits reduce latency by 70% or more.

You reduce latency by streaming output. Even if input processing takes 20 seconds, the user sees the first output tokens within a second or two after processing completes. Streaming makes long-context prompts feel faster even when total latency is high.

You reduce latency by preprocessing documents offline. If you know you will need a specific set of documents for a category of requests, you preprocess them into a standardized format, cache the embeddings, and reuse them. This shifts latency from request time to preprocessing time.

You set timeout policies for long-context requests. If a request exceeds your latency budget, you terminate it and fall back to a faster, lower-context approach. This prevents long-context prompts from degrading user experience for latency-sensitive tasks.

## Quality Monitoring for Long-Context Systems

You monitor long-context systems differently than short-context systems because the failure modes are different. Short-context systems fail when they lack information. Long-context systems fail when they have too much information and lose track of what matters.

You track citation accuracy. If your prompt asks the model to ground its answer in provided documents, you verify that the model cites the correct documents and does not hallucinate sources. Long-context systems sometimes cite "Document 7" when the information came from Document 3, or cite a document that was not included in the context at all.

You track lost-in-the-middle errors. You create synthetic test cases where the critical information appears at different positions in the context—beginning, middle, end—and you measure whether the model reliably finds it. If the model catches information at the beginning and end but misses information in the middle, you have a lost-in-the-middle problem.

You track output specificity. Long-context systems sometimes generate vague answers like "according to the policies provided" instead of specific answers like "according to Section 4.2 of the Employee Handbook." Vagueness suggests the model is not attending to specific sections. You measure specificity by checking for concrete references and citations.

You track cost per request and compare it to quality. If your long-context system costs $4 per request and your old retrieval system cost $1 per request, you need to demonstrate that quality improved by enough to justify the 4x cost increase. If quality is only marginally better, you revert to the cheaper architecture.

## Long-Context Prompt Design Patterns

You design long-context prompts to guide the model's attention. You do not assume the model will automatically focus on the right information just because it is present somewhere in the context. You provide structure, signposts, and explicit instructions.

You use a "context map" at the beginning of your prompt. Before including the full documents, you list what is included: "This context includes: (1) Full text of Employment Contract, 12,000 tokens, (2) Employee Handbook excerpt, 8,000 tokens, (3) Relevant case law, 15,000 tokens." The model knows what it is working with and can plan its approach.

You place your task instructions at the end of the context, after all documents. This exploits the recency effect—models attend more strongly to information near the end of the context. If your task instructions are buried at the beginning and followed by 120,000 tokens of documents, the model might lose track of what you asked it to do.

You use explicit "focus directives" when you need the model to pay special attention to specific sections. Instead of hoping the model notices that Section 4.2 is important, you write "Pay special attention to Section 4.2 of the contract, which governs termination conditions." This is not prompt hacking. It is architectural guidance.

You ask the model to cite its sources. Your prompt includes: "For each claim you make, cite the specific document and section where you found the information." This forces the model to ground its reasoning in the context and makes it easier for you to verify correctness.

The next subchapter covers long-context prompt architecture, focusing on placement, ordering, and attention distribution strategies for contexts that exceed 100,000 tokens.
