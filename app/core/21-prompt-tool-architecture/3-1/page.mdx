# 3.1 — Extended Thinking: When and How to Use Chain-of-Thought Budgets

A Series C legal-tech startup burned through $47,000 in API costs during their November 2025 pilot testing phase, running contract analysis workflows that their head of engineering thought would cost "maybe eight grand, tops." The eight-person product team had configured every single prompt to use Claude's extended thinking feature with maximum token budgets, believing that more thinking time would automatically yield better results. Their contract summarization tasks used 10,000 thinking tokens per request. Their clause extraction tasks allocated 15,000 thinking tokens. Even their simple classification prompts—tagging contracts as "NDA," "MSA," or "SOW"—reserved 8,000 thinking tokens per call.

The engineering lead discovered the problem during a routine cost review meeting. The extended thinking feature was consuming 73% of their total API spend, but when they examined the actual output quality, they found no correlation between thinking time and accuracy for 60% of their use cases. The simple classification tasks showed identical F1 scores whether they used extended thinking or not. The contract summarization tasks improved by only 2% despite using thinking budgets that cost 4x the base prompt. The team had assumed that extended thinking was a performance booster for all tasks, when in reality it was a specialized tool for a narrow set of reasoning-heavy problems.

## Extended Thinking as Compute Allocation, Not Magic Sauce

Extended thinking is not a quality dial you turn up to make outputs better. It is a **compute budget** that allows the model to perform additional internal reasoning steps before generating a response. When you enable extended thinking with a token budget, you are paying for the model to explore multiple solution paths, backtrack from dead ends, verify intermediate conclusions, and refine its reasoning chain before committing to an answer.

This matters because most production tasks do not require exploratory reasoning. If your prompt asks the model to extract named entities from a resume, the model does not need to explore alternative interpretations or verify logical consistency. It sees "Stanford University" and tags it as an organization. If your prompt asks the model to classify a support ticket as "billing," "technical," or "sales," the model does not need a reasoning budget to weigh competing hypotheses. It matches patterns and outputs a label.

Extended thinking pays dividends when the task involves multi-step logical inference, constraint satisfaction, mathematical reasoning, or complex planning. If your prompt asks the model to generate a valid SQL query from a natural language request, verify it does not produce unintended side effects, and explain why it chose one join strategy over another, extended thinking gives the model space to work through those verification steps. If your prompt asks the model to analyze a legal argument and identify logical fallacies, extended thinking allows it to consider multiple interpretations before settling on a conclusion.

You allocate thinking budgets when the task is hard enough that a human would benefit from scratch paper and backtracking. You skip extended thinking when the task is pattern matching, simple extraction, or template filling.

## Cost Structure of Thinking Tokens

Thinking tokens cost the same as output tokens in most API pricing tiers, but they do not appear in the final response the user sees. When you allocate a 5,000-token thinking budget and the model uses 4,200 of those tokens, you pay for 4,200 tokens that produce no visible output. The model's final answer might be only 150 tokens, but your total cost is 4,200 plus 150 plus input tokens.

This cost structure creates a sharp trade-off. Extended thinking is expensive relative to the value it delivers unless the task genuinely requires reasoning. A contract summarization task that costs $0.12 per request without extended thinking might cost $0.48 with a 10,000-token thinking budget. If the thinking budget improves summary quality by 15%, you are paying 4x for a 15% gain. That trade-off makes sense for high-stakes legal review where errors cost thousands of dollars. It does not make sense for internal document tagging where a 15% improvement is worth pennies.

You calculate thinking token costs the same way you calculate output token costs. If your model charges $15 per million output tokens, a 5,000-token thinking budget costs $0.075 per request. If you run 100,000 requests per month, that thinking budget adds $7,500 to your monthly bill. You compare that $7,500 to the value of whatever improvement extended thinking delivers, and you make an economic decision about whether the reasoning budget is worth the spend.

Most teams discover that extended thinking is worth the cost for fewer than 20% of their use cases. The other 80% run faster and cheaper without it.

## Tasks That Benefit from Extended Thinking

Extended thinking delivers measurable value when the task involves **constraint satisfaction**, **multi-step verification**, **logical inference**, or **exploratory problem-solving**. These are tasks where a human would pause, consider alternatives, check their work, and revise their approach based on intermediate findings.

Code generation tasks benefit when you ask the model to write a function, verify it handles edge cases, and refactor for readability. The thinking budget gives the model space to generate a first draft, mentally test it against corner cases, identify bugs, and rewrite. Without extended thinking, the model outputs its first draft and moves on. With extended thinking, it can catch off-by-one errors, null pointer risks, and inefficient algorithms before committing to a final answer.

Mathematical reasoning tasks benefit when you ask the model to solve a multi-step word problem, verify each step, and explain its reasoning. The thinking budget lets the model explore different solution strategies, backtrack when an approach leads to contradiction, and double-check arithmetic. Without extended thinking, the model might output a plausible-looking answer that fails verification. With extended thinking, it catches errors in its own reasoning chain.

Legal and compliance analysis tasks benefit when you ask the model to interpret a contract clause, identify conflicts with other clauses, and assess risk. The thinking budget allows the model to consider alternative interpretations, weigh evidence, and revise its conclusion if it finds contradictory information later in the document. Without extended thinking, the model might commit to an interpretation too early and miss nuance.

Planning tasks benefit when you ask the model to generate a project timeline, verify dependencies, and adjust for resource constraints. The thinking budget gives the model room to explore different sequencing options, identify bottlenecks, and backtrack when a plan violates constraints. Without extended thinking, the model might produce a plan that looks reasonable but breaks under scrutiny.

## Tasks That Waste Thinking Budgets

Classification tasks almost never benefit from extended thinking unless the classification requires multi-step reasoning. If your prompt asks the model to tag a customer email as "complaint," "question," or "feedback," the model does pattern matching. It sees phrases like "this is unacceptable" and outputs "complaint." No reasoning budget required.

Extraction tasks rarely benefit unless the extraction involves ambiguity resolution or cross-reference verification. If your prompt asks the model to pull dates, names, and amounts from an invoice, the model scans for patterns and outputs structured data. Extended thinking does not improve accuracy because there is no reasoning to perform.

Summarization tasks sometimes benefit from extended thinking, but only when the summary requires synthesis across conflicting sources or deep compression of complex arguments. If your prompt asks the model to summarize a straightforward news article, extended thinking is wasted. The model reads, compresses, and outputs. If your prompt asks the model to synthesize five research papers with contradictory findings into a coherent summary, extended thinking helps the model identify conflicts and resolve them.

Template-filling tasks never benefit. If your prompt asks the model to generate a personalized email by filling in customer name, product, and issue description, the model performs variable substitution. No reasoning involved.

Sentiment analysis tasks do not benefit unless the sentiment is ambiguous and context-dependent. If your prompt asks the model to classify a product review as positive or negative, the model matches affective language to sentiment labels. Extended thinking is overkill.

## Setting Thinking Token Budgets

You set thinking token budgets based on task complexity, not task importance. A high-stakes task like legal contract review might not need extended thinking if the task is simple extraction. A low-stakes task like generating a creative story might benefit from extended thinking if the task involves plot coherence and character consistency.

Start with a baseline budget of zero. Run your task without extended thinking and measure output quality. If quality is acceptable, you are done. If quality is poor and the failure mode involves reasoning errors—contradictions, incomplete logic, missed edge cases—add a small thinking budget and measure again.

A small budget is 1,000 to 3,000 tokens. This gives the model enough room to perform one or two verification passes without burning cost. If quality improves, you have evidence that thinking helps. If quality stays the same, extended thinking is not the bottleneck. Your problem is likely prompt design, input quality, or task-model mismatch.

A medium budget is 5,000 to 10,000 tokens. This gives the model room for multi-step reasoning, backtracking, and verification. You allocate medium budgets when the task involves constraint satisfaction, logical inference, or cross-referencing. Most teams find that 5,000 tokens is enough for code generation with verification. Ten thousand tokens handles legal analysis with conflict detection.

A large budget is 15,000 to 30,000 tokens. You allocate large budgets when the task involves deep exploration, adversarial reasoning, or multi-stage planning. These budgets are expensive and should be reserved for tasks where reasoning quality directly impacts business outcomes. A customer-facing legal analysis tool justifies a 20,000-token budget. An internal document tagger does not.

You monitor actual thinking token usage in your API logs. If the model consistently uses 80% or more of your allocated budget, the task might benefit from a larger budget. If the model uses less than 30% of your budget, you are over-allocating and wasting cost.

## Prompt Design for Extended Thinking

Extended thinking works best when your prompt explicitly asks the model to verify, explore, or reason through alternatives. If your prompt says "extract all dates from this document," the model has no reason to allocate thinking tokens. If your prompt says "extract all dates, verify they are formatted consistently, and flag any ambiguous date references," the model has a clear reasoning task.

You scaffold thinking by breaking complex tasks into verification steps. Instead of "generate a SQL query," you write "generate a SQL query, verify it handles null values correctly, check for potential performance issues, and explain your reasoning." The verification steps signal to the model that thinking time is valuable.

You avoid vague instructions like "think carefully" or "be thorough." These add no structure. The model does not know what to think carefully about. Instead, you specify what to verify: "check for edge cases," "identify potential contradictions," "consider alternative interpretations."

You use extended thinking in combination with structured output formats. If your prompt asks the model to output JSON with fields for reasoning, answer, and confidence, the model can use thinking tokens to refine its reasoning before committing to the final JSON structure. Without structured output, thinking tokens improve reasoning quality but do not necessarily improve output consistency.

You do not need to tell the model that extended thinking is enabled. The model automatically allocates thinking tokens when the task warrants it and your budget allows it. Your job is to design prompts that give the model clear reasoning goals.

## Monitoring Thinking Efficiency

You measure thinking efficiency by comparing thinking token usage to output quality improvement. If you allocate a 5,000-token budget and output quality improves by 20%, you calculate the cost per quality point and compare it to alternative interventions like better prompt design or example selection.

You log thinking token counts alongside task metadata. Your logs should record task type, thinking budget, actual thinking tokens used, output quality score, and total cost. Over time, you identify which task types benefit from thinking budgets and which do not. You adjust budgets based on data, not intuition.

You run A/B tests with and without extended thinking for ambiguous use cases. If you are unsure whether contract summarization benefits from a 5,000-token budget, you run half your requests with the budget and half without. You compare output quality across the two groups using human eval or automated metrics. If quality is statistically identical, you drop the thinking budget and save cost.

You track thinking token usage over time. If usage drops as your prompts improve, your prompt design is reducing the need for exploratory reasoning. The model gets clearer instructions and needs less backtracking. If usage stays constant or increases, your tasks are genuinely reasoning-heavy and extended thinking is earning its cost.

You set cost alerts for thinking token spend. If your monthly thinking budget exceeds a threshold, you review which tasks are consuming thinking tokens and whether those tasks justify the spend. Most teams discover that a small number of high-complexity tasks account for 80% of thinking token costs. You optimize those tasks first.

## When Extended Thinking Fails to Deliver Value

Extended thinking does not fix bad prompts. If your prompt is ambiguous, underspecified, or missing critical context, extended thinking gives the model more time to explore bad solution paths. You fix the prompt first, then test whether extended thinking adds value.

Extended thinking does not fix task-model mismatch. If your task requires domain knowledge the model does not have, more thinking time does not create knowledge. The model reasons with the information it has. If that information is incomplete, extended thinking produces elaborate reasoning built on faulty premises. You fix task-model mismatch by providing better context, using retrieval-augmented generation, or switching to a specialized model.

Extended thinking does not fix low-quality inputs. If your input document is poorly formatted, contradictory, or missing key information, the model cannot reason its way to a good answer. It spends thinking tokens trying to reconcile inconsistencies that should not exist. You fix input quality first.

Extended thinking does not replace human judgment for high-stakes decisions. The model can verify logical consistency, check edge cases, and explore alternative interpretations, but it cannot assess business risk, legal liability, or ethical trade-offs. You use extended thinking to augment human decision-making, not replace it.

## Integrating Thinking Budgets into Production Workflows

You configure thinking budgets at the task level, not the system level. Your contract analysis tasks get 10,000-token budgets. Your email classification tasks get zero. Your code generation tasks get 5,000 tokens. You do not apply a single thinking budget across all use cases because task complexity varies.

You expose thinking budget as a tunable parameter in your prompt orchestration layer. Your engineers set default budgets for each task type, and your ops team adjusts budgets based on cost and quality monitoring. You do not hardcode budgets in application logic where they are difficult to change.

You surface thinking token usage in your observability dashboards alongside latency, cost, and quality metrics. Your team sees that task type A uses 80% of its thinking budget and delivers 25% better quality, while task type B uses 20% of its budget and shows no quality improvement. You make data-driven decisions about where to allocate thinking compute.

You version thinking budgets alongside prompt versions. When you update a prompt to add verification steps, you might increase the thinking budget to give the model room to perform those steps. When you simplify a prompt to reduce ambiguity, you might decrease the budget because the model needs less exploratory reasoning.

You document why each task has its current thinking budget. Future engineers need to know whether the budget is based on measured quality improvement, stakeholder requirements, or legacy configuration. Without documentation, teams waste time re-testing budgets that were already optimized.

## Cost-Quality Trade-offs in Extended Thinking Decisions

You treat extended thinking as a cost center with measurable ROI. If a 5,000-token budget costs $0.075 per request and improves quality by 15%, you calculate the dollar value of that 15% improvement. For a legal analysis tool where errors cost $10,000 in missed contract terms, a 15% error reduction is worth $1,500 per avoided miss. The $0.075 thinking cost is trivial. For an internal email classifier where errors cost nothing, a 15% improvement is worthless. The $0.075 thinking cost is wasted.

You prioritize extended thinking for tasks where reasoning quality directly impacts revenue, risk, or user trust. Customer-facing legal tools, financial analysis, medical triage, and safety-critical planning all justify thinking budgets. Internal content moderation, low-stakes classification, and template generation do not.

You experiment with dynamic thinking budgets based on task difficulty. If your system detects that a contract has unusual structure or conflicting clauses, it allocates a larger thinking budget for that specific request. If the contract is straightforward, it uses a minimal budget. This requires additional prompt logic to assess task difficulty, but it optimizes cost across heterogeneous workloads.

You revisit thinking budgets quarterly as models improve. Extended thinking is a capability that evolves with model releases. A task that required 10,000 thinking tokens in November 2025 might need only 5,000 tokens by March 2026 if the base model gets better at reasoning. You do not set budgets once and forget them.

The next subchapter covers how to design prompts for models with 100,000 to 1 million-plus token context windows, where the challenge shifts from reasoning depth to information architecture.
