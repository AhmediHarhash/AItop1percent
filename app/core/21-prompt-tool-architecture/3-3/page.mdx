# 3.3 — Long-Context Prompt Architecture: Placement, Ordering, and Attention

A Series D healthcare-AI startup discovered a silent quality degradation in their clinical decision support system during a January 2026 audit that reviewed 2,400 model outputs from the previous quarter. The twelve-person ML team had been running a prompt architecture that included patient history, lab results, clinical guidelines, and diagnostic criteria—totaling 80,000 to 120,000 tokens per request. They placed task instructions at the beginning of the prompt, followed by patient data, then clinical guidelines stacked in alphabetical order by guideline title. Output quality had been acceptable during initial testing in August 2025, but by December, their clinical partners were reporting that the system sometimes missed relevant diagnostic criteria that were clearly stated in the included guidelines.

The engineering lead ran a series of controlled experiments and found the pattern. When diagnostic criteria appeared in the first 15,000 tokens or the last 10,000 tokens of the context, the model cited them correctly 94 percent of the time. When the same criteria appeared between tokens 40,000 and 80,000—the middle of the context—citation accuracy dropped to 67 percent. The model was not ignoring the middle section entirely, but it was attending to it less reliably than the edges.

The team had assumed that context position did not matter as long as the information was present somewhere in the window. They were wrong. Transformer attention exhibits **position bias**. Models attend more strongly to information near the beginning and end of the context, and less strongly to information in the middle. This is not a bug in the model. It is an architectural property of how attention mechanisms distribute compute across long sequences.

They restructured their prompt architecture to place the most critical clinical guidelines at the end of the context, immediately before task instructions. They ordered guidelines by relevance to the current case rather than alphabetically. They added explicit markers for high-priority sections. Within two weeks, citation accuracy improved from 67 percent to 89 percent for middle-position content and from 94 percent to 97 percent for end-position content. The restructure cost zero dollars in API changes. It was pure architectural optimization based on understanding attention mechanics.

## Primacy and Recency Effects in Transformer Attention

Transformer models exhibit primacy and recency effects analogous to human memory patterns. Information near the beginning of the context receives stronger attention because it is processed first and influences the internal representations that follow. Information near the end of the context receives stronger attention because it is closest to the output generation phase. Information in the middle competes for attention with both earlier and later tokens and frequently loses that competition.

This happens because attention is a weighted sum over all context tokens, and those weights are learned during training on specific data distributions. During training, models see many examples where task-critical information appears at the beginning—system prompts, role definitions, constraints, instructions. They also see many examples where task-critical information appears at the end—user queries, final questions, task specifications. They see fewer examples where task-critical information is buried in the middle of a long document without special markers or emphasis. The learned attention patterns reflect this training distribution.

Research published in late 2025 quantified these effects across multiple models. Claude Opus 4 showed 23 percent higher attention weights for tokens in the first 5 percent and last 5 percent of contexts compared to middle tokens. GPT-4.5 Turbo showed 18 percent higher attention for edge tokens. Gemini Pro 2.0 showed 14 percent higher attention. The magnitude varies, but the pattern is consistent across architectures and training approaches.

You cannot eliminate primacy and recency effects through prompt engineering alone. They are baked into the model's learned weights from training on billions of documents. What you can do is design your prompt architecture to exploit these effects rather than fight them. You place task-critical information at the beginning or end of your context. You place background information, supplementary details, and low-priority material in the middle where lower attention is acceptable. You do not randomly order sections and hope the model figures out what matters.

The middle of the context is not worthless. The model can and does extract information from middle positions. But it does so less reliably, especially when the context exceeds 60,000 tokens and attention dilution compounds. For information that must be caught 95 percent of the time, middle placement is unacceptable. For background information that only occasionally matters, middle placement is fine.

## Where to Place Task Instructions

Task instructions define what the model is supposed to do with the context you have provided. They are the most critical part of your prompt because they determine whether the model even attempts the right task. You have two viable placement strategies: beginning or end. Both work, but they work for different reasons and in different contexts.

Placing instructions at the beginning exploits primacy. The model reads the instructions first, internalizes the task, and processes the rest of the context through the lens of that task. This works well when your instructions are short and the task is straightforward. If your prompt says "Summarize the following document" at the top, the model knows its job before it reads the document. As it processes the document, it is already filtering for summarization-relevant information.

Placing instructions at the end exploits recency. The model reads all the context first, then encounters the task instructions immediately before generating output. This works well when your task is complex, conditional, or requires synthesis across the entire context. If your prompt says "Based on all the case law provided above, identify which precedents apply to this dispute and explain why," the model benefits from having processed all the case law before it encounters the synthesis instruction. The full context is fresh in its attention distribution when it begins reasoning about the task.

Most production systems use end-placement for long-context prompts exceeding 40,000 tokens. The reason is that long contexts often include multiple documents, diverse data sources, or extensive reference materials. If you place instructions at the beginning, the model reads "analyze the contract for compliance issues," then processes 100,000 tokens of contract text and regulations, then starts generating output. By the time it generates, the instructions are 100,000 tokens away and weighted lower in attention. If you place instructions at the end, the model processes the contract and regulations, then encounters "analyze for compliance issues" immediately before output generation. The instructions are fresh and heavily weighted.

Beginning-placement works better for short contexts under 20,000 tokens where attention dilution is minimal. It also works better for tasks where the instruction fundamentally changes how the context should be processed. If your task is "Extract all dates mentioned in the following text," you want the model scanning for dates as it reads, not trying to infer what matters and then being told to extract dates at the end.

You experiment with both placements and measure which yields better task adherence. Task adherence is whether the model does what you asked—format compliance, instruction following, constraint satisfaction. If you asked for a bulleted list and got a paragraph, adherence is low. If you asked for citations and got vague references, adherence is low. Run A/B tests with 100 representative examples and measure adherence rates for each placement strategy.

## Ordering Reference Documents for Optimal Attention

When you include multiple documents in a long-context prompt, the order matters significantly. Documents placed at the beginning or end receive more attention than documents placed in the middle. You exploit this by ordering documents based on expected importance to the task.

You place the most task-critical document last, immediately before your task instructions. If your task is to analyze a contract dispute and you have the contract, case law, and internal notes, you place the contract last because it is the primary source that all analysis hinges on. The model processes case law and notes first as background, then reads the contract with that background knowledge fresh in memory, then encounters your analysis task. This ordering maximizes attention on the document that matters most.

You place high-authority sources near the end of the context. If you are including regulatory guidelines, industry standards, and blog posts, you place the regulatory guidelines near the end because they are authoritative and should be weighted heavily. The model should prioritize regulatory guidance over blog opinions, and end-placement biases attention in that direction. If you place blog posts last, the model might give them undue weight simply because of position bias.

You place low-priority or background documents early in the context. If you are including a glossary, background primer, or tangentially related reference, you place it in the first 20 percent of the context. The model can access it if needed for terminology clarification, but it does not dominate attention during task execution. Background material serves a supporting role and should be positioned accordingly.

You do not interleave high-priority and low-priority documents in alternating fashion. If you alternate contract sections with background notes, the model's attention gets diluted across the entire context rather than concentrated where it matters. You batch similar documents together and order the batches by priority. All background material early, all primary sources late. This creates clear attention zones.

A litigation support platform tested document ordering in September 2025. They processed the same case materials in three orderings: random, alphabetical by document title, and priority-based with key documents last. Random ordering achieved 79 percent accuracy on relevance judgments. Alphabetical achieved 81 percent. Priority-based achieved 88 percent. The 9-percentage-point gain from priority ordering cost nothing but restructuring effort.

Document ordering interacts with document length. If you place a 40,000-token low-priority document early and a 5,000-token high-priority document late, the low-priority document dominates context by sheer volume. The model spends more tokens processing background than processing critical content. When possible, summarize long low-priority documents and include full text only for high-priority documents. This aligns token allocation with information value.

## Structuring Context with Attention Anchors

Attention anchors are structural markers that help the model navigate long contexts and maintain focus on key sections. Without anchors, the model treats the context as an undifferentiated blob of text where everything blends together. With anchors, the model can reference specific sections, revisit key points, and avoid lost-in-the-middle errors where critical information gets buried.

You use document headers as anchors that create clear boundaries. Instead of concatenating documents without separators, you mark each document with distinctive delimiters: "=== Document 1: Employment Agreement ===", "=== Document 2: Severance Policy ===". The model can then generate output like "According to Document 2, severance is calculated as..." instead of vague references like "the policy states." Headers must be distinctive enough that the model cannot confuse them with document content.

You use section markers within long documents to create internal navigation points. If you are including a 40,000-token regulatory code, you preserve or add section markers: "Section 4.2.1: Data Retention Requirements," "Section 4.2.2: Deletion Timelines." The model can cite specific sections instead of gesturing vaguely at "the regulations." Section markers also help the model skip irrelevant sections when scanning for specific information.

You use numbered lists for multi-part information that the model needs to reference individually. If you are providing five policy documents, you number them one through five. If you are listing ten requirements, you number them. Numbered lists create clear reference points that appear in model outputs. The model can say "Requirement 3 is violated" instead of "one of the requirements" which forces readers to search the context.

You use metadata tags to signal document type, priority, and authority level. "Primary Source: IRS Code Section 401(k)," "Supporting Reference: Company Retirement Policy." Tags guide the model's weighting of information when sources conflict. Primary sources should carry more weight than supporting references, and explicit tags make this priority clear. Without tags, the model treats all documents as equally authoritative.

You avoid deeply nested hierarchies that make navigation difficult. If your document has sections, subsections, sub-subsections, and sub-sub-subsections, the model loses track of hierarchical position. A reference to "Section 3.2.4.1" requires holding four levels of hierarchy in memory. Flatten the hierarchy to two or three levels maximum. Use "Section 3.2: Data Privacy, subsection Retention Periods" rather than deep numeric nesting.

A financial services company added structured anchors to their compliance analysis prompts in October 2025. They marked documents with headers, numbered policy sections, and tagged authority levels. Citation specificity improved from 68 percent to 91 percent. Auditors could verify model outputs directly against source documents without searching for vague references. The restructuring took two engineer-weeks and produced measurable quality gains.

## Managing Attention Distribution Across Long Contexts

Attention distribution is how the model allocates its compute budget across the context when generating each output token. In a short context, the model can afford to attend to every token with meaningful weight. In a long context, attention is spread thin, and you must help the model concentrate attention where it matters most.

You use redundancy strategically for critical facts. If a fact is essential to the task, you state it twice—once near the beginning as context-setting, once near the end as a reminder. This ensures the model sees it in both high-attention zones. You do not use redundancy for low-priority information because it wastes tokens without adding value. Redundancy is a tool for emphasizing critical facts, not for padding context.

You use explicit focus directives that tell the model where to concentrate attention. Instead of hoping the model notices that Section 4.2 is important, you write "Section 4.2 is the most relevant section for this analysis. Pay particular attention to the retention period specifications." This does not override attention mechanics, but it provides a heuristic the model can use when weighting information during generation. Focus directives work best when placed near the content they reference.

You compress low-priority sections to reduce token allocation. If you need to include a 30,000-token background document for completeness but it is not central to the task, you include a 3,000-token summary instead. The model gets the gist without spending 27,000 tokens of attention on details that do not matter. You include full text only for high-priority sections where details matter.

You front-load critical facts in each section to exploit local primacy effects. If Section 4.2 is 5,000 tokens long and the critical fact is in paragraph 12, you add a preamble: "Section 4.2 establishes data retention requirements. Key point: retention period is seven years. Details follow." The model sees the key point at the start of the section, where local attention is stronger, before processing the detailed explanation.

You use summary sections at the end of long documents to concentrate key facts. After including a 30,000-token regulatory document, you add a 1,000-token summary: "Key points from the above regulation: retention period is seven years, deletion must be irreversible, third-party processors must certify compliance." This summary appears near the end of the context where attention is strong and provides a compressed reference the model can use during generation.

## Position-Dependent Prompt Templates

You design prompt templates that adapt to the length and complexity of the context. A template that works well for 10,000-token contexts might fail for 100,000-token contexts because attention distribution changes with scale. Position-dependent templates optimize structure for the context length you are working with.

For short contexts under 20,000 tokens, you use a simple structure: instructions at the beginning, context, task summary. The model can attend to everything effectively, so you do not need sophisticated placement strategies. Simple and straightforward wins at this scale.

For medium contexts from 20,000 to 60,000 tokens, you use: brief instructions, low-priority context, high-priority context, detailed task instructions. You order context by priority and place detailed task instructions at the end to exploit recency. The brief instructions at the beginning prime the model for what is coming. The detailed instructions at the end guide generation.

For long contexts from 60,000 to 150,000 tokens, you use: context map, low-priority background, medium-priority references, high-priority primary sources, detailed task instructions with focus directives. You add a context map at the beginning so the model knows what it is working with. You place primary sources and task instructions at the end where attention is strongest. You include focus directives that explicitly guide attention allocation.

For very long contexts exceeding 150,000 tokens, you add explicit navigation aids and attention management: "This context includes six documents. Documents 1-3 provide background and are included for reference only. Documents 4-6 are primary sources for your analysis. Focus your attention on Documents 5 and 6, which contain the specific clauses relevant to this dispute." You cannot rely on the model to infer priority from position alone at extreme scale. You must state priority explicitly.

A legal research platform implemented position-dependent templates in November 2025. They measured context length per query and selected template structures based on length. Queries under 30,000 tokens used simple templates. Queries over 100,000 tokens used structured templates with maps and focus directives. Error rates dropped by 14 percentage points for long-context queries with no change for short-context queries. The adaptation cost was template complexity but the quality gain was substantial.

Template selection can be automated based on measured context length. Your assembly system counts tokens in the context you are building and selects the appropriate template structure. This removes manual decision-making and ensures consistent application of position-dependent patterns across your system.

## Handling Multi-Document Contexts with Cross-References

When your context includes multiple documents that reference each other, you need to help the model track relationships and resolve references. If Document A refers to "the provisions outlined in Document B," the model needs to know which document is which and be able to locate the referenced provisions.

You label documents consistently throughout the context. If you introduce a document as "Document 1: Merger Agreement," you refer to it as "Document 1" or "the Merger Agreement" throughout. You do not switch to "the contract" or "the deal terms" mid-prompt. Inconsistent naming forces the model to infer coreference, which wastes attention and introduces ambiguity.

You resolve ambiguous references before including documents. If Document A says "see Section 4.2" and both Document A and Document B have a Section 4.2, you clarify: "Document A, Section 4.2" and "Document B, Section 4.2" when referring to them in instructions. The model cannot reliably disambiguate which section is referenced when multiple documents share section numbering.

You provide a reference map for complex multi-document contexts. If you have ten documents with cross-references, you include a brief map at the beginning: "Document 1 defines terms used in Documents 2-4. Document 5 supersedes conflicting provisions in Documents 1-4. Document 10 is an amendment to Document 5." This helps the model navigate relationships without re-reading everything to infer the structure.

You inline critical cross-references where they matter. If Document B is most important and it references Document A Section 3, you include a note in your instructions: "When analyzing Document B clause 7, note that it references Document A Section 3 regarding liability limits." This explicit connection helps the model find and integrate the referenced content.

A contract analysis platform handling merger documents implemented cross-reference management in October 2025. They labeled all documents consistently, provided reference maps, and inlined critical cross-references in task instructions. Model accuracy on cross-document questions improved from 71 percent to 87 percent. The improvement came entirely from better context structure, not from model changes.

## Attention Heatmaps and Diagnostic Testing

You cannot directly observe attention distributions in production API calls—model providers do not expose attention weights to users—but you can infer attention patterns through diagnostic testing. You create synthetic test cases where you place the same critical fact at different positions in the context and measure whether the model retrieves it.

You build a test set with 30 to 50 examples where the answer-critical fact appears in the first 10 percent of the context, the middle 50 percent, and the last 10 percent. You run your prompt template against all examples and measure retrieval accuracy by position. If accuracy is 95 percent for first 10 percent and last 10 percent but 70 percent for middle 50 percent, you have confirmed position bias and can quantify its magnitude.

You use these diagnostics to tune your template structure. If critical facts often appear in the middle of source documents and you are seeing retrieval failures, you restructure your template to move those documents toward the end, or you add redundancy to ensure critical facts also appear near the end where attention is stronger.

You run position diagnostics every time you update your prompt template or switch model versions. Attention patterns can change between model releases. A template that worked well with Claude Opus 4.0 might show different attention characteristics with Claude Opus 4.5. GPT-4.5 might have different position bias than GPT-4.0. You validate that your template still works after each model update.

You track diagnostic results over time to detect attention pattern drift. If your September diagnostics showed 94 percent retrieval accuracy in middle positions but December diagnostics show 82 percent, something changed. Maybe your context length increased. Maybe your document structure changed. Maybe the model updated. Trend tracking surfaces these changes before they cause production incidents.

A healthcare AI platform runs position diagnostics weekly on 50 synthetic test cases. They track retrieval accuracy by position bucket and alert when accuracy drops below thresholds. This caught a model update in November 2025 that changed attention patterns. They adjusted their template structure within three days, before the change affected production quality.

## Cost and Latency Trade-offs in Prompt Ordering

Prompt ordering affects latency but not cost at the token level. Reordering sections does not change total token count, so it does not change cost per request. But reordering can affect how quickly the model finds the information it needs, which can affect time-to-first-token and perceived responsiveness.

If you place task instructions at the beginning, the model knows what it is looking for as it processes the context. This can reduce time spent on irrelevant sections because the model can filter during input processing. If you place task instructions at the end, the model processes the entire context before it knows the task, which might increase latency slightly because it cannot filter during processing.

In practice, latency differences from ordering are small—typically a few hundred milliseconds—and are dominated by total context length. You optimize for quality and attention distribution first, latency second. Getting the right answer matters more than saving half a second.

You do consider ordering when you use prompt caching. If you cache a 50,000-token regulatory document and place it at the beginning of your context, you can reuse the cache across requests with different task instructions at the end. If you place the task instructions at the beginning, you cannot cache them because they change per request. Ordering affects cacheability, which affects cost for high-reuse workloads.

A compliance analysis platform caches regulatory documents at the beginning of contexts and varies case details and task instructions at the end. Cache hit rate is 78 percent, reducing costs by 62 percent for cached requests. The ordering choice enabled caching, which provided far larger cost savings than any latency optimization could achieve.

## Multi-Stage Attention Strategies for Ultra-Long Contexts

When your context exceeds 150,000 tokens, single-pass attention becomes unreliable even with optimal ordering. The model cannot maintain focus across that much information in one generation pass. You use multi-stage strategies that break the task into passes with different attention requirements.

You split the task into a two-stage pipeline where stage one has a simple extraction task over the full long context and stage two has a complex reasoning task over a shorter focused context. Stage one: "Read all documents and identify the three most relevant sections for this analysis." Stage two: "Given the three sections identified in stage one, perform the detailed analysis." The first stage uses the full long context but has a simple extraction task with low attention demands. The second stage uses a shorter, focused context and performs the complex reasoning task with high attention demands.

You use a map-reduce pattern for tasks that decompose naturally. You split the context into chunks, run a simple analysis task on each chunk in parallel, then aggregate the results in a final pass. This works well for summarization, extraction, and classification tasks where the output is compositional. It does not work well for tasks that require global coherence or cross-referencing that cannot be decomposed.

You use iterative refinement where you make an initial pass with the full context to get a draft answer, then make a second pass with a focused context that includes only the sections referenced in the draft. This improves citation accuracy and reduces lost-in-the-middle errors. The first pass identifies what matters. The second pass gets it right.

A document analysis platform processing 200,000-token regulatory filings uses two-stage extraction. Stage one identifies relevant sections with a simple "list all sections that discuss X" task. Stage two performs detailed analysis on only those sections. Accuracy is 91 percent versus 76 percent for single-pass full-context analysis. The two-stage approach costs slightly more in API calls but delivers substantially better quality.

## Model-Specific Attention Characteristics

Different models exhibit different attention patterns, and you must tune your templates for the specific model you are using. Claude models tend to have strong recency bias and perform better with task instructions at the end. GPT-4 models tend to balance primacy and recency more evenly. Gemini 2 models have been observed to have weaker lost-in-the-middle effects in some benchmarks, though results vary by task.

You test your prompt template on the specific model you are using. You do not assume that a template optimized for Claude will work equally well on GPT-4. You run position diagnostics, measure retrieval accuracy across context positions, and tune ordering for your production model. What works for one model might be suboptimal for another.

You track model updates and re-validate templates when providers release new versions. When Anthropic releases Claude 4.1, or OpenAI releases GPT-4.6, or Google releases Gemini 2.5, you re-run your diagnostics. Attention characteristics can change between versions. A prompt template that worked perfectly in November 2025 might degrade in February 2026 after a model update if attention patterns shifted.

You consider model-specific features when available. Claude's "extended thinking" feature might affect attention distribution. GPT-4's structured output mode might interact with position bias. Gemini's multimodal attention might differ from text-only attention. You test these features and adjust templates accordingly.

A multi-model platform that supports Claude, GPT-4, and Gemini maintains separate template structures for each model. Claude templates place instructions at the end and use strong focus directives. GPT-4 templates balance beginning and end placement. Gemini templates use less aggressive position optimization. Per-model tuning improved average accuracy by 6 percentage points compared to one-template-fits-all.

## Documentation and Template Versioning

You document why your prompt template uses its specific ordering and structure. Future engineers need to know whether you placed instructions at the end because of recency effects, cacheability, or a legacy decision that is no longer valid. Without documentation, they might "optimize" the template in ways that break attention distribution.

You version your templates alongside attention diagnostics. Your template repo includes not just the template code but also the test results that show it works. When someone proposes a reordering, they run the diagnostics first to validate that the change improves or maintains quality. You do not merge template changes without diagnostic validation.

You avoid cargo-cult ordering where you copy template structures from blog posts or documentation without understanding why they work. If you do not know why your instructions are at the end, you do not know when that placement is wrong. You make ordering decisions based on measurement of your specific use case, not mimicry of generic examples.

You document model-specific considerations when templates are tuned for particular models. If your template works well on Claude but has not been tested on GPT-4, document that limitation. If your template assumes specific model behaviors that might change in future versions, document those assumptions. This prevents surprises when models update or when you switch providers.

The next subchapter examines needle-in-a-haystack and lost-in-the-middle mitigation strategies for contexts where critical information can appear anywhere in a long sequence and you need reliable extraction regardless of position.
