# 7.2 — Tool Schema Design: Names, Descriptions, and Parameter Types

A healthcare AI startup spent six months building a diagnostic assistant with 30 carefully crafted tools for symptom checking, medication lookup, and appointment scheduling. When they launched in September 2024, they discovered that the model called the wrong tool 35% of the time, consistently confusing "check_drug_interactions" with "check_drug_availability" and "schedule_appointment" with "reschedule_appointment". The tools worked perfectly—when called. The problem was that tool names and descriptions were so similar that the model couldn't reliably distinguish between them. After three months of user complaints and $150,000 in emergency redesign work, they learned that schema design matters more than schema functionality.

Tool schema design is where intention meets execution. You can build the most sophisticated tool infrastructure in the world, but if the model doesn't understand what each tool does or when to use it, your system is useless. The schema is the interface between human intent and machine action, and like all interfaces, clarity trumps cleverness.

Most developers treat schema design as an afterthought—a quick documentation step before shipping. But the schema isn't documentation. It's the instruction manual the model uses to understand your system's capabilities. A poorly designed schema doesn't just reduce performance—it fundamentally breaks the model's ability to help users.

## Naming Tools for Model Understanding

Tool names are identifiers, but they're also semantic signals. The model uses tool names to build mental models of what each tool does. A good tool name is self-documenting—it clearly indicates purpose without ambiguity. A bad tool name requires the model to parse the description carefully, and under cognitive load, the model defaults to assumptions based on the name alone.

Follow function naming conventions from programming: verb-noun structure, lowercase with underscores or camelCase. "get_user_profile" is clear. "profile_user_get" is confusing. "fetch_current_authenticated_user_profile_information" is verbose. Strike a balance between clarity and brevity.

Avoid names that differ only in subtle ways. "create_order" and "create_purchase" sound different to humans but are semantically similar to models. If your domain has multiple concepts that could reasonably be called by similar names, use distinct verbs or add qualifying adjectives. "initiate_order" and "record_purchase" are more distinguishable.

Consistency matters. If you name one tool "get_user_data", don't name another "fetch_product_info" and a third "retrieve_order_details". Pick a verb convention—get, fetch, or retrieve—and stick with it. The model learns patterns from your naming, and inconsistency breaks those patterns.

Special-purpose tools benefit from domain-specific prefixes. In a system with both user-facing and admin tools, prefix admin tools with "admin_". This creates a clear namespace and helps the model understand access levels. "admin_delete_user" is obviously different from "deactivate_account", even though both might remove a user.

## Writing Effective Tool Descriptions

The tool description is your opportunity to explain when and how the tool should be used. This isn't a technical specification—it's persuasive writing aimed at an AI model. Your goal is to make the tool's purpose so clear that the model chooses it confidently when appropriate and ignores it when not.

Start with a one-sentence summary of what the tool does. "Retrieves the current user's account balance and recent transaction history." This gives the model an instant understanding of the tool's primary function. Follow with details about when to use it, what information it returns, and any important constraints.

Specify the tool's scope explicitly. If a tool only works for certain user types, say so: "This tool is only available for premium subscribers." If it requires prior authentication, mention it: "User must be logged in to use this tool." The model can't infer system constraints—you must state them.

Include examples of appropriate use cases. "Use this tool when the user asks about their spending, wants to verify a transaction, or needs to check if a payment went through." Concrete examples anchor the model's understanding in real scenarios. Without examples, the model must guess at appropriate contexts.

Warn about inappropriate uses. "Do not use this tool for other users' balances or for historical data older than 90 days." Negative examples are as important as positive ones. They help the model avoid misuse by clearly marking boundaries.

**Parameter descriptions** require the same care. Each parameter should have a clear description explaining what it represents, what format it expects, and what valid values look like. "user_id: The unique identifier for the user. Must be a numeric ID from the authenticated user's account" is better than "user_id: User ID number".

## Parameter Types and Constraints

JSON Schema provides rich type definitions, but not all of them work equally well with language models. The model understands basic types—string, number, boolean, array, object—intuitively. Complex types like anyOf, oneOf, and allOf often confuse the model and lead to invalid calls.

Use the simplest type that captures your requirements. If a parameter is text, make it a string. If it's a count, make it an integer. Don't use a string with a pattern regex when an enum would suffice. The model is better at choosing from a list than matching a pattern.

**Enums** are powerful but require careful design. List all valid values explicitly: "status: Enum of ['pending', 'approved', 'rejected']". Keep enum lists short—more than 10 options often confuses the model. If you have many options, consider whether you actually need an enum or whether a string with good description would work better.

For numeric parameters, specify ranges. "age: Integer between 0 and 120" prevents the model from providing nonsensical values. "amount: Number greater than 0" rules out negative payments. The model can't infer real-world constraints—you must encode them in the schema.

String parameters benefit from format hints. "date: String in YYYY-MM-DD format" or "email: Valid email address format" guides the model toward correct formatting. While the model generally understands common formats, explicit specification reduces errors.

## Required vs Optional Parameters

Marking parameters as required or optional is a critical design decision. Required parameters must be provided for the tool to work. Optional parameters enable enhanced functionality but aren't necessary for basic operation. Getting this distinction wrong causes unnecessary tool call failures or missed opportunities for better results.

Only mark parameters as required if the tool genuinely cannot function without them. If you can provide a sensible default value, make the parameter optional and document the default. "limit: Optional integer, defaults to 10. Maximum number of results to return" tells the model it can omit this parameter.

Be cautious with over-requiring parameters. Each required parameter increases the chance of a failed tool call. If the model doesn't have the information needed for a required parameter, it must either ask the user (disrupting flow) or skip the tool entirely (missing functionality). Required parameters should be information the model can reliably obtain from context.

Optional parameters should enhance, not change, the tool's behavior. An optional "sort_order" parameter that changes results from ascending to descending is fine. An optional "mode" parameter that completely changes what the tool does is poor design—split that into separate tools.

Document the impact of omitting optional parameters. "If start_date is not provided, defaults to 30 days ago." This helps the model decide whether to include the parameter. Without this information, the model might always include it (wasting effort) or never include it (missing optimization opportunities).

## Description Quality Impact on Selection

Tool selection failures almost always trace back to description quality. When the model chooses the wrong tool, it's because the descriptions didn't make the right choice obvious. The model is reasoning based on text—if your text is ambiguous, the model's reasoning will be uncertain.

Test descriptions by asking: "If I only read this description, would I know exactly when to use this tool and when not to?" If the answer is no, revise. A description that makes sense to you, the tool's creator, might be unclear to someone encountering it fresh. The model encounters it fresh every time.

Compare descriptions across similar tools. Do they clearly differentiate the tools' purposes? If you have "search_products" and "filter_products", the descriptions must explain the semantic difference. "search_products: Finds products by keyword, name, or category" versus "filter_products: Narrows product list by price, brand, or availability" makes the distinction clear.

Avoid jargon and internal terminology unless the model has been explicitly trained on your domain. "Initiates the ETL pipeline for customer data" assumes the model knows what ETL means. "Extracts customer data from the database, transforms it for analysis, and loads it into the reporting system" is clearer.

Read descriptions aloud. If they sound unnatural or confusing spoken, they'll confuse the model. Natural language is the model's native interface—leverage that by writing descriptions in clear, conversational language that explains purpose plainly.

## Schema Anti-Patterns

Certain schema patterns consistently cause problems in production. The "Swiss Army knife" tool tries to do everything through mode parameters or action flags. "manage_user" with parameters "action: ['create', 'update', 'delete', 'list']" forces the model to understand multiple tools as one. Split these into separate tools—"create_user", "update_user", etc.

The "mystery parameter" anti-pattern provides minimal description: "options: Object containing configuration". The model has no idea what to put in options. Either fully specify the object schema with described fields or provide a detailed description of expected structure with examples.

**Overly complex nested objects** break model reasoning. Three levels of nesting is usually the maximum the model handles reliably. If your tool requires deeper structures, flatten them or split into multiple tools. A parameter that's an array of objects, each containing arrays of objects, each containing configuration objects, is asking for trouble.

The "guess the format" anti-pattern leaves format ambiguous. "date: String representing a date" could mean ISO 8601, Unix timestamp, localized format, or natural language. The model will guess, and guesses vary. Specify exact format expectations.

Tools that require "magic knowledge" assume the model knows information it can't access. A parameter "user_id: Current user's ID" in a system where user IDs aren't stored in context means the model can't possibly call this tool correctly. Either provide the required information in context or redesign the tool to work without it.

## Testing Schema Clarity

Test your schemas before deployment by having the model choose between tools in ambiguous scenarios. Create test cases where multiple tools seem plausible and see which one the model selects. If it chooses wrong, analyze why: was the description unclear, the naming confusing, or the use case ambiguous?

Generate synthetic user queries that should map to each tool. Run these through your system and verify the model selects the correct tool. A query like "What's my account balance?" should trigger the balance tool, not the profile tool or the transaction history tool. If it doesn't, your schemas need revision.

Test edge cases where tool selection is genuinely ambiguous. If two tools could reasonably apply, which one should the model prefer? Your schemas should encode this priority, either through description clarity or by design. "Use this tool when you need detailed information; use the other tool when you need a quick summary" helps the model decide.

Monitor production tool selection patterns. If users frequently get responses like "I don't have a tool for that" when tools exist, your descriptions aren't making capabilities clear. If certain tools are never called despite being relevant, they're either poorly described or poorly named.

A/B test schema variations in development. Write two versions of a description and see which leads to more accurate tool selection. Small wording changes can have significant impacts on model behavior. "Returns user data" versus "Retrieves the current user's profile information, including name, email, and account settings" might seem similar, but the second is far more effective.

## Parameter Type Edge Cases

Real-world data is messy, and parameter types must handle that messiness. A "phone_number" parameter might receive international formats, extensions, or invalid input. Your schema should specify expected format, but your validation must handle variations gracefully.

Date and time parameters are notorious for issues. The model might provide "tomorrow", "2026-01-30", "January 30th", or "30/01/2026" depending on context. If you require ISO 8601 format, say so explicitly and include a formatting example in the description. Consider accepting multiple formats in validation and normalizing them internally.

Identifiers (user IDs, product codes, transaction numbers) should specify format. "product_id: String in format 'PROD-XXXXX' where X is a digit" prevents the model from providing raw numbers or wrong format strings. If multiple formats are valid, list them all.

Boolean parameters seem simple but cause confusion when the model encounters ambiguous scenarios. "Include archived items?" isn't naturally boolean—users say "yes", "show them", "maybe", or "only archived". Consider using enums for three-state scenarios: ['include', 'exclude', 'only'] is clearer than boolean with separate "only_archived" flag.

Array parameters need clarity about empty arrays. Is an empty array valid? Does it mean "none" or is it an error? "categories: Array of category IDs. Must contain at least one ID" versus "categories: Optional array of category IDs. Empty array returns all categories" makes the semantics clear.

## Cultural and Linguistic Considerations

Tool names and descriptions are English-centric in most systems, but models understand multiple languages. If your product serves non-English users, consider whether the model should translate tool names or preserve them. Generally, preserve English tool names for consistency but write descriptions that translate well.

Avoid idioms, cultural references, or ambiguous phrasing in descriptions. "This tool hits the database" uses "hits" idiomatically, which might confuse non-English models or translation layers. "This tool queries the database" is clearer. Technical precision aids comprehension across languages and model variants.

Be mindful of terminology that varies across English dialects. "Postcode" versus "zip code", "mobile" versus "cell phone", "lorry" versus "truck"—if your system serves international users, choose terminology that matches your primary audience or use both in descriptions.

Consider parameter naming conventions from different programming cultures. Some prefer snake_case, others camelCase, still others kebab-case. Pick one and be consistent. The model adapts to any convention, but mixing conventions within a single system causes confusion.

Units and formats vary globally. If a parameter represents currency, specify which currency or make currency a separate parameter. If it's a measurement, specify units. "distance: Number representing kilometers" is clearer than "distance: Number" in a global context.

## Evolving Schemas Over Time

Schemas aren't static—they evolve as your product grows. Adding new parameters, deprecating old ones, and refining descriptions are normal. But schema changes have implications for model behavior and conversation history that includes old tool calls.

When adding optional parameters, ensure they're truly optional. Old model behaviors that don't provide the new parameter should still work. If a new parameter changes tool behavior significantly, consider creating a new tool instead of modifying the existing one.

Deprecating parameters is delicate. The model might continue calling tools with deprecated parameters based on learned patterns. Either continue accepting the parameter silently or return a clear error that guides the model toward the new schema. Silently ignoring deprecated parameters can cause confusion.

Renaming tools or parameters breaks existing conversations and learned model behaviors. If you must rename, maintain backward compatibility temporarily—accept both old and new names, log warnings about old usage, and gradually phase out the old version. Abrupt changes confuse the model and disrupt user experiences.

Version your schemas if you support multiple model versions or have long-running conversations. Schema v1 and v2 might coexist temporarily during migrations. Be explicit about which version is active, and ensure your system handles tool calls from both versions correctly.

Document schema changes for your team and for observability. When tool selection behaviors shift after a schema update, you need to know what changed. Track schema versions alongside tool call logs so you can correlate behavioral changes with specific schema modifications.

## Multi-Model Schema Compatibility

Different models interpret schemas slightly differently. GPT-4 might reliably handle complex nested objects while Claude prefers flatter structures. Gemini might excel at interpreting natural language parameter descriptions while other models need more rigid specifications. Design schemas that work across model families if you plan to support multiple models.

Test your schemas on all target models. A schema that works perfectly with one model might fail consistently with another. Identify patterns: does one model struggle with certain parameter types, miss optional parameters, or misinterpret descriptions? Adjust schemas to accommodate the lowest common denominator while maintaining clarity.

Consider model-specific schema variants if necessary. You can provide GPT-4 with richer schemas and Claude with simplified versions if testing shows clear differences in capability. This adds complexity to your codebase but improves reliability across models.

Document model-specific quirks for your team. "Claude tends to omit optional parameters unless the description emphasizes their value" or "GPT-4 sometimes misinterprets date parameters as timestamps" helps developers design better schemas and debug issues faster.

Monitor tool calling behavior across models in production. If one model consistently underperforms, investigate whether schema design is the culprit. Different models might need different description styles, parameter structures, or naming conventions to achieve similar performance.

## Schema as Product Interface

Your tool schema is a product interface as much as your UI or API. Users don't see it directly, but it determines what the AI can do for them. A well-designed schema enables capabilities users value. A poorly designed schema makes the AI feel unreliable or limited, regardless of underlying tool quality.

Involve product managers in schema design, not just engineers. The question "What should this tool be called and how should we describe it?" is a product question. Engineers know how tools work; product managers know how users think about them. Bridge this gap for effective schemas.

Review schemas from the user's perspective. When a user asks to "check if they can afford something", which tool should be called? The schema should make this obvious to the model by using language that matches how users think and speak. User mental models should inform schema design.

Iterate on schemas based on real usage patterns. If users consistently ask for something your tools can do but the model doesn't call the right tool, the schema isn't aligning with user language. Adjust descriptions to incorporate common phrasing from actual user queries.

Tool schema design—names, descriptions, parameter types, and constraints—is where you encode your product's capabilities for the model to understand and use. Get this right, and your AI feels intelligent and capable. Get it wrong, and even the most sophisticated tools become useless. Every word in your schema is a choice that shapes behavior, and those choices compound into overall system reliability.
