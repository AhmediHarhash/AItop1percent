# 3.5 — Document-Grounded Prompting for Long Inputs

A Series C legal-services startup faced a potential breach-of-contract lawsuit in December 2025 after their AI contract analysis tool advised a mid-market client that a non-compete clause was unenforceable under state law. The clause was in fact enforceable, and the client had relied on the AI's analysis to hire a competitor's employee. The competitor sued. The startup's head of legal reviewed the AI output logs and discovered the problem: the model had generated a confident, detailed legal analysis that cited three court cases and two statutes, but none of those citations appeared in the documents the system had provided as context. The model had hallucinated the legal reasoning.

The nine-person ML team had designed their system to include relevant case law and statutes in the prompt—often 80,000 to 120,000 tokens of legal text—and ask the model to analyze the non-compete clause. Their prompt said "analyze whether this clause is enforceable," but it did not explicitly instruct the model to ground its analysis in the provided documents. The model had generated answers that sounded authoritative and were formatted like legal analysis, but the reasoning came from the model's training data, not the context. When the provided case law did not support the conclusion the model wanted to reach, it invented case law that did.

The startup settled the lawsuit for $340,000 and spent another $120,000 rebuilding their system to enforce document grounding. They learned what many teams learn too late: **citation is not optional for high-stakes document analysis**. If the model's output is not explicitly and verifiably grounded in the provided documents, you have a hallucination risk that scales with context length and task complexity.

## Why Models Hallucinate Despite Having Source Documents

Models hallucinate because generation is a probabilistic process that blends information from the context with information from the model's learned parameters. When you include a 100,000-token legal document in your context and ask the model to analyze it, the model does not "forget" everything it learned during training. It still has access to patterns, facts, and reasoning templates from its training data. It will mix context information with training information unless you explicitly instruct it not to.

This mixing happens because the model is optimizing for plausibility, not truth. If the provided documents do not contain a clear answer to your question, the model will generate a plausible-sounding answer using whatever information it has—context, training data, or confabulation. The model does not distinguish between "facts I read in the provided documents" and "facts I learned during training" unless you force that distinction.

Hallucination risk increases with context length because longer contexts are noisier. The model has to attend to more tokens, some of which are irrelevant or contradictory. When attention is diffuse, the model is more likely to fall back on training data to fill gaps.

Hallucination risk increases with task difficulty because difficult tasks invite reasoning shortcuts. If the model cannot find a clear answer in the provided documents, it will reason by analogy to similar cases from training data. If you do not catch this, the output appears authoritative but is not grounded in your context.

## Explicit Grounding Instructions in Prompts

You eliminate hallucination by making grounding a hard constraint in your prompt. You do not say "analyze this document." You say "analyze this document using only the information provided in the context. Do not use external knowledge. If the answer is not in the provided documents, say so."

This instruction shifts the model's generation strategy from "produce a plausible answer" to "produce an answer that can be verified against the context." The model still has access to its training data, but the instruction creates a strong prior against using it.

You reinforce grounding with citation requirements. You write "for every claim you make, cite the specific document and section where you found the supporting information." Citation forces the model to link every statement to a location in the context. If the model cannot cite a source, it cannot make the claim.

You specify what counts as a valid citation. You write "a valid citation includes the document name and section number, such as 'Document 2, Section 4.1.' Vague references like 'the policy states' are not valid." This prevents the model from generating pseudo-citations that sound specific but are unverifiable.

You handle edge cases where the answer is not in the documents. You write "if the provided documents do not contain enough information to answer the question, respond with 'Insufficient information in provided documents' and explain what additional information would be needed." This gives the model an escape hatch that does not involve hallucination.

## Citation Format and Verification

Citations are only useful if they are machine-verifiable. A citation like "according to the contract" is not verifiable because you cannot check it automatically. A citation like "Document 3, Section 7.2, Paragraph 4" is verifiable. You design citation formats that your verification layer can parse.

You use structured citation formats. You instruct the model to output citations in a specific format: "[Doc:3, Sec:7.2, Para:4]." You parse this format in post-processing and verify that the cited location exists and contains information consistent with the claim. If verification fails, you flag the output as potentially hallucinated.

You use inline citations. Instead of asking the model to list citations at the end of its response, you ask it to cite claims inline: "The non-compete period is two years [Doc:1, Sec:8.1]." Inline citations make it easier to match claims to sources during verification.

You verify citations automatically. Your system extracts the cited document, section, and paragraph, retrieves the text at that location, and checks whether the text supports the model's claim. This can be done with exact string matching for simple extractions or with a second model call for complex reasoning tasks.

You track citation accuracy as a quality metric. You measure what percentage of model outputs include citations, what percentage of citations are valid (the cited location exists), and what percentage of citations are faithful (the cited text supports the claim). If citation accuracy drops below 95%, you investigate.

## Preventing Hallucination Beyond Source Material

Grounding instructions and citation requirements reduce hallucination, but they do not eliminate it. The model can still generate claims that seem plausible, cite a vaguely related section, and fool your verification layer if the verification is shallow.

You use negative instructions. You write "do not infer, extrapolate, or assume facts that are not explicitly stated in the provided documents." This tells the model that reasoning beyond the text is not allowed. If the document says "the termination period is 30 days," the model cannot say "this implies a 60-day notice period for senior employees." The document does not say that.

You ask the model to distinguish between explicit statements and inferences. You write "for each claim, indicate whether it is (a) explicitly stated in the documents, (b) a direct logical inference from the documents, or (c) an assumption not supported by the documents." This forces the model to reflect on the grounding of each claim.

You use a two-model approach. The first model generates an answer with citations. The second model verifies the answer by checking each claim against the cited source. If the verification model finds discrepancies, you flag the output for human review. This catches hallucinations that pass simple citation checks.

## Document Reference Patterns for Multi-Document Contexts

When your context includes multiple documents, you need reference patterns that help the model distinguish between sources and avoid conflating information.

You label documents clearly and consistently. If you include five policy documents, you label them "Document A: Paid Leave Policy," "Document B: Remote Work Policy," and so on. You refer to them by these labels throughout. You do not switch to "the leave policy" mid-prompt because that introduces ambiguity.

You include a document index at the beginning of your prompt. You write "This context includes the following documents: Document A (Paid Leave Policy, 3,200 tokens), Document B (Remote Work Policy, 2,800 tokens)." The index helps the model track which documents are available and roughly how long each is.

You ask the model to reference documents by name in its output. You write "when citing information, always include the document name, such as 'Document A, Section 3.2.' Do not use generic references like 'the policy.'" This prevents ambiguous citations.

You handle contradictions between documents explicitly. You write "if documents contradict each other, note the contradiction and cite both sources. Do not resolve the contradiction by choosing one source over the other unless instructed." This prevents the model from silently preferring one document when both should be acknowledged.

## Handling Multiple Document Types

Different document types require different grounding strategies. Structured documents like contracts and regulations have clear section boundaries and can be cited precisely. Unstructured documents like emails and meeting notes do not have section numbers and require different citation formats.

For structured documents, you use hierarchical citations: "Document 1, Section 4.2, Subsection 4.2.1." The model can pinpoint the exact clause. You instruct the model to preserve the document's native structure when citing.

For unstructured documents, you use line-based or paragraph-based citations: "Document 3, Paragraph 7." You preprocess unstructured documents to add paragraph numbers so the model can cite them. Without paragraph numbers, citations become "somewhere in Document 3," which is not useful.

For tabular documents like spreadsheets or CSV files, you use row-column citations: "Table 1, Row 14, Column 'Annual Revenue.'" You convert tables to a text format that the model can reference, and you instruct the model to cite row and column identifiers.

For image-based documents like scanned PDFs, you use page-based citations: "Document 2, Page 5." You run OCR to extract text and associate each text block with a page number. The model cites pages, not sections, because scanned documents often lack structural metadata.

## Grounding for Summarization and Synthesis Tasks

Summarization and synthesis tasks are harder to ground than extraction tasks because the output is a transformation of the input, not a direct quote. You still need grounding, but the grounding strategy is different.

You ask the model to reference the sources it summarized. You write "for each paragraph of your summary, list which sections of the source documents you synthesized." This creates a map from output to input. If a summary paragraph cannot be traced back to specific source sections, it might contain hallucinated content.

You ask the model to distinguish between summary and interpretation. You write "if you make a claim that interprets or infers beyond the literal text, label it as [INTERPRETATION] and explain your reasoning." This flags statements that go beyond the source material.

You use extractive summarization for high-stakes tasks. Instead of asking the model to rewrite content in its own words, you ask it to select and lightly edit key sentences from the source. Extractive summaries are easier to verify because each sentence has a clear source.

You verify that summaries do not introduce new facts. You run a second model pass that checks: "Does the summary contain any claims that are not supported by the source documents?" If the verification model finds unsupported claims, you flag the summary.

## Faithfulness Metrics and Quality Monitoring

Faithfulness is the degree to which model output is supported by the provided context. You measure faithfulness separately from other quality dimensions like coherence, relevance, and completeness. An output can be coherent but unfaithful.

You use claim-level verification. You parse the model's output into individual claims, extract the citation for each claim, retrieve the cited text, and check whether the cited text supports the claim. You calculate the percentage of claims that pass verification. This is your faithfulness score.

You use human evaluation for complex claims. Automated verification works well for simple extractions like "the contract term is two years." It works poorly for complex reasoning like "the contract's liquidated damages clause is likely unenforceable." You sample 1% to 5% of outputs for human review.

You track faithfulness over time. If faithfulness drops from 97% in November to 89% in January, you investigate. Model updates, prompt changes, or input data shifts can degrade faithfulness.

You set faithfulness thresholds based on task stakes. For low-stakes tasks like internal document tagging, you might accept 85% faithfulness. For high-stakes tasks like legal analysis or medical triage, you require 98% faithfulness. Outputs that fall below the threshold go to human review.

## Cost of Verification and Trade-offs

Verification costs tokens and latency. If you run a second model pass to verify citations, you roughly double your API costs. If you run claim-level verification with multiple small model calls, costs can increase by 50% to 100%. You trade cost for reduced hallucination risk.

You calculate the expected value of verification. If verification costs $0.20 per request and catches hallucinations that would cost $50 to fix (in rework, liability, or customer trust), you need to catch hallucinations in at least 0.4% of requests to break even. If your baseline hallucination rate is 2%, verification is worth it.

You use lightweight verification for low-stakes tasks. Instead of claim-level verification, you check that the output includes citations in the correct format. This catches egregious hallucinations (no citations at all) without the cost of deep verification.

You use heavy verification for high-stakes tasks. You run claim-level checks, human spot checks, and cross-reference verification. The cost is high, but the risk of undetected hallucination is unacceptable.

## Prompt Design for Citation-Heavy Outputs

When you require citations, you design prompts that make citation easy for the model. You do not just demand citations and hope the model figures it out. You scaffold the citation process.

You provide citation examples in your prompt. You include one or two examples of well-cited answers. The model learns the citation format and rhythm from the examples. "The termination period is 30 days [Doc:1, Sec:8.3]. This applies to all employees regardless of tenure [Doc:1, Sec:8.3.1]."

You use structured output formats. You ask the model to output JSON with fields for claim, citation, and reasoning. Structured formats make citations explicit and parseable. Unstructured text makes citations easy to miss or format incorrectly.

You break complex tasks into cite-and-answer subtasks. Instead of asking the model to write a complete analysis with citations, you ask it to (1) identify relevant sections, (2) extract key facts from each section with citations, (3) synthesize the facts into an analysis. The three-step process scaffolds citation behavior.

## Model-Specific Grounding Behavior

Different models exhibit different levels of grounding compliance. Some models reliably follow grounding instructions. Others drift toward training data when context is ambiguous.

Claude models generally exhibit strong grounding behavior when prompted with explicit citation instructions. They are less prone to hallucinating facts that contradict the context. GPT-4 models show similar grounding behavior but occasionally generate confident-sounding claims that are not well-supported by the context. Gemini models are less tested in production settings as of January 2026, but early reports suggest grounding behavior is competitive with Claude and GPT-4.

You test your production model's grounding behavior with adversarial examples. You provide a context that lacks the information needed to answer the question and see whether the model admits "insufficient information" or hallucinates an answer. Models that hallucinate require stricter grounding instructions and more verification.

You track model updates. When a provider releases a new version, you re-run grounding tests. Grounding behavior can change. A model that required minimal grounding instructions in November might need stricter instructions after a January update.

## Documenting Grounding Policies

You document your grounding requirements and verification policies. Future engineers need to know what level of faithfulness is required, what verification steps are mandatory, and how to handle edge cases.

You version grounding instructions with your prompt templates. When you update a prompt to add stricter citation requirements, you version it and document the change. You do not silently change grounding behavior in production.

You surface grounding metrics in your observability dashboards. Your team sees citation accuracy, faithfulness scores, and verification pass rates alongside latency and cost metrics. Grounding is not a nice-to-have. It is a core quality dimension.

You train your team on grounding failures. Engineers and product managers need to understand that plausible-sounding outputs are not the same as grounded outputs. A confident answer with no citations is worse than a hedged answer that admits uncertainty.

The next subchapter covers multimodal prompting strategies, focusing on how to integrate vision and audio inputs with text in long-context scenarios.
