# 2.13 — Prompt Patterns for Classification, Extraction, and Generation

An e-commerce company built three AI features in September 2025: product categorization, attribute extraction from descriptions, and marketing copy generation. They used the same basic prompt structure for all three tasks, figuring that clear instructions and good examples would work universally. The classification feature worked well. Extraction was acceptable. Generation was a disaster, producing bland, generic copy that their marketing team rejected 68% of the time.

The engineering lead couldn't understand the inconsistency. They had followed the same prompt engineering principles across all three features. Clear task description, relevant examples, output format specifications. Why did the same approach produce such different outcomes? The team spent two weeks tweaking the generation prompt, adding more examples, refining instructions, trying different temperature settings. Improvements were marginal.

The problem was fundamental. Different task types require different prompt architectures. Classification prompts need clear category boundaries and disambiguation logic. Extraction prompts need pattern recognition and structured output handling. Generation prompts need creative constraints and quality criteria. The team was trying to use a classification prompt pattern for a generation task. No amount of tweaking would bridge that architectural mismatch.

## Classification Prompt Architecture

Classification tasks require prompts that make category boundaries explicit and provide disambiguation strategies for edge cases. Your prompt needs to define not just what each category is but how to distinguish between similar categories. For a support ticket classifier, defining "Billing" as "questions about charges" is insufficient. You need to specify how to distinguish billing questions from refund requests, payment method changes, and pricing inquiries if those are separate categories.

Start classification prompts with a clear category enumeration. List every valid category with a brief but distinctive description. Use parallel structure: each category gets the same type of description. If you describe one category with example inputs and another with abstract criteria, the model has inconsistent guidance. Consistency in category definitions produces consistency in classification decisions.

Include boundary examples that show how to handle ambiguous cases. A customer message saying "I was charged twice, I want my money back" straddles the billing-refund boundary. Your prompt should include examples of exactly this kind of ambiguity with clear classifications and brief justifications. These examples teach the model your disambiguation logic, not just your categories.

Specify a default category or rejection mechanism for inputs that don't clearly fit anywhere. In multi-class classification, you might have an "Other" or "General" category. In binary classification, you need to specify whether edge cases should lean positive or negative. Don't leave ambiguous cases undefined—the model will make inconsistent choices without guidance.

## Extraction Prompt Patterns

Extraction prompts need to specify what to extract, where to find it, and how to handle missing or ambiguous information. Unlike classification, where every input gets exactly one label, extraction produces variable-length output. Your prompt architecture must account for zero matches, multiple matches, and partial matches.

Define extraction targets with both abstract criteria and concrete examples. If you're extracting company names from news articles, specify "proper nouns referring to business entities, including corporations, partnerships, and LLCs." Then show examples: "Apple Inc.", "Acme Corp", "Smith & Associates". The abstract definition provides the rule, the examples anchor what the rule means in practice.

Specify the extraction granularity you need. Should the model extract "Google" or "Google Cloud Platform" or "Google Cloud Platform's BigQuery data warehouse"? All three are technically correct for different granularities. If you need entity-level extraction, say so explicitly. If you need fine-grained span extraction, provide examples at that level. Mismatched granularity is a common source of extraction errors.

Handle multi-instance extraction carefully. If a document contains five company names, your prompt needs to specify how to return all five. Structured output formats like JSON arrays work well here. Show an example input with multiple extraction targets and the complete structured output. Don't assume the model will infer the pattern from single-instance examples.

## Structured Output for Extraction

Extraction tasks benefit enormously from structured output formats. Instead of asking for free-form text responses, specify JSON, XML, or another parseable format. This makes downstream processing reliable and catches malformed responses early. Claude 3.5 and GPT-4o both handle structured output well when you specify the schema clearly.

Provide a complete output schema before any examples. Show the JSON structure with field names, types, and optional vs required fields. Then show examples that populate this schema. This ordering—schema first, examples second—helps the model understand the structure as a constraint that examples demonstrate rather than a pattern to infer from examples.

Include an example of empty results. When no extraction targets are found in the input, what should the output look like? An empty JSON array, a null value, or an explicit "none found" marker? Specify this clearly and show an example. Models often stumble on empty cases because they're rare in training data and ambiguous in prompts.

Specify how to handle extraction uncertainty. If the model is 90% confident a span is a company name but not certain, should it include the extraction with a confidence score, omit it, or flag it specially? Your prompt needs to make this decision explicit. Confidence thresholds and uncertainty handling are architectural choices, not implementation details the model should guess.

## Generation Prompt Foundations

Generation tasks need prompts that specify not just what to create but what good looks like. Unlike classification and extraction, where correctness is often objective, generation quality is multidimensional and subjective. Your prompt must make your quality criteria explicit and measurable.

Start with audience and purpose. "Generate marketing copy" is underspecified. "Generate product description for budget-conscious shoppers emphasizing value and practical benefits" gives the model a target. Specify who will read the output and what outcome you want. This context shapes tone, vocabulary, detail level, and emphasis.

Provide quality criteria as specific constraints. Instead of "make it engaging," specify "include a question in the first sentence, use concrete numbers rather than vague claims, and end with a clear call to action." Instead of "keep it concise," specify "maximum 150 words, three paragraphs." Vague quality terms are interpreted inconsistently. Specific constraints produce consistent output.

Show multiple examples that demonstrate variety within your quality criteria. If all your examples use the same structure or tone, the model will imitate that structure too closely. Show different valid approaches—different openings, different emphasis, different structural patterns—all meeting your quality criteria. This teaches the model the boundaries of acceptable variation.

## Task-Specific Example Selection

Classification examples should emphasize category boundaries and edge cases. Don't just show clear, prototypical examples. Show ambiguous cases that could reasonably belong to multiple categories, and demonstrate your disambiguation logic. Show near-misses where something almost fits a category but belongs elsewhere due to a subtle distinction.

Extraction examples should show structural variety in the input and completeness in the output. If you're extracting addresses from forms, show addresses in different formats: single line, multiple lines, with or without apartment numbers, with country codes or without. For each input variation, show complete extraction. This teaches the model to handle input diversity.

Generation examples should span your quality spectrum. Don't show only perfect examples—show acceptable examples at different points in your quality range. If you accept output from terse to elaborate, show both extremes and the middle ground. This calibrates the model's understanding of acceptable variation and prevents it from overfitting to one style.

Keep classification examples brief and numerous. You want coverage of categories and boundaries, not deep illustration of each case. Five to eight examples usually suffice for classification tasks with 3-7 categories. Extraction benefits from fewer, more detailed examples. Three to five examples that show different extraction scenarios work better than ten examples that show the same pattern. Generation needs the fewest but most substantial examples. Two to three high-quality examples that demonstrate different approaches are often sufficient.

## Output Format Specifications

Classification output format is usually simple: return the category label or ID. Specify the exact format: "Return only the category name in uppercase" or "Return the category ID as an integer." Specify what to return if no category fits. Make the output format machine-parseable and unambiguous.

Extraction output format should separate extracted content from metadata. Don't just return extracted text—return it in a structure that includes position information, extraction type, and optionally confidence scores. JSON works well: each extraction gets an object with text, start_position, end_position, and entity_type fields. This format supports downstream processing and validation.

Generation output format varies by use case. For structured generation like product descriptions with specific sections, use headings or JSON keys to delimit sections. For free-form generation like creative writing, simple text output is fine. For generation that fills templates, specify the template structure and how the model's output should populate it.

Always specify trailing matter: should the model include explanations or confidence notes after the primary output, or return only the requested content? By default, models often add commentary. If you want clean output only, state: "Return only the classification label with no explanation or commentary." If you want explanations, specify their format and placement.

## Multi-Task Prompts and Task Switching

Some applications need a single prompt that handles multiple task types based on context. A customer service AI might classify the inquiry, extract key details, and generate a response all in one interaction. These multi-task prompts are complex to structure but powerful when done well.

Sequence tasks explicitly rather than leaving the model to infer order. "First, classify this inquiry into one of these categories: [categories]. Second, extract any relevant account numbers, order IDs, or dates. Third, generate a response using the classification and extracted information." Clear sequencing prevents the model from skipping tasks or conflating them.

Use structured output that separates task results. JSON with top-level keys for each task works well: classification, extracted_entities, and generated_response as separate fields. This structure makes it clear what outputs belong to which task and makes parsing reliable. Avoid free-form output where task results blur together.

Specify dependencies between tasks. If the response generation should use the extracted entities, say so explicitly: "Generate the response using the account number and order ID you extracted." If classification should inform extraction focus, specify that too: "For billing inquiries, extract payment amounts and dates. For technical inquiries, extract error messages and product names."

## Task-Specific Temperature and Sampling

Classification tasks work best at low temperatures, typically 0.0 to 0.3. You want deterministic, consistent category assignment. High temperature introduces randomness that makes identical inputs produce different classifications, which is almost never desired for classification systems.

Extraction also favors low temperatures in the 0.0 to 0.4 range. Extraction should be consistent and deterministic. The same document should yield the same extractions on repeated runs. Higher temperatures can cause the model to hallucinate entities or extract different spans on successive attempts.

Generation benefits from moderate to high temperatures depending on the application. For creative content like marketing copy or storytelling, temperatures from 0.7 to 1.0 produce variety and creativity. For factual generation like report summaries or technical documentation, use lower temperatures (0.3 to 0.6) to maintain accuracy while allowing natural variation in phrasing.

Test temperature sensitivity for your specific task and domain. Some generation tasks produce acceptable variety even at temperature 0.3. Others feel wooden and repetitive below 0.8. Your validation set should include temperature as a parameter you sweep during development. Find the temperature that balances consistency and quality for your use case.

## Hybrid Tasks and Prompt Composition

Some applications blur task boundaries. Sentiment classification with explanation is part classification, part generation. Entity extraction with normalization is part extraction, part transformation. These hybrid tasks need prompt patterns that combine approaches from multiple task types.

For classification with explanation, structure the prompt as a classification task first, then add a generation component: "Classify the sentiment, then explain your reasoning in one sentence." This two-stage structure gives the model clear subtasks. Specify the output format to separate classification from explanation: use JSON with sentiment and explanation fields.

For extraction with transformation, specify the extraction task, then the transformation rules: "Extract all dates mentioned in the text. Convert each date to ISO 8601 format (YYYY-MM-DD)." Show examples of both the extraction and the transformation. The model needs to understand both components and how they connect.

Watch for task interference in hybrid prompts. Sometimes generation components cause the model to be less precise in classification or extraction. If adding explanation degrades classification accuracy, you may need to separate the tasks into sequential API calls rather than combining them in one prompt. Measure task accuracy for hybrid prompts carefully.

## Domain Adaptation for Task Types

Classification in specialized domains requires category definitions that use domain vocabulary. A medical symptom classifier needs to define categories using clinical terminology, not lay descriptions. Don't dumb down your categories—use precise domain language and provide examples that reflect how domain experts think about the categories.

Extraction in technical domains needs to specify technical patterns. Extracting software version numbers requires different patterns than extracting person names. Extracting legal citations requires different patterns than extracting URLs. Your extraction prompt should acknowledge domain-specific patterns and show examples that reflect them.

Generation in specialized domains needs domain-appropriate quality criteria. Marketing copy has different quality dimensions than legal disclaimers or technical documentation. Don't use generic generation prompts across domains. Build domain-specific generation prompts that reflect what quality means in that domain, with examples from domain experts.

Test your domain-adapted prompts with domain practitioners, not just engineers. A prompt that makes sense to you might miss critical domain nuances that experts would catch immediately. Domain review of prompts is as important as code review, especially for regulated industries or high-stakes applications.

## Error Handling for Each Task Type

Classification errors fall into clear categories: wrong category selected, no category selected, or malformed output. Your prompt should specify fallback behavior: if the model can't determine a category, should it choose a default, return an error, or defer to human review? Make this decision in the prompt, not in post-processing code.

Extraction errors include missed entities, hallucinated entities, and incorrect span boundaries. Specify how the model should handle uncertainty: skip uncertain extractions or flag them with confidence scores? Specify the confidence threshold if using scores. For span boundaries, show examples that make clear whether to extract minimal spans or maximal spans when boundaries are ambiguous.

Generation errors are harder to define because output quality is multidimensional. Specify unacceptable output explicitly: "Never generate responses that promise specific timelines or make commitments outside company policy." This negative specification helps the model avoid failure modes even if it doesn't guarantee high-quality output.

Build validation into your prompts when possible. For classification, you can ask the model to return both the category and a confidence score, then filter low-confidence classifications for human review. For extraction, you can ask the model to validate that extracted spans appear verbatim in the input. For generation, validation is harder, but you can ask the model to check its output against specific criteria as a final step.

## Testing Task-Specific Prompts

Your test suite should reflect task-specific success criteria. For classification, measure accuracy, precision, recall, and confusion between categories. For extraction, measure both entity-level accuracy (did you find the right entities) and span-level accuracy (did you extract the right boundaries). For generation, measure against task-specific quality rubrics.

Build task-specific test fixtures. Classification tests need balanced category representation and edge cases at category boundaries. Extraction tests need inputs with varying entity density, from zero entities to high-density documents. Generation tests need quality rubrics and ideally human evaluation for a sample of outputs.

Automate what you can, but recognize when human judgment is necessary. Classification and extraction are largely automatable—you can calculate metrics from ground truth. Generation quality often requires human assessment, at least for a sample. Build tooling that makes human evaluation efficient, like side-by-side comparisons or quality scoring interfaces.

Track task-specific failure modes over time. For classification, which category pairs get confused most often? For extraction, what entity types have the lowest recall? For generation, what quality criteria fail most frequently? These patterns inform prompt refinement priorities and help you focus optimization efforts where they'll have the most impact.

Now that you understand task-specific prompt patterns, you need to recognize common prompt anti-patterns and know how to refactor them.
