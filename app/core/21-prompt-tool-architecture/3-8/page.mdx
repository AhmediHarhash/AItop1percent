# 3.8 — Audio and Voice Prompt Design

A healthcare startup launched a voice-enabled patient intake system in September 2024. The system asked patients to describe their symptoms and medical history through natural speech, then generated structured intake forms for physician review. During the first week of deployment at a clinic serving diverse communities, the system achieved 73% accuracy on native English speakers but only 31% accuracy on patients with accented English or non-native speakers.

The core issue was not the transcription quality, which exceeded 90% even for accented speech. The problem was the prompt design. The system prompted the model to "extract structured medical information from the patient statement" but provided no guidance on handling disfluencies, false starts, corrections, or conversational patterns common in speech. When a patient said "I have been having... well not really having but more like feeling... chest tightness, no wait, chest pressure, especially when I... uh... when I climb stairs," the model extracted multiple contradictory symptoms rather than recognizing the patient's self-corrections.

The startup had designed their prompts for clean, written text, then applied them to messy, spoken language. Three months of corrections and 847 escalations to human review later, they rebuilt the system with voice-specific prompting. The rebuild cost $340,000 and delayed their Series A fundraising by five months. The root cause was treating voice input as transcribed text rather than as a distinct modality with unique characteristics.

## Voice Input Has Different Information Density Than Text

When you design prompts for voice input, you must account for the fundamental difference between spoken and written language. Speech contains more words to convey the same information. People repeat themselves, use filler words, and structure thoughts differently when speaking versus writing.

A written message might say "The system failed at 3:15 PM due to memory exhaustion." The same information spoken might be "So the system, it crashed, this was around, I think it was 3:15 in the afternoon, and what happened was we ran out of memory, the system basically exhausted all available memory." Both convey identical facts, but the spoken version has three times the word count.

Your prompts must instruct the model to extract signal from this noisier input. A naive prompt treats every word as significant. An effective voice prompt says "extract the core information from this spoken statement, filtering out filler words, repetitions, and conversational padding while preserving the speaker's actual meaning."

## Disfluencies Carry Meaning and Noise

Spoken language contains **disfluencies**: pauses, false starts, corrections, fillers like "um" and "uh," and self-interruptions. These are not errors in the traditional sense. They are natural features of speech production. Your prompts must handle them appropriately.

Some disfluencies are noise to ignore. When someone says "I need to... um... schedule a meeting," the "um" adds no information. Other disfluencies signal meaning. When someone says "The project will be done in two... actually three weeks," the false start and correction matter. The speaker explicitly revised their estimate.

Prompts must distinguish between noise disfluencies and meaningful disfluencies. Instruct the model: "Process this spoken input by filtering filler words like um, uh, and like. However, preserve explicit corrections where the speaker uses words like actually, no, or wait to revise their previous statement."

Repeated disfluencies can indicate uncertainty or emphasis. If someone says "I am... I am absolutely certain the deadline is Friday," the repetition suggests emphasis on certainty. If they say "I think... I think maybe... possibly the deadline is Friday," the pattern suggests uncertainty. Prompts for voice analysis should note these patterns: "Pay attention to repetition patterns and hedging language as indicators of speaker confidence."

## Real-Time Voice Requires Latency-Aware Prompting

Voice interactions often happen in real-time, creating latency constraints that do not exist with text. A user speaking to a voice assistant expects responses within 1-2 seconds. Complex prompts with extended reasoning chains introduce unacceptable delays.

**Latency-aware prompting** means optimizing for speed while preserving quality. You cannot use prompts designed for batch processing. Long contextual examples, extensive chain-of-thought reasoning, and multi-stage processing all increase latency beyond acceptable thresholds.

Techniques for reducing voice prompt latency include front-loading critical instructions, using concise language, eliminating redundant context, and deferring non-essential processing. A real-time voice prompt prioritizes immediate response generation, potentially flagging items for follow-up processing rather than completing full analysis before responding.

For streaming transcription scenarios where the model processes speech as it arrives, prompts must handle incomplete information gracefully. The model might receive partial utterances: "I need to schedule a..." followed by silence, then "meeting for next Tuesday." Your prompt must specify how to handle these fragments: "Process incoming speech incrementally. Maintain context across fragments. Wait for natural pauses before finalizing interpretations."

## Tone and Prosody Require Explicit Detection Instructions

The way people say things often matters as much as what they say. Tone of voice carries emotional content, intent, and urgency. Sarcasm, frustration, excitement, and concern all manifest in vocal characteristics. If your application needs to detect these signals, your prompts must explicitly request tone analysis.

Vision and language models process text transcriptions by default, losing prosodic information unless you use audio-native models. When working with transcription-based systems, you must infer tone from word choice and phrasing rather than acoustic features. When working with audio-native models, you can prompt for acoustic analysis.

For transcription-based systems, prompt for linguistic tone markers: "Analyze this statement for indicators of frustration, urgency, or dissatisfaction based on word choice, punctuation patterns in the transcription, and explicit emotional language." For audio-native systems, prompt for acoustic features: "Assess the speaker's emotional state based on vocal tone, pitch variation, speech rate, and volume."

Tone detection prompts must account for cultural and contextual variation. Direct, assertive speech might indicate confidence in one culture and rudeness in another. Your prompts need cultural context: "Analyze tone with awareness that this customer interaction follows Japanese business communication norms, where indirect language and formal politeness markers are standard."

## Voice Commands Need Disambiguating Context

Voice interfaces face ambiguity that text interfaces avoid. When someone types "set alarm 7," the system knows they mean a time. When someone says "set alarm seven," the system must distinguish between 7:00 AM, 7:00 PM, seven minutes from now, or seven hours from now. Spoken numbers lack the contextual markers that punctuation and formatting provide in text.

**Disambiguating prompts** instruct the model to resolve voice-specific ambiguities using context. "When processing time-related voice commands, infer AM/PM based on current time and typical usage patterns. If the current time is 8:00 PM and the user says set alarm seven, default to 7:00 AM unless context suggests otherwise."

Homophones create additional ambiguity. "Right the report" and "write the report" sound identical but mean different things. "Weather" and "whether," "their" and "there," "two" and "too" all require contextual disambiguation. Prompts must instruct the model to select the appropriate spelling based on semantic context: "Resolve homophones in transcribed speech by analyzing grammatical context and sentence meaning."

For domain-specific voice applications, build disambiguation rules into prompts: "This is a medical transcription context. When you encounter homophone pairs like patients/patience or vein/vain, default to the medical terminology unless context clearly indicates otherwise."

## Multi-Turn Voice Conversations Need Context Management

Voice interactions rarely consist of isolated utterances. Users have conversations with context spanning multiple turns. "Schedule a meeting" followed by "make it Tuesday at 2 PM" followed by "actually change that to Wednesday" represents a single coherent interaction across three utterances.

**Context management prompts** maintain state across conversational turns. The model must track what "it" refers to, what "that" means, and how each utterance relates to previous ones. Without explicit context management instructions, the model treats each utterance independently.

Structure context management in your prompts: "Maintain conversation state across utterances. Track entities introduced in previous turns. Resolve pronouns and references to earlier statements. When the user says change that, identify what that refers to based on recent conversation history."

For voice systems, context management must handle interruptions and topic switches gracefully. A user might say "schedule a meeting for Tuesday," then switch topics: "wait, before that, what is the weather tomorrow," then return: "okay, back to that Tuesday meeting." Your prompt needs to handle these patterns: "Track topic shifts and returns. When the user explicitly returns to a previous topic, restore that context."

## Voice Transcription Errors Need Graceful Handling

Even high-quality transcription systems make errors. Background noise, overlapping speech, technical terminology, and proper nouns all introduce transcription mistakes. Your prompts must handle these errors gracefully rather than failing or producing nonsensical outputs.

**Error-resilient prompting** instructs the model to work with imperfect transcriptions. "This text is transcribed from speech and may contain errors. If you encounter unclear phrasing, obvious transcription mistakes, or garbled words, flag them and proceed with your best interpretation of the speaker's likely intent."

Transcription errors often follow predictable patterns. Rare words get replaced with common homophones. Technical terms become similar-sounding common words. Proper nouns get mangled. Prompts can incorporate error models: "Be aware that technical and medical terminology may be incorrectly transcribed as common homophones. If a word seems out of place in context, consider whether a similar-sounding technical term was the speaker's intent."

For critical applications, prompts should surface uncertainty rather than silently correcting errors: "When you identify likely transcription errors that affect meaning, flag them explicitly rather than making assumptions about intended meaning. In safety-critical or medical contexts, do not guess at correcting ambiguous or garbled terms."

## Language Mixing and Code-Switching Patterns

Many speakers mix languages within single utterances or conversations, a phenomenon called **code-switching**. This is particularly common in multilingual communities and technical contexts where English terminology intersects with other languages. Standard prompts designed for monolingual input fail on code-switched speech.

Prompts must handle language mixing explicitly: "This transcription may contain code-switching between English and Spanish. Process mixed-language input by interpreting terms in their appropriate language context. Preserve the speaker's language choices in the output rather than translating everything to English."

Technical domains have their own code-switching patterns. A French engineer might say "Le système a crashed à cause du memory leak." The sentence mixes French grammar with English technical terms. Your prompt needs to recognize this as normal rather than treating it as errors: "Technical discussions may mix languages, particularly using English terms for specialized concepts. Treat this as intentional language choice, not transcription errors."

## Voice-Specific Output Formats

When the output of voice processing will be spoken back to users, structure it for spoken delivery rather than written presentation. Spoken output should use shorter sentences, avoid complex punctuation, and flow naturally when read aloud.

A written output might say: "Your meeting (scheduled for Tuesday, March 15 at 2:00 PM in Conference Room B with attendees from Engineering, Product, and Design) has been confirmed." Spoken output should restructure: "Your meeting is confirmed for Tuesday, March 15 at 2:00 PM. It will be in Conference Room B. Attendees include Engineering, Product, and Design."

Prompt for spoken output structure: "Generate responses optimized for text-to-speech delivery. Use short, clear sentences. Avoid parenthetical asides and complex punctuation. Structure information sequentially rather than embedding details within sentences."

For voice interfaces reading structured data, specify pronunciation guidance: "When outputting numerical data, write numbers as words for amounts under twenty and use digits for larger numbers. For times, use conversational formats: say two PM rather than fourteen hundred hours."

## Domain Vocabulary and Pronunciation Guides

Specialized domains have terminology that transcription systems struggle with and that users may pronounce variably. Medical terms, technical jargon, company names, and product codes all create challenges. Your prompts can include domain vocabularies to improve interpretation.

Provide spelling references in prompts: "Common terms in this medical context include dyspnea (difficulty breathing), tachycardia (rapid heart rate), and myalgia (muscle pain). If the transcription contains similar-sounding words, consider whether these technical terms were intended."

For applications processing voice input about specific entities, list them: "This system handles inventory queries. Product codes follow the pattern X-12345. If the transcription contains phrases like x one two three four five or ex twelve thirty-four five, interpret them as product codes."

The prompt can also guide the model on pronunciation variations: "Users may pronounce SQL as sequel or S-Q-L, IPv6 as I-P-version-six or I-P-vee-six, and GIF as gif or jif. Treat these pronunciation variants as referring to the same terms."

## Testing Voice Prompts With Real Speech Patterns

You cannot evaluate voice prompts using clean, written test cases. Real speech contains all the messiness discussed: disfluencies, false starts, corrections, ambiguity, and transcription errors. Your test sets must include authentic spoken language.

Collect real voice samples from your target user population. Different demographics, accents, speaking styles, and contexts produce different speech patterns. An elderly patient describing symptoms sounds different from a young technical user issuing commands.

Build test sets that include problematic cases: heavily accented speech, noisy audio, overlapping speakers, long pauses, rapid speech, and domain-specific terminology. Evaluate whether your prompts handle these edge cases gracefully or fail.

Run A/B tests comparing voice-optimized prompts against text-optimized prompts on the same transcribed content. The performance gap reveals how much voice-specific design matters for your use case.

The next subchapter examines multimodal fusion, where you combine text, images, and structured data in single prompts to leverage complementary information sources.
