# 3.8 — Audio and Voice Prompt Design

A healthcare startup launched a voice-enabled patient intake system in September 2024. The system asked patients to describe their symptoms and medical history through natural speech, then generated structured intake forms for physician review. During the first week of deployment at a clinic serving diverse communities, the system achieved 73 percent accuracy on native English speakers but only 31 percent accuracy on patients with accented English or non-native speakers.

The core issue was not the transcription quality, which exceeded 90 percent even for accented speech. The problem was the prompt design. The system prompted the model to "extract structured medical information from the patient statement" but provided no guidance on handling disfluencies, false starts, corrections, or conversational patterns common in speech. When a patient said "I have been having... well not really having but more like feeling... chest tightness, no wait, chest pressure, especially when I... uh... when I climb stairs," the model extracted multiple contradictory symptoms rather than recognizing the patient's self-corrections.

The startup had designed their prompts for clean, written text, then applied them to messy, spoken language. Three months of corrections and 847 escalations to human review later, they rebuilt the system with voice-specific prompting. The rebuild cost $340,000 and delayed their Series A fundraising by five months. The root cause was treating voice input as transcribed text rather than as a distinct modality with unique characteristics.

After the rebuild, accuracy rose to 89 percent across all patient populations. The system learned to handle disfluencies, recognize self-corrections, and interpret spoken language patterns. The $340,000 lesson taught them that voice is not just text that happens to be spoken, but a fundamentally different communication channel requiring specialized prompt engineering.

## Voice Input Has Different Information Density Than Text

When you design prompts for voice input, you must account for the fundamental difference between spoken and written language. Speech contains more words to convey the same information. People repeat themselves, use filler words, and structure thoughts differently when speaking versus writing.

A written message might say "The system failed at 3:15 PM due to memory exhaustion." The same information spoken might be "So the system, it crashed, this was around, I think it was 3:15 in the afternoon, and what happened was we ran out of memory, the system basically exhausted all available memory." Both convey identical facts, but the spoken version has three times the word count.

Your prompts must instruct the model to extract signal from this noisier input. A naive prompt treats every word as significant. An effective voice prompt says "extract the core information from this spoken statement, filtering out filler words, repetitions, and conversational padding while preserving the speaker's actual meaning."

The healthcare system's initial prompt assumed concise medical terminology. Real patients said things like "my chest hurts when I, you know, when I'm doing stuff like going upstairs or whatever, like exerting myself." This needed translation to "chest pain on exertion" while preserving the clinical detail. Voice prompts must bridge colloquial speech and professional terminology.

Information density also varies with speaker confidence and language proficiency. Native speakers often use more filler words because they speak faster. Non-native speakers may use fewer fillers but more pauses and reconstructions as they search for words. Your prompt should handle both patterns without penalizing either.

## Disfluencies Carry Meaning and Noise

Spoken language contains **disfluencies**: pauses, false starts, corrections, fillers like "um" and "uh," and self-interruptions. These are not errors in the traditional sense. They are natural features of speech production. Your prompts must handle them appropriately.

Some disfluencies are noise to ignore. When someone says "I need to... um... schedule a meeting," the "um" adds no information. Other disfluencies signal meaning. When someone says "The project will be done in two... actually three weeks," the false start and correction matter. The speaker explicitly revised their estimate.

Prompts must distinguish between noise disfluencies and meaningful disfluencies. Instruct the model: "Process this spoken input by filtering filler words like um, uh, and like. However, preserve explicit corrections where the speaker uses words like actually, no, or wait to revise their previous statement."

Repeated disfluencies can indicate uncertainty or emphasis. If someone says "I am... I am absolutely certain the deadline is Friday," the repetition suggests emphasis on certainty. If they say "I think... I think maybe... possibly the deadline is Friday," the pattern suggests uncertainty. Prompts for voice analysis should note these patterns: "Pay attention to repetition patterns and hedging language as indicators of speaker confidence."

The healthcare system learned to recognize correction patterns: "When a patient revises their statement using words like no, wait, actually, or I mean, treat the revision as the authoritative statement and disregard the initial phrasing. For example, in statement I felt pain in my arm, no wait, my shoulder, extract location as shoulder not arm."

Disfluencies also provide cognitive load signals. Increased hesitation when discussing certain symptoms might indicate embarrassment, confusion, or memory difficulty. While you should not overinterpret these signals, noting them can flag cases needing follow-up. "If patient exhibits significant hesitation or reformulation when describing onset or severity, flag for clinician follow-up."

## Real-Time Voice Requires Latency-Aware Prompting

Voice interactions often happen in real-time, creating latency constraints that do not exist with text. A user speaking to a voice assistant expects responses within 1-2 seconds. Complex prompts with extended reasoning chains introduce unacceptable delays.

**Latency-aware prompting** means optimizing for speed while preserving quality. You cannot use prompts designed for batch processing. Long contextual examples, extensive chain-of-thought reasoning, and multi-stage processing all increase latency beyond acceptable thresholds.

Techniques for reducing voice prompt latency include front-loading critical instructions, using concise language, eliminating redundant context, and deferring non-essential processing. A real-time voice prompt prioritizes immediate response generation, potentially flagging items for follow-up processing rather than completing full analysis before responding.

For streaming transcription scenarios where the model processes speech as it arrives, prompts must handle incomplete information gracefully. The model might receive partial utterances: "I need to schedule a..." followed by silence, then "meeting for next Tuesday." Your prompt must specify how to handle these fragments: "Process incoming speech incrementally. Maintain context across fragments. Wait for natural pauses before finalizing interpretations."

The healthcare system initially used prompts designed for written intake forms, with extensive medical terminology validation and cross-field consistency checking. These prompts averaged 4.2 seconds processing time. The rebuild optimized for real-time interaction, reducing prompts to essential extraction rules and deferring validation to post-processing. Latency dropped to 0.8 seconds while maintaining 89 percent accuracy.

Latency optimization also means choosing when to interrupt versus wait. In conversational systems, immediate acknowledgment matters. "Extract key information from the current utterance and provide immediate acknowledgment, flagging items requiring clarification for follow-up questions rather than waiting for complete information."

## Tone and Prosody Require Explicit Detection Instructions

The way people say things often matters as much as what they say. Tone of voice carries emotional content, intent, and urgency. Sarcasm, frustration, excitement, and concern all manifest in vocal characteristics. If your application needs to detect these signals, your prompts must explicitly request tone analysis.

Vision and language models process text transcriptions by default, losing prosodic information unless you use audio-native models. When working with transcription-based systems, you must infer tone from word choice and phrasing rather than acoustic features. When working with audio-native models, you can prompt for acoustic analysis.

For transcription-based systems, prompt for linguistic tone markers: "Analyze this statement for indicators of frustration, urgency, or dissatisfaction based on word choice, punctuation patterns in the transcription, and explicit emotional language." For audio-native systems, prompt for acoustic features: "Assess the speaker's emotional state based on vocal tone, pitch variation, speech rate, and volume."

Tone detection prompts must account for cultural and contextual variation. Direct, assertive speech might indicate confidence in one culture and rudeness in another. Your prompts need cultural context: "Analyze tone with awareness that this customer interaction follows Japanese business communication norms, where indirect language and formal politeness markers are standard."

The healthcare system learned that tone analysis helped identify patient distress levels. "Assess whether patient exhibits markers of acute distress such as rapid speech, urgent language, or explicit statements of severe pain. Flag high-distress cases for immediate triage rather than standard scheduling."

Tone also affects interpretation of symptom severity. A patient saying "it hurts a bit" in a strained voice is different from the same words said casually. Transcription-based prompts can note: "Pay attention to minimizing language paired with strong descriptive words, which may indicate understatement. Phrase I guess it hurts pretty bad actually suggests significant pain despite hedging language."

## Voice Commands Need Disambiguating Context

Voice interfaces face ambiguity that text interfaces avoid. When someone types "set alarm 7," the system knows they mean a time. When someone says "set alarm seven," the system must distinguish between 7:00 AM, 7:00 PM, seven minutes from now, or seven hours from now. Spoken numbers lack the contextual markers that punctuation and formatting provide in text.

**Disambiguating prompts** instruct the model to resolve voice-specific ambiguities using context. "When processing time-related voice commands, infer AM/PM based on current time and typical usage patterns. If the current time is 8:00 PM and the user says set alarm seven, default to 7:00 AM unless context suggests otherwise."

Homophones create additional ambiguity. "Right the report" and "write the report" sound identical but mean different things. "Weather" and "whether," "their" and "there," "two" and "too" all require contextual disambiguation. Prompts must instruct the model to select the appropriate spelling based on semantic context: "Resolve homophones in transcribed speech by analyzing grammatical context and sentence meaning."

For domain-specific voice applications, build disambiguation rules into prompts: "This is a medical transcription context. When you encounter homophone pairs like patients/patience or vein/vain, default to the medical terminology unless context clearly indicates otherwise."

The healthcare system added extensive homophone handling: "Common medical homophones include ileum/ilium, hyper/hypo prefixes, aural/oral, mucus/mucous. Resolve based on anatomical and clinical context. When uncertain, flag for review rather than guessing."

Disambiguation also handles numeric expressions. "I take my medication two times a day" versus "I take my medication to control pain" requires understanding that "two" is a number while "to" is a preposition. "Parse numeric expressions by identifying whether they modify nouns suggesting quantities or function as prepositions suggesting purpose."

## Multi-Turn Voice Conversations Need Context Management

Voice interactions rarely consist of isolated utterances. Users have conversations with context spanning multiple turns. "Schedule a meeting" followed by "make it Tuesday at 2 PM" followed by "actually change that to Wednesday" represents a single coherent interaction across three utterances.

**Context management prompts** maintain state across conversational turns. The model must track what "it" refers to, what "that" means, and how each utterance relates to previous ones. Without explicit context management instructions, the model treats each utterance independently.

Structure context management in your prompts: "Maintain conversation state across utterances. Track entities introduced in previous turns. Resolve pronouns and references to earlier statements. When the user says change that, identify what that refers to based on recent conversation history."

For voice systems, context management must handle interruptions and topic switches gracefully. A user might say "schedule a meeting for Tuesday," then switch topics: "wait, before that, what is the weather tomorrow," then return: "okay, back to that Tuesday meeting." Your prompt needs to handle these patterns: "Track topic shifts and returns. When the user explicitly returns to a previous topic, restore that context."

The healthcare system handled multi-turn symptom discussions: "When patient provides additional symptoms in follow-up statements, add to existing symptom list rather than replacing. When patient clarifies previous statements, update the specific detail while maintaining other information. For example: Patient says chest pain, then says it is on the left side, record as left-sided chest pain not just left side."

Context windows pose practical limits. Voice conversations can span dozens of turns. Prompts must handle context summarization: "After 10 conversational turns, summarize accumulated information into a structured summary. Use the summary plus recent turns as context for subsequent processing, preventing context overflow while maintaining conversation continuity."

## Voice Transcription Errors Need Graceful Handling

Even high-quality transcription systems make errors. Background noise, overlapping speech, technical terminology, and proper nouns all introduce transcription mistakes. Your prompts must handle these errors gracefully rather than failing or producing nonsensical outputs.

**Error-resilient prompting** instructs the model to work with imperfect transcriptions. "This text is transcribed from speech and may contain errors. If you encounter unclear phrasing, obvious transcription mistakes, or garbled words, flag them and proceed with your best interpretation of the speaker's likely intent."

Transcription errors often follow predictable patterns. Rare words get replaced with common homophones. Technical terms become similar-sounding common words. Proper nouns get mangled. Prompts can incorporate error models: "Be aware that technical and medical terminology may be incorrectly transcribed as common homophones. If a word seems out of place in context, consider whether a similar-sounding technical term was the speaker's intent."

For critical applications, prompts should surface uncertainty rather than silently correcting errors: "When you identify likely transcription errors that affect meaning, flag them explicitly rather than making assumptions about intended meaning. In safety-critical or medical contexts, do not guess at correcting ambiguous or garbled terms."

The healthcare system built error correction into prompts: "Common transcription errors in medical speech include diabetes transcribed as diet betas, hypertension as high pertension, prescription as perscription. When you encounter nonsensical phrases that match these patterns, suggest the likely intended medical term but flag as uncertain transcription."

Error handling also means knowing when to give up. "If transcription quality is so poor that you cannot extract meaningful information with reasonable confidence, return an error indicating poor audio quality rather than attempting to guess at content. Set confidence threshold at 60 percent for medical information extraction."

## Language Mixing and Code-Switching Patterns

Many speakers mix languages within single utterances or conversations, a phenomenon called **code-switching**. This is particularly common in multilingual communities and technical contexts where English terminology intersects with other languages. Standard prompts designed for monolingual input fail on code-switched speech.

Prompts must handle language mixing explicitly: "This transcription may contain code-switching between English and Spanish. Process mixed-language input by interpreting terms in their appropriate language context. Preserve the speaker's language choices in the output rather than translating everything to English."

Technical domains have their own code-switching patterns. A French engineer might say "Le système a crashed à cause du memory leak." The sentence mixes French grammar with English technical terms. Your prompt needs to recognize this as normal rather than treating it as errors: "Technical discussions may mix languages, particularly using English terms for specialized concepts. Treat this as intentional language choice, not transcription errors."

The healthcare system served communities where Spanish-English code-switching was common. "Patients may use Spanish anatomical terms like cabeza, estómago, corazón mixed with English descriptions. Process these naturally, extracting Spanish medical vocabulary with appropriate English translations in structured output. For example: Me duele el estómago should extract as abdominal pain."

Code-switching also happens with cultural idioms. "My abuela says I have empacho" requires understanding that empacho is a folk illness concept not directly translatable to Western medical taxonomy. "When patients use culture-specific illness terms, note them in patient's language and provide closest Western medical equivalent if applicable, flagging for clinician cultural competency review."

## Voice-Specific Output Formats

When the output of voice processing will be spoken back to users, structure it for spoken delivery rather than written presentation. Spoken output should use shorter sentences, avoid complex punctuation, and flow naturally when read aloud.

A written output might say: "Your meeting (scheduled for Tuesday, March 15 at 2:00 PM in Conference Room B with attendees from Engineering, Product, and Design) has been confirmed." Spoken output should restructure: "Your meeting is confirmed for Tuesday, March 15 at 2:00 PM. It will be in Conference Room B. Attendees include Engineering, Product, and Design."

Prompt for spoken output structure: "Generate responses optimized for text-to-speech delivery. Use short, clear sentences. Avoid parenthetical asides and complex punctuation. Structure information sequentially rather than embedding details within sentences."

For voice interfaces reading structured data, specify pronunciation guidance: "When outputting numerical data, write numbers as words for amounts under twenty and use digits for larger numbers. For times, use conversational formats: say two PM rather than fourteen hundred hours."

The healthcare system formatted intake summaries for clinician review via speech interface: "Structure patient summary for spoken delivery. Begin with chief complaint, then current symptoms using plain language, then relevant history, then flagged concerns. Use conversational medical terminology avoiding abbreviations that sound unclear when spoken. Say hypertension not H-T-N, diabetes not D-M."

Voice output also affects information density. Written summaries can pack dense information because readers control pace. Spoken summaries need more breathing room. "Limit spoken summaries to three most important points. Provide detail on request rather than overwhelming listener with complete information in initial output."

## Domain Vocabulary and Pronunciation Guides

Specialized domains have terminology that transcription systems struggle with and that users may pronounce variably. Medical terms, technical jargon, company names, and product codes all create challenges. Your prompts can include domain vocabularies to improve interpretation.

Provide spelling references in prompts: "Common terms in this medical context include dyspnea (difficulty breathing), tachycardia (rapid heart rate), and myalgia (muscle pain). If the transcription contains similar-sounding words, consider whether these technical terms were intended."

For applications processing voice input about specific entities, list them: "This system handles inventory queries. Product codes follow the pattern X-12345. If the transcription contains phrases like x one two three four five or ex twelve thirty-four five, interpret them as product codes."

The prompt can also guide the model on pronunciation variations: "Users may pronounce SQL as sequel or S-Q-L, IPv6 as I-P-version-six or I-P-vee-six, and GIF as gif or jif. Treat these pronunciation variants as referring to the same terms."

The healthcare system maintained a pronunciation guide for common medications: "Common medication pronunciations include: omeprazole as oh-mep-rah-zohl or oh-MEP-rah-zole, atorvastatin as ah-TORE-vah-stat-in or at-OR-vah-stat-in, metformin as met-FOR-min or MET-for-min. Recognize these as the same medication regardless of pronunciation variation."

Domain vocabulary also helps with spelling. Medical terms have standard spellings that transcription may garble. "When you encounter transcriptions like sora-sis or sorry-asis, recognize these as likely attempts to transcribe psoriasis. Apply medical spelling conventions to garbled technical terms rather than treating them as unknown words."

## Testing Voice Prompts With Real Speech Patterns

You cannot evaluate voice prompts using clean, written test cases. Real speech contains all the messiness discussed: disfluencies, false starts, corrections, ambiguity, and transcription errors. Your test sets must include authentic spoken language.

Collect real voice samples from your target user population. Different demographics, accents, speaking styles, and contexts produce different speech patterns. An elderly patient describing symptoms sounds different from a young technical user issuing commands.

Build test sets that include problematic cases: heavily accented speech, noisy audio, overlapping speakers, long pauses, rapid speech, and domain-specific terminology. Evaluate whether your prompts handle these edge cases gracefully or fail.

Run A/B tests comparing voice-optimized prompts against text-optimized prompts on the same transcribed content. The performance gap reveals how much voice-specific design matters for your use case.

The healthcare system built a test set of 500 authentic patient utterances representing their demographic mix: native English speakers, Spanish-English bilinguals, speakers of Chinese, Vietnamese, and Tagalog with varying English proficiency, elderly patients with slower speech, and young patients with rapid colloquial speech. Each utterance was manually annotated with correct extractions. The test set became the gold standard for prompt development and validation.

Testing also revealed unexpected failure modes. The system struggled with regional dialects, children speaking for elderly relatives, and patients describing symptoms while obviously in pain affecting speech patterns. Each failure informed prompt improvements or flagged cases requiring human intervention.

The next subchapter examines multimodal fusion, where you combine text, images, and structured data in single prompts to leverage complementary information sources.
