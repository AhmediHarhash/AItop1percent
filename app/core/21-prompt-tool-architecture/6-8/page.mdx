# 6.8 — Guardrails Architecture: Constitutional AI, Classifiers, and Rule Engines

In November 2025, a legal research platform deployed an AI assistant that helped lawyers draft contract clauses. The system used GPT-4 with carefully engineered prompts. Quality was excellent. Lawyers loved it. The product team was confident.

Six weeks into production, a junior associate at a major law firm used the system to draft a non-disclosure agreement. The AI generated a clause that, under California law, was unenforceable and would have invalidated the entire NDA. The associate didn't catch it. Neither did the reviewing partner. The client signed it.

Three months later, when a dispute arose, opposing counsel identified the flaw. The NDA was worthless. The client's confidential information had no protection. The law firm faced a malpractice claim. The legal research platform faced a lawsuit. The total exposure exceeded $2 million.

The post-mortem found that the prompt explicitly instructed the model to generate legally sound clauses. The model ignored this instruction in favor of generating fluent-sounding text. The prompt had no way to enforce the constraint. There was no mechanism to verify outputs against legal requirements before serving them to users.

They had built a sophisticated language model system but no guardrails to prevent it from generating dangerous outputs. Prompts express intent. Guardrails enforce constraints.

## What Guardrails Actually Are

Guardrails are independent safety and policy enforcement layers that sit outside your core prompt and model infrastructure.

Think of prompts as what you ask the model to do. Guardrails are what you prevent it from doing regardless of what the prompt says or what the model wants to generate.

A prompt might say "provide helpful legal information." A guardrail enforces "never generate clauses that violate California employment law" whether the model understands that instruction or not.

Prompts operate through persuasion. You write instructions and hope the model follows them. Guardrails operate through enforcement. They block, modify, or redirect outputs that violate constraints.

The critical distinction is that guardrails don't trust the model. Prompts assume the model will try to follow instructions. Guardrails assume the model might fail, hallucinate, or be manipulated. They verify and enforce constraints externally.

Guardrails run at multiple points in your system: before input reaches the model (input guardrails), before output reaches users (output guardrails), and during multi-step processes (runtime guardrails).

## The Guardrail Stack

Effective guardrail architecture uses multiple enforcement mechanisms in layers. No single guardrail type catches everything.

Layer one: Rule-based guardrails. These are deterministic checks implemented as code. "If output contains medical diagnosis keywords and user is not a licensed physician, block the output." "If generated SQL query contains DROP or DELETE, block execution." Rules are fast, transparent, and easy to reason about. They catch known patterns reliably but can't adapt to novel violations.

Layer two: Classifier-based guardrails. These are ML models trained to detect policy violations. Toxicity classifiers, PII detectors, bias classifiers, domain-specific safety classifiers. Classifiers catch patterns that are too complex or varied for rules. They're probabilistic—they return confidence scores, not binary decisions. They require training data and can drift over time.

Layer three: Constitutional AI guardrails. These use language models themselves to evaluate and enforce safety constraints. You define principles as natural language statements. Another model evaluates whether outputs violate those principles. Constitutional AI catches nuanced violations that rules and classifiers miss. It's flexible and interpretable but slower and requires careful prompt design for the evaluator model.

Layer four: Human-in-the-loop guardrails. For high-risk decisions, route to human review before finalizing. Humans are your ultimate guardrail for cases where automated checks are insufficient. They're expensive and slow but necessary for regulated domains.

The key is that these layers work together. Fast rules catch obvious violations. Classifiers catch statistical patterns. Constitutional AI catches nuanced violations. Humans catch edge cases and provide ground truth for improving automated guardrails.

## Rule Engines: Fast and Deterministic

Rule-based guardrails are your first line of defense. They're fast (microseconds to milliseconds), deterministic (same input always produces same result), and transparent (you know exactly why something was blocked).

Pattern matching rules detect specific strings or regex patterns. Block outputs containing social security numbers, credit card numbers, API keys. Block SQL queries with dangerous commands. Block code that imports restricted libraries.

Structural validation rules check format and structure. JSON outputs must parse successfully. Required fields must be present. Values must be within allowed ranges. Dates must be valid. This catches malformed outputs before they cause downstream errors.

Metadata validation rules check non-content properties. Response length must be within bounds. Generated code must not exceed token limits. Number of API calls must stay within rate limits. Latency must be below SLO thresholds.

Business logic rules enforce domain-specific constraints. In healthcare: "never suggest medication without dosage." In finance: "never provide specific investment recommendations without disclaimers." In e-commerce: "never recommend out-of-stock products."

The power of rules is their predictability. If you have a known constraint that can be expressed as logic, encode it as a rule. Don't rely on prompts to enforce it.

The limitation of rules is brittleness. Attackers and edge cases find ways around them. "SSN: XXX-XX-1234" gets blocked, but "the last four digits of your social are 1234" does not. Rules need constant updates as new evasion patterns emerge.

Combine rules with pattern learning. When you manually review blocked content and find false positives, refine the rules. When you find content that should have been blocked but wasn't, add new rules. Rules should evolve based on production traffic.

## Classifier-Based Guardrails

Classifiers detect violations probabilistically. They're trained on labeled examples of safe vs unsafe content and generalize to new cases.

Safety classifiers detect harmful content across categories. Toxicity classifiers catch hate speech, harassment, and offensive language. Bias classifiers detect discriminatory content. Violence classifiers catch graphic or threatening content. Self-harm classifiers detect content that encourages dangerous behavior.

Use multiple safety classifiers, not one monolithic model. Different categories require different training data and have different false positive tolerances. A single "unsafe content" classifier will optimize poorly across all safety dimensions.

Domain-specific classifiers catch violations unique to your industry. Medical misinformation classifiers for healthcare products. Financial fraud pattern classifiers for fintech. Legal liability classifiers for legal tech. Privacy violation classifiers for data-handling systems.

Train these on your own domain data, not just generic datasets. Off-the-shelf toxicity classifiers miss context-specific harm patterns. A generic classifier won't catch "this investment is guaranteed to triple in value" as financial misinformation.

PII classifiers detect personally identifiable information in outputs. Names, addresses, phone numbers, email addresses, account numbers, medical record numbers. These need high recall—missing PII is worse than false positives. Err on the side of blocking when uncertain.

Calibrate classifier thresholds by risk tier and violation type. High-risk violations (PII leakage, medical misinformation) use low thresholds (block on 0.5+ confidence). Medium-risk violations (mild toxicity) use medium thresholds (block on 0.7+ confidence). Low-risk violations (style violations) use high thresholds (block on 0.9+ confidence).

Implement classifier ensembles for critical decisions. Run the same content through multiple classifiers trained differently. If any classifier shows high confidence of violation, treat it seriously. Ensemble methods reduce false negatives at the cost of slightly higher false positives.

Monitor classifier drift over time. Classifiers degrade as content distributions change. What worked six months ago might miss new attack patterns today. Re-train quarterly at minimum, using production data to capture emerging patterns.

## Constitutional AI: Principles as Guardrails

Constitutional AI uses language models to evaluate outputs against stated principles. You define what the system should and shouldn't do in natural language. A separate model evaluates whether outputs comply.

The approach is: generate a response, then have an evaluator model critique it against constitutional principles, then either serve it or regenerate based on the critique.

Defining constitutional principles requires precision. Vague principles like "be helpful" don't constrain behavior. Specific principles like "never provide medical diagnoses or treatment recommendations; instead suggest consulting a qualified healthcare provider" actually guide evaluation.

Good constitutional principles are verifiable. "Outputs should be useful" is not verifiable—usefulness is subjective. "Outputs must cite sources for factual claims" is verifiable—you can check whether citations exist.

Good principles are actionable. "Don't be harmful" is too abstract. "Do not generate content that encourages self-harm, provides instructions for dangerous activities, or promotes violence against individuals or groups" is concrete enough to evaluate.

Structure principles by category: safety principles (what must never happen), accuracy principles (truth requirements), policy principles (business rules), tone principles (how to communicate).

For the legal research platform example, constitutional principles might include: "Contract clauses must comply with the specified jurisdiction's laws." "Never generate clauses that courts have ruled unenforceable." "Always include necessary legal disclaimers." "If uncertain about legal validity, state uncertainty rather than generating potentially invalid text."

The evaluator prompt receives: (1) the original user request, (2) the generated response, (3) the constitutional principles, (4) instruction to evaluate compliance and explain any violations.

If the evaluator identifies violations, you have three options: block the output and return an error, regenerate with the violation feedback incorporated into the prompt, or modify the output to fix the violation.

Constitutional AI is slower than rules or classifiers—you're running a second model inference. But it catches nuanced violations that pattern matching misses. It adapts to new scenarios without retraining. And it provides interpretable explanations for why content was blocked.

The limitation is that the evaluator model can also make mistakes. It might miss violations or flag false positives. Calibrate by validating evaluator decisions against human judgment. Build a test set of outputs with known violations and verify the evaluator catches them.

## Combining Guardrail Approaches

The most robust systems use all three approaches in sequence.

First pass: rule-based guardrails. These are fastest. Run deterministic checks first to catch obvious violations. If rules block the content, don't waste compute on classifiers or constitutional AI.

Second pass: classifier-based guardrails. If content passes rules, run it through trained classifiers. Classifiers catch statistical patterns that rules miss. If any classifier shows high-confidence violation, block without proceeding to constitutional AI.

Third pass: constitutional AI. If content passes rules and classifiers, evaluate it against constitutional principles. This catches nuanced violations that require reasoning about context and intent.

This cascade is efficient. Most content is safe and passes all three layers. Obvious violations get caught by fast rules. Statistical patterns get caught by classifiers. Only borderline or complex cases reach the expensive constitutional AI layer.

Log decisions at each layer. Track: how often does each layer catch violations? Which types of violations does each layer catch? Are there violations getting through all layers? This telemetry guides where to invest in improving guardrails.

## Runtime Guardrails for Multi-Step Systems

Agents and multi-step workflows need runtime guardrails that enforce constraints during execution, not just before or after.

Tool call guardrails verify that tool invocations are safe before executing them. Check: is the tool being called with valid parameters? Does the user have permission to execute this tool? Will this tool call modify critical data? Are there side effects that require human approval?

State transition guardrails enforce valid state changes. In a customer service workflow, you might allow transitions from "gathering information" to "providing answer" but not from "user authenticated" to "sharing other user's data." Define allowed and forbidden state transitions explicitly.

Resource guardrails enforce limits on what agents can consume. Maximum API calls per session. Maximum cost per task. Maximum execution time. Maximum data retrieved. Agents that hit limits should halt gracefully, not fail catastrophically.

Rollback guardrails detect when agent actions need undoing. If an agent executes three steps and the fourth fails with a critical error, rollback guardrails can undo previous steps. This requires designing tools with rollback capability.

Implement circuit breakers for agent actions with external effects. If an agent repeatedly tries and fails a dangerous operation, the circuit breaker stops further attempts and escalates to human review.

## Guardrails for Specific Risk Tiers

Different risk tiers require different guardrail depth.

Tier 1 (low risk, internal tools): Rule-based guardrails for basic safety. Maybe lightweight classifiers for PII. Constitutional AI is optional. Focus on preventing obvious failures, not comprehensive safety.

Tier 2 (medium risk, customer-facing): All three guardrail types. Rules for fast checks, classifiers for statistical safety, constitutional AI for nuanced policy compliance. Human review for edge cases.

Tier 3 (high risk, regulated domains): Defense in depth. Multiple classifiers per category. Constitutional AI with strict principles. Mandatory human review for certain outputs. Comprehensive audit logging. Regular third-party security reviews of guardrail effectiveness.

Tier 4 (critical, irreversible actions): Human-in-the-loop required. Automated guardrails flag issues but don't make final decisions. Humans verify every critical action before execution.

Don't under-invest in guardrails for your risk tier. A Tier 3 healthcare product with only rule-based guardrails is underprotected. The legal research platform learned this the hard way.

## Guardrails as Separate Infrastructure

Guardrails should be separate from your core application code, not embedded in prompts or application logic.

Build a guardrail service that receives content (input or output) and returns pass/fail/modify decisions with explanations. Your application calls this service before sending input to models and before serving output to users.

This separation provides several benefits. You can update guardrails without touching application code. You can test guardrails independently. You can reuse the same guardrails across multiple applications. You can monitor and optimize guardrail performance separately from application performance.

Version guardrails like you version application code. When you update a safety classifier or modify constitutional principles, that's a new guardrail version. Deploy new versions gradually, monitoring for increases in false positives or false negatives.

Implement feature flags for guardrails. You should be able to enable or disable specific guardrail rules, classifiers, or constitutional checks without code changes. This allows rapid response when a guardrail starts misbehaving.

Cache guardrail decisions when safe. If you generate the same FAQ answer repeatedly, run it through guardrails once and cache the result. Don't re-evaluate identical content. Cache by hash of content and version of guardrail configuration.

## Performance and User Experience

Guardrails add latency. Every check delays serving the response. Manage this carefully.

Optimize for the fast path. If 95% of content is safe, optimizing the "content passes all guardrails" path is more important than optimizing the "content gets blocked" path.

Run independent guardrails in parallel. If you have five different classifiers, run all five simultaneously instead of sequentially. Parallelize rule checks. Only sequential dependencies should run sequentially.

Set aggressive timeouts. A classifier that takes 300ms is too slow. Timeout at 200ms and make a safe default decision (usually: block if high-risk tier, allow if low-risk tier, based on your tolerance).

Provide meaningful feedback when guardrails block content. "Your request was blocked" frustrates users. "Your request included a request to generate medical advice. I can provide general health information but cannot diagnose conditions or recommend treatments" explains what happened and offers alternatives.

Implement graceful degradation. If a classifier is down, do you block all traffic (safe but terrible UX) or allow traffic through (better UX but risky)? Decide based on risk tier and specific guardrail. Non-critical guardrails can degrade gracefully. Critical guardrails fail closed.

## Monitoring and Continuous Improvement

Guardrails require active monitoring. You need visibility into what they're catching and what they're missing.

Track guardrail block rate by type. How often do rules block content? How often do classifiers? How often does constitutional AI? If block rate suddenly spikes or drops, investigate.

Sample blocked content for manual review. Randomly select 1% of blocked outputs and verify the block was correct. This catches false positives and validates guardrail accuracy.

Track content that passes guardrails but generates user complaints. This is your false negative signal. Users reporting "inappropriate content" or "wrong information" after content passed guardrails means your guardrails have gaps.

Build feedback loops from production. Use content that should have been blocked as negative training examples for classifiers. Use constitutional AI explanations as source data for new rules. Your production traffic teaches you what to improve.

Conduct regular red-team exercises specifically targeting guardrails. Give attackers the goal of generating policy-violating content that passes guardrails. Successful attacks reveal weaknesses to fix.

Measure guardrail latency at P50, P95, and P99. Median latency tells you typical performance. P99 tells you worst-case user experience. Set SLOs for guardrail latency and alert on violations.

## The Legal Research Platform's Rebuild

After the NDA incident, the legal research platform rebuilt their system with comprehensive guardrails.

They implemented rule-based checks for known unenforceable clause patterns by jurisdiction. "Non-compete clauses exceeding two years in California" gets blocked automatically.

They trained classifiers on legal case law to detect clauses courts have ruled invalid. This catches patterns similar to past unenforceable clauses even if not exact matches.

They deployed constitutional AI with principles derived from legal best practices. "Clauses must be consistent with public policy in the specified jurisdiction." "Confidentiality provisions must include reasonable scope and duration." "Non-solicitation clauses must be narrowly tailored."

They added mandatory human review for certain clause types: non-competes, IP assignment, indemnification. These are high-risk enough to require lawyer verification.

The rebuilt system is slower—guardrails add approximately 400ms to response time. But it hasn't generated a legally flawed clause since deployment. Users trust it. Law firms rely on it. The investment in guardrails prevented future incidents that would have destroyed the product.

Prompts express intent. Guardrails enforce constraints. You need both.
