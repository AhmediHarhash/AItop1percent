# 1.11 — Prompt Decomposition: Breaking Complex Tasks Into Prompt Chains

An insurance claims processing company built a single comprehensive prompt in September 2024 to handle their entire adjudication workflow. The prompt analyzed claim submissions, verified policy coverage, checked for fraud indicators, calculated payment amounts, and generated explanation letters. At 4,200 tokens with 73 distinct instructions, it represented their complete business logic.

The system worked acceptably for straightforward claims but failed unpredictably on complex cases. A claim involving both property damage and liability would sometimes calculate correct amounts but generate explanation letters referencing the wrong coverage sections. A claim with minor fraud indicators would sometimes be rejected entirely rather than flagged for review. The team spent six weeks tuning the prompt with marginal improvement.

They decomposed the monolithic prompt into a five-step chain: intake validation, coverage verification, fraud assessment, payment calculation, and letter generation. Each step had a focused prompt of 400-800 tokens. Accuracy improved from 87% to 96% within the first week of deployment. Error diagnosis became trivial because each step's output was visible. The team learned that complex tasks require decomposition, not longer prompts.

## The Monolithic Prompt Trap

A **monolithic prompt** attempts to handle an entire complex workflow in a single model call. This approach feels natural because it mirrors how you might explain the task to a human. You describe all the requirements, all the edge cases, and all the expected outputs in one comprehensive instruction set.

Monolithic prompts fail because they ask the model to maintain too much context and satisfy too many constraints simultaneously. The model must understand the task structure, apply domain rules, handle edge cases, and format output correctly, all while ensuring that decisions in one part of the task remain consistent with requirements in other parts.

Error diagnosis in monolithic prompts is nearly impossible. When the output is wrong, you cannot tell which part of the reasoning failed. Did the model misunderstand the input? Misapply a rule? Generate correct intermediate results but format them incorrectly? The entire process is opaque.

Maintenance becomes progressively harder as requirements evolve. Adding a new rule means inserting it into the already-complex instruction set and hoping it does not interfere with existing logic. You cannot test the new rule in isolation because it is embedded in the larger context.

## Decomposition by Cognitive Function

The most natural decomposition boundary is cognitive function. Different steps in your workflow require different types of reasoning. One step extracts structured data from text. Another applies business rules to that data. A third generates natural language output.

These functions have different accuracy profiles and failure modes. Data extraction fails when inputs are ambiguous or malformed. Rule application fails when rules conflict or edge cases are unclear. Natural language generation fails when tone or structure requirements are complex. Separating them allows you to tune each function independently.

A customer support workflow might decompose into: intent classification, information extraction, knowledge retrieval, response generation, and tone adjustment. Each step has clear inputs and outputs. Each can be tested independently. Each can use different prompts optimized for its specific function.

The cognitive function boundary also aligns with how you think about the task. You naturally describe complex tasks as sequences of steps, not as one undifferentiated blob of requirements. Prompt architecture should mirror this natural decomposition.

## When to Split a Prompt

You consider splitting when any of these patterns appear. First, your prompt exceeds 1,500 tokens and contains multiple distinct instruction blocks. Second, you find yourself using phrases like "after doing X, then do Y" repeatedly in the prompt. Third, failure analysis reveals that different types of errors appear in different sections of the output.

You split when the task has clear intermediate artifacts. If you can describe step outputs that would be useful to see independently, those outputs should be actual intermediate results in a chain. This visibility helps with debugging and creates natural testing points.

You split when different steps have different accuracy requirements. A resume screening workflow might need 98% accuracy for extracting candidate names and contact information, 90% accuracy for assessing skill matches, and 85% accuracy for rating cultural fit. A single prompt cannot apply different thresholds to different outputs.

You avoid splitting when the cognitive overhead of managing the chain exceeds the benefit of decomposition. A task that naturally takes 400 tokens and has no clear intermediate artifacts should stay in one prompt. Decomposition adds latency, complexity, and failure points. Apply it when those costs are justified.

## Context Passing Between Chain Steps

Each step in a decomposed chain needs context from previous steps. The simplest approach passes the raw output of step N as input to step N+1. This works when outputs are well-structured and steps need complete information from predecessors.

Selective context passing includes only relevant information in subsequent prompts. If step 1 extracts 15 fields from a document but step 2 only needs 3 of them, pass only those 3. This reduces token usage and focuses the model's attention on relevant inputs.

Context enrichment adds information at each step. Step 1 might extract entities. Step 2 retrieves additional data about those entities from a database. Step 3 receives both the original extractions and the enriched data. Each step has more information than the last.

The context passing strategy affects error propagation. If step 1 produces a small error and you pass its complete output to step 2, the error might compound. If you pass only selected fields, you might miss information that would help step 2 correct the error. Balancing these concerns requires understanding your specific error patterns.

## Error Propagation in Chains

Errors in chain architectures propagate and amplify. If step 1 has 95% accuracy and step 2 has 95% accuracy, the combined system does not have 95% accuracy. It has at most 90.25% accuracy, and likely less due to error compounding.

Error compounding occurs when a mistake in step N causes step N+1 to make additional mistakes. An entity extraction step might misidentify "Apple" as a fruit rather than a company. The next step, which retrieves contextual information, fetches fruit nutrition data instead of company financials. The third step, which generates a summary, produces nonsense because it is working with wrong information.

You mitigate propagation through validation steps between chains. After extraction, validate that required fields are present and values are within expected ranges. After calculations, check that results satisfy basic sanity checks. These validation steps catch errors early before they cascade.

Confidence scoring at each step allows subsequent steps to handle uncertain inputs differently. If step 1 indicates low confidence in an extraction, step 2 can be more conservative in how it uses that information or flag the case for human review.

## Sequential vs Parallel Decomposition

**Sequential decomposition** chains steps linearly: step 1 feeds step 2, which feeds step 3. This models workflows where each step depends on previous results. Document analysis might go: extract entities, classify document type, route to appropriate processing logic.

**Parallel decomposition** splits work into independent streams that can execute simultaneously. A resume screening workflow might extract work history, extract education history, and extract skills in three parallel steps, then combine results in a final synthesis step.

Parallel decomposition reduces latency when steps are independent. Three steps that each take 2 seconds can complete in 2 seconds if run in parallel versus 6 seconds if run sequentially. This matters for user-facing features where every second of latency affects experience.

The tradeoff is coordination complexity. Parallel steps need a merge operation that combines their outputs. If one parallel branch fails, you need to decide whether to proceed with partial results or fail the entire operation. If branches have dependencies that you did not recognize, parallel execution produces incorrect results.

## Fan-Out and Fan-In Patterns

**Fan-out** patterns send one input to multiple parallel processing steps. A contract analysis workflow might send the contract to separate prompts that extract financial terms, identify legal risks, assess compliance requirements, and check for unusual clauses. Each prompt specializes in one aspect of the analysis.

Fan-out enables specialization. Instead of one generalist prompt trying to handle all aspects of contract analysis, you have multiple specialist prompts, each optimized for a specific sub-task. This typically improves accuracy because each prompt can focus on fewer requirements.

**Fan-in** patterns combine results from multiple parallel steps. After the contract analysis fan-out, a synthesis step receives all the specialized analyses and produces a unified summary with prioritized findings and recommended actions.

The fan-in step is critical for maintaining coherence. Individual specialist steps might produce correct outputs that do not combine well. The financial analysis might flag cost concerns while the legal analysis flags liability concerns, and the synthesis step must prioritize them appropriately and avoid contradictions.

## Designing Step Boundaries

Good step boundaries have clear inputs, clear outputs, and minimal coupling between steps. A step should be independently testable: you can create synthetic inputs, run the step in isolation, and verify outputs without executing the entire chain.

Boundaries often align with data transformations. One step transforms unstructured text to structured data. Another transforms structured data to business domain objects. A third transforms domain objects to user-facing outputs. Each transformation is a natural boundary.

Poor boundaries create tight coupling between steps. If step 2 needs to understand implementation details of how step 1 produced its output, the boundary is wrong. If changing step 1's logic requires changing step 3 even though step 2 is between them, the boundaries are wrong.

You validate boundaries by attempting to swap implementations. If you can replace step 2 with a completely different approach without changing steps 1 or 3, the boundaries are good. If any change ripples through multiple steps, consider redrawing the boundaries.

## Managing State Across Chains

Some workflows require maintaining state across multiple model calls. A multi-turn conversation needs context from previous turns. A long document analysis might process sections incrementally while maintaining aggregate state.

Explicit state management stores intermediate results in a data structure that flows through the chain. Each step reads state, performs its operation, and writes updated state. This makes state visible and debuggable but requires careful design to avoid state bloat.

Implicit state management relies on model context windows to maintain state. You include relevant history in each prompt. For conversations, this means including previous turns. For documents, this means including previously processed sections plus summaries of current understanding.

State scope boundaries prevent state from growing unbounded. A customer service conversation might maintain state for the current issue but reset when switching to a new issue. A document analysis might maintain state per section but not across the entire document.

## Testing Decomposed Systems

Decomposed chains create natural testing boundaries. You can unit test each step independently with synthetic inputs. You can integration test step pairs to verify correct context passing. You can end-to-end test the full chain with realistic scenarios.

Unit tests for individual steps are fast and focused. You create input examples that cover edge cases for that specific step and verify outputs meet requirements. This catches most errors early in development before they get to integration testing.

Integration tests verify that steps compose correctly. They ensure that step 1's output format matches step 2's input expectations, that context passing preserves necessary information, and that errors are handled appropriately at boundaries.

End-to-end tests validate the complete workflow with realistic inputs. These tests are slower and more brittle but necessary to catch emergent behaviors that only appear when all steps interact. They typically use production-like data and verify business outcomes rather than intermediate artifacts.

## Latency Tradeoffs in Decomposition

Decomposed chains add latency because each step requires a separate model call. A task that took 3 seconds in a monolithic prompt might take 8 seconds in a four-step chain if steps run sequentially. This latency matters for user-facing features where response time affects experience.

You minimize latency through parallel execution where possible. If steps 2 and 3 both depend only on step 1's output, run them in parallel. If steps have partially overlapping dependencies, consider restructuring to enable more parallelism.

Streaming responses can reduce perceived latency. Instead of waiting for the entire chain to complete, show users results from early steps while later steps process. A document analysis might show extracted entities immediately while classification and summarization complete in the background.

Model selection per step balances latency and accuracy. Early steps that do simple extraction might use faster models like Claude 3.5 Haiku. Later steps that require complex reasoning might use Claude Opus 4. This hybrid approach optimizes total workflow latency and cost.

## When Monolithic Prompts Win

Decomposition is not always the answer. Some tasks genuinely require maintaining rich context across all decisions. Some workflows are so tightly coupled that step boundaries add more complexity than they remove.

Creative tasks like writing often resist decomposition. Breaking a blog post into "generate outline, write introduction, write body, write conclusion" produces disjointed outputs because each part needs awareness of the others. The natural flow of ideas requires holistic processing.

Tasks with tight feedback loops between components work better as monoliths. A code generation task that needs to maintain consistency across function definitions, tests, and documentation might produce more coherent results in a single pass than in decomposed steps.

Very short workflows with simple logic do not benefit from decomposition. If your entire task fits comfortably in 600 tokens and has no clear intermediate artifacts, keep it monolithic. Decomposition overhead would exceed any benefit.

## Decomposition as Iteration Strategy

Teams often start with monolithic prompts during exploration and decompose as they understand the task better. The first version clarifies requirements. The second version identifies natural boundaries. The third version implements proper decomposition.

This progression is healthy because premature decomposition can lead to wrong boundaries that are expensive to refactor. Understanding the task end-to-end first allows you to identify meaningful decomposition points based on actual failure modes and coupling patterns.

Iterative decomposition also allows gradual migration. You might decompose one problematic section first while keeping the rest monolithic. Validate that the decomposition improves outcomes. Then decompose another section. This reduces risk compared to redesigning the entire system at once.

Understanding when and how to decompose prompts gives you architectural flexibility to manage complexity. The final topic in this chapter examines how risk tier requirements shape these architectural decisions across different deployment contexts.

# 1.12 — Prompt Architecture Patterns Across Risk Tiers