# 7.12 â€” Tool Testing: Unit, Integration, and End-to-End

In September 2024, a travel booking platform deployed a new get_flight_prices tool that passed all unit tests. The tool correctly queried the pricing API and returned formatted results. In production, users reported prices that were consistently 30% higher than what the booking page showed. Investigation revealed the tool was querying a different API endpoint than the booking flow used. The unit tests mocked the API, so they never caught this. The integration testing suite didn't include tool-to-API validation. The company issued refunds totaling $240,000 and rebuilt their testing strategy from scratch.

Tools sit at the intersection of your application logic, external APIs, model inference, and user experience. Testing requires validating each layer independently and in combination. Unit tests verify tool logic in isolation. Integration tests verify tools work with models and external services. End-to-end tests verify complete user flows involving tool use. Skip any layer and you're deploying tools that work in theory but fail in reality.

## Testing Tools in Isolation

Unit testing tools means validating tool logic without models or external dependencies. You call the tool function directly with test inputs and verify outputs match expectations. This is standard software testing, but tool-specific considerations apply.

Schema validation must be tested. Every tool declares input parameters with types and constraints. Your unit tests should verify that the tool rejects invalid inputs: wrong types, missing required parameters, values outside allowed ranges. If get_flight_prices requires origin and destination as strings, test that it rejects numeric values or null.

Parameter combination testing catches edge cases. Some parameters interact in ways that create invalid states. A date range tool might accept start_date and end_date, but end_date before start_date is invalid. Your unit tests should verify the tool detects and rejects impossible parameter combinations.

Return value structure testing ensures tools return data in the expected format. If your tool is supposed to return an object with fields price, currency, and timestamp, test that all fields are present, have correct types, and contain valid data. Missing fields or wrong types break model consumption of results.

Error handling paths need explicit testing. What happens when the tool receives invalid input? When an external dependency fails? When unexpected data appears? Unit tests should trigger error conditions deliberately and verify the tool returns structured error responses, not crashes or exceptions that bubble up unhandled.

Boundary conditions reveal bugs. Test with minimum and maximum values, empty strings, zero-length arrays, null values where optional parameters exist. Tools often have implicit assumptions that break at boundaries. A search tool might work with 1 to 100 results but crash with 0 results or 10,000 results.

## Mocking External Services

Tools typically depend on external services: databases, APIs, microservices. Unit testing requires mocking these dependencies so tests run quickly and reliably without external infrastructure. Mocking introduces its own risks, primarily mocks diverging from real service behavior.

The best mocks are based on real service responses. Capture actual API responses from your external services and use them as mock data. When the service returns {"price": 299, "currency": "USD"}, your mock returns exactly that. This keeps mocks realistic and catches issues when your tool expects different response structures.

Contract testing validates mocks against real services. Periodically run tests against both mocks and real services, verifying they return compatible responses. If mock tests pass but real service tests fail, your mocks have drifted from reality. This catches API changes that break your tools before they reach production.

Mock versioning mirrors service versioning. When an external API releases v2 with breaking changes, create separate mocks for v1 and v2. Your tests can then verify tool behavior against both versions, ensuring compatibility or catching required updates.

Failure mode mocking is critical. External services don't just return success or failure. They timeout, return partial results, return error codes with varying specificity, and have rate limits. Your mocks should simulate these conditions. Test that your tool handles timeout gracefully, not by hanging indefinitely.

Some teams maintain mock service libraries shared across tools. Instead of every tool creating its own mocks for common_api, a central mock library provides standard mocks. This reduces duplication and ensures consistency when multiple tools depend on the same service.

## Integration Testing with Models

Unit tests verify tools work in isolation. Integration tests verify tools work with models. This means testing the complete flow: model decides to call tool, tool executes, model receives results, model generates response. Integration testing surfaces issues invisible in unit tests.

The simplest integration test uses fixed prompts that should trigger specific tool calls. You provide a prompt like "What's the price of flight AA123?" and verify the model calls get_flight_prices with the correct parameters. If the model doesn't call the tool, or calls it with wrong parameters, the test fails.

Result interpretation testing verifies models use tool results correctly. After the tool returns price data, does the model present it accurately? Does it quote the price directly or approximate it? Does it include currency information? Integration tests check that model responses ground themselves in tool outputs, not hallucinated data.

Tool chain testing validates multi-tool workflows. Some user requests require calling multiple tools in sequence. "Book the cheapest flight to Paris" might call search_flights, get_flight_prices, and book_flight. Integration tests verify the model orchestrates these calls correctly, passing results from one tool as inputs to the next.

Error recovery testing is crucial at the integration level. When a tool call fails, does the model recover gracefully? Does it try alternative tools, explain the failure to the user, or hang? Integration tests deliberately trigger tool failures and verify model behavior remains acceptable.

Prompt variation testing checks robustness. Users don't phrase requests uniformly. "What's flight AA123 cost?", "How much is AA123?", and "Price for AA123" should all trigger the same tool call. Integration tests use multiple phrasings for each tool to verify models reliably map user intent to correct tools.

## End-to-End Tool Chain Testing

End-to-end tests verify complete user journeys involving tools. This includes user input, model processing, tool execution, response generation, and user interface presentation. E2E tests catch issues that unit and integration tests miss, particularly around user experience and system integration.

User workflow tests simulate realistic multi-turn conversations. A user asks about flights, receives options, asks for details on one option, and books it. The test verifies each turn works correctly, maintains context properly, and completes the workflow successfully. This catches state management issues between turns.

UI integration testing verifies tool results display correctly in your interface. The tool returns structured data, the model formats it for users, and your UI renders it. E2E tests check that prices appear in the right currency format, dates display in the user's locale, and errors show user-friendly messages, not raw API error codes.

Authentication and authorization flow testing validates tool access control in realistic scenarios. A user logs in, receives a token, makes requests that trigger tool calls, and tools enforce permissions correctly. E2E tests verify the entire security flow, not just that tools check permissions in isolation.

Performance testing under realistic conditions matters. Unit tests call one tool. Production might call ten tools in one conversation. E2E tests with realistic tool usage patterns reveal performance issues: slow cascading tool calls, timeout problems, resource exhaustion when multiple tools run concurrently.

Error presentation testing checks how errors surface to users. When a tool fails, users should see helpful messages, not stack traces. E2E tests trigger various error conditions and verify the UI presents them appropriately. "Flight prices unavailable, please try again later" is much better than "API returned 503."

## Test Coverage for Tool Systems

Measuring test coverage for tool systems requires tracking multiple dimensions. Code coverage shows what percentage of tool implementation code runs during tests. This is necessary but insufficient. You also need functional coverage: what percentage of tool capabilities, parameter combinations, and error paths have tests.

Parameter coverage metrics track which parameter combinations are tested. A tool with five parameters where each parameter has three possible values creates a large combination space. Exhaustive testing is impractical, but you need systematic coverage of common cases, edge cases, and known risky combinations.

Model interaction coverage measures which tools get tested with models and which don't. You might have unit tests for every tool but integration tests for only half. Tools without integration testing are risky in production because you don't know if models can actually use them effectively.

Error path coverage specifically measures testing of failure modes. What percentage of possible error conditions have tests? Tools might have twenty different error conditions: invalid parameters, service timeouts, rate limits, authentication failures. Each needs explicit tests.

Some teams implement coverage gates in CI/CD. Pull requests that add or modify tools must include tests achieving minimum coverage thresholds. 80% code coverage, plus integration tests for new tools, plus E2E tests for new workflows. This prevents untested tools from reaching production.

Coverage dashboards show tool testing health across your system. Which tools have unit tests? Integration tests? E2E tests? Which tools haven't been tested in the last month? Coverage visualization helps prioritize testing efforts and identifies gaps.

## Automated Testing in CI/CD

Tool tests should run automatically on every code change. CI/CD pipelines run unit tests first because they're fast. Integration tests run next because they're slower. E2E tests run last because they're slowest and most resource-intensive. Failed tests block deployment.

Unit test runs take seconds or minutes. They use mocks and don't require external services. These run on every commit to every branch. Fast feedback loops catch tool bugs immediately during development.

Integration test runs take minutes to tens of minutes. They require model API access but usually still use mocked external services. These run on pull requests and before merging to main branches. Integration test failures indicate model compatibility issues that need resolution before merge.

E2E test runs take tens of minutes to hours. They require full system deployment, real external service access or realistic staging environments, and complete user workflows. These run before production deployments and sometimes on schedules overnight. E2E test failures block releases.

Parallel test execution reduces pipeline times. Run unit tests for all tools simultaneously. Shard integration tests across multiple runners. This keeps CI/CD fast even as tool count grows. Sequential execution becomes prohibitively slow at scale.

Test result reporting needs to be actionable. When tool tests fail, developers need to know which tool failed, which test failed, what the failure mode was, and ideally how to reproduce it locally. Good CI/CD systems surface this information clearly rather than burying it in logs.

## Testing Model Non-Determinism

Models are non-deterministic. The same prompt might trigger different tool calls on repeated runs. This makes traditional testing challenging. You can't assert that a specific prompt always produces a specific tool call. You need testing strategies that account for model variability.

Probabilistic testing runs the same test multiple times and verifies success rate. If a prompt should trigger get_flight_prices, run it ten times and verify it triggers the correct tool at least eight times. This accepts some variability while catching systematic failures.

Assertion loosening validates outcomes rather than exact behaviors. Instead of asserting the model calls get_flight_prices with exactly these parameters, assert it calls some flight-related tool and includes flight AA123 in parameters somehow. This accommodates variation in model reasoning while ensuring functional correctness.

Temperature and sampling controls reduce variability during testing. Run tests with temperature=0 or very low temperatures to make model behavior more deterministic. This doesn't eliminate variability entirely, but it reduces it enough that tests become more reliable.

Golden path testing focuses on core workflows that must work reliably. Identify the ten most important tool-based workflows in your system. Test these exhaustively with strict assertions. Allow more looseness in testing edge cases or less critical features, but core functionality needs high reliability.

Some teams implement behavioral testing rather than output testing. Instead of checking exact tool call parameters, verify the eventual outcome. If the user asks to book a flight and goes through the workflow, verify a booking exists at the end, regardless of exact tool call sequences that led there.

## Regression Testing After Changes

When you modify a tool, you need regression tests that verify you didn't break existing functionality. This requires a comprehensive test suite capturing current behavior before changes, then verifying behavior remains consistent after changes.

Snapshot testing captures tool outputs for given inputs and treats them as expected behavior. When you change tool implementation, tests compare new outputs to snapshots. Differences flag potential regressions. You review differences and either fix the regression or update snapshots if behavior should change.

Conversation replay testing is powerful for tools integrated with models. Record production or test conversations that exercise tools, then replay them after tool changes. Verify the conversations complete successfully and produce similar outcomes. This catches regressions in tool behavior, model integration, or conversation flow.

Before-after comparison testing runs the same tests against old and new tool implementations. Tools return price=$299 before changes and price=$299.00 after. Is this a regression? It depends on your formatting requirements. Comparison testing surfaces these differences for review.

Backward compatibility testing explicitly verifies old tool call patterns still work. If you add a parameter, test that tool calls omitting the new parameter still succeed. If you change return format, test that old response parsing still works or at least fails gracefully.

Production monitoring acts as ongoing regression testing. When you deploy tool changes, monitor error rates, response times, and tool call success rates. Sudden increases after deployment indicate regressions that escaped testing. Automated alerts catch these quickly before widespread user impact.

## Testing External API Integration

Tools often wrap external APIs. Testing requires verifying your tool correctly translates user/model inputs to API calls and API responses to tool outputs. This is integration testing at the API boundary.

API contract testing validates your tool's assumptions about the API. If your tool expects the API to return {"price": number, "currency": string}, write tests that call the real API and verify responses match expectations. When APIs change response formats, contract tests fail immediately.

Mock-vs-real comparison tests run the same tool call against both mocked and real APIs. If results differ significantly, either your mocks are outdated or the API behavior isn't what you expected. This catches both mock drift and misunderstandings about API behavior.

API error handling tests deliberately trigger API errors and verify your tool handles them. Call the API with invalid credentials to trigger 401. Exceed rate limits to trigger 429. Send malformed requests to trigger 400. Verify your tool returns structured errors to the model, not crashes.

API version testing validates tools work with different API versions. If the external API has v1 and v2, test your tool against both. This is critical during API migration periods when you need backward compatibility.

Sandbox and staging environment testing uses non-production API environments when available. Many APIs provide sandbox endpoints for testing. Use these for automated tests rather than hitting production APIs. This prevents test traffic from affecting production metrics or incurring costs.

## Test Data Management

Tool tests need realistic data. The data you use in tests affects test validity. Synthetic test data might not reveal bugs that real data would. But using production data in tests raises privacy and security concerns.

Anonymized production data sampling creates realistic test datasets. Take production data, anonymize sensitive fields, and use it in tests. This gives tests realistic data distributions and edge cases while protecting user privacy. Update test data periodically to keep it current.

Synthetic data generation creates test data programmatically. If your tool processes flight searches, generate random flight numbers, airports, dates, and prices. This scales easily and avoids privacy issues, but might miss edge cases present in real data.

Edge case libraries collect specific inputs known to cause problems. If certain airport codes triggered bugs historically, include them in test data. If certain date ranges caused issues, test them explicitly. This targeted approach supplements broader random or sampled data.

Data fixtures provide stable reference datasets for deterministic testing. These don't change between test runs, making tests reproducible. Fixtures work well for unit and integration tests where you need predictable inputs and outputs.

Test data versioning tracks test datasets over time. When you update test data, version it. Tests can specify which test data version they use. This prevents tests breaking mysteriously when someone updates shared test data.

The next chapter examines how to design custom tools for your specific domain, wrapping your unique APIs and business logic in model-friendly tool interfaces.
