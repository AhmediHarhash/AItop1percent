# 8.4 — XML, YAML, CSV, and Non-JSON Structured Outputs

In June 2025, a financial data aggregation company spent six weeks debugging integration issues with their AI-powered regulatory filing parser. The system extracted structured data from SEC filings and output JSON, which then had to be transformed into XML for submission to their compliance system. The transformation layer was brittle, error-prone, and slow. Field mappings between JSON and XML were hardcoded, nested structures didn't translate cleanly, and validation failures happened at the XML generation stage rather than during extraction. After repeated integration failures, the team realized they had made a fundamental error: they were generating JSON only because it was the default structured output format, not because it was the right format for their use case. They rebuilt the system to generate XML directly from the model, eliminated the transformation layer, and reduced integration failures by 90 percent. The lesson was simple: JSON is convenient and universal, but it's not always optimal.

**Format selection** is a design decision that affects reliability, performance, integration complexity, and downstream processing. Different formats have different strengths: JSON for web APIs and general data exchange, XML for document markup and enterprise systems, YAML for human-readable configuration, CSV for tabular data and spreadsheet processing. Choosing the right format for your use case reduces engineering complexity and improves system robustness.

## When XML Is the Right Choice

XML was the dominant structured data format before JSON, and it remains deeply embedded in enterprise systems, legacy applications, and specific domains like finance, healthcare, and legal.

**Enterprise system integration** often requires XML. SAP, Oracle, Microsoft Dynamics, and countless ERP systems use XML for data interchange. Healthcare uses HL7 XML for clinical data exchange. Financial services use XBRL for regulatory reporting. If your AI system needs to integrate with these systems, generating XML directly is simpler than generating JSON and transforming it.

Transformation layers between formats introduce failure points. XML and JSON have different type systems, different ways of representing attributes versus elements, different handling of namespaces, and different conventions for arrays and null values. Transforming between them requires custom logic that can break when data structures change. Generating the target format directly eliminates transformation complexity.

**Document-oriented output** with mixed content benefits from XML's strengths. JSON represents pure data structures. XML represents documents with nested markup, attributes, and mixed text-and-structure content. If you're extracting structured information from legal documents where you need to preserve section structure, annotations, and references, XML's document model is more natural than JSON's data model.

For example, extracting clauses from a contract might produce XML with a clause element carrying an id attribute of 5.2 and type of liability, wrapping the clause text, with nested emphasis and amount elements preserving semantic meaning. This preserves structure, annotations, and semantic markup in a way that JSON (which would need to represent this as nested objects with type annotations) cannot do as cleanly.

**Schema validation with XSD** provides stronger guarantees than JSON Schema in some contexts. XSD (XML Schema Definition) is a mature, standardized schema language with rich validation capabilities: complex type constraints, pattern matching, cardinality rules, and cross-field dependencies. Some industries require XSD compliance for regulatory or interoperability reasons. Generating XML that validates against required XSD schemas directly is more reliable than generating JSON and hoping transformation preserves compliance.

**Namespace support** in XML enables mixing vocabularies from different standards in a single document. If you're generating regulatory reports that need to include elements from multiple XML namespaces—financial data in one namespace, company metadata in another, regulatory annotations in a third—XML handles this natively. JSON has no native namespace concept.

## Prompting Models for XML Output

Language models are trained on massive text corpora that include XML, so they can generate valid XML when prompted appropriately.

**Explicit format specification** in prompts is critical. Saying "generate XML output" is too vague. Specify the root element, required child elements, attribute usage, and namespace requirements. Tell the model to generate an XML document with a report root element, containing company, period, and financials child elements. Direct it to use attributes for IDs and metadata, and ensure all tags are properly closed.

**Schema-driven prompting** provides the XSD or a description of the required structure. "Generate XML matching this schema: [schema snippet]. The output must validate against this XSD." This helps the model understand complex structural requirements. For particularly strict schemas, you might provide example XML documents that validate correctly.

**Validation emphasis** reminds the model to generate well-formed, valid XML. "Ensure all tags are properly nested and closed. Use proper XML escaping for special characters. Do not include comments or processing instructions unless required." Models sometimes generate malformed XML (unclosed tags, unescaped special characters) without explicit prompting.

**Namespace declaration** when required: "Include namespace declarations in the root element: xmlns:fin='http://example.com/finance' xmlns:reg='http://example.com/regulatory'. Use namespace prefixes for all elements." Without explicit guidance, models might omit namespace declarations even when they're required.

## CSV for Tabular Data

When your structured output is fundamentally tabular—rows and columns with uniform structure—CSV is often simpler and more efficient than JSON or XML.

**Spreadsheet integration** is CSV's primary strength. If downstream consumers will open the data in Excel, Google Sheets, or other spreadsheet tools, CSV is the native format. JSON and XML require transformation or specialized importers. CSV opens directly and can be manipulated with familiar spreadsheet operations.

**Database loading** often uses CSV as the import format. Most databases have optimized bulk CSV loaders. If you're extracting data that will be inserted into tables—transaction records, customer information, inventory data—generating CSV directly enables fast database loading without intermediate parsing layers.

**Simple structure enforcement** is natural in CSV. Each row has the same columns in the same order. There are no nested structures, no optional fields in some rows but not others, no complex hierarchies. This simplicity is limiting but also clarifying. If your data fits the tabular model, CSV removes degrees of freedom that would allow malformed outputs.

For example, extracting invoice line items might produce: invoice_id,line_number,description,quantity,unit_price,total, INV-1001,1,Widget A,10,25.00,250.00, INV-1001,2,Widget B,5,50.00,250.00. This is cleaner than nested JSON arrays of line item objects when the consumer is a spreadsheet or database.

**Edge case handling** requires care with CSV. Special characters (commas, quotes, newlines) must be escaped or quoted. Empty fields must be represented consistently (empty string, "NULL", or some sentinel). Numeric precision and date formats must be standardized. These issues exist with any format, but CSV's lack of type information makes them more prominent.

Prompting for CSV: "Generate CSV output with these columns: invoice_id, line_number, description, quantity, unit_price, total. Include a header row. Enclose fields containing commas in double quotes. Use empty strings for missing values. Format numbers with two decimal places."

## YAML for Human-Readable Configuration

YAML excels at representing configuration data that humans need to read, write, and edit. Its syntax is cleaner and more readable than JSON or XML.

**Configuration extraction** benefits from YAML output. If you're using AI to generate application configuration files, infrastructure-as-code definitions, or deployment specifications, YAML is often the standard format (Kubernetes manifests, Docker Compose files, GitHub Actions workflows, Ansible playbooks).

Generating YAML directly means the output can be used immediately without format conversion. Developers can review the AI-generated configuration in the same format they're accustomed to working with. Diffs and version control are cleaner with YAML's syntax than with JSON's verbose bracket notation.

**Comment preservation** is possible in YAML but not JSON. If your AI-generated output should include explanatory comments—why certain configuration values were chosen, what alternatives were considered—YAML supports inline comments. This creates self-documenting output that JSON cannot provide.

**Multi-line string handling** is cleaner in YAML. If your structured output includes long text fields—descriptions, documentation, scripts—YAML's multi-line string syntax (using | or {">"} markers) is more readable than JSON's quoted strings with escaped newlines.

For example, generating a deployment configuration might produce YAML: service: name: web-api, replicas: 3, image: company/api:v2.1, resources: cpu: 500m, memory: 512Mi, environment: LOG_LEVEL: info, CACHE_TTL: 3600. This is more readable than the equivalent JSON with nested braces and quotes.

**Prompting for YAML** requires emphasizing indentation and syntax: "Generate valid YAML output. Use 2-space indentation. Use colons followed by spaces for key-value pairs. Use dashes for list items. Do not use JSON-style braces or brackets."

## Choosing the Right Format

Format selection depends on several factors: downstream consumers, domain conventions, structure complexity, human readability requirements, and integration constraints.

**Start with your consumers.** What format do downstream systems expect. If you're feeding a REST API, use JSON. If you're feeding an enterprise system, check its XML schema. If you're loading a database, use CSV. If you're generating configuration files, use YAML. Choosing the format your consumers need eliminates transformation layers.

**Consider structure complexity.** Deeply nested hierarchies with optional fields and mixed types work well in JSON or XML. Flat tabular data works well in CSV. Configuration trees work well in YAML. Don't force tabular data into JSON just because JSON is familiar, and don't force hierarchical data into CSV just because it's simple.

**Account for human interaction.** If humans need to review, edit, or debug outputs, readability matters. YAML is most readable for configuration. XML with proper indentation is readable for documents. JSON is readable for simple data structures. CSV is readable for tables. Format choice affects how easy it is for humans to work with AI outputs.

**Respect domain conventions.** Healthcare systems use HL7 XML. Financial systems use XBRL. DevOps uses YAML. Data science uses CSV or JSON. Following domain conventions reduces friction and miscommunication. Going against conventions requires justification and adds integration complexity.

**Test format generation quality.** Different models have different strengths with different formats. GPT-4 might generate cleaner JSON than XML. Claude might handle YAML better than CSV. Test your specific model-format combination to understand quality and reliability before committing to production.

## Validation for Non-JSON Formats

Each format requires format-specific validation.

**XML validation** uses XSD schemas, DTD definitions, or RelaxNG. Libraries like libxml2 (C), lxml (Python), and System.Xml (C#) provide validation. XML validation can catch structural errors, type mismatches, required element omissions, and attribute constraint violations. XML validation is generally more strict and expressive than JSON validation.

**CSV validation** is more ad-hoc because CSV lacks a standard schema language. You validate column counts, data types (parsing strings to numbers, dates, etc.), value ranges, and referential integrity (if one column references IDs from another dataset). Libraries like pandas (Python) and csv-validator help with validation.

**YAML validation** can use JSON Schema (YAML is a superset of JSON) or custom validation logic. YAML parsers catch syntax errors. Application-specific validation checks that required keys are present, values have correct types, and configuration is internally consistent.

Validation complexity increases with format complexity. XML with namespaces and XSD is harder to validate than flat CSV. Budget engineering time accordingly.

## Transformation Pipelines When You Need Multiple Formats

Sometimes you need the same data in multiple formats for different consumers.

**Canonical format selection** means choosing one format as the source of truth and transforming to others as needed. Usually JSON is canonical because it's well-supported by AI APIs and easy to manipulate programmatically. You generate JSON from the model, validate it, then transform to XML, CSV, or YAML as needed.

Canonical format centralization means you only need to validate and correct one format. Transformations to other formats are deterministic and testable. The risk is that transformation logic can introduce errors or lose information when formats have different expressive power.

**Parallel generation** requests multiple formats simultaneously from the model. You prompt for JSON and XML and CSV in separate requests. This avoids transformation errors but requires more API calls, more validation logic, and creates consistency challenges when different format outputs conflict.

**Format negotiation** lets consumers specify their preferred format. Your API accepts a format parameter (JSON, XML, CSV, YAML) and generates that format on demand. This provides flexibility but requires implementing and maintaining generation and validation logic for multiple formats.

## Models and Format Support

Not all models handle all formats equally well.

**GPT-4 and Claude 3.5** handle JSON extremely well due to native JSON mode and schema support. They handle XML competently with proper prompting. They can generate CSV and YAML but with less reliability—occasional formatting errors, inconsistent indentation, missing headers.

**Smaller models** struggle more with strict format requirements. GPT-3.5 generates valid JSON most of the time but occasionally produces syntax errors. It generates XML less reliably. CSV and YAML quality degrades further. If you need non-JSON formats with smaller models, expect higher validation failure rates and build robust retry logic.

**Open-source models** vary widely. Some are fine-tuned specifically for code and structured output (CodeLlama, Deepseek Coder) and handle multiple formats well. General-purpose open-source models often struggle with strict format constraints and require more extensive prompting and validation.

Testing is essential. Don't assume a model can generate your required format reliably until you've tested it under realistic conditions with diverse inputs.

## Error Recovery for Non-JSON Formats

When XML, CSV, or YAML generation fails validation, recovery strategies differ from JSON.

**XML repair** can often fix common errors: unclosed tags can be closed automatically, missing namespace declarations can be added, unescaped special characters can be escaped. Libraries like BeautifulSoup (Python) can parse broken XML and reconstruct valid documents. XML repair is more feasible than JSON repair because XML's redundancy (opening and closing tags) provides hints about intended structure.

**CSV repair** handles missing values, inconsistent column counts, and unescaped delimiters. If a row has fewer columns than expected, you can pad with empty values. If a field contains unescaped delimiters, you can heuristically add quotes. CSV repair is heuristic and error-prone but often good enough for forgiving consumers.

**YAML repair** is harder because YAML syntax is sensitive to whitespace and indentation. Incorrect indentation changes structure meaning. Automated repair is risky. Usually you retry generation with clearer prompting rather than attempting repair.

For all formats, if automated repair fails, retry with a modified prompt that emphasizes the specific issue: "Ensure all XML tags are properly closed", "Ensure CSV has exactly 5 columns per row", "Ensure YAML uses consistent 2-space indentation."

## Format Standardization Across Teams

In multi-team environments, format choices should be standardized to avoid integration chaos.

**Team conventions** document which formats are used for which types of data. "User data extraction uses JSON. Regulatory reports use XML. Batch processing uses CSV. Infrastructure configuration uses YAML." This prevents teams from making arbitrary choices that complicate integration.

**Schema registries** maintain schemas for each format and use case. Teams reference shared schemas rather than inventing their own structures. This ensures that the "customer" object in team A's JSON output has the same fields as team B's XML output after transformation.

**Format conversion libraries** provide tested, reliable transformation logic. When format conversion is necessary, use shared libraries rather than bespoke transformation code in each service. This centralizes testing, debugging, and maintenance.

## The Multi-Format Production Stack

Production systems that support multiple output formats share common infrastructure.

**Format abstraction layer** decouples generation logic from format serialization. Your extraction logic produces an internal representation (often a typed object or data structure). Separate serializers convert that representation to JSON, XML, CSV, or YAML. This makes format support modular and testable.

**Format-specific validators** for each supported format, with shared validation logic for business rules. Structural validation is format-specific (JSON schema, XSD, CSV column counts), but semantic validation (value ranges, required fields) is shared.

**Format-specific repair and retry** logic handles format-specific failure modes and recovery strategies.

**Monitoring** tracks validation failure rates, generation times, and error patterns by format. If XML failure rates spike, that's a signal to investigate prompting or model issues specific to XML generation.

JSON is the default choice for most AI structured output, and it's a good default. But when your use case requires XML, CSV, or YAML—because of downstream systems, domain conventions, or human readability needs—generating the right format directly is simpler and more reliable than transforming from JSON. Format choice is a design decision that affects your entire pipeline.

The next section examines markdown and rich text output control: how to generate clean, well-structured markdown for documentation, how to control heading hierarchies and list formatting, and when to strip or convert rich formatting.
