# 2.4 â€” Building a Model Selection Eval Suite: Task-Specific Head-to-Head Comparisons

In early 2025, a legal technology company was selecting a model for contract review automation. The product would analyze commercial agreements, flag non-standard clauses, and assess compliance risk. The team evaluated GPT-5.1, Claude Opus 3.5, and Gemini 2 Pro using public benchmark results. GPT-5.1 had the highest scores on legal reasoning benchmarks. The team selected it, confident they had chosen the best model. Three months into production, the compliance team reported a concerning pattern. The model was missing ambiguous limitation of liability clauses that used non-standard phrasing. It flagged standard force majeure language as high risk. It consistently failed to identify jurisdiction-specific compliance requirements in master service agreements. The team investigated and discovered the problem. The legal reasoning benchmarks tested general legal knowledge and bar exam questions. They did not test the specific contract review patterns the company cared about. The benchmark winner was not the best model for this task. The team had optimized for the wrong evaluation criteria.

You cannot select a model without evaluating it on your tasks. General benchmarks are insufficient. MMLU, HumanEval, and GPQA test broad capabilities, but they do not tell you whether a model will correctly classify your customer support tickets, generate your product descriptions, or moderate your community content. Building a model selection evaluation suite means creating a representative sample of your actual production inputs, defining quality criteria specific to your domain, and running head-to-head comparisons across candidate models. This is not optional. It is the only way to know which model serves your needs. Skipping this step and choosing based on benchmarks, vendor claims, or vibes is professional negligence.

## The Purpose of a Model Selection Eval Suite

The purpose of the eval suite is to answer one question: which model performs best on the specific tasks my system needs to handle. Not which model is best in general. Not which model has the highest benchmark scores. Not which model the vendor recommends. Which model produces the highest quality outputs on your inputs according to your quality criteria. This is an empirical question that can only be answered through measurement.

The eval suite serves three functions. First, it provides objective data for initial model selection. When you are choosing between GPT-5, Claude Opus 4.5, and Gemini 3 Pro for a customer support task, you run all three models on your eval suite and compare the results. The model with the highest scores on your quality criteria wins. Second, it provides a baseline for tracking model performance over time. When a vendor releases a new model version, you run it through your eval suite and compare it to your current model. If the new version performs worse, you do not upgrade. If it performs better, you test it in production before committing. Third, it provides evidence for stakeholder communication. When legal asks why you chose a specific model for a Tier 1 task, you show them the eval results demonstrating that the chosen model had the lowest error rate on safety-critical cases.

The eval suite is not a one-time artifact. It is a living dataset that evolves as your product evolves. You add new examples when you expand into new domains. You add edge cases when you discover failure modes in production. You refine quality criteria when stakeholder priorities change. The eval suite grows with your understanding of what matters.

## Sourcing Representative Evaluation Data

The quality of your eval suite depends entirely on the quality of your evaluation data. If your eval data is not representative of production inputs, your evaluation results will not predict production performance. Sourcing representative data requires three strategies: sampling from production logs, sourcing from domain experts, and generating synthetic examples.

Production logs are the gold standard for representativeness. If you have an existing system handling similar tasks, sample inputs from your production logs. For a customer support chatbot, pull a random sample of user questions from the last six months. For a content moderation system, pull a random sample of flagged posts. For a document summarization tool, pull a random sample of documents. The sample should be large enough to cover the distribution of inputs you actually see. For most tasks, that means hundreds to thousands of examples, not dozens. A 10-example eval suite tells you nothing about statistical performance. A 1000-example eval suite gives you confidence intervals and significance testing.

Stratified sampling improves representativeness when your input distribution is skewed. If 80 percent of customer support questions are about password resets and 5 percent are about billing disputes, a pure random sample will overrepresent password resets and underrepresent billing disputes. Stratified sampling ensures you have enough examples from each category to evaluate performance across the full range of cases. You might sample 200 password reset questions, 200 billing questions, 200 product questions, and 200 technical support questions, even though the production distribution is not uniform. This ensures your eval suite tests all the workflows you care about, not just the most common ones.

When production logs are not available because you are launching a new product, domain experts become your primary data source. Ask customer support agents to provide examples of questions they expect to receive. Ask content moderators to provide examples of posts they expect to flag. Ask legal teams to provide examples of contracts they expect to review. Domain experts have intuition about the range of cases the system will encounter. Their examples will not perfectly match production distribution, but they will cover the known scenarios better than synthetic data alone.

Synthetic generation fills gaps when logs and expert input are insufficient. Use a strong model to generate variations on real examples. If you have 50 billing dispute questions from logs, use GPT-5 to generate 150 additional billing dispute questions with different phrasings, tones, and edge cases. This expands your eval suite without waiting for production data. The risk is that synthetic data can drift away from reality. A model generating customer support questions might produce questions that sound plausible but do not reflect how real users actually phrase problems. Use synthetic generation as a supplement, not a replacement, for real data.

Balance representativeness with edge case coverage. Your eval suite should include both typical cases and edge cases. Typical cases ensure the model handles the common workflows. Edge cases ensure the model does not catastrophically fail on unusual inputs. For a medical triage system, typical cases are straightforward symptoms like headaches and fever. Edge cases are ambiguous presentations like atypical chest pain or non-specific neurological symptoms. For a financial advice system, typical cases are basic retirement account questions. Edge cases are complex multi-jurisdiction tax scenarios. You need both to understand the full performance envelope.

## Defining Task-Specific Quality Criteria

Evaluation is meaningless without clear quality criteria. You need to define what good output looks like for your task, and you need to define it in measurable terms. Abstract criteria like "high quality" or "helpful" do not work because different people interpret them differently. Specific criteria like "includes all required fields," "uses formal tone," and "does not hallucinate facts not present in the source document" work because they can be consistently evaluated.

Start by listing the dimensions of quality that matter for your task. For a customer support chatbot, quality dimensions might include factual accuracy, tone appropriateness, completeness of the response, and adherence to company policy. For a contract review system, quality dimensions might include identification of non-standard clauses, accuracy of risk classification, and coverage of jurisdiction-specific requirements. For a content moderation system, quality dimensions might include precision in flagging policy violations, recall in catching harmful content, and consistency across similar cases. Each task has its own set of quality dimensions based on what stakeholders care about.

For each quality dimension, define a measurement approach. Some dimensions can be measured objectively. Factual accuracy can be measured by comparing the output to a known ground truth. Adherence to formatting requirements can be measured by parsing the output and checking for required fields. Response length can be measured by counting words or tokens. Other dimensions require human judgment. Tone appropriateness requires a human reader to assess whether the tone matches the context. Completeness requires a human expert to determine whether the response addresses all aspects of the question. For dimensions that require human judgment, define a rubric that guides evaluators to consistent assessments.

Scoring rubrics translate quality dimensions into numerical scores. A simple rubric uses a three-point scale: 0 for unacceptable, 1 for acceptable, 2 for excellent. A more detailed rubric uses a five-point scale with specific criteria for each score. For a customer support response, a rubric might define score 2 as "fully answers the question with accurate information, uses appropriate tone, and follows company policy," score 1 as "answers the question with minor issues in tone or completeness," and score 0 as "factually incorrect, inappropriate tone, or violates policy." The rubric ensures that different evaluators assign similar scores to similar outputs.

Map quality criteria to business outcomes wherever possible. Stakeholders care about business metrics like customer satisfaction, resolution time, and legal risk, not abstract quality scores. A customer support eval might weight factual accuracy more heavily than tone because factual errors cause repeat contacts and lower satisfaction. A contract review eval might weight recall of high-risk clauses more heavily than precision because missing a risk is more costly than flagging a false positive. The weighting reflects business priorities and makes eval results actionable.

## The Annotation Process

Annotation is the process of assigning quality scores to model outputs. For simple tasks with objective criteria, annotation can be automated. For complex tasks requiring judgment, annotation requires human evaluators. The annotation process determines the reliability of your eval results. Inconsistent annotation produces noisy data that cannot distinguish between models. Consistent annotation produces reliable data that clearly identifies the best model.

Select annotators based on the expertise required to judge quality. For customer support responses, customer support agents are appropriate annotators because they understand company policy and user expectations. For medical triage outputs, clinical professionals are appropriate annotators because they can assess medical accuracy and safety. For legal contract review outputs, attorneys are appropriate annotators because they understand legal nuances and jurisdiction-specific requirements. Do not assign annotation to people who lack domain expertise. A software engineer cannot reliably judge the quality of medical advice. A product manager cannot reliably judge the compliance accuracy of legal analysis.

Use multiple annotators per example to measure inter-annotator agreement. If three annotators evaluate the same output and assign scores of 2, 2, and 1, the output is likely a strong score 2. If they assign scores of 2, 1, and 0, there is significant disagreement, and the criteria or rubric needs clarification. Inter-annotator agreement metrics like Cohen's kappa or Fleiss's kappa quantify consistency. You want kappa above 0.6, ideally above 0.75. Low agreement indicates the rubric is ambiguous or the annotators need more training.

Train annotators before they begin evaluation. Provide the rubric, explain the quality dimensions, and walk through example outputs with scores. Calibrate by having all annotators score the same set of examples, then discuss disagreements and refine the rubric. This training phase ensures everyone interprets the criteria consistently. It also surfaces edge cases where the rubric does not provide clear guidance. Update the rubric based on calibration discussions, then proceed to full annotation.

Annotation is time-consuming. A single annotator might evaluate 20 to 50 examples per hour depending on task complexity. If your eval suite has 1000 examples and you want three annotators per example, that is 3000 annotations. At 30 annotations per hour, that is 100 hours of annotation time. This is a significant investment. You balance annotation coverage with practical constraints by prioritizing high-value examples. Annotate all edge cases and ambiguous cases with multiple annotators. Annotate a random sample of typical cases with fewer annotators. Use automated scoring for objective criteria like formatting compliance. This hybrid approach maximizes the value of limited annotation time.

## Running Head-to-Head Model Comparisons

Once you have your eval suite and your annotation process, you run the comparison. Select the candidate models you want to evaluate. For a Tier 1 task, that might be GPT-5.2, Claude Opus 4.5, and Gemini 3 Pro. For a Tier 2 task, that might be GPT-5.1, Claude Sonnet 4, and Gemini 3 Flash. Run every model on every input in your eval suite using the same prompt, the same temperature, the same max tokens, and the same system instructions. The only variable is the model. This ensures the comparison is fair and the results are attributable to model differences, not prompt differences.

Collect the outputs in a structured format. A simple spreadsheet works for small eval suites. Each row is an input. Columns include the input text, the output from each model, the quality scores assigned by annotators, and any notes on edge cases or interesting failures. For larger eval suites, use a database or an evaluation platform that supports structured queries and aggregation. You want to be able to filter by input category, by quality dimension, by annotator, and by model to identify patterns.

Aggregate scores to compare models. Start with overall average scores. Model A has an average score of 1.68 across all inputs. Model B has an average score of 1.52. Model C has an average score of 1.71. Model C is the winner on average performance. Then break down scores by category. Model A might outperform on factual questions but underperform on policy questions. Model C might be strong across all categories but slightly weaker on edge cases. These breakdowns reveal model strengths and weaknesses that overall averages obscure.

Statistical significance testing tells you whether observed differences are meaningful or just noise. If Model C scores 1.71 and Model A scores 1.68 on 1000 examples, is that difference real or random variation. A t-test or Mann-Whitney U test gives you a p-value that answers this question. If p is less than 0.05, the difference is statistically significant, and you can confidently say Model C outperforms Model A. If p is greater than 0.05, the difference could be random, and you should consider them equivalent. Do not declare a winner based on a 2 percent difference in average score on 50 examples. That is noise, not signal.

Error analysis identifies failure modes specific to each model. For every output scored 0, review what went wrong. Model A might hallucinate facts not present in the source document. Model B might struggle with ambiguous tone requirements. Model C might fail to follow formatting instructions. Document these failure modes because they inform your decision. If Model C has the highest average score but its failure mode is catastrophic hallucination on Tier 1 tasks, you might choose Model A instead despite its lower average score. Error analysis adds qualitative context to quantitative scores.

## Sample Size and Statistical Power

Sample size matters. An eval suite with 20 examples cannot reliably distinguish between models unless the performance gap is enormous. An eval suite with 1000 examples can detect small but meaningful differences. Statistical power is the probability that your eval suite will detect a real difference if one exists. Higher power requires larger sample sizes.

For most model selection tasks, aim for at least 300 to 500 examples per category you care about. If you have five categories of customer support questions, that is 1500 to 2500 total examples. This gives you enough statistical power to detect differences of 5 to 10 percentage points in quality scores with confidence. If your categories are not evenly distributed in production, use stratified sampling to ensure you have enough examples from rare categories.

If you cannot collect 300 examples per category, acknowledge the limitation. A 100-example eval suite can still provide directional guidance. It can tell you that Model A is clearly better than Model B if the difference is large. It cannot tell you that Model A is slightly better than Model C if the difference is small. Use smaller eval suites for initial screening to eliminate clearly inferior models, then expand the suite for final selection between close candidates.

Continuously expand your eval suite over time. Start with 300 examples. After three months in production, add 200 more examples sampled from production logs. After six months, add another 200. After a year, you have 700 examples covering a wider range of cases and a more robust basis for future model comparisons. The eval suite becomes a strategic asset that compounds in value.

## Tracking Results in a Structured Format

Eval results are valuable only if you can reference them in the future. When a new model is released six months from now, you need to compare it to your current model using the same eval suite and the same criteria. This requires structured storage of eval results, not ad hoc spreadsheets that get lost.

Create an eval results repository. For each eval run, record the date, the models tested, the eval suite version, the prompts used, the outputs generated, the scores assigned, and the aggregate metrics. Store the raw outputs so you can re-score them if your rubric changes. Store the annotator assignments so you can track inter-annotator agreement over time. Store the statistical test results so you can reference them in stakeholder discussions.

Version your eval suite. When you add new examples, increment the version number. When you revise the rubric, increment the version number. This ensures you know which results are comparable. Eval results from version 1.0 of the suite are not directly comparable to results from version 2.0 if the examples changed. You can run older models on the new suite version to create a historical baseline, but you need to know which version you are looking at.

Document your decision rationale. When you select a model based on eval results, write down why. Which quality dimensions mattered most. Which failure modes were unacceptable. Which business constraints influenced the decision. This documentation protects you from second-guessing months later when someone asks why you did not choose the cheaper model. It also creates institutional knowledge so that future model selection decisions build on past reasoning.

## The Eval Suite as a Living Artifact

The eval suite is not static. It evolves as your product evolves. When you launch a new feature, add examples that test the new feature. When you discover a production failure mode, add examples that would have caught it in evaluation. When stakeholder priorities shift, update your quality criteria and re-score existing outputs. The eval suite is a living artifact that reflects your current understanding of what matters.

Set a regular cadence for eval suite review. Quarterly is common. Review the suite, identify gaps, add new examples, refine criteria, and re-run evaluations if significant changes occurred. This ensures the suite stays aligned with production reality and continues to provide value for model selection decisions.

The eval suite also supports regression testing. When you update your prompt or change your system instructions, run the new version on your eval suite and compare results to the baseline. If quality drops, you caught a regression before deploying to production. If quality improves, you have evidence that the change was beneficial. The eval suite becomes your safety net for iterative improvement.

Building a model selection eval suite is an investment. It requires time to source data, time to annotate, time to run comparisons, and time to analyze results. The investment is worth it because it replaces guesswork with data. It ensures you choose the model that actually works best for your tasks, not the model that claims to be best. It provides evidence for stakeholder decisions. It protects you from costly mistakes like the legal tech company that chose based on benchmarks and spent three months in production with the wrong model.

With your eval suite in place and your model selected, you have the foundation for effective model selection. The next step is understanding cost-performance tradeoffs and how to balance quality against operational expenses as your system scales.

