# 3.10 — Latency-Aware Routing: Meeting SLA Targets with Model Selection

In early 2026, a conversational AI platform for enterprise sales teams began losing customers due to slow response times. The platform was designed to assist sales reps during live calls by suggesting responses and retrieving account information in real time. The product team had set a target of 800 milliseconds for time-to-first-token — the time from submitting a query to receiving the first token of output. The engineering team had chosen GPT-5 as the default model because it produced the highest quality suggestions. But GPT-5 was consistently delivering time-to-first-token between 1,200 and 1,800 milliseconds during peak hours. Sales reps saw a noticeable lag between asking a question and seeing the response start to appear. They stopped using the feature. Customer churn increased by 14% over two months. When the product team investigated, they found that the engineering team had optimized for quality without considering latency. There was no latency-aware routing. Every query went to GPT-5 regardless of current response times or SLA requirements. The root cause was treating latency as an infrastructure problem rather than a model selection problem. Latency-aware routing treats response time as a first-class constraint and selects models to meet SLA targets, trading off quality when necessary to stay within the latency budget.

## Latency Budgets Across Product Surfaces

Not all features have the same latency requirements. Real-time interactive surfaces — chatbots, in-call assistants, live coding tools — need responses to begin appearing within 500 to 1,000 milliseconds. Users perceive delays beyond one second as sluggish. Delays beyond two seconds cause users to stop using the feature. This means your latency budget is 500 milliseconds for time-to-first-token in real-time applications. Some of that budget is consumed by network overhead, request serialization, and routing logic. The model inference itself needs to complete in 300 to 400 milliseconds to meet the overall target.

Near-real-time surfaces — email composition assistants, document summarization, search result augmentation — can tolerate 1,500 to 3,000 milliseconds. Users are already waiting for the underlying operation to complete, so an extra second of latency is acceptable as long as the response improves the result. Your latency budget here is 2,000 milliseconds time-to-first-token. The model inference can take 1,500 milliseconds and still feel responsive.

Asynchronous surfaces — batch content generation, overnight report generation, scheduled data analysis — have no meaningful latency constraint. Users submit the request and return later. Whether the model takes 5 seconds or 50 seconds per query does not matter. Your latency budget is effectively unlimited. You can route to the slowest, most capable model if it delivers better quality.

API endpoints expose inference to external developers who impose their own SLA requirements. A public API might promise 95th percentile response times under 2,000 milliseconds. Your latency budget is therefore 1,800 milliseconds to leave margin for variability. If your model selection pushes latency above that threshold, you are violating your SLA and risking customer attrition or contractual penalties.

The routing layer needs to know the latency budget for each query. This comes from product context. The query metadata includes the feature name, the user interface element that triggered it, and the expected response mode — streaming or batch. The routing layer maps these signals to a latency budget. A query from the live chat interface gets a 500-millisecond budget. A query from the email assistant gets a 2,000-millisecond budget. A query from the batch report generator gets no latency constraint. The routing decision uses this budget as a hard constraint. If no model can meet the latency budget while maintaining acceptable quality, the router either returns an error or falls back to a cached or rule-based response.

## Model Latency Profiles and Real-Time Tracking

Model latency is not constant. It varies by provider load, time of day, geographic region, input length, output length, and whether the provider is experiencing an incident. In January 2026, GPT-5 typically delivers time-to-first-token between 600 and 900 milliseconds for prompts under 2,000 tokens during off-peak hours. During peak hours — 9 AM to 5 PM Eastern and Pacific time — latency increases to 1,000 to 1,400 milliseconds. During provider incidents, latency spikes to 3,000 milliseconds or the request times out entirely. Claude Opus 4.5 delivers 400 to 700 milliseconds during off-peak and 700 to 1,100 milliseconds during peak. GPT-5 delivers 300 to 500 milliseconds off-peak and 500 to 800 milliseconds peak. Llama 4 405B self-hosted delivers 200 to 400 milliseconds consistently because it runs on dedicated infrastructure with no multi-tenant load variation.

To make latency-aware routing decisions, you need a real-time latency profile for each model. This is a service that continuously measures time-to-first-token and total response time for each model and exposes current and recent percentiles to the routing layer. The latency tracker sends periodic health check queries to each model — short prompts with predictable outputs — and records the response times. It calculates rolling percentiles over the last 5 minutes, 15 minutes, and 60 minutes. It detects latency anomalies by comparing current latency to the baseline for the same time of day and day of week. It publishes this data to a low-latency key-value store that the routing layer queries before each decision.

The routing layer queries the latency tracker and retrieves the current 50th and 95th percentile time-to-first-token for each candidate model. If the latency budget is 500 milliseconds and GPT-5 is currently at 1,200 milliseconds p50, the router eliminates GPT-5 from consideration. If GPT-5 is at 600 milliseconds p50, the router also eliminates it because it exceeds the budget at median. If Claude Sonnet 4.5 is at 450 milliseconds p50, it stays in the candidate set. If Llama 4 70B is at 250 milliseconds p50, it also stays. The router then evaluates quality among the remaining candidates and selects the one with the highest expected quality. This ensures that latency is a hard constraint and quality is optimized within that constraint.

Latency tracking adds operational complexity. You need to run health checks every few seconds without overwhelming the provider APIs. You need to account for cold start latency when a model has not been used recently. You need to distinguish between model latency and network latency so you are measuring the right thing. You need to handle cases where the latency tracker itself is down or returning stale data. Despite this complexity, latency tracking is mandatory for latency-aware routing. Without it, you are making routing decisions based on outdated assumptions and you will miss your SLA targets during high-load periods.

## Latency Prediction Before Query Execution

Measuring latency after the fact tells you what happened, not what will happen. The routing layer needs to predict latency before sending the query so it can choose the right model upfront. Latency prediction models estimate time-to-first-token based on input characteristics and current system load. The simplest prediction model uses input token count as the primary feature. For GPT-5, time-to-first-token increases roughly linearly with input tokens up to 8,000 tokens, then increases more steeply due to attention computation costs. A 500-token prompt takes 700 milliseconds. A 2,000-token prompt takes 1,100 milliseconds. A 5,000-token prompt takes 1,900 milliseconds. You fit a regression model on historical latency data with input token count as the predictor. For a new query, you tokenize the input, count the tokens, and predict the latency.

More sophisticated prediction models include additional features: output token count estimate, time of day, current provider load from the latency tracker, whether the model has been used recently by this client, and the geographic region of the API endpoint. You train a gradient boosting model or a neural network on historical latency logs and use it to predict latency at routing time. This prediction runs in under 10 milliseconds and provides a latency estimate for each candidate model. The router uses these estimates to filter out models that are likely to exceed the latency budget.

Latency prediction is never perfect. The model may underestimate latency if provider load spikes suddenly or if the output is longer than expected. To account for this uncertainty, you add a safety margin. If the latency budget is 500 milliseconds and the prediction is 480 milliseconds, the model is too close to the edge. You only consider models whose predicted latency is at most 80% of the budget. This leaves room for variability and reduces the risk of SLA violations. The downside is that you may route to a less capable model than necessary. The upside is that you meet your latency target more reliably.

## Fallback Routing for High-Latency Conditions

Even with latency tracking and prediction, you will encounter situations where the preferred model is too slow. The provider is experiencing an incident. Traffic is unusually high. The query is unusually long. The routing layer needs a fallback strategy that maintains acceptable quality while meeting the latency constraint. The simplest fallback is the latency ladder. You rank models by capability and by typical latency. When the top-choice model exceeds the latency budget, you step down to the next model on the ladder. When that model also exceeds the budget, you step down again. You continue until you find a model that meets the latency constraint or until you run out of models.

For a real-time chat application with a 500-millisecond budget, the ladder might be GPT-5, Claude Opus 4.5, GPT-5.1, Claude Sonnet 4.5, GPT-5, Llama 4 70B, GPT-5 Mini. If GPT-5 is at 1,300 milliseconds, skip it. If Claude Opus 4.5 is at 900 milliseconds, skip it. If GPT-5.1 is at 700 milliseconds, skip it. If Claude Sonnet 4.5 is at 450 milliseconds, select it. The query goes to Claude Sonnet instead of GPT-5, accepting a small quality reduction to meet the latency target. If even Claude Sonnet exceeds the budget, you continue down the ladder until you find a model that fits.

A more sophisticated fallback is the cached response strategy. For queries that are similar to recent queries, you may have a cached response from a previous execution. The cache lookup takes 10 to 50 milliseconds, well within any latency budget. If the cache hit rate is high enough, you can meet latency targets even when all models are slow by serving cached responses. This works well for FAQ-style queries, repeated searches, and idempotent transformations. It does not work for queries that require fresh data or personalized responses. The routing layer checks the cache before evaluating models. If a cache hit is found and the cached response is fresh enough, return it. If not, proceed with model selection.

Another fallback is the partial response strategy. If the latency budget allows streaming but the full response will take too long, you start streaming the response from a fast model and stop after a time limit. The user sees the first 200 tokens within 500 milliseconds, and the response stops there even if the model would have generated 800 tokens. This is better than waiting 2,000 milliseconds for the full 800 tokens and violating the SLA. It sacrifices completeness for responsiveness. You indicate to the user that the response was truncated and offer to continue if they wait longer. This approach works in interactive contexts where partial information is better than delayed information.

## The Quality-Latency Tradeoff

Latency-aware routing forces a tradeoff between response time and output quality. Faster models are generally less capable. GPT-5 Mini is five times faster than GPT-5 but produces lower quality outputs on complex reasoning tasks. Llama 4 8B is ten times faster than Claude Opus 4.5 but struggles with nuanced instructions and long-context tasks. When you route to a faster model to meet a latency target, you accept a quality reduction. The question is how much quality you lose and whether that reduction is acceptable.

You measure the quality cost of latency-optimized routing by running your evaluation suite against each model at realistic latency thresholds. For a customer support task with a 500-millisecond budget, GPT-5 is too slow and is not considered. GPT-5 meets the budget and achieves 89% task success. Claude Sonnet 4.5 meets the budget and achieves 90% task success. Llama 4 70B meets the budget and achieves 85% task success. GPT-5 Mini meets the budget and achieves 78% task success. If your quality floor is 85%, you can use GPT-5, Claude Sonnet, or Llama 4 70B. You select Claude Sonnet because it delivers the highest quality within the latency constraint. The quality cost compared to GPT-5 — which achieves 94% but cannot meet the latency target — is 4 percentage points. You decide that is an acceptable tradeoff to deliver a responsive user experience.

For tasks where the quality cost is unacceptable, you have three options. First, relax the latency budget. If increasing the budget from 500 milliseconds to 800 milliseconds allows you to use a model that meets the quality floor, and if the product can tolerate that latency, adjust the budget. Second, optimize the model configuration. Use shorter prompts, smaller contexts, constrained output formats, or few-shot examples to reduce inference time without changing models. Third, change the product design to remove the latency constraint. Move the feature from real-time to near-real-time by adding a loading state or showing a placeholder while the model runs. All three options require collaboration between engineering and product. Latency-aware routing cannot solve a problem where the latency budget and the quality floor are fundamentally incompatible.

You should measure the quality-latency tradeoff continuously as models evolve. A model that is too slow today may become fast enough next quarter. A model that meets the latency budget today may become slower if the provider changes infrastructure or raises prices. You revisit the tradeoff quarterly and update your routing ladder accordingly. This keeps your routing decisions aligned with current model performance rather than assumptions from six months ago.

## SLA-Driven Routing as a Production Requirement

Latency-aware routing is not an optimization. It is a production requirement for any system that promises response time guarantees to users or external customers. If your product advertises real-time assistance, users expect responses to begin within one second. If your API documentation promises 95th percentile response times under 2,000 milliseconds, your customers will monitor that metric and escalate if you miss it. If you have contractual SLAs with financial penalties for violations, missing latency targets costs you money directly. Latency-aware routing is the mechanism that ensures you meet these commitments.

In practice, SLA-driven routing uses the latency budget as a hard constraint and eliminates any model that cannot meet it. The routing layer queries the latency tracker, filters out models whose current latency exceeds the budget, and selects the highest-quality model from the remaining candidates. If no candidates remain, the router either returns an error, serves a cached response, or escalates to a human. It does not violate the SLA by routing to a slow model and hoping for the best. This disciplined approach prevents SLA violations and the downstream consequences — user complaints, customer churn, financial penalties, reputational damage.

You instrument SLA compliance by logging the actual response time for every query and comparing it to the latency budget. You calculate the percentage of queries that met the SLA in each time window — hourly, daily, weekly. You alert when the compliance rate drops below the target, such as 95% of queries meeting the SLA. You investigate the alerts and identify the root cause. Was the latency tracker stale? Did a model experience an incident? Did traffic spike and overwhelm the provider? Did the routing logic fail to filter out a slow model? You fix the root cause and confirm that SLA compliance improves.

SLA-driven routing also informs capacity planning. If your latency budget is 500 milliseconds and the only models that meet it are GPT-5 and Claude Sonnet 4.5, and both are frequently slow during peak hours, you need additional capacity. This might mean provisioning self-hosted models like Llama 4 70B on dedicated GPUs to guarantee latency. It might mean negotiating dedicated throughput with API providers to avoid multi-tenant load spikes. It might mean sharding traffic across multiple providers to spread load. The routing layer identifies the latency bottlenecks and the capacity planning process addresses them.

## Latency-Aware Routing and Streaming Responses

Streaming responses change the latency profile. Instead of waiting for the entire output to generate before returning it, the model returns tokens incrementally as they are produced. The user sees the first token after time-to-first-token and sees subsequent tokens at a rate determined by the tokens-per-second throughput. For a 500-token response, time-to-first-token might be 600 milliseconds and throughput might be 50 tokens per second, so the user sees the first token at 600 milliseconds and the full response at 10,600 milliseconds. The perceived latency is 600 milliseconds, not 10,600 milliseconds, because the user starts reading as soon as the first token appears.

Latency-aware routing for streaming responses focuses on time-to-first-token rather than total response time. The latency budget is the maximum acceptable delay before the first token appears. The routing layer filters models based on predicted time-to-first-token and ignores throughput. This is appropriate for interactive applications where starting the response quickly matters more than finishing it quickly. For batch applications where you need the complete response, you must consider both time-to-first-token and total time, which is time-to-first-token plus output length divided by throughput.

Streaming also enables a hybrid routing strategy where you start with a fast model and fall back to a slower model if the fast model produces low-quality output. You send the query to GPT-5 Mini, which begins streaming within 300 milliseconds. You evaluate the first 50 tokens for quality signals — coherence, relevance, factual accuracy. If the quality is acceptable, continue streaming from GPT-5 Mini. If the quality is poor, cancel the stream, send the query to Claude Sonnet 4.5, and start a new stream. The user experiences a brief interruption but gets a higher-quality response. This approach works when you can detect quality problems early in the output, which is possible for some tasks like summarization and question answering but not for others like code generation and creative writing.

## Monitoring and Alerting for Latency Violations

Latency-aware routing requires continuous monitoring to detect when latency targets are missed and to trigger corrective action. You log the latency budget and the actual latency for every query. You calculate the violation rate — the percentage of queries where actual latency exceeded the budget. You track this rate in a time-series dashboard partitioned by feature, model, and time of day. You set alerts when the violation rate exceeds 5% in a rolling 15-minute window. The alert pages the on-call engineer, who investigates and takes action.

Common causes of latency violations include provider incidents, unexpected traffic spikes, model configuration changes, and routing logic bugs. The on-call engineer checks the latency tracker to see if one model is experiencing elevated latency. If so, they temporarily remove that model from the routing pool until latency returns to normal. They check the provider status page to see if an incident is in progress. They review recent code deployments to see if a change introduced a latency regression. They examine query logs to see if a specific query pattern is causing slow responses. They apply the appropriate fix — disable a slow model, roll back a bad deployment, optimize a problematic query pattern — and confirm that latency violations drop.

Latency monitoring also informs model selection over time. If a model consistently violates latency targets during peak hours, you deprioritize it in the routing ladder or remove it entirely. If a model consistently meets latency targets with room to spare, you consider upgrading to a more capable model with slightly higher latency. The monitoring data drives continuous improvement in routing policy.

## Real-World Latency Constraints and Model Selection

In 2026, the latency characteristics of popular models create clear routing strategies for different latency budgets. For budgets under 500 milliseconds, you must use GPT-5, GPT-5 Mini, Claude Haiku 4.5, or self-hosted Llama 4 70B or smaller. GPT-5, GPT-5.1, Claude Opus 4.5, and Claude Sonnet 4.5 are too slow. For budgets between 500 and 1,000 milliseconds, you can add Claude Sonnet 4.5 and sometimes GPT-5.1 depending on current load. For budgets between 1,000 and 2,000 milliseconds, you can use GPT-5 and Claude Opus 4.5 during off-peak hours. For budgets above 2,000 milliseconds, all models are viable and you optimize for quality and cost instead of latency.

These thresholds shift as providers optimize inference infrastructure and release new models. A model that is too slow today may become fast enough next month. You revisit your latency assumptions quarterly and update routing policies when model performance changes. This keeps your latency-aware routing aligned with reality rather than outdated benchmarks.

Latency-aware routing is the mechanism that ensures your model selection respects response time constraints. It prevents the mistake of always using the most capable model and discovering too late that it is too slow to deliver a good user experience. It forces you to think about the latency budget for each feature and to choose models that fit within that budget. It enables you to meet SLA commitments and to provide responsive interactions even when the most capable models are slow. Teams that implement latency-aware routing deliver consistent, predictable performance. Teams that skip it eventually face a latency crisis that forces an emergency migration to faster models and damages user trust. Latency-aware routing is not optional in production systems. It is a foundational capability that separates systems that meet their performance commitments from systems that make promises they cannot keep.
