# 7.11 — The Adaptation Stack: Layering Prompting, RAG, and Fine-Tuning Together

In late 2025, a legal technology company built a contract analysis system that needed to identify risk clauses in commercial agreements, explain why each clause was risky according to the client's policies, and suggest safer alternative language. They started with prompting alone. They wrote detailed prompts that defined risk categories and provided examples. The model identified some risky clauses but missed nuanced cases and produced inconsistent explanations. They added RAG to inject relevant policy documents and precedent language into the context. Accuracy improved, but the model still struggled with the client's specific risk tolerance and preferred wording. They fine-tuned a model on two thousand annotated contracts from past client engagements. The fine-tuned model learned the client's judgment patterns and language preferences. Finally, they refined the prompts to guide how the fine-tuned model used the RAG context. The resulting system combined all three adaptation strategies: a fine-tuned model that understood the client's standards, RAG that provided current policy context, and prompts that structured the task and output format. Accuracy reached 94 percent, and clients trusted the explanations enough to rely on them in high-stakes negotiations.

The legal tech team had discovered a fundamental principle of production AI systems: **adaptation strategies are not mutually exclusive**. You do not choose between prompting, RAG, and fine-tuning. You layer them. Each strategy addresses different aspects of model behavior. Prompting customizes the task and output format for each request. RAG injects external knowledge that the model needs but does not have memorized. Fine-tuning changes the model's base behavior, teaching it patterns, tone, and judgment that are specific to your domain. When layered correctly, these strategies compound. The whole system performs better than any single strategy alone. When layered incorrectly, they interfere with each other, and debugging becomes a nightmare. Understanding how the adaptation stack works—and how to design, test, and debug it—is essential for building reliable production systems.

## The Adaptation Stack from Bottom to Top

The adaptation stack has four layers, each building on the one below. At the bottom is the **base model**: the pretrained foundation model you start with, such as GPT-5.1, Claude Opus 4.5, or Llama 4 Maverick. The base model provides general language understanding, reasoning capability, and world knowledge. It is the foundation. Everything else adapts the base model to your specific needs.

The second layer is **fine-tuning**. Fine-tuning adjusts the model's weights to change its behavior on your task. It teaches the model patterns that are not well-represented in the pretraining data. It shifts the model's tone, output style, or judgment to align with your standards. Fine-tuning is applied once or periodically through continuous retraining. It is baked into the model. Once fine-tuned, the model behaves differently on all inputs within the task domain, not just on inputs where you apply a specific prompt.

The third layer is **RAG**. RAG retrieves external documents, database records, or other context at inference time and injects them into the prompt. RAG provides knowledge that changes frequently, is too large to fit in the model's context window as static few-shot examples, or is private and was not in the pretraining data. RAG is dynamic. Different inputs trigger retrieval of different context. The model sees different information on every request depending on what the retrieval system surfaces.

The fourth layer is **prompting**. Prompting defines the task, specifies the output format, provides instructions, and includes any per-request customization. Prompting is applied to every request. It is the most flexible layer. You can change prompts instantly without retraining or rebuilding retrieval indices. Prompting is how you steer the adapted model toward the specific outcome you need for each input.

These layers stack in order of permanence and specificity. Fine-tuning is slow to change and applies broadly to all requests in a task domain. RAG is moderately dynamic and applies when relevant context exists for a request. Prompting is instant to change and applies uniquely to each request. The stack ordering matters because each layer transforms what the next layer sees. Fine-tuning changes how the model interprets prompts. RAG changes what information the model has access to when following prompts. Prompting directs how the model uses both its fine-tuned behavior and the RAG context. You cannot understand the system's behavior by examining any single layer in isolation. You must understand the interactions.

## How Each Layer Interacts with the Others

Fine-tuning changes the model's default behavior, which changes how it responds to prompts. If you fine-tune a model to be concise, the same prompt that previously generated verbose outputs will now generate brief outputs. If you fine-tune a model to follow a specific output schema, you might not need to specify the schema in every prompt—the model will default to that schema because fine-tuning taught it to. This interaction can be positive: fine-tuning reduces the prompt complexity needed to achieve the desired behavior. It can also be negative: fine-tuning might make the model less responsive to certain prompt instructions if the fine-tuning data strongly biases the model toward a different behavior.

The legal tech company experienced this interaction when they fine-tuned their model on contracts where risky clauses were always flagged with the phrase "high risk" or "moderate risk." After fine-tuning, the model defaulted to those exact labels even when the prompt asked for a three-tier scale with different labels. The fine-tuning overpowered the prompt instruction. They fixed this by including examples in the fine-tuning data that used various label sets, teaching the model to respect the label schema specified in the prompt rather than defaulting to one fixed schema. The fix required understanding that fine-tuning and prompting interact, not just adding more prompt instructions.

RAG changes what the model knows during inference, which changes how it responds to prompts. If your prompt says "summarize the document," the model's output depends entirely on which document the RAG system retrieved. If the retrieval is wrong, the summary is wrong, even if the prompt is perfect. If the retrieval is incomplete, the summary misses key information. RAG quality determines the ceiling on task performance. The model can only be as good as the context it receives. This means debugging RAG issues often looks like debugging prompt issues—the model's output is wrong—but the root cause is retrieval, not the prompt.

The interaction between fine-tuning and RAG is more subtle. Fine-tuning can teach the model how to use RAG context effectively. If your RAG system retrieves policy documents and the model must extract specific clauses, fine-tuning on examples where the model correctly identifies relevant clauses in retrieved context improves RAG performance. The fine-tuned model learns to focus on the right parts of the retrieved text. Conversely, if you fine-tune a model without including RAG context in the training data, the fine-tuned model might not know how to use RAG context at inference time. It was trained to answer questions from its internal knowledge, so when you inject external context, it might ignore it or weight it incorrectly.

The legal tech company ran into this problem. They fine-tuned their model on annotated contracts without including policy documents in the training context. At inference time, they added RAG to inject policy documents, but the model ignored the policies and relied on patterns it learned during fine-tuning. The outputs were based on old policies baked into the training data, not the current policies in the RAG context. They fixed this by re-fine-tuning with policy documents included in the training examples, teaching the model to prioritize RAG context over memorized patterns. This fix required understanding that fine-tuning sets expectations for what context the model will see at inference time.

Prompting interacts with both fine-tuning and RAG by directing how the model uses its adapted behavior and retrieved context. A prompt might instruct the model to prioritize the RAG context over its internal knowledge: "Base your answer only on the provided document. Do not use information from outside this document." This instruction is only effective if the model was fine-tuned or prompted during training to follow such instructions. If the fine-tuning data never included examples of ignoring internal knowledge, the model might not follow the instruction reliably. Prompting can also guide how the model interprets ambiguous RAG context: "If the document does not specify a threshold, assume the default is 90 percent." This kind of instruction fills gaps in the RAG context and prevents the model from hallucinating or refusing to answer.

## The Stack Ordering Matters

The order in which you apply fine-tuning, RAG, and prompting determines system behavior. You cannot apply these strategies in arbitrary order and expect the same results. Fine-tuning must happen first because it modifies the model's weights. RAG happens at inference time, so it comes after fine-tuning. Prompting happens last because it operates on a per-request basis, after the model is fine-tuned and after context is retrieved. This ordering is fixed by the nature of the strategies, but within each layer you have design choices that affect how the layers interact.

For fine-tuning, the key decision is whether to include RAG context in the training data. If your production system will use RAG, your fine-tuning data should include retrieved context in the same format the model will see at inference time. The training examples should look like: retrieved context, followed by the user query, followed by the expected output. This teaches the model to use retrieved context effectively. If you fine-tune without RAG context and later add RAG at inference time, the model encounters a distribution shift. It was trained to answer questions without external context, and now it must answer questions with external context. Performance will be worse than if you trained with RAG context from the start.

For RAG, the key decisions are what to retrieve, how much to retrieve, and where to place it in the prompt. What to retrieve depends on the task: policy documents for compliance checks, user history for personalization, knowledge base articles for support questions. How much to retrieve is a trade-off between providing sufficient context and overwhelming the model with irrelevant information. Most systems retrieve three to ten documents or chunks per query. More than ten often adds noise without improving accuracy. Where to place the retrieved context matters because models are sensitive to context position. Placing critical context near the end of the prompt often improves performance because models attend more to recent tokens. Some prompts place retrieved context at the start, followed by instructions and the query. Others place the query first, then the retrieved context, then instructions. You test both orderings and use whichever performs better.

For prompting, the key decision is how much of the task definition to encode in the prompt versus how much to encode via fine-tuning. If you fine-tune a model to follow a specific output schema, you do not need to describe the schema in detail in every prompt. A short instruction like "classify this message" is sufficient because the model learned the schema during fine-tuning. If you do not fine-tune, the prompt must include the schema, examples, and detailed instructions. The trade-off is between prompt length and fine-tuning effort. Long prompts cost more tokens and increase latency. Fine-tuning requires training data and retraining infrastructure. For stable tasks with consistent requirements, fine-tuning reduces prompt complexity and improves consistency. For dynamic tasks where requirements change frequently, prompting provides more flexibility.

## Debugging Problems in a Layered Stack

Debugging a system that uses fine-tuning, RAG, and prompting together is harder than debugging any single layer in isolation because failures can originate in any layer or in the interactions between layers. When the system produces a wrong output, you must determine whether the problem is the base model, the fine-tuning, the RAG retrieval, the RAG context quality, the prompt, or the interaction between layers. This requires systematic isolation.

The first debugging step is to test the base model without fine-tuning, RAG, or complex prompts. You run the base model on the input with a minimal prompt and observe the output. If the base model cannot perform the task at all, fine-tuning or prompting will be required. If the base model performs the task but with different behavior than you want, fine-tuning or prompting can steer it. If the base model performs the task well, the problem is likely introduced by one of the adaptation layers.

The second step is to test the fine-tuned model without RAG. You run the fine-tuned model on the input with the same prompt you use in production but without injecting RAG context. If the fine-tuned model produces correct outputs, the problem is in the RAG layer—either retrieval quality or how the model uses the retrieved context. If the fine-tuned model produces incorrect outputs, the problem is in the fine-tuning or the prompt. You then test the fine-tuned model with a simplified prompt to isolate whether the fine-tuning itself is flawed or whether the prompt is confusing the model.

The third step is to test RAG retrieval quality independently of the model. You examine what documents or chunks the retrieval system surfaces for the problematic input. You verify that the retrieved content is relevant, accurate, and sufficient to answer the query. If the retrieval is wrong—irrelevant documents, outdated information, or missing key context—the model cannot produce correct outputs no matter how good the fine-tuning or prompt. You fix retrieval by improving the retrieval algorithm, updating the knowledge base, or refining the retrieval query. If the retrieval is correct but the model still produces wrong outputs, the problem is in how the model uses the retrieved context.

The fourth step is to test the prompt in isolation by replacing the RAG context with known-good context. You manually construct a context block that contains exactly the information the model needs, inject it into the prompt, and run the model. If the model now produces correct outputs, the problem is retrieval quality. If the model still produces incorrect outputs, the problem is the prompt structure or the model's ability to use the context. You iterate on the prompt: reorder sections, make instructions more explicit, add examples, change wording. You test each iteration to see if performance improves.

The fifth step is to test interactions by systematically enabling and disabling layers. You run the model with fine-tuning but no RAG, then with RAG but no fine-tuning, then with both. You compare outputs across configurations. If the fine-tuned model without RAG is correct but the fine-tuned model with RAG is wrong, the interaction between fine-tuning and RAG is the problem. The fine-tuned model might have learned to ignore external context, or the RAG context might conflict with patterns baked into the fine-tuning. You address this by re-fine-tuning with RAG context included, or by adjusting the prompt to explicitly instruct the model to prioritize RAG context.

The legal tech company used this systematic approach when their contract analysis system started hallucinating risk explanations. They first tested the fine-tuned model without RAG and found it produced reasonable explanations. They then examined the RAG retrieval and found it was surfacing outdated policy documents. The retrieval system had not been updated when the client revised their policies. The model was using old policies from the RAG context and producing explanations based on obsolete standards. The fix was updating the knowledge base, not changing the model or prompt. Without systematic isolation, they would have wasted time retraining the model or rewriting prompts when the root cause was stale retrieval data.

## Testing the Stack: Evaluating Each Layer Independently and the Full Stack Together

Effective testing of the adaptation stack requires both component-level evaluation and integration-level evaluation. Component-level evaluation tests each layer in isolation to ensure it works correctly. Integration-level evaluation tests the full stack to ensure the layers work together correctly. Both are necessary. A system where each component passes tests in isolation can still fail when components are combined if the interactions are not handled properly.

Component-level evaluation for fine-tuning involves running the fine-tuned model on a held-out eval set without RAG or complex prompts. You measure accuracy, consistency, and alignment with your standards. You verify that the fine-tuned model performs better than the base model on your task. You check for regressions: cases where fine-tuning made performance worse. You validate that the fine-tuned model generalizes to inputs outside the training distribution. If component-level eval fails, you revisit the fine-tuning data, hyperparameters, or base model choice before integrating with RAG and prompting.

Component-level evaluation for RAG involves testing retrieval quality independently of the model. You sample queries from your eval set, run retrieval, and manually review the retrieved documents. You measure retrieval precision: what fraction of retrieved documents are relevant. You measure retrieval recall: what fraction of relevant documents were retrieved. You check whether the most important context appears in the top results. You verify that retrieval latency meets your requirements. If retrieval quality is poor, you improve the retrieval algorithm, embeddings, or indexing strategy before using RAG in the full stack.

Component-level evaluation for prompting involves testing prompts with known-good models and known-good context. You use the base model or fine-tuned model with manually provided context and verify that the prompt produces the expected output format and behavior. You test edge cases: empty context, ambiguous queries, adversarial inputs. You ensure the prompt handles errors gracefully: if the context is missing key information, does the prompt instruct the model to refuse or to acknowledge uncertainty. If prompts fail component-level tests, you revise them before deploying the full stack.

Integration-level evaluation tests the full stack: fine-tuned model, RAG retrieval, and prompt together. You run the system end-to-end on your eval set and measure task performance: accuracy, latency, user satisfaction. You compare integration-level performance to component-level performance. If component-level tests showed 95 percent accuracy but integration-level tests show 85 percent accuracy, the gap indicates an interaction problem. The layers are not working together correctly. You debug the interaction by isolating which combination of layers causes the drop.

Integration-level evaluation also includes testing edge cases that only appear when layers interact. For example, what happens when RAG retrieves contradictory documents and the prompt does not specify how to resolve conflicts? What happens when the fine-tuned model has a strong prior on a topic and the RAG context contradicts that prior? What happens when the prompt is ambiguous and the fine-tuned model has to guess the user's intent? These edge cases are not visible in component-level tests because they arise from interactions. You must explicitly design integration-level test cases that probe these interactions.

The legal tech company built a two-tier eval process. Component-level eval ran on every fine-tuning iteration, every RAG index update, and every prompt change. Integration-level eval ran weekly on the full stack with production-like queries. When integration-level eval caught a failure, they used component-level eval to isolate the cause. This process allowed them to iterate quickly on individual components while ensuring the full stack remained reliable.

## The Minimal Viable Stack: Start with the Fewest Layers Needed

The complexity of the adaptation stack creates a temptation to over-engineer. Teams assume they need fine-tuning, RAG, and elaborate prompts from day one. This is usually wrong. The minimal viable stack principle says you should start with the simplest adaptation strategy that meets your requirements and only add layers when simpler strategies fail. Most tasks do not need all three layers. Many tasks only need prompting. Some tasks need prompting plus RAG. A smaller number need fine-tuning. Very few need all three.

Start with prompting. Write a clear, detailed prompt that defines the task, provides examples, and specifies output format. Test it on your eval set. If accuracy is acceptable and latency is acceptable, you are done. Deploy the prompt-only system. Prompting is the fastest and cheapest layer to iterate on. You can refine prompts in minutes and redeploy instantly. You do not need training data, fine-tuning infrastructure, or retrieval indices. Prompting is the default starting point.

If prompting alone does not achieve acceptable accuracy, diagnose why. If the model lacks domain knowledge—it does not know your company's policies, your product's features, or your industry's terminology—add RAG. Retrieve the relevant knowledge and inject it into the prompt. If the model knows the information but produces inconsistent outputs or does not follow your preferred style, consider fine-tuning. If the model cannot perform the task at all even with detailed prompts and RAG context, you might need a more capable base model or you might need to reframe the task into something the model can do.

Add RAG when the task requires knowledge that is too large, too dynamic, or too private to include in the prompt. If you need to answer questions about a thousand-page policy manual, you cannot fit the manual in the prompt. You retrieve relevant sections. If you need to provide up-to-date product information that changes daily, you cannot hard-code it in the prompt. You retrieve current data. If you need to access user-specific information that is private, you retrieve it at inference time instead of baking it into a fine-tuned model. RAG is the second layer you add when prompting alone is insufficient because of knowledge constraints.

Add fine-tuning when the task requires behavior changes that cannot be reliably achieved through prompting. If you need the model to output a specific schema and prompting produces schema violations 10 percent of the time, fine-tuning can reduce violations to 1 percent. If you need the model to adopt a tone or voice that is specific to your brand and prompting produces inconsistent tone, fine-tuning teaches the model your tone. If you need the model to make judgments that align with your organization's standards and prompting produces outputs that feel generic, fine-tuning encodes your standards. Fine-tuning is the layer you add when prompting and RAG together still leave a gap between the model's behavior and your requirements.

The minimal viable stack for the legal tech company turned out to be all three layers, but they did not start there. They started with prompting, found it insufficient, added RAG, found it still insufficient, and only then added fine-tuning. Each layer was a deliberate choice driven by eval results, not a default assumption. This incremental approach avoided premature complexity and ensured they understood the value each layer contributed.

## Layering Strategies in Practice

In production systems as of 2026, the most common adaptation stacks are prompting-only, prompting plus RAG, and prompting plus RAG plus fine-tuning. Prompting-only systems are used for tasks where base models already perform well and consistency is not critical: creative writing assistance, brainstorming, general-purpose Q&A. Prompting plus RAG systems are used for knowledge-intensive tasks where accuracy matters: customer support, technical documentation search, internal Q&A over company knowledge bases. Prompting plus RAG plus fine-tuning systems are used for high-stakes tasks where behavior must be tightly controlled: content moderation, legal analysis, medical triage, financial compliance.

Pure fine-tuning without prompting or RAG is rare. Even fine-tuned models benefit from prompts that define the task format and provide per-request customization. Fine-tuning plus prompting without RAG is used when the task requires specialized behavior but does not require external knowledge: sentiment analysis, text classification, summarization with a specific style. Fine-tuning plus RAG without complex prompting is used when the fine-tuned model reliably performs the task and only needs occasional context injection: personalized recommendations where the model is fine-tuned on user behavior patterns and RAG injects current inventory.

The legal tech company's stack—fine-tuning plus RAG plus prompting—represents the high end of complexity. Most teams do not need this level of sophistication. They need to understand the stack so they can make informed decisions about which layers to use, but they should resist the temptation to use all layers by default. Complexity is a cost. Every layer adds latency, increases failure modes, and makes debugging harder. You pay that cost only when the performance gain justifies it.

## Operationalizing the Stack

Once you have designed your adaptation stack, you must operationalize it: version it, test it, monitor it, and maintain it. Each layer has its own versioning and deployment cadence. Prompts change frequently, sometimes multiple times per day. RAG indices change when the underlying knowledge base is updated, typically daily or weekly. Fine-tuned models change when you retrain, typically weekly to monthly. These different cadences require coordination to avoid breaking the system.

Version control for the stack means tracking the version of each layer and the combinations that have been tested together. You version prompts in Git. You version RAG indices by snapshotting the knowledge base and the retrieval configuration. You version fine-tuned models in a model registry. You maintain a configuration file that specifies which prompt version, RAG index version, and model version are deployed together in production. When you update any layer, you test the new combination in staging before deploying to production.

Monitoring the stack means tracking metrics for each layer and for the integration. You monitor fine-tuned model accuracy, RAG retrieval precision, prompt success rate, end-to-end task accuracy, latency, and cost. When any metric degrades, you isolate which layer is responsible. You alert on anomalies: sudden drops in retrieval precision, spikes in prompt refusal rates, increases in latency. You log enough information to trace any production output back to the specific prompt version, RAG results, and model version that produced it.

Maintaining the stack means continuously improving each layer based on production data. You collect examples where the system fails and diagnose the root cause. If failures are due to poor retrieval, you improve the retrieval system. If failures are due to edge cases the fine-tuned model has not seen, you add those cases to the training data and retrain. If failures are due to ambiguous prompts, you refine the prompts. Maintenance is not optional. Production systems degrade over time as user behavior, knowledge bases, and requirements evolve. The stack must evolve with them.

The legal tech company operationalized their stack by building a deployment pipeline that packaged the prompt template, RAG configuration, and model identifier into a single deployable artifact. Every change to any layer created a new artifact version. They tested each artifact in staging with their eval suite before promoting to production. They monitored accuracy, retrieval quality, and latency in production and alerted when any metric fell outside acceptable bounds. They maintained a runbook that documented how to debug issues in each layer and in the interactions. This operational discipline made the complex stack reliable.

The adaptation stack—fine-tuning for behavior, RAG for knowledge, prompting for per-request customization—is the architecture that underlies most high-performing production AI systems in 2026. Understanding how to design, layer, test, and maintain this stack is what separates teams that ship reliable systems from teams that struggle with fragile prototypes. The next chapter explores the choice between open-source and proprietary base models, a decision that sits at the foundation of the adaptation stack and shapes everything built on top of it.
