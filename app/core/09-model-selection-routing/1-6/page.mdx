# 1.6 â€” Embedding and Retrieval Models: Choosing the Right Representation Layer

In September 2025, a SaaS company providing regulatory compliance search for financial institutions rebuilt their retrieval-augmented generation system from scratch. They had been using OpenAI text-embedding-ada-002, the previous generation embedding model, since launching in early 2023. Their users complained that search results were often irrelevant, especially for complex regulatory queries involving multiple jurisdictions. The engineering team assumed the problem was in their retrieval strategy or reranking logic. They spent six weeks optimizing query expansion, tuning retrieval parameters, and experimenting with hybrid search. Nothing moved the needle. Finally, in frustration, the technical lead suggested they test a newer embedding model. They switched to Cohere embed-v4, reindexed their entire 40-million-document corpus, and ran the same evaluation queries. Retrieval precision jumped from 64% to 81% overnight. They had been fighting a losing battle because their representation layer was fundamentally inadequate. The lesson: embedding model selection is not a background infrastructure decision. It is a first-order determinant of retrieval quality.

Embedding models are a distinct model selection decision from generation models. Many teams treat embedding as a solved problem, selecting the most convenient option during initial prototyping and never revisiting the choice. This is a mistake. The quality of your embeddings determines the quality of every downstream system that depends on semantic similarity: retrieval-augmented generation, semantic search, recommendation engines, document clustering, duplicate detection. If your embeddings do not accurately capture the semantic relationships in your domain, no amount of clever retrieval engineering will compensate.

The embedding landscape in 2026 is mature and competitive. OpenAI offers text-embedding-3-large and text-embedding-3-small, successors to the widely deployed ada-002 model. Cohere provides embed-v4 in multiple sizes with strong multilingual support. Voyage AI offers domain-specialized embedding models for code, legal, and biomedical text. Open-source options include the BGE family from Beijing Academy of Artificial Intelligence, the E5 models from Microsoft Research, and the GTE models from Alibaba. Each model has different strengths, weaknesses, cost structures, and operational characteristics. Selecting the right embedding model requires understanding your domain, your data, your retrieval patterns, and your latency and cost constraints.

## Dimensionality Tradeoffs: 256 vs 1024 vs 3072 Dimensions

The first major decision in embedding model selection is dimensionality. Embedding models map text into a high-dimensional vector space where semantically similar texts are close together and dissimilar texts are far apart. The number of dimensions in that vector space affects both the expressiveness of the representation and the computational cost of working with it.

Low-dimensional embeddings, in the 256 to 512 dimension range, are fast to compute, fast to index, and fast to search. A 256-dimensional vector takes one-quarter the memory of a 1024-dimensional vector. Vector similarity search scales with dimensionality. Searching a million 256-dimensional vectors is four times faster than searching a million 1024-dimensional vectors. If you are building a latency-sensitive application where search must complete in under 20 milliseconds, low-dimensional embeddings are essential. The cost is reduced expressiveness. A 256-dimensional space cannot capture as much semantic nuance as a 1024-dimensional space. Fine-grained distinctions between similar documents are lost. If your retrieval task depends on distinguishing closely related content, low-dimensional embeddings will underperform.

Medium-dimensional embeddings, in the 768 to 1024 dimension range, are the industry default. They balance expressiveness and efficiency. Most general-purpose embedding models produce vectors in this range. OpenAI text-embedding-3-large produces 3072-dimensional vectors but allows you to truncate to 1024 or 256 dimensions for storage and search. Cohere embed-v4 produces 1024-dimensional vectors. The BGE family produces 768-dimensional vectors. These models are expressive enough for most retrieval tasks and efficient enough for production deployment at scale. A well-optimized vector database can search tens of millions of 1024-dimensional vectors in under 50 milliseconds. That is acceptable for most applications.

High-dimensional embeddings, in the 2048 to 4096 dimension range, maximize expressiveness at the cost of computational overhead. They capture subtle semantic distinctions that medium-dimensional embeddings miss. If your domain has many near-synonyms, closely related concepts, or overlapping topics, high-dimensional embeddings improve retrieval precision. The cost is memory and latency. A 3072-dimensional vector database uses three times the memory of a 1024-dimensional database. Search latency increases proportionally. High-dimensional embeddings are appropriate when retrieval quality is the dominant constraint and cost and latency are secondary.

The correct choice depends on your retrieval task. If you are building a FAQ search system where queries and documents are short and semantically distinct, 256-dimensional embeddings are sufficient. If you are building a legal document retrieval system where queries and documents are long and semantically overlapping, 1024-dimensional embeddings are appropriate. If you are building a research paper recommendation system where small differences in topic focus matter, 3072-dimensional embeddings may be necessary. The only way to know is to evaluate empirically. Embed your evaluation set with multiple dimensionalities, perform retrieval, and measure precision and recall. The improvement from 768 to 1024 dimensions may be negligible or it may be substantial. You cannot know in advance.

## The Matryoshka Embedding Approach for Flexible Dimensionality

A recent innovation in embedding model design addresses the dimensionality tradeoff directly. Matryoshka embeddings are trained such that the first N dimensions of a high-dimensional vector form a valid lower-dimensional embedding. A model producing 3072-dimensional embeddings can be truncated to 1024, 512, or 256 dimensions, and the truncated vector is still semantically meaningful. This allows you to choose dimensionality at inference time rather than model selection time.

The advantage is flexibility. You embed all your documents at 3072 dimensions and store them in full. When a latency-sensitive user query arrives, you embed the query at 256 dimensions and search the first 256 dimensions of each stored document vector. When a quality-sensitive query arrives, you embed at 1024 dimensions and search the first 1024 dimensions. The same vector database serves both use cases. You trade storage cost for retrieval flexibility. This pattern is particularly valuable in hybrid systems where different query types have different quality and latency requirements.

OpenAI text-embedding-3-large and text-embedding-3-small support matryoshka truncation. Cohere embed-v4 does not as of January 2026 but may in future releases. If your application has mixed retrieval requirements, a matryoshka-capable model simplifies deployment and reduces operational complexity. You maintain a single vector index rather than separate indexes for different dimensionalities. The cost is higher storage requirements, but storage is cheap compared to the engineering complexity of managing multiple embedding models and multiple indexes.

## Multilingual Embedding Quality

If your application operates in multiple languages, multilingual embedding quality is a first-order selection criterion. Early embedding models were trained primarily on English text and performed poorly on non-English content. Modern multilingual embedding models are trained on balanced corpora spanning dozens of languages and produce high-quality embeddings for all of them.

Cohere embed-v4 is explicitly designed for multilingual use cases and supports over 100 languages. It produces embeddings where semantically similar text in different languages is close together in the vector space. A query in English retrieves relevant documents in Spanish, French, or Mandarin. This cross-lingual retrieval is essential for global applications where users and content are not linguistically homogeneous. OpenAI text-embedding-3 also supports multilingual embedding but with less explicit cross-lingual training. The model performs well on major European and Asian languages but less well on lower-resource languages.

The critical test for multilingual embeddings is cross-lingual retrieval accuracy. You should evaluate this directly. Create a test set where queries are in language A and documents are in language B. Measure whether the model retrieves the correct documents. If cross-lingual precision is low, the embedding model is not suitable for your use case. Some applications require within-language retrieval only. In that case, you can use language-specific embedding models that may outperform general multilingual models for your target language. For example, specialized Chinese or Japanese embedding models trained exclusively on those languages often outperform multilingual models for monolingual retrieval in those languages.

## Domain-Specific Embedding Models

General-purpose embedding models are trained on broad web-scale corpora: Wikipedia, web pages, books, forums. They capture general semantic relationships but lack fine-grained understanding of specialized domains. If your retrieval task is domain-specific, a specialized embedding model may substantially outperform a general-purpose model.

Voyage AI offers domain-tuned embedding models for code, law, and medicine. These models are trained on domain-specific corpora and fine-tuned to recognize domain-specific semantic relationships. A code embedding model understands that "for loop" and "iteration" are closely related in ways that a general-purpose model misses. A legal embedding model understands that "plaintiff" and "claimant" are near-synonyms in legal contexts. A biomedical embedding model understands that "myocardial infarction" and "heart attack" refer to the same condition.

The quality improvement from domain-specific embeddings is largest when your domain has specialized vocabulary, abbreviations, and semantic relationships not well-represented in general corpora. If you are building retrieval for software documentation, a code embedding model improves precision. If you are building retrieval for medical records, a biomedical embedding model improves precision. If you are building retrieval for contracts, a legal embedding model improves precision. The cost is reduced generality. A domain-specific model performs worse on out-of-domain content. If your corpus mixes domain-specific and general content, you may need to use separate embedding models for different content types or accept lower performance on out-of-domain content.

The decision to use a domain-specific embedding model depends on the homogeneity of your corpus and the availability of suitable models. If your corpus is 90% legal documents and 10% general business documents, a legal embedding model is appropriate. If your corpus is 50% legal and 50% general, the tradeoff is less clear. You must evaluate both a domain-specific model and a general-purpose model on your specific evaluation set and measure the difference. Domain-specific models are not universally better. They are better for domain-specific retrieval and worse for general retrieval.

## The Relationship Between Embedding Model Choice and RAG Quality

Retrieval-augmented generation depends on embedding quality in a direct and unavoidable way. The RAG workflow is: embed the query, retrieve the top K documents, pass those documents to a generation model as context. If the embedding step produces poor retrieval, the generation model receives irrelevant context and produces poor output. No amount of prompt engineering or generation model capability compensates for bad retrieval. The embedding model is the foundation of RAG quality.

Consider a customer support RAG system. A user asks, "How do I reset my password if I do not have access to my email?" The embedding model must understand that this query is semantically similar to documentation titled "Account Recovery Without Email Access" and dissimilar to documentation titled "Changing Your Email Address." If the embedding model retrieves the wrong documentation, the generation model answers the wrong question. The user receives an irrelevant response, loses trust, and escalates to human support. The failure originated in the embedding layer.

The quality gap between mediocre and excellent embeddings in RAG applications is often 20 to 30 percentage points in downstream task performance. Switching from an older embedding model like ada-002 to a modern model like text-embedding-3-large or embed-v4 can improve end-to-end RAG accuracy by 15% to 25%. That is not a marginal improvement. It is the difference between a system users tolerate and a system users trust. Embedding model selection is not a detail. It is a primary lever for RAG quality.

## Reranker Models: Cross-Encoders vs LLM-Based Rerankers

Embedding-based retrieval is fast but imperfect. The vector similarity score is a coarse approximation of semantic relevance. To improve precision, many systems use a two-stage retrieval architecture: a fast embedding-based retrieval stage that returns 50 to 100 candidates, followed by a slower reranking stage that scores each candidate more accurately and returns the top 5 to 10.

Cross-encoder rerankers are specialized models trained to predict the relevance of a query-document pair. Unlike embedding models, which encode the query and document independently, cross-encoders jointly encode both and produce a relevance score. This joint encoding captures interaction effects that embeddings miss. Cross-encoder reranking typically improves top-5 precision by 10 to 20 percentage points over embedding-only retrieval. Cohere offers a hosted reranker API. Several open-source cross-encoder models are available, including the BGE reranker family.

LLM-based reranking uses a language model to score query-document relevance. The prompt is: "Given query Q and document D, rate the relevance of D to Q on a scale of 0 to 10." The model outputs a score. You score all retrieved candidates, sort by score, and return the top K. LLM-based reranking is slower than cross-encoder reranking but often more accurate, especially for complex queries where relevance depends on nuanced reasoning. GPT-4.5 and Claude Opus 4.5 produce high-quality relevance scores, but at significant cost and latency. LLM-based reranking is appropriate when retrieval quality is paramount and latency is not a hard constraint.

The correct reranking strategy depends on your quality, latency, and cost constraints. If you need sub-100ms total retrieval latency, cross-encoder reranking is the only viable option. If you need the highest possible precision and can tolerate 500ms latency, LLM-based reranking is better. If cost is the dominant constraint, no reranking and tuning your embedding model may be the right choice. Many production RAG systems use hybrid reranking: cross-encoder reranking for most queries, LLM-based reranking for queries flagged as high-value or ambiguous.

## Retrieval Model Selection as Separate from Generation Model Selection

A common mistake is conflating embedding model selection with generation model selection. Teams choose a cloud provider for generation and default to that provider's embedding model without evaluating alternatives. This is lazy architecture. Embedding models and generation models serve different functions, have different performance characteristics, and should be evaluated independently.

You may use OpenAI GPT-4.5 for generation and Cohere embed-v4 for embeddings. You may use Claude Opus 4.5 for generation and Voyage code embeddings for code search. The best embedding model for your domain is often not from the same provider as the best generation model. Provider lock-in at the embedding layer is less of a concern than at the generation layer because embeddings are more portable. If you switch embedding models, you must reindex your corpus, but you do not need to change your prompts or application logic. If you switch generation models, you must revalidate prompts, output parsing, and behavior across your entire application.

The correct architecture is to select the best embedding model for your retrieval task independently of your generation model. If that means using multiple providers, that is acceptable. The operational complexity of managing two APIs is negligible compared to the quality and cost benefits of using the right model for each task. Multi-provider architecture is the industry standard for mature production AI systems.

## Cost Per Embedding vs Quality

Embedding costs are lower per token than generation costs, but they are not negligible. OpenAI text-embedding-3-large costs $0.13 per million tokens. Cohere embed-v4 costs $0.10 per million tokens. If you are embedding 100 million tokens per day, that is $10 to $13 per day, or $3,600 to $4,700 per year. For high-volume applications, embedding costs are a meaningful budget item.

Cost-quality tradeoffs exist in the embedding space just as they exist in the generation space. Smaller, faster embedding models are cheaper but less accurate. Larger, slower embedding models are more accurate but more expensive. The correct choice depends on the volume and quality requirements of your application. For batch indexing where latency does not matter, use the highest-quality embedding model you can afford. For real-time query embedding where latency is critical, use a smaller, faster model that meets your quality floor.

One cost-saving pattern is asymmetric embedding: use a high-quality model to embed documents offline and a lower-quality model to embed queries in real time. This works because document embeddings are computed once and reused many times, while query embeddings are computed on every request. The cost of document embedding is amortized over all future queries. The cost of query embedding is per-request. If you can achieve acceptable retrieval quality with a smaller model for query embedding, you save money without sacrificing document representation quality. This pattern requires that the query and document embedding models use the same vector space, which is true for models within the same family but not across providers.

## When to Fine-Tune Embedding Models

Embedding models, like generation models, can be fine-tuned on domain-specific data. Fine-tuning embedding models is less common than fine-tuning generation models, but it is a powerful technique when your domain is specialized and general-purpose embeddings underperform.

Fine-tuning requires labeled relevance data: sets of queries and documents where you know which documents are relevant to each query. You train the embedding model to place relevant query-document pairs close together and irrelevant pairs far apart. This supervised fine-tuning improves retrieval precision on your specific distribution of queries and documents. The quality improvement from fine-tuning can be substantial, often 10 to 20 percentage points in top-5 precision, especially in narrow domains with specialized vocabulary.

The cost of fine-tuning embeddings is lower than fine-tuning generation models because embeddings models are smaller and training is faster. A fine-tuning run on a 768-dimensional embedding model costs a few hundred dollars and takes a few hours. The main constraint is data availability. You need thousands of labeled query-document pairs to fine-tune effectively. If you do not have that data, you must either label it manually, which is expensive, or generate it synthetically, which introduces noise. Fine-tuning without sufficient data leads to overfitting and degraded performance on out-of-distribution queries.

The decision to fine-tune embeddings depends on three factors: the availability of labeled data, the performance gap between general-purpose embeddings and your quality target, and the stability of your domain. If you have rich labeled data, a large performance gap, and a stable domain, fine-tuning is worthwhile. If any of those conditions are false, fine-tuning is high-risk. Many teams overestimate the benefits of fine-tuning and underestimate the data requirements and maintenance burden.

## Embedding Model Versioning and Stability

Embedding models are updated less frequently than generation models, but they are updated. OpenAI released ada-002 in 2022, text-embedding-3 in 2024, and will likely release updated models in 2026 or 2027. When an embedding model is updated, you face a reindexing decision. Do you reindex your entire corpus with the new model, or do you continue using the old model?

Reindexing is expensive. A 10-million-document corpus may take days to reindex and cost thousands of dollars in compute and API fees. The benefit is improved retrieval quality. If the new model is substantially better than the old model, the quality improvement justifies the reindexing cost. If the new model is only marginally better, the cost may not be justified. You must evaluate the new model on your specific retrieval task before committing to reindexing.

The second consideration is version stability. Some embedding model providers maintain backward compatibility by keeping old model versions available indefinitely. Others deprecate old versions and force upgrades. OpenAI maintains ada-002 alongside text-embedding-3. Cohere deprecated earlier versions when embed-v4 launched. If your application depends on stable embeddings and you cannot tolerate forced reindexing, provider version policy is a selection criterion. Models from providers with strong backward compatibility guarantees are safer for long-term production use.

A third consideration is embedding drift. If you index half your corpus with one embedding model version and half with another version, retrieval quality degrades because the two halves live in different vector spaces. A query embedded with version 2 may not retrieve documents embedded with version 1. If you must reindex, you must reindex the entire corpus atomically. Partial reindexing is worse than no reindexing. This constraint makes reindexing a heavyweight operation that teams avoid unless the quality benefit is clear and large.

## Open-Source vs Hosted Embedding Models

The embedding model ecosystem includes both hosted APIs and open-source models you can self-host. Hosted APIs from OpenAI, Cohere, and Voyage AI offer convenience, scalability, and no operational overhead. Open-source models like BGE, E5, and GTE offer control, cost savings, and the ability to fine-tune and deploy on-premise.

Hosted APIs are the right choice for most teams. The per-token cost is low, the latency is acceptable, and the operational burden is zero. You call an API and receive embeddings. You do not manage inference infrastructure, model updates, or scaling. For teams with limited ML infrastructure capacity, hosted APIs are the obvious choice. The cost of self-hosting is not just compute; it is engineering time, operational complexity, and the opportunity cost of not working on product features.

Open-source models are appropriate when you have specific requirements that hosted APIs cannot meet: on-premise deployment for data privacy, fine-tuning for domain specialization, or cost optimization at very high volume. If you are embedding billions of tokens per day, the cost savings from self-hosting can be hundreds of thousands of dollars per year. If you operate in a regulated industry where data cannot leave your infrastructure, self-hosting is mandatory. If you need to fine-tune the embedding model, open-source models are the only option because hosted APIs do not expose fine-tuning.

The operational burden of self-hosting embeddings is significant. You must manage model serving infrastructure, handle scaling, monitor performance, and update models. You need expertise in ML infrastructure. For most teams, the total cost of ownership of self-hosting exceeds the cost of hosted APIs unless volume is extremely high. The break-even point depends on your specific costs, but a rough heuristic is that self-hosting becomes cost-effective above 10 billion tokens per month.

## Choosing the Right Embedding Model for Your Use Case

Embedding model selection is a structured decision. You must evaluate multiple models on your specific retrieval task, measure performance, and choose based on empirical results. The process is similar to generation model selection but with different evaluation criteria.

Start by defining your retrieval task and evaluation set. What kinds of queries will your users issue? What kinds of documents will you retrieve? Create a test set of at least 100 query-document pairs where you know the correct retrieval results. This evaluation set is your ground truth for measuring retrieval quality. Without it, you are guessing.

Second, select three to five candidate embedding models based on initial criteria: dimensionality, multilingual support, domain specialization, cost, and provider. For example, if you are building a legal document retrieval system, your candidates might include OpenAI text-embedding-3-large, Cohere embed-v4, Voyage legal embeddings, and an open-source model like BGE-large. If you are building a code search system, your candidates might include Voyage code embeddings, OpenAI text-embedding-3-large, and a fine-tuned CodeBERT model.

Third, embed your evaluation set with each candidate model. Perform retrieval using a consistent retrieval algorithm and parameters. Measure precision, recall, mean reciprocal rank, and normalized discounted cumulative gain. These metrics quantify how well each model retrieves relevant documents for your queries. The model with the best metrics on your evaluation set is the best model for your task, regardless of benchmark scores or marketing claims.

Fourth, evaluate cost and latency. Calculate the cost of embedding your entire corpus and the cost of embedding queries at your expected volume. Measure query embedding latency. If two models have similar quality but one is 50% cheaper or 50% faster, choose the cheaper or faster model. If one model is 5% better in quality but twice the cost, you must decide whether the quality improvement justifies the cost.

Fifth, prototype integration. Embed a subset of your production corpus, deploy the embedding model in a staging environment, and run end-to-end retrieval tests. Measure system-level performance: retrieval latency, indexing time, memory usage, error rates. Identify operational issues before committing to the model. Some models that look good in offline evaluation have practical deployment issues like poor API reliability, high tail latency, or unexpected failure modes.

## Embedding Models and Vector Database Requirements

Embedding model choice affects vector database requirements. Different embedding models produce vectors of different dimensionalities, and vector databases have performance characteristics that depend on dimensionality. High-dimensional embeddings require more memory, slower search, and more complex indexing algorithms.

If you choose a 256-dimensional embedding model, you can use a lightweight vector database or even an in-memory search library. If you choose a 3072-dimensional embedding model, you need a scalable vector database with optimized indexing like HNSW or IVF. The cost and operational complexity of your vector database are directly tied to your embedding dimensionality. This coupling means you must consider vector database constraints when selecting an embedding model.

A second consideration is embedding update frequency. If your documents change frequently and you need to reindex often, you need a vector database that supports fast upserts. Some vector databases are optimized for static corpora and have slow upsert performance. Others are optimized for dynamic corpora and handle frequent updates efficiently. If you expect your corpus to grow or change continuously, your vector database choice constrains your embedding model choice because some embedding models are easier to index incrementally than others.

A third consideration is hybrid search. Many retrieval systems combine vector search with keyword search to improve recall. This requires a vector database that supports hybrid queries or integration with a traditional search engine like Elasticsearch. Some embedding models are optimized for vector-only search. Others are designed to work in hybrid systems. If you plan to use hybrid search, you must verify that your embedding model and vector database support it.

## The Future of Embedding Models

Embedding models are improving rapidly. The quality gap between embeddings in 2024 and embeddings in 2026 is substantial. The quality gap between embeddings in 2026 and embeddings in 2028 will likely be equally large. Understanding the trajectory helps you make forward-looking decisions.

The first trend is multimodal embeddings. Current embedding models are text-only. Future embedding models will support text, images, audio, and video in a unified embedding space. A query like "show me product demos" will retrieve video clips, text documentation, and images, all ranked by relevance. Multimodal embeddings are already available in research settings and will become production-ready by 2027. If your application involves multimodal content, planning for multimodal embeddings is prudent.

The second trend is context-aware embeddings. Current embedding models produce a single vector per text, independent of context. Future embedding models will produce different embeddings for the same text depending on the query context. The word "bank" embedded in the context of a finance query will be closer to "financial institution." The same word embedded in the context of a geography query will be closer to "riverbank." Context-aware embeddings improve retrieval precision by disambiguating polysemous terms. This requires rethinking indexing strategies because document embeddings are no longer query-independent.

The third trend is learned retrieval architectures that bypass explicit embeddings entirely. End-to-end neural retrieval models take a query and a corpus and directly output a ranked list of documents without computing explicit embeddings. These models are slower than embedding-based retrieval but more accurate. They are appropriate for applications where quality dominates latency. Learned retrieval is still experimental as of 2026 but will likely become practical for production use by 2027 or 2028.

## Embedding Model Selection Is Not Optional

Many teams treat embedding model selection as an afterthought. They pick the first model that works and move on. This is a mistake. Embedding quality is a first-order determinant of retrieval quality, which is a first-order determinant of RAG quality, which is a first-order determinant of user experience. A poor embedding model undermines everything downstream. A great embedding model makes everything downstream easier.

You should evaluate embedding models with the same rigor you apply to generation models. Define your retrieval task, create an evaluation set, test multiple candidates, measure quality and cost, prototype integration, and choose based on evidence. This process takes time, but it is time well spent. The difference between a mediocre embedding model and an excellent embedding model is the difference between a retrieval system users tolerate and a retrieval system users love.

The next subchapter examines cost and latency tradeoffs in model selection, the practical constraints that determine which models you can afford to deploy at scale.
