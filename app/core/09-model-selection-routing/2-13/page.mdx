# 2.13 â€” Documenting Model Selection Decisions: The Model Decision Record

In mid-2025, a healthcare technology company serving 140 hospital systems faced an audit from their enterprise clients. The question was simple: why did the clinical documentation assistant use Claude Opus 4.5 instead of GPT-5 for summarizing patient encounters? The product manager who had made that decision had left the company eight months earlier. The current team found Slack messages with scattered reasoning, a spreadsheet with evaluation scores but no context about what was tested or why those metrics mattered, and three different people with three different recollections of the decision. The audit consumed two weeks of engineering time reconstructing the rationale, re-running evaluations to validate the choice, and writing a retrospective justification document. The client was not satisfied. They saw a team that could not explain its own technical choices. The contract renewal stalled for four months while the company implemented a formal decision documentation process. The root cause was not a bad technical decision. The root cause was treating model selection as a Slack conversation instead of a documented engineering decision that would need to be defended, explained, and reviewed for years.

Every model selection decision you make will need to be explained multiple times to multiple audiences over a multi-year period. New engineers will ask why this model was chosen when they join the team. Product managers will ask why you cannot just switch to the newest model when it launches. Executives will ask why your costs are what they are. Compliance teams will ask what evidence supports your choice. Customers will ask what model processes their data and why. Auditors will ask how you validated the decision. Six months from now, you will ask yourself why you made the choice you made. Without documentation, every one of these conversations becomes a reconstruction effort, a debate, or a guess. With documentation, they become a reference lookup. The tool for this is the Model Decision Record, a structured document that captures what was decided, why, what alternatives were considered, and what evidence supported the choice. This is not optional process overhead. This is the minimum viable artifact for professional model selection.

## The Core Template: Eight Sections Every MDR Needs

A Model Decision Record follows a consistent structure across all decisions. The first section is the decision date. This is the date the decision was finalized, not the date the document was written or the date the model was deployed. Use ISO format: 2026-01-15. The decision date anchors the entire record in time, making it clear what information and what models were available when the choice was made. When you review this decision in two years, you need to know that GPT-5.2 did not exist yet or that pricing changed three months after the decision.

The second section is the task or feature description. State in two to three sentences what this model will do. This is not a full feature spec. This is the scope of the model's responsibility. For example: "Generate three-sentence summaries of customer support tickets for display in the agent dashboard. Summaries must capture the customer issue, any actions taken, and current status. Used by 450 support agents handling 12,000 tickets per day." The task description defines the boundary of the decision. If the scope later expands, that triggers a new decision record.

The third section is models evaluated. List every model you tested, including the specific version or release date if available. Claude Opus 4.5 released November 2025. GPT-5 released August 2025. Llama 4-70B released October 2025. Do not just write "Claude" or "GPT-5." Model generations matter. A decision made in January 2026 that evaluated GPT-5 is different from a decision made in March 2026 after GPT-5.2 launched. If you tested a model and ruled it out immediately without formal evaluation, note that too: "Gemini 3 Flash excluded due to lack of HIPAA BAA." Documenting what you did not test and why is as valuable as documenting what you did test.

The fourth section is evaluation criteria. List the criteria you used to compare models and the relative importance of each. Criteria might include output quality measured by human eval on 200 examples, latency at p95 measured over 1,000 requests, cost per 1,000 requests, compliance requirements like BAA availability or data residency, API reliability measured by uptime SLA, and integration complexity. For each criterion, note whether it was a threshold requirement or a scored comparison. For example: "BAA availability: threshold requirement, must have. Latency p95: scored comparison, target under 800ms. Output quality: scored comparison, primary criterion." This section shows how you weighted the tradeoffs. If quality was the primary criterion and cost was secondary, say so explicitly.

The fifth section is evaluation results. Present the evidence that informed your decision. This does not mean pasting raw eval data. This means summarizing the findings in a way that supports comparison. For example: "Claude Opus 4.5 scored 91 percent human preference on summary quality. GPT-5 scored 87 percent. Llama 4-70B scored 82 percent. Claude Opus 4.5 p95 latency was 1,200ms. GPT-5 p95 latency was 950ms. Llama 4-70B p95 latency was 1,400ms. Cost per 1,000 requests: Claude Opus 4.5 at 2.10 dollars, GPT-5 at 1.80 dollars, Llama 4-70B at 0.40 dollars via hosted endpoint." The results section is a table written in prose. You are showing the data that mattered for this decision, not every data point you collected.

The sixth section is selected model. State the decision clearly. "Selected model: Claude Opus 4.5, November 2025 release, accessed via Anthropic API." This is the single-sentence answer to "what did you choose." Everything else in the document is context for this sentence.

The seventh section is rationale. This is the most important section. The rationale explains why you chose this model given the evaluation results and the criteria. A good rationale does not just restate the scores. It explains the tradeoff reasoning. For example: "We selected Claude Opus 4.5 despite higher cost and latency because summary quality was the primary criterion for this feature. Support agents rely on summaries to triage tickets, and low-quality summaries cause agents to open the full ticket anyway, negating the value of the feature. The 4 percentage point quality advantage over GPT-5 translated to approximately 8 percent fewer summary rejections in our eval set. The latency difference of 250ms is acceptable because summaries are generated asynchronously when a ticket is created, not in the agent's critical path. The additional cost of 30 cents per 1,000 requests represents approximately 150 dollars per day at current volume, which is within the feature's cost budget. We will monitor summary rejection rate in production and revisit this decision if GPT-5 quality improves or if Claude Opus 4.5 costs increase." That rationale captures the tradeoff, the reasoning, and the conditions under which you would revisit the decision. It is defensible, specific, and grounded in your criteria.

The eighth section is risks and mitigations. Every model selection carries risks. Risks might include model deprecation, price increases, API reliability issues, vendor lock-in, quality regression in future model versions, or insufficient capacity during peak load. For each risk, state the mitigation. For example: "Risk: Claude Opus 4.5 may be deprecated within 18 months based on Anthropic's typical model lifecycle. Mitigation: we will maintain eval infrastructure to re-evaluate alternative models quarterly and ensure we can switch within two weeks if needed. Risk: costs may increase if Anthropic changes pricing or if ticket volume grows faster than expected. Mitigation: cost monitoring alerts set at 200 dollars per day; if triggered, we will evaluate cheaper models or implement summary caching." The risks and mitigations section shows you are not blind to the downsides. It shows you have a plan.

These eight sections form the minimum viable Model Decision Record. Some teams add additional sections for compliance mappings, stakeholder approvals, or deployment timelines, but these eight are the core. Together, they answer the question "why this model" in a way that is complete, defensible, and reviewable.

## Writing a Rationale That Captures Tradeoffs, Not Just Scores

The rationale section is where most MDRs fail. Weak rationales simply restate the evaluation results. "We chose Claude Opus 4.5 because it had the highest quality score." That is not a rationale. That is a summary of the results section. A strong rationale explains why the highest quality score mattered more than the higher cost or the slower latency. It explains what tradeoff you made and why that tradeoff was correct for this task.

Start the rationale by naming the primary criterion. Every decision has a primary criterion, even if you evaluated on multiple dimensions. For model selection, the primary criterion is usually output quality, cost, latency, or compliance. State it explicitly. "The primary criterion for this decision was output quality, measured by human eval on factual accuracy and tone appropriateness." Once you have named the primary criterion, explain how the selected model performed on that criterion and why that performance justified the tradeoffs on secondary criteria.

Next, address the alternatives. A good rationale does not just explain why you chose the winner. It explains why you did not choose the runner-up. "We considered GPT-5, which offered 15 percent lower cost and 20 percent lower latency. However, GPT-5's quality score was 4 percentage points lower, and in our eval set, this translated to a higher rate of summaries that agents rejected as inaccurate or unhelpful. Given that the feature's value depends on agents trusting the summaries, we prioritized quality over cost." That rationale shows you understand the tradeoff. You are not choosing Claude Opus 4.5 because it is "better." You are choosing it because quality matters more than cost for this specific task. If the task were different, the choice might be different.

Then, quantify the tradeoff where possible. Weak rationales use vague language: "Claude Opus 4.5 was significantly better." Strong rationales use specific numbers: "Claude Opus 4.5 reduced summary rejection rate by 8 percentage points in our eval set, which translates to approximately 320 fewer rejected summaries per day at current ticket volume." Quantifying the tradeoff makes the decision concrete. You are not just saying quality is important. You are saying this much quality improvement is worth this much additional cost or latency.

Finally, state the conditions under which the decision should be revisited. Every model selection decision is conditional on the current state of the market, the current pricing, the current quality levels, and the current task requirements. These conditions will change. Your rationale should make explicit what changes would invalidate the decision. "We will revisit this decision if Claude Opus 4.5 costs increase by more than 20 percent, if GPT-5 quality improves to within 2 percentage points of Claude Opus 4.5, if ticket volume doubles and costs become unsustainable, or if a new model offers comparable quality at significantly lower cost." These conditions turn the rationale into a living document. They give future you or future teammates a clear trigger for when to reevaluate.

A rationale written this way is defensible in an audit, useful to a new hire, and actionable for a quarterly review. It is not a justification written after the fact. It is a record of reasoning written at the time of the decision, when the tradeoffs were fresh and the alternatives were clear.

## Storing MDRs in Version Control Alongside Code

Model Decision Records belong in the same version control system as the code that uses the models. This is not a documentation site. This is not a wiki. This is not a Google Doc folder. MDRs live in the repo, in a dedicated directory, versioned and reviewed like code. The standard location is a `/docs/model-decisions/` directory at the root of the repo, or within the service directory if you run a multi-repo architecture.

Each MDR is a markdown file named by task and date. For example: `2026-01-15-ticket-summary-model.md` or `2025-11-03-content-moderation-routing.md`. The filename makes it easy to find the relevant decision when you are working on that feature. If you make a new decision for the same task, you create a new file with a new date. You do not overwrite the old MDR. The history of decisions is valuable. Seeing that you switched from GPT-5 to Claude Opus 4.5 in January 2026 after originally choosing GPT-5 in August 2025 tells a story about how model quality or task requirements evolved.

MDRs go through code review like any other engineering document. When you finalize a model selection decision, you write the MDR and open a pull request. The PR is reviewed by the engineers who will implement the decision, the product manager who owns the feature, and any compliance or architecture reviewers who need visibility into model choices. The review process catches incomplete rationales, missing risks, or unclear task descriptions. It also ensures that the decision is shared with the team, not siloed in one person's head. Once the PR is merged, the MDR becomes the authoritative record. If someone questions the decision later, you link to the file in the repo.

Version control also enables historical analysis. You can see how many model decisions were made in the last year. You can see which models were evaluated and how often. You can see how rationales have changed over time. You can see which risks materialized and which did not. This historical data informs future decisions. If you see that you have switched models three times in six months for the same task, that signals instability in your requirements or in the model market. If you see that cost overruns were listed as a risk in five decisions but never materialized, that signals your cost monitoring is effective. The repo becomes a knowledge base.

Some teams integrate MDRs into their model registry. If you use a model registry tool to track which models are deployed in which environments, you can link each registry entry to the corresponding MDR file. For example, the registry entry for the ticket summary model points to `/docs/model-decisions/2026-01-15-ticket-summary-model.md`. This link closes the loop between deployment and decision. Anyone looking at the registry can immediately find the reasoning behind the choice.

## Connecting MDRs to Routing Configuration and Model Registry

Model Decision Records are not standalone documents. They connect to two other systems: your routing configuration and your model registry. The routing configuration defines which model is used for which task at runtime. The model registry defines which models are approved for use, which versions are deployed, and which compliance or security reviews have been completed. The MDR connects these systems by documenting the decision that led to the routing rule and the registry entry.

When you write an MDR, you should reference the routing configuration that implements the decision. For example, in the rationale section, you might write: "This decision is implemented in the routing configuration at `/config/model-routing.yaml`, where the `ticket_summary` task is mapped to Claude Opus 4.5 via the Anthropic API endpoint." That reference makes it easy to trace from decision to implementation. If the routing configuration changes later, you can find the MDR that justified the original choice and determine whether the change is intentional or accidental.

Conversely, your routing configuration should reference the MDR. Many teams add a comment field to their routing config that links to the decision record. For example, in a YAML routing config, you might have a comment line that says: "decision record: /docs/model-decisions/2026-01-15-ticket-summary-model.md." This bidirectional link ensures that anyone reading the routing config can find the rationale, and anyone reading the MDR can find the implementation.

The model registry connection works similarly. When you add a new model to your registry, you should reference the MDR that justified the addition. For example, the registry entry for Claude Opus 4.5 might include a field for decision records, listing all the tasks for which this model was selected. If you later need to deprecate Claude Opus 4.5, you can look at the registry entry, find all the associated MDRs, and identify which tasks will be affected. This prevents the scenario where you deprecate a model and only discover weeks later that some obscure feature still depends on it.

Some teams go further and generate routing configs from MDRs or validate that every routing rule has a corresponding MDR. This is automation overkill for most organizations, but the principle is sound: there should be a clear, traceable connection between decision and deployment. If a model is in production, there should be an MDR explaining why. If an MDR recommends a model, there should be a routing rule deploying it. Gaps in this connection are a sign of process breakdown.

## The MDR as Audit Trail for Compliance and GPAI Obligations

Under the EU AI Act, general-purpose AI systems deployed in high-risk applications are subject to transparency and documentation obligations. If your AI system makes decisions about employment, creditworthiness, access to services, or safety-critical operations, you are required to document how the system works, what models it uses, and what validation was performed. Model Decision Records are a core component of this audit trail.

An MDR demonstrates that you made a deliberate, evidence-based choice of model. It shows what alternatives you considered. It shows what criteria you used and what evidence you gathered. It shows what risks you identified and what mitigations you implemented. It shows when the decision was made and under what conditions it should be reviewed. This is precisely the documentation that auditors and regulators expect to see. If a regulator asks "why does your credit underwriting system use GPT-5 instead of a domain-specific model," you hand them the MDR. The MDR answers the question with specificity and evidence.

MDRs also support internal compliance reviews. Many regulated industries require regular reviews of AI system components. Healthcare organizations review clinical AI models quarterly or annually. Financial institutions review underwriting models after significant market events or regulatory changes. These reviews ask the same question: is the current model still the right choice given current performance, current risks, and current regulations? The MDR provides the baseline for that review. You compare current performance to the evaluation results documented in the MDR. You compare current risks to the risks identified in the MDR. You check whether any of the conditions for revisiting the decision have been met. If everything still holds, the review is a confirmation. If something has changed, the review triggers a new decision and a new MDR.

For GPAI obligations specifically, the EU AI Act requires documentation of model limitations, known failure modes, and validation procedures. These should all be captured in the risks and mitigations section of the MDR and in the evaluation criteria section. For example, if you know that Claude Opus 4.5 sometimes hallucinates citations in legal summaries, that limitation should be documented as a risk, and the mitigation might be a secondary validation step or a user warning. If your evaluation included adversarial testing for bias, that should be documented in the evaluation criteria section. The MDR becomes the artifact that proves you did the due diligence.

Some organizations extend the MDR template to include explicit compliance mappings. For example, an additional section might list applicable regulations, the specific obligations that apply to this model use case, and how the decision satisfies those obligations. This makes the MDR directly usable in regulatory filings or audit responses. Whether you add this section or handle compliance documentation separately, the MDR is the technical foundation. Compliance documentation without an MDR is just assertions. The MDR is the evidence.

## How MDRs Prevent the Why Are We Using This Model Conversation

The single most common question in any AI engineering team is "why are we using this model?" This question comes from new hires, from product managers, from executives, from customers, from auditors, and from your future self. Without an MDR, the answer is either "I think it was because of quality" or "let me go ask the person who made the decision" or "I do not know, this was before my time." With an MDR, the answer is "see the decision record in the repo." The conversation ends.

This is not just about saving time, though it does save time. The average "why are we using this model" conversation consumes 30 minutes to two hours of collective time across multiple people. It involves reconstructing context, searching Slack, re-running old evaluations, and debating whether the original reasoning still applies. If you field one of these conversations per month, you lose a full day per year. If you have ten people on the team and each person fields one of these conversations per quarter, you lose 20 to 80 person-hours per year. An MDR costs two to four hours to write. The return on investment is immediate.

More importantly, MDRs prevent decision re-litigation. Re-litigation happens when a decision is questioned, the original rationale is unclear, and the team debates the decision again from scratch. Re-litigation is expensive because it consumes time, but it is also dangerous because it leads to decision churn. If you re-litigate model selection every six months, you switch models frequently, you destabilize the system, you lose the ability to compare performance over time, and you erode user trust. MDRs stop re-litigation by making the original reasoning transparent. If the reasoning is sound and the conditions have not changed, the decision stands. If the reasoning was flawed or the conditions have changed, you make a new decision and write a new MDR. Either way, you are moving forward, not in circles.

MDRs also onboard new team members faster. A new engineer joining the team can read the MDRs in the repo and understand what models are used, why, and under what conditions the decisions might change. This is far more effective than asking a senior engineer to explain the model stack in a meeting. The new engineer gets the context in writing, with evidence, and can review it on their own time. Questions that arise during this review are higher quality because they are grounded in the documented reasoning.

Finally, MDRs make model deprecation and migration safer. When a model is deprecated by the vendor or when you decide to switch models, the first step is to find all the places that model is used. The second step is to understand why it was used in each place. The third step is to evaluate whether a replacement model satisfies the original criteria. Without MDRs, step two and step three are guesswork. With MDRs, you have the criteria, the evaluation results, and the rationale for every task. You can systematically evaluate replacement models against the same criteria and determine whether the replacement is acceptable. The migration becomes an engineering process, not a scramble.

## The Quarterly Review Process for Existing MDRs

Model Decision Records are not write-once documents. They are living records that should be reviewed periodically to ensure the decision is still valid. The standard cadence is quarterly review for high-stakes tasks and annual review for lower-stakes tasks. The review process is simple: for each MDR, check whether the conditions for revisiting the decision have been met, check whether new models or new evidence have emerged, and check whether production performance matches the expected performance documented in the MDR.

The review starts with the risks and mitigations section. Did any of the identified risks materialize? If you documented a risk of price increases and the price increased, that triggers reevaluation. If you documented a risk of quality regression and quality has regressed, that triggers reevaluation. If none of the risks materialized, that is useful data too. It suggests your risk assessment was conservative, which is fine, or that your mitigations were effective.

Next, check the conditions under which the decision should be revisited. These conditions are usually stated in the rationale section. If you wrote "we will revisit this decision if ticket volume doubles," check whether ticket volume doubled. If you wrote "we will revisit if a new model offers comparable quality at significantly lower cost," check whether such a model has launched. If the conditions have not been met, the decision stands. If the conditions have been met, you reevaluate.

Then, compare production performance to evaluation results. This is where your monitoring and evaluation infrastructure pays off. You should have metrics in production that correspond to the evaluation criteria in the MDR. If the MDR says you chose Claude Opus 4.5 because it scored 91 percent on human preference for summary quality, you should have a production metric that tracks summary rejection rate or user satisfaction. If production performance matches the evaluation results, the decision is validated. If production performance is worse, that signals a gap between your eval set and production traffic, or it signals that the model has regressed. Either way, it triggers investigation.

Finally, check for new models or new pricing. The model market moves fast. GPT-5.2 might have launched since your last decision. Claude Opus 4.6 might offer better quality. Llama 4 might have become available at lower cost. Pricing might have changed. Each of these changes is a potential trigger for reevaluation. You do not need to reevaluate every time a new model launches, but you should at least consider whether the new model invalidates your decision. If the new model is clearly better on all dimensions, reevaluation is obvious. If the new model offers a different tradeoff, you need to assess whether that tradeoff is now more favorable given your current priorities.

The output of the quarterly review is one of three outcomes. Outcome one: the decision is reaffirmed. No changes needed. Document the reaffirmation in a commit comment or a note appended to the MDR. Outcome two: the decision is updated. You switch to a new model, and you write a new MDR documenting the change. The old MDR remains in the repo as historical record. Outcome three: the decision is flagged for deeper investigation. Something has changed, but it is not clear yet whether the decision should change. You create a task to investigate and set a deadline for the investigation. This outcome prevents analysis paralysis. You do not need to reevaluate every decision every quarter. You only reevaluate when the evidence suggests it is warranted.

Some teams automate parts of the quarterly review process. For example, you might have a script that checks whether any of the models in your MDRs have had pricing changes, or whether any new models matching your criteria have launched. This automation surfaces candidates for review, but it does not replace human judgment. The review itself is a human process. You are asking whether the original reasoning still holds, and that requires understanding the task, the users, and the business context.

Model Decision Records turn model selection from an ad hoc conversation into a documented, reviewable, repeatable process. They answer the question "why this model" with evidence and reasoning. They support compliance obligations. They prevent decision re-litigation. They onboard new team members. They enable safe migration when models are deprecated. And they ensure that six months from now, when someone asks "why are we using this model," the answer is always available. The next subchapter addresses eval overfitting, the silent failure mode that invalidates model selection by optimizing for eval scores instead of production performance.

