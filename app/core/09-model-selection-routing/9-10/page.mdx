# 9.10 â€” The Model Registry: Tracking What Runs Where, With What Config

In November 2025, a SaaS company with fourteen different product teams and forty-two production services discovered they were running seventeen different versions of GPT-5 across their infrastructure, three of which were deprecated, two of which were using prompt templates that had failed security review four months earlier, and one of which was still pointing to a provider endpoint that no longer existed and was silently failing 100% of requests while the team responsible had moved on to other projects. The discovery happened during a routine audit triggered by a compliance question: which services are using models that process PII, and have those models been evaluated for bias? Nobody could answer. There was no central registry. Each team had deployed their own model integrations using their own conventions. Some teams tracked model deployments in Confluence pages. Some tracked them in Jira tickets. Some did not track them at all. The audit took three weeks and required interviewing every engineering lead, reading through hundreds of deployment logs, and reverse-engineering production configurations. The final report was a spreadsheet with forty-two rows and twelve columns, manually compiled, already out of date by the time it was finished. The compliance team was horrified. The engineering leadership team was embarrassed. The root cause was not malice or incompetence. It was decentralized decision-making without centralized visibility. Every team was doing what made sense locally. Nobody was maintaining global state.

You cannot manage what you cannot see. If you do not know which models are running in production, you cannot evaluate them, you cannot monitor them, you cannot upgrade them, you cannot secure them, and you cannot answer basic questions from compliance, security, or executive leadership about what your AI systems are doing. A **model registry** is the single source of truth for every model deployment in your organization. It tracks what models are running where, with what configuration, evaluated when, owned by whom. The registry is not optional infrastructure for organizations running AI at scale. It is foundational infrastructure, as critical as your service catalog, your configuration management database, or your incident tracking system.

## What the Registry Contains

A model registry is a structured database of model deployments. Each entry in the registry represents one deployed model instance: a specific model, from a specific provider, running in a specific service, with a specific configuration. The registry contains enough information to answer every operationally relevant question about that deployment without requiring tribal knowledge or archaeology.

The first required field is **model identifier**: the exact model being used. Not just "GPT-5" but "gpt-5.2-turbo-2026-01-15". Not just "Claude" but "claude-opus-4.5-20251120". The full version string matters because model behavior changes between versions, and you need to know exactly what is running. If you are using a fine-tuned model, the registry includes the fine-tune ID or version hash. If you are using a self-hosted model, the registry includes the model weights version and the inference engine version.

The second required field is **provider**: the company or platform serving the model. OpenAI, Anthropic, Google, AWS Bedrock, Azure OpenAI, self-hosted. This field distinguishes between different deployments of the same model. Running GPT-5.2 on OpenAI direct is different from running GPT-5.2 on Azure OpenAI. They have different SLAs, different rate limits, different geographic availability, and sometimes different behavior due to version skew.

The third required field is **deployment target**: where this model is running. This might be a service name, a Kubernetes namespace, a Lambda function ARN, or a GitHub repository. The deployment target tells you which part of your application is using this model. When you need to upgrade a model or investigate an incident, you need to know which services will be affected.

The fourth required field is **prompt template version**: the identifier for the prompt being used with this model. Your prompt templates should be versioned in source control, and the registry links each model deployment to a specific prompt version. This linkage is critical because model behavior depends on both the model and the prompt. When you evaluate a deployment, you are evaluating the combination of model and prompt. If the prompt changes, the eval results are no longer valid.

The fifth required field is **eval scores**: the most recent evaluation results for this deployment. At minimum, the registry includes the overall pass rate or accuracy score, the date the eval was run, and a link to the detailed eval report. Better registries include breakdowns by eval category: accuracy, safety, latency, cost. The eval scores tell you whether this deployment is performing acceptably. If a deployment has no eval scores, it should not be in production.

The sixth required field is **deployment date**: when this model was first deployed to production. This timestamp is essential for incident investigation. If a production issue started on January 15th and you deployed a new model on January 14th, you have a strong suspect. The deployment date also supports deprecation policies. If a model has been running for eighteen months without re-evaluation, it is overdue for review.

The seventh required field is **owner**: the team or individual responsible for this deployment. Ownership means they are responsible for monitoring, maintaining, evaluating, and upgrading this model. When something goes wrong, you need to know who to page. When a model needs to be upgraded, you need to know who is responsible for doing the work. Ownership prevents orphaned deployments where a model is running in production but nobody knows who maintains it.

Additional useful fields include cost per request, request volume, latency P50 and P95, error rate, retry configuration, timeout settings, rate limit allocation, and security classification. These fields are not strictly required, but they make the registry much more useful as an operational tool. A registry with rich metadata becomes a dashboard for understanding your entire model infrastructure at a glance.

## The Registry as Single Source of Truth

The registry is not documentation. It is not a best-effort catalog that gets updated when someone remembers. The registry is the authoritative, real-time record of production state. If a model is not in the registry, it should not be running in production. If the registry says a model is using prompt template v2.3, and the deployed service is actually using v2.4, the deployed service is out of compliance and must be corrected.

Achieving this level of authority requires enforcement. You cannot ask teams to manually update the registry and hope they do it consistently. Manual processes fail. The registry must be integrated into your deployment pipeline so that updating the registry is not optional, it is automatic. When a service deploys a new model, the deployment script updates the registry. When a prompt template version changes, the CI/CD system updates the registry. When an eval completes, the results are written to the registry. The registry stays in sync with production because staying in sync is built into the deployment process, not bolted on afterward.

Enforcement also requires validation. Your deployment pipeline should reject deployments that are not registered. If a developer tries to deploy a new model without creating a registry entry, the deployment fails with an error message explaining that all model deployments must be registered. This sounds draconian, but it is the only way to maintain registry accuracy. If you allow unregistered deployments "just this once," you will end up with unregistered deployments everywhere.

The registry must support updates and deprecation. When a model is upgraded, the registry entry is updated with the new model version, the new eval scores, and a new deployment date. The old version is marked as deprecated, not deleted. You maintain historical records of what was running when. When a model is decommissioned, it is marked as inactive, not deleted. You need the history for incident investigations, audits, and understanding how your infrastructure has evolved over time.

## Registry-Driven Deployment

The most mature model deployment processes are registry-driven. The registry is not just a record of what has been deployed. It is the specification of what should be deployed. You declare in the registry what model should run in what service, and the deployment system ensures that production matches the registry.

In a registry-driven process, deploying a new model starts with updating the registry. You create a new registry entry specifying the model version, the deployment target, the prompt template, and the expected eval scores. The registry entry is reviewed and approved through a pull request process, just like code changes. The review ensures that the deployment has been properly evaluated, that the owner is correct, that the configuration is valid.

Once the registry entry is merged, the deployment system detects the change and triggers an automatic deployment. The deployment system reads the registry entry, pulls the specified prompt template version from source control, configures the model client with the specified provider and model ID, runs a health check to verify the model is accessible, and deploys the service. The deployment is not a manual process involving SSH and kubectl commands. It is an automated process triggered by a registry update.

This approach has several advantages. First, it ensures that the registry is always accurate because the registry is the source of truth for deployments, not a record of deployments. Second, it makes deployments auditable because every deployment is triggered by a registry change, and every registry change is a pull request with a reviewer and a timestamp. Third, it enables rollback by simply reverting the registry entry to the previous version. Fourth, it prevents configuration drift because the deployment system continuously reconciles production state with registry state, automatically correcting any discrepancies.

Registry-driven deployment requires infrastructure investment. You need a deployment system that can read the registry and deploy models automatically. You need monitoring to verify that production matches the registry. You need alerting when drift is detected. But this investment pays off quickly for organizations running dozens of models across many services. The alternative is manual deployment with manual registry updates, which guarantees that the registry will be inaccurate within weeks.

## Integration with CI/CD: Registry Updates Trigger Eval Runs

The registry should be integrated with your CI/CD pipeline so that registry changes trigger automatic evaluation. When you create a new registry entry specifying a model and prompt combination, the CI/CD system runs your eval suite against that combination and writes the results back to the registry. If the eval fails, the registry entry is marked as failing evaluation and cannot be deployed to production.

This integration closes the loop between evaluation and deployment. You cannot deploy a model that has not been evaluated because the registry will not allow it. You cannot evaluate a model without creating a registry entry because the eval system reads its targets from the registry. Evaluation and deployment are coupled through the registry, ensuring that nothing reaches production without passing evals.

The integration works through webhooks or polling. When a registry entry is created or updated, the registry system sends a webhook to your eval orchestration system. The eval system reads the registry entry, retrieves the specified model and prompt, runs the eval suite, and updates the registry with the results. If the eval passes, the registry entry status changes from "pending evaluation" to "ready for deployment." If the eval fails, the status changes to "evaluation failed" and the entry cannot be deployed.

This process supports continuous evaluation. You can configure the registry to automatically re-run evals on a schedule. Every model in production gets re-evaluated monthly. If eval scores degrade, the registry alerts the owner and marks the deployment for review. This prevents silent quality degradation where a model that passed eval six months ago is still running in production even though the provider has changed the model behavior and it no longer meets your standards.

## Registry Queries: Operational Intelligence

A well-structured registry enables powerful queries that answer operational and compliance questions instantly. Which services are using GPT-5? Query the registry for all entries where model ID starts with "gpt-5". When was this model last evaluated? Query the registry for the eval date field on the relevant entry. Who owns this deployment? Query the registry for the owner field. These queries take seconds instead of days.

The most valuable queries are cross-cutting queries that span multiple dimensions. Which models are processing PII and have not been evaluated in the last six months? Query for entries where security classification is PII and eval date is older than six months ago. Which deployments are using deprecated model versions? Query for entries where model ID matches a list of deprecated versions. Which teams are spending the most on model inference? Query for total cost across all entries grouped by owner. These queries support compliance, cost management, and operational oversight.

You expose these queries through a web interface or a command-line tool. The interface provides pre-built queries for common questions and supports ad-hoc SQL queries for custom analysis. Product managers query the registry to understand which models are being used where. Engineering leads query the registry to find deployments owned by their team. Security teams query the registry to identify high-risk deployments. The registry becomes the central source of truth for understanding your AI infrastructure.

Advanced registries support alerting based on registry state. You define alerts for conditions like "any production deployment with no eval in 90 days" or "any deployment with error rate above 5%" or "any deployment using a model version that has been deprecated." When these conditions are met, the registry sends alerts to the responsible team. This proactive alerting prevents problems before they become incidents.

## Registry Tooling: Custom, MLflow, W&B, or Config Files

You have several options for implementing a model registry. The simplest option is a configuration file in source control. You create a YAML or JSON file listing all model deployments with their metadata. The file is versioned in git. Changes go through pull requests. Deployment scripts read the file to determine what to deploy. This approach works for small teams with a handful of models. It does not scale beyond ten or fifteen deployments because the file becomes unwieldy and queries require parsing the file manually.

The second option is a custom database. You build a PostgreSQL or MongoDB database with a schema for model deployments. You build a REST API for creating, updating, and querying registry entries. You build a web UI for browsing the registry. You integrate the API with your deployment pipeline and eval system. This approach gives you full control and flexibility. You can add exactly the fields you need, build exactly the queries you want, and integrate with your existing infrastructure. The downside is development and maintenance cost. Building a robust registry with API, UI, and integrations is a multi-week project, and maintaining it is an ongoing effort.

The third option is an existing MLops platform like MLflow or Weights & Biases. These platforms include model registry features designed for tracking machine learning models in production. You register models in MLflow, tag them with metadata, link them to eval runs, and deploy them to production through MLflow's deployment integrations. This approach is faster than building a custom registry and provides mature features like versioning, stage transitions, and lineage tracking. The downside is that these platforms are designed for ML engineering workflows, not production operations workflows. They work well if your AI systems are primarily custom-trained models. They are awkward if your AI systems are primarily API-based foundation models.

The fourth option is extending your existing service catalog or CMDB. If your organization already maintains a service catalog tracking all production services, you can add model deployment information to that catalog. Each service entry includes a section listing the models it uses, with the same metadata you would store in a dedicated registry. This approach leverages existing infrastructure and keeps model deployments in the same system as other infrastructure. The downside is that service catalogs are not designed for model-specific workflows like triggering evals or tracking prompt versions, so you end up building those integrations yourself.

Most organizations starting model registries in 2026 should start with a simple custom database or extend an existing MLflow deployment. Configuration files are too limiting. Full MLops platforms are overbuilt for most use cases. A lightweight custom registry or a well-configured MLflow instance provides the right balance of functionality and maintenance burden for teams running ten to one hundred model deployments.

## The Registry as Audit Infrastructure

The model registry is not just operational tooling. It is compliance infrastructure. When regulators, auditors, or internal compliance teams ask questions about your AI systems, the registry provides the answers. Which models are making decisions about creditworthiness? Query the registry for deployments tagged with "credit-decision." Have those models been evaluated for bias? Check the eval scores in the registry. When were they last evaluated? Check the eval dates. Who is responsible for monitoring them? Check the owner field. The registry turns a multi-day audit into a fifteen-minute query.

The EU AI Act, enforced in 2026, requires organizations deploying high-risk AI systems to maintain detailed records of model deployments, evaluations, and changes. The model registry satisfies many of these requirements. The registry provides traceability: you can show exactly what model was running at any point in time. It provides accountability: every deployment has an owner. It provides evidence of evaluation: every deployment has eval scores and eval dates. Without a registry, demonstrating compliance requires assembling evidence from deployment logs, git history, and tribal knowledge. With a registry, compliance is a database query.

Internal audits benefit just as much as regulatory audits. Your security team wants to know which models have access to customer PII. Your finance team wants to know which models are driving the highest inference costs. Your executive team wants to know how many AI systems are in production and whether they are performing well. The registry answers all of these questions instantly and accurately. It transforms AI governance from an archaeological exercise into a data-driven process.

The registry also supports incident investigations. When a production issue occurs, one of the first questions is "did anything change recently?" The registry provides a timeline of model deployments and configuration changes. You can see that a new model version was deployed three hours before the incident started. You can see that a prompt template was updated yesterday. You can see that eval scores dropped last week but nobody noticed. This information dramatically accelerates root cause analysis and reduces time to resolution.

Organizations running production AI systems in 2026 need a model registry. The registry is how you maintain visibility, enforce standards, support compliance, and enable operational excellence. It is not exotic infrastructure. It is basic infrastructure for managing complexity at scale. Understanding how to version your model deployments alongside your code is the natural extension of the registry into your development workflow.
