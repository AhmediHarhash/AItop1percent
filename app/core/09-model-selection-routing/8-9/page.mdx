# 8.9 â€” Licensing and Legal Considerations: Open-Source Licenses, Use Restrictions, and Indemnification

In March 2025, a rapidly growing social platform with 820 million monthly active users began transitioning from Claude API calls to self-hosted Llama 4 Maverick. The engineering team had done the math: at their scale, moving to an open-weight model would save $4.7 million annually in API costs. They provisioned the GPU infrastructure, quantized the weights, migrated their recommendation and moderation workloads, and shipped to production. Three weeks later, their legal team received a cease-and-desist letter from Meta. The company had exceeded Llama 4's commercial use threshold of 700 million monthly active users without obtaining a separate license. The platform was forced to halt all Llama-powered features, revert to API calls, negotiate a retroactive license at a premium rate, and delay two product launches while sorting out the mess. The engineering team had treated "open-source" as synonymous with "free to use however we want." It was not.

The licensing landscape for large language models in 2026 is a legal minefield that most engineering teams navigate with insufficient caution. You cannot treat model licensing the way you treat Python library licensing. The stakes are higher, the restrictions are more complex, and the consequences of non-compliance range from expensive license negotiations to complete product shutdowns. This subchapter maps the legal terrain you must navigate when selecting and deploying models, whether through APIs or self-hosted infrastructure.

## The Misnomer of Open Source Models

The term "open source" has a specific meaning in software licensing, defined by the Open Source Initiative: software whose source code is freely available for modification and redistribution under licenses like MIT, Apache 2.0, or GPL. Most "open-source" language models do not meet this definition. They are **open-weight models**: their trained parameters are publicly available for download, but their use is governed by custom licenses with restrictions that would disqualify them from OSI certification. The confusion between open-weight and open-source creates legal risk.

Llama 4 Maverick, released in late 2025, ships under the Llama 4 Community License. This license permits free use for research, evaluation, and commercial applications, but includes a hard threshold: if your product or service has more than 700 million monthly active users, you must request a separate license from Meta. The threshold exists to prevent the largest technology companies from using Llama to compete directly with Meta's products without paying for the privilege. Most startups and mid-sized companies operate comfortably below this threshold, but high-growth companies hit it faster than they expect. A video platform that crosses from 650 million to 750 million MAU in a single quarter must halt Llama usage until they negotiate terms, even if Llama powers non-customer-facing internal tools.

The license also includes competitive use restrictions. You cannot use Llama 4 to improve another large language model without explicit permission. If your company is training a competing foundation model, you cannot use Llama 4 outputs as training data, use Llama-generated synthetic data for fine-tuning, or distill Llama's capabilities into your own model. These restrictions are enforceable and actively monitored through both automated detection systems and industry tip-offs.

Mistral operates a dual-licensing model that creates even more confusion. Mistral 7B and Mixtral 8x7B are released under the Apache 2.0 license, a true open-source license with no use restrictions, no MAU thresholds, and no competitive limitations. You can use these models for any purpose, including training competing models, without asking permission. But Mistral Large 2 and Mistral Large 3, their flagship dense models, are released under a custom restrictive license similar to Llama's. Teams that assume all Mistral models share the same permissive licensing end up violating terms when they deploy the larger models.

DeepSeek V3.2, released in January 2026, uses an MIT license for both weights and inference code. This is the closest the industry has come to true open-source foundation models at frontier scale. You can use DeepSeek for any purpose, modify the weights, distill it into other models, and deploy it commercially without thresholds or approvals. The trade-off is that DeepSeek offers no warranties, no indemnification, and no support. You are entirely on your own for legal defense if someone claims your use of DeepSeek violates their patents or copyrights.

The first step in model selection is reading the actual license. Not the blog post announcement, not the model card summary, but the full license text. Assign this to your legal team, not your engineering team. Engineers optimize for functionality and cost; lawyers identify the landmines.

## Licensing Components: Weights, Training Data, and Architecture

Model licensing is not monolithic. A complete model release involves multiple components, each potentially under different licenses. The **weights** are the trained parameters. The **training data** is the corpus used to train those weights. The **architecture** is the model design: the number of layers, attention mechanism, tokenizer, and configuration. The **inference code** is the software used to run the model. Each can have separate legal terms.

Most open-weight models release the weights and inference code but not the training data. Llama 4's weights are available under the Llama 4 Community License, but Meta does not disclose the full training corpus. You can download the 405-billion-parameter model, but you cannot reconstruct or audit the data it was trained on. This creates legal exposure if the model produces outputs that infringe copyrights, because you cannot verify whether copyrighted material was included in training. When a publisher alleges that Llama reproduced substantial portions of their copyrighted books, you cannot definitively prove the training set excluded that content.

Some models release training data documentation but not the data itself. Anthropic publishes details about Claude's training data sources, filtering processes, and constitutional AI training methods, but does not release the raw datasets. This gives you transparency for risk assessment without exposing Anthropic's proprietary data curation. You can evaluate whether Claude's training process aligns with your legal risk tolerance, but you cannot independently verify every claim.

Architecture documentation is usually open. Llama 4's architecture paper describes the model design in sufficient detail that you could reimplement it from scratch, though you would need to retrain it on your own data. This openness benefits the research community but also enables competitors. Meta treats architecture as a publishable contribution to AI research; they treat the trained weights as commercial assets with licensing terms.

Inference code licensing matters for self-hosting. If you deploy Llama 4 on your own infrastructure, you need both the weights and the inference engine. Llama's reference implementation is released under a permissive license, but many teams use third-party inference engines like vLLM or TensorRT-LLM. These engines have their own licenses, dependencies, and patent grants. A conflict between the model license and the inference engine license can block your deployment. Before you commit to self-hosting, verify that every component in your serving stack is compatible with your intended use.

The distinction between these components becomes legally significant when you fine-tune or modify models. If you fine-tune Llama 4 on your proprietary data, do you own the resulting weights? The answer depends on whether the license treats fine-tuned weights as derivative works. Most licenses do, which means your fine-tuned model remains subject to the original license terms. You cannot take a model released under a restrictive license, fine-tune it, and then redistribute the fine-tuned version under a more permissive license. The original restrictions follow the weights through every modification.

## Use Restrictions That Catch Teams by Surprise

Beyond MAU thresholds and competitive use clauses, model licenses include use restrictions that seem innocuous until you encounter them. These restrictions are not hypothetical edge cases; they block real deployment scenarios every quarter.

Llama 4's license prohibits use in applications that violate Meta's acceptable use policy. This policy bans using Llama to generate content that promotes violence, hate speech, or illegal activities. On the surface, this seems reasonable and aligned with your own content policies. The problem emerges when your application operates in legally gray areas or controversial domains. A harm reduction organization that uses Llama to generate educational content about safe drug use might find that Meta's policy classifies this as promoting illegal activity, even though harm reduction is legal and evidence-based public health work. A news organization that uses Llama to summarize extremist manifestos for investigative reporting might trigger the hate speech clause, even though journalism is a protected use. The acceptable use policy is interpreted by Meta, not by you, and their interpretation can shut down your use without warning.

Some licenses restrict use in specific industries. Early versions of medical-domain fine-tuned models included clauses prohibiting use in clinical decision-making without regulatory approval. If you deployed such a model to assist doctors in diagnosing patients, you violated the license even if your application included disclaimers and human-in-the-loop safeguards. The model provider did not want liability exposure from medical malpractice claims, so they contractually prohibited the highest-risk use cases. You must verify that your intended use case is not explicitly carved out by the license.

Geographic restrictions appear in models trained or hosted in countries with export controls. If a model is classified as dual-use technology under U.S. export regulations, deploying it in certain countries or making it available to users in those countries may require an export license. DeepSeek, developed in China, faces similar restrictions in reverse: some jurisdictions restrict importing Chinese AI models for use in critical infrastructure. Your legal team must map model provenance to export control regimes before deploying internationally.

Redistribution restrictions limit your ability to share models with partners, customers, or open-source communities. Llama's license allows you to use the model in your own products but restricts redistributing the weights to third parties. If you build a SaaS product powered by Llama and want to offer an on-premise version to enterprise customers, you cannot simply bundle the Llama weights into your deployment package. Each customer must download Llama directly from Meta and agree to the license terms themselves. This complicates your sales process and support model.

Modification restrictions govern fine-tuning and distillation. Some licenses permit fine-tuning for your own use but prohibit distributing the fine-tuned weights. Others allow fine-tuning but classify any fine-tuned model as a derivative work subject to the same license terms as the base model. If you fine-tune Llama 4 on proprietary medical data and the fine-tuned model qualifies as a derivative work, you may be required to make it available under the Llama 4 Community License if you distribute it, even though your training data is proprietary. Legal interpretation of what constitutes a derivative work in the context of fine-tuned model weights is still evolving in case law.

Time-based restrictions occasionally appear in model licenses. Some early releases of GPT-2 and GPT-3 alternatives included staged release schedules where full model weights were initially restricted and then opened after a delay. Bloom, the multilingual model released in 2022, had research-only restrictions for the first six months. These restrictions exist to give the model provider time to assess safety risks before widespread deployment. If you plan your product roadmap around a model that is currently research-only, you must account for the possibility that the license terms will not change when expected.

## Indemnification: What API Providers Offer and Open-Weight Models Do Not

When you call OpenAI's API to generate content, and that content later becomes the subject of a copyright infringement lawsuit, who is liable? The question of legal indemnification separates API providers from self-hosted open-weight models more starkly than any technical consideration.

OpenAI offers **Copyright Shield**, announced in late 2024 and expanded in 2025. If you use OpenAI's API and a third party sues you for copyright infringement based on outputs generated by GPT-5.2, OpenAI will defend the lawsuit and pay any resulting damages, subject to certain conditions. The conditions include that you used the API in compliance with OpenAI's terms of service, implemented reasonable filtering to avoid generating infringing content, and did not intentionally prompt the model to reproduce copyrighted works. Copyright Shield does not cover you if you explicitly prompt GPT-5.2 with "reproduce the first chapter of a bestselling novel" and distribute the output. But if you use the API for legitimate content generation and the model inadvertently produces text similar to copyrighted material, OpenAI assumes the liability.

Anthropic offers similar indemnification for Claude API users. If you are sued for intellectual property infringement based on Claude Opus 4.5 outputs, Anthropic will indemnify you under the same general framework: you must be using the API in good faith, in compliance with terms, without deliberately attempting to extract copyrighted training data. Google provides IP indemnity for Gemini 3 API customers, covering both copyright and patent claims related to model outputs.

These indemnification commitments are not symbolic. They represent real legal risk transfer. A major publisher sued an API customer in mid-2025, alleging that AI-generated marketing copy infringed their copyrighted style guides. The API provider stepped in, covered the legal defense, and settled the case without the customer paying anything beyond their regular API fees. For enterprises operating in litigious industries, this legal protection is worth more than the API cost savings from self-hosting.

Self-hosted open-weight models offer zero indemnification. If you deploy Llama 4 Maverick on your own infrastructure and generate content that infringes someone's copyright, you are solely liable. Meta's license explicitly disclaims all warranties and indemnification. The license states that the model is provided "as is" without any warranty of non-infringement. If you are sued, you defend yourself, you pay your own legal fees, and you pay any settlement or judgment. The same applies to DeepSeek, Mistral, and every other open-weight model. The cost savings from avoiding API fees can be obliterated by a single intellectual property lawsuit.

Patent risk is a related but distinct concern. Large language models may practice patents held by other companies, covering techniques like attention mechanisms, tokenization methods, or training algorithms. API providers typically include patent grants or indemnification: if you use their API and a third party claims you are infringing their patents through the model's operation, the provider assumes that risk. Open-weight models include no such grant. If a patent troll identifies that Llama 4's architecture practices their patent and sues you for deploying Llama, you are the defendant. Meta is not obligated to assist. Patent litigation is even more expensive than copyright litigation, often running into millions of dollars in legal fees before reaching summary judgment.

The indemnification calculation is simple: multiply the probability of being sued by the expected cost of defense and settlement, then compare that expected value to the cost difference between API calls and self-hosting. For a consumer application with user-generated content moderation, the probability is moderate and the stakes are high. For an internal enterprise tool with no public outputs, the probability is low. Adjust your model selection accordingly.

The scope of indemnification matters as much as its existence. Some API providers cap their indemnification at the total fees paid by the customer over the prior twelve months. If you pay $50,000 per year in API fees and face a $2 million copyright claim, the provider covers the first $50,000 and you cover the rest. Other providers offer uncapped indemnification but with extensive carve-outs for customer misuse, prohibited applications, or failure to implement recommended safeguards. Read the indemnification clause in your API contract carefully. Understand what triggers coverage, what excludes it, and what your obligations are to maintain coverage.

## Patent Grants and Open Innovation Commitments

Some open-weight models include **patent grants** that protect users from patent infringement claims related to using the model. A patent grant is a commitment from the model provider that they will not sue you for infringing their patents when you use the model as licensed. This is standard in open-source software licenses like Apache 2.0, which includes an explicit patent grant covering any patents the licensor holds that are practiced by the licensed software.

Llama 4's license includes a limited patent grant. Meta grants you a license under their patents to use, modify, and distribute Llama 4, but only as long as you comply with the license terms. If you violate the license, the patent grant terminates automatically, and you become a potential patent infringer. This termination clause gives Meta leverage to enforce license compliance: if you exceed the MAU threshold without negotiating a new license, Meta can threaten patent litigation in addition to contract claims.

Some models participate in **open innovation commitments** like the Open Invention Network or the LOT Network, which are cross-licensing agreements designed to prevent patent litigation in specific technology domains. These commitments provide some protection against patent trolls but do not eliminate risk entirely. If you deploy an open-weight model and a non-participant third party claims you infringe their AI-related patents, the open innovation commitment does not shield you.

The absence of a patent grant in a model license is a red flag. If the license is silent on patents, you have no explicit protection. The model provider could hold patents on techniques used in the model and could theoretically sue you for using the model, even if they released the weights publicly. This scenario is unlikely but not impossible, especially if the provider later decides to monetize their patent portfolio or if the model is acquired by a patent assertion entity.

## Export Controls and Cross-Border Considerations

Advanced AI models are increasingly classified as dual-use technologies subject to export controls. In the United States, the Bureau of Industry and Security regulates the export of emerging technologies that have both commercial and military applications. As of 2026, frontier language models capable of certain capabilities, such as generating functional malware, assisting in cyber operations, or enabling mass surveillance, may require export licenses for deployment in specific countries.

If you operate a global product and deploy a self-hosted model in data centers across multiple countries, you must verify that your deployment complies with export regulations. Hosting Llama 4 in a U.S.-based data center and serving requests from users worldwide is generally permissible. Transferring the model weights to a data center in a country on the Entity List or subject to trade restrictions may require prior authorization. The controls apply to the model itself, not to the outputs, so serving API-generated content across borders is typically unrestricted, but moving the model weights is controlled.

The EU AI Act, enforced as of mid-2025, imposes transparency obligations on deployers of general-purpose AI models. If you deploy a GPAI model with systemic risk classification, you must provide transparency reports describing the model's capabilities, limitations, and risk mitigations. For self-hosted models, you are the deployer, and you are responsible for these disclosures. For API-accessed models, the provider is typically the deployer, and they handle the transparency obligations. This regulatory distinction shifts compliance burden from you to the provider when you use APIs, another hidden cost advantage.

China's AI regulations include similar deployer obligations, plus requirements for algorithm registration and content filtering. If you deploy a large language model in China, you must register it with the Cyberspace Administration of China and implement content controls aligned with Chinese policy. These requirements apply whether you self-host or use APIs, but API providers operating in China typically handle registration on behalf of customers, simplifying your compliance. Self-hosting means you own the entire compliance stack.

Export controls create complex scenarios when models are developed in one jurisdiction and deployed in another. DeepSeek V3.2, developed in China, is subject to Chinese export regulations. If Chinese authorities classify frontier AI models as controlled exports, deploying DeepSeek in certain countries could require an export license from China. Simultaneously, importing DeepSeek into the United States or EU could require an import review if the model is classified as a security-relevant technology. Your legal team must navigate both export and import regulations when working with models developed in geopolitically sensitive regions.

## The EU AI Act and GPAI Model Obligations

The EU AI Act, which came into full enforcement in 2025, introduces specific obligations for providers and deployers of **general-purpose AI models**. A GPAI model is one that can be used for a wide range of tasks without task-specific training. Llama 4, Claude Opus 4.5, GPT-5.2, and DeepSeek V3.2 all qualify as GPAI models under the Act.

If a GPAI model is classified as presenting **systemic risk**, which generally applies to models with more than 100 billion parameters or equivalent computational training cost, the provider must conduct adversarial testing, evaluate and mitigate systemic risks including cybersecurity threats and societal impacts, track serious incidents, ensure cybersecurity protections for the model infrastructure, and report on energy consumption and environmental impact. These obligations fall on the model provider, not the deployer, when you access the model via API. When you self-host, you become the deployer, and you inherit obligations to implement risk mitigation measures, document your deployment, and report serious incidents to regulators.

The distinction between provider and deployer is critical. If you use Claude Opus 4.5 via Anthropic's API, Anthropic is the provider and handles GPAI obligations. If you download Llama 4 and deploy it on your infrastructure, you are the deployer and must implement your own risk management processes. The compliance burden is not symmetric. API usage shifts regulatory risk to the provider. Self-hosting shifts it to you.

The EU AI Act also requires transparency about the use of AI systems in certain high-risk contexts. If you deploy a GPAI model in a high-risk application as defined by the Act, such as biometric identification, critical infrastructure management, or law enforcement, you must register the system, maintain technical documentation, implement human oversight, and conduct conformity assessments. These requirements apply regardless of whether you use an API or self-host, but API providers increasingly offer compliance documentation packages that simplify the conformity assessment process.

## Legal Review Checklists Before Deploying Any Model

Your legal team must review every model before deployment, whether API-accessed or self-hosted. The review checklist includes the following mandatory items.

First, confirm the license type and verify that your intended use is permitted. Read the full license text, not the summary. Identify any MAU thresholds, competitive use restrictions, geographic limitations, or prohibited applications. Document your user count projections and verify you will remain under thresholds for at least 24 months, accounting for growth.

Second, assess indemnification coverage. If using an API, confirm the provider's indemnification terms cover your use case and jurisdiction. If self-hosting, quantify the legal risk exposure from copyright, patent, and regulatory claims. Calculate the expected cost and determine whether insurance or reserve funds are needed.

Third, evaluate training data transparency. Determine whether the model provider discloses training data sources and filtering methods. If not, assess whether the lack of transparency creates unacceptable risk for your industry. Medical, legal, and financial applications typically require higher transparency than consumer entertainment applications.

Fourth, verify export control compliance. If you operate internationally or plan to, map your deployment architecture to export regulations. Identify any controlled jurisdictions and determine whether you need export licenses or must exclude those regions from your deployment.

Fifth, review acceptable use policies and terms of service. Identify any prohibited use cases and confirm your application does not fall into those categories. If your use case is borderline, seek written clarification from the provider before deploying.

Sixth, assess modification and redistribution rights. If you plan to fine-tune the model, verify the license permits it and clarify whether the fine-tuned model is a derivative work subject to the original license. If you plan to redistribute weights to customers or partners, confirm the license allows it.

Seventh, confirm liability caps and warranty disclaimers. Understand what the provider warrants and what they disclaim. If the model is provided with no warranties, ensure your own terms of service with customers include appropriate disclaimers and limitations of liability.

Eighth, evaluate termination and transition rights. If the provider discontinues the model, changes the license terms, or terminates your access, what are your options? API providers can shut off access immediately; self-hosted models remain available as long as you have the weights, but updates and support may cease.

Ninth, review patent grants and defensive commitments. Confirm whether the license includes a patent grant and whether it terminates if you violate license terms. Assess whether the provider participates in open innovation networks that provide additional patent protection.

Tenth, verify compliance with industry-specific regulations. If you operate in healthcare, confirm the model can be used in a HIPAA-compliant manner. If you operate in finance, confirm it meets SOX or GDPR requirements. If you deploy in the EU, confirm you can satisfy AI Act transparency and risk management obligations.

## When Legal Risk Blocks Model Adoption Entirely

Legal risk blocks model adoption in four scenarios: when your company exceeds use thresholds, when your use case falls into prohibited categories, when you cannot accept unindemnified IP liability, and when regulatory requirements cannot be met under the license terms.

User thresholds are the most common blocker. If your company has 800 million monthly active users, you cannot use Llama 4 under the community license. You must negotiate a separate agreement with Meta, which may involve licensing fees, usage audits, and custom terms. If you are a subsidiary or portfolio company of a larger entity that exceeds the threshold, the restriction applies to you even if your product has a small user base. This makes Llama 4 unsuitable for any company operating at or near the threshold, regardless of cost savings.

Prohibited use cases block adoption when your application falls into a category the license explicitly restricts. DeepSeek's license includes restrictions on certain government and defense applications in specific jurisdictions. If you are building a threat detection system for a restricted government agency, DeepSeek is not an option. Qwen3's license prohibits use in applications that compete directly with Alibaba's cloud services. If you are building a conversational AI product that competes with Alibaba's own offerings, Qwen3 is not an option.

Unindemnified IP liability blocks adoption when the financial risk of an IP claim exceeds the cost savings of self-hosting. If you operate in a high-litigation industry like media, entertainment, or publishing, the risk of a copyright infringement claim based on model-generated content is material. If your API provider indemnifies you up to ten million dollars and your open-weight license disclaims all liability, the risk delta is ten million dollars. That risk may be acceptable if self-hosting saves you fifteen million dollars per year. It is not acceptable if self-hosting saves you two million dollars per year.

Regulatory compliance blocks adoption when your jurisdiction or industry imposes obligations that the license does not address. The EU AI Act requires high-risk AI systems to undergo conformity assessments, maintain technical documentation, and implement risk management processes. Open-weight model licenses do not include compliance certifications, conformity statements, or audit rights that map cleanly to these requirements. If you deploy Llama 4 in a high-risk application under the EU AI Act, you must conduct your own conformity assessment and maintain your own documentation. API providers increasingly offer compliance packages that include EU AI Act documentation, SOC 2 attestations, and ISO certifications. Open-weight models do not, which means you own the compliance burden entirely.

Every model selection decision is a legal decision as much as a technical one. The right model with the wrong license can destroy your product. The wrong assumptions about indemnification can expose you to millions in legal liability. The failure to review export controls can result in regulatory enforcement actions. Legal review is not an optional gate. It is the first filter in your model selection process, before performance benchmarks, before cost analysis, before technical integration. If a model fails legal review, it does not matter how well it performs or how much it costs. You cannot use it. The next subchapter addresses the final cost dimension that most teams underestimate: the total cost of self-hosting beyond GPU bills.
