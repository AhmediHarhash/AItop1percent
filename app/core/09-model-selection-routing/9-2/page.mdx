# 9.2 — Version Pinning: Why You Must Never Point at a Moving Target

In August 2025, a customer support automation platform serving 3,400 small businesses started receiving complaints about their AI chatbot giving "weird responses." The support team investigated and found nothing wrong — no code changes, no configuration updates, no infrastructure issues. But the complaints kept coming. Users reported that the chatbot was now "too formal," that it was "missing context from earlier messages," and that it sometimes "repeated itself unnecessarily." The engineering team dug deeper and discovered the root cause: they had configured their system to use the model alias "gpt-4" instead of a specific dated snapshot. OpenAI had silently updated what "gpt-4" pointed to, switching from GPT-4-0613 to GPT-4-1106-preview in mid-August. The new snapshot had different behavior — slightly more formal tone, different context window handling, different repetition penalties. The behavior changes were not bugs. They were intentional improvements in the newer model. But they broke the implicit contract the support platform had with its users, who had grown accustomed to the old behavior. The company spent three weeks re-tuning prompts, updating system messages, and gradually rolling back to a pinned snapshot. The failure was not a technical accident. It was a design choice: they had pointed their production system at a moving target and paid the price when the target moved.

Version pinning is the practice of specifying exact model versions instead of aliases or automatic-update pointers. When you use "gpt-4-0613" instead of "gpt-4," you are pinning. When you use "claude-3-5-sonnet-20240620" instead of "claude-3-sonnet-latest," you are pinning. When you use "gemini-1.5-pro-001" instead of "gemini-pro," you are pinning. Pinning locks your system to a specific model snapshot with known behavior, known capabilities, and known failure modes. Unpinned aliases give you automatic updates, which sounds convenient until the update breaks your carefully tuned prompts, shifts your evaluation metrics, or changes user-visible behavior in ways your users notice and dislike. Professional AI engineering teams pin their model versions in production. Always. No exceptions. The only time you use an alias is in non-critical paths like internal tools, development environments, or explicit experimentation workflows where behavior changes are expected and acceptable.

## What Happens When You Use an Alias Instead of a Pinned Version

The support platform's experience is not unique. Across 2024 and 2025, dozens of production systems suffered silent behavior changes because they used model aliases that shifted underneath them. A legal document drafting tool using "gpt-4-turbo" discovered in November 2024 that their citation formatting had changed — the model was now placing citations inline instead of at the end of paragraphs. The change happened because OpenAI had updated the "gpt-4-turbo" alias to point to a newer snapshot with different instruction-following behavior. The legal tool's evaluation suite did not catch the change immediately because their tests checked citation presence, not citation placement. Users noticed within two days.

A healthcare chatbot using Anthropic's "claude-3-sonnet" alias experienced a shift in refusal behavior in mid-2025 when Anthropic updated the alias to point from Claude 3 Sonnet to Claude Opus 4.5. The newer model had more conservative safety filters, which caused it to refuse certain mental health questions that the older model had answered. The chatbot's product team had calibrated their system messages and safety rails assuming the Claude 3 Sonnet refusal rate, which was roughly 2% on their test set. Claude Opus 4.5's refusal rate on the same test set was 5%. That three percentage point difference translated to 600 additional refusals per day across their user base. Users interpreted the refusals as the chatbot "getting worse" and "not understanding mental health anymore." The product team had to emergency-tune their prompts to reduce refusals back to the expected baseline, a process that took two weeks and required re-running 18,000 test cases.

A financial analysis tool using Google's "gemini-pro-latest" alias saw their JSON output parsing break in September 2025 when Google updated the alias to point from Gemini 1.5 Pro to Gemini 2.0 Pro. The newer model used slightly different JSON formatting — it added whitespace for readability, which was technically valid JSON but broke the tool's regex-based parser that expected compact formatting. The parsing failures caused 4% of requests to return errors until the engineering team updated their parser to handle both formats. The downtime lasted six hours, affected 1,200 users, and triggered SLA penalties with three enterprise customers.

These failures share a common pattern: the engineering teams did not make a mistake in their code. They did not deploy a bad configuration. They did not introduce a regression. The model providers updated the underlying model that the alias pointed to, and the update changed behavior in ways that broke implicit assumptions. The teams were flying blind — they had no visibility into when alias updates would happen, no changelog of what changed, and no testing window before the change went live in production. They were pointing at a moving target and discovered the target had moved only when users complained.

## How Each Provider Handles Versioning and Aliases

OpenAI offers both dated snapshots and automatic-update aliases. Dated snapshots follow the pattern "gpt-4-0613" or "gpt-4-1106-preview," where the numbers represent the release date. These snapshots are fixed — they never change behavior. When you use "gpt-4-0613," you get the same model behavior today as you did on the day it launched. OpenAI also offers aliases like "gpt-4," "gpt-4-turbo," and "gpt-4o," which automatically point to the latest recommended snapshot in that model family. The alias "gpt-4" has pointed to at least four different underlying snapshots between 2023 and 2025, including "gpt-4-0314," "gpt-4-0613," "gpt-4-1106-preview," and "gpt-4-0125-preview." Each time the alias updates, behavior changes. OpenAI does not announce alias updates in advance. They document the current mapping in their API documentation, but the mapping can shift without notice. If you use an alias in production, you are opting into silent behavior changes.

Anthropic uses versioned model IDs that include release dates, making version pinning the default behavior. Model IDs like "claude-3-5-sonnet-20240620" and "claude-opus-4-5-20251101" are specific snapshots that never change. Anthropic also offers some convenience aliases like "claude-3-sonnet-latest," but these are less commonly used because the versioned IDs are the primary interface. This design choice pushes users toward pinning by default. When you copy a model ID from Anthropic's documentation or API examples, you are copying a pinned version. The trade-off is that Anthropic's model IDs are longer and harder to remember, but the benefit is that accidental alias usage is rare. Anthropic does not silently update model IDs — if you want a newer version, you explicitly change your configuration to reference the new model ID.

Google offers three levels of versioning for Gemini models: specific dated snapshots like "gemini-1.5-pro-001," stable aliases like "gemini-pro-stable," and latest aliases like "gemini-pro-latest." The dated snapshots are fixed and never change. The stable alias points to a verified production-ready snapshot that Google updates infrequently, typically every few months, with release notes and advance notice. The latest alias points to the newest available snapshot, which can update as often as weekly for fast-moving model families. Google's documentation recommends using stable for production workloads and latest for experimentation, but many teams use latest in production because it appears first in code examples and offers the "best" model at any given time. This creates the same alias risk as OpenAI's automatic-update pointers — silent behavior changes that break assumptions.

Meta's Llama models do not have a centralized versioning system because they are released as open weights, not API services. Each hosting provider creates their own versioning scheme. Together AI uses model IDs like "meta-llama/Llama-3-70b-chat-hf" with version tags. Replicate uses versioned API endpoints. AWS Bedrock uses model ARNs that include version identifiers. Azure AI uses model catalog entries with semantic versioning. The fragmentation means version pinning discipline depends entirely on which hosting provider you use. If you self-host Llama models, you control the versioning entirely — you can pin to a specific model checkpoint, run multiple versions in parallel, or update on your own schedule. The open weights model shifts version management from the provider to the operator, which offers control but requires discipline.

## The Version Pinning Discipline: How to Pin Correctly

Version pinning starts with configuration discipline. Your model configuration should never use an alias in production. Store the exact model version string in a configuration file, environment variable, or feature flag system. When you deploy a new use case, start with a pinned version from day one. When you want to upgrade to a newer model, make it an explicit configuration change with testing, validation, and gradual rollout. Never allow "automatic updates" for production model versions. Automatic updates are a feature for operating systems, web browsers, and mobile apps where the update provider has deep testing infrastructure and rollback capability. Model providers do not have that same level of confidence in cross-customer compatibility — they cannot test every prompt, every use case, and every downstream system. You must own the validation.

Your model registry must track version-to-behavior mappings. For every model version you use in production, document the key behavioral characteristics: tone, refusal rate, output length distribution, JSON formatting style, citation placement, context window handling, and any other dimension that matters to your use case. When you evaluate a new model version as a replacement candidate, compare it against this documented baseline. A new version might score higher on your primary accuracy metric but change behavior on a secondary dimension that matters to users. The support platform learned this the hard way — GPT-4-1106-preview had better overall quality than GPT-4-0613, but the tone shift broke user expectations. If they had documented tone as a tracked behavioral dimension, they would have caught the issue in testing instead of production.

Your deployment process must support gradual rollout of new model versions. When you decide to upgrade from one pinned version to another, do not flip a switch and move 100% of traffic instantly. Deploy the new version to 5% of traffic first. Monitor for behavior changes, error rate increases, user complaints, and evaluation metric shifts. If the 5% rollout looks clean after 24 hours, expand to 25%. Then 50%. Then 100%. At each stage, maintain the ability to roll back to the old version instantly if something breaks. This gradual rollout pattern is standard practice for code deployments, infrastructure changes, and feature flags. It applies equally to model version changes. The model is a dependency. Changing it is a deployment. Treat it with the same operational rigor.

Your evaluation framework must support multi-version testing. Before you decide to migrate from version A to version B, run your full evaluation suite against both versions in parallel. Compare the results side by side. Look for regressions on any metric you care about. Look for behavioral shifts that might not show up in aggregate metrics but matter to specific user segments. A model version might have the same average accuracy as its predecessor but perform worse on edge cases, on non-English languages, on technical jargon, or on sensitive topics. Parallel testing surfaces these differences before you commit to the migration.

Your alerting system should warn you when a model version you depend on approaches deprecation. Track the deprecation timeline for every pinned version in production. Alert at 180 days, 90 days, 60 days, 30 days, and 7 days before end-of-life. These alerts give you time to plan migration, test replacement candidates, and execute gradual rollout. If you wait until the deprecation notice arrives to start planning, you are already behind. The best teams identify replacement candidates months in advance, test them in shadow mode, and have a migration plan ready to execute when the deprecation notice becomes official.

## The Exception: When Aliases Are Acceptable

Aliases are not universally wrong — they are wrong in production for critical paths. There are contexts where alias usage is appropriate and even beneficial. Internal tools that you and your team use for research, data analysis, or content generation can safely use aliases because the cost of behavior change is low. If your internal documentation generator shifts tone because "gpt-4-turbo" updated, you notice, you adjust, and you move on. No users are affected. No SLA is violated. The convenience of automatic updates outweighs the risk of behavior change.

Development and staging environments can use aliases to continuously test against the latest model versions. If your staging environment uses "gemini-pro-latest," you get early warning when Google releases a new snapshot that might break your production system. You can catch parsing issues, refusal rate changes, and tone shifts in staging before you intentionally migrate production. This pattern turns alias usage into a proactive testing strategy rather than a reactive risk.

Experimentation workflows and A/B testing frameworks can use aliases when the goal is to compare current production behavior against the latest available model. If you are running an experiment to see whether upgrading to the newest GPT-4 snapshot improves user satisfaction, using "gpt-4-turbo" in the experiment arm is appropriate because you want to test "whatever is newest" rather than a specific snapshot. When the experiment concludes and you decide to promote the winning variant to production, you switch from the alias to the specific pinned version that won.

Non-critical paths in your user-facing product can sometimes tolerate aliases if the value of automatic updates outweighs the risk of behavior change. A low-stakes feature like "generate a fun subject line for this email" might benefit from always using the latest model without the overhead of version testing and migration. But you must make this trade-off explicitly. You must document that the feature uses an unpinned alias, accept the risk of silent behavior changes, and ensure the feature's failure or behavior shift does not cascade into critical user flows. Most features do not meet this bar. Most features should be pinned.

The general principle is this: if a model behavior change could cause user-visible disruption, SLA violations, compliance risk, or revenue impact, pin the version. If a model behavior change is low-stakes, reversible, or even desirable, an alias might be acceptable. When in doubt, pin. The cost of pinning is low — you write a longer model ID string in your configuration. The cost of not pinning is high — silent behavior changes, emergency re-tuning, and user trust erosion.

## How to Set Up Version Tracking in Your Model Registry

A professional model registry tracks every model version in use across your systems. At minimum, it must include the exact model ID, the use case or service using it, the deployment date, the owner team, and the deprecation status. More mature registries also track evaluation metrics per version, known behavioral characteristics, replacement candidates, and migration plan status. The registry is not a static document — it is a living operational system that updates when you deploy new models, when providers announce deprecations, and when you complete migrations.

Your registry should integrate with your deployment and configuration systems. When an engineer deploys a new service that uses a model, the deployment pipeline automatically adds the model version to the registry. When a deprecation notice arrives from a provider, the registry highlights which services are affected and who owns them. When you test a replacement candidate and validate it meets your quality bar, the registry records the tested version and migration readiness status. This automation prevents the registry from falling out of sync with reality.

Your registry should support querying and reporting. Product and engineering leadership should be able to ask "how many model versions are we running in production?" and get an instant answer. Security and compliance teams should be able to ask "which models are approaching end-of-life in the next 90 days?" and trigger migration planning. Finance teams should be able to correlate model version usage with API costs to understand which versions are most expensive to run at scale. The registry is not just a compliance artifact — it is an operational intelligence system.

Some teams build custom model registries using internal databases and configuration management tools. Others use off-the-shelf MLOps platforms that include model versioning features. The implementation matters less than the discipline. If you do not know which model versions you are running in production, you cannot plan migrations, you cannot respond to deprecation notices, and you cannot validate that new versions meet your quality bar before you deploy them. The registry is the foundation of version control discipline.

## Why Version Pinning Is Not Optional in 2026

The model release cadence in 2026 is faster than ever. OpenAI released GPT-5 and GPT-5.2 within six months of each other. Anthropic shipped Claude Opus 4.5 just months after Claude Opus 4. Google launched Gemini 3 with multiple size variants and rapid snapshot updates. DeepSeek released V3.2 with significant capability improvements over V3. The pace is accelerating, not stabilizing. Every new release creates pressure to upgrade, and every upgrade creates risk if you have not pinned your current version and tested the replacement.

The cost of unpinned aliases is no longer a theoretical risk — it is a demonstrated operational failure mode with real business impact. The support platform lost user trust and spent three weeks recovering from a silent model update. The legal document tool faced citation formatting issues that required emergency fixes. The healthcare chatbot saw refusal rates spike and had to re-tune their entire prompt library. These are not edge cases. They are predictable outcomes of pointing production systems at moving targets.

The discipline of version pinning is simple: always specify exact model versions in production configuration, never use aliases for critical paths, track version-to-behavior mappings in your model registry, and test replacement candidates before migrating. The tooling is available — every major provider offers versioned model IDs, and most provide deprecation timelines and migration documentation. The only missing ingredient is discipline. Teams that adopt version pinning as a non-negotiable standard avoid the silent behavior change trap. Teams that treat aliases as convenient shortcuts discover the convenience was an illusion when the model updates and their system breaks.

Version pinning protects you from silent changes, but it does not protect you from the need to migrate when versions deprecate. The next challenge in model lifecycle management is understanding how to evaluate replacement candidates and execute migrations with confidence — a process that begins with building a robust model evaluation framework.
