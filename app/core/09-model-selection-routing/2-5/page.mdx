# 2.5 — Running Model Bake-Offs: Protocol, Sample Size, and Statistical Rigor

In March 2025, a customer service automation company spent three weeks evaluating five different models for their ticket classification system. They ran all five models against what they called a "representative sample" of fifty support tickets, scored the outputs, and selected the model with the highest accuracy. The engineering team was confident. They had run the numbers, documented the results, and made a data-driven decision. Three months after deployment, the production metrics told a different story. The model they had selected performed nearly identically to the runner-up they had rejected, and both were significantly worse than a third candidate they had eliminated in the first round. The bake-off had given them false confidence in a choice that statistical analysis would have revealed as meaningless.

The problem was not that they ran a bake-off. The problem was that they ran a bake-off without statistical rigor. Fifty examples is not enough to distinguish between models that differ by five percentage points in accuracy. The evaluation was not blinded, so the evaluators knew which outputs came from which model, introducing unconscious bias toward the more expensive provider. The sample was not stratified, so rare but critical ticket types were underrepresented. The team treated the bake-off results as definitive when they were actually within the margin of error. This is not how you select a model for production. This is how you create the illusion of a rigorous decision while actually making a guess.

## What a Bake-Off Actually Is

A **bake-off** is a structured head-to-head evaluation of multiple models on the same task, using identical inputs, identical evaluation criteria, and identical conditions. The goal is to identify which model performs best for your specific use case, given your specific constraints. A bake-off is not a theoretical comparison of model capabilities. It is not a review of published benchmarks. It is not a gut-check based on trying a few examples. It is a controlled experiment designed to produce statistically valid conclusions about model performance on your task.

The bake-off is the highest-fidelity method for model selection because it directly measures what you care about. You do not evaluate the model's ability to answer general knowledge questions unless your task is general knowledge questions. You do not evaluate the model's ability to write poetry unless your task is writing poetry. You evaluate the model's ability to perform your task, on your data, with your constraints, using your success criteria. This is the only way to get signal that translates directly to production performance.

But a bake-off is also expensive. You are paying for inference from multiple models, paying for human evaluation time, paying for engineering time to set up and run the comparison, and paying opportunity cost while you delay shipping. You need to know when a bake-off is worth it and when it is not. You need to know how to run one correctly. You need to know how to interpret the results. Most organizations that run bake-offs get at least one of these three things wrong.

## The Bake-Off Protocol

The protocol for running a valid bake-off has six steps. First, you **define the task** with precision. This is the same task definition you created during problem framing, including input schema, output schema, success criteria, and constraints. The task definition must be identical for all models in the comparison. You cannot evaluate one model on ticket classification and another model on ticket summarization and then compare the results. You cannot allow one model to use chain-of-thought reasoning and another to generate direct answers and then treat the comparison as fair. The task must be the same.

Second, you **select the evaluation data**. This is a sample of real or realistic inputs that represent the distribution of cases you will see in production. The sample must be large enough to produce statistically meaningful results, stratified to ensure rare cases are represented, and free from contamination by training data. If you are evaluating a customer support model, the evaluation data is a sample of real customer support tickets. If you are evaluating a contract review model, the evaluation data is a sample of real contracts. The evaluation data is not synthetic examples you made up. It is not cherry-picked cases that highlight one model's strengths. It is a representative sample.

Third, you **set the scoring criteria**. This is the rubric you will use to evaluate model outputs. The rubric must be objective, measurable, and aligned with your success criteria. If your success criteria include accuracy, latency, and tone, your scoring rubric must measure all three. The rubric must be detailed enough that two independent evaluators will produce similar scores for the same output. Vague criteria like "good quality" or "professional tone" are not sufficient. You need precise definitions of what counts as correct, what counts as acceptable, and what counts as failure.

Fourth, you **run all models under identical conditions**. Every model receives the same inputs, in the same order, with the same prompt template, the same temperature, the same max tokens, and the same timeout. If you are testing Claude Opus 4.5, GPT-5.2, and Gemini 3, all three models get exactly the same input for case one, exactly the same input for case two, and so on. You do not tune the prompt for each model. You do not give one model more tokens because it tends to be verbose. You do not retry failures for one model but not the others. Identical conditions means identical.

Fifth, you **score the outputs**. Ideally, this is done by multiple independent evaluators who are blinded to which output came from which model. Blinding prevents bias. If the evaluator knows that output A came from the expensive flagship model and output B came from the cheap distilled model, they are more likely to score A higher even when the quality is identical. Human psychology is predictable. Remove the source of bias by removing the information. Randomize the order, strip the model identifiers, and score based only on the output content.

Sixth, you **analyze the results with statistical rigor**. Calculate the mean score for each model, the standard deviation, the confidence interval, and the p-value for pairwise comparisons. Determine whether the observed differences are statistically significant or could have occurred by chance. Document the results, including the full distribution of scores, not just the mean. A model with a mean score of 85 percent and a standard deviation of 3 percent is not the same as a model with a mean score of 85 percent and a standard deviation of 15 percent, even though the averages are identical. The second model is inconsistent. Inconsistency is a production risk.

## Why Fifty Examples Is Not Enough

The most common mistake in bake-offs is using too few evaluation examples. Fifty examples feels like a lot. It takes hours to evaluate fifty outputs from five models. But fifty examples is not enough to distinguish between models that differ by five percentage points in accuracy, and five percentage points is often the difference between a model that works and a model that does not.

Statistical power is the probability that your test will detect a real difference when one exists. If you run a bake-off with low statistical power, you will fail to detect meaningful differences and conclude that all models perform the same when they do not. The statistical power of a comparison depends on the sample size, the effect size you want to detect, and the variance in the data. For typical classification tasks, detecting a five percentage point difference in accuracy with 80 percent statistical power requires at least 300 examples per model. Detecting a three percentage point difference requires at least 800 examples. Fifty examples gives you the power to detect only very large differences, on the order of fifteen to twenty percentage points. If the true difference between your models is smaller than that, your bake-off will not reliably identify the better model.

This is not hypothetical. Industry experience consistently shows that organizations using fewer than 200 evaluation examples frequently make the wrong model choice, selecting a model that underperforms the best available option by more than five percentage points in production. Organizations using larger evaluation sets — 500 examples or more — make significantly better decisions because the statistical power is sufficient to distinguish real performance differences from noise. Sample size matters.

If you cannot afford to evaluate 300 examples, you cannot afford to run a bake-off. Use a smaller set of models, use a faster evaluation method, or accept that your decision will be based on incomplete information. Do not pretend that fifty examples gives you statistical confidence. It does not.

## Stratification and Representation

Even with a large sample, you need to ensure that the evaluation data represents the full distribution of cases you will see in production. If 80 percent of your support tickets are simple password resets and 20 percent are complex billing disputes, but your evaluation sample is 100 percent password resets, you have optimized for the wrong task. The model that performs best on password resets may perform worst on billing disputes. You will not discover this until production.

**Stratified sampling** means dividing your data into meaningful subgroups and sampling proportionally from each subgroup. If 20 percent of your production cases are billing disputes, 20 percent of your evaluation cases should be billing disputes. If 5 percent of your cases involve non-English languages, 5 percent of your evaluation cases should involve non-English languages. Stratification ensures that rare but important cases are represented in the evaluation.

Stratification also allows you to analyze model performance by subgroup. You may find that model A outperforms model B overall, but model B significantly outperforms model A on the subgroup you care most about. A contract review model might have 90 percent accuracy overall but only 70 percent accuracy on contracts involving intellectual property clauses, which are the highest-risk cases for your legal team. The overall number hides the critical weakness. Stratification reveals it.

If you do not have enough production data to stratify, you need to create synthetic examples for the underrepresented cases. This is one of the few valid uses of synthetic data in evaluation. You cannot evaluate a model on cases it will never see, but you also cannot ignore cases it will see just because they are rare. Create realistic examples of the rare cases, validate them with domain experts, and include them in the evaluation set.

## Scoring with Blind Evaluation

Blind evaluation means the person scoring the outputs does not know which model produced which output. This is standard practice in academic research, clinical trials, and sensory testing, but it is rarely done in industry model evaluations. The reason is simple: it requires more work. You have to randomize the outputs, strip the identifiers, track the mapping separately, and re-associate the scores after evaluation. Most teams skip this step because they believe they are objective enough to avoid bias.

They are wrong. Study after study shows that humans are terrible at avoiding bias when they have information that could influence their judgment. If you know that output A came from GPT-5.2, which costs ten times more than the model that produced output B, you will unconsciously expect output A to be better. This expectation will influence how you interpret ambiguous cases, how you weight minor errors, and how you score overall quality. The bias is not malicious. It is not even conscious. It is simply how human cognition works.

A 2024 study of model evaluation practices at a legal technology company found that blinded evaluations produced different results than unblinded evaluations 30 percent of the time. In unblinded evaluations, the more expensive model was rated higher even when objective metrics showed no performance difference. In blinded evaluations, the ratings aligned with the objective metrics. The bias was not small. It was large enough to change the model selection decision in nearly a third of cases.

Blind evaluation is not optional. It is a basic requirement for valid results. If you are not willing to do it, do not run a bake-off. Use published benchmarks or vendor-provided numbers instead. At least those are transparently biased.

## Handling Ties and Near-Ties

What do you do when two models perform identically, or nearly identically, in your bake-off? This happens more often than you might expect. Modern frontier models are highly capable, and the performance differences on many tasks are small. A bake-off might show that Claude Opus 4.5 has 87 percent accuracy, GPT-5.2 has 86 percent accuracy, and Gemini 3 has 87 percent accuracy. The differences are within the margin of error. Statistically, these models are tied.

The first step is to acknowledge the tie. Do not pretend that an 87 versus 86 percent difference is meaningful when your confidence interval is plus or minus three percentage points. The models are tied on accuracy. Now you evaluate the tiebreakers. What is the latency? What is the cost? What is the rate limit? What is the vendor relationship? What is the ease of integration? These are secondary criteria, but they become primary criteria when the models are tied on the primary dimension.

In some cases, the tiebreaker is a subjective dimension that was not part of the initial scoring rubric. You prefer the tone of one model over another. You prefer the format consistency of one model over another. You trust one vendor more than another. These are valid considerations, but they must be made explicitly. Do not retrofit a justification for a decision you already made. Say clearly: the models are tied on accuracy, we chose model A because of tone, and here is why tone matters for this use case.

In other cases, the right answer is to deploy both models and route between them based on context. If the models are truly tied, there is no single best choice. Use model A for cases where latency matters, use model B for cases where cost matters, and use model C for cases where neither matters. This is not indecision. This is recognizing that different tasks have different constraints and no single model optimizes for all of them.

## Multi-Round Bake-Offs

For high-stakes decisions, a single-round bake-off is not sufficient. You need multiple rounds with progressively narrower focus. The first round is a **screening round** with broad criteria and a large set of candidates. You evaluate six or eight models on a moderately sized sample, using coarse-grained criteria. The goal is to eliminate the models that are clearly not competitive. You are not trying to pick the winner. You are trying to eliminate the obvious losers.

The second round is a **refinement round** with task-specific criteria and a smaller set of candidates. You take the top three or four models from the screening round and evaluate them on a larger sample, using fine-grained criteria that match your production success criteria. You measure not just accuracy but tone, format compliance, latency, cost, and edge case behavior. You stratify the sample to ensure rare cases are represented. You use blind evaluation. You calculate statistical significance. This is where you identify the winner.

In some cases, you need a third round focused on a specific dimension of concern. You might run a safety-focused evaluation, a latency-focused evaluation, or a cost-focused evaluation. The goal is to stress-test the finalists on the dimension that matters most for your use case. A customer-facing chatbot needs a safety-focused round. A real-time trading system needs a latency-focused round. A high-volume batch processing system needs a cost-focused round. The third round is not always necessary, but when it is necessary, skipping it is malpractice.

Multi-round bake-offs take longer and cost more, but they reduce the risk of selecting the wrong model. The cost of a bad model selection decision is almost always higher than the cost of a rigorous bake-off. A three-week delay to run a proper evaluation is trivial compared to a three-month production deployment of a model that does not work.

## When to Run a Bake-Off

You should run a bake-off when the decision matters and when you have the resources to run it correctly. A decision matters when the model will be deployed in production, when the task is high-stakes, when the cost or latency constraints are tight, or when the user-facing quality is critical. You have the resources to run it correctly when you can afford the inference cost, the human evaluation time, and the engineering time to set up the comparison.

Specific triggers for running a bake-off include launching a new AI-powered product, responding to a major model release from a provider, responding to significant cost pressure from finance, responding to quality complaints from users, and responding to a change in your task definition or success criteria. In all of these cases, the existing model may no longer be the best choice, and you need to re-evaluate.

You should not run a bake-off for minor version bumps, for provider bug fixes, for cosmetic changes to the API, or when there are no quality complaints and no cost pressure. A bake-off is expensive. Do not run one unless you have reason to believe the result will change your decision. If you are happy with GPT-5.2 and GPT-5.2.1 is released with a bug fix, you do not need a bake-off. You upgrade. If you are happy with Claude Opus 4.5 and Claude Opus 4.6 is released with a claimed 5 percent improvement in reasoning, you might run a bake-off, but only if reasoning is a bottleneck for your task. Otherwise, you are spending money to confirm what you already know.

The frequency of bake-offs varies by organization. A startup with one production model might run a bake-off once every six months. An enterprise with dozens of production models might run a bake-off every month. The right frequency is the frequency that balances the cost of running the evaluation against the cost of deploying a suboptimal model. If models are improving rapidly and your task is high-stakes, you re-evaluate frequently. If models are stable and your task is low-stakes, you re-evaluate rarely.

## Documenting Bake-Off Results

The output of a bake-off is not just a model selection decision. It is also a **record of why you made that decision**, what alternatives you considered, what tradeoffs you accepted, and what assumptions you made. This record is essential for organizational memory. Six months from now, when a new model is released and someone asks why you are using Claude Opus 4.5 instead of GPT-5.2, you need to be able to answer with data, not vague recollection.

The bake-off documentation includes the task definition, the evaluation data sample size and stratification, the scoring rubric, the models evaluated, the results with confidence intervals, the statistical significance tests, and the final decision with justification. It also includes the date of the evaluation, the team members involved, and the version of each model that was tested. Model versions matter. Claude Opus 4.5 in March 2026 may perform differently than Claude Opus 4.5 in January 2026 if the provider has updated the weights.

The documentation should be stored in a location that is accessible to future team members, searchable, and version-controlled. A wiki page, a shared document, or a structured database all work. The format matters less than the accessibility. If the documentation is in someone's email inbox or a local file, it does not exist. It needs to be in a shared system.

Some organizations create a **model selection log** that tracks all bake-offs, all model changes, and all production performance over time. This log becomes a historical record of what worked, what did not, and why. Over time, patterns emerge. You learn which providers are reliable for which tasks, which evaluation methods are predictive of production performance, and which success criteria actually correlate with user satisfaction. This is how you build institutional knowledge. Without documentation, every bake-off is a fresh start.

## Common Bake-Off Mistakes

The most common mistake, already discussed, is using too few examples. The second most common mistake is **biased evaluation**, either from unblinded scoring or from evaluators who have a preference for a particular provider. The third most common mistake is **ignoring latency and cost dimensions**. A model with 95 percent accuracy that costs ten dollars per call is not the right choice for a high-volume application. A model with 90 percent accuracy and 200 millisecond latency is not the right choice for a real-time application. Accuracy is necessary but not sufficient.

The fourth common mistake is **testing only the happy path**. The evaluation data includes only well-formed inputs, typical cases, and straightforward scenarios. The models are not tested on ambiguous inputs, contradictory instructions, very long inputs, empty inputs, or adversarial inputs. Production includes all of these cases. If you do not test them in the bake-off, you will discover the model's behavior in production, which is the worst possible time to discover it.

The fifth common mistake is **treating the bake-off as a one-time event**. You run the bake-off, select the model, deploy it, and never re-evaluate. Models change. Tasks change. User expectations change. A model that was the best choice in January may not be the best choice in June. You need to re-evaluate periodically, either on a fixed schedule or in response to trigger events like major model releases or production performance degradation.

The sixth common mistake is **over-fitting to the evaluation set**. You tune your prompt, your temperature, and your max tokens to maximize performance on the evaluation data, and then you deploy that configuration to production. The evaluation set is a sample. It is not the universe. A configuration that is optimized for the sample may not be optimized for the full distribution. Avoid prompt tuning during the bake-off. Use a standard prompt template, run the bake-off, select the model, and then tune the prompt for production after you have made the model choice.

## The Cost of Bake-Offs and How to Budget

Running a rigorous bake-off is expensive. If you evaluate five models on 500 examples, you are making 2,500 inference calls. If each call costs five cents, that is 125 dollars in inference costs. If you need two human evaluators to score each output and each evaluator takes 30 seconds per output, that is 1,250 minutes of human time, or about 21 hours. If the evaluators are paid 50 dollars per hour, that is over 1,000 dollars in labor. Add engineering time to set up the evaluation, analyze the results, and document the findings, and the total cost is easily 5,000 to 10,000 dollars for a single bake-off.

This is not a reason to skip the bake-off. This is a reason to budget for it. If you are deploying a model to production that will handle 100,000 requests per month, the cost of selecting the wrong model is far higher than 10,000 dollars. If the wrong model has 5 percent lower accuracy and that translates to 5,000 additional support tickets per month, each requiring 10 minutes of human intervention, you are paying for 833 hours of support time, or about 40,000 dollars per month. The bake-off pays for itself in the first week.

You budget for bake-offs the same way you budget for any other engineering activity. Estimate the number of models you will evaluate, the number of examples you need, the cost per inference call, the time per evaluation, and the fully-loaded cost of the evaluators. Add 20 percent for overhead. Include the cost in your project plan. If the budget is approved, run the bake-off. If the budget is not approved, either reduce the scope or accept that you are making a less informed decision.

Some organizations create a **model evaluation budget** that is separate from the product development budget. This budget funds all model selection activities, including bake-offs, benchmark evaluations, and ongoing monitoring. The advantage of a separate budget is that it makes the cost visible and allows you to track spending over time. The disadvantage is that it creates friction when teams need to evaluate a new model. The right approach depends on your organization's size and structure.

Running a bake-off with statistical rigor is the most reliable way to select a model for production, but it is not the only consideration. Beyond accuracy, you must evaluate tone, format compliance, safety, and edge case behavior, dimensions that often matter more than raw correctness for user-facing systems.
