# 2.9 â€” Tool-Calling Fidelity: Evaluating Multi-Step Tool Use, Parallel Calls, and Error Recovery by Model

In September 2025, a customer support automation platform serving a major telecommunications provider experienced what their VP of Engineering later called "the tool-calling cascade failure." Their agentic system handled account modifications, billing adjustments, and service provisioning through a suite of fourteen internal APIs exposed as tool calls to their language model. The system had worked flawlessly during three months of testing with GPT-4.5, but when they switched to a promising new open-weight model to reduce costs by sixty-seven percent, the failure rate on multi-step customer requests jumped from one point two percent to twenty-three percent. A customer request like "upgrade my plan to Premium and apply the loyalty discount" would trigger the plan upgrade successfully but then call the wrong discount API with malformed parameters. The model could execute single tool calls accurately ninety-four percent of the time, but chained sequences fell apart. After two weeks of customer complaints and three engineering attempts to fix it through prompt engineering, they reverted to GPT-4.5 and absorbed the higher costs. The diagnosis was clear: not all models handle tool calling with equal fidelity, and the gap becomes catastrophic in production agent systems where tool sequences must execute correctly every time.

Tool calling is the fundamental capability that separates chatbots from agents. A chatbot generates text. An agent takes action in the world by invoking functions, APIs, database operations, and external services. The model receives a set of tool schemas defining available functions, their parameters, parameter types, and expected behaviors. When the user makes a request, the model decides which tools to call, in what order, with what arguments, and how to handle the results. This is conceptually simple but operationally complex because the model must perform multiple tasks correctly at once. It must select the right function from potentially dozens of options. It must construct arguments that match the schema precisely, respecting types, formats, required fields, and validation rules. It must sequence calls appropriately when a task requires multiple steps. It must decide when parallel execution is possible versus when sequential dependency exists. It must handle errors gracefully when a tool fails, retrying with corrections or choosing alternative approaches. Each of these capabilities varies dramatically across models, and the variance determines whether your agent system works reliably or fails unpredictably in production.

The first dimension of tool-calling fidelity is **single-tool accuracy**: the model's ability to execute one function call correctly when given a straightforward request. This has three components. Function selection accuracy measures whether the model picks the right tool from the available set. If you provide ten tools and the user asks to "send an email to the marketing team about the Q1 results," does the model call send_email or does it confuse it with send_slack_message or create_calendar_event? Argument type accuracy measures whether the model provides parameters in the correct format. If send_email expects a recipient field as a list of strings and a subject field as a string, does the model provide them correctly or does it send a single string for recipients or an object for subject? Argument value accuracy measures whether the parameters contain the right information extracted from the user request. Does the model correctly identify "the marketing team" should map to the marketing group email address, and does it extract "Q1 results" as the subject?

Single-tool accuracy seems like table stakes but it varies more than teams expect. GPT-4.5, GPT-5, and GPT-5.2 achieve single-tool accuracy above ninety-seven percent on well-designed schemas with clear descriptions. Claude Opus 4.5 performs similarly, occasionally differing in edge cases around optional parameters. Gemini 3 Pro reaches ninety-five percent but uses a slightly different schema format that requires adaptation. Open-weight models show dramatic variance. Llama 4 405B achieves ninety-one to ninety-four percent depending on schema complexity. DeepSeek V3.2 reaches eighty-eight to ninety-two percent. Smaller models like Llama 4 70B drop to eighty-three to eighty-seven percent. Models below 30B parameters often fall below eighty percent and become unreliable for production use. These percentages matter enormously because they compound across multi-step sequences, a problem we will examine shortly.

The second dimension is **multi-tool sequencing**: the model's ability to execute a series of function calls in the correct order when a task requires multiple steps. Consider a customer request to "book a conference room for the leadership meeting next Tuesday at 2pm and invite the executive team." This requires at least three steps performed sequentially. First, check availability of conference rooms for the specified time. Second, book the room once availability is confirmed. Third, send calendar invites to the executive team with the room details. The model must recognize that these steps have dependencies. You cannot book a room before checking availability. You cannot send invites with room details before the room is booked. The model must execute the sequence in order, pass results from earlier steps to later steps, and handle conditional logic if a step fails.

Multi-tool sequencing is where most models begin to diverge significantly. GPT-5 and GPT-5.2 handle sequential dependencies well through their native chain-of-thought reasoning, often making the sequencing explicit in their internal reasoning before executing calls. They correctly identify that step two depends on step one and delay execution until results return. Claude Opus 4.5 handles sequencing effectively but sometimes needs explicit prompting to wait for results before proceeding, particularly in complex scenarios with five or more steps. Gemini 3 performs adequately on three-step sequences but begins showing ordering errors on longer chains. Open-weight models struggle considerably with multi-step sequencing. Llama 4 405B can handle three-step sequences reasonably well if the dependencies are obvious from the tool descriptions, but subtle dependencies cause failures. Smaller models often attempt to execute all steps simultaneously or in the wrong order, leading to cascading failures.

The compounding error problem becomes brutal in multi-step sequences. If each tool call has ninety-five percent accuracy, a single call succeeds ninety-five percent of the time. Two sequential calls succeed ninety point two five percent of the time, because both must succeed. Three calls drop to eighty-five point seven percent. Five calls drop to seventy-seven point four percent. Ten calls drop to fifty-nine point nine percent. This means that with a model achieving ninety-five percent single-tool accuracy, a ten-step sequence fails four out of ten times. Production agent systems routinely execute sequences of five to fifteen tool calls to complete complex user requests. If your model has ninety percent single-tool accuracy instead of ninety-five percent, a five-step sequence succeeds only fifty-nine percent of the time. This is why even small differences in single-tool accuracy create massive differences in end-to-end task success rates for agent systems.

The third dimension is **parallel tool calling**: the model's ability to invoke multiple tools simultaneously when they have no dependencies and can execute concurrently. Consider a user request to "get me the latest sales numbers, the engineering headcount, and the customer satisfaction scores for Q4." These three pieces of information come from three different APIs with no dependencies between them. An efficient agent should call all three tools in parallel, wait for all results to return, then synthesize the response. Executing them sequentially wastes time and increases latency. The model must recognize that no dependencies exist, construct three valid tool calls simultaneously, and handle the results when they return in potentially different orders.

GPT-5.2 introduced improved parallel calling capabilities explicitly designed for agent use cases. It can issue up to eight tool calls simultaneously and handle their results correctly even when they return out of order. GPT-5 supports parallel calling but with a lower practical limit of four to five calls before reliability degrades. Claude Opus 4.5 supports parallel calling but takes a more conservative approach, often defaulting to sequential execution unless the independence is extremely obvious. You can prompt it to prefer parallel execution but this requires careful system prompt design. Gemini 3 supports parallel calling in principle but the implementation sometimes loses track of which results correspond to which calls when more than three tools execute simultaneously. Open-weight models vary enormously. Llama 4 405B can handle two to three parallel calls with reasonable reliability. Smaller models either do not support parallel calling at all or implement it so unreliably that you must disable it and force sequential execution.

The fourth dimension is **error recovery**: the model's ability to handle tool failures gracefully and adapt its approach. Tools fail in production. APIs return errors. Rate limits get hit. Permissions get denied. Network requests timeout. A robust agent must detect these failures, understand their implications, and respond appropriately. Sometimes the right response is retrying with the same parameters. Sometimes it is retrying with corrected parameters. Sometimes it is choosing an alternative tool that accomplishes the same goal through a different path. Sometimes it is informing the user that the requested action cannot be completed and explaining why. The model must distinguish between these scenarios and select the appropriate recovery strategy.

Error recovery varies wildly across models and represents one of the least mature aspects of tool calling. GPT-5.2 demonstrates the most sophisticated error recovery, often parsing error messages returned by tools, identifying the specific problem, and attempting corrections. If a tool returns an error indicating a required field is missing, GPT-5.2 frequently retries with the missing field populated. If a tool indicates insufficient permissions, it explains the limitation to the user rather than retrying futilely. Claude Opus 4.5 performs reasonably well on simple error recovery but tends toward conservative behavior, often reporting failures to the user rather than attempting automatic retries. Gemini 3 handles errors inconsistently, sometimes recovering well and sometimes getting stuck in retry loops. Open-weight models generally perform poorly on error recovery. Most either ignore errors and proceed as if the tool succeeded, or halt entirely and report a generic failure. Neither behavior is acceptable in production agent systems where graceful degradation is essential.

## Building a Tool-Calling Evaluation Suite

Evaluating tool-calling fidelity requires a systematic approach with defined schemas, expected behaviors, and measurement at each capability level. You cannot rely on vibes or manual testing because the failure modes are too subtle and the success rates too high to catch problems without statistical rigor. A proper tool-calling eval suite has five components: tool schema definitions, test scenarios covering all capability dimensions, expected call sequences or outcomes, automated execution and comparison, and detailed failure analysis.

Tool schema definitions are your ground truth. Define a realistic set of tools that mirror the complexity and variety in your actual production system. If your production system exposes twelve tools, create an eval set with ten to fifteen tools covering similar patterns. Include tools with varying numbers of parameters, a mix of required and optional fields, different parameter types including strings, integers, booleans, lists, and nested objects, and realistic descriptions that provide enough context for the model to choose appropriately. Do not create toy schemas with single-parameter functions. The model must face the same complexity it will encounter in production. If your real tools have six to ten parameters each, your eval tools should match that complexity.

Test scenarios must cover all four capability dimensions systematically. For single-tool accuracy, create twenty to thirty scenarios where the correct response is calling exactly one tool with specific arguments. Include edge cases like optional parameters, default values, and ambiguous requests that could map to multiple tools but have one clearly correct choice. For multi-tool sequencing, create fifteen to twenty scenarios requiring two to six sequential steps with clear dependencies. Include cases where later steps require information from earlier step results. For parallel tool calling, create ten to fifteen scenarios where multiple tools should execute simultaneously because no dependencies exist. For error recovery, create ten to fifteen scenarios where the initial tool call will fail with specific error types, and measure whether the model adapts appropriately.

Expected call sequences define correct behavior for each scenario. For single-tool scenarios, specify the exact function name and the exact argument values you expect. For multi-step scenarios, specify the sequence of function calls, the dependencies between them, and the information flow from earlier to later steps. For parallel scenarios, specify which tools should execute simultaneously, acknowledging that the order in which they are issued may vary. For error scenarios, specify the initial call that will fail, the error message that will be returned, and the acceptable recovery behaviors. An acceptable recovery might be retrying with corrected arguments, or choosing an alternative tool, or informing the user of the limitation. Document all acceptable paths so your evaluation can recognize correct adaptations.

Automated execution runs each scenario through the model and compares actual behavior to expected behavior. This requires implementing your tool schemas as actual executable functions or mocks that return predefined results and errors according to your test design. Feed each scenario to the model as a user request. Capture every tool call the model issues, including function name, arguments, and timing. Capture every result or error returned. Capture the model's final response to the user. Compare the actual tool-calling sequence to the expected sequence. Measure success rates across all scenarios and broken down by capability dimension. Track not just binary success or failure but also partial credit. A scenario might call the right function with ninety percent correct arguments. That is better than calling the wrong function entirely.

Detailed failure analysis categorizes every failure to identify patterns. When a model fails a scenario, classify the failure type. Did it select the wrong function? Did it select the right function but provide wrong argument types? Did it provide right types but wrong values? Did it fail to sequence calls correctly? Did it execute sequentially when parallel was appropriate? Did it attempt parallel execution but lose track of results? Did it ignore an error? Did it retry inappropriately? Did it give up too early? These patterns reveal where each model struggles and inform your decision about whether the model is viable for your use case. A model that consistently fails on parallel execution might still be usable if your tasks rarely require parallel calls. A model that consistently fails on error recovery is unusable for production agent systems regardless of its other strengths.

## Model Performance Benchmarks on Tool-Calling Fidelity

Real-world tool-calling performance as of January 2026 shows clear tiers. GPT-5.2 leads across all dimensions with single-tool accuracy at ninety-eight point four percent, five-step sequence success at ninety-one point six percent, parallel calling success on four simultaneous tools at ninety-four point two percent, and meaningful error recovery on sixty-three percent of failure scenarios. GPT-5 achieves single-tool accuracy at ninety-seven point one percent, five-step sequences at eighty-six point three percent, parallel calling on three simultaneous tools at ninety-one point eight percent, and error recovery on forty-eight percent of failures. These two models represent the current standard for production agent systems where reliability matters more than cost.

Claude Opus 4.5 performs nearly as well with single-tool accuracy at ninety-seven point three percent and five-step sequences at eighty-five point nine percent, but parallel calling is less reliable at eighty-three point six percent on three simultaneous tools, and error recovery sits at forty-one percent. The model is extremely capable but requires more conservative architectures that favor sequential execution and include explicit error-handling logic in your orchestration layer rather than relying on the model to recover automatically. This is a viable production choice if you design around the limitations.

Gemini 3 Pro reaches single-tool accuracy at ninety-five point seven percent and five-step sequences at seventy-eight point one percent. Parallel calling on three tools succeeds eighty-one point four percent of the time. Error recovery is at thirty-six percent. This makes Gemini 3 Pro viable for moderately complex agent tasks but risky for mission-critical workflows where failures have significant cost. Teams using Gemini 3 Pro typically implement more sophisticated orchestration layers that handle sequencing and error recovery in code rather than delegating those responsibilities to the model.

Open-weight models show the widest variance. Llama 4 405B achieves single-tool accuracy at ninety-two point eight percent, five-step sequences at sixty-nine point four percent, parallel calling on two tools at seventy-six point two percent, and error recovery at twenty-three percent. This is borderline viable for internal tools where occasional failures are tolerable and cost savings justify the trade-off. Llama 4 70B drops to eighty-six point three percent single-tool accuracy and fifty-one point seven percent five-step sequence success, making it too unreliable for production agent use. DeepSeek V3.2 sits between these with single-tool accuracy at eighty-nine point one percent and five-step sequences at sixty-two point eight percent, usable for some low-stakes automation tasks but not for customer-facing systems.

The practical implication is that tool-calling fidelity creates a hard floor on which models you can use for agent architectures. If your product requires reliable multi-step task execution, you need GPT-5, GPT-5.2, or Claude Opus 4.5. If your product can tolerate fifteen to twenty-five percent task failure rates and implement robust retry and fallback mechanisms, Gemini 3 Pro or Llama 4 405B become options. Anything below that tier is not production-ready for agent use cases. This constraint eliminates the majority of available models and forces you into the high-cost tier unless you fundamentally redesign your product to avoid tool calling altogether.

## Why Tool-Calling Fidelity Is the Gating Factor for Agent Architectures

Agent systems live or die on tool-calling reliability because failures are compounding and often invisible until production. Unlike text generation where a mediocre response is still readable and potentially useful, a failed tool call either does nothing or does the wrong thing. A customer request that should modify their account settings but instead queries their billing history is worse than no action at all because it wastes time and erodes trust. The user asked for an action, the system confirmed it would act, but nothing happened or the wrong thing happened. This is a product failure, not a minor quality degradation.

Compounding errors make agent reliability exponentially worse than text generation reliability. If your text generation model produces acceptable responses ninety percent of the time, ninety percent of users get value. If your agent model has ninety percent per-tool accuracy and the average task requires five tool calls, only fifty-nine percent of users get value. The other forty-one percent experience failures, many of which they will not immediately detect. A scheduling agent that books a room but fails to send invites leaves the user believing the task completed when it partially failed. They discover the problem only when attendees do not show up to the meeting. This delayed failure detection destroys user confidence faster than immediate visible errors.

Tool-calling errors are also harder to debug than text quality issues because they involve complex interactions between the model, your orchestration layer, and external systems. When a text response is wrong, you can read it and understand the problem. When a tool call fails, you must examine logs to determine which call failed, why it failed, whether the model received the error, how it responded, and whether the failure propagated correctly through your system. Many teams discover their tool-calling reliability problems only after launch when customer support tickets reveal patterns of incomplete task execution. By then you have a live system with angry users and a model choice that cannot be easily reversed.

The reliability requirement for agent systems is also categorically higher than for chatbots. Users tolerate imperfect responses from chatbots because they understand the model is generating text based on probabilities. They read critically and verify information. But when users interact with an agent, they expect it to perform actions correctly. If you ask a chatbot "what is the weather today" and it gives slightly wrong information, you check another source. If you ask an agent "set the thermostat to seventy-two degrees" and it sets it to sixty-two degrees, your house is cold and you blame the product. Agent actions have real-world consequences that text responses do not, which means the bar for acceptable reliability is ninety-five percent or higher, not the eighty to eighty-five percent that might be tolerable for conversational chatbots.

Tool-calling fidelity also determines your operational costs in ways that are not immediately obvious. Low tool-calling reliability forces you to implement extensive retry logic, fallback mechanisms, confirmation steps, and human-in-the-loop checks. These add latency, complexity, and often human labor costs that eliminate the automation value you were trying to achieve. If every agent action requires human confirmation before execution, you have built an expensive suggestion engine, not an agent. If you implement automatic retries on every tool call to compensate for low reliability, you increase API costs and latency. If you add fallback chains where the agent tries alternative tools when the first fails, you increase complexity and create new failure modes. All of this is overhead created by insufficient tool-calling fidelity, and it often costs more than just using a more expensive model with higher fidelity from the start.

The model market is aware of this dynamic, which is why tool-calling performance has become a major competitive dimension. GPT-5.2 was explicitly marketed on improved tool-calling and agent capabilities. Claude Opus 4.5 emphasizes its reliability on complex multi-step tasks. Gemini 3 Pro highlights function calling as a core feature. Open-weight model developers now publish tool-calling benchmarks alongside traditional language benchmarks because they recognize that agent use cases drive commercial adoption. This focus will continue intensifying because tool calling is the capability that monetizes language models beyond chatbot use cases. Whoever builds the most reliable tool-calling model captures the agent market, which is larger and higher-value than the chatbot market.

Your evaluation of tool-calling fidelity must therefore be the first and most important filter in model selection for any agent or automation product. Before you evaluate response quality, latency, cost, or any other dimension, you must establish which models can reliably execute the tool sequences your product requires. Build your tool-calling eval suite early, run it on every model you are considering, and eliminate models that do not meet your minimum reliability threshold. Only then should you compare the remaining candidates on other dimensions. This ordering saves enormous time because there is no point optimizing prompts or evaluating response quality on a model that cannot reliably call your tools. Tool-calling fidelity is the gate, and most models do not pass through it.

Once you have confirmed that a model meets your tool-calling reliability requirements, the next critical evaluation dimension is whether it follows complex instructions reliably, because production systems require models to obey detailed behavioral constraints defined in system prompts, and instruction-following fidelity varies as dramatically across models as tool-calling does.
