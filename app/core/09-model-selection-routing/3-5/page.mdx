# 3.5 — Classifier-Based Routers: Training a Lightweight Model to Pick the Right Model

In mid-2025, a legal research platform serving 12,000 attorneys was burning through $140,000 per month in model costs. Every query—whether a simple statute lookup or a complex multi-jurisdiction analysis—hit their flagship model, Claude Opus 4.5, because the team had no principled way to decide when a cheaper model would suffice. The engineering lead proposed a rule-based router with about twenty hand-coded conditions: if the query contained certain legal terms, route to Opus; if it was under fifty words, route to Sonnet; if it mentioned code or regulations by number, route to the specialized legal fine-tune. The rules launched in August 2025 and immediately started misrouting. Simple questions about well-known statutes went to the expensive model because they happened to contain trigger words. Complex cross-reference questions went to the cheap model because they were phrased concisely. By October, the team had added forty more rules to patch the edge cases, and the router config file had become a 600-line unmaintainable mess. Cost dropped by eighteen percent, but quality complaints increased by thirty-two percent because the router was wrong in unpredictable ways. The root cause was not insufficient rules—it was the wrong routing paradigm. Rule-based systems force you to articulate every decision boundary explicitly, and query complexity does not map cleanly to features you can write rules about. What they needed was a classifier-based router: a lightweight machine learning model trained to predict, given a query, which backend model would produce the best response.

A classifier-based router treats model selection as a supervised learning problem. You collect a dataset of queries, each labeled with the model that produced the best output for that query. You train a small, fast classifier to predict the correct model given only the query text and metadata. At inference time, the classifier runs in tens of milliseconds, inspects the incoming query, and returns a model name. The backend system then dispatches the query to that model. The classifier learns patterns that are difficult or impossible to express as rules: syntactic complexity, semantic ambiguity, domain jargon density, implicit user intent signals. It generalizes across unseen queries better than rules do, and it adapts automatically when you retrain it on new data. Classifier-based routing is the natural evolution from rule-based routing for teams whose query distributions are too complex to codify but stable enough to learn.

## Collecting Training Data: Scoring Multiple Models per Query

The classifier needs labeled examples: queries paired with ground-truth model assignments. The ground truth is defined by quality, not by prior behavior. You cannot simply label each query with the model you currently route it to, because your current routing might be wrong. Instead, you run each query through multiple candidate models, evaluate the outputs, and label the query with the model that produced the best response. This is expensive upfront—if you are choosing between four models, you spend four times the inference cost during data collection—but you only need to do this for a sample of queries, not for every query forever.

Start by selecting a representative sample from your production traffic. If you handle 500,000 queries per month, sample 5,000 to 10,000 queries that span your query distribution: different task types, different user tiers, different input lengths, different domains. For each sampled query, invoke all candidate models in parallel and collect their outputs. Now you need to score each output to determine which model won. The scoring method depends on your task. For structured tasks with verifiable answers—code generation, data extraction, classification—you can use automated eval metrics like exact match, unit test pass rate, or schema compliance. For open-ended tasks—creative writing, explanations, summarization—you need human raters or an LLM-as-judge setup. The legal research platform used a hybrid: automated checks for citation accuracy and format compliance, plus a panel of three paralegals who rated response helpfulness on a five-point scale. Each query was labeled with the model that had the highest combined score. If two models tied, the cheaper model won the label.

You accumulate this labeled dataset over weeks or months, covering seasonal patterns and product changes. The legal platform collected data in three-week sprints: week one, sample 2,000 queries; week two, run evals and label; week three, retrain the classifier and deploy. After four sprints, they had 8,000 labeled examples and a classifier that routed correctly eighty-seven percent of the time, saving sixty-two percent of model cost compared to routing everything to Opus while maintaining quality scores within two percent of the all-Opus baseline.

## Classifier Architecture: From Logistic Regression to Fine-Tuned Small LLMs

The classifier itself can range from a simple logistic regression model to a fine-tuned small language model. The right choice depends on your data size, latency budget, and accuracy needs. At the lightweight end, logistic regression over TF-IDF features or sentence embeddings is fast, interpretable, and effective when decision boundaries are relatively linear. You featurize each query into a fixed-size vector—maybe a 384-dimensional sentence embedding from a model like all-MiniLM-L6-v2—and train a multiclass logistic regression classifier with one class per candidate model. Inference takes under ten milliseconds on CPU. This works well when your routing decision correlates strongly with surface features: query length, presence of specific keywords, embedding similarity to known query clusters.

At the midweight tier, small neural networks—two or three dense layers with a few hundred units each—can learn nonlinear decision boundaries. You feed in the same sentence embeddings, add dropout for regularization, and train with cross-entropy loss. Inference still runs in ten to twenty milliseconds. A small feedforward network gave the legal platform a three-point accuracy boost over logistic regression, enough to justify the slightly longer training time.

At the heavyweight end, you can fine-tune a small language model—GPT-5-mini, Llama 4-8B, or a distilled variant—on the routing task. You format each training example as a prompt: "Route this query to the best model: [query text]. Models: Opus, Sonnet, Haiku. Best model:" and fine-tune the small LLM to generate the correct model name. Fine-tuning a small LLM captures deeper semantic features and context, and it can generalize better to out-of-distribution queries, but it adds latency—typically thirty to fifty milliseconds—and requires more labeled data to avoid overfitting. A legal tech competitor with 50,000 labeled examples fine-tuned Llama 4-8B and achieved ninety-three percent routing accuracy, but the added latency was unacceptable for their real-time use case, so they fell back to the feedforward network.

You can even use a prompted LLM as the classifier without fine-tuning: send each query to a cheap, fast model with a prompt like "Classify this query as simple, moderate, or complex. Simple queries go to Haiku, moderate to Sonnet, complex to Opus." This is the easiest to implement but the least accurate and adds twenty to forty milliseconds per query. It works as a cold-start solution before you have enough labeled data to train a real classifier.

## Feature Engineering: What the Classifier Sees

The classifier makes its decision based on features you extract from the query. The simplest feature set is a sentence embedding: run the query through a small embedding model and use the resulting vector as input. This captures semantic meaning and is often sufficient. But you can improve accuracy by adding hand-crafted features that highlight dimensions relevant to routing. Query length in tokens or characters: longer queries may need models with better context handling. Presence of domain-specific keywords: legal terms, medical jargon, code syntax, brand names. Syntactic complexity: average sentence length, dependency tree depth, readability scores. Ambiguity signals: presence of pronouns without clear antecedents, vague terms like "this" or "that," questions with multiple possible interpretations. User metadata: user tier, subscription level, past query history, device type. Time features: time of day, day of week, whether the query is part of a session or standalone.

The legal platform extracted twelve features: a 384-dimensional embedding, query length in tokens, count of legal citation patterns, count of Latin legal terms, count of jurisdiction names, readability score, user subscription tier encoded as an integer, and a binary feature for whether the query was a follow-up in a conversation. These features fed into a two-layer feedforward network. The feature engineering took two weeks of experimentation—early versions included features that did not improve accuracy and only added noise—but the final feature set was stable and required no maintenance.

Feature engineering is less important if you fine-tune a small LLM, because the LLM learns its own feature representations from the raw text. But even then, appending user metadata or task type labels to the prompt can help. The key principle is that the classifier must see the same features at training time and inference time, so any feature that depends on query execution results—like the actual latency or the token count of the response—cannot be used, because those are not available before routing.

## Latency Cost: The Classifier Adds Time but Saves Money

The classifier is a pre-processing step that runs before you invoke the backend model. It adds latency. A logistic regression classifier adds five to ten milliseconds. A small neural network adds ten to twenty milliseconds. A fine-tuned small LLM adds thirty to fifty milliseconds. This latency is front-loaded and unavoidable—you cannot parallelize it with the main model call because the routing decision determines which model to call. For latency-sensitive applications, this is a real cost. The legal platform measured p95 latency before and after deploying the classifier. Before: 1,200 milliseconds. After: 1,215 milliseconds. The fifteen-millisecond increase was imperceptible to users and was dwarfed by the variance in backend model latency. The team accepted the tradeoff because the classifier saved $88,000 per month in model costs and reduced median response time by routing simple queries to faster models.

If the latency cost is unacceptable, you can deploy the classifier asynchronously for non-interactive queries—batch processing, background jobs, email responses—and fall back to a simple rule or a default model for interactive queries. Or you can run the classifier in parallel with a default model call, and if the classifier returns before the default model finishes, cancel the default call and reissue to the classifier's chosen model. This is complex and rarely worth it, but it is possible.

## Training and Updating the Classifier: Adapting to Changing Query Distributions

Your query distribution changes over time. Users discover new features, your product expands into new domains, model capabilities shift, pricing changes. A classifier trained on three months of data in early 2025 will degrade in accuracy by late 2025 if the query mix has shifted. You must retrain periodically. The legal platform retrained monthly, using a rolling window of the most recent 10,000 labeled queries. Each retraining cycle took four hours: two hours to label new queries using the human-in-the-loop eval pipeline, one hour to retrain the model, one hour to validate on a holdout set and deploy. They tracked classifier accuracy in production by sampling a small percentage of routed queries and re-evaluating them with the full multi-model comparison. If accuracy dropped below eighty-five percent, they triggered an emergency retraining.

Retraining cadence depends on how fast your distribution drifts. If you launch a new feature every quarter, retrain quarterly. If your query patterns are seasonal—tax software, retail during holidays—retrain before each season. If you are in a stable domain with slow change, retrain every six months. The cost of retraining is the cost of running multi-model evals on a sample of queries plus the engineering time to manage the retraining pipeline. Automate this: build a retraining pipeline that samples queries, runs evals, retrains the model, validates, and deploys with minimal human intervention.

## The Cold-Start Problem: You Need Data to Train the Router, but You Need a Router to Generate Data Efficiently

Here is the chicken-and-egg problem: the classifier needs labeled data, which you generate by running multiple models and comparing outputs, but running multiple models on every query is expensive, which is why you wanted a router in the first place. If you are starting from scratch with no labeled data, you cannot train the classifier yet. You have three bootstrap strategies.

First, run multi-model evals on a small sample—say, 1,000 queries—by hand, accepting the upfront cost. Use those 1,000 examples to train an initial low-accuracy classifier. Deploy that classifier to production, knowing it will make mistakes. Monitor its decisions, and when you detect low-confidence predictions or user complaints, route those queries through multi-model eval to get ground-truth labels. Use those labels to retrain. This incremental approach builds your dataset over weeks while limiting cost.

Second, start with a rule-based router that you know is imperfect. Route all queries through the rule-based system, but randomly sample a small percentage—say, five percent—and run multi-model eval on those sampled queries. Collect labeled data in the background while the rule-based router handles production traffic. After a few weeks, you have enough data to train a classifier. Deploy the classifier, compare its decisions to the rule-based router's decisions, and continue sampling to expand your dataset.

Third, use an LLM-as-classifier with prompting as a zero-shot router. Prompt a cheap model to classify query complexity and route accordingly. This is inaccurate but better than random. Simultaneously, run multi-model evals on a sample to collect real labels. After a few weeks, replace the prompted classifier with a trained classifier. The legal platform used the second strategy: they deployed a twenty-rule heuristic router, sampled ten percent of queries for multi-model eval, collected 6,000 labels over six weeks, trained a classifier, and cut over to the classifier-based router in week seven.

## Measuring Classifier Accuracy and Its Downstream Impact

Classifier accuracy is not the same as routing quality. A classifier can be eighty-five percent accurate on a held-out test set but still deliver high routing quality if it gets the high-stakes queries right and only errs on queries where model differences are small. Conversely, a ninety percent accurate classifier that systematically routes critical queries to weak models will deliver poor quality. You must measure two things: classifier accuracy as a traditional ML metric, and downstream impact on cost and quality.

Classifier accuracy is precision, recall, and F1 per model class, measured on a held-out test set. The legal platform's classifier had eighty-seven percent overall accuracy, with precision ranging from eighty-two percent for the mid-tier model to ninety-one percent for the top-tier model. This asymmetry was intentional: they tuned the classifier to be conservative, preferring false positives for the expensive model over false negatives. Better to over-route to Opus and waste money than under-route and anger users.

Downstream impact is measured by running an A/B test: route half your traffic through the classifier and half through a baseline strategy—rule-based router, or route everything to the default model—and compare cost and quality. The legal platform's A/B test ran for two weeks with twenty percent of traffic in each arm. Classifier arm: sixty-two percent cost reduction, quality scores 1.8 percent lower than all-Opus baseline. Rule-based arm: eighteen percent cost reduction, quality scores 4.2 percent lower. The classifier won decisively, and the team rolled it out to all traffic.

You also measure classifier confidence calibration. A well-calibrated classifier outputs probabilities that reflect true likelihood: if it predicts Opus with ninety percent confidence, Opus should be the correct choice ninety percent of the time. Calibration matters because you can use confidence scores to implement fallback logic: if the classifier's top prediction has confidence below seventy percent, route to the safe default model instead. The legal platform added this guard rail and caught another 400 queries per month that would have been misrouted.

## When Classifier-Based Routing Outperforms Rules and When It Does Not

Classifier-based routing excels when query complexity is high-dimensional and patterns are learnable but not easily codifiable. It works well when you have enough labeled data—at least a few thousand examples—and when your query distribution is relatively stable, so the classifier's learned patterns remain valid. It works well when the cost of running multi-model evals on a sample is justified by the long-term savings from accurate routing. It works well when your team has ML expertise to build, train, and maintain the classifier.

Classifier-based routing struggles when your query distribution shifts too fast for retraining to keep up. If your product changes every week, the classifier is always out of date. It struggles when you have very few labeled examples—under 500—because the classifier overfits and does not generalize. It struggles when decision boundaries are simple and a few rules suffice, because the added complexity and latency of the classifier provide no benefit. It struggles when your team lacks ML infrastructure: if training and deploying a model is a multi-day engineering project, the operational overhead outweighs the gains.

The legal platform had a stable query distribution, 8,000 labeled examples, and an ML-fluent team. Classifier-based routing was the right choice. A competitor in the same space had a rapidly evolving product with new query types every sprint and only 300 labeled examples. They tried a classifier, saw poor generalization, and fell back to a hybrid of rules for known patterns and a default model for everything else. Know your data, know your product velocity, and choose the routing strategy that matches.

## Hybrid Architectures: Rules for Known Patterns, Classifier for Ambiguity

Many teams run a hybrid: a rule-based pre-filter handles clear-cut cases, and a classifier handles ambiguous queries. If the query matches a high-confidence rule—contains a specific keyword, user is in a premium tier, query length is below ten tokens—route immediately without invoking the classifier. If the query does not match any rule, or matches a rule with low confidence, invoke the classifier. This hybrid reduces classifier invocations, saving latency and cost, while preserving rule-based determinism for easy cases. The legal platform added this optimization in month four: twenty-three percent of queries matched high-confidence rules and bypassed the classifier, cutting classifier invocations and saving twelve milliseconds of latency on those queries.

The hybrid architecture also simplifies cold-start: you deploy rules first, add classifier later. You start with ten simple rules, deploy to production, collect labeled data in the background, train a classifier on the ambiguous queries that the rules do not handle well, and deploy the classifier to catch the edge cases. This incremental path is lower risk than replacing rules wholesale with a classifier.

Classifier-based routing is the practical middle ground between rule-based simplicity and end-to-end learned routing, which we will explore next: embedding-based routing, where queries and models live in a shared latent space and routing is a nearest-neighbor search.
