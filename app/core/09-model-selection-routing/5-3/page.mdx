# 5.3 â€” Model Warm-Up and Cold Start: Serverless vs Dedicated Inference

In June 2025, a customer support platform serving a mid-sized SaaS company deployed an AI assistant to handle ticket triage. The team chose a serverless endpoint from a major cloud provider because the pricing model was attractive: pay only for requests actually processed, no idle infrastructure costs. The system worked perfectly in load testing. When they launched to production with real support agents, the first ticket of each agent's shift took 18 seconds to get a response. The second ticket took 200 milliseconds. Agents started their shifts, typed a question, waited nearly 20 seconds staring at a loading spinner, then assumed the system was broken and refreshed the page. The refresh triggered another cold start. Another 18 seconds. By day three, adoption dropped from 68 agents to 11. The AI team discovered that their serverless endpoint went cold after five minutes of inactivity, and every morning when agents logged in across a 90-minute window, each one triggered a fresh cold start. The infrastructure was working exactly as designed. The design was incompatible with the usage pattern.

This is the cold start problem. Serverless inference endpoints offer compelling economics but introduce initialization latency that can destroy user experience. The choice between serverless and dedicated inference is not about cost alone. It is about matching deployment architecture to traffic patterns, latency requirements, and user expectations. Most teams make this decision based on pricing calculators. The teams that ship reliable systems make it based on how their users actually interact with the system.

## The Cold Start Penalty in LLM Inference

A **cold start** is the initialization time required when an inference endpoint has no active container or instance ready to process a request. For traditional APIs, cold starts are measured in hundreds of milliseconds. For large language models in early 2026, cold starts range from 5 seconds to 30 seconds depending on model size, provider, and infrastructure configuration.

The initialization process loads the model weights from storage into GPU memory, initializes the runtime environment, and prepares the serving infrastructure. For a 70-billion-parameter model, this means loading approximately 140 gigabytes of model weights if using 16-bit precision. Even with fast storage and high-bandwidth interconnects, this takes time. Smaller models have shorter cold starts, but even a 7-billion-parameter model served via serverless infrastructure typically requires 3 to 8 seconds to initialize from a fully cold state.

Provider-specific cold start benchmarks measured in January 2026 show significant variance. OpenAI's serverless endpoints for GPT-5-mini typically initialize in 4 to 7 seconds. Anthropic's serverless Claude Haiku 4.5 endpoints show 5 to 9 seconds. Google's Vertex AI serverless deployment for Gemini 3 Flash averages 6 to 11 seconds. AWS Bedrock serverless invocations for Llama 4 Scout range from 8 to 14 seconds. Azure's serverless endpoints for models hosted via their model catalog show 7 to 13 seconds. These numbers represent the penalty your users pay when an endpoint has fully spun down.

Not all serverless platforms behave identically. Some providers implement **soft cold starts**, where the container is kept warm for a grace period after the last request. If a new request arrives within that window, initialization time drops to under one second. The grace period varies: five minutes is common, though some providers use ten minutes or two minutes depending on pricing tier and configuration. This means your first request of the day takes 12 seconds, but if requests arrive every three minutes, subsequent requests see sub-second response times. The problem is that "every three minutes" is not how most applications generate traffic.

## Dedicated Inference: Always Warm, Always Paying

**Dedicated inference** endpoints solve the cold start problem by keeping infrastructure continuously running. You provision a specific instance type with allocated GPU resources, the model stays loaded in memory, and every request hits a warm endpoint. Response times are consistent: no 15-second outliers, no variability between the first request and the hundredth.

The trade-off is cost structure. With serverless, you pay per request. With dedicated infrastructure, you pay for allocated time regardless of utilization. A dedicated endpoint running 24 hours a day costs the same whether it processes 10 requests or 10,000 requests. If your traffic is bursty or unpredictable, you pay for substantial idle capacity. If your traffic is high-volume and consistent, dedicated infrastructure becomes significantly more economical than serverless on a per-request basis.

Provider pricing in early 2026 shows the crossover point clearly. A dedicated OpenAI deployment running GPT-5-mini on Azure costs approximately 1,200 dollars per month for a single instance. That instance can handle roughly 30 requests per minute at typical prompt and completion lengths. If you process 40,000 requests per month, the per-request cost is 3 cents. Serverless pricing for the same model is 0.8 cents per request. Serverless wins. If you process 400,000 requests per month on that same dedicated instance, the per-request cost drops to 0.3 cents. Dedicated wins by a factor of nearly three.

Anthropic's dedicated Claude Opus 4.5 deployments on AWS follow a similar pattern. A provisioned throughput instance costs approximately 4,500 dollars per month and supports around 50 requests per minute for typical workloads. At 100,000 requests per month, the per-request cost is 4.5 cents. Serverless Claude Opus 4.5 on Bedrock costs approximately 2.2 cents per request. Serverless wins. At 1 million requests per month, dedicated drops to 0.45 cents per request. Dedicated wins decisively.

The decision is not purely financial. Dedicated infrastructure provides **latency predictability**. Every request sees the same p50, p95, and p99 response times because there are no cold starts, no queueing behind initialization, and no variance from instance spin-up. For user-facing features where consistency matters more than average cost, dedicated infrastructure is often the only viable option even when the per-request economics slightly favor serverless.

## Warm-Up Strategies for Serverless Deployments

If you must use serverless infrastructure but cannot tolerate cold starts, you implement a **warm-up strategy**. The goal is to prevent the endpoint from going fully cold by sending periodic keep-alive requests that maintain the container in a ready state.

The simplest approach is a **scheduled warm-up ping**. A cron job or scheduled cloud function sends a minimal request to the serverless endpoint every few minutes. The request can be a trivial prompt that generates a short completion, or it can be a health-check-style invocation if the provider supports that pattern. The cost is the cumulative price of all warm-up requests plus the compute time they consume. If you send a warm-up request every three minutes and each request costs half a cent, you spend approximately 2,400 requests per day or 12 dollars per day for continuous warm-up. Over a month, that is 360 dollars just to keep the endpoint from going cold.

This approach works if your traffic is distributed throughout the day and you need the endpoint available at all times. It fails if your traffic is concentrated in a narrow window. A customer support tool used only during business hours does not benefit from 16 hours of overnight warm-up pings. The better strategy is **scheduled warm-up aligned to usage patterns**. Start the warm-up process 10 minutes before the first users arrive, maintain it during business hours, and let the endpoint go cold overnight. This reduces warm-up cost by two-thirds while still ensuring that users never hit a cold start.

A more sophisticated approach is **traffic-based scaling warm-up**. Some platforms allow you to configure auto-scaling rules that pre-warm additional instances when traffic begins to increase. If your system sees a typical pattern where traffic ramps up over 20 minutes each morning, you configure the platform to start warming additional capacity as soon as the first few requests arrive. The first user may still see a cold start, but the second through hundredth users see warm endpoints because the system anticipated the ramp.

Not all providers support traffic-based warm-up at the application level. If your platform does not, you can implement it yourself using a proxy layer. The proxy tracks request volume over rolling time windows and sends synthetic warm-up requests when it detects traffic beginning to climb. This adds architectural complexity but can reduce cold start exposure by 80 percent or more in workloads with predictable daily patterns.

## When Cold Starts Are Acceptable

There are use cases where cold starts do not matter. An **internal research tool** used by a team of six data scientists a few times per day can tolerate a 10-second cold start on the first request. The scientists are not staring at a loading spinner waiting for real-time feedback. They submit a query, switch to another task, and check results a minute later. A 10-second initialization penalty is irrelevant in that context.

A **batch processing pipeline** that runs nightly and processes thousands of documents does not care about the first request taking 12 seconds if the subsequent 5,000 requests take 300 milliseconds each. The cold start cost amortizes to an insignificant fraction of total processing time.

A **low-traffic admin dashboard** used by three operations staff members a few times per week can use serverless inference without warm-up. If the dashboard generates five LLM requests per day, paying for a dedicated endpoint that sits idle 99.9 percent of the time is wasteful. The staff can wait 8 seconds for the first response of the day.

The pattern is consistent: cold starts are acceptable when **request frequency is low**, when **users do not expect real-time interaction**, or when **the first request is not latency-sensitive**. If your application meets any of these conditions, serverless inference without warm-up is a rational choice that minimizes cost without degrading the user experience in a way that matters.

## When Cold Starts Destroy the Experience

Cold starts are unacceptable when users expect **immediate feedback** and when the system is positioned as a **real-time assistant**. A chatbot interface that shows a loading spinner for 14 seconds on the first message of a session trains users to believe the system is broken. Users do not have a mental model that distinguishes between cold starts and actual failures. They see a delay, assume malfunction, and abandon the interaction.

A **live transcription tool** that takes 18 seconds to initialize before producing the first transcribed sentence is unusable. The meeting has already moved on. The context is lost. The tool fails its core function even though the infrastructure is operating correctly.

A **code completion assistant** in an IDE that introduces a 9-second delay the first time a developer requests a suggestion will be disabled immediately. Developers expect sub-second feedback. A 9-second pause is perceived as a hang. The assistant gets uninstalled before the second request.

User-facing real-time features require **consistent low latency**. The p95 and p99 response times matter as much as the p50. A system that delivers 200-millisecond responses 95 percent of the time but 15-second responses 5 percent of the time has a latency profile that makes it unsuitable for interactive use. Dedicated inference is not optional in these cases. It is the only deployment mode that delivers the consistency the experience requires.

## The Serverless vs Dedicated Decision Matrix

The decision matrix has four inputs: **request volume**, **traffic pattern**, **latency sensitivity**, and **cost tolerance**.

If request volume is below 50,000 per month and traffic is sporadic, serverless wins on cost unless latency sensitivity is extreme. If request volume exceeds 500,000 per month and traffic is steady, dedicated infrastructure wins on both cost and latency predictability. The middle range from 50,000 to 500,000 requests per month is where the decision depends on the other factors.

If traffic follows a **predictable daily pattern**, serverless with scheduled warm-up is viable. You pay for warm-up during active hours and let the endpoint go cold overnight. If traffic is **uniformly distributed across 24 hours**, dedicated infrastructure eliminates the complexity and cost of continuous warm-up.

If latency sensitivity is **high**, meaning users expect sub-second response times and cannot tolerate multi-second outliers, dedicated infrastructure is required regardless of request volume. If latency sensitivity is **moderate**, meaning users accept occasional delays of a few seconds, serverless with warm-up is workable. If latency sensitivity is **low**, meaning users do not interact in real time, raw serverless without warm-up is appropriate.

Cost tolerance is the final factor. If your budget is constrained and you cannot justify paying for idle infrastructure, serverless is the forcing function. If your budget allows for infrastructure that prioritizes user experience over marginal cost savings, dedicated endpoints are the safer choice.

Most teams default to serverless because the pricing model feels lower-risk. You pay only for what you use. The hidden cost is the engineering time spent debugging cold start issues, implementing warm-up strategies, and fielding user complaints about inconsistent performance. Dedicated infrastructure has a higher upfront cost but eliminates an entire class of operational problems.

## Hybrid Patterns: Dedicated for Peak, Serverless for Overflow

The most sophisticated deployment pattern is **hybrid**: dedicated instances handle baseline and peak traffic, serverless instances handle overflow and off-peak requests. This combines the predictability of dedicated infrastructure with the elasticity and cost efficiency of serverless.

A customer service platform expects 200 requests per minute during business hours and 10 requests per minute overnight. A dedicated instance sized for 220 requests per minute runs continuously, handling all business-hour traffic and most off-peak traffic. A serverless endpoint remains configured but idle unless traffic exceeds the dedicated instance's capacity. If an unusual spike pushes traffic to 300 requests per minute, the serverless endpoint activates and absorbs the overflow. The cold start penalty affects only the overflow traffic, which represents a small fraction of total requests and occurs during abnormal conditions when users are more tolerant of slight delays.

This pattern requires a **request routing layer** that directs traffic intelligently. The router sends all requests to the dedicated instance first. If the dedicated instance's queue depth exceeds a threshold or response time degrades, the router begins sending a percentage of requests to the serverless endpoint. As traffic subsides, the router drains the serverless endpoint and returns all traffic to dedicated infrastructure.

The cost structure is predictable. You pay the fixed monthly cost of the dedicated instance plus the variable cost of serverless overflow. In most months, overflow is near zero. In months with unusual traffic spikes, you pay marginally more but avoid the much higher cost of provisioning dedicated infrastructure sized for worst-case traffic.

Hybrid patterns are more complex to implement and operate than pure serverless or pure dedicated deployments. They require traffic management logic, monitoring of queue depth and latency across both endpoint types, and failover handling if the dedicated instance becomes unavailable. The operational cost is justified when traffic variability is high and cost optimization matters. For teams with steady traffic or limited operational capacity, pure dedicated or pure serverless is simpler and sufficient.

## Provider-Specific Deployment Patterns in 2026

Different providers offer different primitives for managing cold starts and provisioning dedicated capacity. OpenAI's Azure-hosted deployments provide **provisioned throughput units**, which guarantee a minimum processing capacity and eliminate cold starts entirely. You pay a fixed monthly rate per unit and can scale units up or down with a few hours of lead time. This is effectively dedicated infrastructure with flexible scaling.

Anthropic's AWS Bedrock integration offers **provisioned throughput** for Claude models, following a similar pattern. You purchase a committed throughput level and receive dedicated capacity. If you exceed provisioned throughput, requests spill over to on-demand serverless instances, creating a hybrid pattern managed by the platform itself.

Google's Vertex AI provides **prediction endpoints** with configurable auto-scaling. You set a minimum number of nodes that remain always active, ensuring no cold starts for baseline traffic, and configure a maximum number of nodes for scaling. The platform auto-scales between the minimum and maximum based on request load. This is a managed hybrid approach where you control the cost floor and ceiling.

Smaller model providers and open-source hosting platforms often provide only serverless deployment options. If you need dedicated infrastructure for an open-source model, you deploy it on your own GPU instances using serving frameworks like vLLM, TensorRT-LLM, or Text Generation Inference. This gives you complete control over warm-up, scaling, and latency but shifts operational responsibility entirely to your team.

The provider you choose determines which deployment patterns are available and how much control you have over cold start behavior. Teams frequently select a provider based on model quality and API design, then discover that the deployment options do not match their latency requirements. Evaluating deployment flexibility should be part of provider selection, not an afterthought during implementation.

## Monitoring and Measuring Cold Start Impact

You cannot manage what you do not measure. Tracking **cold start frequency** and **cold start latency** as distinct metrics is essential for understanding whether your deployment strategy is working.

Instrument your application to tag requests as cold or warm based on response time. If a request takes longer than a threshold you define based on expected warm response time, flag it as a probable cold start. Calculate the percentage of total requests that experience cold starts and monitor that percentage over time. If cold starts represent more than 5 percent of requests in a user-facing application, your warm-up strategy is insufficient or your traffic pattern is incompatible with serverless deployment.

Measure the **latency distribution** separately for cold and warm requests. Warm requests should show tight clustering with p50, p95, and p99 within a narrow range. Cold requests will show much higher latency with significant variance. If the p99 cold start latency exceeds user tolerance thresholds, you need to either eliminate cold starts via dedicated infrastructure or reduce cold start frequency via more aggressive warm-up.

Track cold starts **by time of day**. If cold starts cluster in the early morning when users first log in, scheduled warm-up aligned to business hours will solve the problem. If cold starts are evenly distributed, your traffic is too sparse to keep serverless endpoints warm and you need dedicated infrastructure or more frequent warm-up pings.

Monitor **cost per request** separately for serverless and dedicated deployments. Calculate the total monthly cost of dedicated infrastructure divided by the number of requests processed to get the effective per-request cost. Compare that to the per-request cost of serverless including warm-up overhead. If dedicated infrastructure is within 20 percent of serverless cost and provides better latency, the premium is worth paying.

Cold start impact is not always obvious from aggregate metrics. A system with a median response time of 400 milliseconds and a p95 of 8 seconds looks acceptable in a dashboard but delivers a terrible experience to the 5 percent of users who hit cold starts. Segment your metrics to surface the cold start population explicitly.

## The Real Cost of Serverless: Engineering Time

Serverless pricing looks attractive on a spreadsheet. The hidden cost is the **engineering effort** required to make serverless work reliably for latency-sensitive use cases. You build warm-up logic, implement request routing to retry cold requests, monitor cold start frequency, tune warm-up intervals, and debug why certain users see delays.

A dedicated endpoint costs more in infrastructure but eliminates the entire cold start problem space. You deploy the model, point your application at the endpoint, and response times are consistent. No warm-up logic. No retry routing. No user complaints about intermittent slowness.

For a team of two engineers building an internal tool, the serverless complexity may not be worth the cost savings. Paying an extra 800 dollars per month for dedicated infrastructure buys back 20 hours of engineering time that would otherwise go toward managing cold starts. For a team of 15 engineers building a high-scale consumer product, the engineering time is available and the cost savings from serverless at 2 million requests per month may be 15,000 dollars per month. The complexity is justified.

The decision is not technical. It is economic and organizational. Understand the true cost of each option, including the engineering time required to operate it, before committing to a deployment strategy.

## What Happens Next

You have chosen a deployment mode based on traffic patterns, latency requirements, and cost. The next challenge is ensuring that inference endpoints are located close to the users generating requests. Geographic distance introduces round-trip latency that no amount of optimization can eliminate. The next subchapter covers how to route requests to inference endpoints based on user location and why geographic routing matters far more than most teams expect.
