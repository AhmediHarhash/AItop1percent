# 6.2 — Pipeline Architectures: Different Models for Different Stages

In June 2025, a financial research platform launched an AI-powered earnings call analysis feature. Portfolio managers uploaded earnings call transcripts, and the system generated investment insights—key themes, sentiment shifts, management tone indicators, and forward-looking statements. The initial architecture used Claude Opus 4.1 for the entire workflow: extract structured data from transcripts, identify important moments, generate summaries, score sentiment, and validate outputs for factual consistency. The product worked, but it was slow—median processing time for a one-hour transcript was 48 seconds—and expensive—each analysis cost $2.40 in API fees. At 1,200 analyses per day, the monthly cost was $86,000. The VP of Engineering asked the team to optimize. They rebuilt the system as a five-stage pipeline. Stage one used a specialized financial entity extraction model to pull company names, executive names, financial metrics, and guidance statements—a fine-tuned Llama 4 Scout variant running on their own infrastructure, cost per analysis three cents, latency 1.2 seconds. Stage two used GPT-5.1 to identify thematic segments in the transcript—discussions of product lines, geographic markets, cost initiatives—cost per analysis eighteen cents, latency 3.1 seconds. Stage three used Claude Opus 4.1 to generate insight summaries for each segment, the task that actually required frontier model reasoning—cost per analysis $1.20, latency 8.4 seconds. Stage four used a sentiment analysis model fine-tuned on financial text to score management tone—cost per analysis one cent, latency 0.4 seconds. Stage five used GPT-5-nano to validate that generated insights did not contain unsupported claims or misquoted figures—cost per analysis half a cent, latency 0.6 seconds. The redesigned pipeline cost $1.43 per analysis, a 40% reduction, and median latency dropped to 13.7 seconds, a 71% improvement. Quality stayed within two percentage points of the original system on all measured dimensions. The architecture that made this possible was the **pipeline pattern**: different models optimized for different stages, connected by clear data contracts, each stage doing the task it does best.

Pipeline architectures are the most common multi-model pattern in production AI systems in 2026, because most AI workflows are not single atomic operations—they are sequences of distinct stages with different performance requirements. Understanding how to design pipeline stages, manage data flow between stages, handle error propagation, control latency accumulation, and optimize costs within pipelines is fundamental to building production systems that work at scale. The alternative—using one model for all stages—is almost always slower, more expensive, and lower quality than a well-designed pipeline.

This subchapter covers how to decompose workflows into pipeline stages, how to choose models for each stage, how to connect stages with clean interfaces, how to handle failures and errors that propagate through pipelines, and when pipelines add unnecessary complexity instead of value.

## Decomposing Workflows into Pipeline Stages

The first design decision in a pipeline architecture is determining where to draw stage boundaries. A stage is a distinct processing step with a clear input, a clear output, and a single primary task. The input and output should be well-defined data structures—text, structured objects, embeddings, scores—not vague concepts. The task should be describable in one sentence without the word "and." If you need the word "and" to describe what a stage does, it is probably two stages.

A customer support automation system decomposes a conversation turn into five stages. Stage one: classify customer intent from the message text. Input is message text. Output is intent label and confidence score. Task is classification. Stage two: extract entities relevant to the intent—order IDs, product names, dates, dollar amounts. Input is message text and intent label. Output is structured entity object. Task is named entity recognition. Stage three: retrieve relevant knowledge base articles based on intent and entities. Input is intent, entities, and conversation history. Output is ranked list of article chunks. Task is retrieval. Stage four: generate a response using retrieved context. Input is intent, entities, retrieved chunks, and conversation history. Output is response text. Task is generation. Stage five: validate that the response is safe, accurate, and policy-compliant. Input is response text and conversation context. Output is validation result and any flags. Task is classification and rule checking.

Each stage has a clear boundary and a single responsibility. You can swap the model at any stage without changing other stages, as long as the input and output contracts are honored. You can evaluate each stage independently by measuring its task-specific metrics. You can optimize each stage for its specific performance requirements—latency, cost, accuracy—without affecting other stages.

The most common mistake in pipeline design is creating stages that do too much. A stage that extracts entities and retrieves knowledge and generates a response is not a stage—it is three stages incorrectly bundled together. Bundling prevents you from optimizing each task independently and makes debugging failures nearly impossible, because you cannot tell which task within the stage caused the error. A second common mistake is creating stages that do too little, adding overhead without meaningful separation of concerns. A stage that only lowercases text or counts tokens is not worth the inter-stage coordination cost. The right granularity is stages that correspond to distinct tasks with different model requirements or different performance profiles.

You identify stage boundaries by mapping your workflow as a directed graph where nodes are tasks and edges are data dependencies. If task B cannot start until task A completes and produces output that task B consumes, A and B are separate stages. If task C can run in parallel with task B using the same input from task A, C might be a separate branch of the pipeline. If task D uses outputs from both B and C, D is a merge point. The graph structure reveals the natural stage boundaries and the data flow between them.

## Choosing Models for Each Stage Based on Task Profile

Once you have defined stages, you select a model for each stage by matching the stage's task profile to model capabilities. Task profile includes task type, input and output structure, quality requirements, latency requirements, and cost constraints. Different stages have radically different profiles, and matching models to profiles is how you get better performance at lower cost than single-model architectures.

Classification stages benefit from small fast models with high precision. Intent classification, sentiment analysis, content moderation, spam detection, PII detection—these are tasks where the input is text and the output is a label or score. GPT-5-nano, GPT-5-mini, and fine-tuned Llama 4 Scout models excel at these tasks. They return results in 100 to 400 milliseconds, cost fractions of a cent per request, and achieve 95% to 99% accuracy with proper training. Using a frontier model like Claude Opus for classification is waste—you are paying for reasoning depth you do not use and tolerating latency you do not need.

Extraction stages benefit from models with strong structured output capabilities. Named entity recognition, data extraction from documents, parsing semi-structured text—these are tasks where the input is unstructured text and the output is a structured object with typed fields. GPT-5.1 with structured output mode, Claude Sonnet 4.5, and fine-tuned models trained on domain-specific schemas excel at these tasks. They produce well-formed JSON objects with high field-level accuracy, handle edge cases like missing data or ambiguous references, and integrate cleanly with downstream stages that expect structured inputs.

Retrieval stages benefit from specialized embedding models and efficient vector search. Embedding user queries, embedding document chunks, scoring semantic similarity—these are tasks where the input is text and the output is a dense vector or a ranked list of candidates. Cohere Embed v4, Voyage AI voyage-large-2-instruct, OpenAI text-embedding-3-large, and domain-specific fine-tuned embedding models excel at these tasks. They produce embeddings optimized for cosine similarity, support multilingual retrieval, and integrate with vector databases like Pinecone, Weaviate, and Qdrant. Using a generation model for retrieval is category error—generation models are not optimized for embedding quality.

Generation stages benefit from frontier models with strong reasoning and writing capabilities. Summarization, question answering, creative content generation, complex instruction following—these are tasks where the input is context and instructions and the output is fluent natural language. Claude Opus 4.5, GPT-5.2, Gemini 3 Pro, and Mistral Large 3 excel at these tasks. They handle long context windows, produce coherent multi-paragraph outputs, follow nuanced instructions, and maintain consistent tone and style. This is where you use your most expensive and capable models, because this is the stage where quality directly impacts user-perceived value.

Validation stages benefit from small fast models with high precision on binary or few-class decisions. Checking for PII, verifying factual consistency, detecting policy violations, scoring output quality—these are tasks where the input is generated text and the output is a pass-fail decision or a quality score. GPT-5-nano, fine-tuned classifiers, and rule-based checks excel at these tasks. They return results in under 200 milliseconds, cost almost nothing, and have very high precision to avoid false positives that block valid outputs.

Re-ranking stages benefit from cross-encoder models trained on relevance judgments. Scoring retrieved candidates for relevance to a query, ordering search results, prioritizing recommendations—these are tasks where the input is a query and a set of candidates and the output is a ranked list. Cross-encoder models based on BERT-scale architectures, fine-tuned on domain-specific relevance data, excel at these tasks. They are more accurate than embedding similarity for ranking because they score query-candidate pairs jointly rather than independently, but they are slower than embedding models, so you use them as a re-ranking stage after a fast embedding-based retrieval stage.

The selection process is task-first, not model-first. You define what the stage needs to do, specify quality and performance requirements, then choose the model that meets those requirements at the lowest cost and latency. You do not choose Claude Opus because you like Claude and then figure out what to do with it. You determine that a stage needs deep reasoning over 40,000 tokens of context and high-quality natural language generation, then choose Claude Opus because it is optimized for that profile.

## Designing Data Flow and Interfaces Between Stages

Stages communicate through data contracts—explicit specifications of input and output formats. A well-designed data contract makes stages composable, testable, and replaceable. A poorly-designed data contract couples stages together, makes debugging hard, and makes model swaps risky.

A good data contract specifies structure, types, semantics, and error cases. Structure means the shape of the data—a flat object, a nested object, an array of objects, a string. Types means the data types of each field—string, integer, float, boolean, enum. Semantics means what each field represents and what values are valid. Error cases means what happens when a stage cannot produce a valid output—does it return null, does it return a partial result with a flag, does it raise an exception?

The intent classification stage in a customer support pipeline has this output contract: an object with three fields. Field one: intent, a string from a closed set of twenty valid intents. Field two: confidence, a float between zero and one. Field three: ambiguous, a boolean indicating whether the intent is unclear. If the classifier cannot determine intent with confidence above 0.6, it sets ambiguous to true and intent to "unclear," and downstream stages handle the ambiguity appropriately—either escalate to a human or ask a clarifying question.

The entity extraction stage has this output contract: an object with four fields. Field one: entities, an array of entity objects, each with a type, value, and span. Field two: missing_entities, an array of entity types that were expected but not found. Field three: extraction_confidence, a float between zero and one. Field four: parse_errors, an array of issues encountered during extraction. If the extractor cannot parse part of the input, it includes a parse error with a description and continues processing the rest, so downstream stages get partial results rather than nothing.

These contracts make stages testable in isolation. You can test the intent classifier by feeding it sample messages and checking that outputs conform to the contract and meet accuracy requirements, without running the full pipeline. You can test the entity extractor by feeding it sample text and checking field-level precision and recall, without depending on the classifier. You can swap the model powering a stage—replace GPT-5-nano with a fine-tuned Llama model for intent classification—and as long as the new model produces outputs that conform to the contract, downstream stages work without modification.

Data flow between stages should be explicit and logged. Every stage reads its input from the previous stage's output, transforms it, and writes its output for the next stage to consume. The orchestration layer—the code that runs the pipeline—logs the input and output of each stage with timestamps, model identifiers, and any metadata like token counts or confidence scores. This makes debugging straightforward: if the pipeline produces a bad result, you inspect the logs to see where the failure originated. If the generation stage produced an off-topic response, you check whether the retrieval stage returned relevant context or garbage. If the retrieval stage returned garbage, you check whether the entity extraction stage produced correct entities or missed key information. You trace the failure backward through the pipeline until you find the stage where things went wrong.

Some pipelines use streaming data flow, where stages process inputs as they arrive rather than waiting for the full input to be ready. A summarization pipeline might stream transcript chunks to an embedding stage as they are ingested, retrieve relevant context for each chunk, and generate summaries incrementally. Streaming reduces end-to-end latency for long inputs and enables real-time processing. The trade-off is added complexity in orchestration and error handling, because you cannot roll back a partially processed stream as easily as you can retry a batch.

The orchestration layer should enforce timeouts and retries at each stage. If a stage does not return a result within its SLA—2 seconds for classification, 5 seconds for generation, 500 milliseconds for validation—the orchestrator times out the stage, logs the timeout, and either retries with the same model, retries with a fallback model, or fails the pipeline with a clear error. Retries should be exponential backoff with jitter to avoid thundering herd problems when a model endpoint degrades. The number of retries should be bounded—three retries is a reasonable default—to avoid infinite retry loops that waste money and time.

## Error Propagation Through Pipelines and Handling Failures

Errors in pipelines propagate downstream unless you design explicit error handling. If the intent classification stage misclassifies a customer message, the retrieval stage retrieves irrelevant articles, the generation stage produces an unhelpful response, and the validation stage might pass it because the response is syntactically correct. The error originated in stage one but manifested in the final output, and if you only measure end-to-end quality, you will not know which stage to fix.

The solution is stage-level monitoring and error propagation metadata. Every stage logs its confidence or certainty in its output. The intent classifier logs a confidence score. The entity extractor logs extraction_confidence. The retrieval stage logs a relevance score for the top-ranked result. The generation stage logs a self-assessed quality score if the model supports it. The validation stage logs whether it found any issues. These confidence signals propagate downstream as metadata attached to the data flowing through the pipeline.

When a stage produces low-confidence output, the orchestrator has four options. Option one: proceed with a flag that warns downstream stages the input might be unreliable. The generation stage might use a more cautious prompt or add a disclaimer if the retrieved context has low relevance scores. Option two: retry the stage with a different model or different parameters. If the intent classifier returns confidence 0.58, retry with a larger model. Option three: escalate to a fallback path. If entity extraction fails, skip retrieval and use a general-purpose response template. Option four: fail fast and return an error to the user rather than producing a low-quality result. Which option you choose depends on your product requirements and the cost of false positives versus false negatives.

Error propagation is especially dangerous in pipelines with many stages, because errors compound. If stage one has 95% accuracy, stage two has 95% accuracy, and stage three has 95% accuracy, and errors are independent, the end-to-end accuracy is 0.95 cubed, which is 85.7%. Three stages with good individual accuracy produce a system with mediocre accuracy because errors multiply. This is the **error cascade problem**, and it is one of the biggest risks in pipeline architectures.

You mitigate error cascades by improving accuracy at early stages, because early-stage errors propagate through the entire pipeline. A one-percentage-point improvement in intent classification accuracy—from 95% to 96%—improves end-to-end accuracy more than a one-percentage-point improvement in validation accuracy, because intent classification errors affect all downstream stages. You also mitigate error cascades by adding confidence-based routing, where low-confidence outputs trigger fallback paths or escalation rather than propagating through the pipeline. If the intent classifier is uncertain, escalate to a human or ask a clarifying question instead of proceeding with uncertain intent.

Some pipelines add correction stages that detect and fix errors from earlier stages. A fact-checking stage runs after generation and verifies that generated claims are supported by retrieved context, flagging unsupported claims for removal or revision. A grammar and style correction stage runs after generation and fixes errors that the generation model made. These correction stages add latency and cost, but they prevent early-stage errors from reaching users. The trade-off is whether the correction stage catches enough errors to justify the added complexity.

## Latency Accumulation Across Stages and Optimization Strategies

Every stage adds latency, and in a sequential pipeline, total latency is the sum of per-stage latencies plus orchestration overhead. A five-stage pipeline with per-stage latencies of 200ms, 400ms, 3000ms, 150ms, and 100ms has a total latency of 3850ms plus orchestration overhead, which might be 50ms to 200ms depending on how stages communicate. If your product requirement is sub-2-second response time, this pipeline does not meet the requirement, even though most individual stages are fast, because stage three—the generation stage—dominates total latency.

You optimize pipeline latency with four strategies: parallelize independent stages, use faster models where quality trade-offs are acceptable, use streaming where applicable, and cache where possible.

Parallelization works when stages do not depend on each other's outputs. If your pipeline has an entity extraction stage and a sentiment analysis stage that both consume the same input text, run them in parallel rather than sequentially. Total latency is the maximum of the two, not the sum. If entity extraction takes 400ms and sentiment analysis takes 300ms, running them in parallel gives you 400ms instead of 700ms.

Faster models work when you can accept slightly lower quality for significantly lower latency. If your generation stage uses Claude Opus 4.5 and takes 3000ms, switching to Claude Sonnet 4.5 might reduce latency to 1800ms at the cost of slightly less nuanced outputs. If the quality difference is negligible for your use case—user testing shows no preference—you take the latency win.

Streaming works when you can start processing downstream stages before upstream stages finish. A long document summarization pipeline might stream document chunks to the embedding stage as they are extracted, retrieve context for each chunk as embeddings are generated, and generate summaries incrementally rather than waiting for the entire document to be processed. Streaming reduces user-perceived latency because the first outputs arrive sooner, even if total processing time is the same.

Caching works when many requests share the same intermediate results. If your retrieval stage is expensive and many user queries retrieve the same knowledge base articles, cache the retrieved chunks keyed by query embedding, and serve cache hits without calling the retrieval model. If 40% of queries hit the cache, you eliminate 40% of retrieval latency and cost. Caching is most effective at stages with high reuse—embedding stages where the same content is embedded repeatedly, retrieval stages where popular queries are repeated, classification stages where the same inputs recur.

The latency budget should be allocated based on value. If the generation stage is the only stage that directly impacts user-perceived quality, allocate most of your latency budget to generation and minimize latency everywhere else. If retrieval quality is critical and retrieval latency is 1500ms but necessary to get good results, accept the latency cost and optimize other stages aggressively. The worst mistake is distributing latency evenly across stages without regard for which stages matter most.

You measure stage-level latency with percentile distributions, not just medians. A stage with median latency 300ms and P95 latency 2000ms is a problem, because 5% of requests will blow your latency budget. You investigate P95 outliers by looking for patterns—does the outlier latency happen with specific input characteristics, specific times of day, specific model endpoints? Model provider rate limiting, cold starts, and retries are common causes of latency spikes.

## Cost Optimization Within Pipelines

Pipeline architectures enable fine-grained cost optimization because you can choose different models at different stages based on cost-performance trade-offs at each stage. The most expensive stage in most pipelines is generation, because generation uses frontier models that cost ten to fifty times more per token than classification or embedding models. The second most expensive stage is often retrieval, if you are using embeddings and vector search at scale, because embedding costs accumulate with corpus size.

You optimize cost by using the cheapest model that meets quality requirements at each stage. If GPT-5-nano gives you 97% intent classification accuracy and GPT-5.1 gives you 98% accuracy, and the extra one percentage point does not meaningfully improve end-to-end quality, use GPT-5-nano and save 95% on classification costs. If a fine-tuned Llama 4 Scout model running on your own infrastructure gives you 96% accuracy at one-tenth the cost of GPT-5-nano, use the fine-tuned model. The decision is cost per unit of quality, not quality alone.

You also optimize cost by skipping stages when they are not needed. A customer support pipeline might skip the knowledge retrieval stage for simple FAQ questions where the intent classifier is highly confident and the answer is deterministic—store hours, return policy, contact information. Skipping retrieval saves embedding costs, vector search costs, and latency. The generation stage uses a template instead of generating from scratch. Total cost for simple requests drops from twelve cents to one cent, and latency drops from 2.1 seconds to 400 milliseconds.

Conditional routing is another cost optimization technique. If a content moderation pipeline routes 70% of content to a cheap model, 20% to a mid-tier model, and 10% to an expensive model, the blended cost is much lower than using the expensive model for everything. The router itself is a small classification model that costs almost nothing—GPT-5-nano at one-tenth of a cent per request—so the routing overhead is negligible compared to the savings.

Some pipelines use cost-based fallbacks. If the primary generation model is Claude Opus 4.5 at twelve cents per request, and the system is approaching its daily cost budget, the orchestrator switches to Claude Sonnet 4.5 at four cents per request for the rest of the day. Quality degrades slightly, but the system stays within budget. This is a trade-off decision—predictable cost versus consistent quality—and different products make different choices based on their priorities.

You track cost at the stage level and the task level. Stage-level cost tells you which stages consume the most money and where optimization efforts should focus. Task-level cost tells you which user-facing tasks are expensive and whether pricing or product decisions need to adjust. If summarizing a one-hour earnings call transcript costs $1.43 in API fees, and you charge users $5 per summary, your margin is healthy. If it costs $8 in API fees, your margin is negative, and you either need to optimize the pipeline, raise prices, or exit the market.

## When Pipelines Add Unnecessary Complexity

Pipeline architectures are not universally better than single-stage architectures. Pipelines add value when stages have different performance requirements and different optimal models, and when the coordination cost is justified by improved cost, latency, or quality. Pipelines add unnecessary complexity when all stages have similar requirements and could be handled by one model, when the orchestration overhead exceeds the benefits, or when team capacity is too constrained to operate a multi-stage system.

A simple Q&A product that retrieves a knowledge base article and generates a conversational answer might not need a pipeline. If the retrieval and generation stages both use the same model—Claude Sonnet 4.5 with built-in retrieval-augmented generation support—splitting them into separate stages adds orchestration overhead without enabling better model choices. The single-model approach is simpler and sufficient.

A low-volume internal tool that processes 200 requests per day might not justify a pipeline. The cost difference between an optimized pipeline and a single-model architecture is $8 per day—$240 per month. If building and maintaining the pipeline takes ten engineering hours, and your engineering cost is $150 per hour, the pipeline costs $1500 to build and does not pay for itself for six months. Use one model and spend engineering time on higher-leverage work.

A two-person startup building an MVP should not start with a five-stage pipeline. Start with one model, validate product-market fit, understand your task distribution and performance bottlenecks, then optimize with a pipeline once you have scale and data. Premature optimization is real, and building a complex architecture before you understand your product needs is a common failure mode for early-stage teams.

The decision to use a pipeline should be based on evidence that different stages benefit from different models, and that the cost or performance improvement justifies the added complexity. If you can get equivalent results with one model, use one model. If you have clear evidence that a three-stage pipeline reduces cost by 60% or improves latency by 50% or improves quality by ten percentage points, build the pipeline.

Pipeline architectures are the default for production AI systems at scale in 2026 because most AI workflows have multiple stages with different performance requirements, and matching each stage to an optimized model produces better systems than using one model for everything. But defaults are not mandates. The engineering judgment is knowing when the pattern applies and when it does not, and designing your system accordingly.

Understanding pipeline architectures—how to decompose workflows into stages, choose models for each stage, connect stages with clean interfaces, handle errors and latency, and optimize costs—gives you the foundation to design multi-model systems that work in production. The next question is how to route individual requests to the right model or the right pipeline based on request characteristics, and that requires a different pattern: dynamic routing based on task classification, complexity estimation, or user-level policies.
