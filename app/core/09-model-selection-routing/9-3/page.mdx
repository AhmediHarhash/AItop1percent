# 9.3 â€” Deprecation Response Playbook: From Announcement to Completed Migration

In March 2025, a legal technology company received an email from their primary model provider announcing that GPT-4 Turbo would be deprecated in 90 days. The platform processed contract analysis requests for 140 law firms, all relying on prompts carefully tuned over eight months to extract obligations, dates, and liability clauses with 94% accuracy. The engineering team assumed the recommended replacement, GPT-4.5, would be a straightforward drop-in upgrade. They updated the model identifier in their configuration file, ran a handful of manual tests on three contracts, saw reasonable results, and pushed to production two weeks before the deadline. Within 48 hours, their support queue filled with complaints. The new model was extracting dates in a different format, causing downstream parsing failures. It was more verbose, breaking their token budget assumptions. It refused to process certain insurance contract clauses that the old model handled fine, triggering a cascade of retries that doubled their API costs. They spent the next six weeks in emergency remediation mode, rewriting prompts, adjusting parsers, and explaining to furious clients why their service had suddenly become unreliable. The root cause was not technical incompetence. It was the absence of a systematic deprecation response process. They treated a model migration as a configuration change rather than a product change requiring the same rigor as any other breaking dependency update.

Model deprecation is not an edge case. It is a routine part of operating AI systems in 2026. OpenAI deprecated GPT-3.5 Turbo in January 2025, GPT-4 in June 2025, and GPT-4 Turbo in September 2025. Anthropic sunset Claude 3 Sonnet in April 2025 and Claude 3.5 Sonnet in November 2025. Google deprecated Gemini 1.5 Pro in August 2025. Every major provider operates on a generational model lifecycle: a new model launches, stabilizes for six to twelve months, enters maintenance mode, then gets deprecated as the next generation arrives. If your system depends on a specific model, you will face deprecation. The only question is whether you respond systematically or reactively.

This subchapter provides the complete playbook for responding to a deprecation announcement, from initial triage through final cleanup. It covers the four phases every migration must pass through, the timeline constraints you face, the team structure you need, and the mistakes that turn a manageable transition into a crisis.

## Phase One: Triage and Impact Assessment

The moment you receive a deprecation notice, you have a fixed countdown. Most providers give 90 to 180 days, but some give as little as 30 days, and in rare cases you get a week. Your first task is not to start migrating. It is to understand the scope of what you are migrating.

Begin with an **inventory of all systems** using the deprecated model. This is harder than it sounds if you operate multiple services, multiple environments, or have allowed teams to deploy their own model integrations. You need to identify production uses, staging uses, and any offline processes like batch evaluation jobs or data labeling pipelines. For each system, document the current model identifier, the prompt templates in use, the expected output format, and the performance requirements. If you have a centralized model registry or configuration service, this step takes an hour. If you do not, it can take a week and you will miss things.

Next, assess the **criticality of each affected system**. Not all model uses carry the same risk. A customer-facing chatbot that handles 10,000 queries per day is tier-one critical. An internal tool that summarizes meeting notes for 20 employees is tier-three. A research prototype that three people use is tier-four. The tier determines the migration priority and the level of testing rigor you apply. Tier-one systems require full eval suite validation, shadow deployments, and canary rollouts. Tier-three systems can often migrate with lighter testing. Tier-four systems might be decommissioned rather than migrated.

Estimate the **migration effort** for each system. The effort depends on prompt complexity, integration depth, and eval coverage. If you have a comprehensive eval suite for the system, migration is a known-risk project. You run evals, compare results, tune prompts if needed, and promote. If you have no evals, migration is a high-risk project. You are flying blind, unable to detect regressions until users report them. If the system uses the model output in a tightly coupled way, parsing specific formats or expecting specific tone, the migration effort is higher. If the system uses the output loosely, summarizing or reranking rather than parsing, the effort is lower.

Compile a **triage summary** that lists every affected system, its tier, its migration effort estimate, and the owner responsible for completing the migration. This summary gets reviewed by engineering leadership and becomes the basis for task assignment. Without this summary, migrations become ad hoc, teams duplicate work, and low-visibility systems get missed entirely.

## Phase Two: Evaluate Replacement Models

Once you know what you are migrating, you need to decide what you are migrating to. The deprecation notice usually includes a recommended replacement, but that recommendation is not binding and often not optimal. Providers recommend the newest model in their lineup because it is what they want to promote, not necessarily because it matches your use case.

Your task is to **run a replacement evaluation** using your existing eval suite. For each candidate model, run the full suite of test cases that define success for the deprecated system. The candidates should include the provider's recommended replacement, the closest equivalent from other providers, and any older models still in support that might be cheaper or faster. If you operate a legal contract analyzer and GPT-4 Turbo is being deprecated, your candidates might include GPT-4.5, GPT-5, Claude Opus 4.5, and Gemini 3 Pro. Run all of them through your eval set and compare the results.

The comparison must cover five dimensions: quality, cost, latency, format compatibility, and refusal rate. Quality is measured by your existing grading criteria, typically accuracy or F1 score or task-specific correctness. Cost is measured in dollars per million tokens, multiplied by the average token usage per request. Latency is measured in milliseconds from request to first token and request to completion. Format compatibility measures whether the new model produces outputs that your downstream parsers can handle without modification. Refusal rate measures how often the new model declines to answer a query that the old model handled successfully.

You will often find that no single replacement dominates on all dimensions. GPT-5 might have the highest quality but double your cost. Claude Opus 4.5 might be cheaper but 200 milliseconds slower. Gemini 3 Pro might be fast and cheap but produce outputs in a slightly different JSON structure that breaks your parser. This is normal. The decision is not which model is objectively best. The decision is which tradeoffs your product can absorb.

Present the eval results in a **migration decision report**. The report should include a summary table with all candidate models, their scores on each dimension, and a recommendation with justification. The recommendation should be reviewed by the engineering lead, the product manager, and in some cases the business owner, especially if the cost delta is significant. If you are migrating a tier-one system and the recommended replacement will increase your monthly model bill by 80%, that is a product decision, not just an engineering decision.

Some teams try to skip this phase and assume the provider's recommended replacement will work. This is the single biggest mistake in deprecation response. Providers optimize recommendations for average use cases, not your specific use case. Running evals takes two to five days. Discovering post-migration that the new model has a 12% quality regression takes two to five weeks to remediate and costs customer trust you will never fully recover.

## Phase Three: Migration Execution

With a replacement model selected, you move to execution. The goal is to switch production traffic from the deprecated model to the replacement model with zero user-facing disruption. This requires a staged rollout with validation gates at each stage.

The first stage is a **configuration update** in your non-production environments. Update the model identifier in your development, staging, and QA environments. Run your integration tests and smoke tests. Verify that the system starts, handles requests, and produces outputs in the expected format. This stage catches configuration errors, API compatibility issues, and gross formatting problems. It does not catch quality regressions or subtle behavioral differences.

The second stage is a **shadow deployment** in production. Shadow deployment means sending live production queries to both the old model and the new model, but only returning the old model's responses to users. You log both responses and compare them offline. This tells you how the new model behaves on real production traffic, which often differs from eval suite traffic in distribution, edge cases, and adversarial inputs. Shadow deployment runs for one to two weeks, generating enough data to detect quality differences, latency differences, and refusal rate differences. If the shadow metrics show unacceptable regressions, you halt the migration and either tune the prompts or reconsider the replacement choice.

The third stage is a **canary rollout**. Route a small percentage of production traffic, typically 5%, to the new model while the other 95% continues using the old model. Monitor error rates, latency, user feedback, and downstream system health. If the canary metrics remain stable for 48 hours, increase to 25%. If stable for another 48 hours, increase to 50%, then 100%. The canary rollout catches production-only issues that shadow deployment cannot, such as load-dependent latency spikes, rate limit interactions, or cache behavior differences.

The fourth stage is **full cutover**. Once 100% of traffic runs on the new model and metrics remain stable for at least a week, you disable the old model integration and update your configuration to make the new model the default. You do not delete the old integration code immediately. You leave it in place but inactive, in case you need to roll back.

Throughout execution, maintain a **migration log** that records every configuration change, every rollout percentage increase, and every metric snapshot. This log is essential for debugging if something goes wrong and for learning how to improve the process for the next deprecation.

## Phase Four: Cleanup and Documentation

Migration is not complete when production traffic switches over. It is complete when all references to the old model are removed and all institutional knowledge is updated.

Start with **code cleanup**. Remove the old model identifier from all configuration files. Delete the integration code paths that route requests to the deprecated model. Update any hardcoded references in scripts, documentation, or infrastructure-as-code templates. This cleanup prevents future confusion and ensures that new team members do not accidentally re-enable the deprecated model.

Next, update **eval archives**. Your eval suite likely contains baseline results for the old model. Archive those results with a timestamp and a label indicating the model version. Create a new baseline for the replacement model. This ensures that future eval runs compare against the correct baseline and that you maintain a historical record of how model changes affected system performance.

Update **runbooks and incident response documentation**. If your incident response guide says to roll back to GPT-4 Turbo in case of a model outage, that guidance is now obsolete. Replace it with the new model identifier and the new rollback procedure. If your capacity planning spreadsheet assumes GPT-4 Turbo pricing and token limits, update it to reflect the replacement model's pricing and limits.

Finally, conduct a **migration retrospective**. Gather the team that executed the migration and review what went well, what went poorly, and what you would do differently next time. Common lessons include: we should have started eval runs earlier, we should have notified customer success sooner, we should have automated the canary rollout, we should have documented the shadow deployment process. Capture these lessons in a shared document and incorporate them into your deprecation playbook for the next cycle.

Cleanup typically takes one to two weeks and feels like low-priority work because the user-facing system is already running on the new model. Skipping cleanup is tempting but dangerous. The next deprecation announcement will arrive in six to twelve months, and if your codebase still contains references to three generations of deprecated models, the triage phase becomes exponentially harder.

## Timeline Management and Acceleration

Providers typically give 90 to 180 days notice for major model deprecations, but this is not guaranteed. OpenAI gave 120 days notice for GPT-4 Turbo deprecation. Anthropic gave 90 days for Claude 3 Sonnet. Google gave 60 days for Gemini 1.5 Flash. In late 2025, one provider deprecated a fine-tuned model variant with only 21 days notice due to a compliance issue. You cannot assume generous timelines.

A standard four-phase migration with full rigor takes eight to twelve weeks: one week for triage, two weeks for eval runs and decision-making, four weeks for shadow deployment and canary rollout, one week for cleanup. If you receive a 90-day notice and start immediately, you finish comfortably. If you receive a 60-day notice, you must compress. If you receive a 30-day notice, you must cut scope.

The compression priority is reverse-phase. You compress cleanup first. Instead of fully removing all old model references, you disable them and defer deletion. You compress shadow deployment second. Instead of running shadow traffic for two weeks, you run it for three days and accept higher uncertainty. You never compress eval runs. Running evals against the replacement model is the only validation gate that prevents quality regressions. Skipping it to save time is trading a two-week schedule gain for a multi-month quality remediation.

Some teams try to accelerate by **pre-qualifying replacement models** before deprecation announcements. They maintain a quarterly eval rotation where they test the newest models from all major providers against their production eval suites, even if they are not planning to switch. When a deprecation notice arrives, they already have recent eval data for likely replacements and can skip directly to migration execution. This front-loading costs engineering time every quarter but reduces migration risk and timeline pressure. For tier-one systems with high switching costs, it is worth the investment.

## The Deprecation Response Team

Model migration is not a solo engineering task. It requires coordination across engineering, product, operations, and sometimes customer success. The team structure varies by organization size, but the roles are consistent.

The **migration lead** is a senior engineer who owns the end-to-end timeline, coordinates across teams, and makes final decisions on replacement model selection and rollout gates. This person is accountable for ensuring the migration completes before the deprecation deadline and for managing any post-migration issues.

The **eval engineer** runs the replacement model evaluation, interprets the results, and provides the technical recommendation on which model to select. This person must understand the eval suite deeply and be able to diagnose why a model performs differently on specific test cases.

The **integration engineer** executes the configuration changes, shadow deployment, and canary rollout. This person must understand the production deployment pipeline, the model routing logic, and the observability stack.

The **product stakeholder** reviews the migration decision report, approves tradeoffs between quality and cost, and decides whether schedule pressure justifies cutting scope. This person represents the user perspective and ensures that migration decisions align with product priorities.

The **customer success liaison** monitors user feedback during the canary rollout, triages complaints, and escalates issues to the migration lead. This person ensures that user-facing regressions are detected quickly and communicated clearly.

For small teams, one person may wear multiple hats. For large organizations with dozens of affected systems, you may run multiple migration teams in parallel, each responsible for a subset of tier-one services, with a centralized program manager coordinating across teams.

The team must meet at least twice per week during active migration phases to review metrics, assess risks, and make go or no-go decisions on rollout progression. Asynchronous communication is insufficient. Migration timelines are tight, and decisions that take three days to resolve via email can be made in 20 minutes in a synchronous meeting.

## Common Deprecation Response Mistakes

The most common mistake is **waiting too long to start**. Teams receive a 120-day notice, acknowledge it, then defer action for 60 days because other work feels more urgent. They start triage with 60 days remaining, discover the migration is more complex than expected, and spend the final 30 days in crisis mode, cutting corners and skipping validation steps. The correct response is to start triage the same week you receive the notice, even if that means pausing other work. Deprecation deadlines are hard. Feature deadlines are negotiable.

The second most common mistake is **not running evals**. Teams assume the provider's recommended replacement is equivalent, update the config, do some manual spot-checking, and ship. They discover regressions in production via user complaints. This mistake is inexcusable if you have an eval suite. Running evals is not optional due diligence. It is the minimum bar for professional engineering.

The third most common mistake is **migrating all systems to the same replacement model**. You operate five different AI features, all using the deprecated model. The deprecation notice recommends GPT-5. You migrate all five features to GPT-5 without evaluating whether GPT-5 is optimal for each use case. Feature A benefits from GPT-5's improved reasoning. Feature B sees no quality difference but now costs 40% more. Feature C actually regresses because GPT-5 is more verbose and breaks the token budget. The correct approach is to evaluate replacements per feature, not per deprecated model. Different systems should migrate to different replacements if the eval data supports it.

The fourth mistake is **ignoring cost deltas**. The replacement model is 30% more expensive per token. You migrate, costs increase, finance escalates, and you are asked to justify the decision retroactively. You should quantify cost impact during the eval phase, present it in the migration decision report, and get explicit approval before proceeding. If the cost increase is unacceptable, you either choose a cheaper replacement or redesign the feature to use fewer tokens.

The fifth mistake is **insufficient rollback planning**. You cut over to the new model, metrics look stable, you delete the old model integration code. Two weeks later, the new model starts refusing a category of queries due to a provider-side policy change. You have no rollback path. You are stuck. The correct approach is to keep the old model integration code in place, disabled but functional, for at least 30 days post-cutover. Rollback capability is insurance. You pay the cost of maintaining unused code paths in exchange for the ability to recover from late-breaking issues.

## Deprecation as Operational Hygiene

Model deprecation is not a crisis. It is routine operational hygiene, like dependency updates or certificate renewals. Treating it as a crisis every time reflects a lack of process maturity. High-performing teams have a documented deprecation playbook, a designated migration lead for each major system, and a quarterly eval rotation that pre-qualifies replacement models. They receive a deprecation notice, execute the playbook, and complete migration with 30 days to spare.

If your organization has migrated models three times and each time felt chaotic, the problem is not bad luck or tight timelines. The problem is the absence of a systematic response process. Build the playbook now, before the next notice arrives. Document the four phases, assign roles, create the triage checklist, and establish the eval comparison template. The next deprecation is six months away, not six years. You will use this playbook again.

Having a systematic process for responding to deprecation announcements ensures smooth transitions, but the actual switch to a new model introduces new risks that require rigorous testing, which is the focus of the next subchapter on migration testing protocols.

