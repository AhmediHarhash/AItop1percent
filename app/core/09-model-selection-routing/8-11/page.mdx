# 8.11 â€” Migration Paths: Moving Between Providers and Between API and Self-Hosted

In July 2025, a legal technology company spent eleven months migrating from GPT-4 Turbo to Claude Opus 4.5. The trigger was a pricing increase that would add $340,000 annually to their inference costs. The CTO approved the migration in August 2024, expecting completion by December. Engineering set up parallel running in September, discovered behavioral drift in October, spent four months retuning prompts and adjusting eval thresholds, hit three separate rollback events, and finally cut over the last production workload in June 2025. The migration consumed 2,800 engineering hours across six people and delayed two product launches. The root cause was not technical complexity. The team had no migration playbook, no provider abstraction layer, and no systematic process for validating behavioral equivalence. Every prompt was tightly coupled to GPT-4's specific response patterns, every evaluation threshold was calibrated to its output distribution, and every rollback required manual coordination across five microservices. What should have been a two-month config change became a year-long rewrite because the system was built without migration as a design requirement.

Migration between model providers and between deployment modes is not an edge case. It is a recurring operational necessity driven by pricing changes, capability improvements, compliance requirements, and infrastructure evolution. Teams that treat migration as a one-time event build systems that ossify around a single provider's API contract and behavioral quirks. Teams that design for migration from day one build provider-agnostic abstractions, maintain comprehensive eval suites, and can shift traffic between models in days instead of months. The difference is not luck or vendor choice. It is architecture and process discipline.

## The Provider Migration Playbook

Provider migration follows a five-phase playbook that applies whether you are moving from OpenAI to Anthropic, from Claude to Gemini, or from GPT-4 to GPT-5. The phases are parallel running, eval suite comparison, gradual traffic shift, monitoring for behavioral drift, and rollback readiness. Each phase has specific entry criteria, deliverables, and exit gates that prevent you from moving forward before you have validated the previous step.

**Parallel running** means sending every production request to both the current provider and the migration target, but only returning the current provider's response to end users. You log both responses, both latencies, both token counts, and both failure modes. Parallel running is not optional. It is the only way to measure behavioral equivalence under real production load with real user queries. Teams that skip parallel running and rely on synthetic benchmarks discover behavioral drift after cutover, when rollback is expensive and user-facing. Parallel running requires infrastructure that can duplicate requests, route to multiple providers simultaneously, and reconcile responses without blocking the critical path. If your architecture cannot support parallel running, you do not have a migration-ready system. You have a monolithic coupling to a single vendor.

During parallel running, you collect comparison metrics for every request. Response similarity measured by semantic equivalence, not string matching. Latency distribution at p50, p95, and p99. Token consumption and cost per request. Error rate and error type distribution. Refusal rate and policy trigger patterns. You run parallel for at least two weeks in production to capture weekly usage cycles, seasonal variation, and edge case queries that only appear in real traffic. A three-day parallel run misses weekend patterns, end-of-month spikes, and the long tail of rare query types that break when you cut over.

**Eval suite comparison** takes the production request corpus captured during parallel running and runs it through your offline evaluation suite for both providers. This is where you measure task success rate, output quality scores, and compliance with your success criteria. If your eval suite is calibrated to the old provider's behavior, you will see false negatives. If your success criteria are model-agnostic, you will see true differences in capability. The distinction is critical. A drop in eval pass rate is only meaningful if your criteria measure user value, not model quirks.

Some behavioral differences are acceptable. Claude Opus 4.5 may format lists differently than GPT-5, but if both satisfy the functional requirement, the difference is cosmetic. Other differences are breaking. If the new provider refuses 18 percent of queries that the old provider handled, you have a policy mismatch that requires either prompt retuning or provider abandonment. Eval suite comparison surfaces these differences before you route production traffic, when the cost of fixing them is low.

You also compare cost and latency under production load. The new provider may be 30 percent cheaper at list prices, but if it requires 40 percent more tokens per response to achieve equivalent quality, your total cost increases. If p95 latency jumps from 1.8 seconds to 4.2 seconds, your user experience degrades even if task success rate holds steady. These are not theoretical concerns. They are the primary reasons migrations fail after cutover, when you have already announced the switch to stakeholders and locked in contracts.

## Gradual Traffic Shift and Rollback Criteria

Once parallel running and eval suite comparison confirm behavioral equivalence, you begin gradual traffic shift. Start at one percent of production traffic routed to the new provider, with the other 99 percent on the old provider. Monitor for 48 hours. If success metrics hold, increase to five percent. Monitor for 48 hours. Then ten percent, 25 percent, 50 percent, 75 percent, and finally 100 percent. Each increment is a hypothesis test: the new provider performs equivalently to the old provider at this traffic level. If the hypothesis fails, you roll back to the previous percentage and investigate the failure mode.

Gradual traffic shift is implemented as a feature flag or traffic routing policy, not a code deployment. You change a config value and traffic shifts. You change it back and traffic reverts. If shifting traffic requires a code push, you are doing it wrong. The shift must be instantaneous and reversible without engineering intervention. This requires a routing layer that sits between your application and the model API, with provider selection controlled by a dynamic config service.

**Rollback criteria** are defined before you start the traffic shift, not when something breaks. You specify the metrics that trigger automatic rollback and the thresholds that constitute failure. Task success rate drops below 92 percent: automatic rollback. P95 latency exceeds 3.5 seconds: automatic rollback. Error rate exceeds 2 percent: automatic rollback. Cost per request exceeds $0.045: automatic rollback. These thresholds are derived from your baseline performance with the old provider, with a tolerance buffer for acceptable variance.

Automatic rollback is enforced by the routing layer, not by human decision-making. If the threshold is breached, traffic reverts to the old provider within 60 seconds without waiting for an engineer to investigate. You investigate after rollback, when the user-facing impact is contained. Teams that rely on manual rollback decisions lose tens of thousands of dollars in degraded service while engineers debate whether the metrics are real or noise. Automatic rollback is not optional. It is the safety net that makes gradual migration low-risk.

During traffic shift, you monitor for behavioral drift that does not trigger rollback but indicates emerging problems. User complaints about response quality that do not correlate with eval scores. Subtle changes in refusal patterns for edge case queries. Increased support tickets about formatting or tone. These signals are early warnings that your eval suite is incomplete or your success criteria miss aspects of quality that users care about. You do not roll back immediately, but you do investigate and potentially pause the traffic shift until you understand the root cause.

## Moving from API to Self-Hosted

The migration from API-hosted models to self-hosted models is a different problem than provider-to-provider migration. You are not just changing API endpoints. You are changing deployment infrastructure, ops responsibilities, and often model architectures. The API-to-self-hosted migration is typically driven by cost at scale, data residency requirements, or IP control. A healthcare platform moves from Claude Opus API to self-hosted Llama 4 Maverick to satisfy HIPAA BAA requirements that prohibit sending PHI to third-party APIs. A financial services firm moves from GPT-5 API to self-hosted Qwen 3 to reduce per-request costs from $0.038 to $0.003 at their inference volume. An AI-first product company moves from Gemini API to self-hosted Mistral Large 3 to control model versioning and avoid forced upgrades.

The first problem is **prompt translation**. Open-weight models do not use the same prompt formats, chat templates, or system message conventions as proprietary APIs. Claude expects XML tags for structured prompts. GPT-5 uses JSON mode and structured outputs. Llama 4 uses a specific chat template with begin-of-text and end-of-text tokens. If you migrate from Claude Opus API to self-hosted Llama 4 without translating your prompts, every request will fail or produce gibberish. Prompt translation is not a find-and-replace operation. It requires understanding the semantics of your prompts and reconstructing them in the target model's expected format.

You also face **behavioral differences** between proprietary and open-weight models. GPT-5 and Claude Opus 4.5 are trained with extensive RLHF, safety tuning, and instruction following. Llama 4 Maverick and Qwen 3 Supernova have less aggressive safety filtering and different refusal patterns. A prompt that works perfectly on Claude Opus may produce verbose, unfocused responses on Llama 4 because the instruction-following behavior is calibrated differently. You will spend weeks retuning prompts to achieve equivalent output quality, and some tasks may require architectural changes like adding few-shot examples or splitting single-step prompts into multi-step workflows.

**Eval threshold recalibration** is mandatory. Your eval suite was built with API model behavior as the baseline. Self-hosted models have different output distributions, different error modes, and different failure patterns. If your eval threshold for coherence is 0.88 based on GPT-4 outputs, you may need to lower it to 0.82 for Llama 4 to achieve the same false positive rate, or you may need to raise it to 0.91 because Llama 4 has tighter output variance. You cannot assume eval thresholds transfer. You must re-derive them from the new model's behavior on your validation set.

Infrastructure complexity increases dramatically. API models are managed services. You call an endpoint, you get a response, and someone else handles scaling, availability, and version updates. Self-hosted models require GPU infrastructure, model serving frameworks like vLLM or TensorRT-LLM, load balancing, autoscaling, monitoring, and on-call rotation. You are now responsible for p99 latency, throughput under load, and disaster recovery. If your team has never run ML infrastructure in production, the learning curve is steep and the operational risk is high.

Cost dynamics are non-linear. Self-hosting is cheaper than API at high volume, but only if you reach the volume threshold where fixed infrastructure costs amortize. If you are running 50 million requests per month, self-hosted Llama 4 on eight H100 GPUs costs roughly $18,000 per month in infrastructure plus $12,000 in engineering time, versus $95,000 per month for Claude Opus API at $0.0019 per request. You save $65,000 monthly. If you are running 5 million requests per month, self-hosted costs $18,000 plus $12,000 in eng time, versus $9,500 for API. You lose $20,500 monthly. The break-even point for most teams is between 15 and 30 million requests per month, depending on model size and latency requirements.

## Moving from Self-Hosted to API

The reverse migration, from self-hosted back to API, happens more often than teams admit. The triggers are operational burden, inability to scale, model quality gaps, or executive mandate to reduce infrastructure complexity. A series B startup spent nine months running self-hosted Llama 4 Scout 405B, hit scaling limits at 80 requests per second, could not hire enough ML infra engineers, and migrated back to GPT-5 API in January 2026. A fintech company ran self-hosted Mistral Large 2 for 14 months, discovered that compliance auditors required third-party SOC 2 attestation for the model serving layer, and moved to Anthropic API with a BAA in March 2025 rather than pursue SOC 2 certification for internal infrastructure.

Self-hosted-to-API migration is simpler than API-to-self-hosted because you are moving to a managed service, not taking on operational complexity. The prompt translation problem is the same but reversed. The behavioral difference problem is smaller because API models are generally higher quality than open-weight models at equivalent parameter counts. The eval recalibration problem remains.

The hardest part is **letting go of control**. Teams that invested heavily in self-hosting feel ownership over the infrastructure and resist moving back to API even when the economics and operational reality favor it. The decision to migrate back to API is often delayed by six to twelve months while the team tries increasingly complex workarounds to make self-hosting work. This is sunk cost fallacy, not engineering judgment. If API meets your performance, cost, and compliance requirements, and self-hosting is consuming more engineering time than it saves in inference costs, the migration is justified.

## Multi-Provider as a Migration Enabler

If you already have a multi-provider routing architecture, migration between providers is a configuration change, not a code rewrite. Your application calls a generic LLM interface. The routing layer maps tasks to providers based on policy. To migrate from GPT-5 to Claude Opus for summarization tasks, you update the policy to route summarization to Claude instead of OpenAI, run parallel mode for two weeks, compare eval results, and gradually shift traffic. No application code changes. No prompt refactoring unless behavioral differences require it. No service downtime.

Multi-provider architecture is migration insurance. You pay the upfront cost of building provider abstraction, maintaining eval parity across providers, and running multi-provider eval suites. In return, you get the ability to migrate in weeks instead of months, to negotiate pricing from a position of optionality, and to avoid vendor lock-in that turns into strategic dependency. Teams that skip multi-provider because it seems like over-engineering end up spending ten times the effort on emergency migrations when a provider raises prices by 40 percent or deprecates the model version their entire product depends on.

The **LiteLLM and AI Gateway pattern** is the most common implementation of migration-ready architecture. LiteLLM is an open-source library that provides a unified interface to 100-plus model providers. You write code against the LiteLLM API, and it translates to OpenAI, Anthropic, Google, Cohere, Replicate, self-hosted vLLM, or any other provider. An AI Gateway is a service layer that sits between your application and LiteLLM, adding routing policy, request logging, cost tracking, and failover logic. The combination gives you provider portability, centralized policy control, and migration infrastructure out of the box.

Adopting LiteLLM does not eliminate prompt translation or behavioral tuning, but it reduces the surface area of migration work to the prompts and eval thresholds, not the API client, retry logic, token counting, and error handling. Teams that build custom API clients for each provider spend months refactoring those clients during migration. Teams that use LiteLLM spend days updating routing config and weeks validating behavior.

## Real Migration Timelines and What Goes Wrong

Real-world provider migrations take between four weeks and nine months depending on system complexity, eval suite maturity, and prompt coupling. A well-architected system with provider abstraction, comprehensive evals, and model-agnostic prompts completes migration in four to eight weeks. A tightly coupled system with provider-specific prompts, sparse evals, and no routing layer takes six to nine months. The difference is not model choice. It is architectural readiness.

The most common failure mode is **underestimating behavioral drift**. Teams assume that if two models score similarly on public benchmarks, they will behave equivalently on internal tasks. This is false. GPT-5 and Claude Opus 4.5 have different refusal boundaries, different formatting preferences, different verbosity defaults, and different failure modes on ambiguous queries. What works on one model often fails on the other, even when both models are frontier-class. You discover this during parallel running, not during planning.

The second failure mode is **eval suite inadequacy**. Your eval suite was built to validate one provider's behavior. It passes all the quirks and edge cases of that provider as correct. When you migrate to a new provider, the suite flags legitimate differences as failures, creating noise that obscures real regressions. You end up spending weeks debugging eval failures that are actually eval bugs, not model problems. The fix is to design evals around task success criteria, not model behavior, from the beginning.

The third failure mode is **cost surprise**. The new provider is cheaper per request, but requires more tokens per response, or has higher p95 latency that forces you to add caching, or has a different rate limit structure that requires request batching. Total cost ends up higher than projected, and you discover this after you have committed to the migration and announced it to stakeholders. The fix is to run cost analysis on production traffic during parallel running, not on synthetic benchmarks.

The fourth failure mode is **premature optimization**. Teams try to optimize prompts for the new provider before validating that the new provider meets baseline requirements. They spend weeks retuning prompts to squeeze an extra two percent quality out of Claude Opus, then discover during traffic shift that Claude's p99 latency is unacceptable for their use case and they need to stay on GPT-5. Optimization comes after validation, not before.

## Testing Migration Completeness

Migration is complete when the new provider handles 100 percent of production traffic, the old provider is fully deprecated, and all monitoring, alerting, and documentation reflect the new provider as the production system. Until all three conditions are met, you are in a transitional state that carries operational risk and technical debt.

**Testing completeness** means validating that no traffic is leaking to the old provider, no code paths reference the old provider's API client, no environment variables point to old provider credentials, and no docs describe the old provider as current. You grep the codebase for references to the old provider's SDK. You audit environment configs for old API keys. You check monitoring dashboards for old provider metrics. You review runbooks and incident response docs for outdated provider instructions.

You also validate that rollback capability is intentionally removed, not accidentally broken. After migration is complete, you may choose to keep the old provider as a fallback for six months in case the new provider has an extended outage. This is a deliberate architectural decision, not a lingering vestige of incomplete migration. You document the fallback policy, test the fallback path quarterly, and set a sunset date for full removal.

Migration is a forcing function for architectural discipline. It exposes tight coupling, incomplete abstractions, and operational gaps that are invisible during steady-state operation. Teams that migrate successfully use the migration as an opportunity to improve provider abstraction, strengthen eval coverage, and build routing infrastructure that makes the next migration cheaper. Teams that treat migration as a one-time emergency repeat the same mistakes every two years when the next pricing change or capability shift forces another scramble. The choice is yours, and the consequences are predictable.

Now that you understand how to move between providers and between deployment modes, the next step is deciding which requests are allowed to leave your infrastructure in the first place based on data sensitivity and compliance requirements.
