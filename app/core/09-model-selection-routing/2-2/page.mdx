# 2.2 — Task Taxonomy for Model Selection: Classification, Generation, Extraction, Reasoning, Conversation

In October 2025, a fintech startup built a document processing system that handled customer onboarding workflows. The system performed five distinct tasks: classifying document types, extracting structured data from forms, generating summaries of uploaded financial statements, reasoning through eligibility criteria based on extracted data, and conversing with users to resolve ambiguities. The team used GPT-5.2 for every task because it delivered the best results on their initial tests. After two months in production processing 4,000 onboarding cases per month, they faced a 28,000 dollar model API bill and latency complaints from users who waited 8 to 12 seconds for document classification results. Engineering ran an analysis and discovered that document type classification, which accounted for 40 percent of total requests, worked equally well with GPT-5-nano at 2 percent of the cost and one-fifth the latency. Extraction and reasoning tasks required the frontier model, but classification and basic summarization did not. The team had treated all tasks as equivalent when they were fundamentally different in their model requirements.

Different tasks have fundamentally different characteristics that determine which models are appropriate. A classification task that assigns one of five labels to a text input has different quality dimensions, different latency profiles, and different cost sensitivities than a multi-step reasoning task that evaluates loan eligibility across seven criteria. Using the same model for both is like using the same vehicle for commuting to work and hauling construction materials — technically possible but operationally inefficient. Understanding task taxonomy and how task type maps to model requirements is the foundation for intelligent routing decisions.

## The Five Core Task Types

Tasks involving language models fall into five primary categories: classification, generation, extraction, reasoning, and conversation. Each category has distinct input-output patterns, distinct quality requirements, and distinct failure modes. Recognizing which category a task belongs to tells you immediately what kind of model capacity you need, what quality metrics matter, and where cost and latency optimizations are possible.

**Classification** tasks assign one or more labels to an input. The input is typically text, and the output is a category, a score, or a binary decision. Sentiment analysis assigns positive, negative, or neutral to customer feedback. Intent classification assigns a request type to a support message. Content moderation assigns safe or violating to user-generated content. Spam detection assigns spam or not-spam to an email. Classification tasks are defined by a fixed label set and a decision boundary. The model does not generate new content — it selects from predefined options.

**Generation** tasks produce new content based on an input prompt. The input specifies what to create, and the output is original text that satisfies the specification. Summarization generates a shorter version of a long document. Content drafting generates an email, report, or article from a brief. Translation generates text in a target language from source text. Ad copy generation produces marketing content from product descriptions. Generation tasks are defined by creativity, coherence, and alignment with the prompt. The model is not selecting from a fixed set — it is synthesizing new language.

**Extraction** tasks identify and structure information from unstructured input. The input is raw text or documents, and the output is structured data in a defined schema. Entity extraction pulls names, dates, and amounts from contracts. Form parsing extracts field values from scanned documents. Receipt processing pulls line items, totals, and vendor information from images. Data normalization converts free-text addresses into standardized components. Extraction tasks are defined by completeness, accuracy, and schema conformance. The model is finding and organizing information that already exists in the input.

**Reasoning** tasks perform multi-step logic, analysis, or planning based on input information. The input is a question or problem, and the output is a conclusion supported by intermediate steps. Eligibility evaluation determines whether an applicant meets criteria across multiple dimensions. Diagnostic reasoning identifies the root cause of a system failure from symptoms. Mathematical problem-solving derives an answer through logical steps. Risk assessment evaluates the likelihood and impact of potential outcomes. Reasoning tasks are defined by logical correctness, explanation clarity, and handling of edge cases. The model is thinking through a problem, not just pattern-matching.

**Conversation** tasks maintain multi-turn dialogue with context tracking and persona consistency. The input is a user message and conversation history, and the output is a contextually appropriate response. Customer support chat answers questions and resolves issues across multiple exchanges. Virtual assistant interactions help users complete tasks through back-and-forth dialogue. Interview or assessment bots ask follow-up questions based on previous answers. Tutoring or coaching systems adapt explanations based on user responses. Conversation tasks are defined by context retention, persona maintenance, and natural dialogue flow. The model is participating in an ongoing interaction, not responding to isolated requests.

These five categories are not perfectly distinct — some tasks combine elements from multiple categories. A customer support interaction might involve intent classification, information extraction, and conversational response generation. But most tasks have a primary type that dominates their requirements, and recognizing that primary type is the first step in model selection.

## Classification Tasks and Model Requirements

Classification is the simplest task type in terms of model requirements. The model is choosing from a fixed set of options, not generating open-ended content. This means you need less model capacity than you do for generation or reasoning. Smaller models often perform nearly as well as frontier models on well-defined classification tasks, especially when the label set is small and the decision boundary is clear.

A sentiment analysis task that assigns positive, negative, or neutral to product reviews is a straightforward classification problem. The label set is fixed. The decision boundary is based on tone and word choice. The task does not require deep reasoning or world knowledge. Testing across models in late 2025 showed that GPT-5.2 achieved 94 percent accuracy, GPT-5 achieved 92 percent, and GPT-5-nano achieved 89 percent on a standard review dataset. The quality gap is real but narrow. If your success threshold is 88 percent, all three models qualify. You should choose based on cost and latency, not quality.

Classification tasks benefit from few-shot examples more than other task types. Providing three to five labeled examples in the prompt often closes the quality gap between small and large models because the examples define the decision boundary explicitly. A small model that might struggle with an abstract instruction like "classify sentiment" performs much better when given concrete examples showing what positive, negative, and neutral look like. If you invest in prompt engineering with examples, you can often use a cheaper model without sacrificing quality.

The failure mode for classification is misclassification — assigning the wrong label. Misclassification rates correlate with task ambiguity. If the boundary between positive and neutral sentiment is fuzzy, misclassifications increase. If the intent categories overlap, misclassifications increase. If the label set is large and unbalanced, misclassifications increase. Frontier models handle ambiguity and imbalance better than small models, but the gap is smaller for classification than for generation or reasoning.

Classification latency is low because output size is small. Most classification tasks produce one or a few tokens of output. A binary decision is a single token. A category label is one or two tokens. A confidence score is one token. Low output size means low generation time, which means classification is inherently fast compared to tasks that produce hundreds of tokens. This makes classification a good candidate for latency-sensitive applications even when using moderately-sized models.

Cost for classification is also low because output token usage is minimal. A classification task that consumes 200 input tokens and produces 1 output token costs a fraction of a generation task that consumes 1,000 input tokens and produces 300 output tokens. Even if you use a frontier model for classification, the per-task cost is often under one cent. At high volume, those cents add up, but classification is still the cheapest task type in absolute terms.

The model selection heuristic for classification is to start with a mid-tier or efficient model, measure quality against your threshold, and upgrade only if quality is insufficient. Many teams make the mistake of starting with a frontier model because it is the default recommendation. This wastes budget on tasks where cheaper models work equally well.

## Generation Tasks and Model Requirements

Generation is more demanding than classification because the model must produce coherent, contextually appropriate content rather than selecting a label. Quality varies dramatically across models for generation tasks. Frontier models produce fluent, nuanced, and stylistically consistent text. Small models produce text that is often grammatically correct but awkward, repetitive, or off-tone.

A summarization task that condenses a 2,000-word article into a 150-word summary requires the model to identify key points, synthesize them into coherent sentences, and maintain logical flow. GPT-5.2 produces summaries that read naturally and preserve the most important information. GPT-5 produces summaries that are slightly more mechanical but still coherent. GPT-5-nano produces summaries that miss nuances, repeat phrases, and sometimes misrepresent the source material. The quality gap is much wider than it is for classification.

Generation quality is also domain-sensitive. A model that performs well on general summarization might struggle with technical or domain-specific content. Medical report summaries, legal document summaries, and financial analysis summaries all require specialized vocabulary and context that small models often lack. Frontier models, trained on larger and more diverse datasets, handle domain shifts better. If your generation task involves specialized domains, you need more model capacity.

The failure mode for generation is poor quality output that requires human editing or is unusable. Unlike classification, where a wrong label is a discrete error, generation failures are often partial — the output is not completely wrong, but it is not good enough to use without revision. This makes generation failures harder to detect programmatically. You cannot simply check whether the output matches a known label. You need semantic evaluation, style assessment, or factual verification, all of which are more complex than label matching.

Generation latency is higher than classification latency because output size is larger. A 150-word summary is roughly 200 tokens. A 500-word email draft is roughly 700 tokens. Output generation time scales linearly with output length, and larger models generate tokens more slowly than smaller models. If your generation task produces long outputs, latency can become the bottleneck even if cost and quality are acceptable.

Cost for generation is higher than classification because of output token usage. A summarization task that consumes 3,000 input tokens and produces 200 output tokens costs roughly the same as a classification task that consumes 10,000 input tokens and produces 2 output tokens, because output tokens are 3 to 5 times more expensive than input tokens. Generation tasks quickly dominate your token budget when run at volume.

The model selection heuristic for generation is to test mid-tier models first, evaluate output quality with human review or automated metrics, and upgrade to frontier models only for tasks where quality gaps are unacceptable. Do not assume you need the most expensive model. Many generation tasks work well with mid-tier models, especially when the domain is general and the output length is moderate.

## Extraction Tasks and Model Requirements

Extraction sits between classification and generation in complexity. The model is not generating open-ended content, but it is also not selecting from a fixed label set. It is identifying pieces of information in the input and outputting them in a structured format. Extraction quality depends on completeness, accuracy, and schema conformance.

A form parsing task that extracts name, address, date of birth, and account number from a scanned document requires the model to locate each field, interpret variations in formatting, and output values in a consistent structure. GPT-5.2 handles formatting variations well, correctly interprets ambiguous fields, and produces well-formed structured output. GPT-5 performs similarly on clean inputs but struggles more with poor scans or unusual layouts. GPT-5-nano misses fields, misinterprets values, and produces malformed output more frequently.

Extraction tasks benefit enormously from structured output modes where the model is constrained to produce JSON or another schema-defined format. Structured output reduces the rate of malformed responses and makes validation straightforward. As of January 2026, most major providers support structured output for their mid-tier and frontier models but not always for their smallest models. If structured output is critical, this may disqualify the cheapest models even if their extraction accuracy is acceptable.

The failure mode for extraction is incomplete or incorrect data. A missed field means downstream processes fail. An incorrect value means decisions are made on bad data. Extraction errors are often silent — the output looks valid because it conforms to the schema, but the values are wrong. This makes extraction errors more dangerous than generation errors, which are usually obvious to a human reviewer.

Extraction latency depends on input size and output size. Extracting five fields from a 500-word document is fast. Extracting 40 fields from a 10,000-word contract is slower. Output size is moderate — larger than classification but smaller than most generation tasks. Latency is rarely the limiting factor for extraction unless input documents are extremely large.

Cost for extraction is moderate. Input token usage is often high because you are sending entire documents, but output token usage is moderate because you are emitting structured data, not prose. A typical extraction task might consume 2,000 input tokens and produce 50 output tokens. Cost is higher than classification but lower than generation.

The model selection heuristic for extraction is to prioritize models that support structured output and test them on real documents with realistic formatting variation. Do not rely solely on clean test data. Extraction quality degrades sharply when inputs are messy, and small models degrade faster than large models.

## Reasoning Tasks and Model Requirements

Reasoning is the most demanding task type. The model must perform multi-step logic, consider multiple factors, and arrive at a conclusion that is logically sound. Reasoning tasks are where the gap between frontier and small models is widest. Small models struggle with multi-step reasoning, often skipping steps, making logical errors, or producing conclusions that do not follow from the premises.

An eligibility evaluation task that determines whether a loan applicant qualifies based on income, credit score, employment history, debt-to-income ratio, and collateral value requires the model to check each criterion, apply thresholds, and combine results into a final decision with explanation. GPT-5.2 performs this task reliably, produces clear explanations, and handles edge cases like missing data or borderline values. GPT-5 performs similarly on straightforward cases but makes more errors on edge cases. GPT-5-nano frequently makes logical errors, produces incomplete explanations, and fails on cases that require interpreting ambiguous or conflicting information.

Reasoning tasks are also sensitive to prompt structure. Chain-of-thought prompting, where you instruct the model to show intermediate steps, improves reasoning quality significantly. Frontier models benefit from chain-of-thought prompting but can often reason correctly without it. Small models require explicit prompting to produce multi-step reasoning and even then often fail. If your task requires reasoning, you cannot compensate for a weak model with better prompting.

The failure mode for reasoning is incorrect conclusions that appear plausible. Unlike generation failures, which are usually obvious because the text is awkward, reasoning failures are often subtle. The output reads well, the explanation sounds reasonable, but the logic is flawed. Detecting reasoning errors requires domain expertise or verification against ground truth, both of which are expensive.

Reasoning latency is high because reasoning tasks often produce long outputs. An explanation of a multi-step decision might be 300 to 500 tokens. If you use chain-of-thought prompting, intermediate steps add even more tokens. Reasoning tasks are slow compared to classification and extraction.

Cost for reasoning is high because both input and output token usage are high. You often need to provide substantial context — documents, data, criteria — which drives up input tokens. The output includes reasoning steps and explanations, which drives up output tokens. Reasoning tasks are among the most expensive per-task costs.

The model selection heuristic for reasoning is to use frontier models unless cost is absolutely prohibitive. The quality gap is too wide to ignore. If cost is a hard constraint, consider splitting the reasoning task into simpler subtasks and using a mix of models: small models for straightforward steps and a frontier model for the final integration and decision.

## Conversation Tasks and Model Requirements

Conversation is a hybrid task type that combines elements of classification, generation, and reasoning across multiple turns. The model must interpret user intent, maintain context, generate contextually appropriate responses, and preserve a consistent persona. Conversation quality depends on all of these dimensions simultaneously.

A customer support chat that helps users troubleshoot account issues requires the model to classify the issue type, ask clarifying questions, retrieve or reason through solutions, generate responses in a helpful tone, and remember what the user said in previous turns. Frontier models handle this well. They maintain context across 10 to 20 turns, adapt responses based on conversation flow, and stay in character. Mid-tier models struggle with context retention beyond 5 to 8 turns and sometimes lose track of what was already discussed. Small models fail at conversation tasks except for the simplest single-turn question-answer scenarios.

Conversation latency is critical because users are waiting in real time for each response. A 6-second delay in a chat interface feels unacceptably slow. A 2-second delay is tolerable. A sub-1-second response feels fast. Conversation tasks are latency-bound by default, which often disqualifies the slowest frontier models unless you can parallelize or precompute parts of the response.

Conversation cost is high because context accumulates across turns. Each turn includes the full conversation history as input. A 10-turn conversation where each turn adds 100 tokens of history consumes 1,000 tokens just for context by the final turn. If the user and assistant each contribute 100 tokens per turn, total token usage is roughly 2,000 input tokens and 1,000 output tokens over the session. Multiply by thousands of conversations and costs add up quickly.

The failure mode for conversation is loss of context, inconsistent persona, or repetitive responses. Users notice immediately when the assistant forgets something they said two turns ago or contradicts itself. Conversation failures damage trust and make the system feel unreliable.

The model selection heuristic for conversation is to use a mid-tier or frontier model that balances quality and latency. Small models are rarely acceptable unless the conversation is single-turn or highly scripted. If cost is a concern, use conversation summarization or context pruning to reduce input token usage rather than downgrading the model.

## Task Type as the Foundation for Routing Decisions

Once you categorize each task in your system by type, you can map task types to model tiers. Classification tasks often work with efficient or mid-tier models. Generation tasks need mid-tier or frontier models depending on quality requirements. Extraction tasks need mid-tier models with structured output support. Reasoning tasks need frontier models. Conversation tasks need mid-tier or frontier models with low latency.

This mapping is not rigid. Some classification tasks are hard enough to need frontier models. Some generation tasks are simple enough to work with mid-tier models. The point is that task type gives you a starting hypothesis for model selection, and you refine that hypothesis through testing.

A multi-task system built by an HR technology company in late 2025 used this taxonomy to optimize model costs. They categorized their workload: resume screening was classification, candidate outreach emails were generation, skills extraction from resumes was extraction, interview question generation was reasoning, and chatbot interactions were conversation. They tested models for each category. Resume screening worked well with GPT-5-nano. Candidate emails worked well with GPT-5. Skills extraction required GPT-5 with structured output. Interview question generation required GPT-5.2. Chatbot interactions used GPT-5 with context pruning. Total cost was 60 percent lower than using GPT-5.2 for all tasks, with no measurable quality degradation.

Task taxonomy prevents the mistake of treating all tasks identically. It forces you to recognize that a classification task and a reasoning task have fundamentally different requirements and should not be served by the same model just because it simplifies infrastructure.

## Why a Single Model Across All Task Types Is Suboptimal

The single-model approach is operationally simple but economically wasteful and often technically insufficient. Using a frontier model for classification tasks wastes money on capacity you do not need. Using a small model for reasoning tasks produces unacceptable quality. The optimal solution is almost always a portfolio of models matched to task types.

The resistance to multi-model architectures comes from perceived complexity. Managing five models instead of one requires routing logic, fallback handling, separate monitoring, and separate optimization. This is real complexity, but it is not prohibitive. Routing logic is a few hundred lines of code. Monitoring is a matter of tagging requests by model. Optimization is the same process repeated per model. The operational overhead is modest compared to the cost savings and quality improvements.

A content platform built by a media company in mid-2025 initially used GPT-5.2 for all tasks. They processed 800,000 requests per month at a cost of 52,000 dollars. They re-architected to use GPT-5-nano for classification, GPT-5 for generation, and GPT-5.2 only for reasoning tasks that required it. Volume distribution was 60 percent classification, 30 percent generation, and 10 percent reasoning. Post-optimization cost was 14,000 dollars per month with no measurable change in user-facing quality. The 73 percent cost reduction funded two additional engineers and still left budget surplus.

The single-model approach persists because teams optimize for implementation speed in early development and then never revisit the decision. The first model you test becomes the default. You ship it, it works, and no one questions whether a different model would work better or cheaper. Breaking this inertia requires deliberately scheduling model optimization as a post-launch task and treating it with the same priority as performance optimization or security hardening.

## Task Taxonomy as a Communication Framework

Task taxonomy is not just a technical tool — it is a communication framework that aligns product and engineering on model selection. When product asks why a feature is expensive, you can explain that it is a reasoning task that requires a frontier model. When engineering proposes using a cheaper model, you can explain that the task is conversation, not classification, and context retention matters. The taxonomy gives both sides a shared vocabulary for discussing tradeoffs.

It also clarifies when quality complaints are actionable. If a user complains that a classification is wrong, you can evaluate whether the model is underperforming for the task type or whether the task itself is ambiguous and needs better definition. If a user complains that a generated email sounds robotic, you can evaluate whether the model is insufficient for generation or whether the prompt needs refinement. The taxonomy helps you diagnose whether the problem is the model, the task definition, or the user expectation.

---

Model selection is not a one-size-fits-all decision. Different task types have fundamentally different requirements. Classification, generation, extraction, reasoning, and conversation each demand different model capabilities, have different cost profiles, and fail in different ways. Mapping your tasks to these types is the foundation for building a model portfolio that optimizes cost, quality, and latency across your system. Next, we turn to the mechanics of routing: how to direct each request to the appropriate model based on task type and runtime conditions.
