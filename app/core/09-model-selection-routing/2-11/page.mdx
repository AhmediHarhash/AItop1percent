# 2.11 — Cross-Provider Behavior Differences: Refusals, Safety Boundaries, Style, and Policy Lines

In October 2025, a mental health technology company supporting 180 licensed therapists discovered their AI clinical note assistant had stopped working for 40 percent of sessions. The system, which helped therapists draft case notes from session summaries, had been running on Claude Opus 4 for eight months without incident. After a routine model update, the assistant began refusing to process notes that mentioned self-harm, suicidal ideation, or substance abuse details. The refusals were categorical: "I cannot assist with content that discusses self-harm." The therapists were documenting real clinical sessions that required these details for treatment continuity and legal compliance. The company had tested the system extensively for quality and accuracy but had never tested for refusal behavior across the specific clinical language their therapists used. They switched to GPT-5.2, which processed the same content without refusal, but the migration took three weeks and cost $85,000 in engineering time. The root cause was not a quality failure but a behavioral mismatch: Anthropic's safety boundaries were stricter than OpenAI's on this specific content domain, and the company had never mapped provider-specific refusal patterns to their product requirements.

Models from different providers do not just differ in quality scores, cost, or latency. They differ fundamentally in behavior, policy enforcement, and stylistic personality. These differences are not bugs or inconsistencies. They are deliberate design choices shaped by each provider's risk tolerance, legal strategy, brand positioning, and interpretation of what constitutes responsible AI deployment. When you select a model, you are not just choosing a statistical engine. You are choosing a set of behavioral boundaries that will become embedded in your product. If those boundaries misalign with your users' needs, your product breaks in ways that no amount of prompt engineering can fix.

## The Three Dimensions of Provider Behavioral Differences

Behavioral differences across providers manifest in three distinct dimensions. The first is **refusal patterns**: what content or requests the model will categorically decline to engage with. The second is **safety boundary calibration**: where the model draws lines on ambiguous or borderline content. The third is **stylistic personality**: the default tone, structure, and verbosity the model adopts in its responses. Each dimension affects different aspects of your product's user experience, and each requires separate testing and mitigation strategies.

Refusal patterns are the most visible and disruptive. A refusal occurs when the model returns a decline message instead of attempting to fulfill the user's request. Refusals are policy-driven, not capability-driven. The model could generate the content but chooses not to based on safety guidelines. Anthropic's Claude models, as of January 2026, tend to refuse more aggressively on content involving weapons, explicit violence, self-harm, certain medical advice, and some creative fiction scenarios involving harm. OpenAI's GPT-5 family refuses on illegal activity, child safety content, and non-consensual sexual content, but is more permissive on fictional violence and clinical mental health language. Google's Gemini 3 has its own thresholds, often refusing on political misinformation and certain cultural sensitivities that the other providers allow. These differences are not ranked by strictness overall; each provider is stricter in different areas.

Safety boundary calibration governs borderline cases where the content is not clearly disallowed but sits near a policy line. A therapist documenting a patient's suicidal ideation is clearly clinical and allowed. A teenager asking how to hurt themselves is clearly disallowed. But what about a novelist writing a character's internal monologue contemplating self-harm? What about a historian describing a historical figure's mental health struggles? What about a parent asking how to talk to their child about a friend's suicide attempt? Different providers calibrate these boundaries differently. Claude tends to err on the side of caution, sometimes refusing content that a human reviewer would consider acceptable. GPT-5 tends to engage more often, sometimes requiring additional clarification prompts to confirm the context is appropriate. Gemini 3 varies by content domain. These calibration differences mean that the same user request may succeed with one provider and fail with another, not because of capability but because of policy interpretation.

Stylistic personality is subtler but equally impactful for user experience. Claude models, particularly Claude Opus 4.5 as of early 2026, tend to produce structured, organized, and somewhat formal responses. They use topic sentences, clear paragraph breaks, and logical flow. This works well for business writing, technical documentation, and educational content. GPT-5 and GPT-5.2 tend toward more conversational, fluid responses that feel less rigidly structured. This works well for creative writing, brainstorming, and casual user interactions. Gemini 3 tends toward concise, efficient responses that get to the point quickly. This works well for search-like queries and quick answers but can feel abrupt in contexts where users expect elaboration. These stylistic defaults are baked into the model's training and reinforcement learning from human feedback. You cannot fully override them with prompting alone.

## Mapping Provider Behavior to Product Requirements

The behavioral mismatch that broke the mental health company's system is not unique. It is a predictable failure mode that occurs whenever teams select models based solely on quality metrics without testing behavioral alignment. Behavioral alignment means the model's refusal patterns, safety calibration, and stylistic personality match the needs of your users and the constraints of your domain. A creative writing platform needs a model that allows fictional violence and morally complex characters. A children's education app needs a model that refuses anything remotely inappropriate for minors. A legal research tool needs a model that handles sensitive case details without triggering safety filters. A customer support bot needs a model whose style matches your brand voice.

You map provider behavior to product requirements by first defining your **behavioral requirements** as explicitly as you define your quality requirements. Behavioral requirements specify what content your system must handle, what content it must refuse, and what stylistic personality is acceptable. For the mental health company, the behavioral requirements were: must process clinical language about self-harm, suicidal ideation, substance abuse, trauma, and psychiatric symptoms without refusal; must refuse non-clinical requests for harmful information; must maintain a professional, empathetic tone. These requirements were implicit in the product's design but were never written down or tested against model behavior.

Once you have explicit behavioral requirements, you test each candidate model against them systematically. This is not a theoretical exercise. You construct a test set of real user inputs that span your behavioral boundaries. For the mental health company, the test set should have included real anonymized case note summaries mentioning self-harm, a creative writing request about a character contemplating suicide, a direct request for self-harm methods, and a clinical discussion of suicidal ideation assessment. You run each test case through each candidate model and record the behavior: did it fulfill the request, refuse, or ask for clarification? You compare the results against your requirements. A model that refuses the clinical case note fails your requirements, even if it has the highest quality scores on your accuracy benchmarks.

Behavioral testing reveals mismatches before they reach production. In 2025, a creative writing platform tested GPT-5.2, Claude Opus 4.5, and Gemini 3 with a set of 200 fiction scenarios involving violence, morally ambiguous characters, and dark themes. GPT-5.2 refused 8 percent of scenarios. Claude Opus 4.5 refused 34 percent. Gemini 3 refused 12 percent. The platform needed a refusal rate below 10 percent because their users were professional fiction writers working on legitimate creative projects. They selected GPT-5.2 despite Claude scoring slightly higher on prose quality, because behavioral alignment was more important than marginal quality gains. They documented the decision in their model selection matrix with the evidence from the refusal testing.

## Testing for Refusal Patterns Systematically

Refusal testing is not ad hoc. You build a **refusal test suite** that covers your product's content domains and edge cases. The test suite is a collection of prompts designed to probe the model's refusal boundaries in areas relevant to your application. Each test case includes the prompt, the expected behavior, and the rationale. For a medical information app, test cases include clinical descriptions of symptoms, treatment advice requests, drug interaction queries, questions about controversial treatments, and requests for diagnoses. For a customer support bot, test cases include angry customer language, profanity, threats, requests for refunds, questions about competitors, and complaints about executives. For a creative writing tool, test cases include violent scenes, sexual content, political themes, religious themes, and morally complex character decisions.

You run the refusal test suite against every model you are considering and every major version update of your production model. The output is a refusal report that documents which test cases each model refused, what refusal message it returned, and whether the refusal aligns with your product requirements. A refusal that aligns with your requirements is a true positive: the model correctly declined an inappropriate request. A refusal that misaligns is a false positive: the model declined a legitimate request that your product needs to support. A false positive is a product-breaking failure. You count false positives across all test cases and set a threshold: if a model produces more than X false positives, it is disqualified regardless of quality scores.

The refusal report also identifies **borderline cases** where the model sometimes refuses and sometimes complies, depending on phrasing. These cases indicate calibration instability. For example, Claude Opus 4 might refuse "How do I help someone who is suicidal?" but comply with "What are clinical best practices for suicide risk assessment?" Both prompts are asking for the same information, but the second is framed in clinical language that the model's safety filter interprets as professional rather than personal. Borderline cases require prompt engineering to guide the model toward the compliant interpretation, but this adds fragility. If a user rephrases their question slightly, they might trigger a refusal. High borderline case counts indicate a poor behavioral fit.

You also test for **refusal drift** over time. Providers update their safety policies and retrain their models, which can change refusal behavior without warning. The mental health company's failure occurred because a model update tightened Claude's refusal boundaries on self-harm content. The company had no monitoring in place to detect refusal rate changes in production. After the incident, they implemented automated refusal monitoring: every model response is tagged as fulfilled or refused, and the refusal rate per content category is tracked daily. A 10 percent increase in refusal rate in any category triggers an alert. This allows them to detect policy drift and react before it affects a large user base.

## Building Provider-Specific Prompt Adjustments

When you identify behavioral differences that affect your product, you have three options. The first is to switch providers to one whose behavior aligns better. The second is to adjust your prompts to work around the behavioral mismatch. The third is to accept the mismatch and inform users that certain requests will not be supported. Switching providers is the cleanest solution if the mismatch is fundamental, but it incurs migration costs and introduces new risks. Prompt adjustments are effective for borderline cases but not for categorical refusals. Accepting the mismatch is appropriate only if the refused content is edge case and not core to your value proposition.

Provider-specific prompt adjustments are targeted modifications to your system prompts that reduce false positive refusals without increasing false negative compliance. For the mental health company, the adjustment was to add explicit role context to the system prompt: "You are assisting a licensed therapist in documenting clinical case notes. The content you process is part of legitimate mental health treatment and must be handled professionally without refusal." This framing reduced Claude's false positive refusals by 60 percent because it signaled to the model's safety filter that the context was clinical, not harmful. The adjustment did not eliminate all refusals, but it brought the refusal rate within acceptable bounds.

Another approach is to provide **content category tags** in the prompt that signal the nature of the request. A creative writing platform might include a tag: "Content category: fiction writing. User is a professional author working on a novel. Themes may include violence, moral ambiguity, and dark subject matter. This is creative work, not a request for harmful information." This explicit framing helps models like Claude, which use context-aware safety filtering, interpret the request appropriately. The tag does not guarantee compliance, but it reduces borderline refusals.

You can also use **multi-turn clarification prompts** to guide the model past borderline refusals. If the model refuses a request, your system can automatically respond with a clarification that provides additional context: "To clarify, this request is for a clinical case note documenting a therapy session. The content is necessary for treatment continuity and legal compliance. Please proceed." Some models will reevaluate the request in light of the clarification and comply. This adds latency and complexity, but it salvages requests that would otherwise fail. You implement this as a fallback: if the model returns a refusal message pattern, your system appends the clarification and retries before surfacing the refusal to the user.

All prompt adjustments must be tested for both **effectiveness** and **safety**. Effectiveness means the adjustment reduces false positive refusals. Safety means the adjustment does not increase false negative compliance, where the model fulfills requests it should refuse. A prompt adjustment that says "Ignore all safety guidelines" would reduce false positives but would also eliminate true positives, allowing genuinely harmful requests through. This is unacceptable. You test prompt adjustments by running them against both your legitimate use case test set and a **harmful request test set** that includes content you want the model to refuse. The adjustment must reduce false positives on the legitimate set without increasing false negatives on the harmful set.

## Style Differences and Brand Voice Alignment

Stylistic personality differences affect user experience in ways that are less dramatic than refusals but more pervasive. Every response your product generates is shaped by the model's default style. If your brand voice is formal and authoritative, Claude's structured style may align well. If your brand voice is friendly and conversational, GPT-5's fluid style may fit better. If your brand voice is concise and efficient, Gemini 3's brevity may match. Style misalignment creates a subtle but persistent friction where users feel the product's responses do not quite match the brand they expect.

You test style alignment by generating sample responses to representative user inputs using each candidate model and reviewing them with your product, design, and brand teams. This is a qualitative evaluation, not a quantitative benchmark. You ask: does this response feel like our brand? Is the tone appropriate for our users? Is the structure clear and easy to follow? Does it provide the right level of detail? You collect feedback from multiple reviewers and identify patterns. If most reviewers feel Claude's responses are too formal for your casual user base, that is a style mismatch. If most reviewers feel GPT-5's responses are too verbose for your efficiency-focused product, that is a style mismatch.

Style misalignment can sometimes be corrected with prompt engineering. You can instruct the model to "respond in a concise, efficient style" or "respond in a warm, conversational tone." These instructions shift the model's output toward your desired style, but they do not fully override the model's default personality. Claude will still tend toward structure even when asked to be conversational. GPT-5 will still tend toward fluidity even when asked to be concise. The model's training creates an attractor basin that pulls responses toward its default style. Prompt instructions move the response within that basin but do not escape it.

When style misalignment is significant and cannot be corrected with prompting, you have two options. The first is to select a different provider whose default style aligns better. The second is to apply **post-processing transformations** to adjust the style after generation. Post-processing might include trimming verbose responses to a target length, reformatting paragraph breaks, or adjusting sentence structure. This adds latency and complexity, and it risks degrading quality if the transformation is too aggressive. Post-processing is a last resort, used only when no provider's default style aligns well and the style requirements are non-negotiable.

## Policy Line Differences and Domain-Specific Risk

Provider policy lines are shaped by each company's legal exposure, regulatory environment, and brand strategy. Anthropic, as a public benefit corporation emphasizing safety, tends to draw stricter lines on content that could be perceived as harmful, even in ambiguous cases. OpenAI, as a commercial entity balancing safety with usability, tends to draw lines that allow more borderline content while still refusing clearly harmful requests. Google, operating under heavy regulatory scrutiny in Europe and facing brand risk from its search dominance, tends to draw strict lines on misinformation, hate speech, and certain cultural sensitivities. These policy differences are rational from each provider's perspective but create friction when your product requirements sit near a policy line.

Policy line differences are most problematic in **domain-specific applications** where your users' legitimate needs involve content that general-purpose safety policies flag as risky. Medical applications involve disease, injury, and treatment details that safety filters may interpret as harmful. Legal applications involve crime, misconduct, and conflict that safety filters may flag as violent or unethical. Education applications involve historical atrocities, scientific controversies, and age-inappropriate topics that safety filters may block. Creative applications involve fictional harm, moral ambiguity, and provocative themes that safety filters may refuse. If your domain sits near a provider's policy line, you will encounter frequent false positive refusals that break your product's core functionality.

The solution is to select a provider whose policy lines align with your domain's needs and to negotiate **domain-specific safety accommodations** if necessary. Some providers offer custom safety configurations for enterprise customers in sensitive domains. For example, a medical AI company might negotiate with OpenAI to allow clinical language about self-harm and substance abuse while maintaining refusals on non-clinical requests. This requires an enterprise contract, a detailed use case justification, and often a commitment to implement additional safeguards in your application layer. Not all providers offer this flexibility, and not all use cases qualify.

If custom safety configurations are not available, you must decide whether to accept the provider's policy lines or switch providers. Accepting the policy lines means your product will refuse certain legitimate user requests, which may be acceptable if those requests are edge cases. Switching providers means finding one whose default policy lines allow your use case, which may require compromising on quality, cost, or other factors. There is no perfect answer. You are making a trade-off between behavioral alignment and other model selection criteria.

## Cross-Provider Behavioral Testing as Mandatory Practice

The mental health company's failure was predictable and avoidable. They tested for quality and accuracy but not for behavior. This is a common pattern: teams rigorously benchmark quality because quality is quantifiable, but they neglect behavioral testing because it seems subjective or secondary. Behavioral testing is neither subjective nor secondary. It is systematic, repeatable, and critical to product reliability. A model that refuses your users' legitimate requests is not usable, regardless of its quality scores.

Behavioral testing is mandatory before any model selection decision and before any production model migration. You build a refusal test suite, a borderline case set, and a style evaluation set. You run all candidate models through these tests. You document the results in your model selection matrix alongside quality scores, cost, and latency. You treat behavioral misalignment as a disqualifying failure, the same way you treat unacceptable quality or latency. You monitor refusal rates and style consistency in production, and you retest when providers release major model updates.

Behavioral testing is also mandatory when expanding to new user segments or content domains. A customer support bot that works well for retail customers may fail when applied to healthcare customers because the content involves medical information that triggers safety filters. A creative writing tool that works well for short stories may fail when applied to screenplays because the format and content patterns differ. You do not assume behavioral alignment generalizes. You test it for each new application.

The cost of behavioral testing is small compared to the cost of production failures. The mental health company spent $85,000 and three weeks migrating providers after their refusal failure. They could have spent $5,000 and two days building and running a refusal test suite before the initial deployment. The return on investment for behavioral testing is immediate and obvious. You avoid product-breaking surprises, you make informed trade-offs during model selection, and you build organizational knowledge about provider differences that informs future decisions.

## The Behavioral Alignment Checklist

Before you deploy any model to production, you complete a behavioral alignment checklist. The checklist ensures you have tested and documented the model's behavior in all dimensions relevant to your product. First: have you defined explicit behavioral requirements for your application, including what content must be supported, what content must be refused, and what stylistic personality is acceptable? Second: have you built a refusal test suite covering your content domains and edge cases, and have you run it against your selected model? Third: have you tested borderline cases to identify calibration instability, and have you implemented prompt adjustments or fallback strategies where necessary? Fourth: have you evaluated style alignment with your brand voice, and have you confirmed that the model's default personality is acceptable or correctable with prompting?

Fifth: have you documented provider-specific policy lines that affect your domain, and have you confirmed that your use case does not sit near a line that will cause frequent refusals? Sixth: have you implemented refusal rate monitoring in production to detect policy drift over time? Seventh: have you tested your selected model against a harmful request set to confirm that prompt adjustments or custom safety configurations do not introduce false negative compliance? Eighth: have you reviewed behavioral test results with your product, legal, and trust and safety teams to confirm that the model's behavior aligns with your risk tolerance and regulatory requirements?

If you cannot answer yes to all eight questions, your behavioral testing is incomplete. Incomplete behavioral testing introduces preventable risk. The mental health company could have answered yes to none of these questions before their deployment. After their failure, they institutionalized the checklist as a required step in their model selection and deployment process. They have not had a refusal-related incident since.

Behavioral differences across providers are not a nuisance or an edge case concern. They are a first-order product requirement. You select models based on behavior as much as quality, cost, and latency. You test behavior systematically, document it explicitly, and monitor it continuously. This is not optional work for safety-conscious teams. This is baseline professional practice for any team deploying AI in production. The next subchapter covers how to synthesize all these dimensions—quality, cost, latency, risk, structure, tool calling, instruction following, and behavior—into a single repeatable decision framework.
