# 3.7 â€” Semantic Routing: Embedding-Based Query Classification for Model Dispatch

In mid-2025, a legal research platform serving 1,200 law firms deployed a rule-based routing system to classify incoming queries and send them to either GPT-5.1 for complex legal analysis or GPT-5-mini for simple lookups. The rules were straightforward: queries containing words like "analyze," "interpret," or "precedent" went to the expensive model, while queries with "define," "what is," or "list" went to the cheap model. Within three weeks, attorneys began complaining that straightforward questions were being routed to the slow, expensive model while genuinely complex questions were being handled by the fast model with insufficient reasoning depth. The engineering team reviewed the logs and found that the rule-based system misclassified 34% of queries because legal language is dense with synonyms, nested clauses, and domain-specific phrasing that keyword rules cannot capture. A query asking "What does the doctrine of res judicata mean in the context of federal securities litigation" should have been simple lookup, but the word "context" triggered the complex-analysis rule. A query asking "Give me the holding from Smith v. Jones" looked simple but required deep case law reasoning the mini model could not provide. The root cause was not poor rule design but the fundamental inadequacy of keyword-based routing for semantic tasks. The team replaced the rule system with semantic routing using embedding-based query classification and reduced misrouting to 8% within two weeks.

Semantic routing solves the problem that keyword and regex rules cannot solve: understanding what a query is actually asking for, not just which words it contains. You embed the incoming query into vector space and compare it against pre-computed prototype embeddings that represent each routing category. The nearest prototype determines the route. This approach captures semantic similarity rather than surface-level lexical overlap, so queries that mean the same thing but use different words still route correctly.

## How Semantic Routing Works

You start by defining your routing categories based on the task types and model capabilities in your system. For the legal research platform, the categories were simple factual lookup, multi-step reasoning, case law synthesis, and open-ended legal analysis. For each category, you collect a set of representative queries that typify that category. These are your prototype queries. You then embed all the prototype queries using a fast, high-quality embedding model and compute a centroid for each category by averaging the embeddings of all prototypes in that category. At runtime, when a new query arrives, you embed it using the same embedding model and compute cosine similarity between the query embedding and each category centroid. The category with the highest similarity score wins, and the query is routed to the model designated for that category.

The quality of your routing depends entirely on three factors: the discriminability of your categories in embedding space, the representativeness of your prototype queries, and the quality of your embedding model. If your categories overlap semantically, the centroids will be close together in vector space and routing accuracy will degrade. If your prototype queries do not cover the full range of variation within each category, novel phrasings will be misclassified. If your embedding model does not capture the nuances of your domain, semantically different queries will cluster together incorrectly.

The legal research platform initially used OpenAI's text-embedding-3-small model, which is fast and cheap but trained on general web text. It confused legal jargon with layperson questions because the embedding space did not distinguish between "What is res judicata" from a law student and "What is res judicata" from an attorney drafting a motion to dismiss. The team fine-tuned a smaller embedding model on 50,000 labeled legal queries and improved routing accuracy from 89% to 96% because the fine-tuned embeddings separated domain-specific semantic distinctions that the general model collapsed.

## Building and Maintaining Category Centroids

The first step is collecting prototype queries for each category. You cannot hand-write these from scratch because you will miss the diversity of real user language. Instead, you sample historical queries, label them manually or with a classifier, and select representative examples from each category. For a category like simple factual lookup, you might collect 200 queries that range from single-word definitions to short factual questions. For complex reasoning, you collect 200 queries that require multi-step inference, cross-document synthesis, or nuanced judgment. The key is ensuring that your prototypes span the semantic range of the category, not just the most common phrasing.

Once you have your prototypes, you embed them and compute the centroid. The centroid is the mean of all prototype embeddings in the category, and it represents the semantic center of that category in vector space. You store the centroids in memory or a fast key-value store because you need to retrieve them on every incoming query. The legal research platform stored centroids in Redis and retrieved all four centroids in under 2 milliseconds. You do not need a vector database for this use case because you are not doing nearest-neighbor search over millions of embeddings, you are computing cosine similarity against a handful of centroids.

The hard problem is category overlap. If your categories are semantically close, their centroids will be close together, and queries near the boundary will misroute. A financial services company building a customer support router defined categories for account balance inquiries, transaction disputes, and fraud reports. The centroids for transaction disputes and fraud reports were nearly identical in embedding space because both involved unauthorized charges and account problems. The router misclassified 22% of fraud reports as transaction disputes, sending them to a model that could not escalate to the fraud team. The solution was collapsing the two categories into a single high-urgency financial issue category and routing all queries in that cluster to the most capable model. This reduced granularity but improved routing accuracy and downstream outcomes.

You must also maintain your centroids over time because language drift and new query patterns will shift the semantic distribution of each category. The legal research platform re-computed centroids every quarter by sampling recent queries, re-labeling them, and re-embedding. They found that new legal terminology and evolving case law phrasing caused centroid drift of up to 8% per quarter, which degraded routing accuracy by 3% if not corrected. Automated monitoring flagged when the distance between incoming query embeddings and the nearest centroid exceeded a threshold, indicating that the centroids were stale or that a new category was emerging.

## Embedding Model Choice and Latency Constraints

Semantic routing is only viable if the embedding step is fast enough to fit within your latency budget. A customer-facing chatbot with a 500-millisecond end-to-end latency budget cannot spend 200 milliseconds on embedding. You need an embedding model that runs in under 50 milliseconds at the 95th percentile. OpenAI's text-embedding-3-small model runs in 30-60 milliseconds for queries under 512 tokens, making it suitable for real-time routing. Larger embedding models like text-embedding-3-large or domain-specific models fine-tuned on large architectures may take 150-300 milliseconds, which is too slow for synchronous routing.

The tradeoff is between embedding quality and latency. Smaller embedding models are faster but less discriminative, leading to higher misrouting rates. Larger embedding models are more accurate but too slow for real-time use. The legal research platform experimented with a 768-dimensional fine-tuned embedding model that took 180 milliseconds per query and achieved 97% routing accuracy, versus a 384-dimensional model that took 40 milliseconds and achieved 94% accuracy. They chose the faster model because the 3% improvement in routing accuracy did not justify the 140-millisecond latency penalty, which degraded user experience and increased tail latency for the entire request pipeline.

If your latency budget is tight, you can cache embeddings for common queries. A SaaS company serving 50,000 users found that 40% of queries were exact duplicates or near-duplicates of previous queries within the same session. They cached embeddings for the top 10,000 most common queries and reduced embedding latency by 60% for cached hits. The cache was stored in Redis with a 24-hour TTL, and the hit rate stabilized at 42% after two weeks.

You can also batch embedding requests if your routing system processes queries asynchronously or in micro-batches. A document processing pipeline that routed 10,000 documents per hour to different models batched embedding requests in groups of 50 and reduced per-document embedding latency from 45 milliseconds to 8 milliseconds by amortizing the overhead of the HTTP request and model invocation. Batching is not viable for synchronous user-facing systems but works well for background processing, scheduled tasks, and bulk operations.

## When Semantic Routing Outperforms Classifiers

Semantic routing works best when your categories are semantically distinct and your query distribution is stable. It outperforms rule-based routing because it generalizes to paraphrases and novel phrasing. It outperforms classifier-based routing in cases where you have limited labeled training data or where the category boundaries are clear and stable. A healthcare chatbot routing patient questions to either a symptom checker model or a general health information model used semantic routing with 50 prototype queries per category and achieved 91% accuracy. Training a classifier required 5,000 labeled queries and achieved 93% accuracy, a marginal improvement that did not justify the labeling cost and maintenance burden.

Semantic routing also excels when categories are defined by intent or topic rather than syntax or formatting. A legal contract analysis system routed queries to different models based on contract type: employment agreements, purchase agreements, non-disclosure agreements, and service agreements. Keyword rules failed because the same words appear across all contract types. A classifier required thousands of labeled examples per contract type. Semantic routing with 100 prototype queries per type achieved 89% accuracy because the embedding space naturally clustered contracts by semantic content and legal structure.

The failure mode for semantic routing is category ambiguity. If your categories overlap semantically or if queries can legitimately belong to multiple categories, centroid-based routing will misclassify. A customer support system routing queries to billing, technical support, and account management found that 18% of queries involved both billing and technical issues. The centroid for billing issues and the centroid for technical issues were equidistant from these hybrid queries, and the router assigned them arbitrarily. The solution was multi-label classification: instead of assigning each query to one category, the system assigned confidence scores to all categories and routed queries with high scores in multiple categories to a generalist model capable of handling multi-issue queries.

## Hybrid Approaches: Semantic Routing as Classifier Input

The most robust routing systems combine semantic routing with classifier-based routing by using the embedding similarity scores as features in a downstream classifier. Instead of routing based on the nearest centroid alone, you compute the cosine similarity to all centroids and pass those scores as features to a lightweight classifier that also considers other signals like query length, keyword presence, user context, and historical routing outcomes. This hybrid approach captures both semantic similarity and edge cases that pure embedding-based routing misses.

A financial services company routing customer inquiries used a hybrid system where semantic routing computed similarity scores to five category centroids, and a gradient boosting classifier used those scores plus 12 additional features including query length, presence of account numbers, time of day, and user tenure. The classifier achieved 96% routing accuracy, compared to 91% for semantic routing alone and 94% for a classifier without embedding features. The embedding features were the most predictive, but the other features captured edge cases like urgent queries flagged by short length and high keyword density.

The hybrid approach also enables continuous improvement without retraining the embedding model. You can retrain the lightweight classifier on new labeled data every week while keeping the embedding model and centroids stable. This reduces the cost and complexity of maintaining the routing system while still adapting to drift and new patterns. The legal research platform retrained their classifier monthly using 2,000 newly labeled queries and updated the centroids quarterly using a larger dataset of 20,000 queries. This two-tier maintenance schedule balanced accuracy, cost, and engineering effort.

## Measuring and Monitoring Semantic Routing Accuracy

You measure routing accuracy by logging the route assigned to each query and the ground truth route determined by human review or downstream model performance. If a query routed to the simple model required escalation to the complex model, that is a routing error. If a query routed to the complex model could have been handled by the simple model without loss of quality, that is a cost inefficiency error. Both matter, but routing errors that degrade user experience are higher priority than cost inefficiency errors that waste money.

The legal research platform tracked two metrics: routing precision, the percentage of queries routed to each model that truly belonged there, and routing cost efficiency, the percentage of queries routed to the expensive model that could have been handled by the cheap model. Routing precision for the complex model was 94%, meaning 6% of queries routed there could have gone to the simple model. Routing precision for the simple model was 89%, meaning 11% of queries routed there should have gone to the complex model. The team prioritized fixing the 11% under-routing problem because it degraded user experience, while the 6% over-routing problem only increased cost.

You also monitor the distribution of queries across categories over time. If the distribution shifts, your centroids may no longer represent the true semantic center of each category. The healthcare chatbot saw a sudden spike in queries about a new disease outbreak, which shifted the centroid for symptom-related queries toward outbreak-specific symptoms and degraded routing accuracy for routine symptom questions. The team detected the shift by tracking the mean cosine similarity between incoming queries and their assigned centroid. When mean similarity dropped from 0.82 to 0.74 over three days, they triggered a centroid re-computation using recent queries and restored similarity to 0.81.

## When Not to Use Semantic Routing

Semantic routing is not appropriate when your routing decision depends on structured metadata rather than semantic content. A document processing pipeline routing invoices, purchase orders, and receipts to different models should use rule-based routing on document type metadata, not semantic routing on document text. The document type is explicit and deterministic, and semantic routing adds latency and complexity without improving accuracy.

Semantic routing also fails when your categories are not semantically coherent. A customer support system that routes queries based on user subscription tier, not query content, cannot use semantic routing because subscription tier is not encoded in the query text. The routing decision is based on external context, not query semantics. You must use metadata-based routing or a classifier that incorporates user context features.

Finally, semantic routing struggles with very short queries where the embedding does not capture enough signal to distinguish categories. A search engine routing one-word or two-word queries found that embeddings for short queries clustered randomly because there was insufficient context to determine intent. The team added a fallback rule: queries under five words were routed based on keyword matching, while longer queries used semantic routing. This hybrid approach improved routing accuracy for short queries from 68% to 84%.

## Building Your First Semantic Router

Start by defining your routing categories based on task type, model capability, and cost. Collect 50-200 prototype queries per category from historical data or by generating synthetic examples that represent the range of each category. Embed the prototypes using a fast embedding model and compute centroids. At runtime, embed each incoming query, compute cosine similarity to all centroids, and route to the category with the highest similarity. Log all routing decisions and ground truth outcomes, and measure routing accuracy weekly. Re-compute centroids monthly or quarterly as your query distribution evolves. If routing accuracy is below 85%, either collapse overlapping categories, collect more diverse prototypes, or switch to a hybrid approach that combines embedding similarity with a lightweight classifier.

Semantic routing is not a replacement for all routing logic, but it is the best solution for cases where the routing decision depends on understanding what the query is asking, not just which words it contains. It generalizes better than keyword rules, requires less labeled data than classifiers, and adapts gracefully to paraphrases and novel phrasing that rule-based systems cannot handle. The next challenge is routing queries based on risk, not just semantic category, which requires detecting high-stakes queries and sending them to models with stronger safety and factuality guarantees.
