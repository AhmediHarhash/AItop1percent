# 1.14 — Writing Model Landscape Chapters That Survive Quarterly Churn

In November 2025, a financial services company published an internal model selection guide for their engineering teams. The document was excellent: 47 pages analyzing the capabilities, costs, and compliance characteristics of eight language models. It included detailed comparisons of GPT-5.1 Turbo, Claude Opus 4, Gemini 2 Pro, Llama 4 405B, DeepSeek V3, Qwen3 72B, Grok 3.5, and Mistral Large 2. Each model had a two-page profile covering benchmark performance, pricing, API reliability, context window size, supported features, and recommended use cases. The guide took three engineers two weeks to research and write. By February 2026, it was obsolete. OpenAI had released GPT-5 with restructured pricing. Anthropic had launched Claude Opus 4.5 with a doubled context window. Google had deprecated Gemini 2 Pro in favor of Gemini 3. DeepSeek had changed their rate limits. Four of the eight model profiles contained information that was no longer accurate, and two referenced models that were no longer the current generation. The guide was still being cited in architecture reviews, leading teams to make decisions based on outdated data.

This is the central problem with documenting the model landscape: any static snapshot is outdated within three months. Models are released, updated, deprecated, repriced, and repositioned constantly. The information you write in January is wrong by April. If you write model comparisons as if they are stable reference material, you create a document that decays faster than it can be maintained. The solution is not to give up on documentation. The solution is to structure your model landscape knowledge in ways that survive churn, separate stable patterns from volatile details, and build maintenance into the documentation process from the start. A well-designed model landscape document remains useful across multiple release cycles because it focuses on the categories and principles that persist, not the specific numbers that change.

## The Half-Life of Model Information and What Decays Fastest

Model landscape information decays at different rates. Some facts have a half-life measured in weeks, others in quarters, and a few remain stable for years. Understanding what decays and how fast determines what you document and how you structure it.

Benchmark scores decay fastest. A model's MMLU score, HumanEval score, or Arena Elo rating changes with every model update, every benchmark revision, and every time a competitor releases a new model that shifts the relative rankings. Documenting specific benchmark numbers is documenting information with a two-month half-life. By the time your document is reviewed and published, the numbers are already stale. By the time someone reads it three months later, the numbers are misleading. Benchmark scores belong in a change log or a live dashboard, not in a reference document.

Pricing decays nearly as fast. Model providers adjust pricing every quarter in response to cost reductions from infrastructure improvements, competitive pressure, and strategic positioning. A model that costs two dollars per million tokens in January may cost one dollar fifty in April and one dollar in July. Pricing structures change as well: providers introduce volume tiers, reserved capacity discounts, batch processing discounts, and promotional pricing. Documenting specific prices without a last-updated timestamp makes your document a source of planning errors. Teams read your document, estimate costs based on outdated prices, and blow their budgets when actual costs are 40% higher than the document stated.

API features and limitations decay over quarters. A model that does not support function calling in January gains function calling in March. A model with a 128,000 token context window expands to 256,000 tokens in May. A model with a 20 requests-per-minute rate limit increases to 50 requests per minute for enterprise customers. Structured output support, vision capabilities, multi-turn conversation handling, and streaming support all change as providers add features to remain competitive. Documenting current API capabilities without an update mechanism means your document tells readers that a model cannot do things it can now do, or can do things it no longer does.

Provider reliability and compliance posture decay over six to twelve months. A provider that has never had a significant outage eventually has one. A provider that was not SOC 2 compliant achieves compliance. A provider that operated entirely in US data centers opens European data centers to comply with GDPR data residency requirements. These changes are infrequent enough that you might catch them in an annual review, but frequent enough that a two-year-old document will be wrong about half the providers you cover.

What remains stable across years is the capability tier structure, the task-type taxonomy, and the architectural trade-offs. The fact that there are frontier models, mid-tier models, and small models remains true even as specific models move between tiers. The fact that some tasks require deep reasoning while others require fast pattern matching remains true even as model capabilities improve. The fact that cost, latency, and quality are in tension remains true regardless of which models exist. This is what you document in depth, because this is what your readers can rely on six months after reading.

## Documenting in Terms of Capability Categories, Not Model Names

The key to writing model documentation that survives churn is to organize content by capability category and task type, not by model name. Instead of writing eight sections titled "GPT-5.1 Analysis," "Claude Opus 4 Analysis," and so forth, you write sections titled "Frontier Models for Deep Reasoning Tasks," "Mid-Tier Models for High-Volume Classification," and "Specialized Models for Code Generation." The model names appear as examples within each category, not as the primary organizational structure.

This approach survives churn because when a new model is released, you do not rewrite the entire document. You add the new model to the appropriate category, remove deprecated models, and update the examples. The category definitions remain stable. The guidance about when to use a frontier model versus a mid-tier model remains stable. The cost-quality trade-off analysis remains stable. Only the specific model names and their current positions within the categories change.

A capability-category section might read: "Frontier models as of February 2026 include GPT-5, Claude Opus 4.5, and Gemini 3 Ultra. These models offer the highest performance on complex reasoning, nuanced instruction following, and long-context understanding. They are appropriate for tasks where quality is the primary constraint and cost is secondary. Typical use cases include legal document analysis, medical literature review, advanced coding assistance, and strategic research synthesis. Pricing ranges from three to six dollars per million tokens for input and twelve to twenty dollars per million tokens for output."

Six months later, when GPT-5.2 is released and Gemini 3 Ultra is deprecated in favor of Gemini 3.5 Ultra, you update the first sentence to list the new models and remove the deprecated one. The rest of the section remains accurate. The use case guidance remains accurate. The pricing range may need adjustment, but the structural analysis of when and why to use a frontier model does not change. A reader encountering this document nine months after publication still extracts correct strategic guidance, even if some model names are outdated.

Contrast this with a model-name-centric structure. You write a section titled "Claude Opus 4 Analysis" covering capabilities, pricing, and use cases. When Claude Opus 4.5 is released, the entire section is obsolete. You must either rewrite it to cover Opus 4.5, or add a parallel section for Opus 4.5, or add caveats explaining that this section covers the old model and readers should check for updates. None of these options preserve the value of the original content. The document becomes a historical artifact rather than a living reference.

The category-based structure also makes it easier for readers to find the information they need. A reader does not care about Claude Opus 4 in the abstract. They care about whether they should use a frontier model for their compliance report generation task, and if so, which one. A document organized by capability category answers that question directly. A document organized by model name requires the reader to synthesize information across multiple sections to answer the same question.

You still include model-specific details, but you include them as structured tables or lists within the category sections, not as free-form narrative. A table listing current frontier models with columns for name, context window, pricing, compliance certifications, and last updated timestamp conveys the volatile information in a format that is easy to scan and easy to update. When a new model is released, you add a row. When pricing changes, you update the cell. The table format signals to readers that this information is time-sensitive and should be verified before use.

## The Living Document Pattern and Quarterly Update Cadence

A living document is a document designed to be updated continuously as information changes, with a clear change log and a defined update cadence. The model landscape document is a living document by necessity. If you publish it as a static artifact, it dies within three months. If you publish it as a living document with quarterly updates and a visible change log, it remains useful indefinitely.

The quarterly update cadence aligns with the rate at which major model releases and pricing changes occur. Most providers release significant updates once per quarter. Some release more frequently, but even the fastest-moving providers do not sustain a pace faster than monthly, and most of those monthly releases are minor. A quarterly review catches 90% of the changes that matter. A monthly review catches 98%, but the incremental value does not justify the maintenance cost for most teams. A semi-annual review misses too much; by the time you update, the document has been misleading readers for three months.

The quarterly update process involves assigning an owner, scheduling a review session, collecting updates from providers and public sources, revising the affected sections, and publishing a change log. The owner is a senior engineer or architect who understands model capabilities and maintains awareness of provider announcements. The review session is a scheduled two-hour block on the team calendar, recurring every three months. During the session, the owner and one or two other engineers review recent model releases, pricing changes, feature updates, and compliance announcements. They identify which sections of the document need updates and make the changes.

The change log is a critical component. Every update to the document is recorded with a date and a summary of what changed. The change log appears at the top of the document, so readers immediately see how recent the information is and what has changed since they last read it. A change log entry might read: "February 2026: Added GPT-5 and Claude Opus 4.5 to frontier model category. Updated pricing for Gemini 3 to reflect new volume tiers. Removed Llama 4 405B from recommended models due to persistent API reliability issues. Updated compliance section to reflect DeepSeek SOC 2 Type II certification."

The change log serves two functions. First, it tells readers whether they need to re-read sections they have already seen. If the last update was December 2025 and a reader last consulted the document in January 2026, they scan the February change log to see if anything relevant to their work has changed. If the changes only affect models they are not using, they do not need to re-read. Second, the change log creates accountability for keeping the document current. If the last change log entry is six months old, that is visible evidence that the document is not being maintained.

You version the document explicitly. Each quarterly update increments a version number. Version 1.0 is the initial publication. Version 1.1 is the first quarterly update. Version 2.0 is a major revision that restructures the document or adds new sections. The version number appears in the document header and in file names. When someone references the model landscape document in a design review or architecture decision record, they cite the version number. This prevents confusion when different teams are working from different versions.

The living document is stored in a location that supports version history and collaborative editing. A wiki, a Git repository with Markdown files, or a collaborative document editor like Google Docs or Notion all work. The critical requirement is that all updates are tracked, previous versions are recoverable, and multiple people can contribute. A PDF stored on a shared drive does not meet these requirements. Neither does a Word document emailed to a distribution list. Those formats are for static artifacts, not living documents.

## What to Include: Capability Tiers, Cost Benchmarks, Compliance, Reliability

The model landscape document includes four categories of information: capability tier assessments, cost benchmarks, compliance status, and provider reliability history. These are the dimensions that inform model selection decisions and that can be documented in ways that survive quarterly churn.

Capability tier assessments categorize models into frontier, mid-tier, small, and specialized tiers based on their performance on complex reasoning tasks, instruction following, and output quality. The assessment is qualitative and based on your own evaluations, not on public benchmark scores. You might define frontier tier as "models that achieve above 85% accuracy on our internal contract analysis evaluation and above 90% on our instruction following evaluation." Mid-tier is "models that achieve 70% to 85% on contract analysis and 80% to 90% on instruction following." Small is "models that achieve 50% to 70% on contract analysis, suitable for simple classification tasks but not for complex analysis." Specialized is "models optimized for specific domains like code or mathematics that may outperform frontier models on those domains but underperform on general tasks."

You include a table listing current models by tier, updated quarterly. The table does not attempt to rank models within a tier; it simply assigns each model to a tier. When a new model is released, you run it through your internal evaluations, determine its tier, and add it to the table. When an existing model is updated, you re-evaluate if the update is substantial, and adjust its tier if warranted. The tier assignments remain stable over time because they are based on broad capability categories, not fine-grained performance differences.

Cost benchmarks document the current pricing for each model in terms of input and output tokens, with adjustments for batch processing, cached context, or other pricing features. You present costs as ranges rather than exact numbers, because exact numbers change too frequently. "Frontier models cost three to six dollars per million input tokens and twelve to twenty dollars per million output tokens as of February 2026." This range remains useful even if specific models shift within the range. You include a table with exact current prices for reference, marked with the last-updated date.

You also document cost-performance ratios: the cost to achieve a given level of quality on a representative task. For example, you might benchmark the cost to analyze 100 contracts at 85% accuracy. Frontier Model A costs $8.50 per 100 contracts. Mid-Tier Model B costs $2.10 per 100 contracts at 82% accuracy. Small Model C costs $0.40 per 100 contracts at 68% accuracy. These ratios help teams understand the cost-quality trade-off in concrete terms. When pricing changes, you re-run the benchmark and update the numbers. The underlying analysis—that mid-tier models deliver 80% of the quality at 25% of the cost—remains stable even as specific numbers drift.

Compliance status documents which models meet which regulatory and security requirements. This includes SOC 2 Type II certification, ISO 27001, HIPAA eligibility, GDPR compliance, data residency options, and whether the provider offers Business Associate Agreements for healthcare use cases. Compliance status changes slowly, usually on an annual cycle, but the consequences of being wrong are severe. If your document states that a model is HIPAA-eligible and it is not, a team may deploy it in a healthcare context and create a compliance violation. You verify compliance status during each quarterly review by checking provider documentation and trust center pages.

Provider reliability history documents the uptime, incident frequency, and API stability for each provider over the past 12 months. This is historical data, so it does not decay in the same way that pricing or features do. A provider that had three significant outages in 2025 had those outages regardless of what happens in 2026. You update the history with new incidents as they occur, but you do not delete old incidents. The pattern matters: a provider with frequent small incidents has a different reliability profile than a provider with one catastrophic incident per year. Teams use this information to assess risk when choosing a provider for a high-availability application.

## What Not to Include: Specific Benchmarks, Definitive Rankings, Ephemeral Details

The model landscape document does not include specific benchmark scores, definitive "best model" rankings, or ephemeral details that change weekly. These categories of information decay too fast to maintain and distract from the stable strategic guidance that makes the document valuable.

Specific benchmark scores are not useful for your readers because benchmark scores do not predict task-specific performance, and they change with every model update. If you include a table stating that Model A scores 89.4% on MMLU and Model B scores 87.2%, you have documented a fact that will be obsolete within six weeks and was not decision-relevant when it was current. Your readers do not choose models based on MMLU scores; they choose models based on performance on their tasks. You already told them that by assigning models to capability tiers. The benchmark scores add no value and impose a maintenance burden.

If you feel compelled to reference benchmarks, you do so at the category level: "Frontier models typically score above 85% on MMLU and above 80% on HumanEval. Mid-tier models score 70% to 85% on MMLU and 60% to 80% on HumanEval." This is stable guidance that remains true across model generations. The specific scores for specific models are left out.

Definitive "best model" rankings are not useful because the best model depends on the task, the cost constraints, the latency requirements, and the compliance context. Claiming that Model A is definitively better than Model B is either wrong or trivial. It is wrong if you are making a general claim, because Model A may be better for reasoning tasks while Model B is better for classification tasks. It is trivial if you are making a narrow claim, because you are just restating the tier assignments. Instead of ranking models, you provide guidance on which tiers are appropriate for which task types. The reader applies that guidance to their specific context.

Ephemeral details like promotional pricing, beta features, waitlist status, or rate limit exceptions for specific customers are not included because they change too frequently and apply to too narrow a set of readers. If a provider is offering 50% off for the first three months of a new model, that is useful information for a team making a decision this week, but it is not useful for a team reading the document three months from now. Promotional pricing belongs in internal team chat, not in reference documentation. Similarly, beta features may be available to some users but not others, and their behavior may change before general availability. Documenting beta features creates confusion and maintenance burden. Wait until features are generally available, then add them.

You also avoid subjective assessments that are not grounded in measurable criteria. Statements like "Model A feels more coherent than Model B" or "Model C has a better tone for professional contexts" are vibe-based judgments that do not generalize across readers or use cases. If you have measured coherence or tone on a defined rubric, you can report the results: "Model A scores 4.2 out of 5 on our coherence rubric; Model B scores 3.8." If you have not measured it, do not document it. Subjective impressions belong in Slack discussions during model evaluation, not in the landscape document.

## Structuring a Quarterly Model Landscape Review Cadence

The quarterly review cadence is a scheduled process, not an ad-hoc event. You define who is responsible, when the review happens, what sources are consulted, what updates are made, and how updates are communicated. This structure ensures the document stays current without requiring heroic individual effort.

The owner of the model landscape document is a senior engineer or architect with responsibility for AI infrastructure or model operations. This person is not necessarily the only contributor, but they are accountable for ensuring updates happen on schedule. Ownership is explicit and documented. If the owner leaves the team or changes roles, ownership is transferred to a named successor. Ownership does not diffuse to "the team" or "whoever has time." Diffuse ownership is no ownership, and the document decays.

The review is scheduled on the team calendar as a recurring event every three months: February, May, August, November. The event is two hours long and includes the owner plus one or two other engineers with model evaluation experience. The session is not optional and is not deprioritized when other work is busy. Skipping a review means the document goes six months without updates, at which point it is no longer reliable. Protecting the review cadence is a team priority.

During the review session, the team consults provider release notes, pricing pages, trust centers, and API documentation to identify changes since the last review. They check for new model releases, deprecated models, pricing changes, new API features, compliance certifications, and incidents reported on provider status pages. They review internal Slack channels, design review notes, and incident postmortals to surface learnings from production use of models. They identify which sections of the document need updates based on this information.

The updates are made during or immediately after the review session. The team does not defer updates to "later" because later never comes. They edit the document, update tables, add new models to tier categories, revise pricing ranges, and add entries to the change log. The updated document is published the same day, with the new version number and the current date in the header. If updates require additional research—for example, verifying a compliance claim by contacting a provider—the research is assigned to an owner with a deadline before the next review.

The updates are communicated to stakeholders via the team's standard communication channels. A message is posted to the engineering-wide Slack channel or mailing list with a summary of changes and a link to the updated document. Teams that are actively evaluating models or planning deployments are notified directly. The communication is brief: "Model landscape document updated to v1.4. Key changes: added GPT-5 and Claude Opus 4.5, updated Gemini pricing, removed deprecated models. Full change log in the document." This ensures that teams do not make decisions based on stale information because they were not aware an update had occurred.

The review cadence is adapted if the model landscape becomes more or less volatile. If provider release frequency increases to the point that quarterly reviews miss too many changes, the cadence shifts to monthly or bimonthly. If the landscape stabilizes and quarterly reviews consistently find few changes, the cadence shifts to semi-annual. The default is quarterly, but the team monitors whether that cadence is serving its purpose and adjusts accordingly.

## Connecting Landscape Documentation to Routing Configuration and Eval Suites

The model landscape document is not a standalone artifact. It is integrated with your routing configuration and evaluation suite so that the knowledge it contains flows directly into operational decisions. When the landscape document is updated to add a new model to the frontier tier, that model is automatically added to the next routing evaluation sweep. When the document is updated to mark a provider as having reliability issues, alerts are configured to monitor that provider more closely. The document and the operational systems are connected, not siloed.

Routing configuration is the system that decides which model handles which request in production. It might route simple classification tasks to mid-tier models and complex reasoning tasks to frontier models, or route requests from latency-sensitive users to fast models and requests from quality-sensitive users to high-quality models. The routing configuration references the capability tier definitions from the landscape document. When the landscape document is updated to move a model from mid-tier to frontier, the routing configuration is reviewed to determine whether routing rules should change. This review is part of the quarterly update process.

Evaluation suites are the sets of tests you run to measure model performance on your tasks. The evaluation suite includes tests for each major task type, each capability tier, and each quality dimension that matters for your application. The landscape document lists which models are in each tier, and the evaluation suite includes tests for representative models from each tier. When a new model is added to the landscape document, it is added to the evaluation queue. The next time the evaluation suite runs, the new model is included. This ensures that your tier assignments are continuously validated against actual performance.

The connection between landscape documentation and operational systems is maintained through automation where possible. The landscape document is stored in a structured format—YAML, JSON, or a database table—that can be parsed by scripts. The routing configuration and evaluation suite read from this structured representation. When the landscape document is updated, the changes propagate automatically. This eliminates manual synchronization errors and ensures the document remains the single source of truth.

If full automation is not feasible, the connection is maintained through explicit review steps. The quarterly landscape review checklist includes items like "Review routing configuration for consistency with tier changes" and "Add new models to evaluation queue." These steps are mandatory, not optional. The review is not complete until the operational systems have been updated to reflect the landscape changes.

This integration transforms the landscape document from a passive reference into an active operational tool. It is not just a document that engineers consult when they are curious about models. It is the document that drives routing decisions, evaluation priorities, and incident response. When the document is updated, production behavior changes. This creates strong incentives to keep the document accurate and current, because inaccurate documentation leads directly to production errors.

## Closing Chapter 1: From Landscape to Selection Methodology

Understanding the model landscape—the tiers, the providers, the cost structures, the compliance constraints, and the reliability patterns—is the foundation of model selection. But landscape knowledge alone does not tell you which model to deploy for your application. The landscape tells you what is available. The selection methodology tells you how to choose.

Chapter 1 covered the landscape: what models exist, how they differ, how the landscape changes, and how to document landscape knowledge in ways that survive churn. You now understand capability tiers, context windows, cost models, API reliability, compliance requirements, and the limitations of public benchmarks. You understand that the landscape is volatile and that documentation must be structured as a living artifact, not a static snapshot.

Chapter 2 covers selection methodology: the process you follow to evaluate models for a specific task, compare them on dimensions that matter for your application, and make a defensible decision about which model to deploy. This includes defining task requirements, building evaluation sets, running comparative tests, analyzing cost-quality trade-offs, validating compliance, and documenting the decision. The methodology is stable even as the landscape changes. When a new model is released, you apply the same methodology to evaluate it. When your requirements change, you apply the same methodology with updated criteria.

The relationship between landscape knowledge and selection methodology is that landscape knowledge narrows your search space, and selection methodology makes the final choice. The landscape tells you that frontier models are appropriate for complex reasoning tasks, mid-tier models are appropriate for high-volume classification, and small models are appropriate for latency-critical simple tasks. That narrows 30 available models to five candidates. The selection methodology evaluates those five candidates on your specific task, measures their performance, compares their costs, validates their compliance, and produces a recommendation. The landscape sets the frame; the methodology fills it in.

Chapter 2 begins with defining task requirements in measurable terms. You will learn how to translate vague product requirements like "high quality" and "good user experience" into specific evaluation criteria like "95% accuracy on named entity extraction" and "90th percentile latency under 800 milliseconds." You will learn how to build evaluation sets that predict production performance, how to run comparative evaluations that surface meaningful differences between models, and how to document selection decisions so that future teams understand why a model was chosen and when it should be reconsidered. The landscape is what you know about the world of models. The methodology is how you act on that knowledge.

