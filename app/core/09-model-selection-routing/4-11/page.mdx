# 4.11 — Cost Modeling for Capacity Planning: Forecasting Spend at Scale

In September 2025, a B2B SaaS company discovered their AI feature would cost $340,000 per month at full rollout—nearly triple the $120,000 they had budgeted. They had launched a conversational search feature to their first 5,000 users in July, watched costs hover around $18,000 monthly, and extrapolated linearly to their 90,000-user base. The CFO approved the expansion. Two weeks after opening the feature to all users, their invoice hit $127,000. By month three, with seasonal usage patterns they hadn't anticipated, costs reached $340,000. The feature was profitable on paper at $120,000. At $340,000, it destroyed unit economics. They emergency-throttled the feature, angered users, and spent four months rebuilding with cheaper models and aggressive caching. The root cause was not a technical failure. It was the absence of a proper cost model. They had treated AI spend as a simple linear function when it was actually a multi-variable system with compounding effects, usage patterns that varied by cohort, and seasonal spikes they never measured.

You cannot manage AI costs at scale without a formal cost model. A cost model is not a guess or a back-of-envelope calculation. It is a living financial projection built from actual usage data, updated regularly, and stress-tested against multiple growth scenarios. Most teams build their first cost model only after costs spiral. You need it before you scale. This subchapter shows you how to construct a cost model that forecasts spend accurately, communicates clearly to finance, and breaks down when conditions change so you know when to rebuild it.

## The Anatomy of an AI Cost Model

An AI cost model decomposes total spend into measurable variables you can track and project independently. The foundational formula is straightforward: total cost equals the number of requests multiplied by the average cost per request. But average cost per request is itself a composite of multiple factors: tokens per request, model used, whether the request hit cache, whether it triggered multiple model calls in a chain, and whether it included expensive operations like function calling or vision processing.

Start with your simplest decomposition. For a single AI feature serving one user segment with one model, your cost model might be: monthly active users times average requests per user per month times average tokens per request times cost per token. If you have 10,000 monthly active users, each making 15 requests, each request averaging 2,000 tokens, and you are paying $0.000003 per token for GPT-5-mini input and $0.000012 per output with a 500-token average output, your monthly cost is 10,000 times 15 times 2,500 tokens times a blended token cost. Work this through. Input tokens: 10,000 users times 15 requests times 2,000 tokens times $0.000003 equals $900. Output tokens: 10,000 users times 15 requests times 500 tokens times $0.000012 equals $900. Total: $1,800 per month.

This is your baseline. Now add variables. If 40% of your users are power users making 40 requests per month while 60% make only 5 requests, your average of 15 requests per user hides a bimodal distribution. Model this explicitly: 4,000 users at 40 requests plus 6,000 users at 5 requests equals 190,000 total requests. Now your cost is 190,000 requests times 2,500 tokens times blended token cost, which comes to $2,375 per month—30% higher than the naive average. Segment your users by behavior, not by average.

Next add your model mix. If 70% of requests go to GPT-5-mini at $0.000003 per input token and 30% escalate to GPT-5 at $0.000015 per input token, your blended input cost is no longer a simple average. Calculate the weighted cost: 0.70 times $0.000003 plus 0.30 times $0.000015 equals $0.0000066 per token. If your output token costs follow the same distribution, scale those similarly. Your cost model now reflects reality: not all requests cost the same, and you need to track what percentage routes to which model.

Now add caching. If you implement semantic caching and achieve a 35% cache hit rate, 35% of your requests cost nearly nothing—perhaps $0.0000001 per token for cache retrieval versus $0.000003 for a full model call. Your effective cost per cached request drops by 97%. Model this as two populations: 65% of requests at full cost, 35% at cache cost. If full-cost requests total $2,375 monthly and you cache 35%, you save approximately $831, bringing costs to $1,544. This is why caching is not optional at scale. A cost model quantifies the savings before you build it, which justifies the engineering investment.

Add growth. If you expect to grow from 10,000 monthly active users to 25,000 over the next six months, your cost model projects future spend. At 25,000 users with the same usage patterns, costs scale proportionally to $3,860 per month with caching. If your product roadmap includes a new feature that increases requests per user from 15 to 22, costs jump to $5,660. If that feature drives a model mix shift where 50% of requests now route to GPT-5 instead of 30%, costs climb further. Layer each change independently so you can see which variable drives cost most aggressively.

Your cost model is a spreadsheet with rows for each user segment, columns for requests per user, tokens per request, model mix percentages, cache hit rates, and cost per token for each model. At the bottom, total monthly cost. Add tabs for different scenarios: baseline, moderate growth, aggressive growth, new feature launch, model switch, cache improvements. Update it monthly with actual usage data so your projections stay grounded. This is the spreadsheet every AI team needs. If you do not have it, you are forecasting blind.

## Variables That Drive AI Costs

The precision of your cost model depends on identifying and measuring every variable that materially affects spend. Some variables are obvious. Others emerge only after you have been running in production for months and notice patterns in your invoices that do not match your projections.

**User growth** is the primary driver. If your product grows from 50,000 to 200,000 users, costs scale with it—unless usage per user declines or you improve efficiency faster than growth. Track not just total user count but active users, because only active users generate requests. A vanity metric of 200,000 registered users means nothing if only 30,000 are active monthly. Your cost model uses monthly active users as the base multiplier.

**Requests per user** varies by feature, user segment, and time since onboarding. New users often make fewer requests as they explore the product. Power users make far more. Enterprise customers may have teams of 50 people hammering your AI feature while individual consumers make sporadic queries. Measure requests per user by cohort and by tenure. If your 30-day retention cohort makes 8 requests per month but your 180-day cohort makes 25, your cost model needs separate rows for each cohort weighted by cohort size. Aggregating to a single average hides the growth in per-user intensity that kills your budget.

**Tokens per request** depends on task complexity and prompt design. A summarization request averages 4,000 input tokens and 300 output tokens. A retrieval-augmented generation request might hit 12,000 input tokens if you stuff ten documents into context and 800 output tokens for a detailed answer. A quick classification task uses 150 input tokens and 5 output tokens. Your cost model must segment by task type. If 60% of requests are summarization and 40% are RAG, calculate costs separately and sum them. If you later optimize prompts and cut summarization input tokens from 4,000 to 2,800, your model shows the immediate cost impact.

**Model mix** shifts over time. You may start with 100% of traffic on GPT-5 and then introduce a routing layer that sends 70% to GPT-5-mini and 30% to GPT-5. Cost per token drops dramatically. Then you add Claude Sonnet 4.5 for a specific task where it outperforms GPT, and now you have three models in the mix at different price points. Track the percentage of requests routed to each model monthly. If your routing logic changes—say, you tighten the threshold for escalating to the expensive model—your cost model updates the mix percentages and recalculates total spend. Model mix is the single highest-leverage variable for cost control because switching from a $0.000015 model to a $0.000003 model cuts costs 80% if quality holds.

**Caching hit rates** determine how much of your traffic bypasses expensive inference. You deploy semantic caching and measure a 28% hit rate in week one, 41% in week two as the cache warms, and 55% in week four as it reaches steady state. Your cost model tracks this ramp and projects costs at each plateau. If you later add prompt caching at the provider level and boost total hit rate to 68%, your cost model quantifies the incremental savings. Caching hit rates degrade when you change prompts, add new task types, or experience traffic surges with novel queries. Model this as a range: conservative 40%, expected 55%, optimistic 70%. Budget to the conservative number.

**Feature additions** introduce new request types with different cost profiles. You launch a multimodal feature that processes images, adding vision model costs at $0.00075 per image. If 10% of your users adopt this feature and each makes 3 image requests per month, you add 25,000 images times $0.00075 equals $18,750 monthly. Your cost model adds a row for image requests with its own volume and unit cost. You launch a voice assistant feature using audio transcription at $0.006 per minute. If 5% of users use it for an average of 8 minutes per month, that is 5,000 users times 8 minutes times $0.006 equals $240. Small features compound. Three small features each adding $10,000 monthly turn into $30,000, which is a full headcount in many regions.

**Seasonal patterns** emerge in consumer products and B2B products alike. A tax preparation AI sees 10x usage in March and April compared to July. An e-commerce product assistant sees spikes in November and December. A B2B analytics tool sees higher usage at month-end and quarter-end. If your cost model uses average monthly usage, it will underestimate peak months by orders of magnitude. Model seasonality explicitly. Calculate a seasonality multiplier for each month based on historical data. If December is 2.8x your baseline and your baseline cost is $50,000, budget $140,000 for December. Finance will thank you for not surprising them with a $140,000 invoice when they expected $50,000.

**Provider pricing changes** invalidate your cost model overnight. In early 2026, multiple providers cut prices 30-50% on previous-generation models and raised prices on new flagship models. If your cost model hard-codes token prices and you do not update it, your projections drift. Track provider pricing changes monthly. Subscribe to provider blogs and pricing update emails. When a price change hits, update your model immediately and rerun projections. If you are locked into an enterprise agreement with fixed pricing, note the contract end date and model the cost impact of returning to list pricing.

Every variable in your cost model must tie to a measurable metric you track in production. If you cannot measure it, you cannot model it. Instrument your system to log user IDs, request counts, token counts, model names, cache hits, and task types. Export this data weekly, aggregate it, and feed it into your cost model. Your model is only as good as your data.

## Scenario Planning: Stress-Testing Your Cost Model

A cost model is not a single number. It is a distribution of possible outcomes based on different assumptions about growth, usage, and efficiency. Scenario planning turns your cost model into a decision-making tool by projecting costs under best-case, expected-case, and worst-case conditions. This is how you avoid surprises and build organizational confidence that you understand your spend.

Start with your **baseline scenario**: current usage patterns continue unchanged. Current user count, current requests per user, current model mix, current cache hit rate. This is your floor. If nothing changes, this is what you will spend next month and the month after. Baseline is where you are today. Most teams only build a baseline and stop there. That is not a cost model. That is a bill.

Build your **expected growth scenario**: user count grows at the rate your product team forecasts, requests per user increase modestly as users engage more deeply, and you implement one cost optimization like improved caching. If your baseline is $50,000 monthly and you expect 40% user growth over six months, your expected scenario projects costs rising to $70,000 per month if you achieve a 20% cost efficiency gain from caching. This is your planning number. This is what you tell finance to budget. This is what you hold yourself accountable to.

Build your **aggressive growth scenario**: user count doubles, power user adoption increases, a new feature launches and adds 30% more requests per user, and your cost optimizations deliver less than expected. This scenario might project costs at $140,000 monthly. This is not your plan. This is your stress test. If costs hit $140,000, do your unit economics still work? Can you afford it? Do you need to raise prices, throttle usage, or cut costs elsewhere? Finance wants to know the upside and the downside. Aggressive growth is the downside for your budget and the upside for your product. Model it.

Build a **cost optimization scenario**: you switch 50% of traffic from GPT-5 to GPT-5-mini, increase cache hit rate from 40% to 65%, reduce average tokens per request by 25% through prompt optimization, and negotiate a 20% volume discount with your provider. This scenario might cut costs to $32,000 monthly at current usage levels. This is your cost reduction roadmap. Each optimization is a line item with an estimated impact. When leadership asks how you will control costs, you show them this scenario and commit to delivering each optimization by a specific quarter.

Build a **model switch scenario**: a new model launches with better performance at half the cost, and you migrate 80% of traffic to it. Your costs drop 40% overnight. Or a new model launches at twice the cost but delivers quality improvements that reduce task retry rates from 12% to 3%, saving enough in gross requests to break even. Model both sides of the switch. Switching models is not free—it requires eval work, prompt tuning, and integration effort—but the cost impact can be transformational. A model switch scenario helps you prioritize the migration.

Build a **seasonal spike scenario**: your peak month costs 3x your baseline. If your baseline is $50,000, your peak is $150,000. Do you have budget headroom for that? Do you need to set aside reserves? Can you pre-purchase committed capacity at a discount to smooth costs? Seasonal planning prevents panic when December invoices arrive.

Run each scenario through your cost model and output a summary table: baseline, expected growth, aggressive growth, cost optimization, model switch, seasonal spike. For each, show monthly cost, annual cost, and cost per user or cost per request. Share this table with finance, with engineering leadership, and with your product team. This is how you communicate cost risk and opportunity. This is how you build trust that you are managing spend proactively, not reactively.

Update your scenarios quarterly. As actual costs come in, compare them to your expected scenario. If actual costs are tracking 15% higher, investigate why. Did usage grow faster than forecast? Did cache hit rates underperform? Did a prompt change increase tokens per request? Diagnose the variance, update your assumptions, and rerun scenarios. A cost model is a living document. If you build it once and never update it, it becomes fiction within three months.

## When Cost Models Break Down

Cost models are approximations. They rely on assumptions about user behavior, system performance, and external pricing that hold true most of the time but break down under specific conditions. Recognizing when your cost model no longer reflects reality is as important as building the model in the first place.

Your cost model breaks when **usage patterns change discontinuously**. A viral social media post drives 50,000 new users to your product in one week, none of whom behave like your existing cohorts. They make 40 requests in their first week compared to your baseline of 8 requests per month. Your cost model assumed steady growth at 10% monthly. It did not account for viral spikes. Your invoice for that week is 4x your projection. You cannot predict virality, but you can plan for it. Add a viral spike scenario to your model: what happens if user count doubles in one week. Budget for it or accept that you will eat the cost if it happens.

Your cost model breaks when **task complexity shifts**. You launch a feature that lets users upload documents for analysis. Suddenly, average tokens per request jumps from 2,000 to 9,000 because users are uploading 10-page PDFs. Your cost model assumed 2,000 tokens. Costs triple overnight. The solution is to model task complexity separately for each feature. If your product has three features—chat, summarization, and document analysis—build three cost models with different token assumptions and track adoption of each feature independently. When document analysis adoption exceeds your forecast, your model flags the cost impact immediately.

Your cost model breaks when **provider pricing changes mid-contract**. You built your model assuming $0.000015 per token for GPT-5. The provider announces a price increase to $0.000020. Your cost model is now 33% low. Update it the day the price change is announced. If you are on an enterprise agreement with rate protection, note when the agreement expires and model the step-function cost increase at renewal. If you are on pay-as-you-go, the increase hits immediately.

Your cost model breaks when **caching behavior degrades**. You achieved a 60% cache hit rate and modeled costs accordingly. Then you refactor your prompts to improve quality, which changes the embedding signature of requests and invalidates 80% of your cache. Hit rate drops to 22% for two weeks while the cache rewarms. Your cost model assumed 60%. Costs spike 40%. The fix is to model cache hit rate as a range, not a point estimate. Budget to the conservative end of the range. When you make changes that might affect caching, flag it to finance in advance: expect a cost spike for two weeks while the cache rebuilds.

Your cost model breaks when **rate limits force model substitution**. You modeled 70% of traffic on GPT-5-mini and 30% on GPT-5. You hit GPT-5-mini rate limits during a traffic surge and automatically fail over to GPT-5 for 50% of requests that were supposed to go to the cheap model. Suddenly, 65% of traffic is on the expensive model and 35% on the cheap model—the inverse of your plan. Costs double for the duration of the surge. Model rate limit scenarios explicitly. What is your provider's rate limit? What happens if you hit it? Do you have burst capacity? Do you fail over to a more expensive model or do you queue requests and accept latency spikes?

Your cost model breaks when **new regulations require audit logging**. Your model assumed costs were purely inference. Then a compliance requirement mandates that you log every request and response for seven years. Storage costs for logs add $8,000 monthly. Your model did not include storage. Expand your cost model to include non-inference costs: storage, egress, observability tooling, and any third-party services in your AI pipeline. Total cost of ownership is not just model inference.

When your cost model breaks, do not patch it. Rebuild it. A patched model with band-aid adjustments loses the trust of finance and engineering. A rebuilt model that incorporates new realities and updated assumptions demonstrates that you understand what changed and how to account for it going forward. Rebuilding a cost model takes four hours if you have the data infrastructure in place. That is four hours well spent to avoid months of budget confusion.

## Communicating Cost Projections to Finance and Leadership

Finance does not care about tokens, embeddings, or cache hit rates. Finance cares about total spend, growth rate, and variance to budget. Your cost model must translate technical variables into financial language. This is not dumbing down your model. This is translating it into the language of the stakeholder so they can make decisions.

Start with the summary: current monthly spend, projected spend at current growth rates, projected spend if growth accelerates, and projected spend if you implement cost optimizations. Present this as a simple table with four columns: scenario, monthly cost, annual cost, and cost per user. Add a fifth column for variance to budget. If your current budget is $50,000 monthly and your expected growth scenario projects $68,000, variance is plus 36%. Finance needs to see that number immediately.

Explain the primary cost drivers in one sentence each. User growth is driving 40% of cost increase. Model mix shift is driving 15% cost reduction. New feature launch is driving 20% cost increase. Cache hit rate improvement is driving 10% cost reduction. Net impact: 25% cost increase. Finance does not need to understand how caching works. They need to understand that caching saves money and you are investing in it.

Show the variance analysis: why actual costs differ from projections. If you projected $50,000 and spent $58,000, break down the $8,000 delta. User growth was 10% higher than forecast, adding $4,000. Cache hit rate underperformed by 15 percentage points, adding $3,000. A new feature launched earlier than planned, adding $1,000. This is how you build trust. Finance sees that you are not guessing. You are measuring, analyzing, and explaining.

Present your cost optimization roadmap with financial impact and timeline. Q2: implement semantic caching, projected savings $12,000 monthly, delivered by end of June. Q3: migrate 60% of traffic from GPT-5 to GPT-5-mini, projected savings $18,000 monthly, delivered by end of September. Q4: negotiate volume discount with primary provider, projected savings $8,000 monthly, delivered by end of December. Total projected annual savings: $456,000. Finance wants to see a plan to control costs, not just a projection that costs will rise.

Communicate risks and dependencies. Risk: if user growth exceeds forecast by 50%, costs will overrun budget by $30,000 monthly. Mitigation: we can throttle usage for free-tier users or raise prices. Dependency: cost optimization roadmap assumes engineering capacity to implement caching and routing logic. If engineering is reallocated to other projects, cost savings will not materialize and we will overrun budget. Finance appreciates transparency about risks. Surprises destroy trust. Flagged risks preserve it.

Update finance monthly with a cost report: actual spend, variance to projection, updated projection for next quarter, and status of cost optimization initiatives. This report is one page. It takes thirty minutes to prepare if your cost model is up to date. It prevents the quarterly surprise where finance discovers you are 40% over budget and no one flagged it.

When leadership asks if AI costs are under control, your answer is not yes or no. Your answer is: we are currently spending $58,000 monthly against a budget of $50,000, driven by user growth outpacing forecast. We have a cost optimization roadmap projected to save $38,000 monthly by end of Q3, which brings us to $40,000 monthly at current usage levels and $52,000 at projected Q3 usage levels. We are tracking on plan. This is how you communicate control.

## The Spreadsheet That Every AI Team Needs

Your cost model lives in a spreadsheet. It is not a dashboard, not a BI tool, not a custom application. It is a spreadsheet because spreadsheets are flexible, transparent, and universally understood by finance, engineering, and leadership. You can share it, collaborate on it, and update it without deploying code.

The spreadsheet has six tabs. Tab one: baseline model. This tab has rows for each user segment, columns for requests per user, tokens per request, model name, cost per token, cache hit rate, and total cost. At the bottom, sum total monthly cost. Update this tab monthly with actuals. This is your source of truth.

Tab two: expected growth scenario. Copy the baseline, adjust user count to your six-month forecast, adjust requests per user based on engagement trends, and adjust cache hit rate based on planned improvements. This is your budget number.

Tab three: aggressive growth scenario. Copy the baseline, double user count, increase requests per user by 50%, assume cache hit rate degrades by 10 percentage points, and add a new feature with its own cost structure. This is your worst-case planning number.

Tab four: cost optimization scenario. Copy the baseline, adjust model mix to shift traffic to cheaper models, increase cache hit rate to your target, reduce tokens per request based on prompt optimization, and apply a negotiated volume discount. This is your cost reduction target.

Tab five: variance analysis. Three columns: projected cost, actual cost, delta. Rows for each cost driver: user growth, requests per user, tokens per request, model mix, cache hit rate, other. Calculate the dollar impact of each variance. Update this monthly. This is how you explain to finance why costs differ from projections.

Tab six: monthly actuals. Twelve columns, one for each month. Rows for total cost, user count, requests per user, tokens per request, model mix percentages, cache hit rate, cost per user, cost per request. Fill this in monthly as data arrives. This is your historical record and the foundation for forecasting future quarters.

Link your cost model spreadsheet to your data warehouse if possible. Export user count, request count, token count, and cache hit rate from your observability platform or billing API. Import it into the spreadsheet monthly. This reduces manual data entry and ensures your model stays grounded in reality. If you cannot automate the data feed, set a recurring calendar reminder to update the spreadsheet manually on the first of every month.

Share the spreadsheet with engineering, product, and finance. Give everyone read access. Give your team edit access. Collaborate on assumptions. When product says they expect user growth to accelerate, update the expected growth tab together. When engineering says they can boost cache hit rate to 70%, update the cost optimization tab together. The cost model is not an artifact owned by one person. It is a shared planning tool.

Version the spreadsheet. Name it AI Cost Model 2026-01. When you rebuild it after a major change, increment to 2026-02. Keep old versions archived so you can look back and see what assumptions you made six months ago and why they were wrong. This is how you learn to forecast better.

Every AI team needs this spreadsheet. If you do not have it, stop reading and build it now. It will take you four hours. It will save you months of budget chaos and firefighting. It is the single highest-leverage artifact for managing AI costs at scale. In the next subchapter, we will examine how to use this cost model as leverage in negotiations with providers to secure volume discounts, committed use agreements, and reserved capacity that lock in lower rates and protect you from future price increases.
