# 1.7 — Multimodal Models: Vision, Audio, and Cross-Modal Capabilities in 2026

In mid-2025, a healthcare technology company launched an AI system to process patient intake forms. The forms combined handwritten notes, typed text, checkboxes, and occasionally included photos of insurance cards or medication bottles. The team chose GPT-5 for the task, routing every form through the same model regardless of content type. The system worked reasonably well for text-heavy forms, but accuracy dropped to 61% when handwriting appeared, and plummeted to 34% when forms included photos. After seven months and $180,000 in manual review costs to catch errors, the team discovered the core issue: they had treated multimodal capability as binary—either a model "supports images" or it doesn't. They never evaluated which model was actually best at the specific visual reasoning their task required. When they finally ran comparative evals, Gemini 3 Pro achieved 94% accuracy on the same intake forms at 40% lower cost per form. The failure wasn't technical. It was strategic: they assumed all multimodal models were equivalent and never tested the hypothesis.

The multimodal landscape in January 2026 is vastly more sophisticated than the bolt-on vision capabilities that defined 2023 and 2024. You are now choosing between models with fundamentally different multimodal architectures, training approaches, and capability profiles. Understanding these differences is not optional. Your choice of multimodal model directly determines accuracy, cost, latency, and the types of visual or audio reasoning your system can perform. This subchapter teaches you how to evaluate multimodal models for production use, when to use a specialized vision model versus a general-purpose multimodal LLM, and how to avoid the common failure pattern of treating all multimodal support as equivalent.

## Native Multimodal vs Bolt-On Vision: Architecture Matters

The distinction between native multimodal models and bolt-on vision systems is fundamental. A bolt-on approach means a language model was trained on text, then a separate vision encoder was added later to convert images into tokens the language model could process. The vision encoder and language model were trained separately, then stitched together. This was the dominant pattern in 2023 and early 2024. A native multimodal model, by contrast, was trained end-to-end on text, images, audio, and sometimes video from the beginning. The model learned to reason across modalities during pretraining, not as an afterthought.

GPT-5 and GPT-5, both released in 2024 and 2025 respectively, represent OpenAI's native multimodal approach. These models were trained from scratch on interleaved text, image, and audio data. GPT-5 in particular shows strong cross-modal reasoning: it can answer questions about an image by drawing on textual knowledge not present in the image itself, or describe sounds in an audio clip using visual metaphors. The architecture enables genuine multimodal understanding, not just parallel processing of different input types. GPT-5's voice mode, which allows real-time spoken conversation with sub-200 millisecond latency, is built on this native multimodal foundation. The model doesn't convert speech to text, process text, then convert back to speech. It processes audio directly as a native modality.

Gemini 3 Pro, released in late 2025, is the multimodal leader as of January 2026. Google trained Gemini 3 Pro natively on text, images, video, and audio at unprecedented scale. The model handles up to 90 minutes of video in a single context window, maintaining coherent understanding across thousands of frames. It can answer questions about events that happened 40 minutes into a video, referencing details from earlier scenes, without requiring you to manually timestamp or segment the video. This long-video understanding capability is unmatched. Gemini 3 Pro also excels at document understanding tasks: extracting structured data from invoices, parsing complex diagrams, reading handwritten notes in multiple languages. The native multimodal training means the model doesn't treat images as "foreign" inputs that need translation. It reasons about visual and textual information with equal fluency.

Claude Opus 4.5, released in November 2025, includes strong vision capabilities but is not primarily positioned as a multimodal-first model. Anthropic's approach emphasizes safety and instruction-following in the visual domain. Claude Opus 4.5 excels at tasks where you need careful, controlled visual reasoning: medical image analysis where false positives are costly, legal document review where you cannot miss redacted information, accessibility applications where precise descriptions of images matter. The model is conservative in its visual interpretations. It will say "I cannot determine" rather than guess when an image is ambiguous. This conservatism is valuable in high-stakes domains but may feel overly cautious in consumer applications.

Llama 4, released in early 2025, includes multimodal variants that combine Meta's open-weight language models with vision encoders. These are bolt-on systems, not native multimodal models, but the integration quality is high. Llama 4 Vision performs well on standard image classification and visual QA benchmarks, but struggles with complex cross-modal reasoning or long video understanding. The advantage is cost and deployability: you can run Llama 4 Vision on your own infrastructure, avoiding per-token API costs entirely. For applications where you need basic image understanding at massive scale—processing millions of product photos for e-commerce catalogs, analyzing user-uploaded images for moderation—Llama 4 Vision is often the right choice. You trade some capability for complete cost control.

The architectural difference matters in production. Native multimodal models handle ambiguity and cross-modal reasoning better. Bolt-on systems are faster to train and cheaper to serve, but they hit capability ceilings. You need to test both architectures on your specific task. Do not assume native is always better. For straightforward visual tasks—OCR, logo detection, simple classification—a well-tuned bolt-on system may outperform a general native multimodal model.

## When Multimodal Capability Actually Matters

Multimodal models are not universally necessary. Many production AI systems never process images, audio, or video. You need multimodal capability when your inputs are inherently non-textual or when the task requires reasoning about visual or audio information that cannot be easily converted to text without losing critical meaning. The canonical use cases are document understanding, image classification, visual question answering, audio transcription, and video summarization. Each category has different model requirements.

Document understanding encompasses any task where you extract information from PDFs, scanned forms, invoices, receipts, contracts, or diagrams. The challenge is not OCR—optical character recognition is a solved problem with specialized tools like Tesseract or cloud APIs. The challenge is layout understanding and semantic extraction. An invoice has a specific structure: header information, line items, totals, tax breakdowns. A contract has sections, clauses, subclauses, footnotes, and references. A diagram has labels, arrows, legends, and spatial relationships. Multimodal models can process these documents as images and extract structured information directly, without requiring you to run OCR first, parse the layout separately, then feed text to a language model. Gemini 3 Pro excels here. It can take a 30-page PDF with tables, charts, and footnotes, and extract every relevant field into a structured output with 92% accuracy. GPT-5 is also strong but occasionally hallucinates fields that don't exist. Claude Opus 4.5 is the most accurate for high-stakes document review—legal contracts, medical records—but slower and more expensive.

Image classification is the simplest multimodal task. You have an image and you need to assign it to one or more categories. Product categorization for e-commerce, content moderation for user-uploaded photos, quality control for manufacturing defects. Multimodal LLMs are often overkill for these tasks. A specialized vision model—a fine-tuned ResNet, Vision Transformer, or CLIP variant—will be faster, cheaper, and often more accurate. But multimodal LLMs shine when you need flexible, zero-shot classification. You don't have labeled training data for every category. You want to describe the categories in natural language and have the model figure it out. GPT-5 and Gemini 3 Pro can both classify images based on textual descriptions with surprising accuracy. You can change the categories without retraining. This flexibility is valuable in fast-moving consumer applications where product categories or content policies change monthly.

Visual question answering is where cross-modal reasoning becomes critical. You present an image and ask a question that requires understanding both the image and external knowledge. "What era is this painting from?" requires recognizing visual style and linking it to art history knowledge. "Is this plant safe for cats?" requires identifying the plant species and retrieving toxicology information. "What traffic violation is occurring in this dashcam photo?" requires understanding traffic rules and visual scene understanding. These tasks are impossible with vision-only models. You need a model that can reason across modalities. Gemini 3 Pro is strongest here, followed by GPT-5. Claude Opus 4.5 is more conservative and will refuse to answer when uncertain, which is sometimes the right behavior. Llama 4 Vision struggles with questions requiring external knowledge.

Audio transcription and understanding has matured significantly. GPT-5's native audio processing produces highly accurate transcriptions with speaker diarization, emotion detection, and filler word removal. Gemini 3 Pro handles multilingual audio and can transcribe hour-long recordings with high accuracy. The use case extends beyond transcription to audio reasoning: "What is the speaker's emotional state in this voicemail?" or "Summarize the action items from this meeting recording." Specialized speech-to-text models like Whisper remain faster and cheaper for pure transcription, but multimodal LLMs enable richer audio understanding workflows. You transcribe and analyze in a single API call, rather than chaining tools.

Video summarization and understanding is the newest frontier. Gemini 3 Pro's 90-minute video context window enables production use cases that were impossible in 2024. You can upload a training video, a recorded lecture, a surveillance feed, or a customer support call and ask questions: "At what timestamp does the speaker mention pricing?" or "Summarize the key points from this webinar." The model processes the entire video, maintains temporal coherence, and generates summaries or answers grounded in specific frames. This capability is production-ready but expensive. Video tokens cost 5 to 10 times more than text tokens. You need a clear ROI model before deploying video understanding at scale.

The decision to use multimodal models should be driven by task requirements, not technology novelty. If your inputs are text or can be converted to text without information loss, use a text-only model. If your task requires visual or audio reasoning that cannot be easily represented as text, evaluate multimodal models on your specific data. Do not assume a model's performance on academic benchmarks translates to your domain.

## Evaluating Multimodal Models: Domain-Specific Visual Benchmarks Are Mandatory

The standard approach to LLM evaluation—curating a labeled dataset, running inference, comparing outputs to ground truth—applies to multimodal models, but with added complexity. You need visual ground truth, not just textual labels. You need to account for ambiguity: two humans may describe the same image differently, both correctly. You need domain-specific benchmarks because general-purpose vision benchmarks do not predict performance on your task.

Academic vision benchmarks like ImageNet, COCO, or VQAv2 measure broad visual understanding but rarely correlate with production accuracy. ImageNet is object classification on natural images. COCO is object detection and segmentation. VQAv2 is visual question answering on everyday scenes. These benchmarks are valuable for model development but not for model selection in production. A model that scores 95% on ImageNet may score 60% on your medical imaging task. A model that excels at VQAv2 may fail on your document extraction task. You must build your own evaluation set.

The process starts with data collection. Gather 200 to 500 representative examples from your production domain. If you are building a receipt parsing system, collect 300 receipts in the formats and languages you expect to encounter. If you are building a product moderation system, collect 400 user-uploaded images spanning the categories you need to detect. If you are building a video summarization tool, collect 50 videos of the length and content type you will process. Do not curate an easy set. Include hard cases: blurry images, unusual layouts, ambiguous content, edge cases. Your eval set should reflect the messy reality of production data, not idealized samples.

Next, create ground truth. For classification tasks, this means human-labeled categories. For extraction tasks, this means manually extracted fields. For question answering tasks, this means human-written answers. For summarization tasks, this means human-written summaries. Ground truth creation is labor-intensive. Budget 5 to 20 hours of expert time per 100 examples, depending on task complexity. Use domain experts, not crowd workers, for specialized tasks. A radiologist should label medical images. A lawyer should label contract clauses. A mechanic should label defect photos. Generic crowd workers will introduce label noise that makes your eval unreliable.

Run each candidate model on your eval set. For classification, measure accuracy, precision, recall, and F1 per category. For extraction, measure field-level accuracy and full-document accuracy. For question answering, use exact match, F1, and human judgment. For summarization, use ROUGE scores and human preference ratings. Do not rely on a single metric. A model with high average accuracy may have terrible performance on your most important category. A model with high extraction accuracy may hallucinate fields that don't exist. Inspect failures manually. Understand why the model failed. Was the image too low resolution? Was the layout unusual? Did the model lack domain knowledge? Failure analysis reveals whether you need a different model or better input preprocessing.

Cost and latency matter as much as accuracy. A model that achieves 95% accuracy but costs three times more and takes twice as long may not be the right choice if a 92% accurate model is faster and cheaper. Calculate the total cost per processed item: API cost, retry cost for failures, manual review cost for low-confidence outputs. A more expensive model that reduces manual review may have lower total cost of ownership. Run latency benchmarks with production-like concurrency. How does latency degrade under load? Can you batch requests? Does the model support streaming outputs for long responses?

The multimodal evaluation challenge is that you cannot easily automate failure detection. With text-only models, you can compare outputs to ground truth programmatically. With vision or audio, outputs may be correct but phrased differently than ground truth. You need human-in-the-loop evaluation for meaningful accuracy numbers. Budget for this. Plan on 10 to 30 hours of evaluation time per model candidate. This investment prevents the failure pattern from the opening story: deploying a model based on vibes or vendor claims, then discovering accuracy problems in production.

## The Economics of Multimodal Tokens: Vision and Audio Are Expensive

Multimodal tokens cost significantly more than text tokens. The pricing structure reflects the computational expense of processing images, audio, or video. As of January 2026, image tokens typically cost 3 to 10 times more than text tokens, depending on resolution. Audio tokens cost 2 to 5 times more. Video tokens—only available on Gemini 3 Pro—cost 8 to 15 times more than text tokens. These cost differentials matter at scale. A system processing 100,000 images per day can easily rack up $5,000 to $20,000 per month in API costs. You need cost control strategies.

The first strategy is resolution control. Multimodal models tokenize images based on resolution. A 1024x1024 image might consume 800 tokens. A 512x512 image might consume 200 tokens. If your task does not require high resolution—you are classifying product photos, not reading fine print on labels—downscale images before sending them to the model. This reduces token count and cost proportionally. Test whether accuracy holds at lower resolutions. Often it does. A receipt parsing system might achieve 94% accuracy at 1024x1024 and 92% accuracy at 768x768, a trivial degradation for a 40% cost reduction.

The second strategy is selective multimodal routing. Not every input requires multimodal processing. If your system accepts both text and image inputs, route text inputs to a text-only model and image inputs to a multimodal model. If your system processes documents that may or may not contain images, use a lightweight classifier to detect whether images are present, then route accordingly. This reduces the percentage of requests hitting the expensive multimodal model. The routing logic adds complexity but the cost savings are immediate.

The third strategy is batching and caching. Some multimodal APIs support batching: you send multiple images in a single request and process them together, reducing per-image overhead. Batching is valuable for offline workflows—processing a batch of uploaded documents overnight—but not for real-time user-facing applications. Caching is more broadly applicable. If your system processes the same images repeatedly—user profile photos, product images, standard forms—cache the model's output and reuse it. Vision model outputs are deterministic for deterministic inputs. A cached response costs zero tokens.

The fourth strategy is model tiering. Use a cheap, fast multimodal model for initial triage, then route uncertain cases to a more expensive, accurate model. For content moderation, you might run all images through a lightweight classifier, flag ambiguous cases, then send only those to GPT-5 or Gemini 3 Pro for detailed analysis. This two-stage approach processes 90% of inputs with the cheap model and 10% with the expensive model, dramatically reducing average cost. The trade-off is added complexity and latency for the routed cases.

The fifth strategy is negotiation. If your volume is high—more than 10 million tokens per month—negotiate custom pricing with your model provider. OpenAI, Google, and Anthropic all offer volume discounts for enterprise customers. You may reduce per-token cost by 20% to 50% with a commitment contract. This requires forecasting your usage and locking in spend, but the savings are significant.

Do not let cost prevent you from using multimodal models where they add value. The failure pattern is over-optimizing for cost and under-delivering on accuracy. A system that saves $2,000 per month on API costs but requires $10,000 per month in manual review is a bad trade. Optimize the total cost of ownership, including manual review, error remediation, and customer support for failures. Often the more expensive model has lower total cost because it reduces downstream failure costs.

## Cross-Modal Reasoning: Answering Visual Questions with Textual Knowledge

Cross-modal reasoning is the ability to integrate information from different modalities to answer questions or perform tasks. A user shows you a photo of a plant and asks, "Is this toxic to dogs?" The answer requires identifying the plant species visually, then retrieving toxicology information from textual knowledge. A user uploads a photo of a car dashboard warning light and asks, "What does this mean and is it urgent?" The answer requires recognizing the symbol visually, mapping it to vehicle systems knowledge, and assessing urgency based on make and model information. These tasks cannot be solved by vision alone or language alone. You need a model that reasons across modalities.

Gemini 3 Pro is the strongest cross-modal reasoning model as of January 2026. Google's training data included vast amounts of interleaved text and images from the web, enabling the model to learn associations between visual patterns and textual concepts. The model can identify objects in images and retrieve encyclopedic knowledge about them. It can recognize landmarks and provide historical context. It can identify food dishes and suggest recipes or nutritional information. The cross-modal reasoning is not perfect—it hallucinates occasionally, providing plausible but incorrect information—but it is remarkably reliable for broad-domain knowledge.

GPT-5 also performs well on cross-modal reasoning, particularly for knowledge-intensive questions. OpenAI's training included Wikipedia, academic papers, and other structured knowledge sources alongside image data. The model can answer questions like "What architectural style is this building?" or "What species of bird is this?" with high accuracy. GPT-5 is slightly more conservative than Gemini 3 Pro, refusing to answer when uncertain rather than guessing. This conservatism is valuable in domains where false positives are costly.

Claude Opus 4.5 takes the most conservative approach. Anthropic trained the model to clearly separate what it can infer from the image alone versus what requires external knowledge. When you ask a question requiring knowledge beyond the image, Claude Opus 4.5 often responds, "I can see this is a plant with specific visual characteristics, but I cannot definitively identify the species or toxicity without additional information." This is frustrating in consumer applications where users expect confident answers, but valuable in professional applications where overconfidence is dangerous. A medical image analysis system should not guess. A legal document review system should not hallucinate clauses. Claude's conservatism is a feature, not a bug.

Llama 4 Vision struggles with cross-modal reasoning because it is a bolt-on system. The vision encoder can identify visual patterns, and the language model can retrieve textual knowledge, but the integration between them is shallow. The model can answer simple cross-modal questions—"What color is this object?" or "Is this a dog or a cat?"—but fails on knowledge-intensive questions requiring multi-hop reasoning. For production systems requiring cross-modal reasoning, Llama 4 Vision is not competitive with the commercial native multimodal models.

The evaluation challenge for cross-modal reasoning is that ground truth is often subjective or requires expert judgment. If a model identifies a plant as "likely a variety of pothos" when it is actually a philodendron, is that wrong? The two plants look similar and belong to the same family. The toxicity information is the same. The answer is functionally correct even if taxonomically imprecise. You need evaluation criteria that match your task requirements. For consumer applications, functional correctness may be sufficient. For scientific applications, precise species identification is mandatory. Define your accuracy thresholds based on downstream impact, not arbitrary precision standards.

## Specialized Vision Models vs Multimodal LLMs: When to Use Each

The decision between specialized vision models and multimodal LLMs depends on task complexity, data availability, and cost constraints. Specialized vision models—ResNets, Vision Transformers, CLIP, DINO, Segment Anything—are trained exclusively on visual data for specific tasks. They are faster, cheaper, and often more accurate than multimodal LLMs for narrow visual tasks. Multimodal LLMs are slower, more expensive, and more flexible. They handle open-ended visual reasoning without requiring task-specific training data. You need to match the tool to the task.

Use a specialized vision model when your task is well-defined, you have labeled training data, and accuracy matters more than flexibility. Image classification for content moderation, object detection for inventory management, defect detection for manufacturing QA, face recognition for security systems. These tasks have clear inputs, clear outputs, and existing labeled datasets. Fine-tuning a Vision Transformer on your data will yield higher accuracy and lower latency than prompting GPT-5 or Gemini 3 Pro. The specialized model is optimized for exactly this task. It does not waste capacity on language understanding or cross-modal reasoning. It is also cheaper to serve. Running inference on a ViT model costs fractions of a cent, compared to several cents for a multimodal LLM API call.

Use a multimodal LLM when your task requires language understanding, cross-modal reasoning, or zero-shot generalization. Document understanding, visual question answering, image captioning, scene description, visual search with natural language queries. These tasks are difficult to reduce to fixed categories. The outputs are variable and open-ended. You do not have large labeled datasets for every possible question or document type. Multimodal LLMs provide flexibility: you describe the task in a prompt, provide examples if needed, and the model generalizes. You can change the task or add new categories without retraining. This flexibility has cost: higher latency, higher per-request cost, and occasionally lower accuracy on narrow tasks.

The hybrid approach is often optimal. Use a specialized model for the bulk of requests and a multimodal LLM for edge cases. For e-commerce product categorization, you might fine-tune a ViT on your 50 most common categories, handling 85% of products with high accuracy and low cost. For the remaining 15%—new products, unusual items, ambiguous cases—route to Gemini 3 Pro for zero-shot classification. This two-tier system balances cost and flexibility. The specialized model keeps costs low. The multimodal LLM prevents manual review for edge cases.

Another hybrid pattern is using a specialized model for feature extraction and a multimodal LLM for reasoning. For medical imaging, you might run a specialized segmentation model to identify regions of interest in an X-ray, then pass those regions to Claude Opus 4.5 for diagnostic reasoning. The specialized model handles the low-level visual processing efficiently. The LLM handles the high-level reasoning where language and domain knowledge matter. This division of labor is computationally efficient and often more accurate than end-to-end LLM processing.

Cost modeling drives the decision. Calculate the total cost per processed item for each approach. A specialized model might cost 0.1 cents per image in compute. A multimodal LLM might cost 5 cents per image in API fees. If you process 1 million images per month, the specialized model costs $1,000 and the LLM costs $50,000. The cost difference is enormous. But if the specialized model requires $20,000 in labeling effort and two months of ML engineering time to fine-tune, and the LLM works zero-shot, the LLM may have lower total cost to value. Include development time, maintenance burden, and opportunity cost in your cost model.

## Video Understanding: Capability and Production Readiness

Video understanding emerged as a production-ready capability with Gemini 3 Pro's release in late 2025. The model can process up to 90 minutes of video in a single context window, maintaining coherent understanding across tens of thousands of frames. This unlocks use cases that were impractical in 2024: lecture summarization, surveillance analysis, training video Q&A, content moderation for long-form video, accessibility captioning for movies and webinars. The capability is real and impressive, but production deployment requires careful cost and latency management.

The core use case is video summarization. You upload a 45-minute webinar recording and prompt, "Summarize the key points and action items." Gemini 3 Pro processes the entire video, generates a structured summary with timestamps, and identifies speakers if multiple people are present. The output quality is high—comparable to a human note-taker—but the cost is steep. A 45-minute video at 30 frames per second is 81,000 frames. Gemini 3 Pro samples frames intelligently to reduce token count, but a typical 45-minute video still consumes 50,000 to 100,000 tokens. At current pricing, that is $15 to $30 per video. If you process 100 webinars per month, that is $1,500 to $3,000 monthly. This is viable for high-value content—executive training, customer onboarding, compliance—but prohibitive for user-generated content at scale.

The second use case is temporal question answering. You upload a surveillance video and ask, "At what time does a person enter the frame from the left?" or "How many vehicles pass through this intersection between 2pm and 3pm?" Gemini 3 Pro can answer these questions by analyzing the full video temporally. The accuracy depends on video quality and question clarity. For high-resolution, well-lit footage with clear questions, accuracy is 85% to 95%. For low-resolution, poorly lit, or ambiguous footage, accuracy drops to 60% to 70%. You need human review for critical applications—security monitoring, legal evidence review—but the model reduces manual review burden significantly.

The third use case is accessibility. Generating detailed audio descriptions for visually impaired users, creating accurate captions with speaker identification, summarizing visual content for screen readers. Gemini 3 Pro excels here. The model can describe on-screen actions, read text overlays, identify speakers by voice, and generate natural-sounding descriptions. This is production-ready for media companies, educational institutions, and enterprises with accessibility requirements. The cost is justified by legal compliance and audience reach.

The latency challenge is significant. Processing a 60-minute video takes 30 to 90 seconds on Gemini 3 Pro, depending on resolution and server load. This is acceptable for offline workflows—processing uploaded content asynchronously—but incompatible with real-time applications. You cannot use video understanding for live video analysis or instant user queries. The use case must tolerate delay. Batch processing, overnight jobs, and asynchronous pipelines are the deployment patterns.

The alternative to full-video LLM processing is hybrid pipelines. Use traditional computer vision models to extract keyframes, detect scene changes, identify objects, and track motion. Then use a multimodal LLM to reason about the extracted features. For a training video, you might use a scene detection model to segment the video into chapters, an OCR model to extract text from slides, a speech-to-text model to transcribe audio, then send the structured transcript and keyframes to GPT-5 for summarization. This hybrid approach is faster and cheaper than full-video LLM processing, but requires more engineering effort to build and maintain.

Video understanding is not yet mature for real-time or high-volume applications, but it is production-ready for selective, high-value use cases. If your business model supports $10 to $50 per video processing cost and you can tolerate 30 to 120 second latency, Gemini 3 Pro video understanding delivers real value. If your volume is high or latency requirements are tight, wait for the next generation of models or invest in hybrid pipelines.

## Latency and Throughput: Multimodal Models Are Slower

Multimodal models have higher latency than text-only models. Processing an image or audio clip requires encoding it into tokens, which adds computational overhead. For images, encoding latency is typically 200 to 500 milliseconds, depending on resolution. For audio, encoding latency is 100 to 300 milliseconds per minute of audio. For video, encoding latency can reach several seconds. This encoding latency is in addition to the model's inference latency. A text-only query to GPT-5 might return in 800 milliseconds. The same query with an attached image might take 1,400 milliseconds. The image encoding adds 600 milliseconds.

Throughput is also lower. Multimodal tokens consume more memory and compute per token than text tokens. A model serving text-only requests might handle 100 requests per second. The same model serving image-heavy requests might handle 20 requests per second. The throughput reduction depends on the ratio of multimodal to text tokens in each request. A request with a single small image and a short text prompt has minimal throughput impact. A request with ten high-resolution images and a long text prompt consumes 10x the resources of a text-only request.

The production implication is that multimodal systems require more careful capacity planning. You cannot assume linear scaling. A system that handles 1,000 text requests per second may only handle 200 multimodal requests per second on the same infrastructure. If you are using a third-party API, throughput is limited by rate limits. OpenAI, Google, and Anthropic all impose per-minute request limits and per-minute token limits. High-volume multimodal applications hit these limits quickly. You need rate limit management: request queuing, retry logic, fallback to alternative models. For latency-sensitive applications, you may need to overprovision API quota or negotiate higher limits with your provider.

The mitigation strategy is asynchronous processing where possible. If the user does not need an instant response—document processing, video summarization, batch image classification—process requests asynchronously. Queue the request, return a job ID, process in the background, notify the user when complete. This decouples user-facing latency from model latency. The user waits seconds for job submission, not minutes for processing. The system can batch requests, optimize throughput, and retry failures without blocking the user.

For real-time applications—live chat with image uploads, real-time visual question answering, accessibility tools—latency is non-negotiable. You need the fastest multimodal model available, which as of January 2026 is GPT-5 for image and audio, averaging 1,200 to 1,800 milliseconds for image queries. Gemini 3 Pro is slightly slower, 1,500 to 2,200 milliseconds. Claude Opus 4.5 is slower still, 2,000 to 3,000 milliseconds, reflecting its more deliberate reasoning process. If these latencies are unacceptable, you may need to use a specialized vision model with lower capability but sub-500 millisecond latency.

The next subchapter examines reasoning models and extended thinking: the cost, latency, and capability trade-offs when models spend extra tokens on chain-of-thought reasoning before answering.
