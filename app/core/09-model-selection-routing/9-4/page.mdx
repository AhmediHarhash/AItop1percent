# 9.4 â€” Model Migration Testing: Ensuring the New Version Does Not Break Your Product

In August 2025, a healthcare documentation platform migrated from Claude 3.5 Sonnet to Claude Opus 4.5 as part of a routine model upgrade. The platform transcribed patient-doctor conversations and generated clinical notes for 800 primary care physicians across 23 medical groups. The engineering team ran their standard eval suite, which measured accuracy of symptom extraction, medication identification, and diagnosis capture. The new model scored 96.2% accuracy compared to 95.8% on the old model. Latency was similar, cost per note increased by 18%, and the product team approved the tradeoff. They deployed to production using a canary rollout, monitoring error rates and latency. Everything looked stable. Three weeks later, they received an escalation from one of their largest medical groups. Doctors were complaining that the tone of the generated notes had changed. The notes were more formal, using clinical terminology that patients found intimidating when reviewing their records via the patient portal. The old model had written "patient reports occasional headaches," while the new model wrote "patient endorses intermittent cephalgia." Both were accurate. Both captured the same medical information. But one was readable by a patient with a high school education, and one required medical training. The eval suite had measured correctness but not tone. The migration had introduced a regression that affected user experience without affecting any monitored metric. They spent four weeks rewriting prompts to restore the original tone, then re-ran the migration. The root cause was not that they tested poorly. It was that they tested incompletely. They validated what the model extracted but not how it expressed it.

Model migration testing is not the same as feature testing. Feature testing validates that new code behaves correctly. Migration testing validates that new model behavior does not break existing product assumptions. Those assumptions include not just correctness but tone, format, verbosity, refusal patterns, edge case handling, and cost. A model can be objectively better on accuracy benchmarks and still regress on production requirements.

This subchapter covers the complete migration testing protocol, from running eval suites to conducting behavioral comparisons, identifying edge case failures, categorizing regression types, and compiling the migration test report that determines whether a model is safe to deploy.

## The Migration Eval Protocol

Migration testing begins with your existing eval suite, the same test cases and grading criteria you use for continuous model monitoring. For each candidate replacement model, you run the full eval suite using the same prompts, the same input examples, and the same automated grading functions. This is not a new eval. It is a **comparative eval**, designed to detect differences between the old model and the new model on a fixed set of tasks.

The eval protocol requires strict version control. You must use the exact same prompt templates that production currently uses, not improved versions you have been experimenting with. If production uses a three-shot prompt with specific examples, the eval must use that three-shot prompt with those specific examples. If you test the new model with a refined prompt while the old model ran with the legacy prompt, you are measuring prompt quality, not model quality. The goal is to isolate the model variable.

Run the eval on a statistically significant sample. For most systems, this means at least 500 test cases, ideally 1,000 or more. If your eval suite contains only 50 test cases, you lack the power to detect small but meaningful differences. A 2% accuracy delta on 50 cases could be noise. A 2% accuracy delta on 1,000 cases is a real signal. If you do not have a large eval set, you must either expand it before migration or accept higher uncertainty in your decision-making.

Log both the old model's outputs and the new model's outputs for every test case. Store these outputs in a structured format, typically JSON or a database table, with fields for test case ID, input, old model output, new model output, old model score, new model score, and any metadata like latency or token count. This log becomes the raw material for all downstream analysis.

Compute aggregate metrics: overall accuracy, per-category accuracy, average latency, average cost per request, refusal rate, and any task-specific metrics like F1 score or BLEU score. Compare the new model's metrics to the old model's metrics. A difference of less than 1% on core quality metrics is typically considered equivalent. A difference of 1% to 3% is marginal and requires product judgment. A difference of more than 3% is significant and often disqualifying unless offset by major improvements in cost or latency.

The eval protocol also requires **prompt compatibility testing**. Some model upgrades change how the model interprets prompt instructions. A system prompt that worked perfectly on GPT-4 Turbo might be misinterpreted by GPT-5 if GPT-5 has different instruction-following behavior. Test this by running a small subset of cases with the production prompt and checking for catastrophic failures like the model ignoring instructions, refusing valid requests, or producing outputs in the wrong format. If you detect incompatibility, you must either tune the prompt or disqualify the model.

## Behavioral Comparison Beyond Accuracy

Accuracy is necessary but insufficient. Two models can have identical accuracy scores and still behave differently in ways that matter to users. The migration test must include **behavioral comparison** across multiple dimensions.

**Tone** is the first behavioral dimension. Does the new model use the same level of formality, the same sentence structure, the same vocabulary? A model that shifts from conversational to clinical tone, or from concise to verbose, can degrade user experience even if correctness is preserved. To measure tone, you manually review a sample of 50 to 100 output pairs, comparing old and new. You look for systematic differences in word choice, sentence length, and register. If you find a consistent tone shift, you escalate it to the product team for a decision. Some tone shifts are acceptable or even desirable. Others are deal-breakers.

**Verbosity** is the second dimension. Does the new model use more tokens to express the same information? Increased verbosity affects cost, latency, and user attention. If the old model generated summaries averaging 120 tokens and the new model averages 180 tokens with no increase in information density, you are paying 50% more and delivering a worse user experience. Measure verbosity by computing average output token count across the eval set and comparing old to new. A 10% increase is tolerable. A 30% increase requires intervention, either by tuning the prompt to enforce brevosity or by selecting a different model.

**Format consistency** is the third dimension. Does the new model produce outputs in the same structure as the old model? If your system parses the model output to extract specific fields, any format change is a breaking change. The most common format regression is JSON schema drift. The old model reliably produced JSON with fields name, email, and phone. The new model sometimes produces name, email\_address, and phone\_number. Your parser breaks. To test format consistency, run your production parsing logic against the new model's outputs and measure the parse failure rate. If the rate is above 1%, you have a format regression and must either fix the prompt or update the parser.

**Refusal rate** is the fourth dimension. Does the new model refuse to answer queries that the old model handled successfully? Model providers frequently update safety filters and content policies between versions. A model upgrade can suddenly start refusing requests that involve medical terms, financial data, or politically sensitive topics, even when those requests are legitimate. Measure refusal rate by counting how often the new model returns a refusal response, typically identified by phrases like "I cannot assist with that" or "I am not able to provide that information." If the refusal rate increases by more than 0.5%, investigate why. Sometimes the refusals are correct and the old model was too permissive. Sometimes the refusals are false positives and the new model is too conservative.

**Latency distribution** is the fifth dimension. Does the new model have the same latency profile, or does it introduce tail latency? Measure median latency, p95 latency, and p99 latency. A model that has the same median but a higher p99 creates a worse user experience for outlier requests. Latency regressions are common when migrating to larger models, even within the same provider. GPT-5 is slower than GPT-4.5 on average. Claude Opus 4.5 is slower than Claude 3.5 Sonnet. If your product has strict latency requirements, a 100-millisecond increase in p95 latency might be unacceptable.

**Cost distribution** is the sixth dimension. Does the new model cost the same per request, or does cost vary more widely? Some models have highly variable token usage depending on task complexity. The old model used 500 to 800 tokens per request. The new model uses 400 to 1,200 tokens per request, with the same median but much higher variance. This variance makes cost forecasting harder and can blow your budget on edge cases. Measure cost per request across the eval set and compare the distributions, not just the averages.

## Edge Case Testing: Where Models Break Differently

Aggregate metrics measure average performance. Edge case testing measures how models fail. Every model has weaknesses. The question is whether the new model's weaknesses align with your product's edge cases.

**Adversarial input testing** probes how the model responds to malformed, ambiguous, or manipulative inputs. If your product allows user-generated input, users will eventually send queries that are deliberately confusing, contradictory, or designed to elicit inappropriate responses. Test the new model with a curated set of adversarial cases and compare its behavior to the old model. Common adversarial patterns include prompt injection attempts, requests with conflicting instructions, inputs with extreme length, and queries containing special characters or formatting tricks. If the new model is more vulnerable to prompt injection, that is a security regression. If it is less vulnerable, that is a security improvement.

**Boundary condition testing** probes how the model handles inputs at the edges of your expected range. If your product processes documents, test with a one-word document and a 10,000-word document. If your product handles dates, test with dates in the year 1900 and dates in the year 2100. If your product supports multiple languages, test with rare languages or code-switching between languages. Models often behave unpredictably at boundaries. The old model might gracefully degrade on a 10,000-word document, producing a coherent if incomplete summary. The new model might choke, producing repetitive or nonsensical output. These failures do not show up in average-case evals.

**Ambiguity testing** probes how the model handles inputs with multiple valid interpretations. If a user asks "what is the status of my order" without specifying which order, does the new model ask for clarification, make an assumption, or refuse to answer? Compare the disambiguation strategy to the old model. If the old model asked for clarification and the new model makes an assumption, you have introduced a potential error mode. If the old model refused and the new model asks for clarification, you have improved the user experience.

**Domain-specific edge cases** are the most critical. Every product has quirks. A legal contract analyzer must handle archaic language, nested clauses, and contradictory terms. A medical transcription tool must handle accents, abbreviations, and overlapping speech. A financial report summarizer must handle tables, footnotes, and non-GAAP metrics. Your eval set should include a subset of cases that represent these domain-specific challenges. Test the new model on this subset and compare failure modes. If the new model struggles with archaic legal language but the old model handled it well, you have a domain-specific regression.

## Regression Categories: What Can Go Wrong

Migration testing produces a large volume of comparison data. To make sense of it, you need a **regression taxonomy** that categorizes the types of failures you observe. The five main categories are quality, format, safety, latency, and cost.

**Quality regression** means the new model produces less accurate or less useful outputs than the old model. This is measured by your core eval metrics, typically accuracy, F1 score, or task success rate. A quality regression of more than 3% on tier-one systems is typically disqualifying. A regression of 1% to 3% requires product judgment and might be acceptable if offset by cost or latency improvements. A regression of less than 1% is usually noise.

**Format regression** means the new model produces outputs that break your downstream systems. This includes JSON schema changes, missing fields, unexpected field types, or changes to delimiters and separators. Format regressions are often binary: either your parser works or it does not. If you observe a format regression on more than 5% of test cases, you must either fix the prompt, update the parser, or disqualify the model.

**Safety regression** means the new model is more likely to produce harmful, biased, or policy-violating outputs, or conversely, more likely to refuse legitimate requests. Safety regressions are evaluated using a dedicated safety eval set, typically 100 to 500 cases designed to probe for stereotypes, toxicity, misinformation, and refusals. If the new model increases toxicity or bias, that is a safety regression. If it increases refusals on legitimate queries, that is a usability regression that looks like a safety issue.

**Latency regression** means the new model is slower than the old model, either on average or on the tail. A 10% increase in median latency is often acceptable. A 50% increase in p99 latency is often disqualifying for user-facing systems. Latency regressions are particularly painful because they are hard to fix. You cannot tune a prompt to make a model faster. Your options are to choose a different model or to re-architect the product to hide latency, for example by streaming outputs or moving to asynchronous processing.

**Cost regression** means the new model costs significantly more per request, either because it uses more tokens or because the provider charges a higher rate. A 20% cost increase might be acceptable if quality improves. A 100% cost increase is rarely acceptable unless you redesign the feature to use fewer requests. Cost regressions are evaluated by computing cost per request for every eval case and comparing the distributions. If the cost increase is concentrated in a small fraction of edge cases, you might be able to mitigate it by routing those cases to a cheaper model.

## The Migration Test Report

All migration testing results are compiled into a **migration test report**, a structured document that summarizes the findings and provides a recommendation. The report has a standard structure used across all migrations, ensuring consistency and making it easy for stakeholders to compare migrations over time.

The first section is the **executive summary**, a one-paragraph overview of the recommendation. It states the candidate model, whether it is approved or rejected for migration, and the primary reason. For example: "Claude Opus 4.5 is approved for migration. Quality improved by 2.1%, cost increased by 18%, latency remained stable. No format or safety regressions detected. Recommend proceeding to shadow deployment."

The second section is the **quantitative comparison table**, showing the old model and new model side by side across all key metrics. The table includes accuracy, refusal rate, average tokens per request, median latency, p95 latency, p99 latency, and cost per 1,000 requests. Each metric includes the old value, the new value, and the delta. This table allows stakeholders to quickly assess tradeoffs.

The third section is the **behavioral findings**, a narrative description of tone, verbosity, and format differences. This section is based on manual review and includes specific examples. For instance: "Tone shifted from conversational to formal in 23% of reviewed cases. Example: old model output was 'The system is down right now,' new model output was 'The system is currently unavailable.' Format consistency was 98.7%, with failures concentrated in cases involving nested lists."

The fourth section is the **edge case summary**, listing the types of edge cases tested and any failures observed. This section highlights domain-specific risks and adversarial robustness. For example: "Tested 50 adversarial prompt injection attempts. Old model failed 8, new model failed 3. Tested 40 rare language inputs. Old model succeeded on 32, new model succeeded on 35. Tested 20 extreme-length documents. Old model degraded gracefully, new model produced repetitive output on 4 cases."

The fifth section is the **regression log**, a table of all detected regressions with severity ratings. Each regression is classified as critical, major, minor, or cosmetic. Critical regressions block migration. Major regressions require mitigation or product approval. Minor regressions are noted but do not block. Cosmetic regressions are documented for awareness but have no impact.

The sixth section is the **cost impact projection**, translating the per-request cost delta into a monthly budget impact based on current traffic volume. If the new model costs 22 cents per request instead of 18 cents, and you handle 2 million requests per month, the monthly cost increases by 80,000 dollars. This projection is essential for getting finance and product approval.

The final section is the **recommendation and next steps**. If the model is approved, next steps are typically shadow deployment followed by canary rollout. If the model is rejected, next steps are either to test a different candidate model or to tune the prompt and re-test. If the model is conditionally approved, next steps include specific mitigation work like updating parsers or adjusting prompts to fix tone.

The report is reviewed by the migration lead, the product manager, and the engineering lead. For tier-one systems with high traffic or high business criticality, the report may also be reviewed by finance and executive leadership. The approval decision is documented and attached to the report for audit purposes.

## Statistical Significance: How Many Test Cases You Need

A common question in migration testing is how many test cases are enough. The answer depends on the magnitude of difference you need to detect and the confidence level you require.

If you want to detect a 5% quality difference with 95% confidence, you need approximately 400 test cases. If you want to detect a 2% difference with 95% confidence, you need approximately 2,500 test cases. If you want to detect a 1% difference, you need more than 10,000 test cases. These numbers assume a binomial distribution and a two-tailed test.

Most organizations operate with eval sets of 500 to 2,000 cases, which means they can reliably detect differences of 2% to 5%. Smaller differences are considered noise. If your eval set has only 100 cases, you can only detect differences of 10% or more, which is too coarse for most migration decisions. If you are in this situation, you must either expand your eval set or accept higher risk in migration decisions.

**Stratified sampling** improves statistical power. If your product handles multiple types of tasks, ensure that your eval set includes a representative sample of each type. If 60% of production traffic is summarization and 40% is extraction, your eval set should have a similar distribution. This ensures that you do not miss a regression that affects only one task type. Stratified sampling also allows you to compute per-category metrics and detect cases where the new model improves on one task but regresses on another.

**Repeated trials** reduce variance. If your model produces non-deterministic outputs due to sampling temperature or random seed, run each test case multiple times and average the results. For most migration testing, three to five trials per case is sufficient. This is expensive in API costs but necessary for high-stakes migrations.

## The Close Enough Trap

The most insidious failure mode in migration testing is the **close enough trap**: the new model's metrics look similar to the old model's metrics, so you assume user experience will be similar, but it is not.

A financial services company migrated from GPT-4.5 to GPT-5 for customer support email responses. The eval suite measured response accuracy, tone appropriateness, and issue resolution rate. GPT-5 scored 94.3% compared to GPT-4.5's 94.1%. Latency was nearly identical. Cost increased by 12%, which was approved. They deployed to production. Within a week, customer satisfaction scores dropped by 8 points. The issue was subtlety. GPT-5 was technically correct but less empathetic. When a customer complained about a billing error, GPT-4.5 would say "I apologize for the confusion, let me look into this for you." GPT-5 would say "I have reviewed your account and the charge is correct per the terms of service." Both were accurate. One preserved customer trust. One did not.

The eval suite did not measure empathy because empathy was not in the grading rubric. The team assumed that tone appropriateness covered it, but their tone grading was binary: professional or not professional. It did not distinguish between professional-but-cold and professional-and-warm. They learned this only after production deployment and user complaints.

To avoid the close enough trap, you must **manually review outputs** even when metrics pass. Select a random sample of 100 to 200 test cases and read both the old and new outputs side by side. Look for qualitative differences that metrics miss: empathy, clarity, helpfulness, confidence. If you notice a pattern, escalate it even if the metrics show no regression. Metrics measure what you thought to measure. Manual review measures what you forgot to measure.

You must also **monitor user feedback** during canary rollout with heightened sensitivity. If customer satisfaction scores, support ticket volume, or user complaints increase during canary, investigate immediately even if your technical metrics are stable. Users are the ultimate eval. If they say the experience got worse, they are right, even if your eval suite disagrees.

## When Migration Testing Says No

Not every migration should proceed. Sometimes the testing reveals that the new model is not ready, not suitable, or not worth the cost. The decision to abort a migration is uncomfortable because it feels like wasted effort, but it is the correct decision when the data supports it.

If the new model has a quality regression of more than 3% on tier-one systems, you abort. If it has format regressions that affect more than 5% of cases and would require rewriting your parser, you abort. If it doubles your cost with no corresponding quality improvement, you abort. If it introduces safety regressions, you abort. If it has tail latency regressions that violate product SLOs, you abort.

Aborting does not mean you are stuck on the old model forever. It means you test a different candidate. If you were testing GPT-5 and it failed, test Claude Opus 4.5. If that fails, test Gemini 3 Pro. If all major providers' models fail, you have two options: tune your prompts to make one of the candidates work, or negotiate an extension with your current provider to delay the deprecation deadline. Most providers will grant 30 to 60 day extensions if you have a documented technical blocker and a plan to resolve it.

The worst outcome is not aborting a failing migration. The worst outcome is proceeding with a migration despite test failures because the deprecation deadline is approaching and you feel pressure to ship. This is how quality regressions reach production. This is how you lose user trust. This is how you spend the next three months in reactive firefighting mode. The discipline to abort a failing migration is what separates mature engineering teams from reactive ones.

Model migration testing is the validation gate that prevents production regressions, but even successful migrations must be executed carefully in production, which requires the deployment strategies and rollout protocols covered in the next subchapter on shadow deployments and canary rollouts.

