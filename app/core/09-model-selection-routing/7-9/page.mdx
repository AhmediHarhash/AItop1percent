# 7.9 — When to Distill Instead of Fine-Tune: Synthetic Data from Frontier Models

In mid-2025, a healthcare technology company spent fourteen weeks and $340,000 collecting 25,000 human-labeled medical triaging examples to fine-tune a model that would classify patient inquiries by urgency. They hired medical professionals to read anonymized patient messages and assign urgency scores. The annotation process was slow, expensive, and inconsistent. Different annotators disagreed on borderline cases. Quality control required multiple passes. By the time they had enough data to fine-tune, three months had passed and their launch timeline had slipped twice.

A competitor launched first using a different approach. They spent four days and $8,000 generating synthetic training data by prompting Claude Opus 4.5 with diverse patient inquiry scenarios and capturing the model's urgency assessments. They filtered the outputs for quality, verified a sample against medical guidelines, and used the synthetic dataset to distill the frontier model's behavior into a smaller, faster model. Their fine-tuned model achieved comparable accuracy to the human-labeled version at four percent of the cost and five percent of the timeline. The healthcare company had assumed human labels were inherently superior. They were wrong. For many tasks, **distillation** produces training data faster, cheaper, and more consistent than human annotation.

Distillation is the process of using a large, capable frontier model to generate training examples that teach a smaller model how to perform a specific task. Instead of collecting human labels, you prompt a frontier model with diverse inputs and use its outputs as your training labels. The smaller model learns to mimic the frontier model's behavior on your task. This approach has become one of the most powerful techniques in production AI systems, yet many teams still default to human annotation without considering whether distillation would serve them better.

## The Fundamental Premise of Distillation

Distillation rests on a simple observation: if a frontier model already performs your task well, you do not need humans to label examples. You need the frontier model to label examples. The frontier model becomes your annotator. You design prompts that make the frontier model produce high-quality outputs for your task, run those prompts across a diverse set of inputs, capture the outputs, and use the input-output pairs as training data for a smaller model.

This works because frontier models encode vast amounts of knowledge and reasoning capability learned from massive pretraining datasets. When you prompt a frontier model well, it can perform complex tasks that would require expert human judgment. Medical triaging, legal document classification, content moderation, sentiment analysis, technical support routing—these tasks require expertise, but if a frontier model can already do them with the right prompt, that frontier model can generate thousands of labeled examples in hours instead of weeks.

The smaller model you train on synthetic data does not need to match the frontier model's full capabilities. It only needs to replicate the frontier model's performance on your specific task. This is called **task-specific distillation**. The smaller model is not learning everything the frontier model knows. It is learning the patterns in how the frontier model responds to your particular inputs. This focused learning is what makes distillation efficient. You are not compressing general intelligence into a small model. You are transferring specialized behavior.

The quality of distilled training data depends entirely on the quality of the frontier model's outputs. If the frontier model produces correct, consistent, policy-compliant outputs for your task, the synthetic data will be high quality. If the frontier model hallucinates, produces inconsistent outputs, or misunderstands your task, the synthetic data will be flawed. This means distillation requires the same careful prompt engineering and output validation you would apply to any production use of a frontier model. The difference is that you are running the frontier model offline to generate training data, not online to serve user requests.

## When Distillation is Better Than Human Annotation

Distillation outperforms human annotation in several scenarios. First, when the task requires consistency more than creativity. Human annotators disagree. Even with detailed guidelines, annotation consistency across thousands of examples is hard to achieve. Different annotators interpret edge cases differently. Annotators drift over time as they develop personal heuristics. A frontier model, given the same prompt and temperature settings, produces far more consistent outputs. If your task has clear criteria and you can encode those criteria in a prompt, distillation will give you more uniform training data than a team of human annotators.

Second, when the task requires expertise that is expensive or scarce. The healthcare company needed medical professionals to label urgency. Medical professionals are expensive and have limited availability. A frontier model does not charge by the hour. It does not take weekends off. If a frontier model can replicate expert judgment on your task—verified through sampling and eval—you bypass the cost and scheduling challenges of hiring domain experts for annotation. This does not mean you eliminate domain experts entirely. You still need them to design the prompts, define quality criteria, and validate samples of the synthetic data. But you need hours of expert time, not weeks.

Third, when the task operates in a domain where frontier models already have strong performance. Frontier models in 2026 excel at language understanding, reasoning over text, classification, summarization, and extraction. If your task fits these categories, distillation is likely viable. Frontier models are weaker at highly specialized technical domains with limited public training data, tasks requiring real-time sensory input, or tasks requiring human cultural judgment that varies by community. Distillation works when the frontier model already knows how to do your task. It fails when the frontier model lacks the necessary knowledge or capability.

Fourth, when speed matters more than perfection. Collecting human-labeled data takes weeks or months. Generating synthetic data takes days. If you need to launch quickly, test a hypothesis, or iterate on your model, distillation gives you a training dataset in a fraction of the time. You can generate ten thousand examples overnight. You can run experiments with different prompt variations and compare the resulting models. This speed enables a different development cycle where you test ideas empirically instead of debating them theoretically.

Fifth, when the task has high labeling costs but low inference costs. Human annotation is expensive per example. If you need fifty thousand labeled examples to fine-tune a model that will then handle millions of inferences, the economics favor distillation. You pay frontier model inference costs once during synthetic data generation, then deploy a smaller, cheaper model for production. The upfront cost of synthetic data is lower than the cost of human labeling, and the ongoing cost of running the distilled model is lower than running the frontier model in production.

## The Synthetic Data Pipeline

The distillation pipeline has five stages: input generation, frontier model prompting, output filtering, quality verification, and training. Each stage requires deliberate design. Input generation is the process of creating a diverse set of examples that cover the range of inputs your production system will encounter. If you are building a customer support classifier, your inputs should include common questions, edge cases, ambiguous phrasing, multilingual inputs if relevant, and adversarial examples. The diversity of your synthetic training data determines how well the distilled model generalizes. A narrow input set produces a brittle model.

You can generate inputs manually by brainstorming examples, programmatically by sampling from production logs if you have them, or synthetically by prompting a frontier model to generate diverse inputs. Yes, you can use a frontier model to generate both inputs and outputs. For example, you might prompt Claude Opus 4.5 to generate one hundred diverse customer support questions, then prompt it again to classify each question by topic. The input generation prompt should specify the dimensions of diversity you care about: topic, tone, complexity, edge cases, ambiguity. The more explicit you are, the more useful the synthetic inputs.

Frontier model prompting is where you apply the same prompt engineering rigor you would use in production. Your prompt should define the task, provide examples if needed, specify output format, and include any policies or constraints. The prompt must be good enough that the frontier model's outputs are suitable training labels. This means you iterate on the prompt just as you would for a production system. You test it on sample inputs, review the outputs, identify failure modes, and refine the prompt. The quality of your synthetic data is capped by the quality of your prompt.

Output filtering removes low-quality examples before training. Not every frontier model output will be suitable as a training label. The frontier model might refuse to answer, produce an ambiguous response, hallucinate, or output malformed data. You filter these out. Filtering can be rule-based—discard outputs that do not match the expected format—or model-based—use a separate model or prompt to assess output quality. For example, if you are generating sentiment labels, you might filter out any outputs where the frontier model expresses uncertainty or hedges its answer. You want clear, confident labels.

Quality verification is the process of validating that your synthetic data actually represents the task correctly. You do not need to verify every example, but you must verify a representative sample. Domain experts review a few hundred examples and check that the frontier model's outputs align with the ground truth. If you are distilling medical triaging behavior, medical professionals review a sample and confirm the urgency labels are correct. If the sample pass rate is below your threshold—typically 95 percent or higher—you revise your prompt or switch frontier models. If the sample pass rate is acceptable, you proceed to training with confidence that the synthetic data is sound.

Training is identical to training on human-labeled data. You use the synthetic input-output pairs as your fine-tuning dataset. The training process does not care whether labels came from humans or models. The quality of the resulting model depends on the quality of the labels, not their source. After training, you evaluate the distilled model on a held-out eval set—ideally one with human-verified ground truth—to confirm it generalizes beyond the synthetic training data. If eval performance is strong, you deploy. If not, you investigate whether the problem is insufficient training data, low-quality synthetic labels, or mismatch between synthetic data and production distribution.

## Quality of Synthetic Data Versus Human-Labeled Data

The question teams always ask is whether synthetic data is as good as human-labeled data. The answer depends on the task. For tasks where frontier models perform at or above human expert level, synthetic data is often better. The frontier model is more consistent than humans. It does not fatigue. It does not misread instructions. It applies the same reasoning to every example. A well-prompted Claude Opus 4.5 or GPT-5.1 produces labels with less variance than a team of contractors working from a written rubric.

For tasks where frontier models are competent but not expert-level, synthetic data quality depends on prompt quality and verification rigor. If you can prompt the frontier model to produce correct outputs 95 percent of the time and you filter out the remaining five percent, your synthetic data is high quality. If the frontier model only achieves 80 percent accuracy on your task, synthetic data will introduce noise into your training set. This noise can hurt model performance, especially if the errors are systematic rather than random. Random errors average out during training. Systematic errors teach the model incorrect patterns.

For tasks requiring human judgment that varies by culture, community, or personal values, synthetic data is weaker. A frontier model has statistical tendencies learned from its training data, but it does not represent the specific values of your user community. If your task is content moderation and what counts as acceptable content varies by community norms, human annotators from that community produce more relevant labels than a frontier model trained on broad internet data. This does not mean distillation is impossible, but it means you must validate that the frontier model's outputs align with your community's standards.

The empirical test is to compare models trained on synthetic data versus human-labeled data on the same eval set. Teams that run this experiment often find that synthetic data models perform within a few percentage points of human-labeled models, and sometimes outperform them. The healthcare company that initially dismissed distillation eventually ran the comparison. Their distilled model achieved 94 percent accuracy on a human-verified eval set. Their human-labeled model achieved 92 percent. The distilled model was not just faster and cheaper to produce—it was more accurate, because the frontier model's labels were more consistent than the labels from their annotation team.

## Legal Considerations for Using Model Outputs as Training Data

Using a frontier model's outputs as training data raises legal questions. The most common concern is whether model outputs are protected by copyright or license restrictions. In 2026, most major model providers allow you to use outputs from their models for any purpose, including as training data, as long as you comply with their terms of service. OpenAI, Anthropic, Google, and Meta all permit this use case. You should verify the specific terms of the model you are using, but the default assumption is that you own the outputs and can use them as you see fit.

The second concern is whether using model outputs to train a competing model violates any agreements. If you use GPT-5.1 to generate synthetic data that you then use to fine-tune a Llama 4 model, you are not violating OpenAI's terms as of 2026. The terms prohibit using OpenAI's models to develop competing foundation models—you cannot use GPT outputs to pretrain a rival general-purpose LLM—but they do not prohibit using outputs to fine-tune a task-specific model for your own use. This distinction matters. Training a customer support classifier on GPT-generated data is allowed. Training a new general-purpose chatbot on GPT-generated data is not.

The third concern is data provenance and auditability. If you are operating in a regulated industry, you may need to demonstrate that your training data was collected and labeled appropriately. Synthetic data pipelines are easier to audit than human annotation pipelines because they are fully automated and reproducible. You can show exactly which prompts generated which outputs, which filters were applied, which examples were included in training. This audit trail is cleaner than tracking which human annotators labeled which examples and whether they followed guidelines. Regulators in 2026 are increasingly comfortable with synthetic data as long as the generation process is documented and validated.

The fourth concern is bias and fairness. Frontier models inherit biases from their training data. If you distill a frontier model's behavior into a smaller model, you also distill its biases. This is not unique to synthetic data—human annotators also have biases—but it means you must evaluate your distilled model for fairness just as you would any model. The advantage of synthetic data is that you can control the input distribution to ensure balanced representation. If you are distilling a hiring resume screener, you can intentionally generate synthetic resumes across demographic groups to ensure the training data is balanced. This level of control is harder to achieve with real-world human-labeled data.

## Cost Comparison: Synthetic Data Generation Versus Human Labeling

The economics of distillation are compelling. Human annotation costs range from one dollar to fifty dollars per example depending on task complexity and annotator expertise. Annotating ten thousand examples for a complex task might cost $200,000 and take eight weeks. Generating ten thousand synthetic examples with a frontier model costs the inference price of the model times the number of examples. In 2026, Claude Opus 4.5 costs approximately three cents per thousand input tokens and fifteen cents per thousand output tokens. If your average example is 200 input tokens and 100 output tokens, you are paying roughly 0.002 cents per example, or $20 for ten thousand examples. Even accounting for prompt iteration, filtering, and overhead, synthetic data generation is one to two orders of magnitude cheaper than human annotation.

The time savings are even more dramatic. Human annotation is bottlenecked by annotator availability and throughput. An annotator might label fifty to two hundred examples per day depending on complexity. Ten thousand examples require weeks of calendar time even with multiple annotators working in parallel. Synthetic data generation is bottlenecked only by API rate limits and your willingness to pay for compute. You can generate ten thousand examples in hours or overnight. For teams operating on startup timelines, this time compression is worth more than the cost savings.

The cost comparison shifts if you already have human-labeled data. If you have collected ten thousand human-labeled examples for a previous project and the task is similar, reusing that data is cheaper than generating synthetic data. But if you are starting from zero, or if your task requirements have changed and your old labels are no longer valid, synthetic data is almost always the faster and cheaper path. The exception is tasks where frontier models perform poorly and you need true expert human judgment. In those cases, you pay for human annotation because the alternative does not work.

The long-term cost structure also matters. If you are building a continuous fine-tuning pipeline where you re-train your model monthly as requirements evolve, synthetic data becomes even more valuable. You can regenerate fresh training data for each iteration without recruiting and onboarding new annotators. You can experiment with different prompt strategies and compare the resulting models. This flexibility accelerates your iteration speed and reduces coordination overhead.

## When Distillation Produces Better Results Than Fine-Tuning on Human Data

There are scenarios where distillation outperforms human-labeled fine-tuning not just on cost or speed, but on model quality. The first scenario is when human annotators lack access to the full context that the frontier model has. For example, if you are building a model to summarize technical documentation, the frontier model has read vast amounts of technical writing during pretraining. It knows common summarization patterns, technical jargon, and how to prioritize information. Human annotators, even technical experts, might not have the breadth of exposure that the frontier model has. The distilled model inherits the frontier model's broad knowledge, producing summaries that are more consistent with professional norms.

The second scenario is when the task requires reasoning across multiple steps and human annotators shortcut the reasoning. Consider a legal contract review task where the model must identify clauses that conflict with company policy. A human annotator might rely on intuition or pattern matching to flag risky clauses. A well-prompted frontier model will explicitly reason through each clause against the policy criteria. When you distill this reasoning into a smaller model, the smaller model learns a more systematic approach than it would from human labels that do not expose the reasoning process. You can enhance this by prompting the frontier model to output reasoning steps along with the final label, then training the smaller model to mimic both the reasoning and the conclusion.

The third scenario is when the task is new and human annotators do not yet have established intuition. If you are launching a new product category and need to classify user intents that did not exist six months ago, human annotators are learning on the job. Their early labels will be inconsistent as they develop intuition. A frontier model prompted with clear criteria will produce more stable labels from the start. By the time human annotators have developed consistent intuition, you have already trained and deployed a distilled model. You can always collect human labels later to validate or improve the model, but distillation gives you a functional model immediately.

The fourth scenario is when you need to iterate quickly on task definitions. Early in a project, you might not be sure exactly how to frame the task. Should customer support inquiries be classified into five categories or ten? Should urgency be a binary label or a three-point scale? With human annotation, changing the task definition means re-annotating thousands of examples. With distillation, you change the prompt and regenerate synthetic data in hours. This enables rapid experimentation. You can train models with different task framings, evaluate them, and choose the framing that works best. The ability to iterate on task definitions without re-hiring annotators is a strategic advantage.

The fifth scenario is when you need training data in multiple languages. Human annotation costs scale linearly with the number of languages. If you need labeled data in twenty languages, you need annotators fluent in twenty languages. Frontier models in 2026 handle dozens of languages competently. You write one prompt in English, translate it or write equivalents in other languages, and generate synthetic data across all target languages. The cost is the same regardless of language count. The distilled model learns multilingual task behavior from the start. This approach does not work for all languages—frontier models are weaker in low-resource languages—but for the thirty or forty most common languages, distillation is far more efficient than multilingual human annotation.

## The Distillation Workflow in Practice

Teams that adopt distillation as a standard practice develop a repeatable workflow. First, they define the task and success criteria using the problem framing discipline from Section 2. They write down what the model should output given various inputs, what edge cases exist, and what policies apply. This definition becomes the foundation of the distillation prompt. Second, they select a frontier model known to perform well on similar tasks. In 2026, Claude Opus 4.5, GPT-5.1, and Gemini 3 Pro are common choices. The selection depends on task type—legal reasoning might favor Claude, creative generation might favor GPT, factual accuracy might favor Gemini.

Third, they write and iterate on the distillation prompt. The prompt includes task definition, output format, examples if helpful, and any constraints or policies. They test the prompt on a small set of diverse inputs and review outputs for quality. If outputs are inconsistent or incorrect, they refine the prompt. This iteration process typically takes one to three days. Fourth, they generate a large input set. If they have production logs, they sample from those logs. If not, they generate synthetic inputs using a separate prompt or write inputs manually. The goal is coverage of the production distribution.

Fifth, they run the distillation prompt across all inputs, collect outputs, and apply filters to remove low-quality examples. Filtering might remove five to fifteen percent of outputs depending on task difficulty. Sixth, they verify a sample of the synthetic data with domain experts. The sample size is typically three hundred to one thousand examples. If the verification pass rate is above 95 percent, they proceed. If not, they revise the prompt or reconsider whether distillation is appropriate for this task. Seventh, they train a smaller model on the synthetic data using standard fine-tuning techniques. Eighth, they evaluate the distilled model on a held-out set with human-verified ground truth. If performance meets the target, they deploy. If not, they diagnose whether the issue is prompt quality, input diversity, or model capacity.

This workflow is faster and cheaper than human annotation workflows, but it is not effortless. It requires prompt engineering skill, understanding of the frontier model's strengths and weaknesses, and rigorous eval discipline. Teams that treat distillation as a shortcut and skip the verification or eval steps produce low-quality models. Teams that treat distillation as an engineering process with clear quality gates produce models that rival or exceed human-labeled models at a fraction of the cost.

## When Distillation is Not the Right Choice

Distillation is not appropriate for every task. If the frontier model cannot perform your task competently, distillation will not work. You cannot distill capabilities the frontier model does not have. If you need a model to analyze proprietary data formats that were not in the frontier model's training data, distillation fails. If you need a model to apply highly specialized domain knowledge that frontier models lack—obscure medical subspecialties, proprietary financial instruments, niche legal jurisdictions—human expert annotation is necessary.

Distillation is also inappropriate when the task requires subjective human judgment that varies by individual or community. If you are building a content recommendation system and relevance depends on personal taste, distilling a frontier model's generic preferences will not capture user-specific preferences. You need real user feedback. If you are moderating content for a specific online community and the standards differ from mainstream norms, distilling a frontier model trained on general internet data will misalign with community values. You need annotators from that community.

Distillation is risky when you cannot verify the quality of synthetic data. If you lack domain experts to review a sample, or if the task is so new that no one knows what correct outputs look like, synthetic data might embed incorrect patterns that you will not detect until production. In these cases, starting with a small human-labeled dataset to establish ground truth, then using distillation to scale up, is safer than relying purely on synthetic data from the start.

Finally, distillation is less valuable when you need transparency into how each training example was labeled. Some regulatory contexts require audit trails that trace each label back to a human decision-maker. Synthetic data does not provide this. You can audit the prompt and the model version, but you cannot point to a specific human who made a specific labeling decision. If your compliance framework requires human accountability for every label, distillation introduces risk.

Despite these limitations, distillation has become the default starting point for fine-tuning in many organizations. The speed, cost, and consistency advantages are too significant to ignore. Teams evaluate whether distillation is viable for their task before committing to human annotation pipelines. In most cases, distillation works, and the exceptions are narrower than most teams initially assume. The healthcare company that spent fourteen weeks on human annotation now starts every fine-tuning project with distillation and only falls back to human labels when distillation proves insufficient.

The next question is how to maintain fine-tuned models as your data and requirements evolve, which brings us to continuous fine-tuning and the operational discipline of keeping models current.
