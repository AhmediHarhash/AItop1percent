# 3.13 — Router Failure Modes: Misclassification, Looping, and Cold-Start Problems

In August 2025, a legal document processing company discovered that their routing system had been burning through $47,000 per month in unnecessary GPT-5.2 calls. The router was designed to send complex contract analysis queries to the expensive frontier model and simple metadata extraction tasks to a cheaper Llama 4 deployment. But when the data team investigated, they found that 34% of queries routed to GPT-5.2 were simple extraction tasks that should have cost pennies, not dollars. The router's complexity classifier had drifted over six months as the query distribution shifted. New types of simple queries appeared that the classifier had never seen during training, and it defaulted to the expensive tier when uncertain. The misclassification went undetected because the team monitored aggregate cost trends but never logged individual routing decisions with their classification features. By the time they caught it, they had spent $280,000 more than necessary on queries that a $0.02 model could have handled. The router had failed silently, making confident-looking decisions that were systematically wrong.

Routers fail in predictable ways, and every routing architecture you build will eventually encounter each failure mode. Understanding these failure modes is not academic—it is the difference between a router that saves you money and improves quality versus one that quietly hemorrhages cost while degrading user experience. The six primary failure modes are misclassification, looping, cold-start problems, latency spikes, model drift, and catastrophic router failure. Each has distinct symptoms, distinct costs, and distinct mitigation strategies. Most production routing systems experience all six modes at different times, often simultaneously. You will encounter these failures. The question is whether you detect them in hours or months, and whether you have fallback mechanisms in place or discover the problem through angry stakeholder emails about cost overruns.

## Misclassification: Sending Queries to the Wrong Tier

Misclassification is the most common router failure mode. The router examines a query, extracts routing features, applies its decision logic, and chooses the wrong model. There are two types of misclassification, and they have opposite impacts. Sending a complex query to a cheap model produces a quality failure—the model cannot handle the task, returns a poor response, and the user receives degraded service. Sending a simple query to an expensive model produces a cost failure—the response is fine, possibly better than necessary, but you pay ten times or a hundred times what the query should have cost. Both failures are silent from the model's perspective. The model executes the query as requested and returns a result. The failure is in the routing decision, not the model response.

Complex-to-cheap misclassification is easier to detect because quality metrics usually degrade. If your router sends a nuanced legal analysis question to a Llama 4 model that cannot reason through multi-step contract interpretation, the response will likely fail your quality rubric. Users may complain, downstream systems may reject the output, or your automated evaluation pipeline may flag the response as below threshold. The cost of this failure is reputational and operational—you deliver bad answers, erode user trust, and may need to reprocess queries. The mitigation is straightforward: set conservative thresholds for routing to cheap models. If your complexity classifier is uncertain, escalate to the next tier. Better to occasionally overspend on a simple query than to regularly fail complex ones.

Simple-to-expensive misclassification is harder to detect because quality metrics look fine. The expensive model handles the simple query perfectly. The user gets a correct answer. Downstream systems accept the output. The only signal is cost, and cost is often aggregated over thousands or millions of queries, making individual misclassifications invisible. A legal metadata extraction query that should cost $0.02 on Llama 4 instead costs $0.30 on GPT-5.2. The response is identical, the latency is slightly better, and no alarm fires. But when this happens to 30% of your query volume over six months, you burn a quarter million dollars. The mitigation is logging and analysis. You must log every routing decision with the features that informed it, then periodically sample your logs to verify that queries routed to expensive tiers actually required that tier's capabilities.

Misclassification often comes from feature drift. The classifier was trained on a dataset that represented your query distribution at one point in time, but query distributions change. New types of queries appear. User behavior shifts. Product features evolve. The classifier's decision boundary, which was well-calibrated six months ago, no longer matches reality. A contract analysis system trained in early 2025 might have assumed that queries mentioning "merger" or "acquisition" were complex and required deep reasoning. By late 2025, users routinely asked simple questions like "Does this document mention any acquisitions?" which are keyword searches, not reasoning tasks. The classifier still sees "acquisition" and routes to the expensive tier, even though the query is trivial. The fix is periodic retraining. Treat your routing classifier like any other production model—monitor its input distribution, retrain when drift is detected, validate on held-out recent data, and deploy updates through a controlled release process.

Another source of misclassification is feature extraction failure. Your routing logic depends on features like query length, presence of specific keywords, syntactic complexity, or semantic embeddings. If the feature extraction pipeline breaks—perhaps a dependency fails, an embedding model times out, or a text preprocessing step introduces garbage characters—the classifier receives invalid inputs and makes random decisions. A query that should have a complexity score of 0.73 instead gets 0.01 because the embedding call failed silently and returned zeros. The router sees low complexity and sends the query to the cheapest tier, where it fails. You detect this only if you log the features themselves, not just the routing decision. Feature logging is essential. Every decision log must include the raw features that the router evaluated, so you can diagnose whether the decision was wrong because the classifier failed or because the features were wrong.

## Looping: Escalation Cascades That Never Terminate

Looping occurs in cascade routers or confidence-based routers that escalate queries through tiers until a satisfactory response is achieved. The idea is sound: try the cheap model first, check if the response meets quality thresholds, and escalate to a more capable model if it does not. But if the quality check is poorly designed or the query is genuinely unanswerable, the router can loop through every tier, accumulating maximum cost and latency, and still never produce an acceptable response. A financial services company in late 2025 deployed a cascade router for fraud detection explanations. Tier one was Llama 4, tier two was Claude Opus 4.5, tier three was GPT-5.2. The quality check was a regex pattern looking for specific explanation phrases. If the response did not contain those phrases, escalate. They discovered that 2% of queries looped through all three tiers because the query itself was malformed or the expected phrases were unrealistic. Each looping query cost $1.80 in model calls and took 12 seconds to fail. This was unacceptable for a fraud alert system with a 3-second SLA.

Looping happens when your escalation logic has no termination guarantee. You assume that some tier will eventually satisfy the quality check, but that assumption can be wrong. The query may be ambiguous, adversarial, or outside the scope of all available models. The quality check may be miscalibrated, expecting a level of certainty or structure that no model provides. Or the response may be technically correct but fail the check due to formatting or phrasing differences. Without a hard cap on escalation, the router will exhaust all tiers and still return failure. The user waits through every tier's latency, you pay for every tier's cost, and the outcome is the same as if you had failed immediately at tier one.

The mitigation is escalation caps and timeout budgets. Set a maximum number of escalations—perhaps two tiers, meaning you try the initial tier, escalate once if it fails, and stop there regardless of the outcome. Set a maximum cumulative latency budget—if the query has already consumed 5 seconds across two tiers and your SLA is 6 seconds, do not escalate to tier three. Return the best response you have, even if it did not pass the quality check. This is a pragmatic trade-off: delivering a potentially suboptimal response within SLA is better than looping indefinitely and violating latency requirements. You also need a fallback response for queries that exhaust all tiers. This might be a canned message like "Unable to generate a response for this query," or a default safe answer, or a handoff to a human operator. The worst outcome is looping to failure and returning nothing.

Another looping variant is confidence collapse. Confidence-based routers route queries to a model tier based on how confident they are in their complexity or risk classification. If confidence is below a threshold, escalate to a more capable model. But if the confidence estimate itself is unreliable—perhaps the confidence model was poorly calibrated or the query is genuinely edge-case—the router can escalate repeatedly because it never becomes confident, even after seeing responses from expensive models. A customer support system in mid-2025 used a confidence-based router that escalated queries when uncertainty was high. Certain queries about account-specific edge cases produced low confidence at every tier because the models lacked account context. The router escalated these queries all the way to GPT-5.2, which also returned uncertain responses. The confidence signal was not improving with escalation, but the router kept trying. The fix was to recognize that confidence is not monotonically increasing with model capability—some queries are inherently uncertain, and escalation will not resolve that. Set a confidence floor: if confidence is below a certain level even at tier two, accept that the query is unanswerable and do not escalate further.

## Cold-Start Problems: Routing New Query Types Without History

Cold-start problems arise when a new type of query appears that the router has never encountered. The routing classifier was trained on historical data, the rule-based logic was designed for known query patterns, and the lookup tables were built from months of query logs. Then a new product feature launches, a new user behavior emerges, or a new regulatory requirement introduces a novel query type. The router has no learned signal for this query. If it is a rule-based router, the query matches no rules and falls through to a default tier. If it is a classifier-based router, the query lands in a low-confidence region of feature space and the classifier guesses. Either way, the routing decision is uninformed, and the outcome is often suboptimal.

A healthcare chatbot in early 2026 added a new feature for scheduling specialist referrals. The queries for this feature were structurally different from the existing symptom-checking and prescription-refill queries. The routing classifier had been trained on months of symptom and refill data and had learned that queries with medical terminology were usually complex. The new referral queries also contained medical terminology but were actually simple structured lookups—find available appointments for a cardiologist in a specific ZIP code. The classifier saw the medical terms, assumed complexity, and routed these queries to Claude Opus 4.5, the expensive tier. The correct tier was Llama 4, which handled structured data retrieval efficiently. For three weeks, every referral query cost 15 times more than necessary because the router had no historical signal and defaulted to the wrong tier. The team only noticed when they analyzed cost per feature and saw referral queries consuming a disproportionate share of the budget.

The cold-start mitigation is graceful defaults and fast feedback loops. When you deploy a new feature or anticipate a new query type, manually assign a default routing tier based on your understanding of the task, not based on learned signals. If you believe the new query type is simple, explicitly route it to a cheap tier until you collect enough data to train a proper signal. If you are uncertain, route to a middle tier as a conservative guess, not the most expensive tier. Then instrument these queries with extra logging and monitoring. Track cost, latency, and quality per query type. After a few hundred queries, analyze whether the default routing tier was correct. If you see quality issues, escalate the tier. If you see high cost with no quality benefit, downgrade the tier. This is a manual feedback loop, but it prevents the router from blindly misclassifying an entire category of queries for months.

Another cold-start mitigation is feature-based routing with graceful degradation. If your router uses a complexity classifier and a new query produces an out-of-distribution embedding, flag it. Route out-of-distribution queries to a middle tier by default, and log them for human review. Over time, collect enough examples of the new query type to retrain your classifier with updated decision boundaries. This is a continuous learning loop: new query types appear, you route them conservatively, you collect data, you retrain, you improve routing accuracy. The router becomes more accurate over time as it sees more of the production distribution. But this only works if you have observability into which queries are out-of-distribution and tooling to retrain and redeploy the routing model regularly.

## Latency Spikes: When the Router Itself Becomes the Bottleneck

Routers add latency. You send a query to the router, the router extracts features, evaluates rules or runs a classifier, makes a decision, and forwards the query to the selected model. This entire process takes time. In a well-optimized router, this is 10 to 50 milliseconds—negligible compared to model inference latency. But if the router uses a large language model for classification, or if feature extraction involves expensive operations like embedding generation or database lookups, routing latency can spike to hundreds of milliseconds or even seconds. A content moderation system in late 2025 used a fine-tuned GPT-4o model to classify query risk for routing decisions. The risk classifier itself took 400 milliseconds to run. The cheap tier model took 300 milliseconds to respond. The expensive tier took 1.2 seconds. The total latency for a query routed to the cheap tier was 700 milliseconds—400ms routing overhead plus 300ms model response. This violated the 500ms SLA, even though the model itself was fast enough.

Router latency spikes often go unnoticed during low-traffic periods because the router's feature extraction can be cached or batched. But under load, when thousands of queries arrive simultaneously, the router becomes a bottleneck. Feature extraction pipelines that call external APIs—embedding services, database lookups, third-party enrichment services—can time out or throttle under load. The router queues requests, latency grows, and queries start timing out before they even reach the model. This is a cascading failure: high load increases routing latency, which increases queue depth, which increases load on the router, which increases latency further. The system enters a degraded state where the router itself is the limiting factor, not the models.

The mitigation is router latency budgeting and circuit breakers. Measure your router's p50, p95, and p99 latency under realistic load. If routing latency at p95 exceeds 10% of your end-to-end SLA, you have a problem. Optimize the router: cache feature extraction where possible, use faster classifiers, precompute routing decisions for common query patterns, or move expensive feature extraction to an asynchronous background process. If optimization is not enough, implement circuit breakers. Set a maximum acceptable routing latency—say, 100 milliseconds. If the router exceeds this threshold, bypass complex routing logic and fall back to a simple default rule, such as always routing to the middle tier. This degrades routing accuracy but preserves latency SLAs. The router becomes eventually consistent: under normal load, it makes optimal decisions; under high load, it makes acceptable decisions quickly.

Another latency spike source is routing model cold starts. If your routing classifier runs on a serverless inference platform or a model that scales to zero when idle, the first query after idle time may trigger a cold start, adding seconds of latency. A financial advice chatbot in early 2026 used a fine-tuned Llama 4 model for routing classification, deployed on a serverless GPU platform. During off-peak hours, the routing model scaled to zero. The first query in the morning triggered a cold start, which took 8 seconds to provision a GPU and load the model. The user saw an 8-second delay before their query even reached the response model. The fix was keeping the routing model warm with periodic health checks, or switching to an always-on deployment for critical-path routing logic. Routing must be fast and reliable; cold starts are unacceptable in the routing layer.

## Model Drift and Stale Routing Rules

Routing logic is designed for a specific lineup of models with specific capabilities and costs. But model lineups change. Providers release new versions. Deprecate old versions. Change pricing. Change rate limits. Improve or degrade specific capabilities. A routing rule written in January 2026 might assume that GPT-5 is the frontier model for complex reasoning, Claude Opus 4.5 is the mid-tier model for balanced tasks, and Llama 4 is the cheap model for simple retrieval. By June 2026, GPT-5.2 has been released with better reasoning at half the cost, and Llama 4.1 has been released with improved instruction-following. Your routing rules are now stale. They still send complex reasoning to GPT-5, which is now overpriced compared to GPT-5.2, and they still send simple tasks to Llama 4, which is now outperformed by Llama 4.1 at the same price. The rules are not wrong in the sense that they fail—they still route queries to functional models—but they are suboptimal. You are paying more than necessary and getting lower quality than available.

Stale routing rules are insidious because they degrade performance gradually. The routing system continues to function. Metrics do not suddenly collapse. But over months, cost creeps up and quality creeps down relative to what you could achieve with updated rules. A customer support platform in mid-2025 had routing rules written in early 2024, before GPT-4.5 was released. The rules routed complex multi-turn support queries to GPT-4o, which cost $0.30 per query and achieved 89% resolution. When GPT-4.5 launched in late 2024, it cost $0.18 per query and achieved 92% resolution on the same task. But the routing rules were never updated. For six months, the platform continued to use GPT-4o, paying 67% more per query and getting 3% lower resolution. No single metric flagged this as a failure—cost was stable, quality was stable—but the opportunity cost was enormous.

The mitigation is treating routing configuration as code that is versioned, reviewed, and updated. Your routing rules, thresholds, model mappings, and cost assumptions should live in a configuration file or database that is checked into version control. When a new model is released, when pricing changes, or when a model's capabilities shift, you update the routing configuration through a code review and deployment process. This is not a one-time setup—it is an ongoing maintenance task. Assign ownership of routing configuration to a team or individual who monitors model releases, benchmarks new models on your tasks, and proposes routing updates when beneficial. Treat routing configuration updates with the same rigor as model version updates or feature releases. Test updated routing rules in staging, validate that they improve cost or quality on historical queries, and deploy through canary or blue-green rollout to catch regressions.

Another drift source is query distribution shift. Even if your models and routing rules are stable, the distribution of queries can change. A legal document system might have been designed assuming that 70% of queries are simple metadata extraction and 30% are complex contract analysis. The routing logic was optimized for this distribution, sending most queries to the cheap tier and reserving the expensive tier for the minority of complex cases. But then a new client segment appears—private equity firms requesting detailed merger-and-acquisition clause analysis—and suddenly 50% of queries are complex. The routing logic is still technically correct, but the cost and latency characteristics of the system have changed. What was a 70/30 cheap/expensive split is now 50/50, doubling the average cost per query. This is not a routing failure in the sense that individual decisions are wrong, but the overall system behavior has shifted. The mitigation is monitoring query distribution and rebalancing routing rules when the distribution changes. Track the percentage of queries routed to each tier over time. If you see sustained shifts, investigate whether the routing logic still matches your cost and quality targets.

## Catastrophic Router Failure: When the Router Goes Down

The router is a single point of failure. If the routing service crashes, times out, or becomes unreachable, no queries can be processed. The entire system depends on the router making a decision and forwarding the query to a model. If the router is unavailable, queries queue up, latency grows, and eventually requests time out. A legal research platform in late 2025 ran their router as a single-instance service with no redundancy. During a datacenter network issue, the router lost connectivity for 4 minutes. During those 4 minutes, 18,000 queries queued in the ingress layer. When the router recovered, it was overwhelmed by the backlog, and queries timed out for another 12 minutes while the queue drained. Total downtime was 16 minutes, and 43% of queries during that period failed. The root cause was treating the router as a simple stateless service that did not need high availability design, when in fact it was a critical-path dependency.

Catastrophic router failure mitigation is redundancy and failover. Deploy the router as a highly available service with multiple instances behind a load balancer. Use health checks to detect instance failures and route traffic to healthy instances. Deploy across availability zones or regions to survive datacenter-level failures. This is standard infrastructure design, but it is sometimes neglected for routing services because they are viewed as lightweight decision-making layers rather than critical infrastructure. They are critical infrastructure. If the router fails, the entire AI system fails. Treat router availability with the same priority as model serving availability.

Another catastrophic failure mode is routing logic bugs that affect all queries. A configuration error, a broken classifier deployment, or a code bug in the routing service can cause the router to misroute every query, route all queries to a single tier, or crash on every request. A customer support chatbot in early 2026 deployed a routing configuration update that accidentally swapped the model identifiers for the cheap and expensive tiers. For 20 minutes, every simple query went to GPT-5.2 and every complex query went to Llama 4. Simple queries got expensive perfect responses, complex queries got cheap failures. The inversion was total. The deployment had no canary or gradual rollout—it was pushed to 100% of traffic immediately. The team detected the issue only when cost spiked and quality alerts fired. The fix was a rollback, but the damage was done—$6,000 in wasted model calls and hundreds of failed complex queries.

The mitigation for logic bugs is gradual rollout and automated validation. When deploying routing configuration changes or classifier updates, roll them out to a small percentage of traffic first—perhaps 5% for an hour. Monitor cost, latency, and quality metrics for the canary traffic compared to baseline traffic. If metrics are stable, increase the rollout percentage incrementally. If metrics degrade, halt the rollout and investigate. This is the same progressive delivery strategy used for model deployments, application code deploys, and infrastructure changes. Routing logic is code, and it should be deployed with the same safeguards. Additionally, write automated tests for routing logic. For a rule-based router, test that specific example queries route to the expected tiers. For a classifier-based router, validate that the classifier's predictions on a held-out test set meet accuracy thresholds before deploying. Treat router logic as you would treat production application logic—version controlled, tested, reviewed, and deployed incrementally.

## Building Router Resilience

Router resilience is the combination of all these mitigations: misclassification detection through logging and analysis, looping prevention through escalation caps, cold-start handling through graceful defaults, latency budgeting and circuit breakers, periodic routing configuration updates, and high availability infrastructure. A resilient router does not eliminate failures—it detects them quickly, limits their impact, and recovers gracefully. Misclassifications happen, but you detect them in hours through sampling and alerting, not months through cost analysis. Looping happens, but escalation caps prevent runaway cost. Cold-start misrouting happens, but default tiers are conservative and you retrain quickly. Latency spikes happen, but circuit breakers prevent cascading failures. Model drift happens, but you update routing rules quarterly. Catastrophic router failures happen, but redundancy and gradual rollouts limit the blast radius.

The mental model shift is from viewing the router as a static decision layer to viewing it as a dynamic production system that requires observability, testing, and continuous improvement. You do not build a router once and forget it. You build a router, deploy it, monitor it, detect failures, improve it, and redeploy updates. The router evolves as your models, your query distribution, and your requirements evolve. Resilience comes from treating the router as a first-class production component, not a side project or configuration file. This means assigning ownership, defining SLAs for routing latency and accuracy, instrumenting decision logs, setting up dashboards and alerts, writing runbooks for common failure modes, and scheduling regular reviews of routing performance.

The cost of not building router resilience is silent degradation. Misclassifications accumulate, looping queries burn budget, cold-start misrouting degrades new features, latency spikes erode user experience, stale rules leave performance on the table, and catastrophic failures take down the entire system. These are not hypotheticals—they are the documented failure modes from dozens of production routing systems between 2024 and 2026. Every team that has deployed a router at scale has encountered multiple failure modes. The teams that recovered quickly and cheaply were the ones that had anticipated these failures, instrumented their systems to detect them, and built mitigations in advance. The teams that paid the highest cost were the ones that assumed the router would work correctly indefinitely without monitoring or maintenance.

The next step is making router failures visible through observability. Knowing the failure modes is necessary but not sufficient—you must also log every decision, track every feature, and version every configuration change so you can diagnose failures when they occur and prevent them from recurring.
