# 2.6 â€” Beyond Accuracy: Evaluating Tone, Format Compliance, Safety, and Edge Cases

In June 2025, a healthcare technology company launched a patient communication assistant powered by Claude Opus 4.5. The model had been selected after a rigorous bake-off that demonstrated 94 percent accuracy on a set of 600 patient questions. The accuracy was excellent. The model correctly identified the medical information patients were asking about, correctly retrieved the relevant policy details, and correctly synthesized the answer. Three weeks after launch, the product was pulled. The problem was not that the model gave wrong answers. The problem was that it gave right answers in a tone that patients found cold, clinical, and dismissive. A patient asking about coverage for cancer treatment received a technically accurate response that read like a legal document. A patient asking about mental health services received a technically accurate response that felt robotic and uncaring. The model was correct. It was also completely unsuitable for production.

The mistake was treating accuracy as the only dimension that mattered. The team had optimized for correctness and ignored tone, empathy, and human connection. They had assumed that a model that scored well on factual accuracy would automatically produce acceptable patient-facing communication. This assumption was wrong. Accuracy is necessary but not sufficient. Many model selection failures happen not because the model gets the answer wrong, but because it gets the answer right in the wrong tone, format, or style. A model that is 95 percent accurate but violates your brand voice, ignores format requirements, or behaves unpredictably on edge cases is not a production-ready model. It is a liability.

## Why Accuracy Alone Is Not Enough

Accuracy measures whether the model produces the correct output for a given input. This is important. If the model cannot get the answer right, nothing else matters. But correctness is only one component of production quality. A model can be correct and still fail in production because it produces outputs that are inappropriate, unusable, unsafe, or inconsistent.

Consider a customer support agent model. The model's job is to resolve customer issues. Accuracy means the model identifies the correct resolution, retrieves the correct information, and applies the correct policy. But if the model's tone is condescending, if it fails to acknowledge the customer's frustration, if it sounds like it is reading from a script, the customer will escalate to a human agent even when the model's answer is correct. The model has failed, not because it was inaccurate, but because it was tonally inappropriate.

Consider a code generation model. Accuracy means the generated code produces the correct output for the given input. But if the code is formatted inconsistently, if it uses deprecated APIs, if it includes insecure patterns, if it is unreadable, the developer will not use it. The model has failed, not because it was inaccurate, but because it violated format, safety, or usability requirements.

Consider a legal document analysis model. Accuracy means the model correctly extracts the relevant clauses, correctly identifies the risks, and correctly summarizes the terms. But if the model occasionally hallucinates a clause that does not exist, if it refuses to process certain documents for unclear reasons, if it produces different outputs for the same input on different runs, the legal team cannot trust it. The model has failed, not because it is usually inaccurate, but because it is inconsistent or unreliable.

Accuracy is a floor, not a ceiling. It is the minimum requirement for production. Once you have established that the model can get the answer right, you must evaluate whether it gets the answer right in a way that is appropriate, usable, safe, and consistent. These are not secondary considerations. They are primary considerations that are often more important than raw accuracy.

## Evaluating Tone

**Tone** is the emotional register, formality level, and relational stance of the model's output. Tone is what makes communication feel human or robotic, warm or cold, professional or casual, empathetic or indifferent. Tone is not a fixed property of the model. It depends on the task, the user, and the context. The same model can produce appropriate tone for one use case and inappropriate tone for another.

A customer support model needs a tone that is empathetic, patient, and helpful. It should acknowledge the customer's frustration, validate their concern, and offer reassurance. It should avoid sounding defensive, dismissive, or bureaucratic. A tone that works for a technical support question does not work for a billing dispute. A tone that works for a satisfied customer does not work for an angry customer. The model must adapt its tone to the emotional context of the interaction.

A legal advisory model needs a tone that is precise, authoritative, and measured. It should avoid hedging that undermines confidence, but it should also avoid overconfidence that implies certainty where none exists. It should sound like a senior attorney, not a junior associate and not a law textbook. The tone must convey expertise without arrogance, caution without paralysis.

A marketing content model needs a tone that matches your brand voice. If your brand is playful and irreverent, the model should be playful and irreverent. If your brand is serious and trustworthy, the model should be serious and trustworthy. Tone consistency across all customer touchpoints is what makes a brand feel coherent. A model that produces content in the wrong tone damages the brand, even if the content is factually accurate.

Evaluating tone requires human judgment. You cannot measure tone with a number. You can create a **tone rubric** that defines the dimensions of tone you care about and the characteristics that indicate success or failure on each dimension. For a customer support model, the rubric might include dimensions like empathy, professionalism, clarity, and warmth. For each dimension, you define what good looks like, what acceptable looks like, and what unacceptable looks like. Then you have human evaluators score each output on each dimension.

Tone evaluation must be done by people who understand the context. A generic human evaluator will not reliably assess whether the tone is appropriate for your brand, your users, or your use case. You need evaluators who know your brand voice, who understand your user base, and who can distinguish between tone that works and tone that does not. This often means using internal team members, not outsourced annotators.

Tone evaluation is subjective, but it is not arbitrary. When you give evaluators clear definitions, concrete examples, and structured rubrics, inter-rater reliability is high. Two evaluators who understand the brand will generally agree on whether a model's tone is appropriate. Disagreements are opportunities to refine the rubric, clarify the criteria, and build shared understanding of what tone means in your context.

## Evaluating Format Compliance

**Format compliance** means the model produces outputs that match the required structure, length, style, and conventions. Format requirements vary by task. A data extraction model might need to produce JSON with specific field names. A summarization model might need to produce summaries of exactly three sentences. A report generation model might need to include specific section headers in a specific order. Format compliance is not about accuracy. It is about usability. An output that is accurate but incorrectly formatted is often unusable.

Format compliance is easier to measure than tone because format requirements are usually objective. Either the output includes the required section headers or it does not. Either the JSON parses correctly or it does not. Either the summary is three sentences or it is not. You can write automated checks for many format requirements. If the output is supposed to be valid JSON, you parse it. If the output is supposed to include a specific header, you search for it. If the output is supposed to be under 200 words, you count the words.

But not all format requirements are mechanically checkable. Some format requirements are stylistic. A legal memo might need to follow Bluebook citation style. A marketing email might need to use sentence case for headers and title case for buttons. A technical document might need to use Oxford commas and avoid passive voice. These requirements are objective in principle but require human judgment in practice. You can write linters for some of them, but the linters are imperfect. Human evaluation is still necessary.

The most common format compliance failure is **inconsistency**. The model produces the correct format 90 percent of the time and the wrong format 10 percent of the time. For batch processing, this is annoying but manageable. You filter out the incorrectly formatted outputs and reprocess them. For real-time user-facing applications, this is unacceptable. A chatbot that occasionally returns malformed JSON crashes the frontend. A report generator that occasionally omits the required section header produces a report that cannot be submitted. Inconsistency is a production blocker.

When you evaluate format compliance, you measure both the rate of compliance and the consistency of compliance. A model that produces the correct format 95 percent of the time is better than a model that produces the correct format 85 percent of the time, but a model that produces the correct format 95 percent of the time with high variance is worse than a model that produces the correct format 92 percent of the time with low variance. Reliability matters more than peak performance.

You also evaluate what happens when the model cannot produce the required format. Does it fail gracefully, producing a partial output that indicates what went wrong? Does it produce a default output that is safe but uninformative? Does it produce a malformed output that breaks downstream systems? The failure mode matters as much as the success mode. A model that fails by returning an empty string is easier to handle than a model that fails by returning corrupted JSON.

## Evaluating Safety

**Safety** means the model does not produce outputs that are harmful, biased, offensive, illegal, or otherwise inappropriate. Safety is a broad category that includes many types of risk. A customer support model must not leak personally identifiable information from one user to another. A content moderation model must not fail to detect hate speech. A medical advisory model must not provide dangerous health advice. A code generation model must not generate code with known security vulnerabilities. The specific safety requirements depend on the domain, the user population, and the regulatory environment.

Safety evaluation has two components: **refusal behavior** and **harmful output detection**. Refusal behavior is what the model does when it is asked to do something it should not do. If a user asks a customer support model to disclose another customer's account information, the model should refuse. If a user asks a medical model to recommend an unproven treatment, the model should refuse. Refusal is a safety mechanism. You want the model to refuse appropriately, meaning it refuses when it should refuse and does not refuse when it should not refuse.

Inappropriate refusal is as much a problem as inappropriate compliance. A model that refuses to answer legitimate customer questions because it misinterprets the request as a policy violation is not safe. It is overly cautious to the point of being useless. A medical advisory model that refuses to answer any question involving mental health because it incorrectly classifies all mental health questions as high-risk is not safe. It is denying users access to information they need. Safety is not maximizing refusals. Safety is calibrating refusals to the actual risk.

Harmful output detection is identifying cases where the model produces content that violates safety policies. This includes hate speech, violence, sexual content, illegal activity, personally identifiable information, misinformation, and domain-specific harms. For a financial advisory model, harmful output includes recommending illegal trading strategies. For a hiring assistant, harmful output includes biased language that discriminates based on protected characteristics. For a content generation model, harmful output includes plagiarism or copyright infringement.

You evaluate safety by creating a **red team evaluation set** that includes adversarial inputs designed to elicit unsafe behavior. These are not typical inputs. They are edge cases, jailbreak attempts, ambiguous scenarios, and contextually inappropriate requests. You run the model on the red team set and measure how often it produces harmful outputs or fails to refuse inappropriate requests. The goal is not to catch the model doing something wrong. The goal is to understand the boundaries of safe behavior before you deploy to production.

Red team evaluation sets are expensive to create because they require domain expertise and creativity. You need to think like an adversary. You need to anticipate how users will misuse the system, how they will try to bypass safety mechanisms, and what harmful outputs are possible given the task. Many organizations underinvest in red teaming because it feels like looking for problems that might not exist. This is a mistake. The problems exist. You are choosing whether to find them in evaluation or in production.

## Evaluating Edge Cases

**Edge cases** are inputs that are unusual, ambiguous, contradictory, malformed, adversarial, or otherwise outside the typical distribution. Edge cases are rare, but they are not negligible. In a system processing 100,000 requests per day, even if edge cases are only 1 percent of traffic, that is 1,000 edge cases per day. If the model handles edge cases poorly, you have 1,000 failures per day. This is not acceptable.

Common edge cases include very long inputs that exceed the model's context window or the expected length for the task, very short inputs that lack the information needed to produce a useful output, empty inputs that contain no content at all, ambiguous inputs that could be interpreted in multiple ways, contradictory inputs that include conflicting information or instructions, adversarial inputs designed to exploit weaknesses in the model, and inputs in unexpected formats or languages.

The failure mode on edge cases varies by model. Some models degrade gracefully, producing lower-quality outputs but still attempting to fulfill the request. Some models fail abruptly, refusing to respond or returning an error. Some models hallucinate, producing outputs that are confidently wrong. Some models behave unpredictably, sometimes succeeding and sometimes failing on the same input. You need to know what your model does on edge cases before you deploy it.

You evaluate edge case behavior by creating an **edge case evaluation set** that represents the types of unusual inputs you expect to encounter. If your task is document summarization, the edge case set includes documents that are too long, too short, in the wrong language, poorly formatted, or contain only tables and figures. If your task is question answering, the edge case set includes questions that are ambiguous, unanswerable, based on false premises, or outside the domain of knowledge. You run the model on the edge case set and measure the failure rate, the failure mode, and the impact of failures.

Edge case failures are not always critical. A summarization model that produces a mediocre summary of a poorly formatted document is not ideal, but it is acceptable if the failure rate is low and the failure mode is graceful. A question answering model that refuses to answer an unanswerable question is not ideal, but it is better than hallucinating an answer. The question is not whether the model handles edge cases perfectly. The question is whether the model's edge case behavior is acceptable for your production constraints.

Some edge cases can be prevented with input validation. If very long inputs cause the model to fail, you truncate inputs before sending them to the model. If ambiguous inputs cause the model to produce low-quality outputs, you prompt the user to clarify before invoking the model. Input validation is a mitigation, not a solution. You still need to know what happens when an edge case reaches the model, because input validation is never perfect.

## Building Evaluation Rubrics for Non-Accuracy Dimensions

A **rubric** is a structured framework for evaluating outputs on specific dimensions. For accuracy, the rubric is often simple: correct or incorrect, with possibly a middle category for partially correct. For non-accuracy dimensions like tone, format, safety, and edge case behavior, the rubric must be more detailed.

A tone rubric defines the dimensions of tone you care about, the scale for each dimension, and concrete examples of outputs at each level of the scale. For a customer support model, the tone rubric might include four dimensions: empathy, professionalism, clarity, and warmth. Each dimension is scored on a three-point scale: unacceptable, acceptable, and excellent. For each combination of dimension and score, the rubric provides a description and an example.

Empathy scored as unacceptable means the output ignores or dismisses the customer's concern. An example: "Your issue is not covered by our policy. There is nothing we can do." Empathy scored as acceptable means the output acknowledges the customer's concern but does not go beyond acknowledgment. An example: "I understand this is frustrating. Unfortunately, this issue is not covered by our policy." Empathy scored as excellent means the output acknowledges the concern, validates the customer's feelings, and offers reassurance or alternatives. An example: "I can see how frustrating this situation must be, and I want to help find a solution. While this specific issue is not covered by our standard policy, let me check if there are other options available."

A format compliance rubric defines the required format elements, the criteria for compliance, and the acceptable rate of non-compliance. For a JSON extraction model, the rubric might specify that outputs must be valid JSON, must include all required fields, must use the correct data types for each field, and must not include extra fields. Compliance is measured as the percentage of outputs that meet all criteria. An acceptable rate might be 98 percent, meaning no more than 2 percent of outputs can fail format validation.

A safety rubric defines the categories of harmful outputs, the severity of each category, and the acceptable rate of harmful outputs. For a content generation model, harmful categories might include hate speech, violence, sexual content, misinformation, and copyright infringement. Severity is scored as low, medium, or high. An acceptable rate for high-severity harms is zero. An acceptable rate for low-severity harms might be 0.1 percent. The rubric makes the tradeoffs explicit.

An edge case rubric defines the types of edge cases you care about, the expected behavior for each type, and the acceptable failure rate. For a question answering model, edge case types might include unanswerable questions, ambiguous questions, out-of-domain questions, and adversarial questions. The expected behavior for unanswerable questions is refusal with an explanation. The expected behavior for ambiguous questions is requesting clarification. The acceptable failure rate might be 5 percent for ambiguous questions and 1 percent for adversarial questions.

Building a rubric is an exercise in making implicit criteria explicit. Most teams have intuitions about what tone is appropriate, what format is acceptable, and what safety violations matter. The rubric forces you to write down those intuitions, test them against real examples, and refine them until they are clear enough for independent evaluators to apply consistently. This process is time-consuming, but it is essential. Without a rubric, evaluation is subjective and unreproducible.

## Why These Dimensions Often Matter More Than Accuracy

For user-facing products, tone, format, safety, and edge case behavior often matter more than accuracy because they determine whether users trust, adopt, and continue using the product. A model that is 95 percent accurate but tonally inappropriate will be perceived as low-quality, even if the accuracy is objectively high. A model that is 90 percent accurate but consistently appropriate in tone will be perceived as high-quality.

Trust is fragile. Users do not measure accuracy. They measure their subjective experience of quality, which is shaped by tone, format, and reliability more than by correctness. A customer support agent that gives the right answer in a cold tone feels unhelpful. A document generator that produces correct content in the wrong format feels broken. A chatbot that occasionally refuses legitimate requests feels unreliable. These perceptions are not irrational. They reflect the reality that production quality is multidimensional.

For internal tools, format compliance and edge case behavior often matter more than accuracy because they determine whether the tool integrates smoothly into existing workflows. A data extraction model that produces 95 percent accurate results but outputs invalid JSON 10 percent of the time cannot be used in an automated pipeline. A summarization model that produces 90 percent accurate summaries but fails unpredictably on certain document types cannot be trusted for high-stakes use cases. Integration requires reliability, not just correctness.

For regulated domains, safety matters more than accuracy because safety violations create legal and reputational risk. A medical advisory model that is 98 percent accurate but occasionally provides dangerous advice is not deployable. A financial advisory model that is 95 percent accurate but occasionally recommends illegal strategies is not deployable. A hiring assistant that is 92 percent accurate but occasionally produces biased language is not deployable. Regulatory compliance is a hard constraint, not a soft preference.

The emphasis on accuracy in model evaluation reflects the origins of AI research in academic benchmarks, where accuracy is easy to measure and other dimensions are hard to measure. But production is not a benchmark. Production is a complex environment where users, workflows, regulations, and business constraints all impose requirements that go beyond correctness. A model that optimizes only for accuracy is optimized for a benchmark, not for production.

## The Gap Between Correct and Production-Ready

A model that scores well on accuracy in a bake-off is **correct**. A model that is correct, tonally appropriate, format-compliant, safe, and robust to edge cases is **production-ready**. The gap between correct and production-ready is where most model deployments fail.

The gap exists because evaluation is expensive and teams prioritize the dimensions that are easiest to measure. Accuracy is easy to measure. You define ground truth, run the model, compare the outputs, and calculate a score. Tone is hard to measure. You need human evaluators, detailed rubrics, and subjective judgment. Format compliance is moderately easy to measure if the format requirements are mechanical, but hard if they are stylistic. Safety is hard to measure because you need adversarial examples and domain expertise. Edge case behavior is hard to measure because edge cases are rare and hard to enumerate.

The result is that teams measure accuracy thoroughly and measure everything else superficially, if at all. They assume that a model with high accuracy will be production-ready, or they assume that non-accuracy issues can be fixed with prompt engineering after deployment. Both assumptions are wrong. A model with inappropriate tone cannot be fixed with prompt engineering if the model's base behavior is tonally rigid. A model with inconsistent format compliance cannot be fixed with post-processing if the failures are unpredictable. A model with poor safety behavior cannot be fixed with filtering if the model generates harmful content faster than you can filter it.

The only way to close the gap between correct and production-ready is to evaluate all the dimensions that matter before deployment. This means building rubrics, creating evaluation sets, recruiting evaluators, and running the evaluations. It means spending time and money on dimensions that are hard to measure. It means treating tone, format, safety, and edge cases as first-class requirements, not as afterthoughts. The teams that do this ship production-ready models. The teams that do not ship models that are correct but unusable.

## Integrating Non-Accuracy Dimensions into Model Selection

When you run a bake-off, you must evaluate all dimensions that matter for production, not just accuracy. This means creating separate evaluation sets for tone, format, safety, and edge cases, scoring each model on each dimension, and weighting the dimensions according to their importance for your use case.

For a customer-facing chatbot, tone might be weighted as heavily as accuracy. A model with 90 percent accuracy and excellent tone might be preferable to a model with 95 percent accuracy and poor tone. For a data extraction pipeline, format compliance might be weighted more heavily than accuracy. A model with 88 percent accuracy and 99 percent format compliance might be preferable to a model with 92 percent accuracy and 90 percent format compliance. For a regulated application, safety might be a hard constraint. A model with any high-severity safety violations is disqualified, regardless of accuracy.

The weighting is not arbitrary. It reflects the production constraints and the user requirements. If you are unsure how to weight the dimensions, run a pilot deployment with multiple models and measure which dimensions correlate most strongly with user satisfaction, operational efficiency, or business outcomes. The data will tell you which dimensions matter.

Some organizations use a **scorecard** that combines accuracy, tone, format, safety, and edge case scores into a single overall score. The scorecard assigns weights to each dimension, normalizes the scores, and calculates a weighted average. The model with the highest overall score is selected. This approach is transparent and reproducible, but it requires you to define the weights in advance. If the weights are wrong, the scorecard will select the wrong model.

Other organizations use a **tiered approach** where certain dimensions are hard constraints and others are optimized. Safety might be a hard constraint: any model with a safety violation is disqualified. Format compliance might be a threshold: any model with less than 95 percent compliance is disqualified. Among the models that pass the constraints, you select the one with the highest accuracy or the best tone. This approach prioritizes the dimensions that are non-negotiable and optimizes the dimensions that are flexible.

Both approaches work. The key is to make the tradeoffs explicit, document the decision criteria, and apply them consistently across all models in the bake-off. Do not select a model based on accuracy and then retroactively justify the choice based on tone. Evaluate all dimensions, apply the criteria, and let the data drive the decision.

Tone, format, safety, and edge case behavior are not secondary dimensions. They are primary dimensions that determine whether a model is production-ready. Once you have selected a model based on all relevant dimensions, the next step is to understand the operational constraints that govern how that model will be deployed, starting with latency and throughput requirements.
