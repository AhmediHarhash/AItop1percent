# 1.8 — Reasoning Models and Extended Thinking: GPT-5.2 Thinking, Gemini 3 Deep Think, Claude Extended Thinking, DeepSeek R1, and When to Pay for Reasoning Tokens

In August 2025, a financial services company deployed GPT-5.2 with thinking mode enabled for all customer support queries. The reasoning model dramatically improved answer quality for complex questions—multi-step calculations, policy interpretations, eligibility determinations. Accuracy jumped from 78% to 94% compared to the standard GPT-5 model. The team celebrated the improvement and scaled to production across 12,000 queries per day. Three weeks later, the CFO called an emergency meeting: the AI bill had tripled. What had been $8,000 per month was now $26,000 per month. The culprit was reasoning tokens. Every query that previously consumed 800 tokens now consumed 4,200 tokens—800 visible output tokens plus 3,400 hidden reasoning tokens. The model was spending five times more tokens thinking than answering. The team had never measured reasoning token usage during eval. They assumed the thinking overhead was modest. When they finally analyzed token consumption by query type, they discovered that 60% of queries were simple lookups or single-fact questions that did not benefit from extended thinking. Routing those queries to standard GPT-5 cut the monthly bill to $14,000 while maintaining the accuracy improvement for complex queries. The failure was not deploying a reasoning model. The failure was deploying it universally without understanding the cost structure or evaluating which queries justified the reasoning token premium.

The reasoning model revolution represents the most significant capability leap in language models since GPT-4. Models with extended thinking—GPT-5.2 Thinking, Gemini 3 Deep Think, Claude Opus 4.5 Extended Thinking, and the open-weight DeepSeek R1—spend additional compute tokens on internal chain-of-thought reasoning before generating their final answer. This extended thinking enables dramatically better performance on tasks requiring multi-step logic, mathematical reasoning, code debugging, complex analysis, and planning. But the capability comes with steep costs: reasoning tokens are billed but not shown to the user, often multiplying total token consumption by three to ten times. Latency increases proportionally. In production, you must evaluate whether the accuracy gain justifies the cost and latency penalty for each specific task. This subchapter teaches you how reasoning models work, when they outperform standard models, when they do not help, and how to build cost-effective production systems that use reasoning selectively rather than universally.

## What Extended Thinking Actually Means: Reasoning Tokens and Chain-of-Thought

Extended thinking is not a vague marketing term. It refers to a specific architectural pattern: before generating the user-visible output, the model generates a hidden chain-of-thought reasoning trace. This reasoning trace is a sequence of tokens representing the model's internal problem-solving steps. For a math problem, the reasoning trace might show the model breaking the problem into subproblems, solving each subproblem, checking intermediate results, and combining them into a final answer. For a coding task, the reasoning trace might show the model analyzing the requirements, considering alternative approaches, debugging potential edge cases, and refining the solution. The reasoning trace is not shown to the user, but it guides the final output.

The key mechanism is that reasoning tokens are generated before the output tokens, and the model is trained to use the reasoning trace to improve output quality. During training, the model learns that spending more tokens on reasoning leads to higher-quality final answers. The training signal rewards the model for catching its own errors in the reasoning trace, correcting them, and producing a better final answer. This is fundamentally different from standard language models, which generate outputs autoregressively token-by-token without an explicit reasoning phase. Standard models can perform reasoning, but they must do it while simultaneously generating the user-facing output. Extended thinking models separate reasoning from output generation, allowing more deliberate, multi-step thought processes.

The cost structure reflects this architecture. When you call a reasoning model API, you are billed for both reasoning tokens and output tokens. The reasoning tokens are typically two to ten times the output token count, depending on task complexity and how much thinking the model determines is necessary. Some providers—OpenAI with GPT-5.2 Thinking, Google with Gemini 3 Deep Think—charge the same rate per reasoning token as per output token. Others—Anthropic with Claude Extended Thinking—charge a reduced rate for reasoning tokens, recognizing that they are hidden overhead. DeepSeek R1, being open-weight, has no per-token API cost if you self-host, but you pay in infrastructure: reasoning tokens consume GPU memory and compute, increasing serving costs proportionally.

The reasoning token count is not fixed. The model decides how many reasoning tokens to generate based on the query. A simple question might trigger 200 reasoning tokens. A complex multi-step problem might trigger 8,000 reasoning tokens. You do not control this directly, though some APIs allow you to set a reasoning token budget: a maximum number of reasoning tokens the model can spend. If the model hits the budget, it stops reasoning and generates the best answer it can with the thinking it has completed so far. Budgeting is essential for cost control in production. Without a budget, a single pathological query can consume 50,000 reasoning tokens and cost $15 in a single API call.

The transparency of reasoning tokens varies by provider. OpenAI's GPT-5.2 Thinking API returns both the reasoning trace and the final output, allowing you to inspect what the model was thinking. This is valuable for debugging and trust-building. Google's Gemini 3 Deep Think hides the reasoning trace entirely; you only receive the final output and a token count breakdown. Anthropic's Claude Extended Thinking offers a middle ground: the reasoning trace is available on request but not returned by default. For production systems, inspect the reasoning trace during development to understand how the model approaches your task, then disable it in production to reduce response payload size.

## The Reasoning Model Landscape in January 2026

GPT-5.2 Thinking, released by OpenAI in October 2025, was the first major commercial reasoning model. It builds on the GPT-5 base model but adds a dedicated reasoning phase before output generation. The model excels at mathematical reasoning, multi-step logic puzzles, code debugging, and complex planning tasks. In evals, GPT-5.2 Thinking achieves 92% accuracy on challenging math competition problems where standard GPT-5 scores 61%. On coding challenges requiring multi-file refactoring and bug isolation, GPT-5.2 Thinking succeeds 87% of the time versus 54% for GPT-5. The reasoning token overhead is steep: typical queries consume 3x to 5x as many reasoning tokens as output tokens. A response with 600 output tokens might incur 2,400 reasoning tokens, for a total of 3,000 tokens billed. At $0.03 per 1,000 tokens, that is $0.09 per query. For high-frequency applications processing 100,000 queries per day, that is $9,000 daily or $270,000 monthly. The cost is manageable for high-value queries—financial analysis, medical diagnosis support, legal research—but prohibitive for casual or low-value use cases.

Gemini 3 Deep Think, released by Google in November 2025, is a dedicated reasoning variant of the Gemini 3 family. Unlike GPT-5.2 Thinking, which is a mode you enable on the standard model, Gemini 3 Deep Think is a separate model optimized exclusively for reasoning tasks. Google's approach trades generality for reasoning performance. Gemini 3 Deep Think outperforms GPT-5.2 Thinking on logic-heavy benchmarks—theorem proving, constraint satisfaction, mathematical proofs—achieving 96% accuracy where GPT-5.2 scores 92%. But Gemini 3 Deep Think is weaker on creative or open-ended tasks. It is not designed for storytelling, brainstorming, or casual conversation. It is a specialist tool for specialist tasks. The reasoning token ratio is even higher than GPT-5.2: typical queries consume 5x to 8x as many reasoning tokens as output tokens. Google's pricing partially offsets this by charging $0.02 per 1,000 reasoning tokens versus $0.04 per 1,000 output tokens, a 50% discount recognizing that reasoning tokens are overhead. The result is that Gemini 3 Deep Think is cost-competitive with GPT-5.2 Thinking for reasoning-heavy tasks, despite higher token consumption.

Claude Opus 4.5 Extended Thinking, released by Anthropic in November 2025, takes a more conservative approach. Extended thinking is an optional mode you enable per request, not a separate model. When enabled, Claude spends additional tokens on reasoning, but the reasoning process is more structured and transparent than OpenAI's or Google's implementations. Anthropic trained Claude to explicitly label reasoning steps: "First, I will analyze the constraints. Second, I will generate candidate solutions. Third, I will evaluate each candidate." This structured reasoning is easier to audit and debug. The reasoning token overhead is lower than competitors: 2x to 4x output tokens, not 5x to 8x. This reflects Anthropic's training objective of reasoning efficiency. Claude is trained to think carefully but concisely, avoiding redundant reasoning steps. The cost per query is correspondingly lower. A 500-token output might incur 1,200 reasoning tokens, for 1,700 total tokens at $0.025 per 1,000, or $0.043 per query. This is half the cost of equivalent GPT-5.2 Thinking queries. The trade-off is that Claude Extended Thinking is slightly less capable on the hardest reasoning benchmarks—94% accuracy on math competition problems versus GPT-5.2's 92%, a small but real difference.

DeepSeek R1, released in December 2025, is the open-weight reasoning model. Built on the DeepSeek V3.2 base model, R1 adds reasoning capabilities using reinforcement learning on reasoning traces. The model is fully open: weights, training code, and reasoning trace datasets are publicly available. For organizations willing to self-host, DeepSeek R1 offers reasoning capabilities without per-token API costs. The catch is infrastructure: reasoning models require more GPU memory and compute than standard models. A DeepSeek V3.2 instance might run on four A100 GPUs. DeepSeek R1 requires six to eight A100s to maintain comparable throughput, because reasoning tokens consume memory and compute. The capability is competitive with commercial models on many benchmarks—89% on math competition problems, 82% on coding challenges—but lags on the hardest tasks. DeepSeek R1 is not as well-calibrated as Claude or GPT-5.2. It sometimes generates verbose reasoning traces that do not improve the final answer. But for cost-sensitive applications where you control infrastructure and can tolerate slightly lower accuracy, DeepSeek R1 is a viable option.

The model selection decision depends on task requirements and cost constraints. For the highest accuracy on logic and math-heavy tasks, use Gemini 3 Deep Think. For the best balance of reasoning capability and general-purpose performance, use GPT-5.2 Thinking. For the most cost-efficient reasoning with strong auditability, use Claude Extended Thinking. For zero per-token cost at the expense of infrastructure complexity, use DeepSeek R1. Do not choose based on vendor preference. Eval on your specific tasks and measure accuracy, cost, and latency.

## When Reasoning Models Dramatically Outperform Standard Models

Reasoning models shine on tasks requiring multi-step logic, error correction, constraint satisfaction, and complex planning. The academic benchmarks that demonstrate this are math competition problems, logic puzzles, theorem proving, and code debugging. But the production use cases are more diverse and often surprising. The pattern is that reasoning models help when the task requires the model to consider multiple possibilities, rule out incorrect options, and verify its own work before committing to an answer.

Mathematical reasoning is the canonical use case. A user asks, "If a train leaves station A at 60 mph heading east, and another train leaves station B 180 miles east of A at 40 mph heading west, when and where do they meet?" A standard model often gets this wrong by misapplying formulas or failing to set up the equations correctly. GPT-5 solves it 64% of the time. GPT-5.2 Thinking solves it 97% of the time. The reasoning trace shows the model setting up variables, writing equations for position over time, solving for the intersection, and checking that the result makes physical sense. The extended thinking allows the model to catch algebraic errors and correct them before outputting the final answer.

Code debugging is another strong use case. You provide a buggy function and a description of the incorrect behavior. The task is to identify the bug and propose a fix. Standard models often guess based on pattern matching—"this looks like an off-by-one error"—without deeply analyzing the logic. Reasoning models trace through the code execution step-by-step, tracking variable states, identifying where the actual behavior diverges from the expected behavior, and locating the root cause. On complex multi-file debugging tasks, GPT-5.2 Thinking outperforms standard GPT-5 by 33 percentage points. The reasoning trace often reveals that the model considered multiple hypotheses—"the bug could be in the loop condition or in the accumulator update"—tested each hypothesis, and ruled out incorrect ones before settling on the right answer.

Multi-step planning tasks benefit enormously from reasoning models. A user describes a logistics problem: "I need to ship 5,000 units from three warehouses to four distribution centers, minimizing cost while meeting delivery deadlines and capacity constraints." Standard models generate plausible-sounding plans that violate constraints or miss cost-saving opportunities. Reasoning models enumerate constraints, generate candidate allocations, check each candidate against constraints, calculate costs, and iteratively refine the plan. The output is not just a plan but a justified plan with reasoning steps explaining why this allocation is optimal. For business-critical planning—supply chain optimization, resource allocation, project scheduling—the reasoning trace is as valuable as the final plan because it allows stakeholders to audit the logic.

Complex analysis tasks—financial statement analysis, legal contract review, medical case evaluation—benefit from reasoning models when the task requires synthesizing information from multiple sources and identifying inconsistencies. A reasoning model analyzing a financial statement might notice that revenue growth is 15% year-over-year but accounts receivable grew 40%, a red flag for revenue recognition issues. The reasoning trace shows the model calculating the ratios, comparing them to industry norms, considering alternative explanations, and flagging the anomaly. Standard models sometimes catch these issues, but reasoning models catch them more reliably because they explicitly check for inconsistencies as part of the reasoning process.

Creative problem-solving with constraints is a less obvious use case but highly valuable. A user asks, "Design a database schema for a multi-tenant SaaS application with strong data isolation, support for custom fields per tenant, and efficient querying." Standard models output a schema quickly but often miss edge cases—how do you handle schema migrations when tenants have custom fields, or how do you index custom fields efficiently. Reasoning models spend tokens considering these edge cases, evaluating trade-offs between approaches, and proposing a schema that addresses the constraints. The extended thinking does not guarantee a perfect solution, but it dramatically reduces the probability of overlooked requirements.

The common thread is that reasoning models help when correctness requires deliberation. If the first plausible answer is usually correct, standard models are sufficient. If correctness requires checking your work, considering alternatives, or verifying constraints, reasoning models add significant value.

## When Reasoning Models Do Not Help: Simple Tasks and Creative Work

Reasoning models are not universally superior. There are broad categories of tasks where they add cost and latency without improving output quality. Understanding when not to use reasoning models is as important as understanding when to use them. The failure pattern is over-applying reasoning models because they are new and impressive, paying 5x the token cost for tasks that do not benefit from extended thinking.

Simple classification tasks do not benefit from reasoning. A user uploads an image and asks, "Is this a cat or a dog?" Extended thinking does not help. The model either recognizes the animal or it does not. Spending 3,000 reasoning tokens to classify an image that a standard model classifies correctly in 50 tokens is wasteful. Similarly, sentiment analysis, spam detection, language identification, and other single-step classification tasks show no accuracy improvement with reasoning models. The decision is immediate and does not require multi-step logic.

Extraction tasks rarely benefit from reasoning unless the extraction logic is complex. Pulling a name, email, and phone number from a resume is a lookup task. Standard models perform at 96% accuracy. Reasoning models also perform at 96% accuracy but consume 4x the tokens. The reasoning trace shows the model identifying the fields, extracting them, and formatting the output—steps that standard models perform implicitly without needing to generate reasoning tokens. The exception is extraction with validation or disambiguation. If the task is "extract the email address and verify it is a valid format," reasoning models help because they can generate reasoning tokens checking the regex pattern. If the task is "extract the company name, disambiguating between the current employer and past employers mentioned," reasoning models help because they must track context and make a judgment call.

Creative writing tasks do not benefit from reasoning and may be harmed by it. Writing a marketing email, drafting a blog post, generating product descriptions, composing fiction—these tasks require fluency, tone, and creativity, not logical correctness. Reasoning models optimized for logic often produce stilted or overly formal creative outputs. The reasoning trace for a creative task is not useful: "First, I will introduce the topic. Second, I will provide supporting details. Third, I will conclude." This is not reasoning; it is boilerplate outlining. Standard models trained on diverse text generate more natural, engaging creative outputs. The one exception is creative work with constraints: "Write a poem in iambic pentameter about climate change." The constraint satisfaction benefits from reasoning, but most creative tasks are unconstrained.

Summarization tasks show mixed results. Summarizing a news article or meeting transcript is primarily a compression task. Standard models perform well. Reasoning models offer no advantage and add cost. Summarizing a complex technical document with accuracy requirements—a legal brief, a scientific paper—benefits from reasoning if the model needs to identify key claims, check for consistency, and ensure critical details are not omitted. The decision depends on whether accuracy errors in the summary have consequences. For casual summaries, use standard models. For high-stakes summaries, eval both and measure whether reasoning improves accuracy enough to justify the cost.

Conversational tasks—customer support chat, personal assistant queries, casual Q&A—rarely benefit from reasoning unless the query is complex. A user asks, "What is your return policy?" This is a retrieval task. Reasoning tokens do not help. A user asks, "I ordered three items, returned one, was charged for two, but my account shows three charges. What happened?" This requires multi-step reasoning: reconstructing the transaction history, identifying the discrepancy, proposing explanations. Reasoning models help here. The guideline is that simple, single-turn queries use standard models, and multi-turn or multi-step queries use reasoning models selectively.

The evaluation method is straightforward: run both a standard model and a reasoning model on your eval set, measure accuracy, and calculate cost per query. If the reasoning model improves accuracy by less than 2 percentage points and costs 3x more, it is not worth deploying universally. If it improves accuracy by 10 percentage points on high-value queries, it is worth deploying selectively.

## The Cost Equation: Reasoning Tokens and Total Cost of Ownership

The cost structure of reasoning models is deceptive. The per-token price is the same as standard models, but token consumption is 3x to 10x higher due to reasoning tokens. A naive deployment can increase your monthly AI spend by 400% without delivering proportional value. Production-grade reasoning model deployments require explicit cost management: reasoning token budgets, selective routing, query complexity classification, and continuous cost monitoring.

The first step is measuring reasoning token consumption. Most providers return token count breakdowns in the API response: input tokens, output tokens, reasoning tokens. Log these for every query. Calculate the reasoning-to-output ratio per query type. You will discover that some query types consistently trigger high reasoning token usage—multi-step math problems, code debugging—while others trigger minimal reasoning—simple lookups, classification. This data informs routing decisions.

The second step is setting reasoning token budgets. OpenAI, Google, and Anthropic all support a max reasoning tokens parameter. Set this based on your cost tolerance and task requirements. For a customer support application, you might set a 2,000 token reasoning budget. Most queries use 500 to 1,000 reasoning tokens. Complex queries hit the 2,000 limit and return the best answer the model can produce with that thinking budget. Occasionally the answer is incomplete, but the cost is capped. For a financial analysis application where accuracy is critical, you might set a 10,000 token budget or leave it unlimited, accepting the cost in exchange for thoroughness.

The third step is selective routing based on query complexity. Not all queries need reasoning models. Build a lightweight classifier—this can be a simple heuristic, a fine-tuned model, or even a prompt to a cheap model—that categorizes incoming queries as simple, moderate, or complex. Route simple queries to standard models, moderate queries to reasoning models with tight budgets, complex queries to reasoning models with generous budgets. This three-tier routing can reduce total token consumption by 60% while preserving accuracy on the queries that matter.

The heuristic approach to query classification is surprisingly effective. Flag queries as complex if they contain certain keywords: "calculate," "debug," "compare," "analyze," "plan," "optimize." Flag queries as simple if they are short—less than 50 tokens—or match common templates: "What is," "How do I," "Can you summarize." Route everything else to moderate. This heuristic is 75% accurate at separating queries that benefit from reasoning from those that do not, with zero added latency or cost for classification.

The model-based approach is more accurate but adds latency and cost. Use a small, fast model—GPT-5 mini, Gemini 2.0 Flash, or a fine-tuned classifier—to predict whether a query is complex enough to benefit from reasoning. Train the classifier on historical queries labeled with reasoning token usage and accuracy deltas. The classifier learns patterns: queries mentioning multiple entities, queries with conditional logic, queries requiring numerical calculations. Accuracy reaches 88% with negligent latency—30 to 80 milliseconds—and minimal cost—fractions of a cent per classification.

The fourth step is continuous monitoring and optimization. Track cost per query, reasoning token usage, and accuracy by query type weekly. Identify query types where reasoning models are over-applied—high reasoning token usage but no accuracy gain—and route those to standard models. Identify query types where reasoning models are under-applied—low reasoning token usage but significant accuracy gain potential—and route more of those to reasoning models. This iterative optimization reduces waste and improves ROI.

The total cost of ownership calculation must include manual review costs. If a standard model costs $0.01 per query and achieves 82% accuracy, requiring 18% manual review at $2 per review, the total cost is $0.01 plus 0.18 times $2, or $0.37 per query. If a reasoning model costs $0.05 per query and achieves 96% accuracy, requiring 4% manual review, the total cost is $0.05 plus 0.04 times $2, or $0.13 per query. The reasoning model is 5x more expensive per API call but 65% cheaper in total cost because it reduces manual review. Always calculate total cost, not just API cost.

## Latency Implications: Extended Thinking Adds Seconds

Reasoning tokens are generated before output tokens, and generation is sequential. If a reasoning model generates 4,000 reasoning tokens before generating 800 output tokens, and the model generates 50 tokens per second, the reasoning phase alone takes 80 seconds. Output generation takes another 16 seconds. Total latency is 96 seconds. This is incompatible with real-time user-facing applications. Reasoning models are viable only for use cases that tolerate multi-second or even multi-minute latencies.

The latency breakdown varies by provider and model. GPT-5.2 Thinking generates reasoning tokens at approximately 40 tokens per second. A query consuming 3,000 reasoning tokens takes 75 seconds for the reasoning phase, plus output generation time. Google's Gemini 3 Deep Think is slightly faster, 50 to 60 tokens per second, but often generates more reasoning tokens, so end-to-end latency is comparable. Claude Extended Thinking is the fastest reasoning model, generating 60 to 70 tokens per second and using fewer reasoning tokens, resulting in typical latencies of 15 to 45 seconds for complex queries.

For user-facing applications, these latencies are unacceptable. A user asking a question in a chat interface expects a response in 2 to 5 seconds. Waiting 30 seconds is perceived as a broken system. Reasoning models are viable only for asynchronous workflows: the user submits a query, receives a job ID, and is notified when the response is ready. This works for research tools, financial analysis dashboards, legal research platforms, and batch processing workflows. It does not work for real-time chat, instant customer support, or interactive assistants.

The mitigation strategy is progressive response streaming. Some reasoning model APIs support streaming: the model streams the reasoning trace as it is generated, then streams the final output. The user sees the model "thinking" in real-time. This does not reduce latency but makes the wait more tolerable by providing transparency and progress indication. For a 40-second response, streaming the reasoning trace makes the experience feel interactive rather than hung. OpenAI's GPT-5.2 Thinking supports streaming. Anthropic's Claude Extended Thinking supports streaming. Google's Gemini 3 Deep Think does not stream reasoning traces, only final outputs.

Another mitigation is speculative routing. When a query arrives, route it to both a standard model and a reasoning model in parallel. The standard model returns quickly—2 to 4 seconds. If the reasoning model has not finished by the time the standard model completes, return the standard model's answer with a flag indicating a more thorough answer is being computed. If the reasoning model finishes within the latency budget—say, 8 seconds—return its answer instead. This approach provides a fast answer for most queries and a better answer for complex queries when time allows. The cost is that you pay for both API calls, but you only surface the reasoning model output when it completes fast enough, avoiding the latency penalty for slow queries.

The latency-accuracy trade-off is explicit. Reasoning models deliver higher accuracy at the cost of 10x to 50x higher latency. You must decide whether your use case tolerates the latency. For high-stakes, low-frequency tasks—legal analysis, medical diagnosis support, financial auditing—the latency is acceptable. For high-frequency, latency-sensitive tasks—customer support chat, content moderation, real-time recommendations—reasoning models are not viable in the request path. Use them in offline analysis and model distillation workflows instead.

## Production Patterns: Using Reasoning Models Selectively, Not as Default

The correct production architecture for reasoning models is selective routing, not universal deployment. Reasoning models are specialist tools for complex queries, not general-purpose replacements for standard models. The deployment pattern is a multi-tier routing system: cheap fast models for simple queries, standard models for moderate queries, reasoning models for complex queries. This architecture balances cost, latency, and accuracy.

The routing logic sits in front of your model calls. Incoming queries are classified by complexity using heuristics, a lightweight classifier, or a hybrid approach. Simple queries—lookups, single-fact questions, basic classifications—route to a small fast model like GPT-5 mini or Gemini 2.0 Flash. These models cost 10x less than reasoning models and respond in under 2 seconds. Moderate queries—multi-sentence answers, summaries, explanations—route to standard models like GPT-5 or Claude Opus 4.5. These models provide strong performance at reasonable cost and latency. Complex queries—multi-step reasoning, debugging, analysis—route to reasoning models with appropriate token budgets.

The classification logic should be data-driven. Start by logging all queries and manually labeling 500 to 1,000 examples as simple, moderate, or complex. Train a classifier on these labels. Deploy the classifier in the routing layer. Monitor misclassifications—queries routed to reasoning models that could have been handled by standard models, or queries routed to standard models that would have benefited from reasoning. Retrain the classifier monthly with new labeled examples. Over time, classification accuracy improves from 75% to 90%, and total system cost drops by 40% to 60% compared to universal reasoning model deployment.

The token budget strategy should vary by query complexity. Simple queries routed to standard models have no reasoning token budget. Moderate queries routed to reasoning models have a tight budget—1,000 to 2,000 tokens—to control cost. Complex queries have a generous budget—5,000 to 10,000 tokens—or no limit if accuracy is paramount. The budget prevents pathological queries from consuming 50,000 reasoning tokens and costing $1.50 in a single call.

The fallback strategy handles cases where the reasoning model fails or times out. Set a timeout—20 seconds, 60 seconds, whatever your application can tolerate. If the reasoning model does not respond within the timeout, cancel the request and fall back to a standard model. Return the standard model's answer with a flag indicating it is a fallback response. Log the failure for analysis. If a particular query type consistently times out, adjust the routing to send those queries to standard models instead.

The cost monitoring dashboard tracks total spending, spending per query type, reasoning token usage, and cost per accuracy point. Calculate the marginal cost of reasoning: the additional cost to achieve each percentage point of accuracy improvement. If reasoning models cost $0.08 per query and achieve 94% accuracy, and standard models cost $0.015 per query and achieve 88% accuracy, the marginal cost is 0.065 dollars divided by 6 percentage points, or $0.011 per percentage point. If your business model supports that cost—fewer manual reviews, higher customer satisfaction, reduced error costs—deploy reasoning models for those queries. If not, stick with standard models and accept the accuracy trade-off.

The future trajectory of reasoning models is clear: costs will decrease, latencies will improve, and reasoning capabilities will expand to more model families. Within 18 months, reasoning models will likely be cost-competitive with today's standard models and fast enough for near-real-time applications. But in January 2026, reasoning models are specialist tools requiring careful cost and latency management. Deploy them selectively, measure their impact rigorously, and route intelligently based on query complexity. The organizations that master selective reasoning model deployment will achieve both superior accuracy and controlled costs. The organizations that deploy reasoning models universally without cost management will blow their budgets and gain little incremental value.

The next subchapter examines open-weight models—Llama 4, DeepSeek V3.2, Qwen3—and the build-versus-buy decision for model infrastructure.
