# 9.14 â€” Migration Runbooks: Step-by-Step Procedures for Model Swaps and Provider Changes

In October 2025, a financial services company decided to migrate from GPT-5 to Claude Opus 4.5 based on A/B test results showing a 5.8 percent improvement in task accuracy. The migration was scheduled for a Saturday morning to minimize user impact. The platform engineer on call began the switch at 9 AM by updating configuration files to point to the new model endpoint. Within fifteen minutes, error rates spiked to 12 percent. The engineer discovered that Claude's API required different authentication headers than OpenAI's API, which was not documented in their internal migration plan. Thirty minutes in, they noticed response formatting had broken for three of their seven task types because Claude's output structure differed subtly from GPT-5's. By 10 AM, they decided to roll back, but the rollback procedure was not written down, and the engineer spent twenty minutes searching Slack history to figure out which configuration files needed reverting. The rollback itself triggered a caching bug that required an additional forty minutes to resolve. By the time production was stable again, the incident had lasted two hours, affected 4,200 customer sessions, and generated 89 support tickets. The root cause was not technical complexity. It was the absence of a tested, written, step-by-step runbook that accounted for all the operational details that seem obvious in planning but get forgotten under pressure.

A migration runbook is a written procedure that documents every step required to switch models or providers, every verification check to confirm the switch succeeded, and every rollback step if something goes wrong. Runbooks are not nice-to-have documentation. They are operational prerequisites. Migrating models without a runbook is professional negligence.

## Why Runbooks Matter: Stress, Time Pressure, and Forgetting

Model migrations are stressful. You are changing a core component of your production system, often during a scheduled maintenance window with a hard deadline, with stakeholders monitoring the process and users affected by any mistakes. Stress degrades cognitive performance. Under pressure, you forget steps, skip validations, and make errors you would never make during calm planning.

Runbooks externalize memory. Instead of relying on the on-call engineer to remember that the new provider requires rate limit headers or that the output parser needs updating or that cache invalidation must happen before the configuration change, you write it down in sequence with verification steps. The engineer follows the runbook mechanically, checking off each step as completed, and the procedure gets executed correctly even if the engineer is tired, distracted, or unfamiliar with this specific migration.

Runbooks standardize procedures across the team. Without a runbook, each engineer performs migrations slightly differently, sometimes skipping steps they consider optional, sometimes adding ad-hoc checks that others do not know about. When a migration fails and you need to investigate, you cannot reconstruct what actually happened because there is no canonical procedure. With a runbook, you know exactly what steps were executed, and you can diff the runbook against actual actions to identify deviations.

Runbooks survive team turnover. The engineer who planned and executed your first Claude migration leaves the company. Six months later, you need to migrate from Claude Opus 4.5 to Claude Opus 5. Without a runbook, the new engineer rediscovers all the operational details the previous engineer learned, repeating mistakes and wasting time. With a runbook, the institutional knowledge persists, and the new engineer executes the migration correctly on the first attempt.

Runbooks enable dry runs. Before your production migration, you execute the runbook in a staging environment, discover gaps or errors, update the runbook, and repeat until the procedure works smoothly. Dry runs catch issues like missing permissions, incorrect endpoint URLs, or incompatible API versions before they cause production incidents.

If you are planning a model migration and you do not have a written runbook, you are not ready to migrate. You write the runbook first, test it in staging, then execute it in production.

## The Runbook Template: Structure and Components

A complete runbook has six sections: prerequisites and preparation, pre-migration checklist, migration steps, verification steps, rollback procedure, and post-migration cleanup. Each section is written as a numbered list of imperative commands with expected outcomes.

**Prerequisites and preparation** documents what must be true before you start the migration. This includes access credentials for the new provider, budget approval for any cost changes, confirmation that the new model has been tested in staging, availability of on-call engineers, and communication to stakeholders about the maintenance window. Every prerequisite is a yes-no checklist item. If any item is not checked, you do not proceed.

**Pre-migration checklist** documents snapshot and backup steps that ensure you can roll back if necessary. This includes taking database snapshots, backing up current configuration files, recording baseline metrics for comparison, and creating a rollback branch in version control. You complete the pre-migration checklist immediately before starting the migration steps, and you verify every item before proceeding.

**Migration steps** is the numbered sequence of actions required to switch models. Each step is one imperative sentence: update the model configuration file, deploy the updated service, restart the API gateway, invalidate the response cache. Each step includes the expected outcome: after updating the configuration file, the new model endpoint should appear in the config validation output. Each step includes timing estimates: this deployment takes approximately three minutes. The migration steps are executed in strict order without skipping or reordering.

**Verification steps** documents how you confirm the migration succeeded. This includes checks like sending test requests and verifying responses match expected format, checking error rate dashboards and confirming they remain below baseline, querying logs to verify the new model is receiving traffic, and running end-to-end smoke tests for critical workflows. Verification is not optional. You do not declare the migration complete until all verification steps pass.

**Rollback procedure** documents the exact steps to undo the migration and return to the previous state. Rollback steps are the inverse of migration steps, executed in reverse order: restore the previous configuration file from backup, redeploy the service, restart the API gateway, restore the response cache. Rollback includes its own verification: after rollback, test requests should produce responses identical to pre-migration responses. Rollback also includes a trigger criteria section that defines when you abort the migration and roll back: if error rate exceeds 2 percent for more than five minutes, roll back immediately.

**Post-migration cleanup** documents housekeeping tasks after a successful migration: deleting old configuration backups after confirming stability for 48 hours, updating documentation to reflect the new model, notifying stakeholders that the migration is complete, and scheduling a retrospective to review the process. Cleanup steps have longer timelines, typically 24 to 72 hours after migration, but they are still documented and tracked.

Each section is written in the second person imperative: "you update the config file," "you verify error rates." This style is direct and reduces cognitive load compared to passive voice or procedural descriptions.

## Provider Swap Runbook: Changing API Providers

A provider swap runbook handles migrations from one API provider to another: OpenAI to Anthropic, Anthropic to Google, or any other provider change. Provider swaps are higher risk than model version upgrades because API contracts, authentication mechanisms, rate limiting, error handling, and output formatting all differ between providers.

The provider swap runbook starts with **API contract verification**: you document the differences between the current provider's API and the new provider's API. This includes endpoint URLs, authentication header formats, request body schemas, response body schemas, error response formats, rate limit headers, timeout behavior, and streaming protocols. You test each difference in staging and document how your client code handles it.

Authentication is often the first failure point. OpenAI uses bearer token authentication with API keys in the authorization header. Anthropic uses API key authentication with a custom header format. Google uses OAuth 2.0 with service account credentials. Your runbook includes the exact authentication configuration for the new provider, including where credentials are stored, how they are rotated, and what permissions they require. You verify authentication works in staging before attempting the production swap.

Request and response format differences must be mapped explicitly. If your system sends requests in OpenAI's message format with role and content fields, and the new provider expects a different schema, your runbook documents the transformation logic, where it is implemented, and how it has been tested. If response parsing depends on specific field names or nesting structures, your runbook includes verification that the new provider's responses parse correctly.

Rate limiting and retry logic differ between providers. OpenAI returns 429 status codes with retry-after headers when you exceed rate limits. Anthropic returns 529 status codes with different retry guidance. Your client library must handle the new provider's rate limit signals, and your runbook includes load testing to verify rate limit handling works correctly before production traffic hits the new provider.

Error handling is provider-specific. Every provider has unique error codes, error messages, and transient versus permanent failure signals. Your runbook documents the new provider's error taxonomy, maps error codes to your internal error handling logic, and includes test cases that trigger each error type to verify your system responds appropriately.

The migration steps for a provider swap include: updating API endpoint configuration, updating authentication credentials, deploying updated client library code that handles the new provider's API format, invalidating caches that might contain responses from the old provider, and gradually shifting traffic using a percentage-based rollout rather than an all-at-once cutover. You start with 5 percent of traffic, verify stability, move to 25 percent, verify again, then complete the shift to 100 percent. Each traffic increment is a separate step in the runbook with its own verification criteria.

Rollback for a provider swap includes reverting configuration, reverting client library code, and shifting traffic back to the old provider. Because provider swaps involve code changes, rollback is slower than simple config changes, and your runbook must account for deployment time in rollback duration estimates.

## Model Version Upgrade Runbook: Same Provider, New Version

A model version upgrade runbook handles migrations from one version of a model to a newer version from the same provider: GPT-5 to GPT-5, Claude Opus 4 to Claude Opus 4.5, Llama 4 to Llama 4 Maverick. Version upgrades are lower risk than provider swaps because the API contract usually remains stable, but they still require careful verification because model behavior changes.

The version upgrade runbook starts with **behavioral diff analysis**: you document known differences between the old and new model versions. This includes changes in output formatting, changes in instruction following, changes in safety filtering, and changes in reasoning quality. You run your staging test suite against both versions and document any test failures or unexpected outputs from the new version. If the new version formats structured outputs differently, you update parsers before migration. If the new version interprets certain prompt patterns differently, you update prompts before migration.

Prompt compatibility is the most common failure mode in version upgrades. A prompt that worked reliably with GPT-5 might produce inconsistent outputs with GPT-5 because the new model interprets instructions differently or prioritizes different aspects of the prompt. Your runbook includes regression testing all critical prompts with the new version, identifying any prompts that need updating, and verifying that updated prompts work correctly in staging.

Cost changes are documented explicitly. Model version upgrades often come with price changes: newer models might cost more per token, or they might use tokens more efficiently and reduce costs despite higher per-token prices. Your runbook includes cost projections based on your current traffic volume and expected usage patterns with the new version. If the cost increase exceeds budget, the migration is paused pending budget approval.

Latency changes are measured and documented. Newer models might be faster due to infrastructure improvements, or they might be slower due to increased model size. Your runbook includes latency benchmarks from staging tests, expected p50 and p95 latency changes, and verification that the new latency profile meets your performance requirements.

The migration steps for a version upgrade are simpler than provider swaps: update the model version identifier in configuration, deploy the updated configuration, and shift traffic gradually from the old version to the new version using percentage-based rollout. Because the API contract is stable, no code changes are required unless prompt updates were needed based on behavioral diff analysis.

Verification includes output quality checks: you sample responses from the new model version and verify they meet quality standards, you monitor task success rates and confirm they do not degrade, and you check for any new failure modes that did not appear in staging testing.

Rollback for a version upgrade is straightforward: revert the configuration change to specify the old model version, redeploy, and verify that responses return to baseline quality. Because no code changes are involved, rollback completes in minutes.

## Emergency Migration Runbook: Provider Outage Response

An emergency migration runbook handles the scenario where your current provider experiences an outage, and you must switch to a backup provider immediately to restore service. Emergency migrations are higher risk than planned migrations because you cannot perform gradual rollouts or extensive verification, but they are necessary to maintain availability.

The emergency runbook starts with **outage detection and classification**: you document how you detect that the current provider is down, how you distinguish transient errors from full outages, and how you decide when to trigger emergency migration. The trigger criteria is explicit: if the current provider returns errors for more than 80 percent of requests for more than five minutes, and the provider's status page confirms a major outage, you execute the emergency migration.

Your emergency runbook assumes you have already configured and tested a backup provider in staging. The backup provider is not a hypothetical option. It is a fully configured, tested, ready-to-activate alternative that you maintain continuously. Maintaining a hot backup adds cost because you must keep API credentials active and periodically test the backup to ensure it still works, but the cost is trivial compared to the cost of an extended outage.

The emergency migration steps are streamlined: update configuration to point to the backup provider, deploy the configuration change, verify that requests are reaching the backup provider and receiving responses, and communicate to users that service is restored. Emergency migrations skip gradual rollout and verification steps that would delay restoration. You accept some risk of degraded quality or increased error rates because the alternative is zero availability.

Verification in emergency migrations is limited to availability checks: you confirm that requests are succeeding, error rates are below critical thresholds, and key workflows are functional. You do not verify output quality deeply because quality verification takes time and the priority is restoring service.

Once the original provider restores service, you execute a **return migration** to switch back from the backup provider to the primary provider. The return migration follows the standard planned migration runbook with gradual rollout and full verification, because you are no longer in an emergency state and you can afford to take the time to migrate carefully.

The emergency runbook includes communication templates: messages to send to internal stakeholders when the emergency migration is triggered, messages to send to users explaining the service disruption, and messages to send when service is restored. Communication is pre-written so that during the outage, the on-call engineer can send notifications immediately without composing messages under stress.

## Runbook Testing: Dry Runs Before Production

A runbook that has never been executed is a work of fiction. You do not know whether it is accurate, complete, or actually executable until you run it. Runbook testing happens in staging environments before production migrations.

A dry run is a complete execution of the runbook in staging, performed by an engineer who was not involved in writing the runbook. This verifies that the runbook is clear, unambiguous, and does not assume knowledge that is not documented. If the engineer gets stuck on any step, asks for clarification, or has to improvise, the runbook is incomplete and must be updated.

Dry runs are scheduled events, not ad-hoc exercises. You schedule a two-hour block, gather the team members who will execute the production migration, and execute the staging migration following the runbook verbatim. You measure how long each step takes, identify any steps that fail or produce unexpected outcomes, and note any verification checks that did not work as written.

After the dry run, you update the runbook to fix any issues discovered. Then you run it again. Most runbooks require two to three dry runs before they are production-ready. The first dry run typically finds five to ten issues. The second dry run finds one to three issues. The third dry run should execute cleanly with no surprises.

Dry runs are also used to train new team members. When a new engineer joins the team, they participate in a staging runbook execution as part of onboarding. This teaches them the migration process, familiarizes them with the production systems, and builds confidence before they execute migrations independently.

Runbook testing also includes **chaos scenarios**: you intentionally introduce failures during the staging migration to verify the rollback procedure works. You might corrupt a configuration file mid-migration, or simulate a deployment failure, or trigger an artificial error rate spike, and verify that the rollback procedure successfully restores the system to the pre-migration state. Chaos testing is how you discover that your rollback steps are missing a cache invalidation or that your verification checks do not actually detect certain failure modes.

## Runbook Maintenance: Updating After Each Migration

Runbooks are living documents. After every production migration, you update the runbook to reflect what actually happened versus what the runbook predicted.

Post-migration retrospectives include a runbook review: you compare the executed steps against the documented steps, identify any deviations, and discuss whether the deviations represent missing runbook content or correct improvisation due to unexpected circumstances. If an engineer had to add an unplanned step during the migration, that step gets added to the runbook. If a verification check did not work as documented, the check is corrected. If timing estimates were inaccurate, they are updated based on actual measurements.

Runbooks also evolve as your system changes. If you add new services that depend on model outputs, those services must be included in verification steps. If you change your deployment pipeline, migration steps that reference the old pipeline must be updated. If a provider changes their API, authentication steps must be revised. Runbook maintenance is part of your standard change management process: any change to production systems triggers a review of affected runbooks.

Outdated runbooks are worse than no runbooks. If your runbook documents a configuration file that no longer exists, or a verification dashboard that has been deprecated, or a rollback procedure that references infrastructure you decommissioned, an engineer following the runbook will get stuck, lose trust in the documentation, and start improvising. Runbook accuracy is maintained through continuous updates, not through exhaustive up-front documentation.

You version-control runbooks in the same repository as your infrastructure code. Each runbook is a markdown file, changes are made via pull requests with review, and the version history preserves the evolution of the procedure over time. You can see what changed between migrations, who made the change, and why.

## The Runbook as Institutional Knowledge

A well-maintained set of runbooks is one of the best indicators of operational maturity. Teams with comprehensive, tested, regularly-updated runbooks execute migrations smoothly, rarely trigger production incidents, and onboard new engineers quickly. Teams without runbooks rely on heroics, suffer from repeated mistakes, and accumulate operational risk.

Runbooks are not bureaucracy. They are operationalization of lessons learned. Every step in a runbook represents a mistake someone made once that you are preventing from happening again. Every verification check represents a failure mode someone discovered that you are now guarding against. Runbooks are how you scale operational excellence beyond the founding team who built the system and remember all its quirks.

When you write a runbook, you are writing for your future self during an outage at 3 AM, for the new engineer who joined last week, and for the team that will inherit your system when you move to a new role. Write clearly, write completely, and test rigorously. Your runbook quality directly determines your incident rate and your team's stress level during migrations.

Model migrations are inevitable. Providers deprecate old versions, new models offer better performance, costs change, and you must adapt. The question is not whether you will migrate, but whether you will migrate with confidence and control, or with anxiety and chaos. Runbooks are the difference.

With proper A/B testing to validate model changes and proper runbooks to execute them safely, you have closed the loop on model selection and deployment: you can identify when a model switch is justified, prove it with data, and execute the change without operational drama. This is the foundation of mature model management.
