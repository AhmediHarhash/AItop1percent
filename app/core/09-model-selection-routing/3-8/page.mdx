# 3.8 â€” Risk-Aware Routing: Sending High-Stakes Queries to Safer Models

In late 2025, a mental health support chatbot serving 80,000 users routed all queries to GPT-5 based on query complexity: simple questions went to GPT-5-mini, complex questions went to GPT-5. The routing system optimized for cost and latency, with no consideration for the risk profile of individual queries. Three months after launch, the platform received a complaint from a user who had asked the chatbot for advice on self-harm and received a response that, while not explicitly harmful, lacked the careful safety framing and crisis resource information that a high-risk query requires. The engineering team reviewed the logs and found that the query had been routed to GPT-5-mini because it was syntactically simple and short. The mini model generated a response that was factually adequate but not aligned with the safety protocols the platform had committed to in its terms of service and regulatory filings. The incident triggered a review by the platform's legal and trust teams, who identified 247 other high-risk queries over the prior three months that had been routed to models without sufficient safety alignment. The root cause was not model failure but routing failure: the system treated all queries as equivalent in risk and routed based solely on complexity and cost, ignoring the fact that some queries require models with stronger safety calibration, lower hallucination rates, and better refusal behavior. The team implemented risk-aware routing within two weeks, sending all high-risk queries to Claude Opus 4.5, which had the lowest hallucination rate and strongest safety alignment in their model portfolio. Incident rates for high-risk queries dropped by 91% over the next quarter.

Risk-aware routing solves a problem that complexity-based routing and cost-based routing ignore: not all queries are equal in potential harm. A query asking for a recipe is low-risk even if it is complex. A query asking for medical advice is high-risk even if it is syntactically simple. You must detect the risk level of each query and route high-stakes queries to models that have been trained, fine-tuned, and tested for safety, factuality, and compliance. This is not optional. Under the EU AI Act, which came into full enforcement in 2025, certain high-risk AI applications must use models that meet specific safety and transparency thresholds. Routing a high-risk query to a model that does not meet those thresholds is a compliance violation that can result in fines up to 6% of global revenue.

## Detecting Risk Signals in Queries

The first step in risk-aware routing is identifying which queries carry high risk. Risk signals fall into several categories: harm-related content, regulated domains, sensitive personal information, and high-consequence decision-making. Harm-related content includes queries about self-harm, violence, illegal activity, child safety, and hate speech. Regulated domains include medical diagnosis, legal advice, financial investment guidance, and mental health counseling. Sensitive personal information includes queries that mention protected health information, financial account details, or personally identifiable information about minors. High-consequence decision-making includes queries where the model output will directly influence life-altering decisions such as medical treatment, legal strategy, or financial investment.

You detect these signals using a combination of keyword matching, classifier-based detection, and embedding-based detection. Keyword matching is the fastest and simplest approach: you maintain a list of high-risk keywords and phrases, and any query containing one or more of these terms is flagged as high-risk. The mental health chatbot used a keyword list with 340 terms including "suicide," "self-harm," "overdose," "cutting," and "kill myself." This approach has high precision but low recall because users often phrase high-risk queries using euphemisms, metaphors, or indirect language that keyword lists miss. A user asking "I want to stop feeling anything" or "What is the easiest way to disappear" may be expressing suicidal ideation without using any flagged keywords.

Classifier-based detection addresses the recall problem by training a binary or multi-class classifier to predict whether a query belongs to one or more risk categories. You train the classifier on labeled examples of high-risk and low-risk queries, and at runtime you score each incoming query and flag it if the classifier confidence exceeds a threshold. The mental health chatbot trained a classifier on 15,000 labeled queries using a BERT-based model fine-tuned on mental health text. The classifier achieved 94% recall and 89% precision on a held-out test set, catching euphemistic and indirect expressions of risk that keyword lists missed. The tradeoff is latency: the classifier added 45 milliseconds to the routing pipeline, which was acceptable for a mental health application where safety outweighs speed.

Embedding-based detection works by embedding the query and comparing it to prototype embeddings of known high-risk queries. If the cosine similarity to any high-risk prototype exceeds a threshold, the query is flagged. This approach generalizes well to novel phrasings and paraphrases but requires a well-curated set of high-risk prototypes and a high-quality embedding model. A financial advice chatbot used embedding-based detection to flag queries similar to known fraudulent investment schemes and high-risk trading questions. The system embedded each query using OpenAI's text-embedding-3-large model and compared it to 500 high-risk prototype embeddings. Queries with similarity above 0.78 were flagged as high-risk and routed to GPT-5.2 reasoning mode, which has stronger factuality guarantees than GPT-5 base. The embedding approach caught 87% of high-risk queries with a false positive rate of 6%, which was acceptable given the high cost of false negatives.

## The Risk-Model Mapping

Once you have identified high-risk queries, you must map risk levels to model choices. The mapping depends on the safety, factuality, and alignment characteristics of each model in your portfolio. High-risk queries should go to models with lower hallucination rates, better refusal calibration, and stronger safety alignment. Claude Opus 4.5 is the industry-leading model for safety as of early 2026, with refusal rates on unsafe prompts exceeding 98% and hallucination rates on factual questions below 2% in independent benchmarks. GPT-5.2 reasoning mode offers the strongest factuality guarantees for high-stakes reasoning tasks, with multi-step verification that reduces hallucination rates to under 1% on complex factual questions. Llama 4 and DeepSeek V3.2 have lower safety alignment and higher hallucination rates, making them unsuitable for high-risk queries even though they are faster and cheaper.

The mental health chatbot implemented a two-tier risk mapping: high-risk queries about self-harm, violence, or child safety went to Claude Opus 4.5, which has the strongest safety alignment. Medium-risk queries about general mental health questions, coping strategies, and emotional support went to GPT-5, which balances safety and cost. Low-risk queries about scheduling, account settings, and general wellness went to GPT-5-mini. This mapping reduced incidents on high-risk queries by 91% while keeping median routing cost at $0.04 per query, compared to $0.12 per query if all queries went to Claude Opus 4.5.

A legal advice chatbot used a similar mapping: queries about criminal defense, child custody, and bankruptcy went to GPT-5.2 reasoning mode because these are high-consequence domains where factual errors can result in severe harm. Queries about contract interpretation, small claims disputes, and landlord-tenant issues went to GPT-5 base. Queries about general legal definitions and procedural questions went to GPT-5. The mapping was based on two factors: the potential harm of an incorrect answer and the complexity of the legal reasoning required. High-consequence domains required both strong reasoning and low hallucination rates, so they were routed to the most capable model regardless of cost.

## The False Positive and False Negative Tradeoff

Risk detection always involves a tradeoff between false positives and false negatives. A false positive occurs when you classify a low-risk query as high-risk and route it to an expensive, slow model unnecessarily. A false negative occurs when you classify a high-risk query as low-risk and route it to a model that lacks the safety or factuality guarantees the query requires. False negatives are far more dangerous than false positives because they expose users to harm and expose your organization to liability and regulatory risk.

The mental health chatbot tuned their risk classifier to accept a false positive rate of 12% in exchange for a false negative rate of 3%. This meant that 12% of low-risk queries were routed to Claude Opus 4.5 unnecessarily, increasing per-query cost from $0.02 to $0.08 for those queries. But it also meant that 97% of high-risk queries were correctly identified and routed to the safest model. The incremental cost was $18,000 per month across 80,000 users, which the organization accepted as the cost of doing business in a high-stakes domain. The alternative, optimizing for cost by reducing false positives, would have increased false negatives to 8%, sending 24 additional high-risk queries per day to models without adequate safety alignment. The legal and trust teams rejected that tradeoff because the liability and reputational risk of even one incident far exceeded the cost savings.

A financial advice chatbot faced the same tradeoff and chose a different balance. They tuned their risk classifier to a false positive rate of 5% and a false negative rate of 7% because the financial domain has more gradations of risk and because their user base of professional investors had higher risk tolerance than the mental health chatbot's vulnerable users. High-risk queries about fraudulent schemes and high-leverage trading were critical to detect, but medium-risk queries about portfolio diversification and asset allocation were less likely to result in catastrophic harm if routed to a less capable model. The team accepted higher false negatives on medium-risk queries in exchange for lower false positives, keeping routing costs under control while still protecting users on the highest-risk queries.

## Regulatory Implications Under the EU AI Act

The EU AI Act, which came into full enforcement in 2025, classifies certain AI systems as high-risk and imposes strict requirements on model safety, transparency, and accountability. High-risk AI systems include those used in healthcare, law enforcement, education, employment, and credit scoring. If your application falls into a high-risk category, you must use models that meet specific safety thresholds, maintain detailed logs of model inputs and outputs, and provide transparency reports to regulators on request. Routing a high-risk query to a model that does not meet these thresholds is a compliance violation even if the output is ultimately safe and accurate.

A healthcare diagnostics platform serving European users implemented risk-aware routing to comply with the EU AI Act. Queries that mentioned symptoms, medical conditions, or treatment options were classified as high-risk under the Act's healthcare provisions. These queries were routed to GPT-5.2 reasoning mode, which had been certified by the platform's legal team as meeting the Act's safety and transparency requirements. Low-risk queries about general wellness, appointment scheduling, and health education were routed to GPT-5, which did not meet the high-risk thresholds but was acceptable for low-risk use cases. The platform maintained detailed logs of all high-risk queries, including the model used, the input and output, the risk classification confidence score, and the timestamp. These logs were stored for seven years and made available to regulators on request, as required by the Act.

The compliance burden of risk-aware routing is not trivial. You must maintain a mapping between risk categories and models, ensure that all models used for high-risk queries meet regulatory thresholds, log all routing decisions, and produce transparency reports on demand. A financial services company found that compliance with the EU AI Act added $120,000 in annual engineering and legal costs, plus $40,000 in cloud storage costs for logs. But the alternative, not complying, would have resulted in fines starting at 15 million euros or 3% of global revenue for the first violation, and up to 30 million euros or 6% of global revenue for repeat violations. The compliance cost was a rounding error compared to the penalty risk.

## Auditing Risk-Aware Routing for Compliance

You must audit your risk-aware routing system regularly to ensure it is correctly identifying and routing high-risk queries. Auditing involves sampling a subset of queries, manually reviewing the risk classification and routing decision, and measuring the false positive and false negative rates. The mental health chatbot audited 500 randomly sampled queries per week, with a trust and safety specialist reviewing each query and comparing the automated risk classification to the human judgment. Queries where the automated system and the human reviewer disagreed were flagged for review and used to retrain the risk classifier.

The audit also measured whether high-risk queries routed to the safe model actually received safer outputs than they would have received from a less safe model. The team ran a counterfactual experiment where they re-ran a sample of high-risk queries through both Claude Opus 4.5 and GPT-5-mini and compared the outputs. Claude Opus 4.5 refused to answer or provided a safety-framed response 96% of the time, while GPT-5-mini refused only 78% of the time and provided inadequately framed responses 18% of the time. This confirmed that routing high-risk queries to Claude Opus 4.5 meaningfully improved safety outcomes, justifying the higher cost.

A legal advice chatbot audited their risk-aware routing by having a licensed attorney review 200 high-risk queries per month and rate the quality and safety of the model output. The attorney found that GPT-5.2 reasoning mode provided factually accurate and appropriately cautious legal information 94% of the time, while GPT-5 base was accurate only 86% of the time and occasionally provided overconfident advice that understated legal complexity. This gap justified the 3x cost premium of routing high-risk legal queries to the reasoning model.

## Combining Risk-Aware Routing with Other Routing Strategies

Risk-aware routing is not mutually exclusive with complexity-based routing, cost-based routing, or semantic routing. The most robust routing systems combine multiple signals to make the final routing decision. A healthcare chatbot used a hybrid approach where queries were first classified by risk level, then by complexity. High-risk queries always went to Claude Opus 4.5 regardless of complexity. Medium-risk queries were routed based on complexity: complex queries went to GPT-5, simple queries went to GPT-5. Low-risk queries were routed to GPT-5-mini. This two-stage routing system ensured that risk was the primary gating factor, but cost and latency were optimized within each risk tier.

A financial advice chatbot used a three-stage routing system: first risk classification, then user context, then complexity. High-risk queries about fraud or high-leverage trading went to GPT-5.2 reasoning mode for all users. Medium-risk queries were routed based on user subscription tier: premium users got GPT-5, free users got GPT-5. Low-risk queries were routed based on complexity: complex questions went to GPT-5, simple questions went to GPT-5-mini. This system balanced safety, cost, user expectations, and latency across a diverse user base with varying risk profiles and willingness to pay.

## Monitoring Risk Routing in Production

You must monitor risk-aware routing continuously to detect drift, misclassification, and emerging risk patterns. The mental health chatbot tracked four metrics in real time: the percentage of queries classified as high-risk, the false positive rate on a weekly audit sample, the false negative rate on the same sample, and the incident rate for high-risk queries routed to safe models versus unsafe models. When the percentage of queries classified as high-risk spiked from 8% to 14% over three days, the team investigated and found that a news event had triggered a wave of queries about a specific mental health topic. The spike was legitimate, not a classifier error, and routing cost increased temporarily but returned to baseline within a week.

The financial advice chatbot monitored the distribution of risk scores assigned to queries. They found that 74% of queries had risk scores below 0.2, 18% had scores between 0.2 and 0.6, and 8% had scores above 0.6. The high-risk threshold was set at 0.6, so 8% of queries were routed to the expensive safe model. Over six months, the distribution shifted: the percentage of queries with scores above 0.6 increased to 11%, driven by increased user sophistication and more complex financial questions. The team did not lower the threshold because the shift was real, but they did flag the trend for product and finance teams to plan for increased routing costs.

## When Not to Use Risk-Aware Routing

Risk-aware routing is not necessary for applications where all queries are low-risk and the consequences of model error are minimal. A recipe recommendation chatbot, a weather forecast assistant, and a general trivia bot do not need risk-aware routing because even incorrect outputs do not cause harm. Implementing risk-aware routing in these contexts adds engineering complexity, latency, and cost without improving safety or user outcomes.

Risk-aware routing is also not necessary when you can achieve the same safety outcomes with model-level safety features like system prompts, output filtering, and refusal tuning. A customer support chatbot for an e-commerce company does not need risk-aware routing because all queries are transactional and low-risk. The company uses GPT-5 for all queries and relies on a carefully tuned system prompt to ensure polite, accurate, and policy-compliant responses. Adding risk-aware routing would not improve outcomes because there are no high-risk queries in the domain.

Finally, risk-aware routing is not appropriate when risk is determined by downstream actions, not query content. A query asking "What is the weather tomorrow" is low-risk in a weather app but high-risk in a flight planning app where incorrect weather information could lead to unsafe flight decisions. In this case, the risk is context-dependent, not query-dependent, and you must route based on the application context, not the query text alone.

## Building Your First Risk-Aware Router

Start by defining your risk categories based on the domains your application serves and the potential harm of model errors. Collect labeled examples of high-risk and low-risk queries, and train a binary or multi-class classifier to predict risk level. Tune the classifier to accept higher false positives in exchange for lower false negatives, because false negatives expose users to harm. Map risk levels to models based on safety, factuality, and alignment characteristics: high-risk queries go to Claude Opus 4.5 or GPT-5.2 reasoning mode, medium-risk queries go to GPT-5 or Claude Sonnet, low-risk queries go to GPT-5 or GPT-5-mini. Log all routing decisions and audit a sample weekly to measure false positive and false negative rates. Monitor the percentage of queries classified as high-risk and flag anomalies for investigation. If your application is subject to the EU AI Act or similar regulation, ensure that all models used for high-risk queries meet regulatory thresholds and maintain detailed logs for compliance.

Risk-aware routing is not optional for high-stakes applications. It is the engineering and ethical responsibility of any team deploying AI in domains where errors cause harm, and it is a legal requirement under the EU AI Act and similar regulations. The next step is combining all of these routing strategies into a unified routing framework that balances cost, latency, quality, and risk across your entire model portfolio.
