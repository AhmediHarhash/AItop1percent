# 10.11 â€” Policy Compatibility Testing: Ensuring Product Behavior Survives Provider Swaps

In March 2025, a legal technology company switched their contract analysis feature from GPT-5.1 to Claude Opus 4.5 after their evaluation showed a 12% improvement in clause extraction accuracy. The swap went live on a Friday afternoon. By Monday morning, their support queue held 89 escalations from law firms reporting that the system was now refusing to process employment contracts containing standard non-compete clauses, flagging them as potentially harmful content. The new model had different safety boundaries. It interpreted restrictive employment terms as content that could cause harm to individuals and declined to process them. The company's own content policy allowed analysis of legal documents containing such clauses because that was the core use case. But they had never tested whether their product-level policies would survive a provider swap. They rolled back the deployment, but the incident cost them three enterprise renewals and seven months of trust rebuilding with their legal vertical customers.

The lesson was taught expensively: your product has policies that must hold regardless of which model is serving requests. Policy compatibility testing is the discipline of verifying that your product-level behavior requirements remain enforced when you change providers, change models, or change model versions. It is not the same as model evaluation. Model evaluation tests capability and quality. Policy compatibility testing tests whether the model respects the boundaries you have defined for your product.

## What Product Policies Are

Your product has policies even if you have never written them down explicitly. A policy is any rule about what your system will or will not do, what content it will or will not process, what outputs it will or will not generate. Some policies are safety-related: we do not generate medical diagnoses, we do not produce content that violates our terms of service, we do not process personally identifiable information outside of approved workflows. Some policies are functional: we always return outputs in JSON format, we never refuse to process documents in our supported file types, we provide explanations for all rejections. Some policies are business-driven: we serve requests in under two seconds, we do not upsell users to competitor products, we maintain a professional tone in all customer-facing outputs.

These policies exist independently of any particular model. You chose your initial model because it could satisfy your policies. But models change. Providers update safety filters. New models have different refusal behaviors. A model swap can silently violate policies you assumed were inherent to all models but were actually specific to the one you started with. A healthcare application might have the policy that the system never refuses to process clinical notes containing mentions of controlled substances because that is a normal part of medical documentation. GPT-5 might process those notes without issue. Gemini 3 might flag them as drug-related content and refuse. If you do not test policy compatibility before the swap, you discover the incompatibility in production.

Policy compatibility testing treats your product policies as test cases. Each policy becomes an assertion: given this input that should be allowed under our policy, the model processes it successfully. Given this input that should be rejected under our policy, the model refuses or flags it appropriately. Given this scenario, the model output adheres to our format and tone requirements. You build a test suite that encodes these assertions and you run it against every model before you deploy that model to production.

## The Policy Compatibility Test Suite

The test suite has three categories of test cases. First, boundary tests for allowed content. These are inputs that fall inside your product's acceptable use boundaries and must be processed successfully. For the legal tech company, this would include employment contracts with non-compete clauses, non-disclosure agreements with confidentiality terms, vendor agreements with indemnification language. All of these are standard legal content that the product must handle. A model that refuses them fails the policy compatibility test. Each boundary test asserts: this content is allowed, the model must not refuse it, the model must produce a valid output.

Second, boundary tests for prohibited content. These are inputs that fall outside your product's acceptable use boundaries and must be rejected or handled according to your safety protocols. A resume screening tool might prohibit processing resumes that contain only demographic information without any job-related content, because that creates discrimination risk. A customer service assistant might prohibit generating responses that promise refunds or discounts beyond defined limits. These test cases assert: this content is not allowed, the model must refuse it or trigger our safety handling flow, the model must not produce a compliant output.

Third, format and behavior tests. These verify that the model adheres to your output structure, tone, and interaction policies. If your product requires all outputs in JSON with specific field names, you test that the model reliably produces that format. If your product requires a professional tone and prohibits casual language or slang, you test tone consistency. If your product requires the model to ask clarifying questions before proceeding in ambiguous scenarios, you test that the model asks those questions rather than guessing.

You build this suite by translating your written policies into concrete test cases. Take each policy statement, create positive examples that should pass, create negative examples that should fail, and create edge cases that clarify the boundary. If your policy is "we process medical documentation but do not generate diagnoses," your test cases include: processing a clinical note with symptoms (should pass), generating a summary of a clinical note (should pass), being asked to diagnose a patient based on symptoms (should fail with a specific refusal message), being asked to suggest possible diagnoses as a brainstorming aid (edge case, you decide the policy).

## Common Policy Breakages During Provider Swaps

The most frequent failure mode is mismatched refusal behavior. Different models have different safety filters trained by different teams with different threat models. GPT-5 might allow discussion of historical conflict because it distinguishes educational context from glorification. Claude Opus 4.5 might refuse the same discussion because its safety training is more conservative around violence. Gemini 3 might allow it but add a safety disclaimer that breaks your output format. If your product is an educational history platform, you need a model that processes historical conflict content without refusal or disclaimer injection. If you do not test this before swapping providers, your history lessons start getting blocked in production.

The second common failure is different safety boundary interpretations. Your product might allow users to analyze their own financial data including account numbers and transaction histories. That is not a privacy violation because the user owns the data and has consented. But a model trained with strict privacy filters might refuse to process content containing account numbers regardless of consent context. You test this by submitting representative financial data and verifying the model processes it. If the model refuses, you either choose a different model, configure the model differently if the provider allows safety tuning, or add preprocessing that anonymizes account numbers if that satisfies both your policy and the model's filters.

The third failure mode is format drift. Your product requires JSON outputs with specific field names because your application code parses those fields. Your original model reliably produced that format. Your new model produces JSON but uses different field names, or sometimes produces markdown with a JSON code block instead of raw JSON, or occasionally adds explanatory text before the JSON. These outputs break your parser. Policy compatibility testing catches this by asserting on output structure, not just output presence. You test that the exact format you require is what the model produces, every time.

The fourth issue is tone and style incompatibility. Your product has a professional, formal tone because you serve enterprise legal customers. Your original model matched that tone naturally. Your new model has been fine-tuned to be more conversational and friendly, which is excellent for consumer applications but wrong for legal briefs. Your policy compatibility tests include tone checks: submit a request, verify the output uses formal language, verify it avoids contractions and casual phrasing, verify it uses appropriate legal register. If the new model fails these checks, it is not compatible with your product policies regardless of its capability scores.

## Building Provider-Independent Policy Tests

The test suite must be decoupled from any specific provider or model. You are testing your policies, not the model's advertised features. Write tests that express what your product requires without referencing provider-specific terminology or capabilities. Do not write a test that says "verify GPT-5.2 content filter allows medical terms." Write a test that says "verify the model processes clinical documentation containing standard medical terminology without refusal." The test input is a clinical note. The assertion is that you receive a valid output, not a refusal. This test runs against GPT-5.2 today, against Claude Opus 4.5 tomorrow, against whatever model you evaluate next month.

Structure your tests with clear separation between the policy being tested and the model being tested against. Each test case has a name describing the policy, a category tag, an input, an expected outcome, and a severity level. The severity level tells you whether a failure is a blocking issue or a minor concern. A test verifying that the model does not refuse allowed content is severity critical; if it fails, the model cannot be deployed. A test verifying that the model uses preferred terminology is severity low; if it fails, you might deploy anyway with a note to monitor for user confusion.

You store these tests in a version-controlled repository separate from your model evaluation code. Policy tests change when your product policies change, not when models change. If you decide to expand your acceptable use policy to allow a new content category, you add tests for that category. If a regulation changes and you must now prohibit a previously allowed content type, you update the boundary tests. But the test suite structure remains stable across model swaps.

Run the policy compatibility suite as a gate in your model approval process. Before any model is approved for production deployment, it must pass the full policy test suite at your defined threshold. You might require 100% pass rate on critical severity tests and 95% pass rate on medium severity tests. Any model that does not meet this bar is not deployed, regardless of how well it performs on capability evaluations. Policy compatibility is not negotiable.

## The Policy Regression

A policy regression occurs when a model change causes your product to violate its own policies. The most dangerous form is the permissive regression: the new model allows content that your product policy prohibits. A content moderation system switches to a new model that is less sensitive to certain harm categories. Suddenly, content that should be flagged passes through. Users are exposed to content your product promised to block. This is a critical failure. Your policy compatibility tests must include adversarial examples that should always be caught. If the new model lets them through, you have a permissive regression and the model is not deployed.

The second form is the restrictive regression: the new model prohibits content that your product policy allows. This is what happened to the legal tech company. The new model was more restrictive than the product required. Restrictive regressions are operationally disruptive but less dangerous than permissive regressions. Users cannot do things they could do before. Features break. But you are not violating safety commitments. Still, a restrictive regression makes the model incompatible with your product. Your tests catch it by verifying that all allowed content categories remain allowed.

The third form is the behavioral regression: the model behaves differently in ways that violate implicit policies. It used to ask clarifying questions when inputs were ambiguous; now it guesses. It used to provide sources for factual claims; now it does not. It used to refuse politely with an explanation; now it refuses with a generic error message that confuses users. These regressions are subtle. They do not involve outright refusals or format breaks. But they degrade user experience and violate the behavioral contract your product established. You catch them with behavior tests: given an ambiguous input, verify the model asks a clarifying question; given a factual claim in the output, verify it includes a source reference.

When you detect a regression, you have four options. First, do not deploy the model. Choose a different model or wait for a future version. Second, reconfigure the model if the provider offers safety or behavior tuning that can align it with your policies. Third, add a policy enforcement layer in your application code that corrects the regression. If the model is too permissive, add a secondary filter. If it is too restrictive, add preprocessing that rephrases inputs to avoid triggering overly sensitive filters. Fourth, change your policy if you determine the new model's behavior is actually better aligned with your values than your previous policy was. This is rare but legitimate. Perhaps the new model's stricter safety boundary reflects an updated understanding of harm that your policy should incorporate.

## Continuous Policy Monitoring in Production

Policy compatibility testing does not stop at deployment. Models can change behavior over time due to provider-side updates, concept drift, or adversarial adaptation. You run a subset of your policy test suite continuously in production as a canary system. Every hour, you submit synthetic test cases from your policy suite to your production model and verify the responses meet your policy requirements. These are not real user requests; they are policy probes. If canary tests start failing, you receive an alert that a policy regression may be occurring in production.

The canary suite is smaller than your full pre-deployment suite because you are running it frequently and you do not want to incur excessive cost or latency. You select the highest-severity test cases, the ones that represent your most critical policies. A financial services application might canary-test that the model still refuses to provide investment advice, still processes account data without refusal, still returns outputs in the required JSON structure. These tests run every hour. If any fail three times in a row, the system pages the on-call engineer.

You also analyze production traffic for policy violations. Your logging infrastructure tags every request and response with policy-relevant metadata: was the request allowed content, was the response in valid format, did the model refuse when it should not have, did the model comply when it should have refused. You aggregate these tags and monitor for anomalies. If your refusal rate for allowed content suddenly increases from 0.1% to 2%, you have a potential restrictive regression happening in real time. If your compliance rate for prohibited content drops from 99.8% to 97%, you have a potential permissive regression.

When production monitoring detects a policy issue, you trigger an incident response. You pull recent examples of the failures, analyze whether they represent a model behavior change or a shift in user input patterns, determine severity, and decide on mitigation. If the model behavior has changed, you may need to roll back to a previous model version, switch to a backup provider, or deploy a policy enforcement layer as a hotfix while you investigate root cause.

## Policy Compatibility as a Gate in the Model Approval Process

Every model goes through an approval process before it reaches production. Policy compatibility is one of the required gates. The process is: capability evaluation runs first and produces performance metrics; if the model meets capability thresholds, it advances to policy compatibility testing; if it passes policy tests at the required confidence level, it advances to cost and latency validation; if it meets cost and latency targets, it advances to shadow deployment; if shadow deployment shows no regressions, it is approved for production traffic. Policy compatibility is non-negotiable. A model that fails policy tests does not advance, even if it has better capability scores than the current production model.

Your approval checklist includes a section for policy compatibility with specific signoff requirements. The product team signs off that the model adheres to product behavior policies. The legal or compliance team signs off that the model adheres to regulatory and contractual content policies. The trust and safety team signs off that the model adheres to safety and acceptable use policies. All three signoffs are required. If any team identifies a policy incompatibility, the model is rejected or sent back for mitigation.

You document the policy test results in your model registry. Each model version has a policy compatibility report showing which tests passed, which failed, what the failure modes were, and what mitigations were applied if any. This documentation is critical for audits and incident investigations. If a policy violation occurs in production, you can trace back to the policy test results for that model version and determine whether the issue was known, whether it was within acceptable thresholds, or whether it represents a new failure mode not covered by your test suite.

The registry also tracks policy test suite version. As your policies evolve, your test suite evolves. A model that passed policy tests in January 2026 with test suite version 2.3 might not pass the same policy gate in June 2026 with test suite version 2.8 if your policies have become stricter. When you update your policy test suite, you re-run it against all active models in your rotation. Models that no longer pass are flagged for replacement or reconfiguration.

Policy compatibility testing is the contract enforcement layer between your product promises and your model behavior. Your product makes promises to users: we will process your content, we will not expose you to certain harms, we will deliver outputs in a specific format. Models are the mechanism that fulfills those promises. When you swap models, you must verify that the new mechanism still fulfills the contract. Policy testing is how you verify. It is not optional. It is not a nice-to-have. It is the difference between a controlled model swap and a production incident that costs customer trust.

The legal tech company rebuilt their deployment process after the non-compete clause incident. They extracted their implicit policies into explicit policy statements. They created 287 policy test cases covering allowed legal content, prohibited content, output format requirements, and tone requirements. They integrated policy testing into their model approval process as a required gate. They implemented continuous policy monitoring with hourly canary tests. When they re-attempted the GPT-5.1 to Claude Opus 4.5 migration six months later, the policy test suite flagged the non-compete refusal issue in pre-production testing. They worked with Anthropic to configure a custom safety profile that allowed legal document analysis while maintaining other safety boundaries. The migration succeeded without customer impact. Policy compatibility testing had become a core competency, not an afterthought.

Understanding how to test that your product policies survive model changes prepares you to handle the inevitable reality that models sometimes misbehave in production despite all your testing, which is why you need a dedicated incident response process for model failures.
