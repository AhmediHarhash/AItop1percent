# 6.9 â€” Shared Context Across Models: Prompt Translation and Format Normalization

In mid-2025, a customer support platform serving enterprise clients switched from Claude Opus 4 to GPT-5.1 mid-conversation for cost optimization on what appeared to be simple follow-up questions. The routing logic worked perfectly. The cost savings were real. But customer satisfaction scores dropped 18% over three weeks. The problem wasn't model quality. The problem was that GPT-5.1 couldn't see the emotional context Claude had established. The system passed the literal conversation history but lost the formatting that Claude used to track sentiment states. GPT-5.1 interpreted neutral text as the full context and responded with standard efficiency-focused replies to customers who were already frustrated. By the time the team traced the issue to prompt format translation, they'd lost two major accounts. The failure wasn't in the routing decision. The failure was in assuming that conversation history is just text that any model can consume. Different models expect different structures, different role labels, different formatting conventions. When you route between models, you're not just changing the processor. You're translating between different languages of context representation.

## The Prompt Format Problem

Every major model family expects prompts in a specific structure. OpenAI models use a messages array with system, user, and assistant roles. Anthropic models use a similar structure but handle system messages differently, treating them as separate parameters rather than part of the messages array. Google's Gemini models expect a parts-based structure within each message. Meta's Llama models, when accessed through various providers, may use chat templates that vary by hosting platform. These aren't cosmetic differences. They're structural expectations baked into how models were trained to process conversation context.

The naive approach is to treat all models as if they accept the same format and let the API wrapper handle translation. This works for simple single-turn requests. It fails catastrophically for multi-turn conversations with rich context. When you send a Claude-formatted conversation to GPT-5.1 without translation, the API might accept it but the model won't process it optimally. Role labels might be misinterpreted. System-level instructions might be ignored or treated as user messages. Context that was carefully structured for one model's attention patterns becomes noise for another model's architecture.

The professional approach is explicit prompt translation at every model boundary. You maintain conversation state in a canonical internal format, then translate to each model's expected structure at request time. This means understanding not just the surface format but the semantic expectations. Claude expects system messages to set broad behavioral context. GPT-5 expects system messages to define the assistant's role and capabilities. Gemini expects instructions to be embedded in the first user message unless you're using their system instruction parameter. Same label, different expectations, different optimal translation strategies.

## Role Label Translation and Semantic Preservation

The simplest translation challenge is role labels. Your internal conversation state might use "system," "human," "assistant," and "tool" as role identifiers. Claude's API expects "user" and "assistant" in the messages array with system as a separate parameter. OpenAI expects "system," "user," and "assistant" as message roles. Gemini expects "user" and "model" as roles. DeepSeek expects a chat template format that varies by model version. Direct label mapping seems sufficient until you encounter edge cases.

Tool calls and tool responses introduce the first major complexity. OpenAI represents tool calls as assistant messages with a tool_calls property, and tool responses as separate messages with role "tool." Claude represents tool use as assistant messages with content blocks of type "tool_use," and tool results as user messages containing "tool_result" content blocks. Gemini represents function calls within the model's response parts and function responses within the user's next message parts. Same semantic operation, three completely different structural representations.

When you route from OpenAI to Claude mid-conversation after a tool call, you can't just pass the OpenAI format. You must reconstruct Claude's expected structure: extract the tool call details from the assistant message, create a Claude-format tool_use content block, extract the tool response, wrap it in a tool_result content block within a user message. Miss any of these transformations and Claude either rejects the request or misinterprets the tool interaction as plain text conversation. The translation isn't just reformatting. It's reconstructing the semantic structure that each model was trained to recognize.

System message handling requires even more care. If your conversation starts with Claude and includes a system message, then routes to GPT-5.1, you can pass the system message directly as the first message in the OpenAI format. But if you then route to Gemini, you have three options: embed the system instruction in the first user message, use Gemini's systemInstruction parameter if available, or strip it entirely and rely on the conversation history to establish context. Each choice has different implications for how Gemini interprets its role. Embedding in the first user message makes it part of the task description. Using systemInstruction makes it behavioral guidance. Stripping it means Gemini infers its role purely from the conversation pattern.

## Conversation History Truncation Across Context Windows

Models have different context window sizes, and those sizes change frequently. As of early 2026, GPT-5.2 offers 1 million tokens, Claude Opus 4.5 offers 500,000 tokens, Gemini 3 Pro offers 2 million tokens, and various smaller models offer 32,000 to 200,000 tokens. When you route from a large-context model to a smaller-context model mid-conversation, you must truncate history. The question is what to keep and what to discard.

The naive approach is to keep the most recent N tokens and drop everything older. This works if the conversation is purely linear with no callbacks to earlier context. It fails when the current question references something from message 3 in a 40-message conversation. The user asks, "What did I say about the pricing model earlier?" and your system routes to a cost-efficient small model that can only see the last 15 messages. The model has no access to the earlier pricing discussion, responds with "I don't see any earlier discussion of pricing," and the user's trust in the system drops.

The professional approach is semantic truncation based on relevance. Before routing, you analyze the current user message to identify references to earlier conversation turns. You use embeddings or keyword matching to find relevant prior messages. You keep those messages plus recent context, and drop the middle conversations that aren't directly relevant. This requires maintaining conversation state with message indexing and retrieval capability, not just a linear history buffer.

Some teams implement a summarization layer: when truncating, they summarize the dropped portion and inject that summary as a system or user message. This preserves semantic continuity but introduces a new failure mode. The summarization might miss crucial details, or worse, hallucinate details that weren't in the original conversation. You then route to a new model with a summary that contains false information presented as fact. The new model responds based on the false summary, and the conversation drifts from reality. Summarization-based truncation is only viable if you validate summaries against the original content or accept the risk of compounding errors.

## Structured Output Format Translation

Many routing decisions are driven by different models' capabilities with structured outputs. GPT-5 offers native JSON mode and structured output schemas. Claude 4.5 offers tool use that can be repurposed for structured extraction. Gemini 3 offers function calling that doubles as structured output. DeepSeek and Qwen models support various JSON formatting approaches. When you route from one model to another for a structured output task, you must translate not just the input but the output format specification.

If your task starts with GPT-5 using a structured output schema, then routes to Claude for some reason, you can't pass the GPT-5 schema directly. You must convert it to a Claude tool definition. The schema language is similar but not identical. OpenAI uses JSON Schema with specific extensions. Claude uses a JSON Schema subset with different property names. Required fields are specified differently. Enum constraints are represented differently. If your translation is imperfect, Claude might accept the tool definition but return outputs that don't match your expected structure.

The bigger problem is output parsing consistency. GPT-5's JSON mode guarantees valid JSON in the response content. Claude's tool use wraps JSON in a content block with type "tool_use" and the actual parameters in an "input" property. Gemini's function calling returns JSON in a parts array within the response. If your downstream code expects to parse response.content as JSON, it works with GPT-5, fails with Claude unless you extract the tool input, and fails with Gemini unless you extract from parts. You need model-specific output unwrapping that knows where each model places its structured data.

Some teams solve this by maintaining dual specifications: one for input format translation, one for output format unwrapping. For each model, you define how to construct the structured output request and how to extract the structured result. This doubles your maintenance burden but eliminates runtime parsing failures. The alternative is runtime format detection, where you inspect the response structure and apply the appropriate unwrapping logic. This works but adds latency and introduces new failure modes if a model returns an unexpected format.

## Information Loss in Translation

Every translation between formats risks information loss. Claude's content blocks can contain multiple types in a single assistant message: text, tool use, and thinking blocks. OpenAI's assistant messages contain either text or tool calls, not both simultaneously. If you translate from Claude to OpenAI and the Claude message contains both text and a tool use, you must decide: split into two OpenAI messages, keep only the tool call, or keep only the text. Each choice loses information.

Gemini's parts-based structure allows rich multimedia content within messages. If a Gemini response contains both text and an inline image reference, and you route the next turn to Claude, you must decide how to represent the image reference. Claude doesn't support inline image references in assistant messages the same way Gemini does. You can describe the image in text, omit it entirely, or store it externally and reference it. Each approach changes what the next model sees and how it interprets the conversation.

The most insidious information loss is in metadata and annotations. Some model APIs support message metadata, confidence scores, or internal reasoning chains. GPT-5's reasoning mode returns both the final answer and the reasoning trace. If you route from GPT-5 reasoning mode to Claude and only pass the final answer, Claude loses the reasoning context that explains why GPT-5 reached that conclusion. If the user's next question is "Why did you conclude that?" Claude has no basis to answer. You either need to inject the reasoning trace as a text message or accept that Claude will guess at the reasoning.

Teams that successfully manage multi-model routing maintain a rich internal conversation representation that captures everything: the original message formats, translated versions for each model family, metadata about which model produced each response, and annotations about translation decisions. When routing to a new model, they translate from this rich internal format rather than trying to reverse-translate from one model's API format to another's. This adds complexity but eliminates cumulative information loss over multiple routing hops.

## Maintaining Conversation Coherence Across Model Switches

The hardest problem isn't technical translation. It's maintaining conversational coherence when different models have different communication styles, different levels of verbosity, different tendencies toward hedge language or confidence. Users notice when the assistant's personality changes mid-conversation. A conversation that starts with Claude's careful, nuanced responses and switches to GPT-5-mini's more direct, terse style feels disjointed even if the factual content is correct.

Some teams implement style normalization in post-processing. After a model returns a response, they run it through a rewriting layer that adjusts tone, verbosity, and hedging to match the established conversation style. This adds latency and cost but improves perceived coherence. The risk is that style normalization might remove model-specific strengths. Claude's nuance and careful qualification might be valuable. GPT-5's directness might be what the user needs for a quick clarification. Normalizing everything to a middle ground might make all responses feel bland.

Other teams accept style variation but make it explicit. They show users which model generated each response, framing the switches as bringing in different experts. "Let me bring in our technical specialist for this question" when routing to a code-focused model. "Let me consult our detail-oriented analyst" when routing to a model good at structured data. This turns the multi-model architecture into a feature rather than hiding it. Users understand why the style changed and may even appreciate the specialization.

The professional middle ground is to limit mid-conversation routing to situations where the model switch is semantically justified, not just cost-optimized. Route to a different model when the task type changes, when specialized capabilities are needed, or when the conversation reaches a natural break point. Avoid routing in the middle of a multi-turn reasoning task or an emotionally charged support conversation. The cost savings from routing every message to the cheapest viable model are real, but they're not worth destroying conversation coherence and user trust.

## Context Format Normalization Patterns

Successful multi-model systems implement a canonical conversation format that serves as the source of truth. Every model-specific format is a projection from this canonical format. The canonical format must be rich enough to represent everything any supported model might need: role labels, content types, tool calls, tool responses, system instructions, metadata, and provenance information.

One effective pattern is to store conversations as event logs: each user message, each model response, each tool invocation, each routing decision is an event with a timestamp, a source identifier, and a payload. When routing to a model, you query the event log, filter to relevant events, translate to the target model's format, and send. When storing a model response, you append it to the event log with full metadata. This makes debugging tractable: you can replay any routing decision, inspect exactly what each model saw, and identify where translation failures occurred.

Another pattern is layered translation: maintain three representations for each conversation. The user-facing representation is what the frontend shows. The canonical representation is your internal format. The model-specific representation is generated on demand for each API call. Changes flow unidirectionally: user input is parsed into canonical format, canonical format is translated to model-specific format for requests, model responses are parsed into canonical format, canonical format is rendered to user-facing format. This separation prevents translation artifacts from polluting your source of truth.

The key architectural principle is that translation happens at the edges, not in the core logic. Your routing decisions, your evaluation logic, your monitoring, all operate on the canonical format. Model-specific details are encapsulated in adapter layers that know how to translate to and from each model family. When a new model family emerges or an existing family changes its format, you update the adapter, not the core system. This keeps translation complexity from metastasizing through your codebase.

Beyond the technical formats, the next challenge is understanding how the choice of tokenizer affects every aspect of multi-model routing, from cost estimation to context window management.
