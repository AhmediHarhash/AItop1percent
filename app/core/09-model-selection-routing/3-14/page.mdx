# 3.14 — Router Observability: Decision Logs, Feature Tracking, and Version Control

In October 2025, a financial analytics company received an alert that their model routing costs had increased by 22% over the previous month, with no corresponding increase in query volume. The team investigated, but their logging infrastructure recorded only which model handled each query, not why the router chose that model. They could see that GPT-5.2 usage had doubled, but they could not determine which types of queries were being routed there or whether the routing decisions were correct. After two weeks of manual query sampling and ad hoc log analysis, they traced the cost increase to a routing classifier update deployed four weeks earlier. The update had changed the complexity threshold, causing borderline queries to escalate from the middle tier to the expensive tier. The change was well-intentioned—an attempt to improve quality—but it had not been tested against cost impact, and the team had no observability into the routing feature values or decision rationale. By the time they identified the root cause, they had spent $34,000 more than budgeted, and the fix required another two weeks to validate and deploy. The fundamental problem was not the routing change itself but the lack of observability that would have detected the impact immediately and pinpointed the cause in minutes.

You cannot improve what you cannot observe. Router observability means logging every routing decision with enough context to understand why each query went to each model, tracking the distribution of routing features over time to detect drift, and version-controlling routing configuration so you can correlate performance changes with deployment events. Observability is not optional. It is the difference between detecting a routing failure in hours versus months, diagnosing a cost spike in minutes versus weeks, and iterating on router improvements weekly versus quarterly. Every production routing system must instrument decision logs, feature tracking, version control, dashboards, and alerts. These are not nice-to-have monitoring add-ons—they are foundational requirements for operating a router in production. Without observability, you are flying blind. You see aggregate outcomes but not the causal chain that produced them. You know cost increased but not why. You know quality degraded but not for which queries. You make changes based on intuition and hope rather than data and evidence.

## The Decision Log: Recording Why Each Query Routed Where It Did

The decision log is the single most important observability artifact. It records every routing decision with enough context to reconstruct the decision-making process. At minimum, the log must include query ID, timestamp, routing features extracted from the query, the routing decision (which model tier or model ID), the confidence or score associated with the decision, and identifiers for the routing configuration version and the model versions used. Advanced logs also include the actual query text (if privacy and compliance allow), the model's response, quality metrics for the response, latency, cost, and any errors or retries. This is structured logging, not free-form text. Each field is a typed attribute that can be queried, aggregated, and analyzed programmatically.

Why does the decision log need this much detail? Because when a routing failure occurs, you need to answer several questions: Which queries were misrouted? What features led to the misrouting? Did the features accurately represent the query, or was there a feature extraction failure? Did the routing logic apply correctly given the features, or was there a classifier or rule bug? Was the misrouting isolated to a specific query type, time window, or user segment? You cannot answer these questions if you only log the query ID and the model choice. You need the features that informed the decision, the decision logic version, and the outcome metrics that indicate whether the decision was correct. The decision log is your forensic evidence for diagnosing failures and your training data for improving routing accuracy.

A content moderation system in early 2026 logged decision details that included query ID, model choice, and timestamp, but not the routing features. When they detected a quality regression in moderation decisions, they could see that certain queries had been routed to the cheap tier instead of the expensive tier, but they could not determine why. Was the complexity score too low? Was the risk classification incorrect? Was the cost budget exhausted? They had no visibility into the feature values at decision time. They attempted to reconstruct the features by reprocessing historical queries, but query content was not logged for privacy reasons, and feature extraction depended on real-time context (such as rate limit status and cost budget) that could not be reproduced retroactively. The investigation stalled. They eventually fixed the issue by deploying a more conservative routing rule, but they never understood the root cause, and the problem recurred three months later under different conditions. The lack of feature logging turned a diagnosable failure into an unsolvable mystery.

The format of the decision log should be structured JSON or a database schema, not unstructured log lines. Each decision is a record with typed fields: query_id as a string, timestamp as ISO8601, routing_features as a nested object containing complexity_score, risk_level, query_length, detected_language, cost_budget_remaining, latency_budget_remaining, and any other features your router uses. The routing_decision field includes model_id, model_tier, confidence_score, and decision_reason (a short code or enum indicating whether the decision was based on a rule, classifier, lookup, or fallback). Outcome_metrics include response_latency_ms, response_cost_usd, quality_score if available, and error_code if the request failed. Configuration_version is a Git commit hash or version tag identifying the routing logic that made the decision. This structure enables fast querying: "Show me all decisions in the past week where complexity_score was greater than 0.8 but the model routed was Llama 4" or "Show me all decisions where the cost exceeded $1 and the quality_score was below 0.7."

Where do you store decision logs? For high-throughput systems, write them to a log aggregation service like AWS CloudWatch, Google Cloud Logging, or Datadog. For lower throughput or more complex analytics, write them to a data warehouse like BigQuery, Snowflake, or Redshift. For real-time alerting, stream them to a monitoring system like Prometheus or Grafana. The key requirement is that logs are queryable in near real-time and retained for at least 30 days, ideally 90 days or more. You will need to perform historical analysis to detect gradual drift, compare performance across routing versions, and retrain routing classifiers. Treat decision logs as first-class production data, not ephemeral debug output. They are your observability foundation.

## Feature Tracking: Monitoring the Distribution of Routing Signals

Feature tracking monitors the distribution of routing features over time. If your router uses a complexity score ranging from 0 to 1, feature tracking records the p10, p50, p90, and p99 values of that score every hour, every day, and every week. If your router uses query length, feature tracking records the distribution of query lengths. If your router uses detected language, feature tracking records the percentage of queries in each language. The purpose is to detect drift: changes in the query distribution that might invalidate routing logic. If your complexity score distribution shifts from a median of 0.42 in January to 0.58 in March, your routing thresholds—which were calibrated for the January distribution—may no longer be optimal. Queries that would have been routed to the cheap tier in January are now routed to the expensive tier in March, not because they are more complex, but because the entire distribution has shifted.

Feature drift is common and inevitable. User behavior changes. Product features evolve. New user segments appear. Seasonal patterns emerge. A tax preparation chatbot might see simple queries for most of the year, but in March and April the query distribution shifts toward complex multi-form tax scenarios. If the routing logic was calibrated on out-of-season data, it will misroute heavily during tax season. A retail recommendation system might see simple product lookups during normal months, but during Black Friday the queries become complex multi-constraint searches ("Find me a gaming laptop under $1200 with at least 16GB RAM and a GPU capable of running AAA games, available for pickup in Seattle"). The routing classifier, trained on normal-month data, sees these Black Friday queries as out-of-distribution and either misclassifies them or defaults to an expensive tier. Feature tracking detects this shift in real time, and you can respond by retraining the classifier or temporarily adjusting routing thresholds.

How do you implement feature tracking? For each routing feature, compute summary statistics over a rolling time window—past hour, past day, past week—and log those statistics to your monitoring system. Use your existing observability infrastructure: emit metrics to Prometheus, CloudWatch Metrics, or Datadog. Track not just the mean and median, but also the standard deviation and the 10th and 90th percentiles, because shifts in the tails often indicate emerging query types. For categorical features like detected language or query intent, track the distribution of categories—what percentage of queries fall into each category. Set up alerts for significant distribution shifts. If the median complexity score moves more than 0.1 in a single day, trigger an alert. If the percentage of queries in a new language goes from 0% to 5% in a week, trigger an alert. These alerts are early warnings that routing performance may degrade unless you recalibrate.

A healthcare chatbot in mid-2025 tracked complexity score distributions and detected a sudden shift in the 90th percentile from 0.72 to 0.88 over three days. Investigation revealed that a new product feature had launched allowing users to ask multi-step questions about medication interactions, which were structurally more complex than the previous single-symptom queries. The routing logic had not been updated for this feature, and the complexity classifier was struggling with the new query type, often underestimating complexity. The team used the feature tracking alert to trigger a response: they temporarily adjusted the routing threshold to be more conservative, routed the new query type to a higher tier by default, and retrained the complexity classifier on examples of the new multi-step queries. Within a week, routing accuracy recovered. Without feature tracking, they would have detected the issue only after users complained about quality degradation or after a scheduled monthly performance review.

Feature tracking also reveals feature extraction failures. If your query length feature suddenly shows a spike of zero-length queries, something is wrong with the ingestion pipeline. If your embedding-based similarity feature shows all queries clustering at the same value, the embedding model may have failed or degraded. If your risk classification feature shows a sudden increase in "unknown" or "null" values, the risk classifier may be timing out or misconfigured. These are operational failures in the feature extraction pipeline, and they directly impact routing decisions. Feature tracking turns these invisible failures into visible alerts, allowing you to fix them before they cause widespread misrouting.

## Version Control for Routing Configuration: Treating Rules and Models as Code

Routing logic is code. It may be expressed as rule definitions, classifier hyperparameters, model selection mappings, threshold values, or configuration files, but it is code nonetheless. It changes over time as you optimize performance, respond to new models, adjust for cost or quality targets, or fix bugs. Every change has the potential to improve or degrade system behavior, and you need to track what changed, when it changed, who changed it, and why. This is version control. Your routing configuration should live in a Git repository or equivalent version control system, where every change is a commit with a descriptive message, every deployment is tagged with a version identifier, and every version can be rolled back if it introduces a regression.

A financial advice platform in late 2025 stored routing rules in a database table that was edited directly by team members through an admin UI. Over six months, 14 different people made 47 changes to the rules. When routing performance degraded in December, the team could not determine which change introduced the regression. The database had no change history, no authorship records, and no rollback capability. They eventually restored performance by reverting to a weeks-old database backup, losing all intermediate rule improvements. The root cause was treating routing configuration as mutable production state rather than versioned code. After the incident, they migrated routing rules to a YAML configuration file in Git, required all changes to go through pull requests with code review, and deployed configuration changes through CI/CD with automated testing.

Version-controlling routing configuration means defining all routing logic in files that are checked into version control. For rule-based routers, this is a rules definition file—perhaps YAML or JSON—that lists each rule's conditions and actions. For classifier-based routers, this includes the classifier training code, hyperparameters, feature extraction logic, and decision thresholds. For lookup-based routers, this is the mapping tables or decision trees. These files live in a Git repository. When you want to change routing logic, you create a branch, edit the configuration, write a commit message explaining the change and its rationale, and open a pull request. Another team member reviews the change, validates that it aligns with cost and quality goals, and approves the merge. The merge triggers a CI/CD pipeline that deploys the updated configuration to staging, runs automated tests, and—if tests pass—deploys to production through a gradual rollout.

Why is this better than direct database edits or manual configuration updates? Because it provides an audit trail, review gates, automated testing, and rollback capability. The audit trail shows every change, who made it, when, and why. If routing performance changes, you can correlate it with configuration commits by timestamp. The review gates ensure that changes are validated by at least one other person, reducing the risk of accidental misconfigurations or unconsidered side effects. Automated testing runs regression tests on every configuration change—does this change route historical queries as expected? Does it keep cost within budget? Does it maintain quality thresholds? Rollback capability means that if a configuration change degrades performance, you can revert to the previous version with a single Git operation, not a manual undo process.

The decision log should record the configuration version (Git commit hash or version tag) for every routing decision. This creates a linkage between decisions and the logic that produced them. When you analyze decision logs and see that a certain query type was misrouted, you can identify which configuration version was active at that time, inspect the code for that version, determine what rule or classifier produced the decision, and propose a fix. Without version linkage, you know that decisions changed but not what code change caused them. With version linkage, you have full traceability from outcome to code.

## The Observability Dashboard: Real-Time Routing Performance

The observability dashboard visualizes routing performance in real time. It shows the current distribution of queries across model tiers, the cost and latency per tier, the quality scores per tier, and any active alerts. This is not a static report generated weekly—it is a live dashboard that updates every few minutes, allowing you to monitor routing behavior continuously and detect anomalies as they happen. The dashboard is your operational view of the router. During normal operation, it reassures you that routing is performing as expected. During incidents, it helps you diagnose the issue and assess impact.

Key metrics to display on the routing dashboard: query volume per model tier over the past hour, day, and week. This shows whether the routing distribution is stable or shifting. Cost per tier, both absolute spend and cost per query, over the past hour, day, and week. This shows whether cost is within budget and whether any tier is becoming disproportionately expensive. Latency per tier, showing p50 and p95 latency for each tier. This shows whether any tier is experiencing latency degradation. Quality scores per tier, if you have automated quality metrics. This shows whether any tier is producing lower-quality responses than expected. Routing decision confidence distribution, if your router produces confidence scores. This shows whether the router is making confident decisions or frequently uncertain. Error rates per tier, showing what percentage of queries failed. This shows whether any tier is experiencing elevated failures.

Another useful dashboard view is routing accuracy over time. If you have labeled data or user feedback for a subset of queries, compute routing accuracy: what percentage of queries were routed to the tier that produced the best outcome (highest quality, lowest cost within quality threshold, or fastest response within quality threshold). Plot this accuracy over time. If accuracy declines, it signals that routing logic is degrading and needs recalibration. A legal research platform in early 2026 tracked routing accuracy weekly by sampling 500 queries, manually labeling them with the ideal model tier based on query complexity, and comparing the labels to actual routing decisions. They maintained 91% accuracy for months, but in March accuracy dropped to 84%. Investigation revealed that a new category of regulatory queries had appeared, and the complexity classifier was undertrained on this category. They retrained the classifier on the new data and accuracy recovered to 90%.

The dashboard should also display active alerts. If feature distributions shift beyond thresholds, if cost exceeds budget, if latency exceeds SLAs, or if error rates spike, the dashboard highlights these alerts prominently. This allows on-call engineers or operations teams to respond immediately. Alerts should link to runbooks—predefined troubleshooting guides that explain what the alert means, what the likely causes are, and what steps to take. For example, an alert "Routing cost increased by 30% in the past hour" might link to a runbook that says: check if query volume increased, check if routing distribution shifted toward expensive tiers, check decision logs for feature anomalies, check if a new routing configuration was deployed, check if model pricing changed. The runbook guides responders through diagnosis and remediation, reducing time to resolution.

## Alerting on Anomalies: Detecting Routing Failures in Real Time

Alerts are the proactive counterpart to dashboards. Dashboards require someone to look at them; alerts push notifications when something goes wrong. Effective alerting detects routing failures before they cause significant cost or quality impact, and it does so with low false positive rates so alerts are actionable. There are several categories of routing alerts: distribution shift alerts, cost threshold alerts, quality degradation alerts, latency SLA alerts, and error rate alerts. Each category serves a different purpose and requires different thresholds.

Distribution shift alerts fire when the percentage of queries routed to a specific tier changes significantly. For example, if your baseline routing distribution is 60% Llama 4, 30% Claude Opus 4.5, 10% GPT-5.2, an alert might fire if GPT-5.2 usage exceeds 15% for more than one hour. This could indicate that queries are escalating unexpectedly, or that a new query type is being misclassified as complex. The threshold should be set based on historical variability—if GPT-5.2 usage normally fluctuates between 8% and 12%, set the alert threshold at 15% to avoid false positives. If usage jumps to 20%, that is a real signal. A retail chatbot in mid-2025 set a distribution shift alert at 20% usage of the expensive tier, up from a baseline of 12%. The alert fired during a product launch when a new feature drove complex queries. The team investigated, confirmed that the routing decisions were correct (the queries genuinely required the expensive tier), and accepted the cost increase as expected behavior. The alert prevented surprise cost overruns by giving the team visibility into the shift in real time.

Cost threshold alerts fire when cumulative spending exceeds a budget. This is a simple absolute threshold: if your monthly budget for model routing is $50,000 and you have spent $45,000 by day 20, an alert fires warning that you are on track to exceed budget. This gives you time to investigate whether the cost increase is due to increased query volume (acceptable), misrouting (unacceptable), or pricing changes (external factor). Cost alerts can also be rate-based: if cost per hour exceeds a threshold, alert immediately. A financial services company in late 2025 set a cost rate alert at $200 per hour. The alert fired when a misrouted batch job sent 10,000 simple queries to GPT-5.2 instead of Llama 4, costing $3,000 in 15 minutes. The team caught the issue within 20 minutes, killed the batch job, fixed the routing logic, and reran the job at the correct tier. Without the cost rate alert, the issue would have completed and cost $12,000 before being detected in the next day's cost report.

Quality degradation alerts fire when automated quality metrics fall below a threshold. If you have a quality scoring pipeline—perhaps automated evals that rate responses on correctness, completeness, or helpfulness—you can alert when the percentage of responses scoring above a threshold drops. For example, if your baseline is that 94% of responses score above 0.8, an alert might fire if that percentage drops to 88%. This could indicate that queries are being routed to models that cannot handle them, or that a model's performance has degraded. A customer support chatbot in early 2026 tracked resolution rate—the percentage of queries that resulted in a resolved ticket without escalation. Baseline resolution was 87%. When resolution dropped to 81%, an alert fired. Investigation revealed that a routing configuration update had lowered the complexity threshold, causing more queries to be routed to the cheap tier, which could not resolve complex issues. The team reverted the configuration and resolution recovered.

Latency SLA alerts fire when response latency exceeds your service level agreement. If your SLA is p95 latency under 2 seconds, alert when p95 exceeds 2.2 seconds for more than five minutes. Latency violations can be caused by router latency spikes, model tier latency increases, or cascading escalation in multi-tier routers. A healthcare appointment scheduler in mid-2025 had a 1.5-second SLA and set an alert at 1.8 seconds. The alert fired when a routing classifier update introduced additional feature extraction steps that added 400ms to routing latency. The team optimized the feature extraction and latency returned to normal.

Error rate alerts fire when the percentage of failed queries exceeds a threshold. Failures include model timeouts, model errors, routing errors, or downstream system errors. If your baseline error rate is 0.5%, alert when it exceeds 2% for more than ten minutes. A legal document processor in late 2025 set an error rate alert at 3%. The alert fired when a model provider had an outage and 15% of queries routed to that provider failed. The team temporarily rerouted all queries to a backup provider, and error rate returned to baseline.

## Continuous Router Improvement Through Feedback Loops

Observability enables continuous improvement. The decision logs, feature tracking, dashboards, and alerts provide the data and signals you need to iterate on routing logic. The improvement loop works like this: deploy routing logic, monitor performance through dashboards and alerts, detect anomalies or suboptimal patterns, analyze decision logs to diagnose the cause, propose a configuration update, version control the update, test it in staging, deploy it through gradual rollout, and monitor the impact. This loop runs continuously—weekly, biweekly, or monthly depending on query volume and rate of change.

A content generation platform in late 2025 ran this loop monthly. At the end of each month, they sampled 1,000 decision logs, labeled them with the ideal routing decision based on outcome metrics (cost, quality, latency), and computed routing accuracy. If accuracy was below 90%, they analyzed which query types were misrouted, retrained their routing classifier on recent data including the misrouted examples, validated the updated classifier on a held-out test set, deployed it to 10% of traffic for a week, compared cost and quality to the baseline, and if metrics improved, rolled it out to 100%. This monthly improvement loop increased routing accuracy from 87% in January to 93% by December, saving $18,000 in unnecessary model costs and improving quality scores by 4 points.

Another feedback loop is using decision logs to retrain routing classifiers. If your router uses a machine learning classifier to predict query complexity or risk, that classifier was trained on historical data. Over time, new query types appear, feature distributions shift, and the classifier's accuracy degrades. The decision logs contain the real production distribution of queries and routing outcomes. You can sample these logs, label them with ground truth (what model should have been chosen), and use them as training data for a new version of the classifier. This is a continuous learning loop: production decisions generate logs, logs provide training data, training data improves the classifier, improved classifier makes better decisions, better decisions generate better logs. The loop closes, and routing accuracy improves over time.

Observability also enables A/B testing of routing logic. Deploy two routing configurations to different subsets of traffic—perhaps 50% use configuration A and 50% use configuration B—and compare cost, quality, and latency outcomes. Decision logs track which configuration made each decision, and you can aggregate outcomes per configuration to determine which performs better. A financial advice chatbot in early 2026 A/B tested two routing thresholds: configuration A routed queries with complexity above 0.6 to the expensive tier, configuration B used a threshold of 0.7. After a week, they analyzed decision logs and found that configuration B had 8% lower cost with no quality degradation. They rolled out configuration B to 100% of traffic. A/B testing routing logic is low-risk and high-value—you make incremental changes, measure their impact, and keep what works.

## The Observability Discipline

Router observability is not a set of tools—it is a discipline. It requires that you instrument every decision, log every feature, version every configuration, monitor every metric, alert on every anomaly, and iterate on every failure. It requires that you assign ownership of observability: someone is responsible for maintaining decision logs, someone monitors dashboards, someone responds to alerts, someone runs the monthly improvement loop. It requires that you treat observability as a first-class production requirement, not a post-launch nice-to-have. You do not ship a router without decision logging. You do not deploy a configuration change without version control. You do not operate a router without dashboards and alerts.

The cost of not investing in observability is invisible failures. Misrouting goes undetected for months. Cost overruns are discovered in quarterly reviews, not real-time alerts. Quality degradation is reported by users, not by automated metrics. Routing logic ossifies because you lack the data to improve it. You make decisions based on intuition and anecdotes rather than data and analysis. The system works, in the sense that queries get routed and models respond, but it works suboptimally, and the gap between actual performance and potential performance grows over time. Observability closes that gap. It makes the router's behavior transparent, its failures diagnosable, and its performance improvable.

The teams that operate the most effective routing systems in 2026—the ones that achieve 90%+ routing accuracy, stay within cost budgets, meet latency SLAs, and continuously improve—are the ones that invested in observability from day one. They log every decision. They track every feature. They version every configuration. They monitor every metric. They alert on every anomaly. They iterate every month. Observability is not optional. It is the foundation of routing excellence.

The next step is expanding your routing strategy beyond single-model selection to more complex patterns: multi-model orchestration, parallel execution, and hybrid human-AI routing, which we will explore in the chapters ahead.
