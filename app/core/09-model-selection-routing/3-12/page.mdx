# 3.12 — Router Evaluation: Measuring Whether Your Router Actually Saves Money Without Losing Quality

In late 2025, an enterprise software company deployed a complexity-based router that reduced their AI infrastructure costs by 58% in the first month. Engineering celebrated, Finance was thrilled, and the router was declared a success. In month two, the Customer Success team reported a 23% increase in escalations from customers who were dissatisfied with AI-generated responses. The quality degradation had not been measured during router evaluation because the team compared router outputs against eval suite benchmarks rather than measuring quality on actual production traffic. The eval suite consisted of carefully curated examples that were not representative of the long tail of production queries, and the router worked well on the eval suite but failed on real user requests. By the time Customer Success flagged the problem, the router had processed 4.2 million queries, and the company had to manually review and re-process 180,000 of them. The root cause was evaluating the router on cost alone and assuming that passing the eval suite meant quality was preserved. It did not.

A router is only valuable if it reduces cost without degrading quality below an acceptable threshold. Measuring cost is straightforward: you sum the per-query costs for all routed queries and compare against the baseline of sending everything to your most capable model. Measuring quality is harder because quality is subjective, context-dependent, and often only observable through downstream user behavior. Measuring the interaction between cost and quality is harder still because you must answer the question: did the cost savings justify the quality tradeoff? Router evaluation is the discipline of measuring whether your router delivers value in production, and it requires instrumentation, baselines, metrics, and continuous monitoring that most teams underinvest in during initial deployment.

## The Quality Metric: Comparing Router Quality to Baseline Quality

The quality metric for a router is the average quality of responses produced by the router compared to the average quality of responses produced by a baseline routing strategy. The baseline is typically "send everything to the best model" — the strategy you would use if cost were not a constraint. You measure quality using the same criteria you use for model evaluation: task success rate, human preference ratings, downstream conversion metrics, escalation rate, or domain-specific quality dimensions. The quality metric is not a single number; it is a distribution of quality scores across your query population, and you must measure not only the mean but also the tail.

Running a quality comparison requires executing both routing strategies on the same set of queries and measuring the quality of the resulting responses. In shadow mode, you route every query through both the router and the baseline, execute both dispatch decisions, generate two responses, and log both. You then evaluate the quality of both responses using your quality measurement system — human raters, automated judges, downstream metrics, or a combination. You compare the quality distributions and measure whether the router's quality is statistically distinguishable from the baseline. If the router's quality is indistinguishable from the baseline, the router preserves quality. If the router's quality is lower but within your acceptable degradation threshold, the router is acceptable. If the router's quality is lower than your threshold, the router is unacceptable and must be tuned or rolled back.

The acceptable degradation threshold is a business decision, not a technical one. A customer support application might tolerate a 5% reduction in task success rate if it reduces cost by 50%. A medical diagnosis application might tolerate zero quality degradation regardless of cost savings. A content generation application might tolerate a 10% reduction in human preference ratings if it reduces cost by 70%. The threshold depends on your domain, your users, your competitors, and your business model. Setting the threshold requires input from Product, Engineering, Finance, and Customer Success. Engineering cannot set this threshold alone because it is fundamentally a tradeoff between user experience and cost, and only Product and Finance can make that tradeoff.

Measuring quality on production traffic is expensive because it requires generating two responses per query during the evaluation period. If you process one million queries per month and run a two-week evaluation, you generate two million responses during those two weeks instead of one million. Your inference costs double during the evaluation period. This is why many teams try to shortcut quality evaluation by using eval suites instead of production traffic. This is a mistake. Eval suites are not representative of production traffic, and a router that works well on an eval suite may fail catastrophically on production queries. The cost of running a proper evaluation is small compared to the cost of deploying a bad router and discovering the failure after millions of queries have been processed.

## The Cost Metric: Comparing Router Cost to Baseline Cost

The cost metric for a router is the total cost of all routed queries compared to the total cost of the baseline routing strategy. Cost is measured in dollars, and the calculation is straightforward: for each query, you multiply the input token count by the input token price and the output token count by the output token price, sum across all queries, and compare to the same calculation for the baseline. The cost savings percentage is one minus the ratio of router cost to baseline cost. A router that costs 42 cents per hundred queries when the baseline costs one dollar per hundred queries achieves 58% cost savings.

Cost measurement requires accurate token counting and pricing data. Your logging system must record the input token count, output token count, and model identifier for every query. Your cost calculator must apply the correct per-token price for each model, and prices change frequently enough that you cannot hardcode them. As of January 2026, GPT-5.2 costs 15 dollars per million input tokens and 60 dollars per million output tokens. Claude Opus 4.5 costs 15 dollars per million input tokens and 75 dollars per million output tokens. Gemini 3 Pro costs 7 dollars per million input tokens and 21 dollars per million output tokens. These prices will change in February 2026 when OpenAI updates its pricing, and they will change again when Anthropic and Google respond. Your cost calculator must pull prices from a configuration system that can be updated without redeploying code.

The cost metric is only meaningful if you measure it on the same traffic as the quality metric. If you measure cost on all production traffic but measure quality on a small sample, you may discover that your cost savings come from queries where the router degrades quality unacceptably. If you measure cost during a low-traffic period but measure quality during a high-traffic period, you may discover that the traffic distribution is different and the router behaves differently. The evaluation period for cost and quality must be identical, and the query set must be identical. This is why shadow mode is the correct evaluation strategy: you route every query through both the router and the baseline, measure cost and quality on the same queries, and compare the outcomes.

Cost savings are not uniform across your query distribution. Some query types are easy to optimize and deliver 80% cost savings with zero quality degradation. Other query types are hard to optimize and deliver 20% cost savings with 15% quality degradation. Your cost metric must be broken down by query type, customer segment, traffic source, and time of day to identify where the savings come from and where the quality degradation occurs. If your aggregate cost savings are 58% but your quality degradation is concentrated in enterprise customer queries, you have a problem. If your aggregate quality degradation is 3% but it is concentrated in high-value use cases, you have a problem. Aggregate metrics mask these patterns, and breaking down metrics by segment reveals them.

## The Combined Metric: Quality-Adjusted Cost Savings

Cost savings are only valuable if quality is preserved. A router that saves 90% of costs but degrades quality by 40% is not valuable; it is destructive. A router that saves 10% of costs with zero quality degradation is valuable. The combined metric that captures this tradeoff is **quality-adjusted cost savings**: the cost savings achieved while maintaining quality above your acceptable threshold. You compute this by measuring cost savings on the subset of queries where the router's quality meets or exceeds your quality floor. Queries where the router degrades quality below the floor are treated as failures and excluded from the cost savings calculation.

Here is how quality-adjusted cost savings works in practice. You run a two-week evaluation with 200,000 queries. Your baseline is "send everything to GPT-5.2 at 15 dollars per million input tokens and 60 dollars per million output tokens." Your router dispatches 60% of queries to Gemini 3 Pro at 7 dollars per million input tokens and 21 dollars per million output tokens, 30% to GPT-5 at 10 dollars per million input tokens and 40 dollars per million output tokens, and 10% to GPT-5.2. Your total cost for routed queries is 8,200 dollars. Your total cost for baseline queries is 18,400 dollars. Your raw cost savings are 55%. Now you measure quality. For 92% of queries, the router's quality is indistinguishable from the baseline. For 8% of queries, the router's quality is below your acceptable threshold. You exclude those 8% of queries from the cost savings calculation and recompute. The router cost for the 92% of acceptable queries is 7,500 dollars. The baseline cost for those same queries is 16,900 dollars. Your quality-adjusted cost savings are 56%. The difference is small in this case, but if the router had degraded quality on 30% of queries, the quality-adjusted cost savings would have been much lower than the raw cost savings.

Quality-adjusted cost savings penalizes routers that achieve cost savings by sacrificing quality on a significant fraction of queries. It rewards routers that achieve cost savings while preserving quality broadly. It provides a single number that captures the value of the router and allows you to compare different routing strategies objectively. If router A achieves 60% raw cost savings with 10% quality degradation, and router B achieves 50% raw cost savings with 2% quality degradation, which is better? Quality-adjusted cost savings gives you the answer: if your quality floor tolerates 5% degradation, router B is better because its quality degradation is within tolerance and its quality-adjusted cost savings are higher.

The quality floor is the most important parameter in the quality-adjusted cost savings calculation. Set it too high, and no router will meet it; you will conclude that routing is not viable. Set it too low, and routers will degrade quality unacceptably while still passing the threshold. The quality floor must be set based on business impact, not statistical significance. A 5% reduction in task success rate might be statistically significant but have negligible business impact if the tasks are low-stakes. A 2% reduction in task success rate might be statistically insignificant but have severe business impact if the tasks are high-stakes and the failures result in user churn. Product and Customer Success must define the quality floor based on observed user behavior and business outcomes, not based on abstract quality scores.

## Running Router Evaluation: A/B Testing and Shadow Mode

There are two strategies for running router evaluation: A/B testing and shadow mode. A/B testing splits your production traffic into two groups, routes group A through the router and group B through the baseline, and compares cost and quality between the two groups. Shadow mode routes every query through both the router and the baseline, generates responses from both, logs both, and uses the baseline response for the actual user while evaluating the router response offline. A/B testing measures real-world impact on users but only evaluates the router on half of your traffic. Shadow mode evaluates the router on all of your traffic but doubles your inference costs during the evaluation period. For most teams, shadow mode is the better choice because the cost of doubling inference for two weeks is small compared to the risk of deploying a bad router to half of your users.

A/B testing requires careful randomization to ensure that the two groups are comparable. You cannot split traffic by time of day because query complexity varies by time of day. You cannot split traffic by user because some users generate more queries than others. You must split traffic randomly at the query level, and you must ensure that the randomization is stable — the same query routed twice should go to the same group. This requires hashing the query or the session ID and using the hash to assign the group. You must also ensure that the two groups are balanced on dimensions that affect cost and quality: query complexity, query length, customer segment, and use case. If the two groups are imbalanced, your cost and quality comparisons will be confounded.

Shadow mode is simpler to implement and produces cleaner data. Every query goes through both paths, so you measure cost and quality on identical traffic. You do not need to worry about randomization or balance. You do not expose users to a potentially worse router. The tradeoff is that you generate twice as many responses during the evaluation period, doubling your inference costs. For a two-week evaluation period, this is acceptable. For a longer evaluation, the cost becomes prohibitive, and you must switch to A/B testing or sample-based evaluation.

Sample-based evaluation is a compromise: you select a random sample of queries — typically 10% to 20% — route them through both the router and the baseline in shadow mode, and evaluate cost and quality on the sample. You route the remaining 80% to 90% through the router only and measure aggregate cost but not per-query quality. Sample-based evaluation reduces inference costs while still providing high-quality data on a representative sample. The sample must be large enough to detect the quality degradation you care about. If you want to detect a 3% reduction in task success rate with 95% confidence, you need at least 2,000 queries in your sample. If you want to detect a 1% reduction, you need at least 10,000 queries.

## Common Evaluation Mistakes

The most common router evaluation mistake is measuring cost savings without measuring quality impact. Engineering deploys a router, observes a 50% reduction in inference costs, declares victory, and moves on. Two months later, Customer Success reports that users are complaining about AI quality, and Engineering discovers that the router has been degrading quality on 30% of queries since deployment. This happens because cost is easy to measure — it is a single number in your billing dashboard — while quality is hard to measure and requires deliberate instrumentation. If you do not measure quality, you will not discover quality degradation until it manifests as user complaints, and by then the damage is done.

The second most common mistake is measuring quality on eval suites instead of production traffic. Eval suites are curated examples that are often easier, cleaner, and more representative of common cases than production traffic. A router that works well on an eval suite may fail on production traffic because production traffic includes edge cases, adversarial queries, multilingual queries, ambiguous queries, and malformed queries that are absent from eval suites. The only reliable way to measure router quality is to measure it on production traffic. If you do not have the infrastructure to evaluate production traffic, you do not have the infrastructure to deploy a router safely.

The third most common mistake is ignoring latency impact. A router that reduces cost by 60% but increases latency from 800 milliseconds to 2,400 milliseconds may degrade user experience more than the cost savings justify. Latency impact is often invisible in eval suite testing because eval suites measure quality, not latency. You must measure end-to-end latency in production, including the time to compute routing signals, the time to execute the routing decision, and the time to generate the response. If your routing logic adds 200 milliseconds of overhead, and your small model is 400 milliseconds faster than your large model, you only save 200 milliseconds of latency by routing to the small model. If the routing logic is complex and adds 500 milliseconds of overhead, you may actually increase latency by routing to the small model.

The fourth most common mistake is running evaluation once during initial deployment and never re-evaluating. Routers degrade over time as your query distribution shifts, your models change, your business priorities change, and your cost constraints change. A router that achieved 55% quality-adjusted cost savings in October 2025 may achieve only 30% in January 2026 because your query distribution now includes more complex queries, or because your models have changed and the routing thresholds are no longer optimal, or because your quality floor has tightened and queries that were acceptable in October are unacceptable in January. You must re-evaluate your router every time you deploy a new model, every time your query distribution shifts significantly, and at least once per quarter even if nothing changes.

The fifth most common mistake is evaluating aggregate metrics without breaking down by segment. A router might achieve 50% cost savings with 2% quality degradation in aggregate, but if you break down by customer segment, you might discover that it achieves 70% cost savings with zero quality degradation for free-tier users and 20% cost savings with 10% quality degradation for enterprise users. Enterprise users are more sensitive to quality degradation, and a 10% degradation is unacceptable even if aggregate metrics look good. You must break down every metric by customer segment, use case, query type, and traffic source to identify where the router works and where it fails.

## How Often to Re-Evaluate Your Router

Router evaluation is not a one-time process; it is a continuous monitoring process. You must re-evaluate your router every time a relevant input changes: when you deploy a new model, when you add a new routing signal, when you adjust routing thresholds, when your query distribution shifts, when your business priorities change, or when your cost constraints change. Each of these changes can alter the cost and quality tradeoffs your router achieves, and you must measure the impact to ensure the router continues to deliver value.

Model changes require immediate re-evaluation because model capabilities, costs, and latencies change with every model release. When GPT-5.3 launches in March 2026, it may be faster and cheaper than GPT-5.2, which changes the optimal routing strategy. When Claude Opus 4.6 launches in April 2026, it may be more capable than Claude Opus 4.5, which changes the complexity threshold for routing to the frontier model. You must re-evaluate your router within one week of deploying a new model to measure whether the cost and quality tradeoffs have changed and whether your routing thresholds need adjustment.

Query distribution shifts require re-evaluation because a router optimized for one distribution may perform poorly on another. If your customer base shifts from consumer users to enterprise users, your query complexity distribution shifts upward, and a router tuned for consumer queries will under-route enterprise queries to capable models. If your product adds a new feature that generates a new query type, your router has no data on how to route those queries, and you must collect data and retrain or retune. You detect query distribution shifts by monitoring the distribution of routing signals — if your complexity score distribution shifts significantly over a two-week period, your query distribution has shifted, and you must re-evaluate.

Business priority changes require re-evaluation because your quality floor and cost constraints change with your business context. If you are in a growth phase, you may tolerate higher costs to maximize quality and user acquisition. If you are in a profitability phase, you may tighten cost constraints and accept more quality degradation. If a competitor launches a higher-quality product, you may need to tighten your quality floor to remain competitive. These changes require adjusting your routing thresholds and re-evaluating to ensure the new thresholds achieve the desired tradeoffs.

Even if nothing changes, you must re-evaluate your router at least once per quarter to detect gradual degradation that is too slow to trigger alerts but significant enough to matter over time. Model providers update models silently, query distributions drift gradually, and infrastructure changes introduce subtle latency regressions. Quarterly re-evaluation catches these gradual changes before they compound into significant problems.

## The Evaluation Dashboard: What Metrics to Track in Real Time

Router evaluation requires a dashboard that tracks cost, quality, latency, and routing distribution in real time. You cannot wait two weeks to discover that your router is degrading quality; you must detect problems within hours and roll back within a day. The evaluation dashboard displays the metrics that matter and alerts you when any metric crosses a threshold that indicates a problem.

The cost panel displays total cost per hour, cost per query, cost savings percentage relative to baseline, and cost distribution by model. You track cost per hour to detect sudden cost spikes caused by traffic surges or routing failures. You track cost per query to detect gradual cost increases caused by query distribution shifts or model price changes. You track cost savings percentage to verify that the router continues to deliver the expected savings. You track cost distribution by model to detect routing distribution shifts — if the router suddenly routes 80% of queries to the most expensive model, something is wrong.

The quality panel displays task success rate, human preference rating, downstream conversion rate, escalation rate, and user satisfaction score. These metrics are lagging — they take hours or days to become available — but they are the ground truth for quality. You compare each metric to the baseline and to historical values and alert when any metric drops below the quality floor or diverges significantly from the baseline. You also display the quality distribution by customer segment, use case, and query type to detect localized quality degradation that is masked in aggregate metrics.

The latency panel displays median latency, 95th percentile latency, 99th percentile latency, and latency distribution by model. You track median latency to detect average latency increases. You track 95th and 99th percentile latency to detect tail latency increases that affect a small fraction of users but are severe enough to matter. You track latency distribution by model to identify which models contribute most to latency and whether routing decisions are increasing or decreasing overall latency. You also display the latency overhead of the routing logic itself — the time between receiving a query and dispatching it to a model — to ensure that routing does not add unacceptable overhead.

The routing distribution panel displays the percentage of queries routed to each model, the distribution of routing signals, and the frequency of routing decision overrides. You track the percentage of queries routed to each model to detect shifts in routing behavior. If the router suddenly routes 10% more queries to the expensive model, you need to investigate whether the query distribution shifted or the routing logic changed. You track the distribution of routing signals — complexity scores, risk scores, confidence scores — to detect signal distribution shifts that indicate query distribution changes. You track the frequency of routing decision overrides — cases where a human or an automated system overrode the router's decision — to identify failure modes that the router does not handle well.

The dashboard must support drill-down and filtering. You must be able to click on a spike in cost per hour and see which queries contributed to the spike. You must be able to filter by customer segment and see the cost, quality, and latency metrics for enterprise customers separately from free-tier customers. You must be able to filter by time of day and see whether routing behavior differs between peak and off-peak hours. You must be able to filter by query type and see whether the router works well on some query types but poorly on others. Without drill-down and filtering, the dashboard is a set of numbers that does not enable action.

Router evaluation is the difference between a router that saves money and a router that saves money without losing quality. The evaluation infrastructure — shadow mode testing, quality measurement, cost measurement, latency measurement, the evaluation dashboard, and continuous re-evaluation — is not optional. It is the foundation of responsible router deployment, and teams that skip evaluation discover their mistakes only after the damage is done.

