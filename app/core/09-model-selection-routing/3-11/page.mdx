# 3.11 — Hybrid Routing Strategies: Combining Multiple Signals for Dispatch Decisions

In mid-2025, a customer support platform serving 400 companies routed queries using a complexity classifier that analyzed linguistic patterns and determined whether each request needed a frontier model or could be handled by a smaller alternative. The router worked well in testing, achieving 73% cost savings with only a 4% quality degradation on their evaluation suite. They deployed it to production in August 2025, and within three weeks, their Trust and Safety team flagged a pattern: queries containing threats, self-harm indicators, or abuse were being routed to the smaller model because they were often linguistically simple. A message reading "I know where you live" is structurally straightforward, but treating it with anything less than the most capable model was unacceptable. The complexity classifier had no mechanism to detect risk signals, and the engineering team had no mechanism to override low-complexity classifications when other factors demanded frontier-model treatment. They shut down the router and spent six weeks rebuilding it as a hybrid system that evaluated complexity, risk, urgency, and compliance requirements simultaneously. The root cause was treating routing as a single-dimensional problem when production reality demanded multi-signal decision-making.

Routing decisions in production systems are never truly single-dimensional. A query might be low-complexity but high-risk. Another might be high-complexity but low-urgency, allowing you to batch it for cost optimization. A third might be medium-complexity but subject to regulatory requirements that mandate specific model capabilities. A fourth might be simple but time-sensitive, requiring the fastest available backend regardless of cost. Your router must evaluate multiple signals simultaneously and resolve conflicts when they disagree. This is **hybrid routing** — combining complexity estimation, confidence assessment, risk detection, cost constraints, latency requirements, compliance mandates, and business priority into a unified dispatch decision. Hybrid routing is the difference between a router that works in testing and one that works in production.

## The Weighted Scoring Approach

The simplest hybrid routing strategy is weighted scoring. You define a set of signals, assign each signal a weight, compute a score for each candidate model, and dispatch to the model with the highest score. A typical signal set includes complexity, confidence, risk, latency, and cost. Each signal produces a normalized score between zero and one for each candidate model. You multiply each score by its weight, sum the weighted scores, and select the model with the highest total. This approach is transparent, interpretable, and easy to tune. The challenge is determining the correct weights and ensuring the signals are measured on comparable scales.

Here is how weighted scoring works in practice. You have three candidate models: a frontier model at 60 cents per thousand tokens, a mid-tier model at 8 cents, and a small model at 0.4 cents. Your signals are complexity (weight 0.3), risk (weight 0.4), latency requirement (weight 0.2), and cost (weight 0.1). For a given query, your complexity classifier estimates complexity at 0.7, your risk detector flags no risk indicators (score 0.1), your latency requirement is moderate (score 0.5), and cost sensitivity is high (score 0.8 for the cheapest model, 0.2 for the most expensive). You compute the weighted score for each model. The frontier model scores high on complexity (0.7 times 0.3 equals 0.21) and low on risk (0.9 times 0.4 equals 0.36, because lower risk detection means the frontier model is less necessary), moderate on latency (0.8 times 0.2 equals 0.16), and low on cost (0.2 times 0.1 equals 0.02), for a total of 0.75. The mid-tier model scores moderate on complexity (0.5 times 0.3 equals 0.15), moderate on risk (0.6 times 0.4 equals 0.24), high on latency (0.9 times 0.2 equals 0.18), and high on cost (0.6 times 0.1 equals 0.06), for a total of 0.63. The small model scores low on complexity (0.2 times 0.3 equals 0.06), low on risk (0.1 times 0.4 equals 0.04), moderate on latency (0.7 times 0.2 equals 0.14), and very high on cost (0.8 times 0.1 equals 0.08), for a total of 0.32. The frontier model wins, and you dispatch to it.

The weights encode your priorities. A weight of 0.4 on risk means risk considerations are the most important factor in routing decisions. If your risk detector flags a query as high-risk, the frontier model will almost always win regardless of other signals. A weight of 0.1 on cost means cost is the least important factor — you are willing to pay for quality and safety. You adjust weights based on your business context. A healthcare application might assign risk a weight of 0.6 and cost a weight of 0.05. A consumer chatbot might assign cost a weight of 0.4 and risk a weight of 0.1. The weights are not discovered through machine learning; they are business decisions made by Product, Engineering, and Trust and Safety together.

Determining signal weights requires running controlled experiments with different weight configurations and measuring the resulting cost, quality, and safety outcomes. You start with an initial hypothesis about priorities, deploy the router in shadow mode, and measure what would have happened under different weight configurations. If your initial weights route too much traffic to expensive models, you reduce the cost weight or increase the complexity and latency weights. If your initial weights route high-risk queries to small models, you increase the risk weight. You iterate until the router's behavior matches your desired tradeoffs. This process takes weeks, not days, because you need enough production traffic to see the full distribution of query types and edge cases.

## The Decision Tree Approach

Weighted scoring assumes all signals are commensurable — that you can meaningfully combine complexity, risk, and cost into a single numeric score. In practice, some signals are absolute constraints rather than weighted preferences. Risk is often an absolute constraint: if a query is flagged as high-risk, it must go to the frontier model regardless of complexity, cost, or latency. Compliance requirements are absolute constraints: if a query involves regulated data, it must go to a model that meets compliance standards. Latency requirements can be absolute constraints: if a query is marked as real-time, it cannot go to a model with multi-second response times. When you have absolute constraints, weighted scoring does not work because it allows other signals to override the constraints. You need a decision tree approach instead.

A decision tree router evaluates signals in priority order, making hard cutoffs at each level. The first level is risk: if the query is flagged as high-risk, route to the frontier model and stop. If not, proceed to the second level. The second level is compliance: if the query involves regulated data, route to a compliant model and stop. If not, proceed to the third level. The third level is latency: if the query requires sub-second response time, route to a fast model and stop. If not, proceed to the fourth level. The fourth level is complexity: if the query is high-complexity, route to a capable model; if low-complexity, route to a cheap model. This cascade structure ensures that absolute constraints are never violated, while lower-priority factors are used to optimize cost and quality within the space of acceptable models.

The decision tree approach is more transparent than weighted scoring because the priority order is explicit. You can explain to stakeholders exactly why a query was routed to a specific model: "It was flagged as high-risk, so risk handling took precedence over cost optimization." You can audit the router by inspecting the decision tree and verifying that the priorities match your policies. You can test the router by constructing queries that trigger each branch of the tree and verifying the resulting dispatch decisions. The tradeoff is that decision trees are less flexible than weighted scoring: if you want to make a complex tradeoff between two non-constraint signals, you have to encode it as a sequence of hard cutoffs, which may not capture the nuance you want.

Building a decision tree router requires defining the priority order and the thresholds at each level. The priority order is a business decision: does risk come before compliance, or compliance before risk? Does latency come before complexity, or complexity before latency? There is no universal answer; it depends on your domain. A financial services application might prioritize compliance first, then risk, then complexity, then latency, then cost. A gaming application might prioritize latency first, then complexity, then cost, then risk, then compliance. The thresholds are empirical: at what risk score do you route to the frontier model? At what complexity score do you route to the mid-tier model? You determine thresholds by measuring the cost and quality outcomes of different cutoff points and selecting the thresholds that achieve your desired tradeoffs.

## The Machine Learning Approach

Both weighted scoring and decision trees require you to manually define how signals combine to produce routing decisions. An alternative is to train a machine learning model that takes all signals as input features and predicts the optimal backend model. You collect a dataset of queries, signals, and outcomes — for each query, you record the complexity score, risk score, latency requirement, cost constraint, and the quality and cost of responses from each candidate model. You train a classifier to predict which model will produce the best quality-adjusted cost outcome for a given set of signals. The trained model becomes your router: at inference time, you compute the signals for a new query, feed them into the routing model, and dispatch to the predicted backend model.

The machine learning approach can discover non-obvious interactions between signals that human-designed routers miss. For example, it might learn that high-complexity queries with moderate risk scores are better handled by mid-tier models than frontier models because frontier models tend to over-explain and produce unnecessarily verbose responses that harm user experience. It might learn that low-complexity queries with high cost sensitivity should go to small models only if latency is not time-sensitive, because the small model is slower and the latency impact outweighs the cost savings for time-sensitive queries. These interactions are difficult to encode in weighted scoring or decision trees, but a machine learning model can learn them directly from data.

The tradeoff is that machine learning routers are opaque and require significant data and infrastructure to train and maintain. You need thousands of labeled examples to train a routing model, which means you need to run experiments where you route queries to multiple models and measure the outcomes. You need to retrain the model regularly as your candidate models change, your query distribution shifts, and your quality and cost tradeoffs evolve. You need to monitor the model for drift and degradation. You need to debug failures when the router makes bad decisions, and debugging a learned model is harder than debugging a decision tree. For most teams, the machine learning approach is overkill unless you have a massive query volume and a dedicated team to maintain the routing infrastructure.

If you do choose the machine learning approach, start with a simple model architecture. A gradient-boosted decision tree or a shallow neural network is sufficient for most routing tasks. Do not use deep learning or transformer-based models for routing; the signal space is low-dimensional and the decision boundaries are not complex enough to justify the overhead. Use cross-validation to prevent overfitting. Use interpretability tools like SHAP to understand which signals the model relies on most heavily, and verify that the learned priorities align with your business policies. If the model learns to prioritize cost over risk, you have a problem, and you need to adjust your training data or add constraints to the model.

## Signal Interaction and Conflict Resolution

The defining challenge of hybrid routing is handling interactions and conflicts between signals. A query can be low-complexity but high-risk, and these two signals point in opposite directions: complexity suggests routing to a cheap model, while risk suggests routing to a frontier model. A query can be high-complexity but low-urgency, allowing you to batch it for cost optimization, but your complexity signal suggests routing to an expensive model immediately. A query can be medium-complexity but subject to compliance requirements that eliminate most of your candidate models, forcing you to route to a specific compliant model regardless of cost or quality tradeoffs. Your router must resolve these conflicts in a way that respects your business priorities and never violates absolute constraints.

The most common conflict is between cost and quality. Your complexity classifier predicts that a query is at the boundary between mid-tier and frontier model capability — a complexity score of 0.6 where your threshold is 0.5. Your cost constraint is tight because you are near your monthly budget limit. Do you route to the mid-tier model to save money, accepting a small quality degradation, or do you route to the frontier model to preserve quality, accepting a small budget overrun? The answer depends on your priorities, and your router must encode those priorities explicitly. If quality is more important than cost, you route to the frontier model. If cost is more important than quality, you route to the mid-tier model. If the priority depends on context — quality for enterprise customers, cost for free-tier users — you add a customer tier signal to your router and use it to resolve the conflict.

The most dangerous conflict is between risk and any other signal. If your risk detector flags a query as potentially harmful, nothing else matters. You route to the frontier model regardless of complexity, cost, latency, or any other consideration. Risk is an absolute constraint, not a weighted preference. If your router uses weighted scoring, the risk signal must have a weight high enough that it always overrides other signals when risk is detected. If your router uses a decision tree, risk must be the first branch in the tree. If your router uses machine learning, you must ensure that the training data includes examples of high-risk queries being routed to frontier models, and you must validate that the learned model respects this priority. A router that routes high-risk queries to cheap models for cost savings is a liability, not an optimization.

Conflict resolution also applies to interactions between non-conflicting signals. A query might be high-complexity and high-risk, both pointing toward the frontier model, but the latency requirement is strict and the frontier model is slow. Do you route to a faster mid-tier model and accept the quality degradation, or do you route to the frontier model and accept the latency violation? The answer is almost always to respect the quality and risk constraints and add latency optimization as a separate layer. You route to the frontier model and optimize its latency through prompt engineering, caching, or infrastructure improvements. You do not sacrifice quality or safety for speed.

## When to Use Hybrid Routing and When to Keep It Simple

Hybrid routing adds engineering complexity, and complexity is only justified when it delivers meaningful value. If your routing decision is dominated by a single signal — if risk detection alone determines 90% of your routing decisions — you do not need a hybrid router. You need a simple risk-based router with a fallback complexity classifier for non-risky queries. If your cost constraints are so tight that you must route everything to the cheapest model unless absolutely necessary, you do not need a hybrid router. You need a whitelist of queries that require frontier models and a default route to cheap models for everything else. Hybrid routing is valuable when multiple signals contribute meaningfully to routing decisions and when the interactions between signals are non-trivial.

The clearest sign that you need hybrid routing is when your single-signal router produces unacceptable outcomes that you cannot fix by tuning the signal. If your complexity-based router routes high-risk queries to small models, adding a risk signal is not optional. If your cost-based router routes time-sensitive queries to slow models, adding a latency signal is not optional. If your latency-based router routes complex queries to fast but incapable models, adding a complexity signal is not optional. Each time you discover a new dimension of routing failure, you are discovering a new signal that your router must incorporate.

The engineering complexity of hybrid routing scales with the number of signals and the sophistication of the combination logic. A two-signal router using weighted scoring is straightforward: compute two scores, multiply by weights, sum, and dispatch. A five-signal router using a decision tree with conditional branches is moderate complexity: you need careful logic to ensure the branches are evaluated in the correct order and the thresholds are applied consistently. A ten-signal router using machine learning is high complexity: you need training infrastructure, monitoring infrastructure, retraining pipelines, and interpretability tooling. For most teams, a three-to-five-signal router using weighted scoring or decision trees is the right tradeoff between capability and maintainability.

If you are unsure whether you need hybrid routing, start with the simplest single-signal router that addresses your primary constraint, deploy it in shadow mode, and measure the failure cases. If the failures are rare and inconsequential, you do not need a hybrid router. If the failures are common or severe, analyze the failure cases to identify the missing signals. Add one signal at a time, validate that it fixes the failures it was intended to fix, and measure whether it introduces new failures elsewhere. Hybrid routing is iterative. You do not design the perfect multi-signal router on the first attempt. You start simple, measure, add complexity as needed, and stop when the marginal value of additional signals drops below the marginal cost of additional complexity.

## Testing Hybrid Routers: Interaction Effects Are the Hard Part

Testing a hybrid router is fundamentally different from testing a single-signal router because you must test not only each signal individually but also the interactions between signals. A complexity classifier might work perfectly in isolation, and a risk detector might work perfectly in isolation, but when combined in a hybrid router, they might produce bad decisions because the combination logic is wrong or the weights are mistuned or the priority order is incorrect. Testing a hybrid router requires constructing test cases that exercise every combination of signal values and verifying that the resulting routing decisions match your expected priorities.

Start by testing each signal in isolation. Verify that the complexity classifier produces reasonable scores for low, medium, and high-complexity queries. Verify that the risk detector flags queries with threats, abuse, self-harm indicators, and other harmful content. Verify that the latency estimator predicts response times accurately. Verify that the cost calculator computes per-query costs correctly. Verify that the confidence estimator produces calibrated confidence scores. These are unit tests for your signals, and they are necessary but not sufficient.

Next, test the combination logic with synthetic queries that have known signal values. Construct a query that is low-complexity and low-risk and verify that it routes to the cheap model. Construct a query that is low-complexity but high-risk and verify that it routes to the frontier model despite being easy. Construct a query that is high-complexity but low-urgency and verify that it routes to the batch-optimized model. Construct a query that is medium-complexity with strict latency requirements and verify that it routes to the fast model even if it is more expensive. These are integration tests for your router, and they verify that the signals combine correctly to produce the intended dispatch decisions.

Finally, test the router on production traffic in shadow mode. Route every query through both the hybrid router and a baseline router (typically "send everything to the best model"), execute both routing decisions, and compare the outcomes. Measure the quality, cost, and latency of responses from each router. Identify queries where the hybrid router produces worse outcomes than the baseline and analyze why. The most common failure mode is that the hybrid router under-routes complex or risky queries to cheap models because the combination logic does not weight risk or complexity heavily enough. The second most common failure mode is that the hybrid router over-routes simple queries to expensive models because the combination logic is too conservative. Shadow mode testing reveals these failures before they impact users.

Testing interaction effects requires deliberately constructing edge cases where signals conflict. A query that is high-complexity but low-confidence — the router thinks the query is hard but is not sure. A query that is low-complexity but high-risk — the router thinks the query is easy but dangerous. A query that is medium-complexity but subject to compliance requirements that eliminate most candidate models. A query that is high-urgency but high-cost-sensitivity — the user wants a fast response but is on a free tier. These edge cases are rare in aggregate but common enough that your router will encounter them daily at scale, and your router must handle them correctly.

The test suite for a hybrid router is an order of magnitude larger than the test suite for a single-signal router because the number of test cases grows exponentially with the number of signals. A three-signal router with three possible values per signal requires 27 test cases to cover every combination. A five-signal router with three possible values per signal requires 243 test cases. You do not need to test every combination exhaustively, but you do need to test every combination that represents a meaningful conflict or interaction. If two signals never conflict — if high complexity and high risk always point to the same model — you do not need to test their interaction. If two signals frequently conflict — if complexity and cost often disagree — you must test their interaction thoroughly.

## Router Versioning and Evolution

Hybrid routers evolve over time as you add new signals, adjust weights, update thresholds, and change priority orders. Each change is a new version of the router, and you must track versions carefully to understand which router produced which outcomes and to enable rollback if a new version degrades quality or cost. Router versioning is often overlooked in initial implementations and becomes a crisis when a bad router version routes millions of queries before anyone notices the degradation.

Every router deployment must be tagged with a version identifier that is logged with every routing decision. When you analyze router performance, you group by version to measure whether the new version improved cost, quality, latency, or other metrics relative to the previous version. When you discover a failure, you check the router version to determine whether it is a new failure introduced by the latest version or a long-standing failure that was present in earlier versions. When you need to roll back, you redeploy the previous version and verify that the failure rate drops. This requires infrastructure: a versioned router artifact, a deployment system that tracks versions, and a logging system that records the version for every routing decision.

Router evolution is driven by failure analysis and metric tracking. You deploy a router, measure its performance, identify failure modes, analyze the root causes, and add signals or adjust weights to fix the failures. You redeploy, measure again, and repeat. Each iteration should improve one or more metrics without degrading others. If a new version reduces cost but degrades quality below your quality floor, you roll back and try a different adjustment. If a new version improves quality but increases cost beyond your budget, you roll back and try a different adjustment. Router evolution is not a one-time design process; it is an ongoing optimization process that continues as long as the router is in production.

The next subchapter covers router evaluation: how to measure whether your router actually saves money without losing quality, and how to build the evaluation infrastructure that makes router optimization possible.

