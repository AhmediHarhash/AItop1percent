# 7.5 â€” HITL Interface Design: Presenting Agent Decisions for Review

In March 2025, a healthcare automation startup called MediFlow deployed an agent system that routed prior authorization requests to insurance companies. The agent would read patient medical records, extract relevant clinical information, and draft authorization requests. Before submission, human clinical reviewers approved each request. Within two weeks, the approval rate dropped to 34 percent, and average review time ballooned to twelve minutes per case. The clinical reviewers complained that they could not understand what the agent was proposing or why. The interface showed the final authorization letter, the raw medical record excerpts the agent had referenced, and a wall of text explaining the reasoning. Reviewers spent most of their time scrolling through screens trying to piece together what had changed, what mattered, and whether the proposal made medical sense. The interface had every piece of information a reviewer might need, but it presented that information in a way that made good decisions nearly impossible. MediFlow had built a human-in-the-loop system with a broken loop, and the breakage was not in the agent or the humans but in the interface between them.

You face this interface design challenge every time you build a human-in-the-loop agent system. The agent makes proposals, and humans review them, but the quality of those reviews depends entirely on how you present the agent's decisions. Show too little information, and reviewers approve dangerous actions because they cannot see the risks. Show too much information, and reviewers become paralyzed by detail, either rubber-stamping everything to get through their queue or rejecting everything out of caution. The interface is not just a presentation layer. It is a decision support system that shapes whether your human reviewers make good decisions quickly or bad decisions slowly.

## The Five Elements Reviewers Actually Need

When a human reviews an agent decision, they need to answer a single question: should I approve this action or not? To answer that question well, they need five specific pieces of information presented in a specific order. First, they need to see the proposed action itself, stated clearly and concisely. Not the reasoning, not the context, just what the agent wants to do. Second, they need to understand why the agent is proposing this action, the core justification in one or two sentences. Third, they need the relevant context that makes this proposal appropriate or inappropriate, the specific facts that matter for this decision. Fourth, they need to know the risk level, how confident the agent is, and what could go wrong if this action is executed. Fifth, they need to see the alternatives, what other actions the agent considered and why it rejected them.

Most interfaces get this order backwards. They show context first, often pages of it, before the reviewer even knows what decision they are making. They bury the proposed action in the middle of a paragraph of reasoning. They omit the risk level entirely or show it as an isolated confidence score without explaining what that score means. They never show alternatives, leaving reviewers to wonder whether the agent considered obvious options or just picked the first thing that seemed plausible. The result is an interface that forces reviewers to do archaeology, excavating the actual decision from layers of surrounding information.

MediFlow's interface showed the final authorization letter first, which was the output of the proposed action but not the action itself. Reviewers had to read through the letter to figure out what clinical codes the agent was requesting authorization for, what procedures it was justifying, and what medical necessity argument it was making. The reasoning section dumped the agent's entire chain-of-thought, including irrelevant internal deliberations about which sections of the medical record to prioritize. The context section showed raw excerpts from the patient's chart without highlighting which excerpts justified which parts of the proposal. There was no risk level, no confidence score, and no alternatives. Reviewers felt like they were reading a mystery novel where they had to figure out the plot themselves.

The fix started with reordering the interface. The top of the screen showed the proposed action in a single sentence: "Request prior authorization for lumbar MRI with contrast, CPT code 72148, medical necessity based on chronic lower back pain with radicular symptoms unresponsive to six weeks of conservative treatment." Below that, a two-sentence justification: "Patient meets clinical criteria for advanced imaging per insurance policy requirements. Conservative treatment documented in three physical therapy visits and two primary care follow-ups." Below that, a context section with exactly three highlighted excerpts from the medical record, each labeled with why it mattered. Below that, a risk assessment: "High confidence recommendation, low denial risk, potential issue if insurance disputes timeline of conservative treatment." At the bottom, alternatives: "Agent also considered requesting non-contrast MRI, but contrast is clinically indicated per radiologist consultation note."

This reordering reduced average review time from twelve minutes to three minutes, and approval rates climbed to 89 percent. Reviewers could answer the core question quickly because the interface presented information in decision-making order, not in data-dump order.

## Information Overload and the Paralysis Problem

The instinct when designing review interfaces is to show everything. If a piece of information might be relevant, include it. If the agent accessed a data source, display it. If there is a log of the agent's reasoning process, present it. This instinct comes from a reasonable place, the fear that a reviewer will miss something important and approve a bad decision. But the effect of showing everything is the opposite of what you intend. Reviewers who see too much information make worse decisions, not better ones.

Cognitive load research is unambiguous on this point. When humans are presented with more information than they can process in working memory, they default to heuristics. They scan for keywords they recognize, they pattern-match to previous cases they have seen, or they give up and either approve everything or reject everything based on gut feeling. They do not systematically evaluate the decision. They cannot, because the interface has overloaded their cognitive capacity.

In June 2025, a legal research agent company called CaseMind built a system that suggested case citations for legal briefs. The agent would read a draft argument, find relevant precedents, and propose citations to insert. Attorneys reviewed these proposals before accepting them. The interface showed the proposed citation, the paragraph where it would be inserted, the full text of the cited case, a summary of the case's holding, a list of other cases that cited it, and a relevance score. Attorneys spent an average of eight minutes per citation review, and many reported that they felt less confident after reviewing than before. The interface was providing so much information that attorneys could not distinguish signal from noise.

The breakthrough came when CaseMind implemented progressive disclosure. The initial view showed only the proposed citation, the paragraph it would be inserted into, a one-sentence summary of why the case was relevant, and a relevance score. If the attorney wanted more detail, they could expand sections to see the full case text, the citation network, or the agent's reasoning. Most reviews never expanded anything. Attorneys could make a confident decision from the summary view alone. Average review time dropped to ninety seconds, and attorney satisfaction scores doubled.

Progressive disclosure works because it matches the information load to the decision complexity. Simple decisions get simple interfaces. Complex decisions get access to deep detail, but only when the reviewer actively requests it. The default is clean, focused, and fast. This requires you to make a hard choice about what goes in the summary view. You cannot include everything that might be relevant. You have to prioritize ruthlessly based on what actually drives the decision.

## Highlighting What Changed and What Matters

One of the hardest review tasks is understanding what an agent has modified. When an agent edits a document, generates a report, or updates a database record, the reviewer needs to see not just the final output but what changed from the previous state. Showing a before-and-after side-by-side comparison is a start, but it is not enough. If the agent changed two hundred words in a two-thousand-word document, the reviewer has to visually scan for differences. If the agent updated ten fields in a hundred-field database record, the reviewer has to compare columns line by line. This works for small changes, but it does not scale.

In September 2025, a financial compliance agent company called AuditPilot built a system that reviewed expense reports and flagged anomalies. The agent would suggest edits to expense categories, flag missing receipts, and recommend policy violations be sent for manager review. The interface showed the original expense report on the left and the agent-edited version on the right. Reviewers were supposed to spot the differences and approve the changes. In practice, reviewers approved 96 percent of suggestions without actually reading them, because finding the differences in a side-by-side view of a fifty-line expense report was too much work.

The fix was to add change highlighting and change annotation. The agent's edits were highlighted in yellow in the after view, and each edit had a tooltip explaining why the change was made. A summary at the top listed all changes in plain language: "Recategorized three meals from client entertainment to team meals due to missing client attendee documentation. Flagged hotel charge exceeding per-diem limit. Removed duplicate Uber receipt." Reviewers could see exactly what changed and why without hunting for differences. Approval rates stayed high, but rejection rates for actually problematic expenses increased by 300 percent, meaning reviewers were now catching real issues instead of rubber-stamping everything.

The principle generalizes beyond document editing. Whenever an agent modifies something, your interface should explicitly enumerate the changes and explain the rationale for each one. Forcing reviewers to infer changes from a before-and-after comparison is lazy design. Changes should be first-class objects in your interface, visible and annotated.

## Showing Confidence Scores and Uncertainty

Agents operate under uncertainty, and they should communicate that uncertainty to human reviewers. A confidence score is the most common way to do this, but most interfaces display confidence scores badly. They show a percentage or a color-coded indicator without explaining what it means or how it should affect the reviewer's decision. A confidence score of seventy-two percent tells you nothing unless you know what seventy-two percent represents. Is that the probability the action is correct? The probability it will achieve the intended outcome? The agent's calibrated uncertainty? Without context, the number is decoration.

In November 2025, a customer support agent company called SupportAI deployed a system that suggested responses to customer emails. The interface showed the suggested response and a confidence score as a colored circle: green for high confidence, yellow for medium, red for low. Support agents were supposed to review low-confidence suggestions more carefully. In practice, agents treated green-circle suggestions as pre-approved and often sent them without reading. One green-circle suggestion told a customer their refund had been processed when it had not, leading to an escalated complaint and a social media incident. The agent had been highly confident in a factually incorrect statement, and the interface gave no indication of what the confidence score actually measured.

The fix required breaking confidence into components. Instead of a single score, the interface showed three indicators: factual accuracy confidence, tone appropriateness confidence, and policy compliance confidence. Each had a score and a plain-language explanation. A suggestion might be green on tone, yellow on factual accuracy because it referenced an order status the agent could not verify, and green on policy compliance. This granular breakdown helped support agents understand what to double-check. Factual accuracy issues dropped by sixty percent because agents knew which parts of the response needed verification.

Beyond confidence scores, your interface should surface uncertainty explicitly. If the agent is making assumptions, state them. If the agent is extrapolating from incomplete data, highlight the gaps. If the agent considered multiple interpretations and picked one, show the alternatives. Uncertainty is not a weakness to hide. It is critical information that helps reviewers know where to focus their attention.

## Side-by-Side Comparison of Alternatives

Most agent decisions involve choosing between options. The agent evaluated multiple actions and picked one. Reviewers need to see not just the chosen action but the alternatives the agent rejected and the reasons for rejection. This transparency serves two purposes. First, it lets reviewers spot cases where the agent made the wrong choice, where an alternative was actually better. Second, it builds trust by demonstrating that the agent did consider obvious options rather than latching onto the first plausible answer.

In January 2026, a procurement agent company called BuySmarter built a system that recommended vendors for purchase orders. The agent would analyze the requested items, find vendors that could fulfill the order, and recommend one based on price, delivery time, and vendor reliability. The interface showed the recommended vendor and the justification. Procurement specialists approved ninety-two percent of recommendations, but several approvals led to problems. In one case, the agent recommended a vendor with a slightly lower price, but the vendor had a poor track record of on-time delivery, and the delayed shipment caused a production line shutdown. A procurement specialist later pointed out that if they had known there was an alternative vendor with a higher price but better reliability, they would have overridden the recommendation.

The fix was to add an alternatives table. The interface showed the recommended vendor at the top, then a table with the top three alternatives, each with key metrics: price, delivery time, reliability score, and a note explaining why the agent did not pick it. The recommended vendor might have the lowest price, but the table would show that an alternative had a ninety-eight percent on-time delivery rate versus the recommended vendor's seventy-four percent rate. Procurement specialists could make informed tradeoff decisions. Override rates increased to eighteen percent, which was a good thing because those overrides prevented bad outcomes. The agent was still doing valuable work by narrowing the options, but the interface gave humans the information they needed to exercise judgment.

Side-by-side comparisons work best when they are structured, not narrative. Do not write paragraphs explaining alternatives. Use tables, comparison cards, or lists with consistent fields. Make it easy for reviewers to scan the key differences at a glance.

## Mobile-Friendly Review Interfaces for On-Call Reviewers

Many human-in-the-loop systems require urgent reviews outside of business hours. An agent detects an anomaly, proposes a remediation action, and needs approval from an on-call engineer. An agent drafts a critical customer communication, and a manager needs to approve it before it goes out. These reviews happen on mobile devices, often while the reviewer is doing something else. If your review interface is not mobile-friendly, on-call reviewers will either ignore the request until they are at a computer or approve it without proper review just to clear the notification.

In February 2026, a database operations agent company called DBGuardian built a system that detected performance issues and proposed index optimizations. When the agent detected a slow query pattern, it would generate an index creation plan and send it to an on-call database engineer for approval. The interface was a web dashboard designed for desktop screens. It showed query execution plans, table statistics, and index impact projections in multi-column layouts. On-call engineers received alerts on their phones, clicked through to the approval interface, and saw an unreadable jumble of tiny text and overlapping columns. Most engineers would approve without reviewing, figuring that if the agent was confident enough to propose the change, it was probably fine. This worked until the agent proposed an index on a high-write table during peak traffic hours, causing a fifteen-minute site slowdown that cost the company an estimated eighty thousand dollars in lost transactions.

The fix required a mobile-first redesign. The approval interface started with a single-screen summary: the proposed action, the expected impact, the risk level, and approve/reject buttons. All details were collapsed by default but accessible with a tap. The query execution plan was reformatted as a vertical list instead of a multi-column table. The index impact projections were shown as simple before-and-after metrics instead of charts. On-call engineers could make an informed decision in thirty seconds on a phone screen. Approval quality improved, and incidents caused by unreviewed approvals dropped to zero.

Mobile-friendly review interfaces require ruthless simplification. You cannot just shrink a desktop interface to fit a phone screen. You have to rethink the information hierarchy for a context where the reviewer has one hand free, is possibly distracted, and wants to make a decision in under a minute. Single-column layouts, large touch targets, collapsible sections, and summary-first design are not optional niceties. They are requirements for any review interface that might be used outside of office hours.

## The Interface as Decision Architecture

The way you design a review interface is the way you architect human decisions. If you show information in the wrong order, humans will make decisions in the wrong order. If you overload them with detail, they will shortcut their evaluation. If you hide uncertainty, they will overestimate confidence. If you omit alternatives, they will assume the agent considered all options when it may not have. The interface is not a neutral window into the agent's proposal. It is an active shaper of the reviewer's cognitive process.

This means interface design for human-in-the-loop systems is not a UI problem. It is a decision quality problem. You are not building screens. You are building cognitive scaffolding that helps humans make better decisions faster. Every element you include or exclude, every order you choose, every default you set, either helps or hinders that decision process. Treat interface design as seriously as you treat agent design, because a perfect agent paired with a broken interface produces broken outcomes just as reliably as a broken agent paired with a perfect interface. The human-in-the-loop is only as strong as the loop itself, and the loop is the interface.
