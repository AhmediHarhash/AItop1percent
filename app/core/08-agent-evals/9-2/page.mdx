# 9.2 — Agent Test Environments: Sandboxes, Mocks, and Simulations

In August 2025, a healthcare technology company ran integration tests for an agent designed to assist clinicians with patient record searches and documentation tasks. The agent had access to tools that queried an electronic health records system, retrieved lab results, and drafted clinical notes. The engineering team ran their integration tests against a shared staging database that contained anonymized patient data copied from production. Tests passed reliably during development. Two days after deploying the agent to production, clinicians reported that the agent was retrieving records for the wrong patients—it matched patients by first name and birth month instead of by medical record number, a bug that had existed in the retrieval tool for weeks but never surfaced in testing. The root cause was that the staging database contained only 400 patient records, and none of those records had overlapping first names and birth months. In production, with 180,000 active patient records, name collisions were common, and the retrieval bug caused the agent to return incorrect records in 11% of queries. The staging environment had **data distribution mismatch**: it was too clean, too small, and too homogeneous to reveal failure modes that only appear at production scale with production diversity. The team had testing infrastructure, but their test environment did not replicate the conditions under which the agent would actually operate.

## The Requirement for Isolated Test Environments

Agent tests must run in **isolated environments** that prevent the agent from affecting production systems while still exercising realistic behavior. Isolation has two dimensions: **action isolation**, which ensures the agent cannot modify production state, and **data isolation**, which ensures the agent operates on test data, not production data. Without action isolation, a buggy agent during testing could delete resources, send emails to real users, or initiate financial transactions. Without data isolation, tests leak production data into logs, pollute production datasets with test artifacts, or violate privacy and compliance policies by exposing sensitive data in test outputs.

You achieve action isolation by running agents in **sandbox environments**: restricted execution contexts where the agent can call tools and interact with services, but all side effects are contained within the sandbox and discarded after the test completes. For agents that interact with databases, you use isolated test databases or transactions with rollback. For agents that call external APIs, you use mock services or sandbox API accounts provided by the vendor. For agents that modify files or cloud resources, you use temporary directories, isolated namespaces, or test-specific accounts with permissions scoped to non-production resources. The goal is to let the agent execute its full logic—reasoning, tool selection, state management—while ensuring that nothing it does persists beyond the test or affects systems outside the test boundary.

You achieve data isolation by using **synthetic or anonymized test datasets** instead of production data. If your agent handles customer support, you create a test dataset with fictional customer records, support tickets, and interaction histories. If your agent processes medical records, you use de-identified or synthetic patient data that mirrors production distributions but contains no real PII. If your agent retrieves information from a knowledge base, you populate a test knowledge base with curated documents that cover the scenarios you are testing. Data isolation not only protects privacy and compliance but also gives you control over test data: you can craft edge cases, inject ambiguous scenarios, and ensure coverage of rare conditions that may not appear in production samples.

The challenge is that isolation must not compromise **realism**. A sandbox that behaves differently from production—faster or slower, with different error modes, or with simplified logic—produces tests that pass in the sandbox but fail in production. Your test environment must replicate production behavior closely enough that failures in testing predict failures in production and successes in testing predict successes in production. This requires careful construction of mocks, simulations, and sandbox infrastructure.

## Sandbox Environments for Stateful Tools

Agents that modify state—databases, file systems, cloud resources, external services—require sandboxes that capture side effects without persisting them. The simplest sandbox is a **transaction with rollback**: you begin a database transaction before the test, let the agent execute queries and updates within the transaction, verify the results, and then roll back the transaction at the end of the test. The database returns to its initial state, and no changes persist. This approach works well for relational databases and any storage system that supports transactional semantics. It is fast, deterministic, and requires no additional infrastructure.

For tools that interact with APIs or services without transactional semantics, you use **ephemeral sandbox accounts or namespaces**. Many SaaS platforms provide sandbox or test environments where you can create resources, execute actions, and then delete everything without affecting production. For example, payment processors like Stripe provide test API keys that interact with a sandbox environment where you can create charges, refunds, and customer records that never touch real money or real customers. Cloud providers like AWS offer isolated accounts or resource tagging schemes that let you create and destroy resources programmatically. You configure your agent to use sandbox credentials during testing and production credentials during deployment, ensuring that test runs never touch production resources.

For services that do not provide sandboxes, you build **local or containerized replicas**. If your agent interacts with a PostgreSQL database, you run a PostgreSQL container with test data for each test run. If your agent calls a proprietary API, you build a mock server that implements the same endpoints and returns predefined responses. These replicas do not need to implement every feature of the production service—they only need to replicate the behaviors your agent depends on. A mock API for an email service does not need to actually send emails; it only needs to accept email payloads, validate them, and return success or error responses based on test scenarios.

You also use **in-memory or temporary storage** for tools that write files or store artifacts. Instead of writing to a persistent file system, the agent writes to a temporary directory created at the start of the test and deleted at the end. Instead of uploading objects to a production S3 bucket, the agent uploads to a local file store or an in-memory mock. This approach is fast and eliminates the need for cleanup logic, but it requires that your tool implementations accept configurable storage backends so you can swap in the sandbox backend during testing.

The critical principle is that **sandbox behavior must match production behavior** for the aspects your agent depends on. If your production database enforces foreign key constraints, your test database must enforce them too. If your production API returns 429 rate limit errors under load, your mock API should simulate those errors in tests that exercise retry logic. If your production file system has latency and occasional failures, your sandbox should inject similar latency and failures to verify the agent handles them. A sandbox that is simpler, faster, or more reliable than production creates false confidence.

## Mock Services for External Dependencies

Agents often call external APIs—third-party services, internal microservices, data providers—that are expensive, slow, or unreliable to call during testing. You replace these dependencies with **mocks**: lightweight services that implement the same interface but return predefined responses based on test scenarios. Mocks eliminate external dependencies, reduce test latency, and give you control over responses, error conditions, and edge cases.

A mock service for an external API implements the same HTTP endpoints as the real API but responds with canned data. For example, if your agent calls a weather API to retrieve current conditions, your mock weather API responds with predefined JSON payloads for different cities and conditions. You configure the mock to return sunny weather for San Francisco, rainy weather for Seattle, and a 500 error for InvalidCity. Your tests specify which mock responses to use, and the agent interacts with the mock as if it were the real API. The mock does not call the real API, does not incur costs, and responds instantly, making tests fast and deterministic.

You build mocks in two ways. First, you use **mock server frameworks** like WireMock, MockServer, or Prism that let you define API specifications and responses in configuration files. You write OpenAPI or JSON schemas that describe the API endpoints, then specify request-response mappings: if the agent calls this endpoint with these parameters, return this response. The framework starts an HTTP server that implements the API, and you point your agent to the mock server during testing. This approach is fast to set up and works for any HTTP-based API. Second, you write **custom mock implementations** when you need more control over logic or state. A custom mock can maintain internal state, simulate multi-step workflows, or generate dynamic responses based on the sequence of requests. For example, a mock database API might track which records the agent has queried and return different responses on subsequent queries to simulate changes in the underlying data.

Mocks must handle **error conditions and edge cases** as carefully as they handle success cases. You configure mocks to return errors—timeouts, rate limits, malformed responses, missing data—and verify that the agent handles them correctly. You write tests that simulate API downtime by making the mock return 503 errors, tests that simulate rate limiting by returning 429 errors after a certain number of requests, and tests that simulate data inconsistencies by returning conflicting information on repeated calls. These error injection tests catch bugs in retry logic, fallback behavior, and error handling that would otherwise only surface in production when the real API fails.

You also version mocks alongside your agent code. When the external API changes—adds a field, deprecates an endpoint, changes error codes—you update the mock to match. You track the API version the mock implements and ensure it matches the version your production agent uses. If your agent supports multiple API versions, your mock supports them too, and your tests verify behavior across versions. This discipline ensures that your tests remain valid as external dependencies evolve.

## Simulations for Complex Multi-Agent or Multi-Turn Scenarios

Some agent behaviors only emerge in **complex scenarios**: multi-turn conversations with evolving context, multi-agent interactions where agents coordinate or conflict, or long-running workflows that span hours or days. Testing these scenarios with real tools and real time is impractical. You need **simulations**: controlled environments where you compress time, script interactions, and inject scenarios that would be rare or slow to occur naturally.

A multi-turn conversation simulation defines a sequence of user messages and expected agent responses, then runs the agent through the conversation, verifying that it maintains context correctly, updates state appropriately, and produces coherent responses at each turn. For example, a customer support simulation might script a conversation where the user asks about an order, then asks to modify the shipping address, then asks to cancel the order. The simulation verifies that the agent retrieves the order in turn one, updates the address in turn two, and processes the cancellation in turn three, without losing track of which order is being discussed or repeating actions already completed. These simulations catch **context drift**: cases where the agent forgets or misinterprets earlier turns, and **state inconsistency**: cases where the agent's internal state diverges from the external state it is managing.

A multi-agent simulation defines a scenario where multiple agents interact, either cooperatively or competitively, and verifies that they coordinate correctly. For example, in a supply chain system, one agent might manage inventory, another might manage orders, and a third might manage shipping. A simulation scripts a scenario where an order arrives, the order agent checks inventory, the inventory agent confirms availability, and the shipping agent schedules delivery. The simulation verifies that the agents communicate through the expected channels, pass the correct information, and reach the correct final state. These simulations catch **coordination failures**: cases where agents assume responsibilities overlap, where information is lost between agents, or where agents deadlock waiting for each other.

You also simulate **temporal scenarios**: workflows that unfold over time with delays, retries, and asynchronous events. For example, an agent that processes loan applications might need to wait for credit checks, employment verification, and manual review before approving or denying the loan. In production, this workflow could take days. In a simulation, you compress time by injecting events programmatically: you start the agent, trigger a credit check event immediately, trigger an employment verification event one second later, and trigger a manual review event two seconds later. The agent processes these events as if they arrived over days, but the simulation completes in seconds. You verify that the agent handles each event correctly, updates state appropriately, and reaches the correct final decision. These simulations catch **asynchronous race conditions**: cases where the agent assumes events arrive in a certain order, or where delayed events cause the agent to make decisions based on stale information.

Simulations also let you test **rare scenarios** that are hard to reproduce with real data. You script interactions that only occur once in a thousand production runs—unusual combinations of user input, tool failures, and edge cases—and verify the agent handles them. You craft adversarial scenarios designed to stress-test the agent's reasoning: ambiguous queries, contradictory tool outputs, missing information. These adversarial simulations are how you discover the boundaries of your agent's capabilities before users do.

## Test Data Generation and Synthetic Datasets

Realistic test data is the foundation of effective agent testing. Test data must **mirror production diversity**: the range of inputs, edge cases, and data distributions your agent will encounter in production. If your production users ask questions in 15 languages with varying formality, your test data must include questions in those languages and formality levels. If your production database has records with missing fields, duplicate entries, and inconsistent formatting, your test data must include those anomalies. Homogeneous, sanitized test data produces tests that pass in the lab but fail in the field.

You generate test data in three ways. First, you **sample and anonymize production data**. You take a random sample of production queries, documents, records, or interactions, strip PII, and use them as test cases. This ensures your test data reflects real user behavior and real data quality. You anonymize by replacing names, emails, phone numbers, and identifiers with synthetic values, masking sensitive fields, or using differential privacy techniques if aggregation is acceptable. You also ensure that anonymization preserves the statistical properties of the data—if production data has a certain distribution of query lengths, your anonymized data has the same distribution. Anonymization that introduces bias or removes edge cases defeats the purpose of using production data.

Second, you **synthesize adversarial and edge-case examples**. You write test cases designed to exercise rare conditions, boundary values, and failure modes. You craft queries that are ambiguous, self-contradictory, or unusually long. You create database records with missing required fields, duplicate keys, or circular references. You generate API responses with malformed JSON, unexpected data types, or values outside expected ranges. These adversarial examples are not representative of typical production data, but they test the agent's robustness and error handling in scenarios that will eventually occur even if they are rare.

Third, you **generate synthetic data programmatically**. You use templates, grammars, or generative models to create large volumes of test data that cover a defined space of variations. For example, you generate customer support queries by sampling templates like "I need help with my account" or "Can you explain the charges on my bill" and filling in variable slots with randomized values. You generate database records by defining schemas and sampling values from distributions that match production—names from census data, addresses from postal databases, timestamps from realistic activity patterns. You generate documents by using LLMs to write synthetic articles, FAQs, or product descriptions that cover topics your agent must handle. Synthetic data generation scales: you can produce thousands of test cases with a script, covering a combinatorial space of inputs that would be impractical to hand-write.

You also **version and manage test datasets** as code artifacts. You store test queries, documents, and records in your repository, version them alongside your agent code, and regenerate test databases and indexes from these datasets before each test run. This ensures test reproducibility: the same test run on the same code always uses the same data and produces the same results. When you discover a bug caused by a specific input, you add that input to your test dataset, ensuring the bug does not regress.

## Managing Costs and Latency in Test Environments

Integration and end-to-end tests that call LLMs and external APIs incur **cost and latency**. A single integration test might make five LLM calls and ten API calls, taking ten seconds and costing a few cents. Multiply by hundreds of tests, and your test suite costs dollars per run and takes tens of minutes to complete. This cost and latency compounds as your test suite grows, eventually slowing development velocity and straining budgets. You manage this by optimizing test execution, caching results, and selectively running expensive tests.

You optimize test execution by **parallelizing tests**. Most agent tests are independent—they do not share state or depend on the order of execution—so you can run them in parallel across multiple workers or containers. Parallelization reduces wall-clock time proportionally to the number of workers, making a suite that takes 30 minutes serially complete in five minutes with six workers. You use test frameworks that support parallel execution—pytest with xdist, Go's parallel test runner, or custom orchestration with CI/CD systems—and ensure your test infrastructure can handle the increased concurrency.

You cache results for **deterministic tests**. If a test calls an LLM with temperature zero and a fixed prompt, the result is deterministic, and you can cache the response keyed by the prompt and model. Subsequent runs of the test skip the LLM call and use the cached response, reducing cost and latency. You invalidate the cache when the prompt, model, or test logic changes, ensuring cached results remain valid. Caching works for unit tests and some integration tests but not for end-to-end tests that depend on external state or non-deterministic behavior.

You selectively run expensive tests using **test impact analysis and risk prioritization**. You run all unit tests on every commit because they are fast and cheap. You run integration tests only for code paths affected by the current change: if you modify a tool, you run integration tests that exercise that tool, but skip tests for unrelated workflows. You run end-to-end tests only on pull requests or scheduled nightly runs, not on every commit. You also tier tests by criticality: critical tests that cover high-risk workflows run on every deploy, while informational tests that cover edge cases run on a schedule. This selective execution reduces cost and latency while maintaining coverage for the scenarios that matter most.

You also use **smaller or faster models** for tests that do not require the full capabilities of your production model. If your production agent uses GPT-4o, but a test only validates tool selection logic, you can run the test with GPT-4o-mini or another smaller model that is faster and cheaper. The test may not validate the exact reasoning quality of the production model, but it validates the structural behavior—tool calls, state updates, error handling—at a fraction of the cost. You reserve full-model tests for critical paths and use smaller models for regression tests and low-risk scenarios.

## Continuous Integration and Test Automation

Your test environments integrate with your **CI/CD pipeline**, running automatically on every commit, pull request, and deployment. You configure your pipeline to spin up sandbox environments, populate test databases, start mock services, run the test suite, and tear down the environment after tests complete. This automation ensures tests run consistently, catches regressions immediately, and eliminates manual testing bottlenecks.

Your pipeline stages tests by **cost and criticality**. Fast, cheap unit tests run on every commit and block merging if they fail. Integration tests run on every pull request and block deployment if they fail. End-to-end tests run before production deployment and trigger rollback if they fail. Informational tests run on a schedule and alert on failure but do not block deployment. This staged approach balances speed and thoroughness: developers get fast feedback during development, and high-confidence validation gates production deployment.

You also implement **test result tracking and trend analysis**. You log test outcomes, execution times, and failure rates, and track them over time. You alert when tests become flaky, when execution time increases significantly, or when failure rates spike. You use these signals to prioritize test maintenance: fixing flaky tests, optimizing slow tests, and investigating failure trends. Test result tracking transforms your test suite from a pass-fail gate into a source of insights about agent reliability and quality.

Your test environments are also used for **pre-deployment validation**. Before deploying a new agent version, you run your full test suite in a staging environment that mirrors production as closely as possible—same models, same tools, same data distributions—and verify that all critical tests pass. You also run a subset of tests in a **canary deployment**: you deploy the new version to a small percentage of production traffic, monitor test outcomes and production metrics, and roll back if tests fail or metrics degrade. This gradual rollout reduces risk and catches issues that only appear under production load or with production data distributions.

## Debugging Failed Tests and Root Cause Analysis

When a test fails, you need to **diagnose the root cause** quickly. Agent test failures are harder to debug than traditional test failures because the failure could originate in the model's reasoning, tool logic, state management, prompt construction, or external dependencies. You instrument your test environments with detailed logging, tracing, and observability so that when a test fails, you can replay the failure, inspect intermediate states, and identify where the agent's behavior diverged from expectations.

You log every tool call, model generation, and state update during test execution. You capture the full prompt sent to the model, the raw model response, the parsed tool calls, the tool outputs, and the updated state after each step. You store these logs per test run and make them easily accessible: in CI/CD logs, in a test result dashboard, or in a dedicated debugging interface. When a test fails, you inspect the logs to see exactly what the agent did, identify the step where it diverged from the expected trajectory, and determine whether the failure was due to incorrect reasoning, a tool error, a prompt issue, or a test data problem.

You also use **assertion diffs and comparison tools**. When an assertion fails, your test framework shows the expected value and the actual value side-by-side. For trajectory assertions, you show the expected sequence of tool calls and the actual sequence, highlighting where they diverged. For semantic assertions, you show the expected information and the actual output, highlighting what was missing or incorrect. These diffs make it immediately clear what went wrong, reducing the time spent manually inspecting logs.

You maintain a **knowledge base of failure patterns**: common failure modes, their root causes, and how to fix them. When a test fails, you check the knowledge base to see if the failure matches a known pattern. If it does, you apply the known fix. If it does not, you debug the failure, document the root cause, and add it to the knowledge base for future reference. Over time, this knowledge base becomes a valuable resource for both debugging and agent improvement.

Test environments are not just infrastructure—they are the foundation of reliable agent development. Sandboxes isolate agent actions, mocks simulate external dependencies, and simulations replicate complex scenarios in controlled settings. Together, they enable rigorous testing without production risk, fast iteration without infrastructure bottlenecks, and high confidence that agents work correctly before deployment. The next step is instrumenting agents with observability—logging, tracing, and monitoring that make agent behavior visible in production and enable rapid debugging when issues arise.
