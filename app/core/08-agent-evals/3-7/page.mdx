# 3.7 — Extended Thinking in Agents: Chain-of-Thought for Complex Decisions

In September 2025, an AI research team at a financial services company discovered a pattern in their loan underwriting agent that changed how they thought about decision quality. The agent was approving loans at a 94 percent accuracy rate compared to human underwriters, which seemed excellent until they analyzed the 6 percent of cases where the agent and humans disagreed. These disagreements were not random—they clustered in a specific category of applications where multiple risk factors were present but none were individually disqualifying. A self-employed applicant with fluctuating income, good credit history, but recent job change. A salaried applicant with excellent income, mediocre credit, but strong collateral. The agent was making snap judgments, essentially pattern-matching to the most similar historical cases and copying their outcomes. When the team implemented extended thinking—giving the agent explicit instructions and token budget to reason through each risk factor, how they interact, what mitigations exist, and what the appropriate risk-adjusted decision should be—accuracy on these complex cases jumped from 86 percent to 97 percent. The agent was finally thinking hard enough about decisions that deserved hard thinking.

Extended thinking is not just chain-of-thought—it is chain-of-thought taken to its logical conclusion. Standard chain-of-thought prompting gets your model to show its reasoning steps: "First I will do X, then Y, then Z." Extended thinking gives your model the space and instruction to reason deeply about the problem before committing to a solution: "Let me consider what X means in this context, what alternatives to X exist, what would happen if X fails, whether X is actually the right approach, what assumptions I am making about X." The difference is not just length—it is depth. Standard CoT walks through a predetermined reasoning path. Extended thinking explores the problem space to find the right path.

## Standard Chain-of-Thought vs Extended Thinking

Standard chain-of-thought is procedural. You prompt your model to explain its reasoning step-by-step, and it produces a linear sequence of thoughts leading to an answer. "To solve this problem, I will first identify the customer's issue, then check their account status, then determine the appropriate resolution, then draft a response." This reasoning makes the model's decision process transparent and often improves accuracy by preventing the model from jumping to conclusions. But the reasoning is shallow—the model is narrating what it is doing, not deeply deliberating about what it should do.

Extended thinking is deliberative. You prompt your model to actually think about the problem from multiple angles before deciding what to do. "What are the possible interpretations of what the customer is asking? What information would I need to distinguish between these interpretations? What are the risks of acting on the wrong interpretation? What would a cautious response look like versus a confident response? What similar cases have I seen and how are they different from this one?" The model is not executing a predetermined procedure—it is working through the ambiguity and complexity of the problem to construct an appropriate response strategy.

The token cost difference is substantial. Standard chain-of-thought might add 100 to 300 tokens of reasoning overhead to a task that would otherwise use 50 tokens. Extended thinking might add 1000 to 5000 tokens or more. You are not just asking the model to narrate its steps—you are asking it to explore the problem space, consider alternatives, evaluate tradeoffs, and justify its conclusions. That exploration takes tokens.

The latency difference is equally significant. Generating 3000 tokens of extended thinking takes 3 to 12 seconds depending on your model and API tier. If your agent makes 10 decisions per user interaction and uses extended thinking for each, you are adding 30 to 120 seconds of latency to the interaction. This is rarely acceptable for real-time applications, which is why extended thinking needs to be applied selectively.

The decision quality difference is where extended thinking earns its cost. For simple, well-defined problems, standard CoT and extended thinking produce similar results. The problem does not have enough complexity to benefit from deeper deliberation. For complex, ambiguous, or high-stakes problems, extended thinking produces meaningfully better decisions. The model catches edge cases it would have missed with shallow reasoning. It recognizes when the obvious answer is wrong. It constructs nuanced responses that address the actual complexity of the situation rather than forcing it into a simple template.

## When Extended Thinking Improves Agent Decisions

Extended thinking shines in situations where the problem has multiple plausible interpretations and choosing the right one requires careful analysis. A user says "I need to cancel my subscription but keep my data." Does this mean they want to downgrade to a free tier, or export their data then fully cancel, or pause their subscription, or something else? Standard CoT might pick the most common interpretation and proceed. Extended thinking would reason through what each interpretation implies, what information would disambiguate them, and what clarifying questions would help. The deeper reasoning prevents the agent from confidently acting on the wrong interpretation.

Multi-factor decisions where factors interact in non-obvious ways demand extended thinking. Should the agent approve this refund request? The purchase was 35 days ago (outside the 30-day window), but the customer reported the issue at 28 days (inside the window). The customer has a history of refund requests (potential abuse signal), but all previous requests were legitimate (not actually abusive). The product category has a high return rate (common issue), but this specific complaint is unusual (might indicate a real defect). Standard CoT would likely process these factors sequentially and weight them equally. Extended thinking would reason about how these factors interact—the timing ambiguity, the pattern interpretation, the signal-to-noise question—and construct a nuanced judgment.

Situations with conflicting constraints or goals require extended thinking to find acceptable compromises. Your agent needs to schedule a meeting between three people with conflicting availability, different timezone constraints, and different preferences for meeting length and format. No option satisfies all constraints. Standard CoT would likely apply a simple heuristic—majority vote, most senior person's preference, earliest available time. Extended thinking would reason through what constraints are hard versus soft, what compromises each person would likely find acceptable, what alternatives exist to a synchronous meeting, and what recommendation balances the competing needs.

Novel situations where your agent has not seen this exact pattern before benefit enormously from extended thinking. If your customer support agent encounters a request that does not fit any known category, shallow reasoning will fail. The agent needs to reason from first principles about what the customer wants, what constraints apply, what options exist, and what response would be helpful. Extended thinking gives the agent space to construct this reasoning rather than forcing it to pattern-match to an inadequate template.

High-stakes decisions justify extended thinking even when the problem is not particularly complex. If your agent is about to take an action with significant consequences—large financial transaction, permanent data deletion, security policy change, compliance reporting—the stakes warrant careful deliberation. Extended thinking here is not about resolving ambiguity—it is about ensuring the agent has considered all relevant factors and potential consequences before acting. The cost of a bad decision is high enough to justify the cost of thorough thinking.

Adversarial or safety-critical contexts demand extended thinking. If your agent is evaluating whether a user's request might be an attempt to manipulate the agent into violating policies, shallow reasoning will miss sophisticated attacks. Extended thinking gives the agent space to consider whether the request has hidden implications, whether it is trying to exploit ambiguities in policies, whether approving it would set a bad precedent, and whether there are safer alternatives. The adversarial nature of the context means the obvious answer might be the wrong answer.

## The Token Cost of Extended Thinking

Extended thinking is expensive in ways that matter to production systems. The most direct cost is API billing. If your agent uses extended thinking to generate 3000 tokens of internal reasoning per decision, and your model charges $15 per million output tokens, each decision costs $0.045 in reasoning tokens alone. Multiply that by 100,000 decisions per day and you are spending $4,500 per day on reasoning. For many applications, this cost is prohibitive unless the decision quality improvement directly translates to revenue or cost savings.

The context window cost is less obvious but equally important. Extended thinking consumes context space. If your agent has a 128,000 token context limit and generates 50,000 tokens of extended thinking across multiple decision points, it has consumed 39 percent of available context. That space could have held more conversation history, more documentation, more examples, or more tool outputs. Spending it on reasoning means you have less space for these other inputs. At some point, you start having to truncate useful context to make room for reasoning, which can degrade performance in different ways.

The latency cost affects user experience. Users waiting for your agent to respond do not see the extended thinking happening—they just see a loading indicator. If extended thinking adds 8 seconds to response time, and users start abandoning interactions after 5 seconds of waiting, the thinking quality is irrelevant because users are not staying to see the result. The latency cost is not just a technical metric—it is a conversion and satisfaction metric.

The computational cost is infrastructure spending. Generating tokens requires GPU time. Extended thinking means each user interaction consumes more GPU cycles. If your infrastructure is already running near capacity, extended thinking will push you over, requiring additional hardware or cloud spending. The cost is not just the per-token API charge—it is the underlying compute that makes those tokens possible.

The monitoring and debugging cost scales with reasoning length. When your agent makes a decision, you want to understand why. If the decision came from 100 tokens of standard CoT, reading and understanding that reasoning is quick. If the decision came from 3000 tokens of extended thinking, reviewing it takes substantially longer. Debugging failures, auditing decisions, and improving prompts all get harder as reasoning length increases. This operational cost persists over time.

## Selective Application of Extended Thinking

The key to making extended thinking viable in production is applying it selectively—reserving it for the critical decision points where it actually improves outcomes. Most agent interactions have a few critical decisions and many routine decisions. The critical decisions deserve extended thinking. The routine decisions do not. Your job is to identify which is which and route accordingly.

One pattern is to use extended thinking only at plan formulation. When your agent first receives a user request, it uses extended thinking to deeply understand what is being asked, what the user's actual goal is, what constraints apply, and what approach would work. This planning phase might consume 2000 tokens of extended thinking. Once the plan is formed, execution of individual steps uses standard reasoning or even shallow pattern matching. The extended thinking happens once at the start rather than repeatedly at every step.

Another pattern is to trigger extended thinking based on confidence. Your agent attempts to solve the problem with standard reasoning. If the confidence score on that reasoning is high, it proceeds. If the confidence score is low—indicating ambiguity, novelty, or complexity—the agent shifts to extended thinking mode. This way, extended thinking only fires when standard reasoning is struggling. The majority of straightforward cases never trigger it.

You can also trigger extended thinking based on stakes. Before taking any action, your agent evaluates the stakes of that action using a quick heuristic. Low-stakes actions—sending a confirmation message, logging an event, updating a timestamp—proceed with standard reasoning. High-stakes actions—approving a large transaction, deleting data, changing security settings—trigger extended thinking. The stakes threshold determines how much of your decision space gets the extended thinking treatment.

Task complexity can be a trigger. Your agent uses simple heuristics to estimate task complexity—number of entities involved, number of constraints mentioned, number of conditional statements in the user's request, novelty of the vocabulary used. Simple tasks proceed with standard reasoning. Complex tasks trigger extended thinking. This requires your agent to do a quick complexity assessment, but that assessment can be very lightweight compared to full extended thinking.

Error recovery is a natural place for extended thinking. If your agent takes an action and receives feedback that the action was wrong or unhelpful, the next attempt should use extended thinking. The error is evidence that standard reasoning was insufficient. Rather than retrying with the same reasoning depth, the agent escalates to deeper thinking that considers why the first attempt failed and what alternative approach would work.

Some teams implement extended thinking as a second-pass verification. The agent uses standard reasoning to generate a proposed action. Before executing it, the agent uses extended thinking to verify that the action is appropriate—does it achieve the user's goal, does it violate any constraints, are there unintended consequences, is there a better alternative? This verification step catches errors before they become visible to users. The cost is lower than using extended thinking for both planning and verification, but higher than using standard reasoning alone.

## The Relationship Between Thinking Budget and Decision Quality

Decision quality does not scale linearly with thinking budget. The first 500 tokens of extended thinking often produce large gains—the agent goes from pattern matching to actually reasoning about the problem. The next 500 tokens produce smaller gains—the agent is now considering edge cases and alternative approaches. Beyond 2000 tokens, marginal returns diminish sharply for most problems. The agent is exploring increasingly unlikely scenarios and fine-tuning already-good decisions.

Your thinking budget should match the actual complexity of the problem. For genuinely complex problems with many interacting factors, a 3000-token thinking budget might be appropriate. The problem has enough depth that extended reasoning continues to add value throughout that range. For moderately complex problems, a 1000-token budget is often sufficient to capture the key considerations without wandering into unproductive speculation. For simple problems masquerading as complex ones, even 500 tokens of extended thinking is wasteful.

The relationship between thinking budget and decision quality is also task-dependent. Some tasks benefit from broad exploration of the problem space—considering many possible interpretations and approaches. These tasks reward longer thinking budgets. Other tasks benefit from deep analysis of a single interpretation—really understanding the implications and consequences of one approach. These tasks reward focused thinking more than extensive thinking. Your budget should align with what kind of thinking the task demands.

You can measure the thinking budget vs quality relationship empirically. Run your agent on a sample of tasks with different thinking budgets—500, 1000, 2000, 4000 tokens. Measure decision quality at each level. Plot the curve. For most task categories, you will find an elbow point where additional thinking stops meaningfully improving quality. That elbow point is your optimal budget for that category. Below it, you are underthinking. Above it, you are wasting tokens.

Some models respond better to thinking budgets than others. Models with strong reasoning capabilities can use extended thinking budgets effectively—the additional tokens translate to better reasoning. Models with weaker reasoning capabilities often do not—they fill the token budget with repetitive or unproductive thoughts. If you are using a less capable model, a smaller thinking budget might actually produce better results by forcing the model to focus on the most important considerations.

## Production Patterns for Managing Extended Thinking

The most common production pattern is two-tier reasoning: standard reasoning by default, extended thinking on exception. Your agent uses standard chain-of-thought for every decision. When it encounters a trigger—low confidence, high stakes, high complexity, novel situation—it switches to extended thinking mode for that specific decision. This minimizes extended thinking usage while ensuring it is available when needed. Implementation requires clear triggers and smooth mode switching.

Another pattern is separate planning and execution models. Your planning model uses extended thinking to formulate a plan from the user's request. Your execution model uses standard reasoning to carry out plan steps. The planning model might be a more capable, expensive model because planning happens once per interaction. The execution model might be a faster, cheaper model because execution happens many times per interaction. This separation lets you use extended thinking where it matters most without paying for it at every step.

Some teams implement thinking budgets as hard constraints. The agent is told it has a budget of 1000 tokens for thinking and must stay within it. This prevents runaway reasoning and makes costs predictable. The constraint forces the agent to prioritize—what are the most important things to think about within this budget? The downside is that genuinely complex problems might need more budget, and the hard constraint prevents the agent from using it.

Others implement thinking budgets as soft guidance. The agent is told to think thoroughly but concisely, with examples showing what appropriate thinking depth looks like for different problem types. This gives the agent flexibility to use more thinking for complex problems and less for simple ones. The downside is less predictable costs and the risk that the agent over-thinks simple problems.

Progressive thinking is another pattern. The agent starts with a small thinking budget—maybe 300 tokens—to form an initial understanding of the problem. Based on that initial thinking, it decides whether to invest more tokens in deeper thinking. If the initial thinking reveals complexity, ambiguity, or high stakes, the agent proceeds to a second round with a larger budget. If the initial thinking indicates a straightforward problem, the agent proceeds directly to action. This progressive approach invests more thinking only when initial thinking justifies it.

Some production systems implement extended thinking as a background process. The agent responds to the user with standard reasoning, providing a quick initial response. In parallel, it runs extended thinking to verify or refine that response. If the extended thinking reveals that the initial response was wrong, the agent sends a correction. This pattern minimizes user-facing latency while still getting the benefits of extended thinking. The tradeoff is that users occasionally see corrections, which can be confusing.

## Model-Specific Considerations

Different models have different capabilities and costs for extended thinking. Claude models as of 2026 are strong at sustained reasoning and can effectively use thinking budgets of 2000 to 4000 tokens when problems warrant it. The reasoning tends to be well-structured and focused. GPT-4 class models also support extended thinking well, though reasoning style differs—more exploratory, sometimes more verbose. Smaller or less capable models often struggle to use large thinking budgets productively—they repeat themselves or wander off-topic.

Some models have specific modes or parameters for extended thinking. Claude 3.5 Sonnet introduced an extended thinking mode that explicitly encourages deeper reasoning. GPT-4 has temperature and top-p parameters that affect reasoning style—lower temperature produces more focused reasoning, higher temperature more exploratory reasoning. Understanding your model's specific capabilities and how to elicit extended thinking is important for effective implementation.

Token costs vary dramatically by model. Extended thinking with GPT-4 Turbo at $10 per million output tokens costs half as much as extended thinking with Claude Opus at $20 per million output tokens for the same token count. But if Claude produces better reasoning in 2000 tokens than GPT-4 produces in 4000 tokens, Claude might be cheaper on a per-decision basis despite higher per-token costs. Cost optimization requires considering both token efficiency and per-token pricing.

Context window sizes affect how much extended thinking you can afford. Models with 128k context windows can accommodate more extended thinking than models with 32k windows. If your agent needs to maintain long conversation history and extensive documentation in context, a smaller context window forces you to choose between extended thinking and other context usage. Larger context windows give you more flexibility.

Some models expose thinking tokens separately from response tokens. This is valuable for extended thinking because it lets you hide the thinking from users while still benefiting from it. The model generates 2000 tokens of thinking, then generates a concise 100-token response. The user sees only the response, but the response is informed by the thinking. If your model does not separate thinking and response, you need to do that separation yourself—prompting the model to output thinking in a tagged section that you strip before showing the user.

Latency characteristics differ across models and providers. Some providers offer lower-latency tiers that are crucial for real-time extended thinking. Others have consistent latency but it is higher. If extended thinking adds 8 seconds with one provider and 3 seconds with another, that latency difference might matter more than token cost for your application. Evaluate both cost and latency when choosing models for extended thinking.

## When Extended Thinking Backfires

Extended thinking can hurt performance in specific situations. For time-sensitive tasks, the latency is unacceptable. If your agent is monitoring a production system and needs to respond to an alert within 1 second, spending 8 seconds on extended thinking about whether the alert is real means the system has already failed. Fast action with occasional false positives is better than slow, perfect action that arrives too late.

For simple tasks, extended thinking introduces errors by overthinking. A user asks what time the store closes. The correct answer is to look up the hours and state them. Extended thinking might reason about whether the user is asking about today or generally, whether there are exceptions for holidays, whether the user actually wants to visit or just wants to know for future reference. This overthinking complicates a simple task and can lead to a response that is less helpful than a straightforward answer.

Extended thinking can degrade performance when the model uses the thinking budget unproductively. Some models, when given a large thinking budget, will fill it with repetitive reasoning, circular logic, or tangential considerations. The extended thinking does not actually improve the decision—it just consumes tokens and time. If your model does this, shorter thinking budgets produce better results by forcing the model to focus.

In adversarial contexts, extended thinking can create vulnerability. If users can see the agent's extended thinking, they can use it to understand how the agent makes decisions and craft attacks that exploit that reasoning process. Even if users cannot see the thinking, the latency is a signal about how hard the agent is thinking, which can be information an adversary uses. For security-sensitive applications, the transparency of extended thinking might be a liability.

Extended thinking can also backfire when it is applied inconsistently. If some user requests trigger extended thinking and others do not, and users cannot predict which, the inconsistent latency creates a poor user experience. Users learn they cannot rely on fast responses and become frustrated with unpredictable wait times. Consistency—either always fast or always thoughtful—is sometimes preferable to optimal-but-inconsistent depth.

## The Strategic Choice to Think Deeply

Extended thinking is not a feature you add to every agent interaction. It is a strategic choice about where to invest cognitive resources for maximum impact. The best agents in 2026 think deeply when deep thinking matters and act quickly when speed matters. The distinction is not capability—it is judgment about what each situation deserves.

Your implementation should make extended thinking an option, not a default. Build the capability to invoke extended thinking when needed. Design triggers that identify when it is needed. Measure whether it actually improves outcomes in those situations. Tune the thinking budget to match task complexity. Monitor costs and latency. Iterate based on real performance data.

Extended thinking is powerful because it lets your agent engage with genuinely complex problems in a way that shallow reasoning cannot. But that power has a cost—tokens, time, context space, operational complexity. The engineering challenge is harnessing the power while controlling the cost. When you get it right, your agent thinks exactly as hard as each problem deserves, producing better decisions without wasting resources on overthinking simple cases.

