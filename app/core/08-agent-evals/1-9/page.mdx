# 1.9 — The Cost and Latency Reality of Agentic Systems

Multi-step agentic systems cost twenty to forty times more than teams initially estimate. That multiplier destroys unit economics, blows through budgets, and kills products that would otherwise be viable. In June 2025, an enterprise software company launched an AI coding assistant designed to help developers write, test, and deploy code changes. The product was technically impressive: the agent could read codebases, generate functions, write tests, run those tests, interpret failures, fix bugs, and commit changes. In internal demos, it completed tasks in 15 to 20 steps that would have taken a human developer an hour or more. The executive team projected that each agent-assisted task would cost approximately 50 cents in API fees, based on back-of-the-envelope estimates of token usage.

Three months after public launch, the finance team flagged an alarming trend. The average cost per completed task was not 50 cents. It was 12 dollars. Some tasks cost over 40 dollars. The culprit was not a bug or runaway loops—though those happened too. The culprit was the fundamental economics of multi-step agentic systems. Each reasoning step consumed tokens for input context, chain-of-thought reasoning, and generated output. Each tool call consumed tokens to format the request and parse the response. Reflection and self-critique steps doubled token usage. By the time a task was complete, the agent had made 30 to 50 model calls, consumed 400,000 to 800,000 tokens, and blown past the original cost estimates by 20x.

The company's pricing model assumed 50 cents per task. At 12 dollars per task, the unit economics were underwater. They had to choose between raising prices—which would kill adoption—or absorbing the loss—which would kill the company. They chose a third option: they radically simplified the agent. They removed reflection loops, limited tool calls to a maximum of ten, reduced context size, and made the agent less autonomous. The quality of outputs declined, but costs dropped to 3 dollars per task, which was barely sustainable. User satisfaction fell. Churn increased. The product never recovered.

This story is not unique. It is the default outcome for teams that build agentic systems without understanding the cost structure. Agents consume 10x to 100x more tokens than single-turn calls, and latency stacks multiplicatively, not additively. The economics are brutal, and most teams discover this too late.

## Why Agents Cost 10x to 100x More Than Single Calls

A single LLM call is simple to cost out. You send a prompt, the model generates a response, you pay for input tokens and output tokens. If your prompt is 1,000 tokens and the response is 500 tokens, you pay for 1,500 tokens total. At current API pricing (as of early 2026), that is roughly 0.002 dollars for GPT-4 class models—two-tenths of a cent. Cheap enough that cost is rarely a concern for low-to-moderate volume applications.

An agent doing the same task might make 15 model calls. Each call includes the full conversation history (which grows with every step), the task description, the system prompt, and any tool outputs from previous steps. By step 10, the input context might be 15,000 tokens. Each reasoning step generates 800 tokens of output. Each tool call adds another 1,200 tokens for formatting the call and parsing the result. Over 15 steps, the agent consumes 300,000 tokens. At the same pricing, that is 0.40 dollars—200x more expensive than the single call.

This is not an exaggerated scenario. It is typical for moderately complex agent tasks. The token multiplication comes from several sources:

**Context growth.** Every step adds to the conversation history. The agent's reasoning, tool calls, and tool outputs all become part of the context for the next step. If the agent runs for 20 steps, the final input context includes everything from all 20 steps. A task that starts with a 2,000-token prompt might end with a 40,000-token prompt.

**Reasoning overhead.** Agents often use chain-of-thought reasoning, reflection, or self-critique. These techniques improve output quality but double or triple token usage. A simple response might be 200 tokens; a response with chain-of-thought reasoning might be 600 tokens. A response with reasoning, self-critique, and revision might be 1,500 tokens.

**Tool call formatting.** Tool calls are not free. The agent generates a structured request (often JSON), which consumes tokens. The tool returns structured output, which also consumes tokens. If the tool returns a 5,000-token document, that entire document goes into the next reasoning step. A single tool call can add 10,000 tokens to the conversation.

**Retries and error handling.** When a tool call fails, the agent retries. When the agent realizes it made a mistake, it backtracks and tries a different approach. When the task is ambiguous, the agent explores multiple paths before committing. All of this exploration consumes tokens. In the worst cases, the agent loops—trying the same thing repeatedly—and costs spiral.

**Multi-model architectures.** Some agents use multiple models: a fast model for planning, a powerful model for reasoning, a specialized model for code generation. Each model call adds cost. The sum can be much higher than using a single model for everything.

The result: what looks like a 50-cent task in a spreadsheet becomes a 5-dollar task in production, and that is if everything goes well. If the task is complex, if the agent struggles, or if the agent loops, costs can easily hit 20 to 50 dollars per task.

## Latency Stacking: Why Agents Are Slow

Cost is one problem. Latency is another, and it is often worse.

A single LLM call with a modern API typically takes 1 to 3 seconds for a response of a few hundred tokens. Fast enough for interactive use. If you are building a chatbot, users tolerate 2-second response times.

An agent making 15 sequential calls takes 15 to 45 seconds, assuming each call is 1 to 3 seconds. But that is the best case. In practice, latency is worse because:

**Tool calls add latency.** If the agent calls a search API, that API might take 2 seconds to return results. If it calls a database, the query might take 1 second. If it calls an external service, network latency might add 3 seconds. These tool latencies are on top of model latency, not instead of it. A single step might be: 2 seconds for the model to decide which tool to call, 3 seconds for the tool to execute, 2 seconds for the model to process the result. That is 7 seconds for one step.

**Sequential dependencies.** Agents are inherently sequential. The agent cannot make the second tool call until it has processed the result of the first. It cannot decide whether to continue until it has reflected on progress so far. Even if you parallelize where possible—calling multiple independent tools at once—the critical path is still long.

**Reflection and planning add steps.** An agent that pauses to reflect on its progress, critique its output, or plan the next few steps adds extra model calls. Each reflection step adds 2 to 4 seconds. If the agent reflects after every third action, you are adding 15 to 30 percent more latency.

**Retries and error recovery.** When something goes wrong, the agent retries. Retries add latency proportional to the number of failed attempts. If the agent tries a tool call three times before succeeding, that step takes 3x as long.

The result: a task that could hypothetically be solved in 5 seconds with a single call takes 30 to 90 seconds with an agent. For some use cases, this is fine. Research tasks, report generation, and background workflows can tolerate high latency. For interactive use cases—customer support, real-time assistance, live coding help—90-second response times are unusable. Users abandon the tool.

## Real Cost Breakdowns from Production Systems

Let's look at real numbers from production agent systems, anonymized but representative.

**Research agent (legal case analysis).** Task: analyze a legal case, find relevant precedents, and summarize findings. Average task: 25 model calls, 450,000 tokens consumed, 8 dollars per task. Latency: 45 to 90 seconds depending on number of documents retrieved. Business model: charges 50 dollars per report, so the margin is healthy. But the company originally projected 2 dollars per task and priced at 20 dollars per report. They had to raise prices, which reduced demand.

**Customer support agent (troubleshooting).** Task: diagnose a customer issue, search knowledge base, and suggest a solution. Average task: 8 model calls, 60,000 tokens consumed, 0.80 dollars per task. Latency: 12 to 20 seconds. Business model: saves human support time, so even at 0.80 dollars the ROI is positive. But the company originally assumed 0.10 dollars per task and nearly canceled the project when they discovered true costs were 8x higher.

**Code generation agent (write and test function).** Task: write a function, generate tests, run tests, fix bugs if tests fail. Average task when successful: 18 model calls, 280,000 tokens, 5 dollars per task. Average task when unsuccessful (tests fail repeatedly): 40 model calls, 600,000 tokens, 13 dollars per task. Latency: 25 to 60 seconds. Business model: sells as part of enterprise IDE subscription. Cost per task is hidden in subscription pricing, but the company loses money on power users who run hundreds of tasks per day.

**E-commerce workflow agent (process return).** Task: validate return request, check policy, issue refund or generate return label. Average task: 5 model calls, 35,000 tokens, 0.25 dollars per task. Latency: 6 to 10 seconds. Business model: saves customer service time, and the value of automation is much higher than 0.25 dollars. This is one of the few cases where agent economics work well, because the task is simple and bounded.

The pattern: agents work economically when the task is high-value and low-volume, or when the task is simple and bounded. Agents struggle when the task is complex and high-volume, because costs scale linearly with volume and the margin per task is low.

## The Relationship Between Agent Quality and Cost

There is a direct trade-off between agent quality and cost. The better the agent, the more expensive it is to run.

**Reasoning quality.** More sophisticated reasoning—chain-of-thought, reflection, self-critique—improves output quality but increases token usage by 2x to 5x. An agent that generates a quick answer in 200 tokens is cheap. An agent that reasons step-by-step, critiques its reasoning, and revises the answer consumes 1,000 tokens. The second agent is better, but it costs 5x more.

**Tool coverage.** The more tools an agent has access to, the more it costs to run. Each tool call adds latency and tokens. An agent with 20 tools might try several before finding the right one. An agent with 5 tools is faster and cheaper, but less capable.

**Context size.** Keeping more context—full conversation history, all tool outputs, detailed task state—improves agent reliability but increases token usage. Agents that aggressively summarize or truncate context are cheaper but make more mistakes because they forget important details.

**Error recovery.** Robust error handling—retrying failed tool calls, validating outputs, backtracking when things go wrong—makes agents more reliable but increases cost. Every retry is another model call. Agents that fail fast are cheaper but less useful.

**Iteration limits.** Allowing the agent to iterate until it succeeds produces better results but unbounded costs. Setting a strict limit on iterations (e.g., "stop after 10 steps") controls cost but means some tasks fail.

The fundamental tension: users want high-quality, reliable agents. High-quality agents are expensive. Businesses need unit economics that work. Controlling cost often means degrading quality. This is why so many agent products struggle.

## Strategies for Cost Management Without Sacrificing Quality

The best teams manage costs without destroying agent quality. Here is how they do it.

**Tier the agent by task complexity.** Not every task needs the full agent. Simple tasks use a lightweight version with fewer tools and less reasoning. Complex tasks use the full agent. This requires upfront classification—"Is this task simple or complex?"—but the savings are significant. If 60 percent of tasks are simple, you save money on those and spend more on the 40 percent that need it.

**Use smaller models for planning and routing.** The expensive GPT-4 class models are often overkill for deciding which tool to call or whether to continue. Use a smaller, faster, cheaper model for these steps, and reserve the powerful model for the hard reasoning. This can cut costs by 30 to 50 percent with minimal quality loss.

**Summarize aggressively.** After a few steps, summarize the conversation history and tool outputs into a compact representation. This keeps context size manageable and prevents runaway token growth. The risk is losing important details, so the summarization must be high-quality. Some teams use a small model to summarize and a large model for reasoning.

**Cache common context.** If many tasks share the same instructions, examples, or knowledge, cache that context at the API level. Some model providers support prompt caching, which reduces the cost of repeated context. This is especially valuable for agents with long system prompts or large knowledge bases.

**Set hard limits on iterations and tools.** Do not let the agent run indefinitely. Set a maximum number of steps (e.g., 15), a maximum number of tool calls (e.g., 10), and a timeout (e.g., 60 seconds). When limits are hit, the agent stops and escalates to a human. This prevents cost explosions from runaway tasks.

**Monitor cost per task and flag outliers.** Track cost at the task level, not just aggregate API spend. Identify tasks that cost 10x or 100x more than average and investigate why. Often, these outliers reveal bugs—loops, inefficient tool usage, unnecessary retries—that can be fixed to bring costs down.

**Optimize tool calls.** Tool calls are expensive because they add tokens and latency. Reduce tool call overhead by batching (call multiple tools in one step), caching tool results (do not re-fetch the same data), and designing tools to return concise outputs (do not return 10,000-token documents when a 500-token summary suffices).

**Fail fast on low-confidence tasks.** If the agent is unsure it can complete the task, have it escalate immediately rather than trying for 20 steps and failing anyway. This requires the agent to estimate its own likelihood of success, which is hard, but even a rough heuristic (e.g., "if I have not made progress after 5 steps, escalate") can save costs.

None of these strategies are free. They require engineering effort, monitoring infrastructure, and careful tuning. But they are necessary if you want agents to be economically viable.

## Latency Budgets for Different Use Cases

Different use cases have different latency tolerance. You need to design your agent to fit the budget.

**Interactive chat (under 5 seconds).** If the agent must respond to user messages in real-time, you have almost no room for multi-step reasoning. You might get 2 to 3 model calls max, which severely limits agent capabilities. Most "agents" in this category are not really agents; they are single-turn calls with light tool use.

**Assisted workflows (10 to 30 seconds).** If the user is willing to wait while the agent works—e.g., "analyze this document" or "draft an email"—you can afford 10 to 15 steps. This is the sweet spot for many agent use cases. Latency is noticeable but acceptable because the task is valuable.

**Background tasks (1 to 10 minutes).** If the task runs asynchronously—e.g., "compile a research report" or "run a security audit"—you can afford longer execution. You might allow 50 to 100 steps, aggressive reflection and iteration, and high-quality outputs. Cost is a bigger constraint than latency, but latency still matters because users will not wait forever.

**Batch processing (hours).** If the agent is processing tasks overnight or on a schedule, latency is irrelevant. You optimize purely for quality and cost. This is rare for user-facing products but common for internal tools.

You must design the agent to fit the latency budget. If you build a 60-second agent for a 5-second use case, the product fails no matter how good the outputs are.

## Why Prototypes Are Always 10x Over Budget in Production

Nearly every team underestimates agent costs during prototyping. Here is why.

**Prototypes use toy examples.** During development, you test the agent on simple, well-formed tasks. "Research Company X" where Company X is well-known, well-documented, and easy to analyze. In production, users ask about obscure companies, ambiguous queries, and edge cases. The agent takes more steps, makes more tool calls, and costs more.

**Prototypes do not account for failure.** In testing, you assume the happy path. In production, tool calls fail, APIs are slow, data is missing, and the agent retries. Retries double or triple costs.

**Prototypes do not handle real-world complexity.** In testing, tasks are clean. In production, tasks are messy. Users provide incomplete information, ask follow-up questions, change their minds mid-task, and request revisions. Each of these adds steps and tokens.

**Prototypes do not include monitoring and logging overhead.** Production agents log every step for debugging and compliance. Logs add tokens to context (so the agent can reference its own history) and add storage costs. Prototypes skip this.

**Prototypes use unrealistic context limits.** In testing, you might use small prompts and limited context. In production, you need full context to handle edge cases, which increases token usage.

The result: a prototype that costs 0.50 dollars per task in testing costs 5 dollars per task in production. Teams that do not budget for this 10x gap end up with unsustainable unit economics.

## When Cost Kills the Business Case

Some use cases simply do not work with agentic economics, no matter how much you optimize.

**High-volume, low-value tasks.** If you need to process a million tasks per day and the value per task is 5 cents, you cannot afford an agent that costs 50 cents per task. You need a simpler, cheaper solution: rule-based systems, traditional ML, or single-turn LLM calls.

**Real-time, latency-sensitive tasks.** If users need sub-second responses, agents are not viable. You need single-shot systems.

**Commodity tasks with thin margins.** If competitors offer similar functionality at 1 cent per task, you cannot compete with an agent that costs 2 dollars per task. The quality might be better, but not 200x better.

**Speculative or exploratory use cases.** If the business value is uncertain, you cannot justify high per-task costs. Agents are for proven, high-value use cases, not experiments.

Elite teams recognize when agents are not economically viable and choose different patterns. Mediocre teams build the agent anyway because it is technically interesting, then struggle to make the unit economics work, and eventually kill the product.

## The Long-Term Cost Trajectory

There is good news: agent costs are decreasing over time. Model providers are lowering API prices, improving inference speed, and offering features like prompt caching and batch processing that reduce costs. Models are getting more efficient—better outputs with fewer tokens. Tools are being optimized to return more concise, relevant results.

But costs are not decreasing fast enough to change the fundamental economics. Agents will likely always be 10x to 100x more expensive than single-turn calls, because they do 10x to 100x more work. The question is whether that work is worth the cost, and for many use cases, the answer is no.

The teams that succeed with agents are the ones that choose high-value use cases where the cost is justified, design the agent to fit a realistic cost and latency budget, and ruthlessly optimize to stay within that budget. The teams that fail are the ones that assume "costs will come down eventually" and build agents for use cases where the economics never work.

## The Path Forward

Agents are expensive and slow. This is not a temporary problem. It is inherent to the architecture. Multi-step reasoning, tool use, and state management consume tokens and latency. You cannot eliminate this overhead; you can only manage it.

If you are building an agent, you must do three things:

**One: Budget realistically.** Assume 10x to 100x higher costs than single-turn systems. Assume latency of 30 to 90 seconds for moderately complex tasks. Do not assume costs will magically decrease. Build a business model that works with these constraints, or choose a different architecture.

**Two: Optimize relentlessly.** Use smaller models where possible. Summarize context aggressively. Set hard limits on iterations and tool calls. Monitor cost per task and fix outliers. Treat cost as a first-class metric, not an afterthought.

**Three: Choose high-value use cases.** Do not build agents for commodity tasks or low-margin use cases. Build agents for tasks where the value delivered is much higher than the cost. If a task saves a user 30 minutes and that time is worth 50 dollars, a 5-dollar agent is a great deal. If the task saves 30 seconds and the value is 50 cents, the agent does not work.

The brutal economics of agents separate serious teams from hobbyists. Serious teams understand the costs, design for them, and build sustainable products. Hobbyists build cool demos that cost 10 dollars per task and wonder why no one will pay for them.

The next step is understanding how to evaluate these expensive, complex systems so you know whether they are actually working.
