# 7.1 â€” HITL for Agents: When Humans Must Intervene

In November 2025, a procurement agent at a manufacturing company ordered six million dollars worth of industrial adhesive instead of six hundred thousand. The agent had been delegated authority to place orders under one million dollars without human approval. The order was for five hundred ninety-seven thousand dollars, just under the threshold. The agent placed it autonomously. The supplier confirmed the order. Trucks started arriving three days later. The warehouse manager called the procurement director in a panic. They had nowhere to put five years worth of adhesive. The director canceled the order, but the cancellation fee was two hundred thousand dollars. The company's CFO demanded an explanation. The procurement director showed him the agent logs. The agent had correctly calculated the order quantity based on forecasted demand. The problem was that the forecast was wrong. A data entry error had inflated demand by a factor of ten. The agent had no way to know the forecast was wrong. It trusted the data. The CFO asked why no human had reviewed the order. The procurement director explained that the order was under the approval threshold. The CFO immediately changed the policy. All orders over one hundred thousand dollars now require human approval, regardless of what the agent recommends. The company lost two hundred thousand dollars and learned that autonomy without oversight is a liability.

Human-in-the-loop design is the art of deciding when humans must intervene in agent decisions. Full autonomy is fast and scalable but risky. Full human oversight is safe but slow and expensive. The right design sits somewhere in between. Agents handle routine decisions autonomously. Humans review high-stakes decisions before they are executed. The challenge is defining high-stakes. It is not just about dollar amounts. It is about irreversibility, downstream impact, regulatory requirements, and reputation risk. A decision that costs ten thousand dollars but can be reversed in five minutes is lower stakes than a decision that costs one thousand dollars but cannot be reversed and damages customer relationships.

The autonomy spectrum ranges from fully automated to fully human-supervised. At one end, the agent makes decisions and executes them without human involvement. At the other end, the agent proposes decisions, a human reviews and approves every one, and the agent only executes after approval. In between, you have conditional autonomy where the agent decides based on risk. Low-risk decisions are executed autonomously. High-risk decisions are escalated for human review. The procurement agent was supposed to be in the conditional autonomy zone, but the threshold was set too high. Six hundred thousand dollars was not low-risk, even though the policy said it was.

Risk-based intervention is the dominant pattern in 2026. You define a risk model that assigns a risk score to every agent decision. The risk score is based on factors like cost, irreversibility, uncertainty, and compliance requirements. Decisions below a risk threshold are executed autonomously. Decisions above the threshold are sent to a human reviewer. The reviewer sees the agent's recommendation, the supporting evidence, and the risk score. They approve, reject, or modify the decision. If they approve, the agent executes. If they reject, the agent does nothing. If they modify, the agent executes the modified version.

Building a risk model requires domain expertise. You cannot use a generic risk function. A high-risk decision in procurement is different from a high-risk decision in customer service. For procurement, risk is driven by cost, supplier reliability, and inventory impact. For customer service, risk is driven by customer lifetime value, churn probability, and brand reputation. You sit with domain experts and map out the factors that make a decision risky. You quantify those factors. You define thresholds. You test the risk model on historical decisions and tune the thresholds until the model correctly classifies high-risk and low-risk cases.

One company I worked with built a risk model for a customer refund agent. The agent could approve refunds autonomously up to a certain risk score. The risk score was based on refund amount, customer tenure, refund history, and fraud indicators. A fifty-dollar refund for a five-year customer with no refund history had a low risk score and was approved automatically. A five-hundred-dollar refund for a new customer with multiple recent refunds had a high risk score and was escalated to a human agent. The model reduced the human review rate from eighty percent of refunds to fifteen percent while maintaining the same fraud detection rate. The company saved hundreds of hours of agent time per month.

Regulatory requirements for human oversight are increasing. The EU AI Act, effective in 2025, classifies certain AI systems as high-risk and requires human oversight for high-risk decisions. High-risk categories include systems that affect access to essential services, employment, credit, and law enforcement. If your agent makes decisions in these categories, you must implement human oversight regardless of your internal risk model. The law does not specify how oversight must be implemented, but it requires that humans can understand the agent's reasoning, override decisions, and take responsibility for outcomes.

In practice, regulatory compliance drives many teams to implement human-in-the-loop even when they would prefer full autonomy. A lending agent that evaluates loan applications must have a human review every decision because lending is classified as high-risk under the AI Act. A hiring agent that screens resumes must have a human review every decision because hiring is classified as high-risk. The human reviewer does not have to redo the agent's work, but they must review the recommendation, understand the reasoning, and confirm that the decision is appropriate. This adds latency and cost but is legally required.

The cost of human intervention is substantial. Latency is the most visible cost. An autonomous agent responds in seconds. An agent with human review responds in minutes, hours, or days, depending on reviewer availability. For time-sensitive decisions, this latency is unacceptable. A customer service agent that needs human approval to issue a refund cannot keep the customer waiting for hours. The customer will escalate, leave a bad review, or churn. You either give the agent autonomy to issue refunds or lose customers.

Staffing is another cost. Human reviewers are expensive. If your agent escalates fifteen percent of decisions for review, and each review takes five minutes, you need one full-time reviewer for every thousand decisions per day. At scale, this adds up. A customer service operation handling ten thousand decisions per day needs ten full-time reviewers just to handle escalations. You can reduce this cost by improving the risk model to escalate fewer decisions or by increasing agent autonomy to handle more decisions without review. Either way, the cost is real and must be planned for.

Scalability limits are a third cost. Human review does not scale linearly. As decision volume grows, you need more reviewers. But hiring and training reviewers takes time. If decision volume spikes, you cannot instantly scale the review team. The result is a backlog. Decisions wait for review. Latency increases. Users get frustrated. The agent becomes a bottleneck. This is especially painful for seasonal businesses where decision volume fluctuates. A retail agent might handle ten thousand decisions per day in January and one hundred thousand per day in December. You cannot hire ninety reviewers for December and lay them off in January. You either overprovision year-round or accept degraded service during peak periods.

When not to require human intervention is as important as when to require it. Over-gating kills agent value. If you require human approval for every decision, the agent is just a proposal generator. It does not save time. It does not improve efficiency. It adds overhead because now humans have to review proposals instead of making decisions directly. The procurement company made this mistake after the adhesive incident. They dropped the approval threshold to one hundred thousand dollars, which meant that ninety percent of procurement decisions required human review. The procurement team spent all day reviewing agent proposals. They had no time for strategic work. Procurement velocity dropped. Suppliers complained about delays. Six months later, the company raised the threshold back to five hundred thousand dollars and invested in better demand forecasting to prevent bad data from reaching the agent. They learned that fixing the root cause is better than adding human review.

Finding the right balance between autonomy and oversight is iterative. You start with conservative thresholds. High oversight, low autonomy. You monitor outcomes. You track how often human reviewers override agent decisions, how often they approve them unchanged, and how often agent decisions lead to bad outcomes. If reviewers almost always approve, you have over-gated. Increase autonomy. If bad outcomes are frequent, you have under-gated. Increase oversight. You iterate until you find the threshold where most decisions are handled autonomously and most escalations are genuine edge cases that require human judgment.

One useful metric is the override rate. What percentage of escalated decisions do humans override? If the override rate is below ten percent, your risk model is too conservative. You are escalating decisions that the agent could handle autonomously. If the override rate is above fifty percent, your risk model is too aggressive. You are letting the agent make decisions it should not make. A healthy override rate is twenty to thirty percent. This means most escalations are borderline cases where human judgment genuinely adds value.

Another metric is the false negative rate. What percentage of autonomous decisions lead to bad outcomes? This is harder to measure because you have to define what constitutes a bad outcome and track outcomes over time. For the procurement agent, a bad outcome was an order that had to be canceled or significantly modified. For the refund agent, a bad outcome was a refund that was later identified as fraud. You track these outcomes, calculate the rate, and compare it to your risk tolerance. If the false negative rate is above your tolerance, you tighten the risk model and escalate more decisions.

The procurement company eventually implemented a hybrid approach. Orders under one hundred thousand dollars were fully autonomous. Orders between one hundred thousand and five hundred thousand dollars were semi-autonomous. The agent placed the order but flagged it for human review within twenty-four hours. The reviewer could approve or cancel. Most orders were approved, but the review gave the team a chance to catch errors before they became expensive. Orders over five hundred thousand dollars required pre-approval. The agent generated a proposal, a human reviewed it, and the agent placed the order only after approval. This three-tier system balanced speed, cost, and risk. Ninety-five percent of orders were under one hundred thousand dollars and fully autonomous. Four percent were semi-autonomous with post-approval review. One percent required pre-approval. The system scaled with the team's capacity and caught errors before they became disasters.

Human-in-the-loop design also affects user experience. Users interact differently with fully autonomous agents versus agents that require approval. A fully autonomous agent feels fast and capable. You ask for something, it does it, done. An agent that requires approval feels bureaucratic. You ask for something, it says it needs approval, you wait, eventually it gets done. Users tolerate approval delays for high-stakes decisions but not for routine ones. If your agent requires approval to schedule a meeting, users will stop using it. If it requires approval to purchase a million-dollar asset, users understand and accept the delay.

Transparency about approval requirements helps. When the agent escalates a decision, it should tell the user why. "This refund requires manager approval because the amount exceeds your approval limit. I have sent the request to your manager. You will receive a notification when it is approved." This sets expectations. The user knows the delay is policy, not a technical failure. They know who is reviewing the request. They know when to expect a response. Clear communication turns approval delays from frustrating black holes into understandable processes.

Some teams implement tiered approval based on user roles. Junior employees have low autonomy limits. Senior employees have high limits. Managers have very high limits. The same agent action might be autonomous for a senior employee and require approval for a junior employee. This aligns agent autonomy with organizational trust. You trust senior employees to make high-stakes decisions, so the agent does too. You require oversight for junior employees, so the agent does too. Role-based autonomy is common in procurement, HR, and finance applications where decision authority is already tiered.

The future of human-in-the-loop is adaptive autonomy. The agent learns from human review decisions and adjusts its risk model over time. If humans consistently approve a certain type of decision, the agent lowers the risk score for that decision type and starts handling it autonomously. If humans consistently override a certain type of decision, the agent raises the risk score and escalates more often. The agent becomes more autonomous as it learns what humans trust it to do. This requires tracking review outcomes and feeding them back into the risk model, which most teams do not yet do. But the teams that implement adaptive autonomy report significant improvements in both autonomy and safety over time.

In 2026, human-in-the-loop is not optional for high-stakes agent applications. Regulatory requirements, risk management, and user trust all demand human oversight. The question is not whether to implement it but how. Design your risk model carefully. Set thresholds based on data, not guesses. Monitor override rates and false negatives. Iterate toward the right balance. Communicate approval requirements clearly to users. Train reviewers to make decisions quickly. Invest in tools that make review efficient. Treat human-in-the-loop as a core feature of your agent, not an afterthought. The manufacturing company learned this lesson at a cost of two hundred thousand dollars. You can learn it for free by designing oversight into your system from the start.
