# 8.17 — Data Minimization: What the Agent Is Allowed to See

Your agent needs access to 200 patient records at any given time. You give it access to 400,000 records spanning 12 years, including psychiatric notes, substance abuse records, HIV status, and genetic test results. This is not security through obscurity. This is a HIPAA violation waiting to be discovered during an audit. Data minimization is not a compliance checkbox—it is the architectural principle that determines your blast radius when something goes wrong.

Data minimization is not a compliance checkbox—it is a core architectural principle for any system that automates decision-making or information retrieval. The principle is simple: an agent should have access only to the data required to complete its assigned task, scoped as narrowly as possible by time, entity, field, and sensitivity level. This is not just a privacy requirement under GDPR Article 5 or the EU AI Act's transparency obligations—it is a practical security and operational control. The more data an agent can see, the more data it can leak, misuse, or expose through prompt injection, misconfiguration, or downstream tooling errors. Data minimization reduces the blast radius of agent failures and makes your system auditable, testable, and defensible.

## The Scope Explosion Problem

When you build an agent, you face a tension between convenience and control. It is easier to give the agent broad access—read permissions on entire tables, API keys with full account scope, file system access to whole directories—because it reduces the number of integration points you need to manage. You do not have to think about filtering logic, row-level security, or dynamic scoping. But this convenience creates a systemic risk that compounds over time. Agents make hundreds or thousands of tool calls in production. Each call is an opportunity for data exposure. If the agent has access to ten thousand customer records but only needs to retrieve data for one customer per session, you have created a 10,000x exposure multiplier. A single prompt injection that causes the agent to iterate over all accessible records, a single logging configuration that captures tool call responses in plaintext, a single downstream bug that writes agent outputs to an unencrypted cache—any of these failures now exposes data you never intended to make available.

The scope explosion problem is especially acute in multi-tenant systems. An agent serving multiple customers must be scoped to see only the data belonging to the customer in the current session. If you use a single service account with cross-tenant read access, you are one user-ID validation bug away from a data breach. In early 2025, a SaaS company providing AI-powered customer support discovered that their agent had been returning data from the wrong customer accounts in approximately 0.3% of sessions due to a race condition in how the session context was passed to the database query builder. The agent had access to all tenant data through a shared read-replica credential. The bug exposed support ticket content, customer names, and email addresses across 140 accounts over a six-week period. The company faced regulatory penalties under GDPR, lost two enterprise customers, and spent nine months rebuilding trust with their user base. The fix was not a code patch—it was an architectural change to enforce tenant-scoping at the database credential level, so that even if the session context was corrupted, the agent could only ever query data for a single tenant.

You cannot rely on application logic alone to enforce data minimization. Application logic fails. Variables get overwritten, conditionals get bypassed, session state gets corrupted. The enforcement layer must be at the infrastructure level—database roles, API token scopes, file system permissions, network policies—so that even if your application code has a bug, the agent cannot see data it should not have access to. This is defense in depth applied to data access.

## Field-Level and Entity-Level Scoping

Data minimization operates at multiple levels of granularity. The coarsest level is entity scoping: which customers, which users, which documents, which accounts can the agent access? The next level is field-level scoping: within a given entity, which attributes can the agent read? The finest level is temporal scoping: for how long is the data accessible, and under what conditions does access expire? You need to apply all three layers to build a defensible agent architecture.

Entity-level scoping is the most obvious control. If your agent is helping a customer service representative resolve a support ticket, the agent should only have access to the customer record associated with that ticket. It should not have access to other customers, other tickets, or historical data beyond the current interaction. This scoping should be enforced by passing a dynamically generated credential or session token that is bound to the customer ID. In a SQL database, this might be a row-level security policy that filters all queries by customer ID. In an API, this might be a short-lived OAuth token scoped to a single user. In a document retrieval system, this might be a query filter that restricts results to documents tagged with the current session's user ID. The key is that the scoping is not optional—it is baked into the data access mechanism itself.

Field-level scoping is where most teams fail. They think about entity scoping but give the agent access to every field within the scoped entity. This is a mistake. If your agent is generating a shipping confirmation email, it needs the customer's name, order ID, shipping address, and tracking number. It does not need the customer's payment method, credit card last four digits, purchase history, or account creation date. Each unnecessary field is a liability. In mid-2025, a logistics company's agent began including customers' phone numbers in shipping confirmation emails due to a prompt engineering change that added "contact information" to the email template. The phone numbers were not necessary for the email's purpose and had not been included in prior versions. The company received complaints from customers who had not consented to having their phone numbers shared with shipping carriers. The fix required a field-level access audit and a schema redesign to separate contact fields into consent-gated and non-gated categories.

Field-level scoping is implemented by creating purpose-specific views, API endpoints, or query projections that return only the fields the agent needs. Do not give the agent a SELECT-star query. Give it a named view that returns exactly the columns required for the task. Do not give the agent an API response with 40 fields. Give it a filtered response with six fields. This also improves agent performance—smaller payloads mean faster tool calls, lower token costs, and less prompt space consumed by irrelevant data. Field minimization is both a security and a performance optimization.

Temporal scoping is the least understood control. Data that was necessary ten seconds ago may not be necessary now. If your agent retrieves a customer record to answer a question, that record should not remain in memory, in logs, or in the agent's context after the question is answered. In practice, this means flushing context between sessions, purging cached data after use, and setting expiration times on any agent-accessible credentials. A common anti-pattern is to give an agent a long-lived API token that remains valid for hours or days. This token can be extracted via prompt injection, logged accidentally, or leaked through error messages. Instead, issue short-lived tokens that expire after the current session or task completes. In database systems, use temporary credentials or session-scoped roles that are revoked when the agent's execution context ends. Temporal scoping limits the window of exposure for any data the agent accesses.

## Tooling and Infrastructure for Minimization

Data minimization is not achieved through documentation or policy—it is achieved through tooling and infrastructure that makes it the default, easy path. If your agent framework requires developers to manually implement field filtering, row-level security, and credential scoping for every tool, those controls will be implemented inconsistently or not at all. You need to build abstractions that enforce minimization by default.

One effective pattern is the scoped tool factory. Instead of giving your agent a generic database query tool that accepts arbitrary SQL, you provide a set of purpose-specific tools: get customer shipping info, get order status, get support ticket history. Each tool is backed by a pre-defined query or API call that returns only the fields necessary for that purpose. The tool's implementation handles entity scoping, field filtering, and credential management. The agent developer does not need to think about minimization—it is encoded in the tool's design. This pattern also makes your agent easier to audit. You can look at the list of tools the agent has access to and immediately understand the data exposure surface. You do not have to inspect every prompt, every conditional, every query string to verify that minimization is enforced.

Another pattern is session-scoped credential injection. When an agent session starts, your orchestration layer generates a temporary credential—a database role, an API token, a file system permission set—that is scoped to the session's context: the user, the tenant, the task. This credential is injected into the agent's environment and used for all data access during the session. When the session ends, the credential is revoked. This pattern is common in database systems that support dynamic role creation, such as PostgreSQL with row-level security policies or cloud data warehouses with temporary access grants. The agent never sees a long-lived credential, and the credential itself enforces minimization at the infrastructure level. Even if the agent is compromised via prompt injection, the attacker gains access only to the data scoped to that specific session.

A third pattern is data access logging with automated anomaly detection. Every data access by the agent is logged with metadata: which fields were accessed, which entities, at what time, in response to what user request. These logs are analyzed in real time to detect anomalies: an agent accessing fields it has never accessed before, accessing an unusually large number of entities in a single session, accessing data outside the expected temporal window. Anomalies trigger alerts and, in high-sensitivity environments, automatic session termination. This pattern does not prevent over-access, but it detects it quickly, allowing you to respond before significant data exposure occurs. In late 2025, a financial services company detected a prompt injection attack within 90 seconds of its initiation because the agent suddenly began querying customer Social Security numbers, a field it had never accessed in six months of prior operation. The session was terminated, the credential revoked, and the attack contained before any data was exfiltrated.

## Regulatory and Compliance Drivers

Data minimization is not optional if you operate under GDPR, HIPAA, the EU AI Act, or most sectoral privacy regulations. GDPR Article 5 requires that personal data be "adequate, relevant and limited to what is necessary in relation to the purposes for which they are processed." This is data minimization stated as a legal obligation. If your agent has access to customer data fields that are not necessary for its task, you are in violation of GDPR, and you cannot claim compliance by pointing to access controls that rely on application logic. The access must be structurally minimized.

Under the EU AI Act, high-risk AI systems must implement technical measures to ensure that data processed by the system is relevant and limited to what is necessary. The Act also requires that data governance and management practices be documented and auditable. This means you must be able to show, in an audit, exactly what data your agent has access to, why it has access, and how that access is scoped. You cannot produce a document that says "the agent follows the principle of least privilege." You must produce logs, configuration files, database role definitions, and API token scopes that demonstrate minimization in practice.

HIPAA's minimum necessary standard requires covered entities to limit access to protected health information to the minimum necessary to accomplish the intended purpose. For agents operating in healthcare contexts, this means you cannot give the agent access to a full patient record if only a subset of fields is needed. You cannot give the agent access to historical records if only current data is required. HIPAA audits specifically look for over-scoped access, and violations can result in fines up to $1.5 million per year for uncorrected deficiencies. In 2025, a healthcare technology company was fined $420,000 for providing an AI-powered triage agent with access to the full electronic health record system despite only needing access to current visit notes and vital signs.

Even outside regulated industries, data minimization reduces your legal and reputational risk. If your agent is breached, the scope of the breach is determined by the data the agent could access, not the data it actually accessed. If the agent had access to ten million customer records but only used ten thousand, the breach notification and remediation scope is ten million. You must notify all potentially affected users, conduct forensic analysis on all accessible data, and assume the worst-case exposure. If the agent had been scoped to access only ten thousand records, your breach response is proportionally smaller, faster, and cheaper.

## Implementation Patterns Across Data Stores

The specific mechanisms for enforcing data minimization vary by data store type, but the principles remain consistent. For SQL databases, use row-level security policies, dynamic database roles, and purpose-specific views. PostgreSQL's row-level security allows you to define policies that automatically filter query results based on the current session's user or role. You create a policy that restricts access to rows where the customer ID matches the session's customer ID, and this filter is applied transparently to all queries. The agent's application code does not need to include WHERE clauses for customer scoping—the database enforces it. You can also create temporary database roles that are granted only for the duration of the agent session, with permissions limited to the specific tables and columns the agent needs.

For NoSQL databases like MongoDB or DynamoDB, implement tenant scoping at the query level and use attribute-based access control to limit field visibility. In MongoDB, use field projection to return only the necessary fields, and combine this with query filters that enforce entity scoping. In DynamoDB, use fine-grained access control policies that allow the agent's IAM role to read only items matching a specific partition key or attribute value. For document stores like Elasticsearch, use document-level security and field-level security features to restrict both which documents are visible and which fields within those documents can be accessed.

For APIs, use scoped OAuth tokens or API keys with resource-specific permissions. Many modern APIs support token scoping at the resource or endpoint level, allowing you to issue a token that can only access a specific user's data or a specific set of API endpoints. Generate these tokens dynamically at the start of each agent session and revoke them when the session ends. Do not reuse tokens across sessions, and do not issue tokens with broader scope than necessary. If your API does not support fine-grained token scoping, build a proxy layer that enforces scoping by inspecting requests and filtering responses based on the session's context.

For file systems and object storage, use directory-level permissions and short-lived signed URLs. If your agent needs to read files from cloud storage, generate signed URLs that grant access only to the specific files required for the current task, with expiration times measured in minutes, not hours or days. Do not give the agent IAM credentials with bucket-level read access. Use service account impersonation or temporary security credentials that are scoped to the minimum set of objects. In Google Cloud Storage, use signed URLs with object-specific scopes. In AWS S3, use session tokens with policies that restrict access to specific key prefixes or objects.

## Auditability and Minimization Verification

Data minimization is not a one-time design decision—it is an ongoing operational practice that must be verified continuously. You need tooling and processes to audit what data your agent has access to, detect scope creep over time, and validate that minimization controls are functioning as designed. Scope creep is inevitable. As your agent evolves, new features are added, new tools are integrated, new data sources are connected. Each change is an opportunity for minimization controls to degrade. A developer adds a new field to a query because it makes debugging easier. A new API endpoint is integrated with broader permissions because the scoped version is not yet available. A temporary database role is granted additional privileges to resolve a production incident and never revoked. Over six months, your agent's data access surface doubles without anyone noticing.

Implement automated audits that compare the agent's current data access scope against a baseline definition of what it should have access to. This baseline is defined in a configuration file or policy document that lists the tables, fields, APIs, and entities the agent is permitted to access for each task type. The audit tool queries the actual permissions—database grants, IAM policies, API token scopes—and flags any deviations. Deviations are reviewed and either corrected or explicitly approved and documented. This process runs weekly or monthly and produces a report that is reviewed by security, compliance, and engineering leadership.

You also need runtime verification that minimization controls are active. Include test cases in your agent evaluation suite that attempt to access out-of-scope data and verify that the access is denied. If your agent is scoped to customer A's data, write a test that tries to query customer B's data and asserts that the query returns an empty result set or an access denied error. If your agent is scoped to specific fields, write a test that tries to read excluded fields and verifies that they are not present in the response. These tests run in CI/CD and in production monitoring, providing continuous assurance that minimization is enforced.

## The Trade-Off Between Minimization and Agent Capability

There is a tension between data minimization and agent capability. The more narrowly you scope data access, the less flexible and capable your agent becomes. An agent with access to a full customer record can answer a wider range of questions than an agent with access to only shipping information. An agent with access to all historical data can provide richer context than an agent limited to current session data. This tension is real, and you must navigate it deliberately.

The answer is not to abandon minimization in favor of capability. The answer is to design your agent's task scope to match the data access you are willing to grant. If you are only willing to give the agent access to current shipping information, do not ask it to answer questions about purchase history. If you are only willing to give the agent access to a single customer's data, do not ask it to perform cross-customer analytics. Align task scope and data scope. If a new capability requires broader data access, evaluate whether that access is justified by the value of the capability and whether the risk can be mitigated through other controls. Sometimes the answer is no, and you should not build the capability.

In some cases, you can achieve broader capability without broader access by using aggregated or anonymized data. If your agent needs to compare a customer's usage patterns against peer benchmarks, provide it with aggregated statistics rather than access to individual peer records. If your agent needs to detect anomalies, provide it with summary metrics rather than raw event logs. This approach preserves capability while limiting exposure.

Data minimization is not about crippling your agent—it is about making intentional, auditable decisions about what data the agent is allowed to see and ensuring that those decisions are enforced at the infrastructure level, not just the application level. Every field, every entity, every temporal window of access is a deliberate choice, documented and justified.

The next subchapter examines a specific and insidious attack vector that exploits agents' conversational interfaces and tool access: secret-adjacent attacks, where adversaries use prompt injection and social engineering to trick agents into revealing API keys, internal prompts, environment variables, and other sensitive operational data that should never be exposed to users or external systems.
