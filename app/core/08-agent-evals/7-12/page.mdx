# 7.12 — HITL Anti-Patterns: Over-Gating and Approval Fatigue

You need human judgment where it matters, but if you invoke it everywhere, you destroy its value—over-gating creates approval fatigue, approval fatigue creates inattention, and inattention creates the very failures you built the gate to prevent. In early 2025, a B2B SaaS company required human review for every single sales outreach email their agent generated. By week three, the backlog grew to 480 pending emails. By week five, the average review time ballooned from two minutes to eleven seconds as reviewers clicked approve without reading. One sales rep admitted she approved 60 emails in a row while on a conference call. Two weeks later, a major prospect received an email referencing a competitor's product name in the value proposition. The deal died, the agent was shut down, and the team reverted to manual outreach.

The failure was not a lack of oversight. It was the wrong oversight model. The team treated every decision as equally critical and funneled everything through the same approval gate. Humans became bottlenecks, then rubber stamps, then liabilities. This is the central tension in HITL design: you need human judgment where it matters, but if you invoke it everywhere, you destroy its value. Over-gating creates approval fatigue, approval fatigue creates inattention, and inattention creates the very failures you built the gate to prevent. Understanding HITL anti-patterns is not an abstract exercise. It is the difference between a system that improves over time and one that collapses under its own safeguards.

## The Over-Gating Trap: Treating All Decisions as Equal

Over-gating occurs when you require human review for actions that do not justify the cost, latency, or cognitive load of intervention. It emerges from a failure to differentiate decision risk. Every agent action carries some uncertainty, but not every uncertainty warrants human attention. Over-gating systems treat low-stakes, high-frequency decisions the same way they treat high-stakes, low-frequency ones. The result is a review queue that overwhelms human capacity and trains reviewers to disengage.

The most common form of over-gating is universal pre-approval. Every agent output, regardless of confidence, context, or consequence, gets routed to a human before execution. In the email outreach example, this meant reviewing 500 emails per week, most of which were routine, on-brand, and safe. The cognitive cost of reviewing 500 similar items is not 500 times the cost of reviewing one. It is higher, because repetition breeds autopilot behavior. Humans are poor at sustained vigilance for rare events in high-volume streams. This is why airport security screeners miss threats and why radiologists miss anomalies after hours of normal scans. Your reviewers are no different.

Another variant is confidence-blind gating. The agent produces a confidence score, but the HITL system ignores it. A response with 97 percent confidence and a response with 62 percent confidence both go to the same queue with the same priority. This removes the signal that helps humans allocate attention. High-confidence outputs should bypass review or receive lightweight spot-checks. Low-confidence outputs deserve deep inspection. Treating them identically wastes human effort on safe cases and under-resources risky ones.

Over-gating also appears in multi-stage approval chains. A content moderation agent flags a post, a junior reviewer escalates it, a senior reviewer escalates again, and a policy lead makes the final call. Each stage adds latency and dilutes accountability. If four people touch a decision, no one owns it. The junior reviewer assumes the senior will catch errors. The senior assumes the policy lead will adjudicate edge cases. The policy lead assumes the junior did the initial diligence. The result is diffusion of responsibility and slower, lower-quality decisions. Multi-stage review has a place for truly ambiguous cases, but it should be the exception, not the default.

The root cause of over-gating is often organizational anxiety, not technical necessity. Teams fear being blamed for agent mistakes, so they insert humans everywhere as insurance. But insurance only works if the humans add value. If they are too overwhelmed to review carefully, you have not reduced risk. You have added latency and created a false sense of safety. The fix is not to remove all gates. It is to place gates where they matter and remove them where they do not.

## Approval Fatigue: When Humans Stop Paying Attention

Approval fatigue is the predictable consequence of over-gating. It occurs when the volume, similarity, or low stakes of review tasks cause humans to disengage cognitively. Reviewers stop reading carefully, start pattern-matching on superficial features, and approve items reflexively. The behavior is rational: if 95 percent of reviews result in approval, and rejections carry no reward, why spend mental effort on each one? The system has trained reviewers that their job is not judgment, but throughput.

The warning signs of approval fatigue are measurable. Review time drops over weeks while volume stays constant. Approval rates rise without corresponding changes in agent accuracy. Rejection explanations become generic or copy-pasted. Reviewers batch-approve items in rapid succession, often during other activities. If you see average review time under 15 seconds for tasks that should require 60 seconds of reading, you have approval fatigue. If approval rates exceed 98 percent, you are not catching edge cases—you are rubber-stamping.

Approval fatigue is dangerous because it inverts the purpose of HITL. You built the gate to catch bad outputs. Fatigued reviewers approve bad outputs at the same rate as good ones. Worse, they approve bad outputs with human attribution, which makes the failure look deliberate rather than algorithmic. In the email outreach case, the sales rep who approved the competitor-name email became the scapegoat, even though the system set her up to fail. She was asked to review 60 emails in one sitting, with no tooling to highlight anomalies, no differentiation by risk, and no feedback loop to show her what good review looks like.

The psychological driver is cognitive depletion. Humans have finite attention and decision-making capacity. Each review draws from that budget. If you ask someone to make 200 low-stakes decisions in a row, their performance on decision 180 will be worse than decision 20, even if they are motivated and well-trained. This is not a personnel problem. It is a design problem. You have exceeded human cognitive limits and expected sustained performance anyway.

Approval fatigue also compounds with ambiguity. If the review criteria are vague—"does this email sound good?"—then the reviewer has no objective standard to apply. They fall back on gut feeling, which is fast but inconsistent. Over time, "gut feeling" becomes "does this look roughly like the others I just approved?" The reviewers are no longer evaluating quality. They are pattern-matching on familiarity, which means novel good outputs and novel bad outputs both get approved as long as they look plausible.

## The False Safety of Process Theater

Process theater is the anti-pattern where HITL exists to satisfy compliance, optics, or organizational politics, not to improve decisions. The review step is ceremonial. It provides documentation that "a human was in the loop," but the human has neither the time, tools, context, nor authority to change outcomes. Process theater creates the appearance of oversight without the substance.

A common example is the pre-launch legal review for agent-generated content. Legal is given 48 hours to review 300 outputs, with no prioritization, no risk scoring, and no ability to delay the launch if issues are found. Legal knows that rejecting outputs will not stop the launch—it will only create friction with Product. So Legal approves everything with a boilerplate disclaimer. The review happened on paper, but it added zero risk reduction. It was theater.

Process theater is particularly prevalent in regulated industries. A healthcare company built an agent to draft patient communication emails and required clinical review "for compliance." The clinical reviewers were nurses with no training in the agent's task, no rubric for what to check, and no time allocated beyond their existing patient care duties. They approved emails during shift changes, on their phones, in under 30 seconds per email. When an email later contained outdated medication guidance and a patient complained, the company pointed to the clinical review as evidence of due diligence. Regulators were not impressed. The review was documented, but it was not substantive.

The danger of process theater is that it insulates the organization from accountability while providing no actual protection. Worse, it creates a paper trail that can backfire. If you document that a human reviewed and approved a harmful output, that human and the organization are now liable for the harm. If the review was cursory and the human had no real capacity to identify the issue, that looks like negligence, not oversight. You are better off with no review and clear agent attribution than with fake review and false human attribution.

Process theater also demoralizes the reviewers. They know their input does not matter. They are checking boxes, not making decisions. Over time, they disengage entirely or leave the role. The best reviewers, the ones who take the work seriously, burn out first because they are the ones trying to do a job the system has made impossible. You end up with a review team of people who have learned to stop caring.

The fix is to make review meaningful. If you cannot give reviewers the time, tools, and authority to change outcomes, do not ask them to review. Use deterministic rules, automated checks, or post-hoc audits instead. HITL should be reserved for decisions where human judgment genuinely improves the result and where the system is designed to act on that judgment.

## The Escalation Spiral: When Review Becomes a Political Game

The escalation spiral occurs when reviewers, unsure of their authority or afraid of making mistakes, escalate decisions upward rather than resolving them. The agent flags an edge case. The junior reviewer escalates to the senior reviewer. The senior reviewer escalates to the policy lead. The policy lead escalates to the VP. The VP escalates to Legal. Legal escalates back to Product. Six people touch the decision, it takes four days to resolve, and the final answer is often "approve it," the same outcome the junior reviewer could have chosen in five minutes.

Escalation spirals emerge from unclear decision rights and blame-averse culture. If reviewers do not know what they are empowered to decide, they escalate to avoid risk. If the culture punishes wrong calls more than it rewards right ones, reviewers escalate to diffuse responsibility. If escalation is easier than deciding, escalation becomes the default. The result is a system where every ambiguous case becomes a multi-day committee process, and clear cases are the only ones that get handled quickly.

The cost of escalation spirals is not just latency. It is also decision quality. Each escalation step strips away context. The junior reviewer saw the full agent output, the user query, the confidence score, and the surrounding conversation. The VP sees a two-sentence summary in a Slack thread. The VP is making a decision with 5 percent of the information, based on trust that the summary is accurate and complete. Often, it is not. Key details get lost in translation. Nuance gets flattened. The final decision is less informed than the initial one.

Escalation spirals also create resentment. Junior reviewers feel their judgment is not trusted. Senior reviewers feel they are being used as rubber stamps for decisions others should make. The VP feels buried in trivia. Everyone is unhappy, and the agent is sitting idle while humans debate. If the agent is in a customer-facing loop, the customer is waiting too, which compounds the problem.

The anti-pattern is especially harmful in time-sensitive domains. A content moderation agent flags a post as potential misinformation. The post is spreading rapidly. The junior moderator escalates because the policy is ambiguous. The senior moderator escalates because the topic is politically sensitive. The policy lead escalates because they do not want to own the call. By the time someone decides to take the post down, it has been shared 40,000 times. The escalation spiral turned a fast-moving problem into a slow-moving committee process, and the delay made the decision irrelevant.

The solution is clear decision rights and pre-defined escalation thresholds. Junior reviewers own decisions below a certain risk or ambiguity threshold. Senior reviewers own decisions above that threshold. Escalation is triggered by objective criteria, not subjective discomfort. Each level of review has a defined SLA, and if the SLA is missed, the default action is taken automatically. You design the system so that escalation is the rare exception, not the common path.

## The Ignored Feedback Loop: When Review Produces No Learning

A HITL system generates two outputs: the immediate decision and the long-term data about what humans choose. Most teams use the first and ignore the second. Reviewers approve or reject agent outputs, but no one analyzes the patterns in those decisions to improve the agent, refine the criteria, or adjust the gating rules. The feedback loop is open, not closed. Every review is a wasted opportunity to learn.

Ignored feedback loops are easy to spot. Reviewers make the same corrections repeatedly. The agent keeps making the same mistakes. Rejection reasons are logged but never aggregated or analyzed. No one asks "why are we rejecting 12 percent of email subject lines?" or "why do senior reviewers overturn junior reviewers 30 percent of the time?" The data is there, but it is not being used to drive improvement.

The consequence is stagnation. The agent does not get better because the team does not train it on the corrections. The review process does not get more efficient because the team does not identify which gates are adding value and which are not. Six months in, the system looks the same as it did on day one, except the backlog is larger and the reviewers are more burned out.

One enterprise software company ran a HITL loop for agent-generated SQL queries. Reviewers flagged queries that were inefficient, incorrect, or insecure. Over eight months, reviewers flagged 1,200 queries. The corrections were logged in a ticketing system, but no one ever pulled the data to retrain the agent or update the prompt. The same error patterns appeared month after month: missing indexes, Cartesian joins, exposed credentials. The reviewers became demoralized because their feedback was clearly not being used. The agent did not improve. The company eventually scrapped the agent, blaming "poor model performance," when the real failure was organizational—they had no process to turn review data into model updates.

Closing the feedback loop requires three things. First, structured rejection reasons. Reviewers do not just say "rejected." They select from a predefined taxonomy: factual error, tone mismatch, policy violation, formatting issue, security risk. Second, regular analysis. Every week or sprint, someone aggregates rejection reasons, identifies the top three patterns, and files tickets to address them. Third, action. The tickets result in prompt updates, training data additions, or rule changes. The loop is only closed if the review data changes the system.

Ignored feedback also applies to reviewer performance. If you never analyze inter-reviewer agreement, you will not catch inconsistent reviewers or ambiguous policies. If two reviewers see the same output and one approves while the other rejects, that is a signal. Either the output is genuinely borderline, in which case you need clearer criteria, or one reviewer is miscalibrated, in which case you need retraining. Without analysis, you never know which.

## Designing HITL to Avoid Fatigue and Theater

The path out of these anti-patterns is a HITL design that respects human limits, differentiates risk, and closes feedback loops. You start by segmenting decisions into tiers. Tier one decisions are low-risk, high-confidence, or have bounded impact. They bypass human review or get lightweight spot-checks. Tier two decisions are moderate-risk or moderate-confidence. They get single-reviewer approval with clear rubrics. Tier three decisions are high-risk, low-confidence, or novel. They get deep review, possibly multi-stage, with time and tooling to support careful analysis.

You assign review budget based on tier. If you have 10 hours of human review capacity per week, and tier one represents 80 percent of volume, you allocate zero hours to tier one. You allocate 6 hours to tier two and 4 hours to tier three. The allocation matches the risk, not the volume. This prevents the high-volume low-risk cases from drowning out the low-volume high-risk ones.

You reduce approval fatigue by reducing unnecessary volume. You tune the agent to improve confidence on tier one cases, pushing more outputs above the review threshold. You implement automated checks that catch common errors before they reach humans. You batch reviews intelligently—rather than reviewing 60 similar emails in a row, you show reviewers a diverse sample that keeps them engaged. You limit review sessions to 20-30 minutes to prevent cognitive depletion.

You eliminate process theater by auditing every review step. For each one, you ask: does the reviewer have the expertise to evaluate this? Do they have the time? Do they have the authority to reject? If the answer to any is no, you remove the step or redesign it. You do not create review steps to satisfy compliance unless the reviewer can substantively evaluate the compliance dimension. You document what the review actually checks, not just that it happened.

You prevent escalation spirals by defining decision rights upfront. You write a RACI matrix: who is Responsible for each tier of decision, who is Accountable, who is Consulted, who is Informed. You set escalation triggers based on objective criteria, not reviewer anxiety. You measure escalation rates and treat high escalation as a sign of unclear authority, not careful review.

You close feedback loops by building review data into your sprint process. Every week, you pull the top rejection reasons, the inter-reviewer agreement score, and the tier distribution. You ask: are we reviewing the right things? Are the reviewers aligned? Is the agent improving? You turn the answers into action items. The HITL system is not a static gate. It is a dynamic learning loop that improves the agent, the process, and the reviewers over time.

Your HITL design is a statement about how you value human judgment. If you treat humans as infinite-capacity, zero-cost approvers, you will burn them out and get bad decisions. If you treat them as finite-capacity, high-value experts, you will allocate their attention carefully and get good decisions. The anti-patterns are not edge cases. They are the default outcome of lazy design. Avoiding them requires discipline, measurement, and a willingness to say no to review steps that do not earn their cost. The next subchapter covers how to operationalize that discipline at scale through review queue operations: SLAs, backlog management, routing, and prioritization.
