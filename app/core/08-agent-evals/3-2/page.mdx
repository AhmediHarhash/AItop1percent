# 3.2 â€” Goal Decomposition: Breaking Tasks into Executable Steps

On October 8, 2025, a product manager at a SaaS startup called MetricFlow asked their AI agent to "research our top three competitors and identify gaps in our feature set." The request seemed straightforward. The agent, running on a sophisticated framework with chain-of-thought reasoning and access to web search, databases, and document analysis tools, spent seventy-two minutes and thirty-four dollars producing a report that was simultaneously too detailed and completely useless. The agent had decomposed the high-level goal into exactly one step: "search for information about competitors." It executed this step by running ninety-three web searches with queries like "MetricFlow competitors," "SaaS analytics tools," and "product features comparison," aggregating the results into a 14,000-word document that repeated the same surface-level information ninety-three times. The agent never identified the top three competitors, never listed their features systematically, never compared those features to MetricFlow's features, and never identified gaps. The failure was not in execution; the web searches worked fine. The failure was in decomposition: the agent did not break the high-level goal into the specific, executable steps needed to accomplish it.

You give your agent a goal: research competitors, write a report, analyze customer feedback, investigate a bug, prepare a presentation. The goal is abstract and high-level. The agent's tools are concrete and low-level: search a database, call an API, read a file, run a calculation. The gap between the abstract goal and concrete tools is the decomposition problem. How do you break "research competitors" into "search for company names in industry database," "extract feature lists from their websites," "compare feature lists to our feature list," "identify features they have that we do not," and "summarize findings in a structured report"? This is goal decomposition, and it is one of the hardest reasoning challenges your agent will face. Decomposition determines whether the agent solves the task or wanders aimlessly. It determines whether the agent executes efficiently or wastes time and money on irrelevant actions. It determines whether the agent produces useful output or verbose nonsense.

## The Decomposition Challenge: From High-Level Goals to Concrete Actions

The fundamental challenge is that goals are expressed in terms of outcomes, but agents execute in terms of actions. "Research competitors" describes a desired outcome: you want to end up with knowledge about competitors. It does not describe actions. To achieve the outcome, the agent must decide which actions to take: which searches to run, which documents to read, which data to extract, which comparisons to make. The space of possible actions is enormous. The subset of actions that efficiently achieves the goal is tiny. Finding that subset is the decomposition problem.

Decomposition is hard because it requires domain knowledge, task understanding, and strategic reasoning. Domain knowledge tells you which data sources are likely to contain relevant information. Task understanding tells you what "research competitors" means in your specific context: are you looking for feature comparisons, pricing comparisons, market positioning, customer reviews, or all of the above? Strategic reasoning tells you how to sequence actions efficiently: should you identify competitors first, then research each one, or should you search broadly and filter as you go?

Humans perform decomposition intuitively, drawing on experience and common sense. If you ask a human analyst to research competitors, they will immediately break it down: first, identify who the competitors are; second, visit each competitor's website; third, list their key features; fourth, compare to our features; fifth, identify gaps. This seems obvious, but it is the result of years of experience with similar tasks. The analyst knows what "research competitors" means in a business context. They know that features are found on websites and in product documentation. They know that comparisons require side-by-side structured data. An AI agent does not have this accumulated experience unless you encode it in prompts, examples, or learned patterns.

LLMs have some decomposition ability from pretraining. They have seen countless examples of task decomposition in their training data: project plans, recipes, instructions, tutorials. They can leverage these patterns to decompose novel tasks. But their decomposition is only as good as their understanding of the task and their reasoning about which patterns apply. When the task is ambiguous, unusual, or requires domain-specific knowledge, LLM decomposition often fails. The agent might decompose the task in a way that is logically coherent but practically useless, like the MetricFlow example.

## Top-Down vs Bottom-Up Decomposition

There are two primary decomposition strategies: top-down and bottom-up. Top-down decomposition starts with the high-level goal and recursively breaks it into subgoals until you reach atomic actions. "Research competitors" becomes "identify competitors" and "analyze each competitor." "Identify competitors" becomes "search industry database" and "extract company names." You continue until each subgoal is an executable tool call. This is the classic hierarchical planning approach.

Top-down decomposition is systematic and complete. If you decompose correctly, you end up with a full plan that covers all aspects of the goal. It also produces a hierarchical structure that makes dependencies clear: you must identify competitors before you can analyze them. The hierarchy provides a natural execution order and a way to track progress: you have completed identifying competitors, now you are analyzing them.

The challenge with top-down decomposition is that it requires accurate upfront understanding of the task. If you misunderstand the goal or lack domain knowledge, your initial decomposition will be wrong, and all subsequent decomposition will inherit that error. If "research competitors" is misunderstood as "find names of competitors" rather than "analyze competitors' features and positioning," the entire decomposition tree will miss critical steps. Top-down decomposition is also rigid: once you commit to a decomposition, changing it requires re-decomposing from a higher level.

Bottom-up decomposition starts with available tools and data, identifies relevant actions, and composes them into a solution. "I have a web search tool, a database query tool, and a document analysis tool. To research competitors, I can search for competitor names, query the database for known competitors, search for feature pages on their websites, extract features from those pages, and compare to our feature list." You build up from concrete actions to a complete solution.

Bottom-up decomposition is adaptive and opportunistic. It leverages whatever tools and data are available. If a tool is unavailable or a data source is empty, you simply use different actions. It is less likely to produce plans that require unavailable capabilities. It also discovers creative solutions by combining tools in unexpected ways.

The challenge with bottom-up decomposition is that it can miss the forest for the trees. The agent gets anchored on available tools and might fail to recognize that the goal requires actions not easily expressed with those tools. If the goal requires synthesizing information from multiple sources and the agent's tools are all single-source queries, bottom-up decomposition might produce a plan that queries many sources but never synthesizes the results. Bottom-up decomposition also tends to produce flatter, less structured plans, which can be harder to execute and debug.

In practice, effective decomposition uses both strategies. Start top-down to understand the goal and identify major subgoals. Then go bottom-up within each subgoal to identify concrete actions using available tools. Then reconcile: does the bottom-up set of actions actually achieve the top-down subgoals? If not, revise either the subgoals or the actions. This iterative refinement produces decompositions that are both complete and executable.

## Granularity Decisions: How Fine-Grained Should Steps Be

One of the most critical decomposition decisions is granularity: how fine-grained should the steps be? Should "research competitors" decompose into three steps, ten steps, or fifty steps? There is no universal answer. The right granularity depends on the agent's capabilities, the task's complexity, and the execution strategy.

Fine-grained decomposition produces many small steps, each representing a single tool call or a simple reasoning operation. "Identify competitors" becomes "search database for competitors," "extract company names from search results," "deduplicate company names," "rank by relevance," "select top three." Each step is trivial to execute. The advantage is clarity and debuggability: you can see exactly what the agent is doing at each step. The disadvantage is overhead: generating, tracking, and executing fifty steps costs more than generating, tracking, and executing five steps. Fine-grained decomposition also increases the risk of over-specification: the plan becomes so detailed that it is brittle and cannot adapt to unexpected situations.

Coarse-grained decomposition produces few large steps, each representing a significant chunk of work. "Identify competitors" is a single step that internally handles database search, extraction, deduplication, ranking, and selection. The advantage is efficiency: fewer steps mean less overhead. The disadvantage is opacity: you cannot see what happens inside each step, making debugging harder. Coarse-grained decomposition also places more burden on the execution layer: each step must handle multiple operations, which requires more complex prompting or tool implementations.

The optimal granularity is usually task-dependent. For tasks where each action is cheap and fast, fine-grained decomposition is acceptable because the overhead is small. For tasks where each action is expensive and slow, coarse-grained decomposition is necessary to minimize overhead. For tasks where you need tight control and observability, fine-grained decomposition is worth the overhead. For tasks where you trust the agent and want maximum efficiency, coarse-grained decomposition is better.

Another consideration is the agent's reasoning capability. Less capable agents benefit from finer-grained decomposition because each step is simpler and the agent is less likely to make mistakes. More capable agents can handle coarser-grained decomposition because they can internally manage the complexity of multi-operation steps. As LLMs improve, the optimal granularity shifts toward coarser steps, reducing overhead while maintaining quality.

A practical heuristic is to aim for steps that are individually verifiable. Each step should produce a concrete, checkable result: a list of company names, a feature comparison table, a summary paragraph. If a step's result is too abstract to verify, like "understand the competitive landscape," the step is too coarse. If a step's result is trivial, like "increment counter," the step is too fine. Verifiable steps enable debugging and provide natural checkpoints for progress tracking.

## Dependency Identification Between Steps

Many tasks have dependencies: step B cannot execute until step A completes. "Extract features from competitor websites" depends on "identify competitors." "Compare features to our features" depends on "extract features from competitor websites" and "extract features from our product documentation." Correct decomposition must identify these dependencies and order steps accordingly. Incorrect dependency handling leads to failures: the agent tries to compare features before extracting them, or tries to extract features from competitors it has not yet identified.

Explicit dependency identification is straightforward: for each step, list the steps it depends on. This produces a directed acyclic graph where nodes are steps and edges are dependencies. Topological sort of the graph gives you a valid execution order. Many planning frameworks use this approach, representing plans as DAGs. The advantage is precision: dependencies are explicit and execution order is guaranteed to be valid.

The challenge is that dependencies are not always obvious from high-level step descriptions. "Research competitor A" and "research competitor B" might seem independent, but if they both use the same rate-limited API, they have an implicit resource dependency: executing them in parallel might exceed the rate limit. "Extract features from website" might depend on "set up web scraping session" if the website requires authentication. These implicit dependencies are hard to identify without detailed domain knowledge.

Another challenge is dynamic dependencies: dependencies that emerge during execution based on the data. "Extract features from competitor websites" depends on "identify competitors," but you do not know how many competitors there are until you execute the identification step. If there are three competitors, you have three extraction steps. If there are ten, you have ten. The dependency structure is not fully known until runtime. Planning frameworks must handle dynamic dependencies, either by generating steps lazily during execution or by representing steps parametrically.

Some tasks have conditional dependencies: step B depends on step A only if A's result meets certain criteria. "Send alert to on-call engineer" depends on "check system health" only if the health check fails. Planning frameworks must represent conditional logic, which complicates the plan structure. Pure DAG representations do not handle conditionals well; you need richer representations like control flow graphs or state machines.

In practice, most agent systems do not perform explicit dependency analysis. Instead, they rely on step ordering and implicit dependencies. The decomposition produces a linear sequence of steps, and the sequence is designed so that dependencies are satisfied by the ordering. Step three implicitly depends on steps one and two because it is ordered after them. This works for simple tasks with clear sequential structure, but it fails for tasks with complex dependencies, parallelism, or conditional logic.

## The Risk of Over-Decomposition Creating Unnecessary Complexity

Over-decomposition is when you break a task into more steps than necessary, creating unnecessary complexity. Instead of "search database for competitors" as one step, you have "construct database query," "execute database query," "receive database response," "parse database response," "extract company names," "validate company names." Each step is trivial, and the decomposition adds overhead without adding value.

Over-decomposition increases cognitive load. The plan is harder to understand because you have to track many small steps. It increases implementation complexity: more steps mean more state to track, more error handling, more coordination logic. It increases cost: generating the plan costs more, executing the plan costs more, and monitoring the plan costs more. It also increases brittleness: the more steps you have, the more opportunities for individual steps to fail, and the more failure-handling logic you need.

A common source of over-decomposition is confusing logical steps with physical steps. Logically, "search database for competitors" involves constructing a query, executing it, and parsing the response. But physically, these all happen within a single tool call to a database search function. You do not need to decompose into three steps; you just call the tool. Over-decomposition happens when the agent reasons about the logical process without considering the abstraction level of available tools.

Another source is premature optimization. The agent tries to optimize the execution by breaking steps into smaller pieces that can be parallelized, cached, or retried independently. But unless you actually need these optimizations, the added complexity is not justified. Start with coarse-grained steps, and only decompose further if you identify a specific need for finer granularity.

Avoiding over-decomposition requires discipline. When decomposing, ask: is this step necessary, or is it just a substep of a larger operation? Can this step be combined with adjacent steps without loss of clarity? Does decomposing this step actually make execution easier, or does it just make the plan longer? If the answer is unclear, err on the side of coarser decomposition. You can always refine later if needed.

## The Risk of Under-Decomposition Creating Vague Steps

Under-decomposition is the opposite problem: steps that are too coarse and vague to execute reliably. "Research competitors" as a single step is under-decomposition. The agent does not know what "research" means or how to execute it. The step is not actionable. Under-decomposition leads to failures during execution because the agent must perform additional reasoning to figure out what the step means, and that reasoning might be incorrect or incomplete.

Under-decomposition also hides complexity. A step that says "analyze customer feedback" might internally require reading hundreds of documents, extracting sentiment, categorizing issues, and summarizing trends. If all of this is hidden in one step, you cannot see what the agent is doing, you cannot track progress, and you cannot debug failures. The step becomes a black box.

Another problem is that under-decomposed steps often require multiple tool calls, and the sequencing and error handling for those calls is left implicit. If "analyze customer feedback" requires calling a document search tool, a sentiment analysis tool, and a summarization tool, who decides the order? What happens if one tool fails? Without explicit decomposition, these decisions are made ad-hoc during execution, which is error-prone.

Under-decomposition is common when the agent lacks domain knowledge or task understanding. If the agent does not know what "research competitors" entails, it cannot decompose it properly. The agent produces a vague step and hopes that the execution layer will figure it out. This might work if the execution layer is sophisticated and has domain knowledge, but in most systems, the execution layer is dumb: it just calls tools as instructed. Vague steps lead to vague execution.

Avoiding under-decomposition requires ensuring that each step is actionable: it should be clear which tool to call or which reasoning to perform. A good test is to ask: could a junior engineer execute this step without additional clarification? If not, the step is too vague and needs to be decomposed further. Another test is to check if the step maps to a single tool call or a small, well-defined sequence. If it requires many tool calls or complex reasoning, it should be decomposed.

## How to Evaluate Decomposition Quality

Evaluating decomposition quality is difficult because there is no ground truth. For a given task, there are many valid decompositions, and the "best" one depends on your priorities: speed, cost, quality, robustness. However, there are some general criteria you can use to assess whether a decomposition is good.

Completeness: does the decomposition cover all aspects of the goal? If the goal is to research competitors and the decomposition includes identifying competitors and extracting features but not comparing features or identifying gaps, the decomposition is incomplete. Check that each component of the goal has corresponding steps.

Correctness: does executing the steps actually achieve the goal? This requires reasoning about the logical flow from steps to goal. If the goal is to identify feature gaps and the decomposition includes extracting competitor features and our features but does not include a comparison step, the decomposition is incorrect. The steps might be individually correct but collectively insufficient.

Efficiency: does the decomposition avoid redundant or unnecessary steps? If the decomposition includes searching three different databases for the same information, it might be inefficient. If it includes steps that do not contribute to the goal, it is wasteful. Efficiency is harder to evaluate because it requires understanding which information is necessary and which data sources are redundant.

Executability: can the agent reliably execute each step? This depends on whether the steps are concrete and actionable. If steps are vague or require capabilities the agent does not have, the decomposition is not executable. Check that each step maps to available tools and that the agent has enough information to execute the step.

Robustness: does the decomposition handle likely failure modes? If a step depends on data that might not exist, is there a fallback? If a step might fail, is there error handling or a retry strategy? Robustness is often overlooked during decomposition but is critical for production systems.

A practical evaluation method is to have the agent explain its decomposition. After decomposing the task, ask the agent: "For each step, explain what it does, why it is necessary, and how it contributes to the goal." The explanations reveal whether the agent understands the task and whether the decomposition is coherent. If the agent cannot explain why a step is necessary, the step is probably unnecessary or the decomposition is flawed.

Another method is to simulate execution. Walk through the decomposition step by step, imagining what each step would produce. Does the output of step one provide the input needed for step two? Does the final step produce the desired outcome? Simulation catches many decomposition errors before execution.

You can also compare the agent's decomposition to human decomposition. Give the same task to a human expert and ask them to list the steps. Compare the human's steps to the agent's steps. Differences highlight potential issues. If the human includes steps the agent omitted, the agent's decomposition is likely incomplete. If the agent includes steps the human omitted, the agent might be over-decomposing or misunderstanding the task.

## Teaching Agents to Decompose Through Examples and Prompts

Since decomposition is a reasoning task, you can improve it through prompt engineering and few-shot examples. The simplest approach is to include explicit decomposition instructions in your agent's system prompt: "When given a high-level goal, break it into 3-7 concrete subgoals. For each subgoal, list the specific actions needed to achieve it. Ensure each action is executable using available tools."

Few-shot examples are more effective. Provide examples of goals and their correct decompositions. "Goal: research competitors. Decomposition: 1. Identify top competitors by searching industry database. 2. For each competitor, extract key features from their website. 3. Extract our key features from product documentation. 4. Compare competitor features to our features. 5. Identify features they have that we do not. 6. Summarize gaps in a report." The agent learns the pattern and applies it to novel goals.

The quality of your examples determines the quality of learned decomposition. Examples should cover diverse task types: research tasks, analysis tasks, creation tasks, investigation tasks. They should demonstrate different decomposition strategies: top-down, bottom-up, hybrid. They should show appropriate granularity for different task complexities. Curating a good example set requires effort, but it pays off in more reliable decomposition.

Another technique is chain-of-thought decomposition prompting. Instead of asking the agent to directly produce a decomposition, ask it to reason through the decomposition process: "First, what is the high-level goal? Second, what are the major components of this goal? Third, for each component, what information is needed? Fourth, where can that information be found? Fifth, what tools can retrieve it?" This structured reasoning often produces better decompositions than direct prompting.

You can also use iterative refinement. The agent produces an initial decomposition, then critiques it: "Is this decomposition complete? Are any steps vague? Are any steps unnecessary?" Based on the critique, the agent revises the decomposition. This self-critique loop improves decomposition quality at the cost of additional LLM calls.

Some systems use learned decomposition models. Instead of prompting a general-purpose LLM, you fine-tune a model specifically for decomposition on a dataset of goals and high-quality decompositions. This requires upfront investment in creating the dataset and training the model, but it can outperform prompting for domain-specific tasks. The fine-tuned model learns domain-specific decomposition patterns that are hard to encode in prompts.

## Decomposition in the Context of Hybrid Planning

When you use hybrid planning, decomposition happens at two levels: rough planning and reactive execution. During rough planning, you decompose the goal into high-level phases. During reactive execution within each phase, you decompose the phase goal into concrete actions. This two-level decomposition is easier than single-level decomposition because each level operates at a different abstraction.

For rough planning decomposition, the goal is to identify major phases, not to plan every action. "Research competitors" decomposes into "identify competitors," "gather information about each competitor," "compare to our product," and "summarize findings." Each phase is still abstract, but the decomposition provides structure. This decomposition is coarse-grained by design.

For reactive execution decomposition, the goal is to translate the current phase into immediate actions. When executing "identify competitors," the agent decomposes it reactively: "search industry database," then based on the results, "search for additional names on competitor tracking websites," then "deduplicate and rank by relevance." This decomposition is fine-grained and adaptive.

The two-level approach reduces the burden on each decomposition step. Rough planning does not need to worry about tool-level details. Reactive execution does not need to worry about the overall task structure. Each level focuses on its abstraction, making decomposition more tractable.

## Practical Implementation for Your Agent System

When you implement decomposition in your agent system, start by deciding whether you need explicit decomposition at all. For very simple tasks, you might skip decomposition and use direct prompting: "Use the search tool to find X, then summarize the results." For complex tasks, explicit decomposition is necessary.

If you use explicit decomposition, implement it as a separate reasoning step. Before executing any tools, the agent receives the goal and generates a decomposition. The decomposition is stored and used to guide execution. This separates planning from execution and makes both easier to implement and debug.

Provide clear decomposition guidelines in your prompts or system instructions. Specify the desired granularity, the format for representing steps, and any domain-specific decomposition patterns. If your tasks often involve data gathering followed by analysis, make that pattern explicit: "For research tasks, first gather data, then analyze it."

Use few-shot examples tailored to your domain. If you are building an agent for customer support, provide examples of support task decomposition. If you are building an agent for data analysis, provide examples of analysis task decomposition. Domain-specific examples are much more effective than generic examples.

Implement decomposition evaluation as part of your agent's workflow. After generating a decomposition, have the agent check it for completeness and correctness before execution. This adds latency but catches many decomposition errors early, saving the cost of executing a flawed plan.

Instrument your system to log decompositions and their outcomes. For each task, store the goal, the decomposition, and the execution results. Analyze patterns: which types of decompositions succeed, which fail, which are efficient, which are wasteful. Use this data to refine your prompts, examples, and decomposition strategies.

Finally, remember that decomposition is a skill that improves with practice and feedback. As you deploy your agent and observe its decomposition in production, you will identify patterns of errors and successful strategies. Feed this knowledge back into your prompts and examples. Decomposition is not a one-time design decision; it is an iterative process of learning what works for your specific tasks and domain.
