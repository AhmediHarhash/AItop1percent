# 8.10 â€” Hallucination Prevention in Agents: Grounding and Verification

In March 2025, a healthcare technology company deployed a medical documentation agent designed to summarize patient consultation notes for insurance claims. The agent read transcripts of doctor-patient conversations, extracted key clinical findings, and generated structured summaries that physicians reviewed before submitting to insurers. During a routine audit, a physician noticed that a summary included a medication dosage that had not been mentioned in the consultation. The physician checked the transcript. The medication was discussed, but the dosage was not specified. The agent had inferred a standard dosage and inserted it into the summary as if it had been stated by the doctor. The physician caught the error before submission, but a broader review revealed that 4.3 percent of all agent-generated summaries included similar fabricated details: test results rounded to typical values, diagnoses stated more definitively than the doctor had phrased them, and follow-up timelines specified when the doctor had said only "soon." None of these fabrications were malicious. The agent was trying to be helpful by filling in missing details with medically reasonable inferences. But in a clinical documentation context, fabrication is fraud. The company immediately suspended the agent, manually reviewed 18,000 previously submitted summaries, and spent $1.2 million on legal and compliance remediation. The technology worked. The agent reduced documentation time by 40 percent. But the hallucination problem made it unusable in any regulated context.

The root cause was not a flaw in the model's training. The root cause was the absence of grounding mechanisms that enforce the distinction between what the model observes in source data and what the model infers from its training. Hallucination is not a bug that you fix with better prompts. It is a fundamental property of language models: they generate plausible-sounding text based on statistical patterns, and they have no inherent mechanism to verify that the generated text corresponds to facts in the real world or facts in the provided context. Hallucination prevention is not optional for agents that produce outputs used in decision-making, compliance, or customer-facing communication. This subchapter explains how to design grounding and verification architectures that reduce hallucinations from common occurrences to rare exceptions.

## What Hallucination Means for Agents: Fabrication, Distortion, and Confabulation

Hallucination in language models refers to generating content that is not supported by the input context or by verifiable external facts. In agents, hallucination manifests in three forms: fabrication, distortion, and confabulation. Each form has different causes and requires different prevention strategies.

Fabrication is the invention of facts, entities, or events that do not exist in the source data. An agent summarizing a customer support ticket might include a refund amount that was never mentioned. An agent generating a financial report might cite a revenue figure that does not appear in the database query results. Fabrication happens because language models are trained to produce fluent, coherent text, and when the model lacks specific information needed to complete a sentence, it generates a plausible substitute rather than admitting ignorance. Fabrication is the most dangerous form of hallucination because the fabricated content is often plausible enough that human reviewers do not immediately recognize it as false.

Distortion is the misrepresentation of facts that do exist in the source data. An agent reads that a customer expressed frustration about a delayed shipment, and it summarizes the sentiment as "angry" rather than "frustrated." An agent reads that a financial transaction was flagged for review, and it reports that the transaction was blocked. Distortion happens because language models compress and paraphrase text, and in the compression process, nuance is lost. A fact that was stated with hedging, uncertainty, or qualification is restated as definitive. A fact that was presented as one option among many is restated as the chosen option. Distortion is harder to detect than fabrication because the distorted content is related to the source data and may even seem like a reasonable interpretation.

Confabulation is the generation of plausible-sounding explanations or reasoning that the agent did not actually perform. An agent is asked to explain why it recommended a particular action, and it generates a post-hoc rationalization that sounds logical but does not reflect the actual decision process. An agent is asked to cite sources for a claim, and it generates references that look like real citations but do not correspond to any actual documents. Confabulation happens because language models are trained to produce coherent narratives, and when asked to explain a decision, the model generates an explanation that fits the narrative structure, regardless of whether the explanation is grounded in the agent's actual reasoning.

All three forms of hallucination are exacerbated in agentic workflows because agents operate over multiple steps, and hallucinations in early steps propagate and compound in later steps. An agent fabricates a fact in step three, and in step five, it uses that fabricated fact as the basis for a decision. The final output appears coherent and well-reasoned, but it is built on a false premise. The longer the reasoning chain, the higher the probability that a hallucination occurs somewhere in the chain and contaminates the final result. Hallucination prevention must operate at every step, not just at the final output.

## Grounding Architectures: Retrieval, Attribution, and Closed-Domain Constraints

Grounding is the practice of constraining the agent's outputs to content that is directly derived from verified sources. A grounded agent does not generate freeform text. It extracts, quotes, and synthesizes content from source documents, and it provides explicit attribution for every claim it makes. Grounding architectures reduce hallucination by limiting the agent's ability to invent content.

The simplest grounding architecture is retrieval-augmented generation, where the agent queries a knowledge base, retrieves relevant documents, and generates outputs that are explicitly based on those documents. The agent does not rely on its parametric knowledge, the facts encoded in the model's weights from pretraining. It relies on the retrieved documents, which are curated, verified, and updated independently of the model. Retrieval-augmented generation is effective for factual question answering, for summarization tasks where the source material is well-defined, and for any task where correctness is more important than creativity.

Retrieval is not a guarantee against hallucination. A poorly designed retrieval system that returns irrelevant or low-quality documents will lead to outputs that are grounded in those documents but still incorrect. Grounding requires that the retrieval system returns high-precision results: the top-ranked documents must be relevant to the query, and they must contain accurate information. This means your knowledge base must be maintained with the same rigor as production data. Outdated documents, draft documents, or documents with known errors must be removed from the retrieval index. The retrieval ranking algorithm must be tuned to prioritize authoritative sources over speculative or opinion-based content.

Attribution is the mechanism that makes grounding auditable. An agent that produces a grounded output includes citations that show which parts of the output are derived from which source documents. The citations are not vague references like "according to company policy." They are specific pointers: document ID, section number, paragraph number, or even direct quotes with character offsets. A human reviewer or an automated verification tool can follow the citation back to the source document and confirm that the agent's output is a faithful representation of the source content.

Attribution is implemented by requiring the agent to emit structured outputs where each claim is paired with a source reference. Instead of generating a paragraph of freeform text, the agent generates a list of claim-source pairs. Each claim is a single factual statement, and each source is a reference to the document, chunk, or sentence from which the claim was extracted. The final output is assembled by rendering the claims as coherent prose, with footnotes or inline citations linking back to the sources. If the agent cannot find a source for a claim, it does not include the claim in the output. This is a hard constraint: no source, no claim.

Attribution also enables automated verification. You can implement a verification tool that takes the agent's output, parses the citations, retrieves the cited sources, and checks that each claim is actually supported by the cited source. This is done using a separate model or a rule-based checker that performs sentence-level entailment: does the source sentence entail the claim sentence? If the entailment check fails, the claim is flagged as unsupported, and the agent's output is rejected or sent for human review. Automated verification is not perfect, but it catches egregious hallucinations where the cited source contradicts the claim or where the citation is fabricated.

Closed-domain constraints are architectural restrictions that prevent the agent from generating content outside a predefined scope. A closed-domain agent operates on a fixed schema, a controlled vocabulary, or a limited set of allowed outputs. An agent that extracts structured data from invoices does not generate freeform descriptions of invoice contents. It extracts specific fields: invoice number, date, vendor name, line items, amounts. Each field has a defined type and validation rules. The agent cannot hallucinate a field that is not in the schema. It can extract incorrect values, but it cannot invent new fields or new record types.

Closed-domain constraints are particularly effective for agents that produce outputs consumed by downstream systems. A database update agent that writes records to a production table operates on a fixed schema. A workflow automation agent that triggers actions in external APIs operates on a fixed set of action types and parameters. The schema or API contract is the grounding mechanism. The agent cannot produce outputs that violate the schema or the contract. If the agent's reasoning leads it to attempt an invalid operation, the tool call fails with a validation error, and the agent is forced to revise its approach or request human guidance.

Closed-domain constraints reduce hallucination by reducing the degrees of freedom. The agent is not generating open-ended text where any plausible sentence might be accepted. It is generating structured data where every field is validated against a schema. The validation layer is the hallucination filter. This does not prevent the agent from extracting incorrect values, but it prevents the agent from inventing fields, types, or structures that do not exist in the target system.

## Verification Strategies: Redundancy, Cross-Checking, and Consistency Checks

Grounding reduces the opportunity for hallucination. Verification detects hallucinations that slip through grounding mechanisms. Verification strategies operate on the agent's outputs and validate that those outputs meet correctness criteria before they are used.

Redundancy is the simplest verification strategy. You run the same task multiple times with slightly different prompts, different sampling parameters, or different model instances, and you compare the outputs. If all runs produce the same result, you have higher confidence that the result is correct. If the runs produce different results, you flag the task for human review or you use a voting mechanism to select the most common result. Redundancy works because hallucinations are stochastic. The model's fabrications vary across runs, but facts grounded in source data are consistent.

Redundancy is expensive. Running the same task three times triples your inference cost and latency. But for high-stakes tasks where errors are costly, redundancy is a worthwhile investment. A financial reconciliation agent that processes million-dollar transactions runs each reconciliation three times and compares the outputs. If all three runs agree, the transaction is auto-approved. If any run disagrees, the transaction is flagged for manual review. The cost of running three inferences is negligible compared to the cost of a single incorrect transaction.

Redundancy can also be implemented with model diversity. Instead of running the same model three times, you run three different models: GPT-4o, Claude 3.5 Sonnet, and Llama 3. Each model has different training data, different architectural biases, and different hallucination patterns. If all three models produce the same result, you have strong evidence that the result is correct. If the models disagree, you use a tie-breaking mechanism: you defer to the model with the best historical accuracy on this task type, or you escalate to a human reviewer.

Cross-checking is a verification strategy where the agent's output is validated against a secondary source. The agent generates a summary of a legal document, and the summary is cross-checked against the original document by a separate verification model. The verification model is prompted to identify discrepancies: claims in the summary that are not supported by the source document, facts in the source document that are missing from the summary, or facts that are distorted in the summary. The verification model does not need to be as capable as the generation model. It only needs to perform entailment checking and factual alignment. You can use a smaller, faster model for verification, or you can use a rule-based checker if the task has well-defined validation rules.

Cross-checking is particularly effective for summarization tasks, extraction tasks, and any task where the agent's output is derived from a single authoritative source. The source is the ground truth. The agent's output is validated against the ground truth. If the validation fails, the output is rejected, and the agent is given feedback: "Your summary claims X, but the source document states Y. Revise your summary to accurately reflect the source." The agent retries the task with the feedback incorporated, and the revised output is cross-checked again. This iterative refinement continues until the output passes validation or until a retry limit is reached.

Consistency checks validate that the agent's outputs are internally consistent and consistent with known facts about the world or the domain. An agent generates a financial report that includes revenue, cost, and profit figures. A consistency check verifies that profit equals revenue minus cost. If the arithmetic does not hold, the report is flagged as invalid. An agent generates a timeline of events. A consistency check verifies that events are ordered chronologically and that no event occurs before its prerequisites. Consistency checks are implemented as automated rules, constraints, or verification queries. They do not require additional model calls. They are fast, deterministic, and highly reliable.

Consistency checks are domain-specific. A consistency check for financial data validates accounting identities. A consistency check for medical data validates that prescribed medications are not contraindicated for the patient's conditions. A consistency check for logistics data validates that shipment routes are physically possible given time and distance constraints. You define consistency rules during problem framing, based on domain knowledge and business logic. These rules are encoded as verification functions that run on every agent output. Outputs that fail consistency checks are rejected before they can cause downstream errors.

Consistency checks also apply to the agent's reasoning trace. If the agent claims it performed a database query and the query returned zero results, but then the agent proceeds to summarize the query results, the reasoning trace is inconsistent. A consistency checker flags this as an error. The agent either retries the task or escalates to a human. Reasoning trace consistency checks catch logical errors that would not be detected by output-only verification.

## Confidence Scoring and Uncertainty Quantification: When the Agent Does Not Know

Hallucinations often occur because the agent is uncertain but expresses its outputs with false confidence. An effective hallucination prevention strategy includes explicit uncertainty quantification. The agent does not just produce an output. It produces an output with a confidence score that reflects its certainty about the output's correctness.

Confidence scores are not the same as model logits or perplexity. Logits measure how surprising the generated tokens are given the prompt. High logits indicate fluent, expected outputs. But fluent outputs can be hallucinations. A model hallucinates most fluently when it invents content that fits smoothly into the narrative. What you need is a confidence score that measures the agent's epistemic certainty: does the agent have sufficient evidence to support this claim?

Epistemic confidence is estimated by prompting the agent to self-assess. After generating an output, the agent is asked: "On a scale from 0 to 100, how confident are you that this output is correct? If you are uncertain about any part of the output, explain why." The agent generates a confidence score and a justification. The justification is reviewed to identify the sources of uncertainty: missing data, ambiguous inputs, conflicting information in source documents, or lack of domain knowledge. The confidence score is used to route outputs: high-confidence outputs proceed directly to production, medium-confidence outputs are sent for human review, and low-confidence outputs are rejected or escalated to more capable models.

Self-assessed confidence is not perfectly calibrated. Models tend to be overconfident: they assign high confidence scores to incorrect outputs and medium confidence scores to correct outputs. Calibration is improved through training. You collect a dataset of agent outputs, human correctness judgments, and the agent's self-assessed confidence scores. You train a calibration model that learns to predict the true probability of correctness given the agent's confidence score and other features of the output. The calibrated confidence score is used for routing decisions. A well-calibrated confidence scorer allows you to set thresholds that achieve target precision levels: if you require 95 percent precision for auto-approved outputs, you set the confidence threshold such that empirically 95 percent of high-confidence outputs are correct.

Confidence scoring also enables partial outputs. If the agent is highly confident about some parts of the output but uncertain about other parts, it can produce a partial output with the uncertain parts marked as unknown or flagged for human completion. A medical documentation agent that is confident about the chief complaint and the physical exam findings but uncertain about the diagnosis generates a partial summary with the diagnosis field left blank and a note: "Diagnosis not clearly stated in transcript. Requires physician input." The physician completes the missing field, and the summary is finalized. Partial outputs are better than full hallucinated outputs because they make the uncertainty explicit and invite human correction.

Uncertainty quantification also applies to tool calls. When the agent calls a tool, it can request that the tool return not just the result but also metadata about the reliability of the result. A database query tool returns the query results and a flag indicating whether the query executed successfully, whether the results are complete, or whether the query hit a timeout and returned partial results. An API call tool returns the API response and an HTTP status code. The agent incorporates this metadata into its reasoning and into its confidence assessment. If a tool call failed or returned partial results, the agent lowers its confidence in any downstream outputs that depend on that tool call.

## Adversarial Testing for Hallucination: Red Teaming Agent Outputs

Hallucination prevention is validated through adversarial testing. You deliberately construct inputs designed to elicit hallucinations, and you measure how often the agent produces grounded, verified outputs versus fabricated or distorted outputs. Adversarial testing for hallucination is distinct from adversarial testing for safety. Safety adversarial tests focus on harmful outputs. Hallucination adversarial tests focus on incorrect outputs.

Adversarial test cases for hallucination include ambiguous inputs, incomplete inputs, and inputs with conflicting information. An ambiguous input is one where multiple interpretations are plausible, and the agent must choose one or explicitly state the ambiguity. A customer support ticket that says "the order is wrong" could mean the wrong items were shipped, the wrong quantity was shipped, the billing amount is wrong, or the delivery address is wrong. A hallucination-prone agent picks one interpretation and generates a response as if that interpretation is certain. A grounded agent acknowledges the ambiguity and asks clarifying questions before proceeding.

Incomplete inputs are those where key information needed to complete the task is missing. An invoice extraction agent is given an invoice image where the total amount is not visible due to image quality. A hallucination-prone agent infers a total amount by summing the line items and inserts that inferred amount as if it appeared on the invoice. A grounded agent flags the total amount as missing and outputs a structured record with the total field marked as null or with a confidence score of zero.

Inputs with conflicting information contain facts that contradict each other. A financial report states that Q1 revenue was $10 million in one section and $12 million in another section. A hallucination-prone agent picks one number and generates a summary that glosses over the contradiction. A grounded agent detects the conflict, flags it in the output, and either escalates to a human or requests additional information to resolve the discrepancy.

Adversarial test cases also include inputs designed to exploit the agent's parametric knowledge. The agent is given a prompt that includes a factual question where the correct answer is in the source documents but the agent's pretraining knowledge suggests a different answer. A grounded agent prioritizes the source documents. A hallucination-prone agent defaults to its parametric knowledge. For example, the source document states that the company's fiscal year ends in March, but the agent's pretraining knowledge assumes fiscal years end in December. If the agent is asked to calculate annual revenue and it uses December as the fiscal year-end, it has hallucinated by relying on parametric knowledge instead of grounding in the source document.

Red teaming for hallucination is performed by domain experts who understand the task and the failure modes. The experts craft inputs that stress-test the agent's grounding and verification mechanisms. They create scenarios where the correct output requires careful reading, cross-referencing multiple sources, or explicitly stating uncertainty. They review the agent's outputs and classify them as correct, incorrect due to fabrication, incorrect due to distortion, or incorrect due to confabulation. The red teaming results inform prompt improvements, grounding mechanism enhancements, and verification rule additions.

Red teaming is ongoing. As the agent is deployed and as new edge cases are discovered in production, those edge cases are added to the adversarial test suite. The agent's performance on the adversarial test suite is tracked over time. If performance degrades after a model update or a prompt change, the update is rolled back or revised. Adversarial test pass rate is a lagging indicator of hallucination risk. A high pass rate means the agent is resilient to known hallucination-inducing inputs. A low pass rate means the agent requires further grounding and verification work before it is production-ready.

## Human-in-the-Loop for High-Stakes Outputs: Review, Edit, and Feedback

For tasks where hallucination risk is high and where errors are costly, the final defense is human-in-the-loop review. The agent produces an output, and a human reviews the output before it is finalized or sent to the end user. The human is not proofreading for grammar or style. The human is validating factual correctness, verifying that claims are grounded in source data, and checking that the output does not include fabricated or distorted content.

Human-in-the-loop review is most effective when the agent's output is presented alongside the source materials. The human sees the agent's summary and the original document side by side. The human sees the agent's extracted data fields and the source invoice or form. This parallel presentation makes discrepancies immediately visible. If the agent claims the invoice total is $10,350 but the invoice shows $10,530, the human catches the error in seconds. If the agent summarizes a customer sentiment as "satisfied" but the customer's message includes phrases like "disappointed" and "frustrated," the human corrects the sentiment.

Human review is not a bottleneck if it is designed into the workflow from the beginning. The agent does the heavy lifting: it reads the source material, extracts or generates the initial output, and structures the information. The human does the verification: confirms correctness, fixes errors, and approves the output. This division of labor is faster than having the human perform the entire task manually. A medical documentation agent reduces documentation time from 15 minutes per patient to 3 minutes: 2 minutes for the agent to generate the draft, 1 minute for the physician to review and approve. The time savings is still 80 percent, even with human review.

Human review also generates training data. When the human edits the agent's output, the edits are logged as correction examples. The original agent output, the human-corrected output, and the diff between them are stored. These correction examples are used to fine-tune the agent's model, to improve prompts, or to adjust grounding and verification rules. Over time, the frequency of corrections decreases as the agent learns from its mistakes. The goal is not to eliminate human review. The goal is to reduce the review burden to the point where it is a lightweight verification step rather than a heavy editing task.

Feedback loops from human review also improve confidence calibration. When a human corrects an agent output, you record the agent's original confidence score for that output. If the agent assigned high confidence to an output that required significant corrections, the agent was overconfident. This data is used to recalibrate the confidence model. You also track which types of errors are caught by human review: fabrications, distortions, confabulations, or simple extraction mistakes. Error type frequency informs where to invest in better grounding mechanisms, better verification rules, or better adversarial testing.

Human-in-the-loop is not sustainable at arbitrary scale. If you are processing millions of documents per day, you cannot review every output. This is where confidence-based routing becomes essential. High-confidence outputs are auto-approved and logged for spot-check audits. Medium-confidence outputs are sent for human review. Low-confidence outputs are rejected or escalated to more capable models. The confidence thresholds are tuned to balance throughput and error rate. If human reviewers are finding errors in 20 percent of the outputs they review, the confidence threshold is too low. You raise the threshold so that only the most uncertain outputs are sent for review. If human reviewers are finding errors in less than 2 percent of reviewed outputs, the threshold is too high. You lower the threshold to catch more errors before they escape to production.

## Grounding Trade-Offs: Creativity Versus Correctness

Grounding and verification mechanisms reduce hallucination, but they also constrain the agent's outputs. A heavily grounded agent that only outputs content directly extracted from source documents cannot synthesize insights, cannot generalize across examples, and cannot produce creative or novel solutions. The trade-off between creativity and correctness is managed through task-specific grounding policies.

For tasks where correctness is paramount and creativity is not required, you enforce strict grounding. A legal document summarization agent is not allowed to paraphrase, infer, or generalize. It extracts verbatim quotes and organizes them into a structured summary. An invoice processing agent does not infer missing fields. It extracts only the fields that are explicitly present in the invoice and marks all other fields as missing. Strict grounding eliminates hallucination but also eliminates any possibility of the agent filling gaps or resolving ambiguities.

For tasks where some degree of inference or synthesis is necessary, you implement partial grounding. The agent is allowed to make inferences, but those inferences must be explicitly marked as such, and they must be accompanied by confidence scores. A customer support agent that reads a ticket and infers the customer's intent is allowed to state the inferred intent, but it must also include the verbatim customer message and flag the inference as an interpretation rather than a direct quote. The human or downstream system can then decide whether to trust the inference or rely on the original message.

For tasks where creativity is required, such as drafting marketing copy, generating product recommendations, or brainstorming solutions to open-ended problems, you relax grounding constraints but compensate with stronger verification and human review. The agent is allowed to generate novel content that is not directly extracted from source data, but that content is reviewed for factual accuracy, brand consistency, and alignment with guidelines. Creativity is not an excuse for fabrication. A marketing agent that invents product features that do not exist is producing hallucinations, not creativity.

The grounding policy is specified during problem framing. You document which parts of the agent's output must be strictly grounded, which parts can include inferences, and which parts can be creative. You also document the verification mechanisms that apply to each output type. This grounding policy is encoded in the agent's prompt, in its tool definitions, and in its output schema. The policy is tested during evaluation and is enforced at runtime through automated verification checks.

## Model Selection and Prompting Techniques for Hallucination Reduction

Not all models hallucinate at the same rate. Model selection is a lever for hallucination prevention. As of January 2026, models vary significantly in their propensity to fabricate content, in their ability to admit uncertainty, and in their adherence to grounding constraints. When hallucination risk is high, you choose models with demonstrated low hallucination rates on your specific task type.

GPT-4o and Claude 3.5 Sonnet are generally strong at following grounding instructions and at admitting when they do not have sufficient information to answer a question. Llama 3 and Gemini 2 models have improved hallucination rates compared to earlier generations, but they still require more aggressive grounding and verification than the frontier models. Model choice is validated through empirical testing on your task. You run the same evaluation set through multiple models and measure hallucination rate, defined as the percentage of outputs that include fabricated, distorted, or confabulated content. The model with the lowest hallucination rate, subject to cost and latency constraints, is selected for production deployment.

Prompting techniques also influence hallucination rates. Instructing the model to "only use information from the provided documents" reduces reliance on parametric knowledge. Instructing the model to "state if you are uncertain or if information is missing" increases the frequency of explicit uncertainty statements. Instructing the model to "cite your sources" increases attribution accuracy. These instructions must be tested empirically. Some models ignore grounding instructions when the task pressure is high or when the prompt is ambiguous. You validate that the model actually follows the grounding instructions by measuring adherence on a held-out test set.

Chain-of-thought prompting can reduce hallucination by making the agent's reasoning explicit. Instead of generating a final answer directly, the agent generates a reasoning trace that shows how it arrived at the answer, which sources it consulted, and which inferences it made. The reasoning trace is easier to verify than a direct answer because it exposes the intermediate steps. If the reasoning trace includes an unjustified leap or an unsupported claim, that is visible in the trace and can be caught by verification tools or human reviewers.

However, chain-of-thought prompting can also increase hallucination if not properly constrained. A model that is prompted to generate a reasoning trace may fabricate reasoning steps to justify a conclusion it has already reached, leading to confabulation. To prevent this, you combine chain-of-thought with grounding instructions: "For each reasoning step, cite the source of the information you are using. If you are making an inference, state that explicitly and explain the basis for the inference." This combination of chain-of-thought and attribution reduces both direct hallucination in the final answer and confabulation in the reasoning trace.

Few-shot examples in the prompt can demonstrate grounded behavior. You include examples where the agent is given a task, correctly extracts information from source documents, cites those sources, and admits uncertainty when information is missing. The examples teach the model the expected behavior pattern. Few-shot examples are particularly effective when they include near-miss cases: examples where the model might be tempted to fabricate or infer, but the correct behavior is to state uncertainty or request additional information.

You do not rely on prompting alone. Prompting is a first-line defense. Grounding architecture, verification mechanisms, and human review are the subsequent layers of defense. A production-grade hallucination prevention system combines all of these layers into a defense-in-depth strategy.

The next subchapter addresses how to build comprehensive agent evaluation frameworks that measure not just task success but also safety, reliability, hallucination rate, and alignment with organizational policies across all the dimensions we have explored in this chapter.
