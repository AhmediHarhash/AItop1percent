# 9.5 — Production Agent Observability: Logging, Metrics, and Traces

Ninety-four percent success looks excellent until three enterprise clients discover that the six percent failure rate includes contracts with prohibited clauses that expose them to regulatory penalties. Then the success metric becomes irrelevant, and the only question that matters is: why did the system fail, and why did no one notice until customers found the errors weeks later? The answer is almost always the same. The team shipped an agent with top-level metrics—overall success rate, latency, error count—but no observability into the agent's internal execution. They could see that the agent processed 14,000 contracts, but they could not see which tool calls returned unexpected results, which reasoning steps produced low-confidence outputs, or which contracts triggered edge cases that should have been escalated to human review. Production observability is not a monitoring dashboard. It is the operational foundation that determines whether you detect failures before customers do, whether you can diagnose root causes without spending eleven days guessing, and whether you maintain stakeholder trust when things go wrong.

The failure was not in the agent's design. It was in the production operations model. The team had treated the agent as a black box: input goes in, output comes out, log whether it succeeded or failed. That approach works for stateless APIs with deterministic logic. It does not work for agentic systems where the path from input to output involves multi-step reasoning, tool orchestration, external API calls, iterative refinement, and probabilistic decision-making at every node. You cannot debug what you cannot see, and you cannot optimize what you do not measure. Production agent observability is not a nice-to-have feature you add after launch. It is the operational foundation that determines whether you can detect failures, diagnose root causes, optimize performance, and maintain stakeholder trust when things go wrong. This subchapter defines the three-layer observability model for production agents: structured logging that captures the full reasoning trace, metrics that quantify system health and step-level performance, and distributed tracing that reconstructs end-to-end execution paths across tools, APIs, and model calls.

## The Three-Layer Observability Model

Production observability for agents requires three complementary data streams, each serving a distinct diagnostic purpose. The first layer is structured logging: timestamped, machine-parsable records of every significant event in the agent's execution, including tool calls, model requests, reasoning steps, confidence scores, fallback triggers, and error conditions. Structured logs are your primary diagnostic tool when investigating individual failures or reconstructing what happened during a specific session. The second layer is metrics: aggregated quantitative measurements of system performance, such as latency distributions, success rates, tool call counts, token consumption, confidence score distributions, and error rates by category. Metrics are your health monitoring layer, the data you use to detect degradation, identify trends, and trigger alerts before failures become customer-visible. The third layer is distributed tracing: end-to-end execution graphs that show the causal path through all services, model calls, and tool invocations involved in processing a single request. Traces are your performance profiling and dependency analysis tool, the data you use to find bottlenecks, understand cross-service latency, and diagnose failures that span multiple systems.

Many teams build only the first layer, capturing logs without metrics or traces. This gives you the ability to reconstruct individual failures after they happen, but it does not give you early warning when performance degrades, and it does not give you the quantitative data you need to optimize the system. Other teams build only the second layer, tracking high-level success rates and latency percentiles without capturing the detailed logs needed to diagnose why a specific session failed. This gives you alerting and trend analysis, but when an alert fires, you have no forensic data to investigate the root cause. The three layers are not redundant. They are complementary perspectives on the same system, each optimized for different operational questions. Structured logs answer "what happened in this specific session?" Metrics answer "how is the system performing overall?" Traces answer "where is time being spent and where are dependencies failing?" You need all three.

The cost of missing any layer is measured in time to detection, time to diagnosis, and time to resolution. Without structured logs, you cannot reconstruct individual failures, which means every production issue requires reproducing the failure in a test environment before you can begin diagnosing it. Without metrics, you cannot detect gradual degradation until it crosses a threshold that triggers customer complaints, which means you lose the opportunity to fix issues before they impact users. Without distributed tracing, you cannot isolate performance bottlenecks in multi-service workflows, which means optimization becomes guesswork rather than data-driven decision-making. A three-layer observability model is not over-engineering. It is the minimum viable infrastructure for operating agentic systems at production scale.

## Structured Logging for Agent Execution

Structured logging means every log entry is emitted as a machine-parsable data structure with consistent fields, not as free-form text strings. The standard format is JSON lines: one JSON object per line, each containing a timestamp, log level, event type, session identifier, and event-specific fields. For agent systems, every significant event in the execution lifecycle must generate a log entry. Significant events include agent session start and end, each tool call with arguments and results, each model invocation with prompt hash and response metadata, each reasoning step with intermediate outputs, each confidence score calculation, each fallback trigger, and each error or exception. This is not optional. If an event is not logged, it does not exist from an operational perspective.

Start with session-level logging. Every agent execution begins with a session start event that logs the session identifier, user or client identifier, input task description, configuration parameters, and timestamp. The session identifier is a unique value that ties all subsequent log entries for this execution together, enabling you to filter logs to reconstruct a single session. Session start events also log the agent version, model version, and environment identifier so you can correlate failures with specific deployments or configuration changes. When the session completes, emit a session end event that logs the final status, total duration, total tokens consumed, total tool calls made, and any top-level success or error conditions. Session start and end events give you the outer boundary of each execution and the high-level summary metrics.

Between session start and end, log every tool call as a discrete event. A tool call event includes the tool name, input arguments, timestamp when the call was initiated, timestamp when the call returned, result status, result data or error message, and latency. Tool call logging is critical because tools are where agents interact with external systems, and external systems are where most production failures originate. If a tool call fails, you need to know which tool, what arguments were passed, what error was returned, and how long the call took. If a tool call succeeds but returns unexpected data, you need the result data captured in logs so you can diagnose why the agent's subsequent reasoning was incorrect. Tool call logs also enable you to track tool-level success rates, latency distributions, and usage patterns, which are essential for capacity planning and dependency health monitoring.

Log every model invocation with prompt metadata and response metadata. You do not log the full prompt text in every log entry, because prompts can be thousands of tokens and logging them verbatim creates cost and storage issues. Instead, log a hash of the prompt, the model name and version, the temperature and sampling parameters, the timestamp, the response token count, the finish reason, and any usage metadata returned by the API. If the model invocation fails, log the error type, error message, and retry behavior. Prompt hashing allows you to correlate model responses with specific prompt templates without storing redundant prompt text. If you later need to investigate why a specific model call produced a certain output, you can look up the prompt hash in your prompt version registry and reconstruct the full prompt used. Model invocation logs give you the data needed to track model-level latency, token consumption, error rates, and finish reason distributions, all of which are leading indicators of performance degradation or API issues.

Log reasoning steps and intermediate outputs. For multi-step agents, the reasoning process is the most complex and least transparent part of the system. If the agent generates a plan, log the plan structure. If the agent evaluates multiple options and selects one, log the options considered and the selection criteria. If the agent computes a confidence score, log the score and the factors that contributed to it. If the agent decides to invoke a fallback or escalate to human review, log the trigger condition and the decision rationale. Reasoning step logs are the data you use to understand why the agent made a particular decision, which is essential for diagnosing cases where the output was incorrect but no technical error occurred. These logs also enable you to build metrics on reasoning quality, such as how often the agent triggers fallback, how often it selects the correct option on the first attempt, and how confidence scores correlate with actual success rates.

Emit logs in real time, not in batch at the end of execution. Real-time logging means each event is written to the logging system immediately after it occurs, not buffered in memory and flushed when the session completes. Real-time logging is critical for debugging failures that cause the agent process to crash or time out, because if logs are buffered, a crash means you lose all buffered data and have no record of what happened before the crash. Real-time logging also enables you to monitor in-flight sessions, which is useful for detecting sessions that are taking longer than expected or consuming more resources than budgeted. The performance cost of real-time logging is negligible compared to the diagnostic value it provides.

## Metrics for System Health and Performance

Metrics are aggregated measurements computed from logs and emitted to a time-series monitoring system. While logs give you event-level detail, metrics give you statistical summaries: counts, rates, percentiles, distributions. The difference is granularity and query performance. You cannot efficiently query millions of log entries to answer questions like "what was the 95th percentile latency for tool calls over the last hour?" but you can efficiently query a time-series metric that pre-aggregates latency into percentile buckets. Metrics are your real-time health dashboard and your alerting foundation. If you do not have metrics, you have no systematic way to detect when the system is degrading.

Start with session-level metrics. Track session count, session success rate, session duration at p50, p95, and p99, and session error rate by error type. These are your top-level health indicators. If session success rate drops from 96% to 89% over the course of a day, that is a signal that something in the system has degraded, even if no single failure is severe enough to trigger a customer complaint. If p95 session duration increases from 8 seconds to 23 seconds, that is a signal that performance has regressed, even if the system is still meeting SLA. Session-level metrics tell you whether the system as a whole is healthy and whether performance is stable or degrading.

Track tool-level metrics. For each tool, emit metrics on call count, success rate, latency percentiles, and error rate by error type. Tool-level metrics enable you to isolate which dependency is causing failures or latency. If your agent calls twelve different tools and session latency suddenly increases, tool-level metrics tell you which of the twelve tools is the bottleneck. If session success rate drops and tool-level metrics show that one specific tool's error rate has spiked from 1% to 18%, you know the root cause is in that tool's upstream service, not in the agent logic or model performance. Tool-level metrics also enable you to track usage patterns, which is essential for capacity planning. If a specific tool is called 10,000 times per day and you plan to scale the agent to 10x traffic, you need to know whether that tool's upstream service can handle 100,000 calls per day.

Track model-level metrics. For each model you use, emit metrics on invocation count, token consumption, latency percentiles, error rate, and finish reason distribution. Model-level metrics tell you whether the model API is performing consistently and whether your token consumption is within budget. If model latency p95 suddenly increases from 1.2 seconds to 4.7 seconds, that is a signal that the model provider is experiencing performance issues, and you may need to implement fallback to a different model or provider. If token consumption increases by 40% without a corresponding increase in session count, that is a signal that something in your prompt logic has changed and is generating longer prompts or responses than expected. Finish reason distribution is a particularly valuable metric: if the proportion of responses finishing with "length" instead of "stop" increases, that is a signal that the model is hitting token limits and truncating responses, which often leads to incomplete or incorrect outputs.

Track reasoning quality metrics. These are derived from intermediate reasoning steps logged during execution. For example, if your agent computes confidence scores for each output, emit a metric on confidence score distribution. If confidence scores are normally distributed around 0.88 and suddenly shift to a distribution around 0.62, that is a signal that the agent is less certain about its outputs, which often precedes an increase in error rate. If your agent uses a fallback mechanism that triggers human review when confidence is below a threshold, emit a metric on fallback trigger rate. If fallback rate increases from 3% to 11%, that is a signal that the agent is encountering more ambiguous cases or that the task distribution has shifted. Reasoning quality metrics are leading indicators: they tell you when the system is struggling before failures become visible in top-level success rates.

Emit metrics with dimensional labels that enable you to slice by client, task type, geography, or other relevant dimensions. For example, if you track session success rate as a single global number, you cannot detect a failure that only affects one client or one task type. If you track session success rate with labels for client ID and task type, you can detect that success rate is 96% overall but only 78% for contract review tasks submitted by client X, which tells you the failure is specific to that client or task type rather than a system-wide issue. Dimensional metrics enable you to isolate failures to specific segments and diagnose root causes faster.

## Distributed Tracing for End-to-End Visibility

Distributed tracing instruments each request as it flows through multiple services, creating a directed acyclic graph that shows the causal relationship between all operations involved in processing that request. Each operation is represented as a span, which records the operation name, start time, end time, parent span, and metadata. Spans are linked together to form a trace, which represents the full execution path for a single request. For agent systems, a trace starts when the agent session begins and includes spans for every model call, every tool invocation, every external API request, and every significant internal operation. Tracing is essential for multi-service architectures where a single agent execution involves calls to model APIs, internal microservices, third-party APIs, databases, and caching layers.

The value of distributed tracing is in dependency analysis and latency attribution. Logs tell you what happened, metrics tell you how often it happened, but traces tell you where time was spent and how operations depend on each other. If your agent's p95 latency is 12 seconds and you want to reduce it, traces show you which operations consumed the most time: was it the model call, the tool invocations, the database lookups, or the external API calls? If a session failed and you want to understand why, traces show you the sequence of operations leading to the failure and which service returned the error. Without tracing, you are diagnosing distributed system failures by manually correlating timestamps across multiple log streams, which is slow and error-prone.

Instrument every model call as a span. The span should record the model provider, model name, prompt token count, response token count, start time, end time, and any error or retry information. Model spans are often the longest-duration spans in an agent trace, and understanding model latency is critical for performance optimization. If a trace shows that a single agent session made seven model calls and the model spans consumed 9.2 of the 12-second total duration, you know that reducing model calls or switching to a faster model is the highest-leverage performance improvement.

Instrument every tool call as a span. The tool span should record the tool name, start time, end time, and a child span for every external API call or database query made by the tool. Tool spans often reveal cascading latency issues. For example, a tool call might appear to take 3 seconds, but the trace shows that the tool made four sequential API calls, each taking 0.7 seconds, meaning the latency is driven by sequential dependency rather than any single slow operation. This insight changes the optimization strategy: instead of trying to make individual API calls faster, you redesign the tool to parallelize the calls or batch them.

Instrument reasoning steps as spans. If the agent generates a plan, create a span for the planning step. If the agent evaluates multiple options, create a span for the evaluation step. Reasoning step spans are typically short duration, but they are useful for understanding where the agent is spending time on logic versus waiting for external dependencies. Reasoning spans also create structure in the trace, making it easier to understand the high-level flow of the agent's execution.

Link traces to logs using shared identifiers. Every log entry should include the trace ID and span ID for the operation that generated it, and every span should include a reference to the log entries emitted during that span. This linkage enables you to move fluidly between trace view and log view: when you identify a slow span in a trace, you can jump directly to the logs for that span to see detailed diagnostic information. When you find an error in logs, you can jump to the trace to see the full context of what the agent was doing when the error occurred.

Use trace sampling for high-traffic systems. If your agent processes 100,000 sessions per day, storing full traces for every session is expensive and often unnecessary. Instead, implement sampling: capture full traces for a percentage of sessions, and capture traces for all sessions that fail or exceed latency thresholds. For example, you might sample 5% of successful sessions, 100% of failed sessions, and 100% of sessions where duration exceeds p95. This approach ensures you have trace data for all anomalous executions while keeping storage costs manageable. Sampling must be consistent: if you sample a trace, you must sample all spans in that trace, not sample individual spans within a trace.

## Observability Tooling and Integration

Production observability requires choosing and integrating a logging system, a metrics system, and a tracing system. The specific tools you use depend on your infrastructure, scale, and budget, but the architectural principles are the same. Your logging system must support structured logs, real-time ingestion, fast querying by session ID and timestamp, and retention policies that balance cost with diagnostic needs. Common choices include Elasticsearch, Datadog, Splunk, or cloud-native solutions like AWS CloudWatch Logs or Google Cloud Logging. Your metrics system must support time-series data, dimensional labels, percentile aggregation, and low-latency querying for dashboards and alerting. Common choices include Prometheus, Datadog, Grafana Cloud, or cloud-native solutions like AWS CloudWatch Metrics or Google Cloud Monitoring. Your tracing system must support distributed tracing standards like OpenTelemetry, trace sampling, and trace-to-log correlation. Common choices include Jaeger, Datadog APM, Honeycomb, or cloud-native solutions like AWS X-Ray or Google Cloud Trace.

The three systems do not need to be from the same vendor, but using a unified observability platform simplifies integration and correlation. For example, Datadog provides logging, metrics, and tracing in a single platform with built-in correlation between the three data types. Unified platforms reduce the operational complexity of running multiple systems and make it easier to move between logs, metrics, and traces during incident investigation. The downside is vendor lock-in and cost: unified platforms are often more expensive than best-of-breed open-source tools. The trade-off depends on your team size and operational maturity. Small teams often benefit from unified platforms because they reduce operational overhead. Large teams with dedicated platform engineering often prefer best-of-breed tools because they provide more control and lower cost at scale.

Regardless of tooling, observability instrumentation must be integrated into the agent framework, not bolted on as an afterthought. Instrumentation should be automatic: every tool call, every model invocation, every reasoning step should emit logs, metrics, and trace spans without requiring the developer to manually add instrumentation code. This is typically achieved by building observability into the agent orchestration layer. For example, if your agent framework provides a tool calling interface, the framework itself should automatically emit a log entry, update tool call metrics, and create a trace span every time a tool is invoked. The developer who implements a new tool should not need to add any observability code. Automatic instrumentation ensures consistency, reduces developer toil, and prevents observability gaps where a new feature ships without logging or metrics.

Configuration must support environment-specific behavior. In development, you may want verbose logging, no sampling, and no metrics export. In staging, you may want production-like observability configuration to validate that instrumentation is working correctly. In production, you may want sampling, retention policies, and alerting configured. Environment-specific configuration is typically managed through environment variables or configuration files that the agent reads at startup. The same agent code should work in all environments with only configuration changes.

## Alerting and Incident Response

Observability data is only valuable if it drives action. Metrics must be connected to alerting rules that notify the team when the system degrades, and alerting must be connected to incident response procedures that define how the team investigates and resolves issues. Alerting rules should be based on metrics, not logs, because metrics are aggregated and can be evaluated efficiently in real time. Define alerting thresholds for session success rate, session duration percentiles, tool error rates, model error rates, and reasoning quality metrics like fallback trigger rate. For each metric, define both warning thresholds and critical thresholds. A warning threshold indicates degradation that should be investigated but is not yet customer-impacting. A critical threshold indicates degradation that is customer-impacting and requires immediate response.

Alerting rules should account for statistical noise. Do not alert on single data points. Alert on sustained degradation over a time window. For example, do not alert if session success rate drops below 95% for one minute. Alert if session success rate drops below 95% for five consecutive minutes or if the ten-minute moving average drops below 95%. Sustained degradation rules reduce false positives caused by transient spikes or measurement noise. Alerting rules should also account for traffic volume. If you alert on absolute error count, you will get false positives when traffic increases and false negatives when traffic decreases. Alert on error rate, which is normalized by traffic volume.

When an alert fires, the incident response procedure should follow a structured diagnostic flow. First, check the metrics dashboard to identify which metrics are out of range and whether the issue is isolated to a specific dimension like client, task type, or geography. Second, query logs for recent sessions that failed or exceeded latency thresholds to identify commonalities. Third, pull traces for those sessions to identify where latency was spent or where failures originated. Fourth, correlate the timing of the degradation with recent deployments, configuration changes, or external service incidents. Fifth, form a hypothesis about the root cause and validate it by reproducing the failure or analyzing additional sessions. Sixth, implement a fix or mitigation and validate that metrics return to healthy levels. This diagnostic flow moves from high-level metrics to detailed logs and traces, using each layer of observability to narrow the scope of investigation.

Incident response requires tooling that supports this flow. Your metrics dashboard should link directly to log queries filtered by time range and dimension. Your log queries should link to traces for the sessions shown in logs. Your traces should link back to logs for individual spans. This linkage enables rapid investigation without requiring the responder to manually copy session IDs or timestamps between tools. The faster you can move from alert to root cause, the faster you can resolve incidents and restore service quality.

## Privacy, Compliance, and Log Retention

Observability data often contains sensitive information, including user identifiers, task inputs, tool outputs, and reasoning steps. If your agent processes personal data, healthcare data, financial data, or other regulated information, your observability data is subject to the same privacy and compliance requirements as the application data. You cannot log user inputs verbatim if those inputs contain personal data covered by GDPR, and you cannot log tool outputs verbatim if those outputs contain protected health information covered by HIPAA. Privacy-preserving observability requires redacting or hashing sensitive fields before they are logged.

The standard approach is to define a set of field-level redaction rules. For example, log session identifiers and task type, but do not log the task description if it contains user-provided text. Log tool names and result status, but do not log tool arguments or result data if they contain sensitive information. Instead, log a hash of the data or a summary statistic like data length or data type. Redaction must be applied at the point of log emission, not later, because once sensitive data is written to a log stream, it is difficult to ensure it is not retained somewhere in the logging infrastructure.

For fields that must be logged for diagnostic purposes, use tokenization or pseudonymization. For example, replace user identifiers with pseudonyms that are consistent within a session but cannot be linked back to the real user without a separate mapping table that is access-controlled. This allows you to correlate all events within a session without exposing the actual user identity in logs. Tokenization and pseudonymization are more complex to implement than redaction, but they preserve more diagnostic value.

Log retention policies must balance operational needs with compliance requirements and cost. Detailed logs are expensive to store, and most logs are never queried after the first few days. Define a tiered retention policy: retain full logs for seven to fourteen days, retain sampled logs for thirty to ninety days, and retain aggregated metrics indefinitely. Retention periods should be longer for production environments than for staging or development. Retention policies must also account for compliance requirements: if GDPR requires you to delete user data within thirty days of a deletion request, your log retention policy must ensure that logs containing user data are deleted within that window.

Access control must restrict who can query observability data. Logs and traces often contain sensitive information, and access to observability systems must be treated with the same rigor as access to production databases. Implement role-based access control that grants access based on job function: engineers on the agent team can query all observability data, engineers on other teams can query only their own services, and customer support can query only sanitized summary data. Audit all queries to observability systems so you can detect unauthorized access or data exfiltration.

Observability is not optional infrastructure you add after the agent is working. It is the operational foundation that determines whether you can detect failures, diagnose root causes, optimize performance, and maintain trust when things go wrong. The legal tech company that lost $340,000 in service credits learned this the hard way. Production observability is the difference between operating agents as black boxes you hope will work and operating agents as instrumented systems you know are working and can fix when they are not. Next, we will examine how to use this observability data to profile agent latency and identify bottlenecks in multi-step workflows.
