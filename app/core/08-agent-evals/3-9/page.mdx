# 3.9 — Planning Under Uncertainty: Ambiguous or Incomplete Information

On November 14, 2025, an AI agent at a healthcare technology company called MedInsight was asked by a product manager to "evaluate the feasibility of adding real-time eligibility verification to our platform." The request was deliberately vague because the product manager herself did not know exactly what was feasible and wanted the agent to explore the space. The agent, running on a state-of-the-art planning system with access to technical documentation, API catalogs, and competitive analysis tools, generated a detailed twelve-step plan in forty-seven seconds. Step one: integrate with ClearingHouse API for eligibility checks. Step two: design caching layer for frequently-checked patients. Step three: implement retry logic for failed requests. The plan was specific, well-structured, and completely worthless. The agent had assumed that "real-time eligibility verification" meant checking patient insurance eligibility, that ClearingHouse was the right vendor, that caching was necessary, and that the existing platform architecture could support the integration. Every assumption was wrong. The product manager had meant verification of provider credentials, not patient insurance. ClearingHouse did not offer provider verification. The platform's authentication system was incompatible with any third-party APIs without major refactoring. The agent had transformed an ambiguous request into a concrete plan by making a series of unstated assumptions, and because none of those assumptions were validated, the plan was useless. The agent spent six additional hours executing the first three steps before a human noticed the problem, wasting two hundred twelve dollars and delaying the actual project by four days.

You will never have perfect information when planning. User requests are ambiguous because users do not know exactly what they want or do not have the technical knowledge to express it precisely. Context is incomplete because you do not have access to all relevant information, because some information does not exist yet, or because gathering it would be too expensive. Tool outputs are unreliable because APIs fail, data sources contain errors, and external systems behave unpredictably. The environment state is unknown because you cannot observe everything, because the state changes during planning, or because the state is fundamentally uncertain. Planning under uncertainty is not an edge case; it is the normal condition of agent systems in 2026. The difference between agents that succeed in production and agents that fail is not whether they face uncertainty, but how they handle it. Do they recognize when they are uncertain? Do they take actions to reduce uncertainty before committing to expensive decisions? Do they make assumptions explicit and validate them? Do they plan for multiple scenarios instead of betting everything on a single interpretation? These are the capabilities that separate robust agents from brittle ones.

## Sources of Uncertainty in Agent Planning

Uncertainty comes from four primary sources, each requiring different handling strategies. Ambiguous user requests are the most common source. Users say "research competitors" without specifying which competitors, which aspects to research, or what format they want the results in. They say "fix the bug" without specifying which bug or what constitutes a fix. They say "analyze customer feedback" without defining the time range, feedback sources, or analysis dimensions. This ambiguity is not laziness; it is inherent in natural language communication. Users assume shared context that does not actually exist, or they genuinely do not know the details until they see initial results.

Missing context is the second source. The agent does not know the user's broader goals, the constraints they are operating under, or the decisions they are trying to make. A request to "analyze Q4 sales data" means different things if the user is trying to set next year's targets versus if they are investigating why a product underperformed. The agent does not know which interpretation is correct without additional context. Missing context also includes technical details: which database contains the sales data, what schema it uses, which fields are reliable, which are known to have data quality issues. The agent could discover this information through exploration, but exploration takes time and costs money.

Unreliable tool outputs are the third source. APIs return errors, timeouts, or rate limit rejections. Web scraping tools encounter pages that have changed structure since the scraper was written. Database queries return stale data because replication lag has not caught up. Language models hallucinate facts that sound plausible but are incorrect. Search tools return irrelevant results because the query was poorly constructed or the index is incomplete. The agent plans based on the assumption that tools will work and return accurate data, but this assumption is violated constantly. A plan that does not account for tool unreliability will fail during execution.

Unknown environment state is the fourth source. The agent does not know whether a file exists until it tries to read it. It does not know whether an API requires authentication until it calls the endpoint. It does not know whether two databases are in sync until it queries both and compares results. It does not know whether a computation will exceed memory limits until it runs the computation. Some of this uncertainty is reducible: you can check if a file exists before trying to read it. Some is irreducible: you cannot know if a web service will be available when you need it, only whether it is available right now.

Each source of uncertainty requires different mitigation strategies, but all of them share a common principle: the agent must recognize that it is uncertain. An agent that treats uncertain information as certain will make confident, wrong decisions. An agent that recognizes uncertainty can take actions to reduce it, make assumptions explicit, or hedge its plans to handle multiple scenarios.

## Strategies for Planning Under Uncertainty: Clarifying Questions

The most straightforward strategy for handling ambiguous user requests is to ask clarifying questions before committing to a plan. If the user says "research competitors," the agent can ask: "Which competitors would you like me to focus on?" "What aspects should I research: features, pricing, market positioning, customer reviews?" "What format would you like the results in: a summary report, a detailed comparison table, a list of key findings?" These questions transform an ambiguous request into a concrete specification.

The challenge is knowing when to ask questions and when to proceed with reasonable assumptions. If you ask too many questions, the user gets annoyed and feels like the agent is not smart enough to figure things out on its own. If you ask too few, you risk misinterpreting the request and wasting effort on the wrong task. The boundary is context-dependent and requires judgment. A good heuristic is to ask questions when the ambiguity would lead to significantly different plans. If "research competitors" could mean either analyzing features or analyzing market positioning, and those require completely different tools and data sources, ask. If the ambiguity is only about formatting details, make a reasonable assumption and proceed.

Another heuristic is to ask questions when the cost of getting it wrong is high. If the task will take two hours and cost fifty dollars, asking three clarifying questions upfront is a good investment. If the task will take thirty seconds and cost ten cents, just proceed with your best guess. The cost-benefit analysis should account for both direct costs like API calls and indirect costs like user frustration and delay.

When you do ask questions, make them specific and actionable. Avoid open-ended questions like "What do you want me to do?" which just push the entire specification burden back to the user. Ask questions that narrow the ambiguity: "Should I include international competitors or only US-based ones?" "Are you interested in their current product lineup or their product roadmap?" Questions should be answerable quickly and should materially reduce uncertainty.

One implementation approach is to use a two-phase workflow: clarification phase, then planning phase. In the clarification phase, the agent identifies ambiguities and generates a list of questions. The questions are presented to the user in a single batch, not one at a time, to minimize interaction rounds. The user answers all questions, and then the planning phase begins with the disambiguated request. This is more efficient than iterative questioning, where each question requires a full round-trip.

Another approach is to ask questions opportunistically during planning or execution. If the agent is planning the task and realizes it needs to know which competitors to focus on, it asks at that moment. If it is executing the plan and encounters ambiguity about data format, it asks before proceeding. This reduces upfront latency because the agent starts working immediately, but it increases total latency if questions pause execution.

In 2026, most production agent systems use a hybrid approach. They ask a small number of high-impact clarifying questions upfront, then proceed with assumptions, and pause to ask additional questions only if they encounter critical ambiguity during execution. The threshold for what constitutes "critical" is tunable based on the application's cost sensitivity and the user's tolerance for interruptions.

The implementation details matter significantly. Some systems use a confidence threshold: if the agent's confidence in its interpretation is below seventy percent, trigger question generation. Others use a branching factor heuristic: if the ambiguity could lead to three or more significantly different plans, ask for clarification. Still others use cost-based triggers: if the most expensive interpretation costs more than ten times the cheapest, verify which one is correct before proceeding.

Question presentation also varies. Consumer-facing agents tend to ask questions conversationally, one at a time, to maintain a natural dialogue feel. Enterprise agents batch questions into a single form to minimize interruptions for busy users. Some systems allow users to set preferences: verbose mode where the agent asks many questions, or autonomous mode where it makes reasonable assumptions and proceeds. The right approach depends on your user base and use cases.

Another consideration is question timing. Asking all questions upfront maximizes clarity but delays action, which frustrates users who want immediate results. Asking questions during execution provides faster initial response but risks interrupting flow. A middle ground is to ask critical questions upfront and defer nice-to-have clarifications until natural breakpoints in execution. If the agent finishes phase one and is about to start phase two, that is a natural point to ask "I am about to analyze pricing data; would you like me to include competitor promotional pricing or only list prices?"

The quality of questions is as important as the quantity. Bad questions are vague, unanswerable, or irrelevant. "What do you want?" is a bad question. "Should I include international competitors in the analysis?" is a good question because it is specific, binary or with clear options, and directly impacts the plan. Training agents to generate good questions requires examples and feedback. Show examples of ambiguous requests and high-quality clarifying questions. When the agent generates questions, have humans rate their quality. Use the ratings to fine-tune prompts or provide better examples.

## Strategies for Planning Under Uncertainty: Making and Validating Assumptions

When you cannot or should not ask clarifying questions, the alternative is to make assumptions and validate them. An assumption is an explicit statement about something uncertain: "I assume the user wants to analyze US-based competitors only." "I assume the sales data is in the revenue_2024 database table." "I assume the API uses JSON format." Making assumptions explicit has two benefits. First, it enables assumption validation: you can check whether the assumption is correct before committing to a plan that depends on it. Second, it makes the agent's reasoning transparent: if the plan turns out to be wrong, you can trace the error back to a bad assumption.

Assumption validation varies depending on the type of assumption. For assumptions about data, you can validate by querying the data source. If you assume the sales data is in a particular table, query the table schema to confirm. If you assume a file exists at a particular path, check if the path is valid. These validations are cheap and fast, usually a single tool call, and they dramatically reduce the risk of planning based on false assumptions.

For assumptions about user intent, validation is harder. You can infer intent from context: if the user previously asked about US markets, assume they want US competitors. You can use heuristics: if the user does not specify a time range, assume they want recent data. You can make conservative assumptions that are likely to be correct: if the user asks for a report, assume they want a summary rather than raw data. But you cannot validate these assumptions with certainty without asking the user, which brings you back to clarifying questions.

For assumptions about external systems, validation often requires trying the operation and seeing if it works. If you assume an API accepts JSON, try calling it with JSON and check the response. This is more expensive than querying metadata, but it is definitive. A pattern called "fail-fast validation" is useful here: before generating a full plan, perform minimal validation of critical assumptions. If the validation fails, revise the assumptions or ask for clarification. If the validation succeeds, proceed with the plan.

Some systems implement assumption confidence scoring. Each assumption is tagged with a confidence level: high, medium, low. High-confidence assumptions are based on strong evidence and do not need validation. Low-confidence assumptions are guesses and must be validated before use. Medium-confidence assumptions are validated if the cost is low but used without validation if validation is expensive. This allows the agent to allocate validation effort where it has the most impact.

Another technique is assumption dependency tracking. The plan explicitly notes which steps depend on which assumptions. If assumption A is later found to be false, you know which steps need to be revised. This is more sophisticated than most 2026 systems implement, but it is valuable in high-stakes applications where you need to trace the impact of changed assumptions through a complex plan.

When you communicate plans to users, make key assumptions explicit in the plan summary. "I will analyze US-based competitors, assuming you are interested in the domestic market. If you need international competitors, let me know." This gives the user a chance to correct wrong assumptions before execution begins. It also builds trust by showing that the agent is aware of its uncertainties rather than pretending to have perfect information.

The format for communicating assumptions matters. Burying assumptions in fine print at the end of a plan makes them easy to miss. Highlighting them at the beginning ensures visibility. Some systems use a structured format: "Based on your request, I plan to X. This plan assumes: A, B, and C. If any of these assumptions are incorrect, please let me know before I proceed." This makes assumptions impossible to overlook.

Visual formatting helps. Use bold text, bullet points, or color coding to make assumptions stand out. In a user interface, consider using an expandable "Assumptions" section that is collapsed by default but visible. This allows advanced users to review assumptions while not overwhelming casual users with details.

Assumption tracking across conversations is valuable for recurring tasks. If a user asks to "analyze competitors" every week, the agent can reference assumptions from previous executions: "Last time, you wanted US-based software companies with over fifty employees. Should I use the same criteria this time?" This reduces the need for repeated clarification and makes the interaction more efficient.

You can also implement assumption learning. Track which assumptions are frequently corrected versus which are accepted. If ninety percent of users accept assumption A but fifty percent correct assumption B, make assumption A the default but always verify assumption B. This data-driven approach improves assumption quality over time.

Another advanced technique is assumption provenance. For each assumption, record why it was made: "I assume you want Q4 data because your previous three requests specified Q4." This helps users understand the agent's reasoning and makes corrections more targeted. Instead of saying "no, that is wrong," users can say "actually, this time I need Q3 data because I am comparing year-over-year trends."

## Strategies for Planning Under Uncertainty: Multi-Scenario Planning

When uncertainty is high and cannot be easily resolved, plan for multiple scenarios instead of committing to one. Multi-scenario planning generates several plans, each based on different assumptions, and defers the choice between them until more information is available. This is more expensive than single-scenario planning because you generate multiple plans, but it is more robust because you are prepared for different outcomes.

A common pattern is optimistic and pessimistic plans. The optimistic plan assumes everything works smoothly: APIs are available, data is clean, tools succeed on the first try. The pessimistic plan assumes things go wrong: APIs fail, data is messy, tools need retries. During execution, you start with the optimistic plan. If you encounter failures, you switch to the pessimistic plan, which already accounts for those failures. This avoids the need to re-plan mid-execution, which is expensive and breaks execution flow.

Another pattern is exploratory plans with decision points. The plan starts with an exploration phase: try calling the API to see if it works, query the database to see what data is available, search for relevant documents to see what exists. After the exploration phase, the agent evaluates the results and chooses one of several pre-planned branches based on what was discovered. If the API works, follow branch A. If the API fails but an alternative data source is available, follow branch B. If neither works, follow branch C, which involves asking the user for help.

Multi-scenario planning is especially useful for tasks with high environmental uncertainty. If you are building an integration with a third-party system and you do not know the system's API format, data schema, or reliability, plan for multiple possibilities. Have a plan for the case where the API is RESTful with JSON, a plan for SOAP with XML, a plan for GraphQL. Start execution by probing the API to determine which scenario applies, then execute the corresponding plan. This is faster than discovering the API format through trial and error.

The cost of multi-scenario planning grows with the number of scenarios. Generating three plans costs roughly three times as much as generating one plan. You need to limit the number of scenarios to those that are meaningfully different and have non-negligible probability. Do not plan for edge cases that are theoretically possible but practically unlikely. Focus on the two to four most likely scenarios.

Another technique is partial multi-scenario planning. Instead of generating complete plans for each scenario, generate plans that share a common prefix and diverge only at decision points. All scenarios start with "gather information about the environment," then diverge based on what is found. This reduces duplication and makes the multi-scenario plan more efficient to generate and execute.

In 2026, full multi-scenario planning is rare in production systems because of cost. It is used in high-stakes applications like financial trading, medical diagnosis support, and critical infrastructure management, where the cost of a wrong decision far exceeds the cost of planning for multiple scenarios. For most applications, simpler approaches like fail-fast validation and opportunistic re-planning are sufficient.

However, lightweight multi-scenario planning is becoming more common. Instead of generating complete plans for each scenario, agents generate decision trees with branch points. The trunk of the tree is common to all scenarios: gather initial information, analyze the situation, identify options. The branches represent different scenarios: if condition X holds, follow branch A; if condition Y holds, follow branch B. This structure is cheaper than full multi-scenario planning but more robust than single-scenario planning.

Decision trees work well when uncertainty resolves early in execution. If you can determine which scenario applies by running a few probe queries, the decision tree lets you plan for all scenarios without executing all of them. You only pay the full execution cost for the scenario that actually applies.

Probabilistic planning is another variant. Assign probabilities to scenarios based on historical data or heuristics. If the "API works smoothly" scenario has eighty percent probability and the "API fails" scenario has twenty percent probability, optimize the plan for the high-probability scenario while including fallbacks for the low-probability scenario. This balances efficiency for the common case with robustness for edge cases.

Some systems use ensemble planning combined with voting. Generate three to five plans independently, execute the first step of each, observe which produces the best results, and commit to that plan. This is expensive—you execute multiple first steps—but it quickly identifies which approach is most promising without committing to a full plan upfront. Ensemble planning is useful when the right approach is unclear until you try it.

The choice between single-scenario, multi-scenario, and ensemble planning depends on the uncertainty profile of your tasks. High uncertainty with early resolution favors decision trees. High uncertainty with late resolution favors ensemble methods. Low uncertainty favors single-scenario planning. Measure the uncertainty profile of your tasks and choose accordingly.

## The Cost of Premature Commitment vs Excessive Information Gathering

There is a fundamental tradeoff in planning under uncertainty: commit early and risk being wrong, or gather information first and incur delay and cost. Premature commitment happens when the agent generates a plan based on incomplete information and starts executing without validating assumptions. If the assumptions are wrong, execution fails, and you waste time and money. The MedInsight example at the beginning of this chapter is premature commitment: the agent committed to a specific technical approach without verifying that it matched the user's intent.

Excessive information gathering happens when the agent spends too much time and money resolving uncertainty before acting. An agent that asks twenty clarifying questions before starting work, or that performs exhaustive validation of every assumption, or that explores every possible data source before choosing one, is gathering information excessively. The cost of information gathering exceeds the benefit in reduced risk.

The optimal point on this tradeoff depends on the costs and probabilities involved. If the cost of gathering information is low and the cost of a wrong decision is high, gather information first. If the cost of gathering information is high and the cost of a wrong decision is low, commit early and fix mistakes reactively. If both costs are high, you are in a difficult situation that requires careful reasoning about which uncertainties matter most.

A useful framework is expected value analysis. For each piece of uncertain information, estimate the cost of gathering it, the probability that it will change your decision, and the cost difference between making the right decision and the wrong decision. The expected value of gathering the information is the probability of changing your decision times the cost difference, minus the cost of gathering. Gather information when the expected value is positive.

This sounds quantitative and precise, but in practice, the estimates are rough guesses. You do not know the probability that information will change your decision until you gather it, which defeats the purpose. But even rough estimates are better than nothing. If you guess that there is a twenty percent chance that asking a clarifying question will change your plan, and the plan costs fifty dollars to execute, and the question takes ten seconds to ask, the expected value is clearly positive. If the plan costs fifty cents, the expected value is clearly negative.

Another heuristic is the reversibility of decisions. If committing to a plan is reversible—you can detect errors early and switch to a different plan without much wasted cost—then premature commitment is acceptable. If committing is irreversible—once you start down a path, you are locked in even if it turns out to be wrong—then gathering information first is worth the cost. Irreversibility often comes from side effects: if executing the plan modifies external state, sends emails, or makes purchases, you cannot undo it easily.

In software engineering tasks, most decisions are reversible because code can be rewritten and tests can be re-run. In tasks that interact with the real world, like sending communications to customers or placing orders with vendors, decisions are less reversible. Agent systems for the latter must be more conservative about premature commitment.

The concept of decision checkpoints helps manage this tradeoff. Identify points in the plan where reversal becomes expensive or impossible. Before each checkpoint, validate assumptions and gather any critical missing information. After the checkpoint, accept that reversal is costly and proceed with confidence. For example, in an agent that sends emails, the checkpoint is the moment before clicking send. Before the checkpoint, gather information, validate recipients, review content. After the checkpoint, assume everything is correct and send without further validation.

Decision checkpoints can be automated or manual. Automated checkpoints use validation rules: before executing any irreversible operation, run a suite of checks. Manual checkpoints require human approval: before sending emails to more than one hundred recipients, get manager approval. The choice depends on risk tolerance and the cost of manual review.

Another technique for managing the tradeoff is staged commitment. Instead of committing to a full plan upfront, commit to phases. Commit to phase one, execute it, review results, then commit to phase two. Each phase is a mini-plan with its own information gathering and validation. This reduces premature commitment risk by breaking the overall task into smaller, independently validated pieces.

Staged commitment is especially valuable for exploratory tasks where early findings inform later steps. If you are investigating a bug, you do not know what information you will need until you start investigating. Commit to gathering initial diagnostics, review what you find, then commit to the next diagnostic step based on what the first step revealed. This adaptive approach handles uncertainty better than upfront planning.

Some systems use a time-boxing strategy for information gathering. Allocate a fixed amount of time or cost for reducing uncertainty. If you can resolve uncertainty within the budget, do so. If not, proceed with the best assumptions you have. This prevents analysis paralysis where the agent spends forever gathering information and never acts. Time-boxing forces action while still allowing reasonable information gathering.

The right time-box duration depends on task urgency and uncertainty level. For urgent tasks, use short time-boxes: spend thirty seconds clarifying, then act. For important tasks, use longer time-boxes: spend five minutes gathering information, then proceed. Calibrate based on production data: track how long information gathering typically takes and how often it changes the plan.

## How to Build Agents That Recognize Their Own Uncertainty

The hardest part of planning under uncertainty is not handling uncertainty once you know it exists; it is recognizing that you are uncertain in the first place. An agent that does not know it is uncertain will confidently execute a flawed plan. An agent that recognizes uncertainty can take appropriate actions: ask questions, validate assumptions, gather information, plan for multiple scenarios.

Uncertainty recognition requires meta-reasoning: reasoning about your own reasoning. The agent must evaluate its own confidence in its understanding of the task, its assumptions about the environment, and its predictions about tool outputs. This is difficult because LLMs are not calibrated: their confidence scores do not reliably correlate with correctness. A model can be highly confident and completely wrong, or uncertain and correct.

One approach to uncertainty recognition is to prompt for explicit confidence judgments. After generating a plan, ask the agent: "How confident are you that this plan is correct? What are the key assumptions? Which assumptions are most uncertain?" The agent's answers provide signals about where uncertainty exists, even if the confidence scores themselves are not calibrated. If the agent says "I assume the data is in table X, but I have not verified this," you know there is uncertainty about the data source.

Another approach is to detect uncertainty from the agent's reasoning process. If the agent generates multiple different plans during reasoning, that suggests uncertainty about the right approach. If the agent hedges its language—"probably," "likely," "I think"—that suggests uncertainty. If the agent asks itself questions during reasoning—"Which competitors should I focus on?"—that suggests recognized ambiguity. These signals can be detected through analysis of reasoning traces.

You can also use external signals of uncertainty. If the user's request is short and vague, uncertainty is likely. If the task is in a domain the agent has not handled before, uncertainty is likely. If the tools required are new or have high failure rates, uncertainty is likely. These heuristics are imperfect but useful.

Some research systems use ensembling to detect uncertainty. Generate multiple plans independently, compare them, and identify points of divergence. If five different plans all include the same first step, confidence in that step is high. If five different plans propose five different first steps, confidence is low. The diversity of plans is a proxy for uncertainty. This is expensive—you pay for generating multiple plans—but it is more robust than single-plan confidence estimates.

Once uncertainty is recognized, the agent needs policies for how to respond. A simple policy: if uncertainty is high, ask a clarifying question. If uncertainty is medium, make an assumption and validate it. If uncertainty is low, proceed with the plan. The thresholds for high, medium, and low are tunable based on your application's risk tolerance and cost sensitivity.

More sophisticated policies consider the type of uncertainty. If uncertainty is about user intent, ask a question. If uncertainty is about data availability, run a probe query. If uncertainty is about tool reliability, include retries in the plan. Different uncertainties call for different responses.

Building meta-cognitive prompts helps agents recognize uncertainty. Include explicit instructions in the system prompt: "Before generating a plan, identify what you do not know. List assumptions you are making. Assess your confidence in your understanding of the request." This metacognitive scaffolding encourages the agent to reason about its own reasoning.

You can also use adversarial prompting. After the agent generates a plan, prompt it to critique the plan from a skeptical perspective: "What could go wrong with this plan? What assumptions might be incorrect? What information is missing?" This critique often surfaces uncertainties that the initial planning overlooked.

Another technique is to use uncertainty markers in the agent's internal representation. When the agent decides to use a particular data source, it tags the decision with a confidence level. When it makes an assumption about user intent, it marks it as an assumption. These markers make uncertainty explicit in the data structure, enabling downstream processing like assumption validation or confidence-based prioritization.

Some systems use Bayesian reasoning for uncertainty quantification. Assign prior probabilities to competing hypotheses about what the user wants, update probabilities based on evidence, and trigger clarification when no hypothesis has high probability. This is formal and principled but requires significant implementation effort and careful prior selection.

A simpler approach is heuristic uncertainty scoring. Count the number of ambiguities in the request, the number of missing pieces of information, the number of untested assumptions. If the total score exceeds a threshold, the request is high-uncertainty and requires extra validation. This is less principled than Bayesian methods but easier to implement and tune.

User feedback is critical for improving uncertainty recognition. When users correct the agent's assumptions or clarify ambiguous requests, log those corrections. Analyze patterns: which types of requests are most often misunderstood? Which assumptions are most often wrong? Use these patterns to improve prompts, add clarification questions, or refine default assumptions.

## Handling Uncertainty During Execution, Not Just Planning

Much of this chapter focuses on handling uncertainty during planning, but uncertainty also arises during execution. A plan might be based on valid assumptions at planning time, but those assumptions become invalid during execution because the environment changes. A tool that was working during planning fails during execution. A data source that was available becomes unavailable. Handling execution-time uncertainty requires adaptive execution strategies.

The simplest strategy is reactive re-planning. When the agent encounters an unexpected failure or result, pause execution, re-plan based on the new information, and resume. This is the "try, fail, adapt" loop that characterizes reactive planning. The downside is the cost of re-planning, especially if failures are frequent. Re-planning also disrupts execution flow and makes it harder to track progress.

A more sophisticated strategy is contingent planning. The original plan includes fallback steps for likely failures. "Try API A; if it fails with a rate limit error, wait sixty seconds and retry; if it fails with an authentication error, try API B instead." This encodes handling for uncertainty directly in the plan, avoiding the need for re-planning. The challenge is predicting which failures are likely and including appropriate fallbacks without bloating the plan with rarely-used contingencies.

Another strategy is progressive commitment. Instead of committing to a full plan upfront, commit to small pieces at a time. Plan the next two steps, execute them, observe the results, plan the next two steps based on what you learned. This reduces the cost of premature commitment because you are only committing to small chunks, and it reduces the cost of information gathering because you gather information through execution rather than upfront queries. Progressive commitment is essentially hybrid planning taken to the extreme.

You can also use uncertainty-aware execution monitoring. Track which steps in the plan were based on uncertain assumptions. When executing those steps, monitor the results extra carefully for signs that the assumptions were wrong. If an assumption-dependent step produces an unexpected result, trigger re-planning or validation before proceeding. This focuses your validation effort on the parts of the plan most likely to be wrong.

Execution-time uncertainty manifests in different ways than planning-time uncertainty. During planning, uncertainty is about what to do. During execution, uncertainty is about whether what you are doing is working. Tool outputs might be ambiguous: an API returns a two hundred status code but an empty result set. Is that success or failure? Data might be inconsistent: database A says there are ten customers, database B says twelve. Which is correct? Intermediate results might be unexpected: you expected to find fifty relevant documents but found five hundred. Should you proceed or revise the plan?

Handling these execution-time uncertainties requires adaptive strategies. One approach is result validation. After each significant step, validate that the result matches expectations. If you expect to find ten to fifty competitors and you find three, something is probably wrong. Validate by checking the query, examining the data source, or asking the user if three is reasonable. Result validation catches errors early before they cascade.

Another approach is anomaly detection. Track the distribution of results for similar tasks. If most competitor analyses return twenty to thirty companies but this one returns three, flag it as anomalous. Anomalies do not necessarily indicate errors—maybe this is a niche market with few competitors—but they warrant extra scrutiny.

You can also use sanity checks during execution. After extracting features from a competitor website, check that the features make sense: are they actually product features, or did the extraction fail and capture navigation menu items instead? Sanity checks are domain-specific rules that catch common failure modes.

When execution-time uncertainty is detected, the agent has several options. Re-plan based on new information: the data source is not what we expected, so switch to an alternative. Escalate to human review: the results are ambiguous, ask a human to interpret them. Proceed with a warning: flag the uncertainty but continue execution, noting in the results that this step had high uncertainty. The choice depends on risk tolerance and the severity of uncertainty.

Logging execution-time uncertainties is valuable for system improvement. Track which steps frequently produce unexpected results, which assumptions are often invalidated, which tools are unreliable. Use this data to improve planning: make fewer assumptions about unreliable tools, include more contingencies for frequently-unexpected results, validate critical assumptions more thoroughly.

## Practical Implementation for Your Agent System

When you build an agent system that must handle uncertainty, start by cataloging the sources of uncertainty in your domain. Which aspects of user requests are typically ambiguous? Which data sources are unreliable? Which environmental conditions are unknown at planning time? Understanding your uncertainty landscape guides your design decisions.

For user request ambiguity, implement a clarification question generator. After receiving a request, the agent identifies potential ambiguities and generates questions. Present questions to the user before planning, or at least before executing expensive steps. Make questions specific and actionable, not open-ended.

For missing context, provide the agent with as much relevant context as possible. If there is a user profile, include it. If there is task history, include it. If there are organizational conventions or preferences, encode them in the system prompt. The more context the agent has, the fewer assumptions it needs to make.

For unreliable tools, implement robust error handling and retries. Tools should return structured errors that the agent can reason about: is this a transient error that merits a retry, or a permanent error that requires a different approach? The agent's plan should include retry logic for known-flaky tools.

For unknown environment state, implement probing and validation. Before committing to a plan that depends on a file existing, check if the file exists. Before assuming an API accepts JSON, send a test request. These probes are cheap insurance against wasted execution.

Make assumptions explicit in your plan representation. Each step that depends on an assumption should link to that assumption. When presenting plans to users, highlight key assumptions so users can correct them. When logging execution, record which assumptions were made and whether they turned out to be correct. Use this data to improve assumption quality over time.

Implement uncertainty thresholds and policies. Define what constitutes high, medium, and low uncertainty. Define how the agent should respond to each level: ask questions for high, validate assumptions for medium, proceed for low. Tune these thresholds based on production experience.

Finally, build feedback loops to learn from uncertainty. When the agent makes a wrong assumption, log it. When a user corrects an ambiguous request, log the correction. When a plan fails due to unknown environment state, log what was unknown. Analyze these logs to identify patterns, and use the patterns to improve prompts, add validation steps, or adjust uncertainty policies. Handling uncertainty is a continuous learning process, not a one-time design decision.

## Real-World Patterns: Common Uncertainty Scenarios and Solutions

Certain uncertainty patterns appear repeatedly across domains. Recognizing these patterns and having standard solutions accelerates agent development. One common pattern is scope ambiguity: the user specifies a task but not its boundaries. "Analyze customer feedback" could mean all feedback ever, feedback from the last month, feedback about a specific product, or feedback from a specific customer segment. The standard solution is to define sensible defaults based on domain norms—for most businesses, "recent" means the last thirty to ninety days—and state those defaults explicitly when confirming the plan.

Another common pattern is format ambiguity: the user wants information but does not specify how to present it. Should the agent produce a detailed report, a summary, a visualization, a spreadsheet, or something else? The standard solution is to choose the format that best suits the data type and task: comparisons work well as tables, trends work well as visualizations, insights work well as prose. Include a sample or preview of the chosen format in the plan confirmation so users can request changes before full execution.

Priority ambiguity is frequent in resource-constrained tasks. The user wants multiple things but has not specified which is most important. "I need market data, competitor analysis, and customer research by Friday." If all three cannot be completed by Friday, which should you prioritize? The standard solution is to ask directly or infer from context: if previous tasks focused on competitor analysis, prioritize that. Document the prioritization decision so users can adjust if needed.

Temporal ambiguity appears when time ranges are unspecified. "How are sales trending?" Trending compared to what: last month, last quarter, last year? The standard solution is to use multiple time scales: show month-over-month, quarter-over-quarter, and year-over-year trends, then let users focus on whichever is most relevant. This is more expensive than a single comparison but eliminates back-and-forth clarification.

Identity ambiguity happens when references are unclear. "Compare our pricing to competitors." Which competitors: all competitors, top three competitors, direct competitors, or a specific set? The standard solution is to use a reasonable heuristic—top three by market share in your category—and validate with a quick confirmation. "I will compare to Company A, Company B, and Company C. Is this the right set?" This confirms intent without requiring users to enumerate competitors manually.

Data source ambiguity is technical but important. When multiple data sources could provide the requested information, which should you use? The standard solution is to prefer authoritative sources over derived sources, primary data over aggregated data, and recently updated sources over stale ones. Document which source was used so users can request alternatives if the default is not appropriate.

Tool capability ambiguity arises when the task could be accomplished with different tools that have different tradeoffs. Should you scrape competitor websites or use a competitive intelligence API? Scraping is free but fragile; the API is expensive but reliable. The standard solution is to prefer reliable tools for high-stakes tasks and cheap tools for exploratory tasks. Make the tradeoff explicit when presenting the plan.

These patterns and solutions are not universal—the right defaults depend on your domain—but recognizing that ambiguity follows patterns lets you build reusable handlers rather than treating each ambiguous request as unique.

## Measuring and Improving Uncertainty Handling

To improve your agent's uncertainty handling, you need metrics that capture how well it performs. One metric is clarification rate: what percentage of requests trigger clarification questions? A very high rate suggests the agent is not inferring enough from context. A very low rate suggests the agent is making too many unchecked assumptions. The optimal rate depends on your application, but monitoring trends is valuable: if the clarification rate increases over time, your agents are becoming more conservative; if it decreases, they are becoming more confident or more reckless.

Another metric is assumption correction rate: what percentage of stated assumptions are corrected by users? High correction rates indicate poor default assumptions. Track corrections by assumption type to identify which kinds of inferences the agent struggles with. If user intent assumptions are frequently corrected but data source assumptions are rarely corrected, focus improvement efforts on intent inference.

Plan revision rate during execution is another indicator. How often do plans generated during the planning phase get revised during execution because assumptions turned out to be wrong? High revision rates suggest that validation during planning is insufficient. Low revision rates suggest that planning is robust, or that execution is not adaptive enough to recognize when revision is needed. Distinguish between revisions due to environmental changes—external APIs failed—and revisions due to invalid assumptions—we assumed the wrong data schema.

Task success rate as a function of initial uncertainty is revealing. Segment tasks by the level of uncertainty at planning time—measured by number of assumptions, number of clarifying questions, user request vagueness—and measure success rates for each segment. If high-uncertainty tasks have much lower success rates than low-uncertainty tasks, your uncertainty handling needs improvement. If success rates are similar across uncertainty levels, your strategies are working.

User satisfaction correlates with uncertainty handling but is not determined by it. Users appreciate agents that recognize and handle uncertainty gracefully, but they also value speed and autonomy. Measure both: track whether users rate their experience higher when agents ask clarifying questions versus when agents make assumptions. In some contexts, users prefer questions; in others, they prefer speed. Let data guide your tradeoffs.

Another useful metric is average time or cost spent on information gathering relative to total task time or cost. If information gathering consumes fifty percent of resources, you might be over-gathering. If it consumes one percent, you might be under-gathering and relying too heavily on assumptions. The optimal ratio depends on task characteristics, but extreme values in either direction warrant investigation.

A/B testing is valuable for tuning uncertainty policies. Run variant A that asks more clarifying questions and variant B that makes more assumptions. Compare success rates, user satisfaction, task completion time, and cost. The variant that performs better on your priority metrics should inform your default policy. Continuously run tests as your agent evolves to ensure policies remain optimal.

## The Future of Planning Under Uncertainty

Uncertainty handling in agent systems is advancing rapidly. In 2026, most systems use rule-based policies: if uncertainty exceeds threshold X, trigger response Y. Future systems will use learned policies: train models on historical data about when clarification helped, when assumptions were safe, when multi-scenario planning was necessary. Learned policies adapt to the specific uncertainty patterns of your domain without manual rule tuning.

Another frontier is proactive context gathering. Instead of waiting for users to provide information, agents will pull context from available sources: user profiles, previous task history, organizational knowledge bases, industry norms. The more context available, the fewer assumptions needed. Systems that integrate deeply with organizational data will handle uncertainty better than isolated agents.

Collaborative uncertainty resolution is emerging. When an agent is uncertain, instead of asking the user directly, it might consult other agents, search for similar past tasks, or query knowledge bases. Only if these sources do not resolve uncertainty does it escalate to the user. This reduces user interruptions while still avoiding unchecked assumptions.

Better calibration of LLM confidence scores would transform uncertainty recognition. Current models are not well-calibrated: a ninety percent confidence score does not mean ninety percent correctness. Research into calibration techniques—temperature scaling, ensembling, Bayesian methods—will make confidence scores more reliable, enabling better automated uncertainty detection.

Finally, uncertainty-aware planning will become more sophisticated. Instead of treating uncertainty as a binary—certain or uncertain—systems will represent degrees and types of uncertainty, propagate uncertainty through plans, and optimize for expected value under uncertainty. This requires richer plan representations and more advanced reasoning, but it enables principled decision-making in uncertain environments.

Uncertainty is unavoidable in production agent systems, but it is manageable. Agents that recognize uncertainty, take actions to reduce it, make assumptions explicit, and adapt when assumptions turn out wrong will outperform agents that assume perfect information. The difference between brittle agents and robust agents is not the absence of uncertainty, but the presence of strategies to handle it. As agent systems mature, uncertainty handling will evolve from ad-hoc rules to sophisticated, learned, context-aware strategies that gracefully manage the inherent ambiguity of real-world tasks.
