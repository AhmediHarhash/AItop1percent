# 1.3 — When You Need an Agent (and When You Don't): Decision Framework

In February 2025, a Series B startup in the legal tech space decided to rebuild their contract review product as an "AI agent." The existing system was straightforward: users uploaded contracts, the system extracted key clauses using fine-tuned models, highlighted risks based on rules, and generated a summary. It worked well. Customers were happy. Revenue was growing.

But the executive team read that agents were the future, and they did not want to be left behind. They convinced themselves that their contract review tool needed to be agentic: it should autonomously research case law, iteratively refine its analysis, query external databases, and adapt its review strategy based on contract type. They staffed a team of eight engineers, allocated six months, and set out to build "the world's first autonomous contract analysis agent."

Eight months and 2.1 million dollars later, they had a system that was slower, more expensive to run, less reliable, and harder to explain to customers than the original. The "agent" took 40 seconds to review a contract that the old system handled in 4 seconds. It cost twelve times as much per review due to the dozens of LLM calls and external API queries. It occasionally looped on edge cases, requiring manual intervention. Customers hated it. The team reverted to the original architecture in September 2025 and wrote off the entire project.

The problem was not execution—the engineers built what they set out to build. The problem was that contract review did not need an agent. The task was not variable enough to require multi-step adaptation. The existing fine-tuned extraction models were more accurate than iterative LLM-based analysis. The rule-based risk detection was faster and more consistent than agentic reasoning. External research, when needed, could be triggered on-demand by users, not executed automatically on every contract.

They built an agent because agents were trendy, not because the problem required one. The cost was millions of dollars, eight months, team morale, and customer trust. This is the most common agent failure mode in 2026: building agentic systems for problems that do not need them.

## The Agent Decision Framework

Before you build an agent, you need a framework to determine whether you actually need one. The framework has four pillars: task variability, tool use requirements, multi-step reasoning needs, and error recovery requirements. If your use case does not meet the threshold on most or all of these dimensions, you do not need an agent.

**Task variability.** Does the task require different approaches for different inputs, or can it be solved with a consistent pipeline? If every input follows the same pattern and can be handled with the same sequence of steps, you do not need an agent—you need a pipeline or a fine-tuned model. Agents add value when tasks are variable, when the path to completion depends on intermediate results, and when no single fixed workflow works for all cases.

Contract review, in the legal tech startup's case, had low variability. Every contract needed extraction, risk analysis, and summarization. The specific clauses varied, but the process did not. A fixed pipeline with specialized models was the right architecture.

Compare this to a research agent. Research tasks are highly variable: the information needed depends on the query, what sources are available, what the initial search returns, and what gaps remain. You cannot know the steps upfront. The agent must adapt based on what it finds. High variability justifies an agent.

**Tool use requirements.** Does the task require calling tools, and if so, how many and in what sequence? If the task requires no tools, you do not need an agent—you need a good prompt or a fine-tuned model. If the task requires one tool call in a fixed position in the workflow, you do not need an agent—you need tool-augmented chat or a pipeline with a tool step. If the task requires multiple tool calls, in a sequence that depends on intermediate results, and the choice of which tool to call next depends on what previous tools returned, you might need an agent.

The contract review system needed zero dynamic tool calls. Extraction was model inference, not tool use. Risk rules were code, not tools. Summarization was generation. There was no need for the agent to decide "call this tool, observe the result, then decide which tool to call next." The entire task was inference and logic, no tools required.

Research agents, by contrast, need multiple tools: search, web scraping, document retrieval, calculators, API queries. The sequence depends on results: if the first search returns nothing, try a different query; if the document is too long, extract summaries first; if data needs analysis, call a calculation tool. Tool use is dynamic, sequential, and result-dependent. This justifies an agent.

**Multi-step reasoning needs.** Does solving the task require iterating through multiple rounds of reasoning, where each round builds on the previous one? Or can the task be solved in one reasoning pass, possibly with retrieval or tool augmentation? If one pass suffices, you do not need an agent. If the task requires chaining inferences, refining hypotheses, or building up an answer over multiple steps, you might need an agent—but first ask if a prompt chain or a pipeline would work.

The contract review task required multi-step processing—extraction, then risk analysis, then summarization—but not multi-step reasoning. Each step could be done independently, in a fixed order. The risk analysis did not need to "reason" about what the extraction found in order to decide what to do next—it just applied rules. This is multi-step processing, not multi-step reasoning. A pipeline handles it fine.

A troubleshooting agent, on the other hand, requires multi-step reasoning: observe symptoms, hypothesize causes, test hypotheses, observe results, refine hypotheses, test again. Each step depends on the previous one. You cannot know upfront how many steps you will need or what they will be. This justifies an agent.

**Error recovery requirements.** Does the task require adapting when things go wrong, or can errors be handled with retries and fallbacks? If an API call fails, does the system need to reason about why and try a different approach, or can it just retry with backoff? If a tool returns unexpected data, does the system need to adapt its plan, or can it just return an error to the user?

The contract review system had low error recovery needs. If extraction failed on a clause, the system marked it as unextractable and moved on. If risk analysis hit an edge case, it flagged it for human review. Errors were handled with fallbacks and escalation, not adaptation. An agent was not needed.

An agent that coordinates a multi-step workflow across external systems has high error recovery needs: if one step fails, the agent must decide whether to retry, skip, compensate, or abort. If a service is down, the agent must find an alternative. If data is malformed, the agent must clean it or request it again. Error recovery requires reasoning and adaptation, which justifies an agent.

If your task scores low on all four dimensions—low variability, minimal tool use, single-pass reasoning, simple error handling—you do not need an agent. If it scores high on three or four, you probably do. If it scores high on one or two, you are in the gray zone: an agent might help, but simpler architectures might suffice.

## When Single LLM Calls Suffice

The simplest architecture is a single LLM call: user input goes in, model output comes out. This works for a surprisingly large set of tasks, and teams often overlook it because it feels too simple.

**Use a single LLM call when** the task is a pure transformation, no external data or tools are needed, and the input contains all the information required to produce the output. Classification, summarization, extraction from provided text, translation, rewriting, simple question-answering, content generation.

Example: A customer support system that categorizes incoming tickets. The ticket text contains all the information needed. The task is to output a category label. A single LLM call with a well-tuned prompt is sufficient. No tools, no iteration, no agent.

Example: A writing assistant that rewrites user text to be more formal. The input text contains what needs to be rewritten. The output is the rewritten version. One call, no agent.

Example: Extracting structured data from emails. The email contains the data, the task is to pull it into a structured format. One call, potentially with JSON mode or structured output, no agent.

Single LLM calls are fast, cheap, and simple to evaluate. Latency is one inference pass. Cost is one API call. Evaluation is input-output pairs. Deployment is straightforward. Error handling is simple. If this architecture solves your problem, use it. Do not add complexity because you think you should.

The contract review startup could have solved most of their needs with single calls per subtask: one call to extract clauses, one call to summarize, one call to flag risks in plain language. Instead, they built an agent that iterated over each clause, researched precedents, and synthesized findings. The iteration added latency and cost without improving accuracy.

## When RAG Is Enough

Retrieval-augmented generation is the next step up: you need external knowledge, but you do not need iteration. The task is still input-to-output, but the output depends on information not in the input.

**Use RAG when** the task requires external knowledge, the knowledge is retrievable via search or lookup, and one retrieval step is sufficient to answer the query. Question-answering over documents, customer support over a knowledge base, code assistants that reference documentation, research assistants that retrieve and summarize.

Example: A customer support bot that answers questions by retrieving relevant help articles. The user asks a question, the system retrieves articles, passes them to the LLM along with the question, the LLM generates an answer. One retrieval, one generation, no iteration, no agent.

Example: A code assistant that looks up API documentation when you ask how to use a function. Retrieve the docs, pass them to the LLM with your question, generate an answer. No agent.

Example: A research tool that takes a query, retrieves the top ten relevant papers, and generates a summary. Retrieve, summarize, done. No iteration, no agent.

RAG adds one step—retrieval—before generation, but it is still a single-pass architecture. You retrieve, you generate, you return the result. It is more complex than a single LLM call, but much simpler than an agent.

The contract review startup could have used RAG if they needed to reference case law: retrieve relevant cases based on contract clauses, pass them to the LLM, generate a risk analysis that cites precedents. This would have been faster and cheaper than an agent that iteratively researches case law.

Teams often confuse RAG with agents because retrieval can look agentic: the system is "going out and getting information." But RAG is not agentic unless it iterates. If you retrieve once, generate once, and return the result, it is RAG, not an agent. If you retrieve, evaluate the results, decide you need more information, retrieve again with a different query, and iterate until you have enough information, then it is an agent.

## When Fine-Tuning Is Better

Fine-tuning is often overlooked in 2026 because prompting and agents get more attention. But fine-tuning is the right choice for tasks that are well-defined, repetitive, and require consistent behavior.

**Use fine-tuning when** you have a task with clear input-output structure, you have or can create a dataset of examples, and you need the model to perform the task reliably, consistently, and efficiently. Classification, extraction, style transfer, domain-specific generation, tasks where prompting is too slow or inconsistent.

Example: The contract review startup's extraction task. Extracting key clauses from contracts is a well-defined task. They had thousands of labeled contracts. A fine-tuned model trained on this data would be faster, cheaper, and more accurate than prompting or agentic reasoning. They already had this—it was their original architecture, and it worked well.

Example: Code generation in a specific framework or style. If you need the model to always generate code following your company's conventions, and you have a corpus of example code, fine-tuning is better than prompting. The model learns the style and applies it consistently.

Example: Domain-specific summarization. If you summarize medical records and need the summaries to follow a specific format and terminology, fine-tuning on labeled examples will outperform generic prompting.

Fine-tuning trades upfront investment—data collection, training, evaluation—for long-term efficiency and consistency. Once trained, fine-tuned models are fast and cheap to run. They do not require complex prompts, retrieval, or iteration. For high-volume, repetitive tasks, fine-tuning often has the best cost-performance tradeoff.

Agents are the opposite: high per-task cost, high latency, high variability. Agents are for tasks that change every time, where you cannot predefine the solution. If your task is the same every time, fine-tuning is better.

## When You Actually Need Multi-Step Reasoning with Tool Use

Now we get to when agents are the right choice. You need an agent when the task requires multiple steps, the steps depend on intermediate results, the sequence cannot be predefined, and the system must adapt based on what it observes.

**Use an agent when** the task is goal-directed and the path to the goal is not known upfront, when intermediate results determine what to do next, when tool use is dynamic and sequential, when errors require reasoning and adaptation, and when the task cannot be reduced to a fixed pipeline or a single call with retrieval.

Example: A research agent that investigates a complex question. The user asks "what caused the decline in sales last quarter?" The agent does not know upfront what data to query or what analysis to run. It must explore: query sales data, identify anomalies, hypothesize causes, query related data (marketing spend, product changes, competitor actions), test hypotheses, refine understanding, and synthesize findings. The path depends entirely on what the data shows at each step. This is agent territory.

Example: A debugging agent. The user reports a bug. The agent must reproduce it, read logs, hypothesize root causes, inspect code, test fixes, verify the fix worked. The sequence cannot be predefined because bugs vary wildly. The agent must adapt based on what each step reveals. Agent required.

Example: A workflow orchestration agent that coordinates a complex multi-step process: provision resources, run jobs, monitor progress, handle failures, clean up. The workflow has structure, but failures and variability require adaptation. If job A fails, the agent must decide whether to retry, skip, or abort. If resources are unavailable, the agent must wait or find alternatives. Agent justified.

The key is variability and adaptation. If the task is different every time and you cannot write a script or pipeline that handles all cases, you need an agent. If intermediate results change what you do next in ways you cannot predict upfront, you need an agent.

## The Cost of Unnecessary Agent Complexity

Building an agent when you do not need one is expensive in multiple dimensions.

**Development time.** Agents are more complex to build than single-call systems, RAG, or fine-tuned models. You need state management, iteration logic, termination criteria, error recovery, tool orchestration, and observability. A task that could be solved with a single LLM call in one day might take two weeks as an agent. A task that could be solved with RAG in a week might take a month as an agent.

The contract review startup spent eight months building an agent for a problem that was already solved. The opportunity cost was eight months of feature development, bug fixes, and customer-requested improvements. By the time they reverted, competitors had shipped features they had planned.

**Runtime cost.** Agents are expensive to run. Every iteration is an LLM call. If an agent takes ten iterations to complete a task, and each iteration costs two cents, you are paying twenty cents per task. A single-call system would cost two cents. A 10x cost increase is common when moving from single-call to agent architectures.

The contract review agent cost twelve times more per contract than the original system. At scale—thousands of contracts per day—this made the product unprofitable. The agent architecture destroyed the unit economics.

**Latency.** Agents are slow. Each iteration is an inference pass, and iterations are sequential. If each inference takes one second and the agent takes ten iterations, you are looking at ten-plus seconds of latency. Users perceive anything over two or three seconds as slow. The contract review agent took 40 seconds per contract versus 4 seconds for the original system. Customers complained that the product had become unusable.

**Reliability.** Agents have more failure modes than simpler systems. They can loop, fail to terminate, make incorrect tool calls, misinterpret results, or diverge from the intended goal. Single-call systems fail in one way: bad output. Agents can fail in dozens of ways, and some failures are subtle—producing an answer that looks right but is based on flawed reasoning several steps back.

**Explainability.** Agents are harder to explain. A single-call system is transparent: input, processing, output. An agent is a multi-step process with branching, iteration, and adaptation. When a user asks "why did the system do this?" the answer might involve reconstructing a fifteen-step episode with tool calls, reasoning, and backtracking. Users and regulators find this opacity concerning.

**Maintenance.** Agents are harder to maintain. Changes to tools, to prompts, or to the environment can cause subtle regressions in multi-step behavior that do not show up in single-step tests. Debugging agent failures requires tracing through episodes, understanding state at each step, and identifying where reasoning went wrong. Single-call systems are easier to debug: you see the input and output, and you tune the prompt or model.

Each of these costs compounds. An agent that is 10x more expensive, 10x slower, less reliable, harder to explain, and harder to maintain is a bad trade if a simpler architecture would have worked.

## Decision Criteria Summary

To decide whether you need an agent, evaluate your task against these criteria:

**Task variability:** Does the task require different approaches for different inputs, or is it consistent across inputs? If consistent, you do not need an agent.

**Tool use:** Does the task require dynamic, sequential tool use where the choice of tool depends on previous results? If no tools are needed, or if tool use is fixed, you do not need an agent.

**Multi-step reasoning:** Does solving the task require iterating through reasoning steps where each builds on the previous one? If one pass suffices, you do not need an agent.

**Error recovery:** Does the task require adapting when things go wrong, or can errors be handled with retries and fallbacks? If simple error handling suffices, you do not need an agent.

**Cost tolerance:** Can your use case absorb the 10-100x increase in cost and latency that agents introduce? If not, find a simpler solution.

**Reliability requirements:** Can you tolerate the additional failure modes and complexity that agents introduce? If your use case requires deterministic behavior or high reliability, agents are risky.

**Explainability requirements:** Do users or regulators need to understand why the system made specific decisions? If yes, agents are harder to justify.

If you score low on most criteria, do not build an agent. If you score high on most, an agent may be justified, but start with the simplest agentic architecture—reactive, user-initiated, short episodes—and only add complexity if needed.

## Real Examples of Teams That Over-Engineered with Agents

The contract review startup is not alone. In 2025 and 2026, dozens of teams over-engineered with agents when simpler approaches would have worked.

**Example: E-commerce product recommendations.** A mid-sized retailer built an agent to recommend products. The agent would query user history, search the catalog, reason about preferences, query inventory, check pricing, and iteratively refine recommendations. It took 8 seconds per recommendation and cost 15 cents. A collaborative filtering model trained on user behavior would have been faster, cheaper, and more accurate. They eventually replaced the agent with a traditional recommendation system and saw better conversion at a fraction of the cost.

**Example: Meeting summarization.** A startup built an agent to summarize meetings. The agent would transcribe audio, identify speakers, extract topics, search for related documents, synthesize context, and generate a summary. It took 90 seconds to summarize a 30-minute meeting. A fine-tuned summarization model would have done it in 10 seconds with comparable quality. The agent added complexity without adding value.

**Example: Data pipeline orchestration.** A data team built an agent to orchestrate ETL pipelines. The agent would decide which jobs to run, in what order, based on data freshness and dependencies. It worked, but it was slower and less predictable than a traditional DAG-based orchestrator like Airflow. The agent introduced latency in scheduling decisions and occasionally made mistakes in dependency resolution. They reverted to Airflow after three months.

In each case, the team chose agents because agents were exciting, not because the problem required them. The result was wasted time, wasted money, and worse outcomes than simpler approaches.

## The Right Starting Point

When you encounter a problem, resist the urge to jump to agents. Start with the simplest architecture that could work and only add complexity when you hit a clear limitation.

**Start with a single LLM call.** Can you frame the task as input-to-output transformation? Try it. If it works, you are done. If it does not, identify why: do you need external data, multiple steps, iteration?

**If you need external data, try RAG.** Can you retrieve the data you need and generate a response in one pass? Try it. If it works, you are done. If it does not, identify why: do you need multiple retrievals, do you need to adapt based on what you retrieve?

**If you need consistency and volume, try fine-tuning.** Can you define the task clearly, collect examples, and train a model? Try it. Fine-tuning is underrated in 2026. It is often faster and cheaper than prompting or agents for well-defined tasks.

**If none of these work, consider an agent.** But start with the simplest agentic architecture: a reactive agent, user-initiated, short episodes, basic tools. Prove it works. Then, if needed, add planning, longer episodes, more autonomy.

This progression saves time and money. You do not build an agent and then discover a single LLM call would have worked. You try the simple thing first, and if it fails, you know exactly why and what capability you need to add.

## When in Doubt, Start Simpler

The failure mode of the contract review startup, the sales intelligence team, the product recommendation engine, and dozens of other teams is the same: they started with agents when they should have started simpler. Agents are powerful, but power comes with cost. Most tasks do not need that power.

When in doubt, start simpler. Single call, RAG, fine-tuning, pipeline. Prove these do not work before you build an agent. You will ship faster, spend less, and end up with a more maintainable system. And if you do need an agent, you will know exactly why, which will inform every architectural decision downstream.

Agents are not the default. They are the exception, reserved for tasks that are variable, complex, multi-step, and adaptive. Everything else is simpler, cheaper, and better served by architectures that do less.
