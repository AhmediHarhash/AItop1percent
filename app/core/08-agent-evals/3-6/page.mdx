# 3.6 — Reasoning Depth Control: When to Think More vs Act Faster

In March 2024, a customer support automation team at a Series B SaaS company noticed something strange in their logs. Their AI agent was taking an average of 4.2 seconds to respond to simple password reset requests—questions that human agents answered in under a second. The agent was running a full multi-step reasoning process for every single interaction, analyzing customer sentiment, checking for potential security concerns, evaluating policy exceptions, and considering escalation paths. For the 89 percent of requests that were straightforward password resets, this deep reasoning was pure waste. The team was burning through their API budget at twice the projected rate, and customers were complaining about the lag. When they implemented reasoning depth controls—allowing the agent to use shallow reasoning for routine requests and reserving deep reasoning for ambiguous or high-stakes situations—response times dropped to 0.8 seconds for simple cases, API costs fell by 64 percent, and customer satisfaction scores jumped 23 points. The agent was finally thinking at the appropriate depth for each situation.

Agents that use the same reasoning depth for every task waste resources on simple problems and underperform on complex ones. In 2026, the most effective production agents dynamically adjust how much they think based on what each situation demands. You face the same fundamental tradeoff every time you build an agent: thinking deeply produces better decisions but costs more time and money. Shallow reasoning is fast and cheap but more error-prone. The key is not choosing one or the other—it is dynamically adjusting reasoning depth based on what each situation demands. This is not about making your agent dumber for simple tasks. It is about teaching your agent to recognize when a problem requires careful thought and when quick action is the right move. The most effective agents in 2026 operate across a spectrum of reasoning depths, investing cognitive resources proportionally to task complexity and stakes.

## The Reasoning Depth Spectrum

Reasoning depth exists on a continuum from immediate pattern matching to extended multi-step deliberation. At the shallow end, your agent recognizes a familiar pattern and acts based on learned associations. A customer says "reset my password" and the agent immediately initiates the password reset flow without analyzing alternative interpretations or considering edge cases. This is fast, deterministic, and works perfectly for routine situations. At the deep end, your agent engages in explicit multi-step reasoning, considering multiple hypotheses, evaluating evidence, anticipating consequences, and constructing a justified plan before acting. A customer says "I cannot access my account" and the agent reasons through whether this is a password issue, a permissions problem, a billing suspension, a technical outage, or something else entirely.

The spectrum is not binary. Between these extremes lie intermediate depths where your agent does some reasoning but not exhaustive analysis. You might have a tier where the agent checks for common edge cases but does not explore unlikely scenarios. Another tier where it considers two or three plausible interpretations but not all theoretically possible ones. The art is matching reasoning depth to the actual complexity and stakes of each decision.

In practice, shallow reasoning often manifests as direct prompt-to-action mappings or simple decision trees. Your agent sees a pattern it recognizes and follows a predetermined response strategy. Deep reasoning manifests as explicit chain-of-thought where the agent works through the problem step by step, often generating hundreds or thousands of tokens of internal reasoning before selecting an action. The cost difference is dramatic—shallow reasoning might consume 50 tokens while deep reasoning consumes 2000 tokens for the same decision point.

The spectrum also includes what you might call medium-depth reasoning, where the agent performs focused analysis on specific aspects without full decomposition. A medium-depth approach to "I cannot access my account" might check three common causes—password, billing, technical—without exploring edge cases like geographic restrictions, browser compatibility, or account merger scenarios. This middle ground is often the sweet spot for moderate-complexity situations where you need more than pattern matching but less than exhaustive deliberation.

## When Deep Reasoning Actually Helps

Deep reasoning earns its cost in specific situations. Novel situations where your agent has not encountered this exact pattern before demand deeper thought. If your customer support agent sees a request it cannot immediately classify into a known category, shallow pattern matching will fail. The agent needs to reason through what the customer actually wants, what information is missing, what clarifying questions would help, and what response strategy makes sense given the ambiguity. This reasoning is expensive but necessary—shallow reasoning in a novel situation leads to confident errors.

High-stakes decisions justify deep reasoning regardless of familiarity. Even if your agent has processed hundreds of refund requests, a refund over a certain threshold might warrant deeper analysis. What is the customer's history? Are there signs of fraud? What are the business implications of approving versus denying? Is this part of a pattern that suggests a product issue? Deep reasoning here is not about uncertainty—it is about ensuring the decision factors in all relevant considerations before taking an action with significant consequences.

Ambiguous inputs require deep reasoning to resolve what the user actually wants. When a customer says "this is not working," shallow reasoning might trigger a generic troubleshooting flow. Deep reasoning would consider what "this" refers to, what "not working" means in context, what the customer has likely already tried, and what information the agent needs to provide actual help. The ambiguity is the signal that pattern matching will not suffice.

Conflicting information or constraints force deep reasoning. If your agent sees that a customer is eligible for a discount based on one policy but the discount is excluded based on another policy, shallow reasoning will either apply the first rule it matches or throw an error. Deep reasoning works through the conflict, considers which policy takes precedence, evaluates whether there are exceptions, and constructs a response that addresses the contradiction rather than ignoring it.

Multi-factor decisions with tradeoffs demand deeper thought. Choosing which product to recommend based on price alone is shallow reasoning. Choosing which product to recommend based on the customer's stated needs, budget constraints, previous purchases, feature requirements, and compatibility considerations requires weighing multiple factors. Deep reasoning lets your agent consider these dimensions and their interactions rather than optimizing for a single variable.

Situations requiring explanation or justification also benefit from deep reasoning. When your agent needs to explain why it made a particular recommendation or denied a request, shallow reasoning provides no basis for explanation. Deep reasoning generates the reasoning trace that becomes the explanation. This matters especially in regulated domains where you must document decision rationale for compliance purposes. The reasoning itself becomes an artifact with business value beyond just improving the decision.

## When Shallow Reasoning Suffices

Routine tasks with clear patterns do not benefit from deep reasoning. If 89 percent of password reset requests follow the same simple pattern—user requests reset, agent verifies identity, agent sends reset link—running a full reasoning process for each one wastes resources without improving outcomes. Shallow reasoning handles these cases perfectly because the pattern is well-established and the action is deterministic. The errors you prevent with deep reasoning do not justify the cost when the base accuracy of shallow reasoning is already near perfect.

Time-sensitive actions cannot afford deep reasoning latency. If your agent is monitoring a production system and detects a memory leak crossing a critical threshold, the correct action is to trigger an alert and initiate a restart. Deep reasoning about whether this might be a false positive or whether there are alternative interventions delays the response during the window when speed matters most. Shallow reasoning—see pattern, take predetermined action—is the right tradeoff when latency has real consequences.

Well-understood domains with stable patterns favor shallow reasoning. If your agent has processed 10,000 similar requests with a 99.2 percent success rate using pattern matching, deep reasoning on request 10,001 will not meaningfully improve that number. The domain is well-understood, the patterns are stable, and the shallow reasoning strategy has been validated. Save the reasoning budget for the 0.8 percent of cases that do not fit the pattern.

Low-stakes decisions do not justify reasoning costs. If your agent is choosing which emoji to include in a confirmation message, deep reasoning about the emotional connotations and cultural interpretations of each option is absurd. Pick one that fits the general tone and move on. The cost of a suboptimal emoji is negligible, so the cost of reasoning about it cannot be justified. Shallow reasoning—or even random selection from a predetermined set—is the economically rational choice.

Highly constrained decision spaces do not need deep reasoning. If your agent has exactly two allowed actions and simple criteria for choosing between them, there is no reasoning work to do. Shallow pattern matching to determine which criterion is met and executing the corresponding action is sufficient. Deep reasoning adds no value when the decision tree has two branches and clear splitting criteria.

Situations where users expect instant response also favor shallow reasoning. Chat interfaces train users to expect sub-second responses. If your agent pauses for four seconds to think deeply about a greeting message, the user experience feels broken even if the eventual response is marginally better. User expectations create a latency budget, and shallow reasoning is often necessary to meet that budget on routine interactions.

## Dynamic Reasoning Depth Adjustment

The most sophisticated agents adjust reasoning depth dynamically based on signals in the task and their own confidence. This is not about having two modes—simple and complex—it is about continuously calibrating how much thinking each decision point deserves. Your agent starts with a default reasoning depth based on task classification, then adjusts based on what it encounters.

Confidence signals are the primary adjustment mechanism. If your agent's initial shallow reasoning produces a high-confidence classification—this is definitely a password reset request—it proceeds with shallow reasoning. If the initial classification is low-confidence—this might be a password reset or it might be a locked account or it might be a permissions issue—the agent shifts to deeper reasoning to resolve the ambiguity. You can implement this by checking the model's output probability distributions or by having the agent explicitly rate its confidence and use that rating to trigger reasoning mode switches.

Task complexity signals should modulate reasoning depth. If your agent detects that a request has multiple sub-parts, involves multiple entities, references previous context, or includes conditional logic, these are signals that shallow reasoning will likely fail. The agent can recognize these complexity markers—multiple questions in one message, references to previous tickets, if-then constructions in the user's language—and shift to deeper reasoning preemptively rather than failing with shallow reasoning first.

Novelty detection triggers deeper reasoning. If your agent uses embeddings to measure how similar the current input is to previously seen examples, a low similarity score indicates novelty. When the current situation does not closely match any known pattern, shallow reasoning based on pattern matching will not work. The agent should recognize this and engage deeper reasoning to construct a response from first principles rather than by analogy to past cases.

Error recovery often requires depth adjustment. If your agent takes an action based on shallow reasoning and receives feedback that the action was wrong or unhelpful, the next iteration should use deeper reasoning. The error is evidence that the shallow strategy was insufficient for this case. Rather than retrying the same shallow reasoning, the agent should shift to a deeper mode that considers alternative interpretations and response strategies.

Stakes escalation is another adjustment trigger. If your agent starts processing what looks like a routine request but discovers during processing that the dollar amount is unusually high, the customer is in a special status tier, or the action would violate a policy, these discoveries should trigger a shift to deeper reasoning. The initial classification suggested shallow reasoning would suffice, but new information revealed higher stakes that justify more careful thought.

User behavior can signal when deeper reasoning is needed. If a user rejects your agent's first suggestion, that rejection is feedback that shallow reasoning missed something important. The second attempt should employ deeper reasoning to understand what the user actually needs. Similarly, if a user asks follow-up questions that reveal misunderstanding, the agent should increase reasoning depth to resolve the communication gap.

## The Cost of Overthinking

Deep reasoning has real costs that you cannot ignore. The most obvious is token consumption. If your agent generates 2000 tokens of internal chain-of-thought reasoning for every decision, and it makes 50 decisions per user interaction, you are consuming 100,000 tokens per interaction—most of which is invisible to the user but billed by your API provider. At fifteen dollars per million tokens for output, that is one dollar and fifty cents per interaction. If 80 percent of your interactions are routine cases where shallow reasoning would have worked, you are wasting one dollar and twenty cents per interaction on unnecessary reasoning.

Latency is the second cost. Generating 2000 tokens of reasoning takes time—typically 2 to 8 seconds depending on your model and API tier. If your agent is helping a user in real-time, that latency is user-facing wait time. Even if the reasoning produces a marginally better decision, users will often prefer a fast good decision over a slow perfect decision. The latency cost is not just technical—it is user experience degradation.

The cognitive cost of deep reasoning is context window consumption. Every token of reasoning your agent generates occupies space in the context window. If you are working with a 200,000 token context limit and your agent generates 50,000 tokens of reasoning across multiple decision points, you have consumed a quarter of your available context. That space could have been used for more examples, better instructions, relevant documentation, or conversation history. Spending it on reasoning that does not materially improve decisions is a bad tradeoff.

Overthinking can actually degrade decision quality in some cases. When your agent engages in deep reasoning for a simple problem, it can overcomplicate the situation, finding ambiguities and edge cases that are not actually present. A straightforward password reset request becomes a referendum on account security policy, password strength requirements, potential social engineering attacks, and regulatory compliance. The deep reasoning introduces considerations that are theoretically relevant but practically irrelevant to this specific routine request. The final decision might be worse—more hedged, more delayed, more confusing to the user—than the simple immediate response shallow reasoning would have produced.

The operational cost of overthinking is harder to debug and monitor. When your agent uses shallow reasoning, the decision path is usually short and traceable. You can see exactly what pattern it matched and what action it took. When your agent uses deep reasoning, the decision path is a complex web of considerations, hypotheses, and evaluations. Debugging why the agent made a particular decision requires reading through hundreds of tokens of reasoning. Monitoring decision quality requires understanding not just what the agent did but how it thought about the problem. This operational complexity has ongoing costs in engineering time and system observability.

Deep reasoning also increases variance in agent behavior. Shallow reasoning is deterministic or near-deterministic—the same input produces the same output reliably. Deep reasoning involves more model creativity and exploration, which means the same input might produce different reasoning paths and potentially different conclusions on different runs. This variance makes testing harder, A/B comparisons less clean, and user experience less predictable. When consistency matters—regulatory compliance, brand voice, policy adherence—excessive reasoning depth can undermine reliability.

## Implementation Patterns for Reasoning Depth Control

The simplest implementation pattern is explicit mode switching based on task classification. Your agent starts by classifying the incoming request into categories—routine, complex, ambiguous, high-stakes. Each category maps to a reasoning depth tier. Routine requests get a prompt that encourages immediate action without extensive deliberation. Complex requests get a prompt that explicitly instructs the agent to think step-by-step through the problem. This classification-based approach is straightforward to implement and reason about.

You can implement this with different system prompts for different tiers. Your shallow reasoning prompt might say "You are a fast customer support agent. For routine requests, take the standard action immediately without extended analysis." Your deep reasoning prompt might say "You are a thoughtful customer support agent. For each request, analyze what the customer wants, what information is missing, what options are available, and what the implications of each option are before deciding how to respond." The same model behaves differently because the instructions set different reasoning expectations.

Confidence-based depth adjustment requires your agent to self-assess and adapt. You can prompt the agent to rate its confidence in its initial interpretation on a scale from 1 to 10, then use that rating to determine whether to proceed with shallow reasoning or shift to deep reasoning. A confidence rating above 8 triggers immediate action. A rating below 8 triggers a deeper analysis phase. This approach lets the agent adapt to what it encounters rather than relying solely on upfront classification.

Token budget constraints can enforce reasoning depth limits. You can tell your agent it has a budget of 200 tokens for reasoning on routine requests and 2000 tokens for complex requests. The agent knows it needs to be concise for routine cases and can afford to be thorough for complex ones. Some models respond well to explicit token budgets in prompts. Others require post-processing where you truncate reasoning outputs that exceed the budget.

Multi-stage reasoning architectures separate classification from execution. Your first stage is a fast, cheap model that classifies the request and determines appropriate reasoning depth. Your second stage is the actual task execution using the reasoning depth the classifier selected. This separation lets you use a small, fast model for routing decisions and reserve your expensive, capable model for actual reasoning and action. The two-stage approach adds architectural complexity but can be cost-effective at scale.

Reasoning depth can also be controlled through few-shot examples. Your prompt includes examples of shallow reasoning for routine cases—input, brief analysis, action—and examples of deep reasoning for complex cases—input, extended analysis considering multiple factors, action. The agent learns from these examples what reasoning depth is appropriate for situations similar to the examples. This is less explicit than mode switching but can be more flexible as the agent interpolates between the example depths.

Some teams implement reasoning depth as a continuum controlled by a single parameter. You might have a "thinking temperature" that ranges from 0 to 1, where 0 means "take the most obvious action immediately" and 1 means "engage in extended deliberative reasoning." The agent receives this parameter with each request and adjusts its reasoning style accordingly. The parameter can be set based on task classification, user preferences, time constraints, or cost budgets.

Adaptive depth control uses feedback loops to tune reasoning over time. Your agent starts with moderate reasoning depth, measures outcomes, and adjusts depth based on whether deeper reasoning improved results. If a task type consistently succeeds with shallow reasoning, the default depth for that type decreases. If a task type shows improved outcomes with deeper reasoning, the default depth increases. This data-driven approach lets your system learn optimal reasoning depths from production experience.

## Monitoring and Tuning Reasoning Depth

You cannot tune what you do not measure. Effective reasoning depth control requires instrumentation that tracks how much reasoning your agent is doing and what outcomes result. At minimum, you need to log reasoning token count per decision, decision latency, and decision outcome. These three metrics let you analyze the relationship between reasoning depth and decision quality.

The analysis you want to run is stratified by task type. For routine password resets, what is the relationship between reasoning tokens and success rate? If success rate is 99 percent with 50 tokens of reasoning and 99.1 percent with 2000 tokens of reasoning, you are wasting tokens. If success rate is 92 percent with 50 tokens and 99 percent with 2000 tokens, the deeper reasoning is earning its cost. Run this analysis for each major task category to find the optimal reasoning depth per category.

A/B testing is the gold standard for tuning reasoning depth. Run a fraction of your traffic with shallow reasoning and a fraction with deep reasoning for the same task types. Measure success rate, user satisfaction, resolution time, and cost per interaction. The data will show you whether deeper reasoning is actually helping or just burning budget. For many task types, you will find that shallow reasoning performs nearly as well at a fraction of the cost.

User feedback can reveal overthinking. If users frequently interrupt your agent mid-response or express frustration with slow responses, you may be using deeper reasoning than the situation warrants. If users frequently have to retry requests or escalate to humans, you may be using shallower reasoning than the situation demands. Feedback patterns tell you whether your reasoning depth choices align with user needs.

Cost analysis should factor in both API costs and opportunity costs. The API cost of deep reasoning is explicit and easy to measure. The opportunity cost is what else you could do with those tokens—more context, more examples, longer conversations before hitting context limits. If deep reasoning on routine tasks is consuming tokens that force you to truncate conversation history, you are creating a different kind of failure. The full cost picture includes these tradeoffs.

The tuning process is iterative. Start with broad defaults—shallow reasoning for anything that looks routine, deep reasoning for anything that looks complex. Instrument and measure. Identify categories where you are overthinking or underthinking. Adjust the classification rules or reasoning depth settings for those categories. Measure again. Over time, you build a nuanced mapping from task characteristics to optimal reasoning depth.

Outlier analysis helps identify edge cases. Look for individual interactions where reasoning depth was mismatched to the task. A routine request that triggered deep reasoning and succeeded anyway suggests your classification was too conservative. A complex request that used shallow reasoning and failed suggests your classification missed complexity signals. Review these outliers to refine your classification logic.

Reasoning efficiency metrics provide another lens. Calculate reasoning tokens per successful outcome. If your agent uses an average of 800 tokens of reasoning to resolve customer support tickets with a 94 percent success rate, and reducing to 200 tokens maintains 93 percent success, you have found a massive efficiency gain with minimal quality cost. Run this calculation across task categories to find where you are over-investing in reasoning.

Latency budgets should drive depth decisions at the infrastructure level. If your product requirements specify that agent responses must complete within three seconds, and two seconds are consumed by tool calls, you have one second for reasoning and response generation. That constraint limits how much deep reasoning you can afford regardless of task complexity. The latency budget becomes a forcing function that prevents overthinking even when deeper reasoning might theoretically improve outcomes.

## The Reasoning Depth Decision Matrix

Building a practical reasoning depth strategy requires a decision framework. The framework has two primary dimensions: task complexity and decision stakes. Complexity measures how many variables the agent must consider, how ambiguous the input is, and whether the task matches known patterns. Stakes measure the consequences of getting the decision wrong—financial impact, user experience impact, regulatory risk, brand risk.

Low complexity, low stakes tasks demand shallow reasoning. Password resets, status checks, simple FAQ responses fall here. The agent should act immediately based on pattern recognition with minimal deliberation. The cost of overthinking vastly exceeds any quality improvement from deeper analysis. Your goal is sub-second response times and single-digit token counts for reasoning. These interactions represent the bulk of traffic for most production agents, so optimizing them has outsized impact on overall system efficiency.

High complexity, low stakes tasks benefit from medium reasoning depth. A customer asking for product recommendations has a complex decision space but limited downside if the recommendation is not perfect. The agent should consider multiple factors but not exhaustively analyze every possibility. Target moderate reasoning token budgets and two to three second response times. The key is identifying which complexity factors matter most for the specific recommendation and focusing reasoning on those dimensions rather than attempting comprehensive analysis.

Low complexity, high stakes tasks require procedural verification rather than deep reasoning. Large refunds on routine items fall here. The task itself is simple but the stakes justify extra caution. The agent should follow a shallow reasoning path but include explicit verification steps—double-check the amount, confirm authorization limits, validate policy compliance. This is not deep reasoning about what to do, it is careful execution of a known procedure with guardrails. The verification adds latency but not cognitive complexity.

High complexity, high stakes tasks justify full deep reasoning. Novel legal questions, large enterprise negotiations, fraud investigations deserve extended deliberation. The agent should engage multi-step reasoning, consider alternative hypotheses, evaluate evidence carefully, and construct well-justified decisions. Token counts in the thousands and response times of five to ten seconds are acceptable when the decision quality impact is substantial. These are your agent's hardest problems, and treating them as such is correct.

The matrix also reveals combinations to avoid. High complexity, low stakes tasks should not receive deep reasoning proportional to their complexity—the stakes do not justify it. Low complexity, high stakes tasks should not trigger deep reasoning about the complexity—the procedure is clear, just execute it carefully. The matrix guides you to match reasoning investment to the situation rather than defaulting to one extreme or the other.

## Real-World Reasoning Depth Calibration

Calibrating reasoning depth in production requires continuous measurement and adjustment. Start with conservative defaults: use medium reasoning depth for most tasks until you have data showing which tasks can use shallow reasoning without quality loss. This conservative approach prevents premature optimization that saves cost but degrades user experience. Once you have baseline performance data, you can identify opportunities to reduce reasoning depth for high-volume, low-complexity tasks.

Domain-specific patterns emerge from production data. In customer support, you might discover that billing questions require deeper reasoning than technical questions because billing involves policy interpretation and exceptions, while technical questions map to known troubleshooting procedures. In content moderation, you might find that text moderation can use shallow reasoning for clear violations but image moderation needs deeper analysis because visual ambiguity is higher. These patterns are not obvious from first principles—they emerge from observing which tasks succeed with which reasoning depths.

Seasonal and temporal patterns affect optimal reasoning depth. During product launches or major updates, novel questions spike and reasoning depth should increase to handle unfamiliar scenarios. During stable operational periods, most questions are routine and shallow reasoning suffices. Some teams implement dynamic depth adjustment based on novelty scores: measure how different current requests are from historical patterns and increase reasoning depth when novelty is high.

User segment differences also matter. Enterprise customers might expect and appreciate thorough analysis that takes longer but provides detailed explanations. Consumer users might prefer fast responses even if they are less comprehensive. B2B agents serving regulated industries might need deep reasoning for compliance documentation. B2C agents serving casual users might optimize for speed. Reasoning depth becomes a product decision, not just a technical one.

The economic calculation changes based on your business model. If you charge per interaction, deeper reasoning that increases quality might justify higher pricing. If you offer unlimited interactions, minimizing reasoning cost per interaction directly impacts profitability. If your revenue is advertising-based, response speed matters more than reasoning depth because engagement drives revenue. The optimal reasoning depth depends on how your business makes money, not just on technical performance metrics.

Competitive dynamics also influence reasoning depth decisions. If your competitors offer instant responses and users expect that speed, you cannot afford multi-second deep reasoning even on complex tasks. If your market differentiates on quality and thoroughness, shallow reasoning that saves cost but reduces accuracy could damage your competitive position. The right reasoning depth is not just about cost-benefit analysis in isolation—it is about delivering the experience your market demands and your positioning promises.

## The Judgment to Think or Act

Reasoning depth control is ultimately about judgment—teaching your agent to recognize when a situation demands careful thought and when quick action is the right move. This is not a fully automatable decision. Different contexts have different cost-benefit tradeoffs. A customer support agent at a high-touch enterprise software company might justify deeper reasoning on every interaction because customer satisfaction is paramount and volume is low. A customer support agent at a consumer app with millions of interactions per day cannot afford deep reasoning on routine requests.

Your job is to encode the judgment criteria that make sense for your context. What stakes threshold triggers deeper reasoning? What confidence threshold allows shallow reasoning? What novelty signals demand additional thought? What time constraints force fast action even when deeper reasoning would be better? These are strategic decisions that depend on your business model, user expectations, and cost structure.

The best implementations make these tradeoffs visible and tunable. Rather than hardcoding reasoning depth rules into your agent's prompt, expose them as configuration that can be adjusted based on observed performance and changing priorities. As your agent handles more volume, you may identify new task categories that can be handled with shallow reasoning. As your product evolves, new edge cases may emerge that require deeper thought. The system should be designed to accommodate these adjustments without rewriting the core agent logic.

Reasoning depth control is not about making your agent dumber. It is about making your agent wise—knowing when to think deeply and when to act decisively. The most effective agents in 2026 are not the ones that think the hardest on every problem. They are the ones that think exactly as hard as each problem deserves, no more and no less. That efficiency—cognitive, economic, and temporal—is what makes them viable in production at scale.

The next dimension of agent planning architecture is how your agent handles uncertainty and partial information, which we explore in the next subchapter on reasoning under incomplete knowledge.
