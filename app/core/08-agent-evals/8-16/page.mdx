# 8.16 â€” Two-Person Rule Patterns: Model-Model and Model-Human for High Risk

A single model making a high-stakes decision is a single point of failure. The model might misunderstand requirements, misclassify inputs, apply flawed logic, or hallucinate plausible-sounding facts. When that decision controls hundreds of thousands of dollars, a patient's treatment plan, or a legal commitment, relying on one model's judgment is negligence. Two-person rule patterns require agreement between independent decision-makers before execution.

The two-person rule is a control principle from high-security and high-stakes operations: no single individual can complete a critical action alone. Nuclear launch protocols, bank vault access, evidence handling in criminal investigations, these domains all require that two authorized people independently verify and approve before the action proceeds. The principle prevents both malicious abuse and honest mistakes. In agent systems, the two-person rule translates to requiring agreement between two independent decision-makers, either two models with different prompting and context, or a model and a human reviewer, before executing high-risk actions.

## Why Single-Model Approval Fails for High-Risk Actions

A single model, no matter how capable, can make reasoning errors that lead to catastrophic actions. The model might misunderstand requirements, misclassify inputs, apply incorrect logic, or hallucinate facts that seem plausible. When the action is low-risk, these errors are tolerable: you catch them in post-action review, you correct them, you move on. When the action is high-risk, a single error can destroy customer trust, trigger regulatory enforcement, or cause financial losses that exceed the value the entire agent system provides.

Single-model approval fails because models have systematic biases and blind spots. A model trained heavily on certain task types will confidently mishandle edge cases that fall outside its training distribution. A model prompted to optimize for speed will sometimes skip verification steps that slow it down. A model given extensive context will sometimes lose track of critical details buried in that context. These are not random errors; they are predictable failure modes. Relying on a single model means accepting that these failures will eventually occur, and when they do occur on high-risk actions, the consequences are severe.

The problem compounds when the model is prompted for action rather than critique. An agent model is optimized to make decisions and move forward. Its prompt emphasizes taking action, achieving goals, completing tasks. This mindset, valuable for productivity, creates a confirmation bias where the model finds reasons to proceed rather than reasons to stop. The model generates a plan, evaluates whether the plan could work, decides yes, and executes. It does not naturally adopt the adversarial mindset of asking what could go wrong, whether assumptions are valid, or whether alternative interpretations of the requirements might lead to different actions.

Two-person rule patterns address this by introducing a second decision-maker with different incentives, different prompting, or different information access. The second decision-maker is not asked whether the action is good; they are asked whether the action is safe, compliant, and free of the errors that the first decision-maker might have missed. This separation of concerns creates a check-and-balance system that catches errors neither decision-maker would catch alone.

## Model-Model Two-Person Patterns

In a model-model two-person architecture, two separate model calls evaluate the proposed action independently. The primary model, your agent, analyzes the situation and proposes an action. The secondary model, the verifier, receives the proposed action and context, evaluates it for safety and correctness, and issues an approve-or-reject decision. The action proceeds only if the verifier approves.

The key to effective model-model verification is differentiation: the two models must be prompted differently, they must focus on different aspects of the decision, and ideally they should use different model versions or providers to avoid correlated errors. The primary model is prompted as an actor: here is the goal, here are the available actions, decide what to do. The verifier model is prompted as a critic: here is a proposed action, here are the requirements and safety criteria, identify any problems that would make this action incorrect or unsafe.

The verifier prompt emphasizes adversarial thinking. You instruct the model to actively look for mistakes, to question assumptions, to consider edge cases and failure modes. You provide the verifier with requirement specifications, policy documents, examples of past errors, and checklists of common failure patterns. You ask the verifier not just to evaluate whether the action could work, but to identify specific risks, ambiguities, or requirement violations. The verifier's output is a structured decision: approve, reject, or flag for human review, along with reasoning that explains which checks passed and which concerns, if any, were identified.

Differentiation also means providing different context. The primary model receives operational context: the current state, the task goal, recent history, available tools. The verifier receives compliance context: the policies that govern this action type, the requirements specification, recent incidents involving similar actions, the criteria that define a correct action. By separating operational and compliance context, you reduce the risk that both models make the same error due to the same misleading information.

Using different model providers or versions adds another layer of independence. If your primary agent runs on GPT-5, run the verifier on Claude Opus 4.5 or Llama 4. Different models have different training data, different reasoning patterns, different strengths and weaknesses. An error that GPT-5 makes due to a training distribution gap might be obvious to Claude, and vice versa. This cross-model verification is particularly valuable for catching hallucinations, factual errors, and edge-case mishandling.

Model-model verification is fast enough for production use. The verifier call adds latency, typically two to five seconds depending on prompt complexity and model speed, but this overhead is acceptable for high-risk actions. You can optimize by running the verifier call in parallel with action preparation steps, so that by the time the agent is ready to execute, the verification result is already available. The latency cost is small compared to the risk prevented.

## Prompt Design for Verifier Models

The effectiveness of a verifier model depends entirely on prompt design. A poorly prompted verifier will rubber-stamp the primary model's decisions, providing false confidence without actual safety benefit. A well-prompted verifier will catch the errors that matter while minimizing false positives that slow operations unnecessarily.

The verifier prompt begins with role definition: you are a safety and compliance verifier for agent actions, your job is to identify errors and risks before actions are executed, you are not evaluating whether the action is optimal, only whether it is safe and correct. This framing focuses the verifier on finding problems rather than on optimizing solutions.

Next, you provide the decision criteria: the action must comply with these policies, must satisfy these requirements, must avoid these prohibited patterns, must fall within these risk limits. The criteria are explicit and specific, not vague principles. Instead of "the action must be appropriate," you specify "the financial transaction must not exceed ten thousand dollars, must target an account on the approved vendor list, and must include a valid cost center code." Specificity enables the verifier to perform concrete checks rather than subjective judgment.

You then provide the verifier with examples of failure modes to watch for. These are real errors from past incidents or from testing, framed as what-to-avoid patterns. If your agents have previously made errors by confusing customer IDs with account IDs, you include that pattern: check that the customer ID field contains a customer ID, not an account ID, customer IDs start with C, account IDs start with A. If agents have made errors by assuming default values when fields are missing, you include that pattern: check that all required fields are explicitly populated, do not assume defaults.

The verifier prompt includes a structured output format. The verifier must return a decision, approve or reject, and must provide reasoning that lists which checks were performed and what the results were. If the decision is reject, the reasoning must identify specifically which criteria failed or which risks were detected. This structure ensures that verification decisions are transparent and debuggable. You can review the verifier's reasoning to understand why an action was blocked and whether the block was appropriate.

For high-stakes verifications, you use chain-of-thought prompting where the verifier is instructed to think step-by-step through each verification check before issuing a final decision. The verifier examines the proposed action, lists each criterion that must be checked, performs each check explicitly in its reasoning trace, and then summarizes the results. This approach reduces the risk of the verifier skipping checks or making hasty judgments.

You also calibrate the verifier's sensitivity to the risk level of the action. For extremely high-risk actions like irreversible deletions or large financial transactions, you prompt the verifier to reject on any uncertainty or ambiguity, adopting a fail-safe posture. For moderate-risk actions, you prompt the verifier to approve if no clear violations are found, even if minor ambiguities exist. This calibration prevents the failure mode where overly cautious verification blocks too many legitimate actions, eroding trust in the system and leading operators to bypass verification.

## Model-Human Two-Person Patterns

For the highest-risk actions, model-model verification is not sufficient. You need a human in the loop. Model-human two-person patterns require that a human reviewer explicitly approves the action before it executes. The model proposes, the human decides. This pattern is slower and more expensive than model-model verification, but it is the only acceptable approach for actions where errors could cause irreversible harm or violate legal and regulatory requirements that demand human judgment.

Model-human verification requires careful user experience design. The human reviewer must be presented with enough context to make an informed decision but not so much context that decision fatigue sets in. You provide the proposed action, a summary of why the agent chose this action, the key parameters and their values, any flags or warnings the agent or verifier models raised, and the expected outcome. You do not dump the entire conversation history or reasoning trace; you curate the information the human needs.

The approval interface must make it easy to approve correct actions and easy to catch incorrect ones. You highlight the parameters that are most likely to be wrong: the transaction amount, the recipient, the modification being made. You provide comparison context: this customer's previous orders, this account's normal transaction patterns, the policy rule that governs this action type. You ask the human to confirm specific facts rather than to review everything. Instead of "approve this transaction," you ask "confirm that the recipient account number ending in four-seven-two-one is correct for this vendor." Specific confirmation questions catch errors that general reviews miss.

Model-human verification also requires role clarity. The human reviewer is not there to second-guess the agent's judgment on every decision; they are there to catch the errors that automated verification cannot. You train reviewers to focus on the questions that require human judgment: does this action align with customer intent as expressed in the conversation, does this comply with policy requirements that involve subjective interpretation, does this action seem reasonable given domain knowledge that models lack. Reviewers are not QA testers looking for any possible flaw; they are safety checks looking for critical errors.

The timing of human review matters. For some actions, review must happen before execution: the agent proposes the action, the system pauses, a human reviews and approves, then the action executes. For other actions, execution can proceed immediately but a human review happens shortly after, with the ability to halt or reverse the action if problems are found. Pre-execution review is necessary for irreversible actions. Post-execution review is acceptable for actions that can be corrected or reversed if needed. The choice depends on action irreversibility and the acceptable window for catching errors.

Model-human patterns also require fallback procedures for when human reviewers are not available. If the agent proposes a high-risk action at three in the morning and no reviewer is on duty, the action must either wait until a reviewer is available or escalate through an on-call process. You do not default to auto-approval because a human is not around; you defer or escalate. This requires clear policies about action urgency and reviewer availability.

## Approval Consensus and Conflict Resolution

When you implement two-person rule verification, you must define what happens when the two reviewers disagree. In model-model patterns, the primary model proposes an action, and the verifier model rejects it. In model-human patterns, the agent recommends approval, but the human rejects, or the agent flags a concern, and the human overrides. These conflicts require resolution logic.

The safest approach is to require consensus for approval: the action proceeds only if both reviewers agree it should proceed. If either reviewer rejects, the action is blocked. This fail-safe posture prevents any action where there is disagreement about safety or correctness. The downside is that it gives veto power to both reviewers, which can lead to over-blocking if either reviewer is overly cautious or if reviewers are calibrated differently.

An alternative is weighted consensus where one reviewer has final authority but both reviewers' input is considered. In model-human patterns, the human typically has final authority: the verifier model can flag concerns, but the human can override and approve anyway. In model-model patterns, you might give the verifier model final authority: if the verifier rejects, the action is blocked, even if the primary model is confident. The weighting depends on which reviewer is more reliable for the specific action type and failure mode.

For cases where the verifier raises concerns but does not outright reject, you implement a risk-level escalation system. The verifier identifies potential issues and assigns a risk level: low, medium, high. Low-risk concerns are logged but the action proceeds. Medium-risk concerns trigger additional automated checks or a secondary verifier. High-risk concerns block the action and escalate to human review. This graduated response balances safety with operational efficiency.

Conflict resolution also requires feedback loops. When a verifier blocks an action and the block is later determined to have been incorrect, you log that as a false positive and analyze why the verifier over-reacted. When a verifier approves an action that later proves incorrect, you log that as a false negative and update the verifier prompt or checks to catch similar cases. Over time, you tune the consensus logic and verification prompts to minimize both false positives and false negatives.

## Selective Application of Two-Person Rules

Two-person verification is expensive in latency, in compute cost, and in human time when model-human patterns are used. You cannot apply it to every action without destroying agent efficiency. The art is in selective application: identifying which actions require two-person verification and which can proceed with lighter-weight checks.

Action risk classification, as discussed in the previous subchapter, drives this decision. Actions classified as tier four, irreversible or high-impact, always require two-person verification. Actions classified as tier three, consequential writes, require two-person verification if they meet additional criteria: the action affects high-value accounts, the action involves unusual parameters, the agent expressed uncertainty in its reasoning, recent similar actions have had high error rates.

You implement dynamic triggering where the decision to invoke two-person verification depends on runtime context. For a financial transaction, if the transaction amount is below one thousand dollars and targets a frequently used vendor, single-model approval is sufficient. If the amount exceeds ten thousand dollars or targets a new vendor, two-person verification is required. For a customer data update, if the update aligns with a recent authenticated customer request, single-model approval is sufficient. If the update contradicts recent account activity or involves sensitive fields like payment methods, two-person verification is required.

Dynamic triggering also responds to system state. If your agent's recent error rate is elevated, you temporarily lower the threshold for two-person verification, subjecting more actions to secondary review until the error rate returns to normal. If verification is experiencing high false-positive rates that are slowing operations, you temporarily raise the threshold, allowing more actions to proceed with single-model approval. These adjustments are data-driven and reversible, enabling the system to adapt to changing risk levels.

Selective application also considers operational context. In a customer support agent, routine responses proceed with single-model approval. Responses that involve policy exceptions, financial adjustments, or account access changes require two-person verification. In a code review agent, standard refactoring suggestions proceed with single-model approval. Suggestions that modify security-sensitive code, change API contracts, or affect production infrastructure require two-person verification. The selectivity is tuned to the domain and to the failure modes that matter most.

## Architectural Patterns for Two-Person Implementations

Implementing two-person verification requires architectural support for pausing agent execution, routing proposed actions to verifiers, collecting verification results, and resuming or blocking execution based on those results. The architecture must handle asynchronous verification, timeout conditions, and reviewer unavailability.

The basic pattern is the verification queue. When the agent proposes a high-risk action, the action is serialized and placed in a verification queue. The agent's execution pauses or proceeds to other tasks. A verification worker, either a model-based verifier or a human review interface, picks up the action from the queue, performs verification, and writes the result back to the queue. The agent polls for the verification result and proceeds or revises based on the outcome.

For model-model verification, the verification worker is an automated service that calls the verifier model, parses the response, and updates the action status. The latency is typically a few seconds, short enough that the agent can wait synchronously for the result. For model-human verification, the verification worker is a human review interface, and the latency can be minutes to hours. The agent cannot wait synchronously; it must either suspend that task and work on others or escalate the task as blocked pending review.

The architecture must also handle verification timeouts. If a verifier does not respond within a defined time window, the action either fails safe, rejecting by default, or escalates to a higher-level review process. You define timeout policies based on action urgency and risk. For non-urgent actions, a timeout leads to rejection, and the agent retries later or escalates to human review. For urgent actions, a timeout triggers an escalation notification to on-call staff who can perform manual verification.

Logging and auditability are critical. Every proposed action, every verification result, every approval or rejection, and the reasoning behind each decision must be logged with full context. These logs support incident investigation, compliance auditing, and system improvement. When an action causes a problem, you trace back through the logs to see what the agent proposed, what the verifier checked, what the outcome was, and whether the verification process failed. The logs also support training: you use them to generate examples of correct and incorrect actions for few-shot prompts and to identify patterns that need additional verification rules.

## Balancing Safety and Operational Speed

The central tension in two-person rule implementation is the tradeoff between safety and speed. Two-person verification prevents errors, but it slows operations. Too much verification, and your agent becomes so slow that users bypass it or abandon it for faster alternatives. Too little verification, and preventable errors cause incidents that undermine trust in the entire system.

You balance this tradeoff by treating verification as a dial, not a binary switch. For low-risk actions, verification is minimal or absent. For moderate-risk actions, verification is automated and fast. For high-risk actions, verification is rigorous and may involve human review. The dial setting adjusts based on action risk, system error rates, and operational priorities.

You also balance by optimizing the verification process itself. Model-model verification can be faster if you use smaller, faster models for verification when the checks are straightforward. Human review can be faster if you design review interfaces that surface the critical decision points and allow one-click approval for actions that pass obvious sanity checks. Verification does not have to be slow to be effective; it has to be thorough on the dimensions that matter.

Another balancing technique is post-hoc verification for certain action types. The agent executes the action immediately, logs it, and a verifier reviews the action shortly after execution. If the verifier finds a problem, the system alerts the responsible team, and corrective action is taken. This approach works for actions that are reversible or where a short delay in catching errors is acceptable. It provides safety without blocking the agent's operational flow.

Finally, you balance by measuring and communicating the value of verification. Track how many errors verification catches, what the potential impact of those errors would have been if they had reached production, and how much verification costs in latency and compute. Use this data to calibrate verification thresholds and to justify the overhead to stakeholders. When verification is catching real errors that would have caused real harm, the cost is justified. When verification is catching only trivial issues or generating false positives, you dial back the rigor.

## Testing and Validating Two-Person Verification

Two-person verification systems must be tested to ensure they actually catch the errors they are designed to catch and that they do not over-block legitimate actions. Testing requires both positive cases, where verification should approve, and negative cases, where verification should reject.

For negative case testing, you generate or collect examples of incorrect actions: actions with invalid parameters, actions that violate policy, actions that would cause unintended effects. You run these through your two-person verification system and confirm that the verifier rejects them. You also analyze the verifier's reasoning to ensure it identified the correct problem. If the verifier rejects for the wrong reason, the rejection is correct by luck, not by design, and the verification logic needs refinement.

For positive case testing, you generate or collect examples of correct actions and confirm that verification approves them. You measure the false positive rate: how often does verification block actions that should have been approved. High false positive rates indicate that verification is too strict, and you need to relax criteria or improve the verifier's understanding of valid edge cases.

You also test adversarial cases where the primary model is manipulated or prompted to propose unsafe actions deliberately. These tests validate that the verifier is truly independent and not subject to the same manipulation. If adversarial prompting can cause both the primary model and the verifier to agree on an unsafe action, your differentiation is insufficient, and you need to strengthen the verifier's grounding in safety criteria.

Load testing is also necessary. Two-person verification adds latency and compute cost. You test how the system performs under peak load: can verification keep up with the rate of actions the agent proposes, do verification queues grow without bound, do timeouts occur frequently. If verification becomes a bottleneck, you optimize by scaling verification workers, by parallelizing checks, or by selectively reducing verification coverage during peak periods while maintaining it for the highest-risk actions.

## Evolution of Two-Person Rules with Model Capability

As models become more capable, the need for two-person verification will evolve but not disappear. Smarter models make fewer obvious errors, reducing the number of cases where verification catches mistakes. But smarter models also take on more complex and higher-stakes tasks, increasing the severity of the errors they can make. The nature of verification shifts from catching basic mistakes to catching subtle reasoning errors and edge-case mishandling.

With models like GPT-5 and Claude Opus 4.5, two-person verification focuses on errors of misunderstanding: the model misinterprets requirements, misclassifies inputs, applies the wrong policy rule. As models improve, these errors become rarer, but new error types emerge: the model makes a sophisticated judgment call that is defensible but wrong, the model optimizes for the stated goal in a way that violates unstated norms, the model handles a novel situation in a way that is reasonable but misaligned with human values.

Two-person verification for these subtler errors requires verifier models or human reviewers with domain expertise. The verifier must understand not just the rules but the context, the norms, the edge cases that rules do not cover. This means more sophisticated verifier prompts, more domain-specific verification criteria, and likely more human-in-the-loop verification for the most ambiguous cases.

The long-term trajectory is toward verification that focuses less on catching errors and more on ensuring alignment: does the agent's action reflect the intent behind the request, does it align with organizational values, does it handle tradeoffs in a way that stakeholders would endorse. This is a harder verification problem than checking rule compliance, but it is the problem that matters as agents move from executing scripted tasks to making judgment calls in ambiguous situations.

Two-person rule patterns are the strongest form of verification you can implement without removing agent autonomy entirely. They ensure that high-risk actions receive independent review before execution, catching errors that single-model systems would miss. Implementing these patterns requires careful design of verifier prompts, clear criteria for when two-person verification is required, architectural support for verification workflows, and continuous testing and refinement. The next subchapter examines agent isolation and sandboxing, architectural techniques that limit the blast radius of agent errors by constraining what resources and systems the agent can access.

