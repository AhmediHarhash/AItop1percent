# 8.7 â€” Time and Latency Guardrails: Preventing Runaway Agents

In August 2025, a logistics optimization company deployed an agent system to coordinate warehouse inventory transfers across fifteen distribution centers. The agent was designed to analyze stock levels, predict demand, and autonomously generate transfer orders between facilities. During the first production run, the agent entered what engineers later called a "recursive refinement loop." It generated an initial transfer plan, then decided the plan needed optimization, so it re-analyzed the same data with slightly different parameters. That analysis suggested further refinement. Then another. The agent ran for eleven hours before an engineer manually terminated it, having consumed $47,000 in API costs and blocked three critical overnight inventory decisions. The agent never completed its task. It simply kept refining, convinced each iteration brought it closer to the optimal solution.

The root cause was not a bug in the agent's logic. The system worked exactly as designed: it had been given an optimization goal with no time constraints and no maximum iteration limit. The engineering team had focused entirely on decision quality, assuming the agent would naturally converge to a solution. They had not considered that optimization problems rarely have clear stopping points. Without explicit time and iteration guardrails, the agent defaulted to its training behavior of continuous improvement. The company spent the next two weeks rebuilding their agent architecture with hard limits on execution time, iteration counts, and cumulative token usage. They learned what every production agent system eventually discovers: agents without temporal boundaries are agents waiting to fail.

## The Asymmetry of Agent Time

Traditional software has predictable execution profiles. An API endpoint takes 200 milliseconds or it times out. A batch job runs for thirty minutes or it fails. But agent systems introduce fundamental timing asymmetry. An agent might solve a simple request in ten seconds or spend ten hours on the same request if it encounters an edge case that triggers deep reasoning. The same agent, given the same type of task on different days, can vary in execution time by two orders of magnitude.

This asymmetry creates operational risk. You cannot predict agent runtime from input characteristics alone. A three-sentence user request might trigger a thirty-minute research process if the agent decides it needs comprehensive context. A complex multi-step workflow might complete in five minutes if the agent finds a direct path through its tool set. Standard timeout mechanisms fail because they are either too restrictive, killing legitimate long-running tasks, or too permissive, allowing runaway executions to consume resources for hours.

The solution is not a single timeout value. It is a layered system of temporal guardrails that adapt to agent behavior in real time. You need guardrails at multiple levels: maximum total execution time, maximum time per reasoning step, maximum time between observable outputs, and maximum iterations on repeated patterns. Each guardrail serves a different failure mode. Total execution time prevents infinite loops. Per-step time prevents individual reasoning deadlocks. Output interval time detects silent failures. Iteration limits prevent refinement spirals. Together, they create a temporal safety net that allows legitimate agent flexibility while preventing catastrophic time overruns.

## Maximum Execution Time: The Hard Deadline

Every agent invocation needs a hard deadline. This is not negotiable. The deadline is the absolute maximum time the agent is allowed to run before forced termination. It applies regardless of what the agent is doing, what tools it is calling, or how close it believes it is to completion. When the deadline arrives, the agent stops.

Setting the deadline requires understanding your operational context. For user-facing agents handling support requests, the deadline might be sixty seconds because users will not wait longer. For background agents processing data pipelines, the deadline might be six hours because that is your batch window. For research agents exploring complex questions, the deadline might be thirty minutes because that balances thoroughness with resource costs. The deadline is not about average execution time. It is about the maximum time you are willing to pay for, wait for, or risk on a single agent invocation.

Implement the deadline at the orchestration layer, not within the agent itself. The agent cannot be trusted to enforce its own time limits because the agent might be stuck in a loop that prevents it from checking the deadline. The orchestration layer runs in a separate execution context. It tracks elapsed time from the moment the agent starts. When elapsed time exceeds the deadline, the orchestration layer sends a termination signal to the agent runtime. If the agent does not respond within five seconds, the orchestration layer kills the agent process entirely. This is a forced stop, not a graceful shutdown.

When a deadline is reached, your system must decide what to return to the caller. You have three options. First, return an error indicating the agent timed out with no partial results. This is appropriate when partial work has no value and the caller needs to know the task failed. Second, return whatever partial results the agent produced before termination. This is appropriate when the agent generates incremental outputs and partial progress is better than nothing. Third, return a degraded response from a fallback system. This is appropriate when the agent is an enhancement to a baseline system and you can fall back to the non-agent path. The choice depends on your user experience requirements and whether partial agent work is meaningful.

## Per-Step Time Limits: Preventing Reasoning Deadlock

Agents operate in discrete steps. Each step is a reasoning cycle: the agent receives context, generates a thought or action, executes tools, and produces output. Most steps complete in seconds. But certain reasoning patterns can cause individual steps to hang. The agent might enter a complex reasoning chain that requires generating thousands of tokens. It might call a tool that deadlocks or waits indefinitely. It might hit a model inference timeout that leaves it in a suspended state. A single hung step can block the entire agent, consuming resources while producing no progress.

Per-step time limits prevent individual reasoning deadlocks from cascading into full agent failures. You define a maximum duration for any single agent step. If a step exceeds that duration, it is terminated and the agent moves to error handling. The per-step limit is much shorter than the total execution deadline. Where total execution might allow thirty minutes, per-step limits are typically sixty to one hundred twenty seconds. The limit reflects the maximum time you expect a single reasoning and tool execution cycle to take.

Implementing per-step limits requires instrumentation in your agent framework. Before each step begins, record the step start time. After the step completes or before the next step begins, check elapsed time. If elapsed time exceeds the per-step limit, terminate the current step and log a step timeout event. Then decide whether to retry the step, skip it and continue the agent workflow, or fail the entire agent invocation. The decision depends on whether the step is critical to the agent's goal and whether retrying is likely to succeed.

Per-step timeouts catch a different failure class than total execution timeouts. A total execution timeout catches agents that loop forever. A per-step timeout catches agents that get stuck on a single hard problem. The logistics company from the opening story would have benefited from per-step limits. Their agent was not looping at the workflow level; it was spending hours on individual optimization steps, re-analyzing the same data repeatedly. A per-step limit of ninety seconds would have terminated each overlong analysis and forced the agent to either proceed with the current plan or fail explicitly.

## Output Interval Guardrails: Detecting Silent Agents

Some agent failures are not fast loops or long-running steps. They are silent hangs. The agent appears to be running. The process is alive. But no output is produced. No tool calls are made. No logs are written. The agent is stuck in an internal state that prevents observable progress but does not trigger crashes or exceptions. These silent failures are the hardest to detect because standard timeout mechanisms see an active process and assume it is working.

Output interval guardrails solve this by measuring time between observable outputs. An observable output is any event that indicates the agent is making progress: a tool call, a reasoning log entry, a token streamed to the user, a status update. You define a maximum interval between outputs. If the agent goes silent for longer than that interval, you assume it is stuck and terminate it. The interval is longer than per-step limits because legitimate reasoning can take time. Typical output intervals are three to five minutes for background agents, thirty to sixty seconds for interactive agents.

Implementing output interval guardrails requires a heartbeat mechanism. Every time the agent produces an observable output, it updates a last-output timestamp. A separate monitoring process checks the timestamp every ten seconds. If the time since last output exceeds the configured interval, the monitor raises an alert and optionally terminates the agent. This runs independently of the agent itself, so it works even if the agent's internal monitoring is broken.

Output interval guardrails catch failures that per-step and total execution timeouts miss. They catch deadlocks in tool execution where the tool call hangs without returning. They catch inference stalls where the model API accepts the request but never responds. They catch framework bugs where the agent enters a busy-wait state. A financial services company in late 2025 used output interval guardrails to detect a subtle bug in their agent's document retrieval logic. The retrieval tool occasionally entered an infinite pagination loop, fetching the same page of results repeatedly without advancing. The agent was active, the process was running, but no new documents were retrieved. The output interval guardrail detected six minutes without a new retrieval result and killed the agent, preventing a midnight batch job from hanging indefinitely.

## Iteration Count Limits: Stopping Refinement Spirals

Agents often repeat actions. They call the same tool multiple times with different parameters. They retry failed operations. They refine outputs based on feedback. Repetition is normal and necessary. But unbounded repetition is a failure mode. An agent that retries the same failed tool call one hundred times is broken. An agent that regenerates the same output fifty times, each time claiming to improve it, is stuck. You need iteration count limits to detect and stop repetition-based failures.

Iteration limits apply to specific patterns, not to the agent as a whole. You define limits for repeated tool calls, repeated reasoning patterns, and repeated output regeneration. For example, you might allow an agent to call the same tool up to five times in a single workflow. After five calls, further attempts to call that tool are blocked and the agent must either proceed without that data or fail explicitly. Similarly, you might allow the agent to regenerate an output up to three times based on validation feedback. After three attempts, the agent must return its best effort even if validation still flags issues.

Implementing iteration limits requires state tracking across agent steps. Your agent framework maintains counters for each tracked pattern. When the agent calls a tool, increment the counter for that tool. When the agent regenerates an output, increment the regeneration counter. Before allowing the next iteration, check the counter against the limit. If the limit is exceeded, block the action and return an error to the agent. The agent's error handling logic must then decide how to proceed.

Iteration limits prevent the most common runaway agent pattern: the refinement spiral. The agent generates output, evaluates it, decides it is insufficient, regenerates it, evaluates again, decides it is still insufficient, regenerates again. Each iteration produces marginally different output but never crosses the threshold the agent has set for success. Without an iteration limit, this continues until the total execution timeout. With an iteration limit, the refinement spiral stops after a defined number of attempts, forcing the agent to either accept its current output or fail with a clear signal that it could not meet its own quality bar.

A healthcare technology company in mid-2025 implemented iteration limits after their patient messaging agent entered refinement spirals on empathetic responses. The agent would draft a message, evaluate its empathy score, find it lacking, redraft, re-evaluate, repeat. The agent had been trained to prioritize empathy but had no concept of diminishing returns. After implementing a three-iteration limit, the agent was forced to send its third draft even if it scored lower than the agent preferred. Patient satisfaction scores were unaffected. The messages were already empathetic by the second draft. The agent's perfectionism was operational overhead, not quality improvement.

## Token Budget Guardrails: Limiting Cumulative Cost

Agent runtime is not just a time cost. It is a financial cost. Every token the agent consumes, whether in input prompts or output generation, incurs API charges. Long-running agents can accumulate enormous token counts. The logistics company from the opening story spent $47,000 in eleven hours because their agent generated millions of tokens analyzing and re-analyzing the same inventory data. Token budget guardrails prevent this by capping the total number of tokens an agent can consume in a single invocation.

Token budgets are defined per agent invocation, not per step. You allocate a maximum token budget when the agent starts. As the agent runs, your orchestration layer tracks cumulative token usage across all model calls. When cumulative usage approaches the budget, the orchestration layer warns the agent. When cumulative usage exceeds the budget, the orchestration layer terminates the agent. The budget is a hard cap. The agent does not get to finish its current thought or complete its current step. It stops immediately.

Setting token budgets requires understanding your cost tolerance and typical agent behavior. A customer support agent handling simple queries might have a budget of 10,000 tokens, enough for several reasoning steps and tool calls. A research agent conducting deep analysis might have a budget of 500,000 tokens, allowing hours of exploration. The budget reflects the maximum amount you are willing to spend on a single agent task. It is not the expected token usage. It is the ceiling beyond which you prefer failure to continued execution.

Token budgets interact with time guardrails. An agent might hit its time deadline before its token budget, or it might hit its token budget before its time deadline. Both are valid termination conditions. In practice, time deadlines catch agents that loop with low token usage per iteration, while token budgets catch agents that generate enormous outputs or analyze massive contexts. A financial analysis agent in early 2026 hit token budgets regularly because it ingested full quarterly earnings transcripts into context. The time guardrails allowed the agent to run for ten minutes, but the token budget capped it at 300,000 tokens. This prevented analysts from accidentally triggering $200 analyses on simple questions that pulled in too much context.

## Adaptive Guardrails: Adjusting Limits Based on Context

Fixed guardrails work for most cases. But elite agent systems use adaptive guardrails that adjust limits based on the specific task and observed agent behavior. Adaptive guardrails recognize that not all tasks are equal. A high-priority user request might deserve a longer time budget than a background analytics job. A task flagged as complex by an initial classifier might receive higher iteration limits than a task classified as routine.

Adaptive guardrails use runtime signals to adjust limits dynamically. For example, if an agent consistently produces high-quality outputs in its first iteration, the system might reduce the iteration limit for that agent to two. If another agent frequently hits time limits without completing tasks, the system might increase its time budget by 50 percent or route its tasks to a different agent architecture. Adaptation is based on historical data, not real-time negotiation. The agent does not request more time. The orchestration layer decides based on patterns.

Implementing adaptive guardrails requires telemetry and feedback loops. Your system logs every agent invocation with its input characteristics, assigned guardrails, actual resource usage, and outcome. Periodically, analyze this data to identify patterns. If certain task types consistently complete in half the allocated time, reduce their time budgets. If certain agents consistently hit iteration limits and fail, investigate whether those limits are too restrictive or whether the agent architecture needs revision. Adaptation happens offline, in configuration updates, not inline during agent execution.

Adaptive guardrails prevent two failure modes. First, they prevent over-provisioning, where you give every agent worst-case resource limits and waste capacity on simple tasks. Second, they prevent under-provisioning, where you set tight limits based on average cases and block legitimate long-running tasks. A legal research firm in late 2025 implemented adaptive guardrails for their contract analysis agents. Routine NDAs completed in two minutes, so their time budget was three minutes. Complex merger agreements took twenty minutes, so their time budget was thirty minutes. The system classified incoming contracts using document length and clause complexity, then assigned appropriate budgets. This reduced average cost per analysis by 40 percent while maintaining 99th percentile success rates for complex documents.

## Guardrail Violations: Logging and Alerting

When an agent hits a guardrail, your system must log the violation and decide whether to alert. Every guardrail violation is logged with full context: which limit was exceeded, the agent's state at termination, cumulative resource usage, the input task, and any partial outputs produced. These logs are critical for post-incident analysis and guardrail tuning. Without detailed logs, you cannot distinguish between legitimate complexity that requires higher limits and runaway behavior that requires architectural fixes.

Alerting decisions depend on violation frequency and impact. A single time limit violation is normal. Agents occasionally encounter hard problems that take longer than expected. But ten time limit violations in an hour indicates a systematic problem. Your alerting system should track violation rates, not individual violations. When the violation rate for a specific guardrail exceeds a threshold, raise an alert to the engineering team. The threshold is typically defined as a percentage of total agent invocations. For example, alert if more than 5 percent of agents hit time limits, or if more than 2 percent hit iteration limits.

Different guardrails have different alerting thresholds because they indicate different failure modes. Token budget violations are often configuration issues: the budget is set too low for the task complexity. Time limit violations might indicate performance problems in tool execution or model latency spikes. Iteration limit violations usually indicate agent logic bugs or insufficiently specified tasks. Your alerting system should differentiate these and route alerts to appropriate teams. Token budget alerts go to cost management. Time limit alerts go to infrastructure. Iteration limit alerts go to agent logic owners.

When a guardrail violation occurs during a user-facing interaction, your system must provide a meaningful error message. Do not tell the user "agent timeout." Tell them what happened and what they can do. If the agent hit a time limit while researching a complex question, tell the user "this request is taking longer than expected, try breaking it into smaller questions." If the agent hit an iteration limit while refining an output, tell the user "we generated a response but could not refine it further, here is our best attempt." The error message acknowledges the failure while providing actionable guidance.

## Testing Guardrails: Forcing Failure Modes

Guardrails only protect you if they work. You cannot assume guardrails function correctly until you test them by forcing the failure modes they are meant to prevent. This means deliberately creating runaway agents in staging environments and verifying the guardrails terminate them correctly. If your time limit is thirty minutes, deploy an agent that loops forever and confirm it is killed at thirty minutes. If your iteration limit is five, deploy an agent that retries the same action indefinitely and confirm it stops at five.

Testing guardrails requires test harnesses that simulate failure modes. Build agents that loop. Build agents that generate infinite output. Build agents that call the same tool one hundred times. Build agents that hang silently without producing outputs. Run each test agent in your staging environment with production guardrails enabled. Measure whether the guardrail triggers, how long it takes to trigger, and what happens after termination. Verify logs are written, alerts are raised, and error messages are returned correctly.

Guardrail testing is not a one-time exercise. It is part of your continuous testing pipeline. Every time you modify agent architecture, tool implementations, or orchestration logic, re-run guardrail tests. Changes in agent behavior can invalidate guardrail assumptions. A new reasoning pattern might take longer per step, requiring per-step limit increases. A new tool might have higher latency, requiring output interval adjustments. Regression testing ensures guardrails remain effective as your agent system evolves.

A SaaS platform in early 2026 discovered their output interval guardrails had stopped working after a framework upgrade. The upgrade changed how tool calls were logged, and the heartbeat mechanism was no longer receiving updates. Silent hang detection was disabled for three weeks before a test caught it. The company now runs guardrail tests in their CI pipeline. Every deployment includes synthetic runaway agents. If any guardrail fails to trigger, the deployment is blocked. This has prevented two similar regressions in the months since.

## Guardrails as Operational Contracts

Guardrails are not just safety mechanisms. They are operational contracts between your agent system and its stakeholders. When you deploy an agent with a thirty-minute time limit, you are promising users that no agent invocation will consume more than thirty minutes of their time. When you set a token budget of 200,000, you are promising finance that no single agent run will cost more than a defined amount. When you set iteration limits, you are promising engineering that agents will not loop indefinitely and degrade system performance.

These contracts enable trust. Product teams trust agents will not hang user requests. Finance teams trust agents will not cause budget overruns. Operations teams trust agents will not consume all available compute capacity. Without explicit guardrails and the operational contracts they represent, agent adoption remains limited to low-risk use cases. With guardrails, you can deploy agents in production-critical paths because stakeholders understand the boundaries.

Documenting guardrails is as important as implementing them. Your agent documentation must specify every guardrail: maximum execution time, per-step time limits, output intervals, iteration limits, and token budgets. It must explain what happens when each limit is reached. It must provide guidance on when to increase limits and when to investigate agent logic instead. This documentation is your operational contract made explicit. It is the reference stakeholders use to understand what protections exist and what risks remain.

Your next challenge is ensuring those guardrails remain effective even when the entire agent system needs to be stopped immediately. Guardrails prevent individual agent failures, but production systems also need mechanisms to stop all agents at once in response to systemic issues, security incidents, or critical bugs. Agent kill switches provide that emergency shutdown capability, and their design determines whether you can respond to crises in seconds or suffer through chaotic manual intervention.

