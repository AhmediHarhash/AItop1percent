# 1.8 — Agent vs RAG vs Fine-Tuning: Architectural Decision Matrix

Building an agent when RAG would suffice is the most expensive architectural mistake in AI engineering. In March 2025, a legal technology startup spent four months and roughly 800,000 dollars building an autonomous agent system to help attorneys draft contract clauses when their users actually needed fast, reliable search. The agent could search legal databases, retrieve precedent language, analyze clause variations, and generate customized text based on the specific deal parameters. The system used a sophisticated multi-step reasoning loop with reflection, self-critique, and iterative refinement. The founders believed they were building the future of legal AI.

When they piloted the system with a mid-sized law firm, adoption was near zero after the first week. The feedback was consistent: "It takes too long, the output is not reliably better than what I can write myself, and I do not trust it enough to use it without heavy editing." One senior partner said, "I would rather have a fast, reliable search tool that shows me relevant precedents and lets me copy-paste. I do not need the AI to draft the clause. I need it to find examples so I can draft it myself."

The startup had built an agent when what users actually needed was RAG. They had optimized for autonomy and sophistication when users wanted speed, transparency, and control. A simple retrieval system with good ranking would have cost a tenth as much to build, shipped in a month instead of four, and delivered more value.

This is one of the most common and expensive mistakes in AI product development: choosing the wrong architectural pattern for the problem. Agents, RAG, and fine-tuning are not interchangeable. They solve different problems, have different cost structures, and impose different constraints. Using the wrong pattern does not just waste resources. It often means the product fails entirely, because the pattern does not match what users need or what the business can sustain.

## The Three Patterns and What They Actually Do

Before you can choose between agents, RAG, and fine-tuning, you need to understand what each pattern is designed for.

**Retrieval-Augmented Generation (RAG)** is a pattern for grounding language model outputs in external knowledge. The system retrieves relevant information from a knowledge base—documents, databases, APIs—and includes that information in the prompt to the language model. The model generates a response based on both its training and the retrieved context. RAG does not change the model's behavior or capabilities. It changes what the model knows at inference time.

RAG is for tasks where the model needs access to information that was not in its training data, or where that information changes frequently. Customer support bots that need to reference up-to-date product documentation. Research tools that need to cite specific papers. Internal tools that need to query proprietary company data. The model itself is general-purpose; RAG makes it situationally knowledgeable.

**Fine-tuning** is a pattern for modifying a pre-trained model's behavior by training it on task-specific data. You start with a general-purpose base model, continue training it on examples of the task you care about, and the model learns to perform that task better or in a specific style. Fine-tuning changes the model's weights. It makes the model better at a particular task, better at imitating a particular style, or better aligned with particular values.

Fine-tuning is for tasks where the model's default behavior is not good enough and you cannot fix it with prompting alone. Domain-specific language generation, where the model needs to use technical jargon correctly. Stylistic adaptation, where the model needs to write in a very specific tone. Instruction-following improvements, where the model needs to be more reliable at structured tasks. You are not adding knowledge; you are changing how the model behaves.

**Agents** are systems that plan, take actions, observe outcomes, and decide what to do next. An agent uses a language model as a reasoning engine, but the model is one component in a larger loop that includes tool calling, state management, and decision logic. Agents are for multi-step tasks that require interacting with external systems, adapting based on feedback, and operating semi-autonomously.

Agents are for tasks that cannot be solved in a single model call. Tasks that require retrieving information, reasoning about it, taking an action, observing the result, and deciding whether to continue or try something else. Research tasks, workflow automation, complex decision-making, and any task where the path to completion is not known in advance.

The key distinctions: RAG is about what the model knows. Fine-tuning is about how the model behaves. Agents are about what the model can do through multi-step interaction with external systems.

## The Decision Matrix: When to Use Each Pattern

The choice between these patterns comes down to five key questions.

### Does the Task Require External, Up-to-Date, or Proprietary Knowledge?

If the task requires information that the model was not trained on, or information that changes over time, you need RAG or an external system. If the task requires only general knowledge that the model already has, you do not need RAG.

Examples where RAG is necessary: answering questions about internal company policies, generating summaries of recent news articles, citing specific research papers, querying customer account data, referencing product documentation.

Examples where RAG is not necessary: writing creative fiction, generating code for common algorithms, translating between languages, explaining general concepts, drafting generic email templates.

If you need RAG, the next question is whether you also need multi-step reasoning. If the task is "retrieve relevant documents and summarize them," RAG alone is sufficient. If the task is "retrieve documents, identify gaps, search for additional sources, synthesize findings, and produce a structured report," you need an agent that uses RAG as one of its tools.

### Does the Task Require Behavior or Style That the Base Model Cannot Achieve with Prompting?

If you can get the model to do what you want by writing a good prompt, you do not need fine-tuning. Fine-tuning is for cases where prompting is not enough: the model does not reliably follow complex instructions, it does not use domain-specific terminology correctly, it does not write in the required style, or it makes too many mistakes on structured tasks.

Examples where fine-tuning helps: medical report generation where the model needs to use clinical terminology precisely, code generation in a proprietary internal framework, customer service responses that need to match a very specific brand voice, instruction-following for complex multi-step tasks where the base model is unreliable.

Examples where prompting is sufficient: general summarization, question answering, translation, creative writing with flexible style requirements, simple classification tasks.

Fine-tuning also makes sense when you need to reduce prompt length. If your prompt is 5,000 tokens of instructions and examples, fine-tuning can bake that behavior into the model and let you use a shorter, cheaper prompt at inference time. The trade-off is upfront cost and complexity: you need labeled data, training infrastructure, and a process for iterating on the fine-tuned model.

### Does the Task Require Multi-Step Reasoning, Tool Use, or Interaction with External Systems?

If the task can be solved in a single model call—input goes in, output comes out, done—you do not need an agent. If the task requires taking actions, observing results, and deciding what to do next, you probably need an agent.

Examples where agents make sense: "Research this company and compile a due diligence report" requires searching databases, reading documents, identifying gaps, searching again, and synthesizing findings. "Deploy this code change" requires reading the current codebase, writing tests, running tests, checking results, and deciding whether to proceed or roll back. "Book a flight that meets these constraints" requires searching flights, checking availability, comparing prices, and making a purchase.

Examples where agents are overkill: "Summarize this document" is a single-step task. "Classify this support ticket into one of ten categories" is a single-step task. "Generate three subject line variations for this email" is a single-step task. Adding an agent loop to these tasks adds latency and cost without adding value.

The litmus test: if you can describe the task as a single input-output transformation, you do not need an agent. If the task requires conditional logic—"do X, and if Y happens, do Z"—you might need an agent, or you might need traditional code with LLM components. Not everything that involves decision-making requires an agent.

### What Are Your Latency and Cost Constraints?

RAG adds latency (retrieval time plus generation time) but not much cost beyond the longer prompt. Fine-tuning adds upfront cost but reduces inference cost if it lets you use shorter prompts or smaller models. Agents add both latency (multiple sequential model calls and tool calls) and cost (many more tokens consumed per task).

If your use case requires sub-second response times, agents are probably not viable unless the task is very simple. A typical agent task with five reasoning steps and five tool calls might take 10 to 30 seconds, even with fast models and optimized tool execution. If you need to respond to user queries in real-time, you need a single-shot pattern: RAG, fine-tuned model, or carefully prompted base model.

If your use case has a strict cost budget per task, agents might price you out. An agent that makes 20 model calls consumes 20 times as many tokens as a single-shot system. If your budget is "less than one cent per task," agents are not viable. If your budget is "less than one dollar per task," agents might work for high-value use cases but not for high-volume use cases.

Cost and latency are often deal-breakers. A technically superior agent system is useless if it costs too much or runs too slowly for the business model.

### What Is the Risk Profile and What Level of Human Oversight Is Acceptable?

RAG systems are relatively low-risk because the model is only generating text based on retrieved information. The worst-case failure is a bad summary or a hallucinated claim, which is bad but usually not catastrophic. Fine-tuned models have similar risk profiles, unless the fine-tuning was done poorly and introduced new failure modes.

Agents are higher-risk because they take actions. An agent that can send emails might send emails to the wrong people. An agent that can modify databases might corrupt data. An agent that can execute code might introduce bugs or security vulnerabilities. The risk scales with the power of the tools the agent has access to.

If the task involves actions that cannot be undone, or actions with significant consequences, you need heavy human oversight, sandboxing, or approval workflows. This often means the agent is not fully autonomous; it is a copilot that proposes actions and waits for human approval. In that case, you need to weigh whether the complexity of an agent is worth it, or whether a simpler assisted workflow would achieve the same outcome with less risk.

## Combining Patterns: When You Need More Than One

The patterns are not mutually exclusive. Many production systems combine them.

**RAG-powered agents** are common. The agent uses retrieval as one of its tools. When the agent needs information, it calls a retrieval tool, gets relevant documents, incorporates them into its reasoning, and decides what to do next. This is how most research agents and question-answering agents work. The agent provides the multi-step reasoning and planning; RAG provides the knowledge grounding.

**Fine-tuned agent models** are less common but increasingly valuable. If your agent needs to operate in a specialized domain—medical, legal, financial—you might fine-tune the underlying reasoning model on domain-specific data to make it more reliable at understanding domain concepts, using domain terminology, and following domain-specific instructions. The fine-tuning improves the agent's reasoning quality; the agentic loop provides the multi-step capabilities.

**RAG with fine-tuned models** is a powerful combination for domain-specific applications. You fine-tune the model to understand domain terminology and follow domain-specific instructions, and you use RAG to ground its outputs in up-to-date or proprietary information. A medical diagnosis assistant might be fine-tuned on clinical language and use RAG to retrieve patient history and recent lab results. The fine-tuning makes the model better at medical reasoning; RAG makes it knowledgeable about the specific patient.

The key is to understand what each pattern contributes and only pay for the complexity you need. Do not build an agent if RAG solves the problem. Do not fine-tune if prompting works. Do not add RAG if the model already knows what it needs to know.

## Common Mistakes: Using the Wrong Pattern

**Mistake one: Using agents when RAG suffices.** This is the legal tech startup's mistake. They built a multi-step agent with reflection and self-critique when users just needed fast, reliable retrieval. Agents are seductive because they feel futuristic, but they are expensive, slow, and complex. If the task is "find relevant information and present it," RAG is almost always better than an agent. Save agents for tasks that actually require multi-step reasoning.

**Mistake two: Using fine-tuning when RAG works.** A team wants their model to answer questions about internal company policies. They consider fine-tuning the model on policy documents. This is the wrong move. Policies change frequently. Fine-tuning is slow and expensive to update. RAG is much better: index the policy documents, retrieve relevant sections at inference time, and generate answers based on retrieved context. Fine-tuning is for behavior changes, not knowledge updates.

**Mistake three: Using RAG when fine-tuning is needed.** A team wants their model to write in a very specific brand voice: casual but authoritative, friendly but not overly enthusiastic, concise but not terse. They try to achieve this with prompt engineering and RAG, retrieving examples of good brand writing and including them in the prompt. This works poorly because style is hard to specify in prose and the model does not reliably imitate examples. Fine-tuning on hundreds or thousands of examples of good brand writing works much better. The model learns the style implicitly and applies it consistently.

**Mistake four: Using agents for tasks that should be deterministic code.** Not every multi-step workflow needs an agent. If the steps are well-defined and the logic is deterministic, write code. Use if-else statements, loops, and function calls. Only use an agent when the steps are not fully known in advance, when the agent needs to adapt based on observations, or when the reasoning is too complex to hard-code. Agents are for handling ambiguity and variability, not for replacing traditional software engineering.

**Mistake five: Building agents without the evaluation infrastructure to support them.** Agents are much harder to evaluate than single-shot systems. If you do not have task-level evals, trajectory analysis, and the ability to measure success across multi-step episodes, you are not ready to ship an agent. This is the subject of later chapters, but the point here is: do not choose the agent pattern unless you are also choosing to invest in agent-specific evaluation infrastructure. Without that infrastructure, you will not know if your agent works.

## The Decision Tree in Practice

Here is a simplified decision tree for choosing between patterns:

**Start with: Can the task be solved in a single model call?** If yes, you do not need an agent. Go to the next question. If no, you probably need an agent or traditional code with LLM components.

**Does the model need external knowledge to complete the task?** If yes, use RAG. If no, move on.

**Can you achieve the desired behavior with prompting?** If yes, stop here. Use a prompted model, optionally with RAG. If no, consider fine-tuning.

**Is the cost and time to fine-tune worth it?** Fine-tuning requires labeled data, training time, and ongoing maintenance. If the task is high-value and high-volume, fine-tuning might pay off. If the task is low-volume or rapidly changing, the cost might not be justified.

**Does the task require multi-step reasoning or tool use?** If yes, build an agent. If no, you can likely solve it with RAG, fine-tuning, or a well-prompted base model.

**Can you afford the latency and cost of an agent?** If no, you need to simplify the task or use a different pattern. If yes, proceed with agent development, but invest heavily in evaluation and safety infrastructure.

This tree is not exhaustive, but it covers the most common decision points.

## Real-World Case Studies

**Case one: Customer support chatbot.** A SaaS company wants to automate Tier 1 support. The task is: answer common customer questions using information from the knowledge base. This does not require multi-step reasoning. It does require external knowledge that changes as the product evolves. The right pattern: RAG. Retrieve relevant help articles, generate an answer grounded in those articles, provide citations so users can verify. An agent would be overkill. Fine-tuning might help with style but is not necessary if prompting achieves acceptable tone.

**Case two: Code review assistant.** An engineering team wants AI to review pull requests and suggest improvements. The task requires reading code, understanding context, identifying issues, and generating suggestions. This could be done in a single call with a long prompt that includes the diff and relevant context. However, for large pull requests, the context might exceed the model's window, so the system might need to chunk the diff and process it in multiple steps. If the system also needs to run tests, check code style, or query documentation, it might need tool use. The right pattern: start with a single-shot model that reads diffs and generates suggestions. Add RAG if the model needs to reference coding standards or API docs. Consider light agent behavior if the system needs to run linters or tests and incorporate results into the review.

**Case three: Financial research agent.** An investment firm wants AI to compile research reports on public companies. The task requires searching SEC filings, news articles, earnings transcripts, and proprietary databases; extracting key facts; identifying trends; and synthesizing findings into a structured report. This is clearly multi-step. It requires tool use (search, retrieval, data APIs) and adaptive reasoning (if revenue is declining, dig deeper into the cause). The right pattern: agent with RAG. The agent orchestrates the research workflow; RAG tools provide access to documents and data. Fine-tuning might help if the agent needs to use financial terminology precisely or follow firm-specific report formats.

**Case four: Content moderation.** A social media platform wants to classify posts as safe or unsafe. The task is a single classification call, but it requires understanding context, nuance, and evolving platform policies. The right pattern: start with a prompted base model. Add RAG if the model needs to reference detailed policy guidelines at inference time. Consider fine-tuning if the base model is not reliable enough at distinguishing edge cases, especially if you have a large labeled dataset of moderation decisions. An agent is not needed; this is a single-shot classification task.

The pattern is clear: most tasks do not need the full complexity of agents. RAG handles knowledge grounding. Fine-tuning handles behavior and style. Agents are for the minority of tasks that require adaptive, multi-step reasoning with tool use.

## The Cost of Choosing Wrong

Choosing the wrong pattern has concrete costs.

**If you use agents when RAG suffices:** You build a system that is 10x more expensive, 5x slower, and 3x harder to evaluate. Users complain about latency. The business model does not support the cost per task. The agent loops or hallucinates completion, and you spend months debugging state management issues. Meanwhile, a competitor ships a fast, reliable RAG system and wins the market.

**If you use fine-tuning when RAG works:** You spend weeks or months collecting training data, fine-tuning models, and evaluating results. By the time you ship, the knowledge you fine-tuned on is out of date, and you have to do it all over again. A RAG system would have taken a week to build and would automatically stay up-to-date as the knowledge base changes.

**If you use RAG when fine-tuning is needed:** Your system works inconsistently. Sometimes it matches the desired style, sometimes it does not. You tweak prompts endlessly. You add more examples to the prompt, and now you are using 3,000 tokens per call just to set context, which makes the system slow and expensive. Fine-tuning would have solved the problem cleanly, but you avoided it because it seemed complicated.

**If you use traditional code when an agent would help:** You hard-code logic for every possible scenario. The code becomes a tangled mess of if-else branches. Every edge case requires a new branch. Maintaining it is a nightmare. An agent would have handled variability and ambiguity gracefully, but you dismissed agents as "too experimental."

The right pattern depends on the task, the constraints, and the trade-offs you are willing to make. There is no universal answer, but there are clear heuristics. Use the simplest pattern that solves the problem. Only add complexity—fine-tuning, RAG, agents—when simpler approaches fail.

## The Evolution of Patterns Over Time

One final consideration: the right pattern might change as your product evolves.

You might start with a simple prompted model to validate the use case. If users like it but the model is not reliable enough, you add RAG or fine-tuning. If the task grows in complexity and users start asking for multi-step capabilities, you migrate to an agent architecture.

Conversely, you might start with an agent because the task seems complex, then realize that 80 percent of the value comes from a single retrieval step, and you simplify to RAG. Or you start with RAG and fine-tuning, collect a large dataset of user interactions, and retrain the model to bake in the retrieval logic, reducing latency and cost.

The patterns are not permanent commitments. They are tools. You choose the tool that fits the current problem, and you switch tools when the problem changes. The key is to make deliberate choices based on task requirements, constraints, and trade-offs, not based on what sounds cool or what the latest blog post recommended.

Elite teams master all three patterns and know when to use each. Mediocre teams pick one pattern—usually agents because they are trendy—and try to force every problem into that shape. The result is overengineered systems that cost too much, run too slowly, and fail to deliver value.

The next question is: what does it actually cost to run an agent in production, and how do you budget for it?
