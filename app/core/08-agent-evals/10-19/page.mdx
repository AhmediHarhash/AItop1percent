# 10.19 â€” Acceptance Tests for Agent Goals: Scenario-Based Validation

Your agent passed every unit test, every integration test, and every manual QA scenario. On day three of production, it approved a 48,000-dollar purchase from a vendor whose contract expired six months ago. The gap between technical correctness and business goal achievement is called acceptance testing, and it happens before deployment, not after.

This failure illustrates a fundamental gap in how teams validate agents before deployment. You can test every function, every API call, every error handler, and still ship an agent that fails to accomplish what it was built to do. Acceptance tests for agent goals are not component tests or integration tests. They are scenario-based validations that answer one question: does this agent reliably achieve its intended outcome under the conditions it will face in production? These tests are written from the user's perspective, expressed in business terms, and evaluated against success criteria that stakeholders care about. They are the final gate before deployment, and they are where you discover whether your agent actually works in the way that matters.

## What Acceptance Tests Validate That Other Tests Miss

Unit tests validate individual functions. Integration tests validate how components interact. Acceptance tests validate whether the agent achieves its goal. The distinction matters because an agent can pass every technical test and still fail to deliver business value. Consider a research agent built to summarize medical literature for clinical decision support. Unit tests verify that the retrieval function returns relevant papers, that the summarization function produces coherent text, and that the citation function formats references correctly. Integration tests verify that retrieved papers flow correctly into the summarizer and that summaries include properly formatted citations. But none of these tests validate whether a clinician using the agent in the middle of a patient consultation receives accurate, actionable guidance within the time window when the decision must be made. Acceptance tests validate that complete workflow under realistic conditions.

The gap between technical correctness and goal achievement emerges from three sources. First, business logic is often distributed across multiple components, and testing each piece independently does not guarantee that they combine to produce the right outcome. A fraud detection agent might correctly score individual transactions, correctly retrieve historical patterns, and correctly format alerts, but still fail to escalate high-confidence fraud cases to human review within the required two-minute window because no one tested the end-to-end timing under load. Second, agents operate in environments with hidden constraints that technical tests do not capture. A hiring agent might generate excellent candidate summaries but fail acceptance testing when recruiters discover that the summaries omit the visa status information they need to make decisions, a requirement that never made it into the technical specification. Third, success often depends on human factors that only emerge when real users interact with the agent under real conditions. A customer service agent might produce grammatically perfect responses that score well on every quality metric but fail acceptance testing when support reps report that the agent's tone feels dismissive to upset customers, a quality dimension that was never instrumented in the technical tests.

Acceptance tests are scenario-based rather than component-based. Each test describes a realistic situation the agent will encounter, specifies the goal the agent must achieve in that situation, and defines what success looks like from the user's perspective. For a contract review agent, a unit test might verify that the clause extraction function correctly identifies indemnification language, but an acceptance test presents a complete contract and validates that the agent produces a risk summary that enables a procurement manager to make a sign-or-escalate decision in under five minutes. The test does not care how the agent extracts clauses, what data structures it uses internally, or what API calls it makes. It cares only whether the agent achieves the outcome the user needs. This perspective shift is critical. Acceptance tests are written in the language of the business, not the language of the implementation.

You write acceptance tests by working backward from the agent's stated goal. Start with the one-sentence description of what the agent is supposed to accomplish: "The agent triages customer support tickets and routes urgent issues to human agents within three minutes." Then enumerate the scenarios in which that goal must be achieved: a customer reporting a payment failure during checkout, a customer asking about a refund policy, a customer complaining about a defective product, a customer escalating a previous unresolved issue, a customer using abusive language, a customer submitting a request in a language the system does not support. For each scenario, you define the input the agent receives, the actions the agent is expected to take, the outcome the agent must produce, and the success criteria by which that outcome is judged. The acceptance test then executes the scenario end-to-end and validates that the agent meets the success criteria. If the agent routes the payment failure to a specialist within 90 seconds and includes the transaction ID and error code in the handoff, the test passes. If the agent routes the request but omits the transaction ID, forcing the human agent to ask the customer for it again, the test fails.

The scenarios you test must be representative of production conditions. Acceptance tests fail when teams design scenarios that showcase the agent's strengths while avoiding the edge cases, ambiguities, and adversarial inputs the agent will face in the real world. A summarization agent tested only on well-structured academic papers will fail in production when users feed it poorly scanned PDFs with missing sections, tables without headers, and references formatted inconsistently. A scheduling agent tested only with cooperative users will fail when a user provides contradictory constraints, changes their mind mid-conversation, or asks the agent to violate a policy. Your acceptance test suite must include not just the happy path but the difficult cases: incomplete inputs, conflicting requirements, ambiguous requests, time pressure, high-stakes decisions, and inputs designed to test the boundaries of the agent's capabilities. If your agent will encounter rude users, your acceptance tests must include rude users. If your agent will encounter incomplete data, your acceptance tests must include incomplete data. The test suite is a rehearsal for production, and rehearsals must reflect reality.

## Building the Scenario Library Before Deployment

You build your acceptance test suite by cataloging the scenarios the agent must handle and translating each scenario into a test case. This work begins during problem framing, when you and stakeholders enumerate the situations the agent will face, but it crystallizes during deployment preparation, when you turn those situations into executable tests with defined success criteria. The process is collaborative. Engineers bring technical knowledge of what the agent can measure and validate. Product and domain experts bring knowledge of what outcomes matter and what edge cases occur in practice. Together, you build a scenario library that covers the agent's operational envelope: the full range of conditions under which the agent is expected to achieve its goal.

Start by listing the primary scenarios: the common situations the agent is designed to handle. For a travel booking agent, primary scenarios include booking a one-way domestic flight, booking a round-trip international flight with a layover, booking a hotel for a multi-night stay, booking a rental car, modifying an existing reservation, canceling a reservation, and checking the status of an upcoming trip. Each primary scenario becomes a test case. You define the input the user provides, the goal the user wants to achieve, the actions the agent must take, and the criteria that determine whether the agent succeeded. For the round-trip international flight scenario, success might be defined as: the agent presents options that meet the user's dates and budget, the agent explains baggage policies and layover durations, the agent completes the booking with confirmation numbers, and the agent sends a summary email with itinerary details. The test executes the scenario, captures the agent's behavior, and validates each success criterion.

Next, add the edge case scenarios: the situations that are less common but critical to handle correctly. For the travel agent, edge cases include booking a flight departing in less than 24 hours, booking for an unaccompanied minor, booking with a credit card that declines, booking a flight where no options meet the user's exact dates but nearby dates have availability, handling a user who changes their destination mid-conversation, and handling a user who asks to book a route the airline does not serve. Edge cases are where agents most often fail, and they are where acceptance tests provide the most value. If your agent has not been tested on a scenario, you do not know whether it will succeed in that scenario. If the scenario will occur in production, the test must occur before deployment. The edge case library is built by mining historical data, interviewing users who will interact with the agent, and brainstorming with domain experts who know the full range of situations the domain includes.

Then add the adversarial scenarios: the situations where users intentionally or unintentionally push the agent beyond its capabilities. Adversarial scenarios test robustness. For the travel agent, adversarial scenarios include a user asking the agent to book illegal travel, a user providing fake payment information, a user asking the agent to override pricing rules, a user pasting a 10,000-word travel itinerary and asking for help, a user switching languages mid-conversation, a user using profanity or abusive language, and a user asking the agent to perform a task outside its domain, such as providing medical advice. The goal of adversarial scenarios is not to make the agent handle every possible abuse but to validate that the agent degrades gracefully: it refuses inappropriate requests clearly, it escalates ambiguous situations to human oversight, it does not hallucinate capabilities it lacks, and it does not expose vulnerabilities that could be exploited at scale. An agent that passes adversarial acceptance tests is not invincible, but it is resilient enough to deploy.

For each scenario, you define the success criteria in measurable terms. Success criteria must be objective, verifiable, and aligned with the agent's goal. Subjective criteria like "the agent's response feels helpful" cannot be automated and cannot be evaluated consistently. Objective criteria like "the agent's response includes the confirmation number and total price" can be automated and evaluated consistently. Success criteria come in four categories. Outcome criteria specify what the agent must produce: the booking is completed, the summary includes required fields, the escalation reaches the right human. Timing criteria specify how quickly the agent must act: the response is delivered within five seconds, the escalation occurs within two minutes, the summary is ready before the user's meeting starts. Quality criteria specify attributes the output must have: the summary is factually accurate compared to source documents, the recommendation is consistent with policy, the tone is appropriate for the user's emotional state. Safety criteria specify harms the agent must avoid: the agent does not disclose private information, the agent does not approve out-of-policy requests, the agent does not generate content that violates platform guidelines. Every scenario in your acceptance test suite must have clearly defined success criteria in at least one of these categories, and critical scenarios should have success criteria in multiple categories.

You express scenarios as structured test cases that can be executed programmatically. Each test case includes a scenario description written in natural language, the input provided to the agent, the expected actions the agent should take, the expected output the agent should produce, and the assertions that validate success. The test case for the international flight booking scenario might look like this in prose form: Description: user requests a round-trip flight from New York to London, departing March 15 and returning March 22, budget up to $1,200, prefers non-stop flights. Input: user message stating those requirements. Expected actions: agent queries flight APIs, filters results by budget and non-stop preference, ranks options by price and schedule convenience, presents top three options with baggage and layover details, assists user in selecting an option, processes payment, confirms booking. Expected output: confirmation message including confirmation number, flight numbers, departure and arrival times, total price, baggage allowance, and a note that an itinerary email has been sent. Assertions: confirmation number is present and valid, total price is under $1,200, flights are non-stop, email is sent within 30 seconds, booking appears in the user's trip history. This structure is detailed enough to execute programmatically but written in language that stakeholders can read and verify.

## Executing Acceptance Tests in Pre-Production Environments

You execute acceptance tests in an environment that mirrors production as closely as possible. This is not your development environment, where data is synthetic and integrations are mocked. This is a pre-production staging environment where the agent connects to real APIs, accesses realistic data, and operates under production-like constraints. The purpose of acceptance testing is to validate the agent's behavior under real conditions, and that validation is only meaningful if the conditions are real. If your agent will call a third-party API in production, it must call that API or a realistic sandbox version during acceptance testing. If your agent will query a database with millions of records in production, it must query a similarly sized database during acceptance testing. If your agent will experience network latency and rate limits in production, it must experience those constraints during acceptance testing. The closer your test environment mirrors production, the more confidence your acceptance tests provide.

You run acceptance tests both as automated regression tests and as manual exploratory sessions. Automated tests execute the scenario library programmatically, validating that every defined scenario passes its success criteria. These tests run on every deployment candidate build, and any failure blocks deployment. Automated acceptance tests are your safety net: they catch regressions, they validate that changes to the agent do not break existing functionality, and they provide a pass-fail signal that tells you whether the agent is ready to ship. But automated tests can only validate scenarios you anticipated. Manual exploratory sessions are where you discover scenarios you did not anticipate. You invite stakeholders, domain experts, and representative users to interact with the agent in the staging environment and attempt to break it. They try edge cases, they ask ambiguous questions, they provide contradictory inputs, they simulate time pressure, they mimic real-world chaos. When they discover a failure mode, you add it to the scenario library as a new test case, fix the agent, and re-run the full suite. The combination of automated coverage and manual exploration gives you confidence that the agent will perform in production.

When an acceptance test fails, the failure tells you something different than a unit test failure. A unit test failure indicates a bug in a specific function. An acceptance test failure indicates that the agent does not achieve its goal in a specific scenario. The fix may involve changing code, but it may also involve changing the prompt, retraining a model, adjusting a policy, or clarifying a requirement. In the logistics company example from the opening story, the acceptance test failure revealed that the procurement agent's goal definition was incomplete: the agent was built to approve financially sound purchases, but the actual goal included contractual compliance. The fix required adding a new validation step, updating the agent's decision logic, and adding a new check to the acceptance test. Acceptance test failures often reveal gaps between what the agent was built to do and what the agent needs to do, and fixing those gaps may require revisiting problem framing, not just patching code.

You track acceptance test pass rates over time as a deployment readiness metric. At the beginning of the project, pass rates are low because the agent is immature and the scenario library is incomplete. As the agent improves and the scenarios stabilize, pass rates increase. You define a deployment threshold: the agent must pass 95% of primary scenarios, 85% of edge case scenarios, and 90% of adversarial scenarios before it is approved for production. These thresholds are not arbitrary. They are negotiated with stakeholders based on the agent's risk profile, the cost of failure, and the availability of fallback mechanisms. A low-stakes agent with strong human oversight might deploy at 80% pass rate. A high-stakes agent handling sensitive data or high-value transactions does not deploy until pass rates exceed 95%. The thresholds are documented, enforced by CI/CD gates, and reviewed quarterly as the agent's scope and risk profile evolve.

## Aligning Acceptance Criteria with Stakeholder Sign-Off

Acceptance tests are only valuable if stakeholders agree that passing the tests means the agent is ready to deploy. This alignment happens through a formal acceptance criteria review before testing begins. You convene the product owner, domain experts, engineering leads, and any other decision-makers who have authority over deployment. You present the scenario library, walk through the success criteria for each scenario, and ask for explicit sign-off. The conversation surfaces misalignments early. Product might say, "We also need the agent to handle same-day flight changes, that is not in the scenario library." Legal might say, "The adversarial test for policy override is not stringent enough, the agent must refuse and log the attempt, not just warn the user." Domain experts might say, "The edge case for unaccompanied minors is missing a check for parental consent documentation." You incorporate the feedback, update the scenario library, and return for a second review. The process continues until all stakeholders agree: if the agent passes these tests, it is ready to deploy.

This sign-off is contractual. It establishes shared accountability. Engineering commits to building an agent that passes the tests. Stakeholders commit to approving deployment if the agent passes. When acceptance tests pass and stakeholders later raise new concerns, you have a decision point: either the new concern represents a scenario that should have been in the library and you add it now, or the concern is out of scope for the current release and you defer it to the next iteration. The acceptance criteria review prevents scope creep during deployment. It also prevents the opposite failure mode: engineering declaring the agent ready without stakeholder validation. In organizations where acceptance criteria are not formalized, agents often get stuck in deployment limbo, with engineering insisting the agent works and stakeholders insisting it does not. The scenario library and sign-off process eliminate ambiguity. The agent either passes the agreed-upon tests or it does not. If it passes, it deploys. If it does not, it gets fixed.

Stakeholder involvement extends beyond sign-off. Representatives from each stakeholder group participate in manual exploratory testing. They are the ones who know what "success" looks like in practice, and they are the ones who will notice when the agent's behavior is technically correct but practically wrong. In the logistics company case, procurement managers participated in acceptance testing and immediately flagged that the agent was approving orders without checking contract status. They could identify the issue because they understood the policy and its rationale. Engineering could not identify the issue from logs or metrics because the agent's behavior matched the technical specification. Stakeholder participation in acceptance testing is not optional. It is how you bridge the gap between technical correctness and business value.

## Scenario-Based Testing for Multi-Agent Systems

When your deployment includes multiple agents that interact or hand off tasks, acceptance tests must validate not just individual agent goals but the system-level goal the agents collectively achieve. A multi-agent customer service system might include a triage agent that routes tickets, a resolution agent that handles inquiries, and an escalation agent that coordinates human handoffs. Each agent can pass its own acceptance tests and the system can still fail if the handoffs between agents lose context, introduce delays, or create inconsistent customer experiences. System-level acceptance tests validate the complete workflow from the customer's initial request to the final resolution, measuring success from the customer's perspective, not from the perspective of any individual agent.

System-level scenarios describe end-to-end workflows that span multiple agents. For the customer service system, a system-level scenario might be: customer submits a ticket reporting a billing error, triage agent categorizes it as billing and routes it to the resolution agent, resolution agent retrieves the customer's transaction history and identifies the error, resolution agent processes a refund, escalation agent sends a confirmation email to the customer and logs the incident for quality review. The acceptance test executes this workflow end-to-end and validates that the customer receives an accurate resolution within the promised timeframe, that no information is lost during handoffs, that the incident is properly logged, and that the customer experience is seamless. If the triage agent routes correctly but the resolution agent requests information the triage agent already collected, the test fails because the workflow wastes the customer's time.

Multi-agent acceptance tests also validate failure propagation. When one agent in the system fails, how do the other agents respond? If the resolution agent cannot connect to the billing API, does the escalation agent notify the customer of the delay, or does the customer's request disappear into a black hole? If the triage agent misclassifies a ticket, does the resolution agent detect the misclassification and re-route, or does it attempt to resolve the ticket using the wrong workflow? System-level acceptance tests include scenarios where individual agents fail, time out, or produce ambiguous outputs, and they validate that the system degrades gracefully, escalates appropriately, and keeps the customer informed. Multi-agent systems are only as reliable as their weakest handoff, and acceptance tests expose those handoffs before deployment.

## Using Production Shadow Mode for Final Validation

The final stage of acceptance testing is shadow mode: running the agent in production alongside the existing system but without affecting user-facing outcomes. In shadow mode, the agent receives real production inputs, processes them as it would in full deployment, and produces outputs that are logged and evaluated but not acted upon. Users do not see the agent's outputs, and the agent does not trigger any side effects. Shadow mode lets you validate the agent's behavior under real conditions with real data at real scale without risking production failures. It is the dress rehearsal before opening night.

You run shadow mode for a defined period, typically one to four weeks, depending on the agent's complexity and the volume of traffic it will handle. During shadow mode, you collect the agent's outputs, compare them to the outcomes produced by the current system or by humans performing the same task, and measure agreement rates, error rates, and quality differences. For a fraud detection agent, shadow mode means the agent scores every transaction but does not block any transactions. You compare the agent's fraud predictions to the fraud labels eventually determined by human investigators, measure precision and recall, and identify patterns where the agent and humans disagree. If the agent flags transactions the humans approve, you investigate whether the agent is over-sensitive or whether the humans missed fraud. If the agent approves transactions the humans flag, you investigate whether the agent is under-sensitive or whether the humans are over-cautious. Shadow mode data reveals gaps that no pre-production test can capture.

Shadow mode also validates the agent's behavior under production load, production data distributions, and production edge cases. Pre-production acceptance tests run on curated scenarios. Shadow mode runs on the full chaos of production. You discover inputs you did not anticipate, edge cases you did not test, and failure modes you did not imagine. A scheduling agent might pass every pre-production acceptance test and then encounter a user in shadow mode who submits a meeting request in a language the agent was not trained on, or a user whose calendar contains overlapping events due to a sync issue, or a user who asks the agent to schedule a meeting at a time that violates the company's core hours policy. Each of these cases becomes a new scenario in your acceptance test library, and the agent does not exit shadow mode until it handles the new scenarios correctly.

You establish exit criteria for shadow mode before it begins. The agent must achieve a defined level of agreement with human decisions, typically 90% or higher for high-confidence cases. The agent must handle 95% of production inputs without errors or escalations. The agent must meet latency targets under peak load. The agent must not produce any outputs that violate safety or policy guardrails. When the agent meets all exit criteria for the defined duration, it exits shadow mode and moves to a phased rollout. If the agent fails to meet exit criteria, it returns to development, the failure modes are added to the acceptance test suite, and shadow mode restarts after fixes are deployed. Shadow mode is not a checkbox. It is a rigorous validation phase that provides the final signal of deployment readiness.

## Maintaining Acceptance Tests as the Agent Evolves

Acceptance tests are not static. As the agent evolves, the scenario library evolves with it. When you add new capabilities to the agent, you add new scenarios to the test suite. When you discover failure modes in production, you add regression tests to ensure those failures do not recur. When stakeholder requirements change, you update the success criteria. The acceptance test suite is a living document that reflects the current definition of what the agent must accomplish. Teams that treat acceptance tests as a one-time deployment gate discover months later that the agent has drifted from its original goals because no one was validating those goals in production.

You version the acceptance test suite alongside the agent. Each release of the agent has a corresponding version of the test suite, and the release notes document what scenarios were added, what success criteria were updated, and what tests were removed because they no longer apply. Versioning ensures that you can reproduce test results from previous releases and that you can trace changes in agent behavior to changes in requirements. When a stakeholder reports that the agent no longer handles a scenario correctly, you check whether the scenario is still in the test suite, whether the success criteria changed, and whether the agent's behavior diverged from the specification. Versioned acceptance tests provide an audit trail that supports debugging, compliance, and governance.

You also run acceptance tests in production as synthetic monitoring. Synthetic tests periodically execute a subset of your acceptance test scenarios against the live production agent, validating that the agent still achieves its goals under current conditions. Synthetic tests catch regressions that other monitoring might miss. If the agent's latency increases due to a dependency change, synthetic tests detect it. If the agent's output quality degrades due to a model update, synthetic tests detect it. If the agent starts failing a scenario that used to pass, synthetic tests detect it. Synthetic monitoring is not a replacement for real-user monitoring, but it is a complement. Real-user monitoring tells you what is happening. Synthetic monitoring tells you whether the agent still meets the acceptance criteria you defined before deployment.

Acceptance tests for agent goals are the bridge between engineering's definition of done and stakeholder's definition of ready. They validate that the agent reliably achieves its intended outcome under the conditions it will face in production, expressed in business terms, measured by criteria that stakeholders care about. Without acceptance tests, you deploy agents based on faith, hoping that technical correctness translates to business value. With acceptance tests, you deploy based on evidence, knowing that the agent has been validated against the scenarios that matter. The logistics company that deployed the procurement agent without goal-level validation learned this lesson the expensive way. You validate before deployment, not after.

The next subchapter examines how to validate agent behavior across versions and model updates through regression testing and version control strategies.