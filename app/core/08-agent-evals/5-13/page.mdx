# 5.13 — When to Use Multi-Agent vs Single-Agent With Many Tools

In March 2025, a legal technology company spent four months building a multi-agent contract review system. They had five specialized agents: one for document parsing, one for clause extraction, one for risk identification, one for compliance checking, and one for report generation. Each agent was carefully designed with its own prompt, its own tools, and its own evaluation suite. The architecture diagram was beautiful. The separation of concerns was textbook. The system worked. But it was slow. Processing a 50-page contract took 37 seconds because of coordination overhead between agents. Users complained. The team spent another month optimizing inter-agent communication, message serialization, and context passing. They got it down to 29 seconds. Still too slow. Then an engineer asked: what if we use one agent with all the tools? They built a prototype over a weekend. One agent, one prompt, twenty tools. Same functionality, 11-second processing time. The multi-agent system they had spent five months building was solving a coordination problem they had created by choosing a multi-agent architecture in the first place. They shipped the single-agent version and deleted the multi-agent code. The architectural elegance had been a distraction from the actual goal: review contracts quickly and accurately.

This is the most important architectural decision you will make when building an agent system: single-agent with many tools, or multiple specialized agents? The default assumption in 2026 is that more sophisticated problems require multi-agent systems. This assumption is often wrong. Single-agent systems with well-designed tool suites can handle remarkably complex tasks with lower latency, simpler debugging, and easier deployment than multi-agent alternatives. Multi-agent systems provide real benefits in specific scenarios: when you need true parallel execution, when different subtasks require different models or contexts, when security isolation demands physical separation. But these scenarios are less common than the current hype suggests. Most agent applications are better served by starting with a single agent and only adding additional agents when you hit concrete limitations that cannot be solved any other way.

## The Case for Single-Agent Simplicity

The case for single-agent starts with simplicity. One agent means one prompt to write, one prompt to maintain, one context to manage, one execution trace to debug. When a user reports a bug, you examine one agent's behavior. When you want to improve performance, you optimize one component. When you need to add a new capability, you add a tool to one agent and update one prompt. There is no coordination protocol to design, no message format to specify, no routing logic to implement, no inter-agent state management to debug. The system is a straight line: input goes to agent, agent calls tools, agent produces output. Every tool call is visible in one trace. Every decision is visible in one reasoning chain. The cognitive load of understanding and maintaining the system is minimal.

Compare this to a multi-agent system. You have multiple prompts, each describing part of the overall task. You have coordination logic that decides which agent handles which part of the work. You have message passing that transfers data between agents. You have state management that tracks what each agent has done and what still needs doing. When a user reports a bug, you need to figure out which agent caused it, which might require examining interaction logs across multiple agents. When you want to improve performance, you need to optimize not just individual agents but also the coordination overhead between them. When you add a new capability, you need to decide which agent should have it, or whether you need a new agent, and then update routing logic accordingly. The cognitive load is an order of magnitude higher.

In April 2025, a customer support company compared development velocity between their single-agent and multi-agent teams. The single-agent team was iterating on a chatbot that handled common support queries using one agent with 15 tools for knowledge base search, order lookup, account management, refund processing, and ticket creation. The multi-agent team was building a more complex system with separate agents for intent classification, knowledge retrieval, response generation, and escalation handling. Both teams had similar goals and similar team sizes. After three months, the single-agent team had shipped four major feature updates—adding new product categories to the knowledge base tools, implementing multi-turn conversation memory, adding sentiment analysis, and integrating with a new CRM system—and was iterating on improvements based on user feedback. The multi-agent team was still working on their second major feature because each change required coordinating updates across multiple agents and testing interaction paths. The coordination tax was real and substantial. The multi-agent team was not less capable. They were solving harder problems created by the architecture itself.

Development velocity matters because agent systems are never finished. You build an initial version, deploy it, get user feedback, and iterate. The faster you can iterate, the better your final system becomes. Every hour spent debugging inter-agent coordination is an hour not spent improving accuracy or adding user-requested features. Single-agent architectures remove entire categories of complexity, freeing your team to focus on the actual problem you are solving rather than the infrastructure for solving it.

## Latency and Performance

Latency is another critical advantage of single-agent systems. Every agent transition adds latency. When Agent A finishes and passes results to Agent B, you pay the cost of serializing the message, transmitting it through your message queue or orchestration layer, deserializing it, loading Agent B's context and prompt, and invoking the language model for Agent B. These overheads compound across multiple agents. In a three-agent sequential pipeline, you pay the transition cost twice. In a five-agent pipeline, four times. For user-facing applications where response time matters, this overhead can be the difference between acceptable and unacceptable user experience.

In May 2025, a voice assistant company needed to respond to user queries in under two seconds to feel responsive in conversational interactions. They initially designed a three-agent system: one agent for understanding the query and extracting intent, one for retrieving relevant information from knowledge bases and APIs, one for generating the response in natural language. Sequential execution meant three separate LLM calls with two handoffs involving message serialization and context loading. Even with aggressive optimization—caching prompts, using faster models, minimizing message size—they could not consistently hit the two-second target. Median latency was 2.8 seconds. Tail latency at the 95th percentile was 4.1 seconds. They redesigned as a single agent with tools for information retrieval, knowledge base search, and API calls. One LLM call, zero handoffs, consistent 1.4-second median response times and 2.1-second tail latency. The multi-agent design made their latency target impossible. The single-agent design made it trivial. They shipped the single-agent version and users immediately noticed the improved responsiveness.

Latency is not just about user experience. It is also about cost. Language model API pricing typically includes both per-token costs and per-request costs. More agents mean more requests, which means higher costs even if total token usage is similar. In June 2025, a content generation platform analyzed costs for their multi-agent blog post generator. They had four agents: topic research, outline creation, section writing, and final editing. Each blog post required four LLM API calls. At 100,000 blog posts per month, they were making 400,000 API calls. Their provider charged a per-request fee of 0.002 dollars in addition to token costs. The per-request fees alone totaled 800 dollars per month, before any token costs. A single-agent architecture would reduce this to 100,000 requests and 200 dollars per month in per-request fees, saving 600 dollars monthly on a cost component they had not even considered when designing the architecture.

## Debugging and Observability

Debugging is dramatically simpler with single agents. When something goes wrong in a single-agent system, you load the trace for that request and see exactly what happened: what tools the agent called, what results it got, what reasoning it used, what output it produced. The entire causal chain is in one place. You can follow the agent's thinking from input to output in one linear sequence. When something goes wrong in a multi-agent system, you need to reconstruct what happened across multiple agents. Agent A did something, which influenced what Agent B did, which affected what Agent C did. The bug might be in Agent C's logic, or in the data Agent B sent to Agent C, or in the data Agent A sent to Agent B that Agent B misinterpreted. You need distributed tracing, interaction logs, correlation IDs across agents, and cross-agent analysis to even understand what happened, let alone fix it.

In July 2025, a document processing company tracked mean time to resolution for bugs in their single-agent and multi-agent systems. Both systems had similar complexity in terms of what they did for users: extract data from invoices, validate the extracted data against business rules, and update accounting systems. The single-agent system had 45-minute average resolution time: reproduce the issue by rerunning the trace, examine the trace to see which tool call produced incorrect results, identify whether the problem was in the tool implementation or the agent's use of the tool, fix the prompt or tool code, deploy, verify with the same input that originally caused the bug. The multi-agent system had 3.5-hour average resolution time: reproduce the issue, identify which agent was involved by searching logs across all agents for the correlation ID, trace backward through interaction logs to find which upstream agent provided problematic input, determine whether the fix should be in the downstream agent's logic or in the upstream agent's output format, fix, test inter-agent interactions with synthetic data to verify the fix does not break other flows, deploy often requiring coordinated deployment of multiple agents to maintain message format compatibility, verify with the original input. The debugging experience was night and day.

Observability tools help but do not eliminate the fundamental complexity. Distributed tracing systems like Jaeger or OpenTelemetry can stitch together traces across multiple agents, showing you the full request path. But you still need to understand multiple agents' logic, multiple prompts, multiple message formats. The cognitive overhead is unavoidable. In August 2025, a fraud detection company implemented comprehensive distributed tracing for their five-agent fraud analysis system. Traces showed the full path through all agents. But when a false negative occurred—a fraudulent transaction that the system missed—debugging still required understanding why the transaction feature agent extracted certain features, why the pattern matching agent did not flag those features, why the risk scoring agent gave a low score, why the decision agent approved the transaction. Each step involved examining a different agent's prompt and reasoning. The observability infrastructure made the data visible, but the engineer still needed to hold five different contexts in their head simultaneously.

Single-agent debugging is cognitively simpler because you only need to understand one agent. You trace through its tool calls, examine its reasoning, understand its prompt. If it made a mistake, you can usually identify why by reading the trace. The agent called the wrong tool, or called the right tool with wrong parameters, or interpreted the tool results incorrectly, or applied incorrect logic to the results. Each of these is visible in the trace and mappable to specific parts of the prompt or tool definitions. Multi-agent debugging requires holding a mental model of the entire system's state across multiple agents, which is harder than it sounds when you are trying to understand why a specific edge case failed.

## Cost Analysis

Cost is another hidden advantage of single-agent systems. Each agent invocation costs tokens: the system prompt, the task context, the tool descriptions, the conversation history if multi-turn, the generated output. In a multi-agent system, you pay these costs multiple times. Agent A has its prompt and context. Agent B has its prompt and context. Agent C has its prompt and context. You are paying for three separate LLM invocations with three separate context loads. In many cases, the total token usage of a multi-agent system is significantly higher than the equivalent single-agent system for the same task because context is not perfectly shared. Agent B might need some of the same context Agent A already had, requiring you to pass it through messages, which adds tokens.

In September 2025, a content moderation platform analyzed the cost difference between their single-agent and multi-agent implementations. Their multi-agent version used three agents: content extraction that parsed posts and extracted text, images, and metadata, analysis that classified content for various policy violations, and decision that combined classifications into a final moderation decision. Average total tokens per item: 2800 input tokens spread across three prompts plus shared context passed through messages, and 400 output tokens across three LLM calls. Their single-agent version with tool calling for the same functionality: one prompt with all logic, tools for content parsing and policy classification, 1900 input tokens and 280 output tokens in one LLM call. At their scale of 15 million items per month and their provider's pricing of 3 dollars per million input tokens and 15 dollars per million output tokens, the multi-agent version cost 132,000 dollars per month in LLM API costs. The single-agent version cost 54,000 dollars per month. The cost difference was 78,000 dollars monthly, which was not marginal. It was a different budget category entirely. They migrated to single-agent and used the cost savings to improve their evaluation infrastructure.

The cost difference comes from both token usage and request overhead. Multi-agent systems use more tokens because context is duplicated across agents. They also make more requests, and many providers charge per-request fees. The combination can double or triple your costs for the same functionality. This matters less if you are processing 100 requests per day. It matters enormously if you are processing millions.

## When Multi-Agent Is Actually Needed

So when should you actually use multi-agent instead of single-agent? The first legitimate reason is model specialization. When different parts of your task require fundamentally different models—vision models for image analysis, speech models for audio transcription, specialized code models for programming tasks—you need separate agents to wrap each model. A single agent cannot invoke multiple different foundation models in one execution because each agent is bound to one model. In October 2025, a video analysis company needed to process videos for content moderation. They needed visual analysis of frames to detect violence or nudity, audio transcription and analysis to detect hate speech or threats, and text generation for moderation reports. These required Claude 4 with vision capabilities for visual analysis, Whisper for speech transcription followed by GPT-4o for text analysis, and Claude 3.5 Sonnet for report generation. They built three agents, each wrapping a different model. This was not architectural preference. It was technical necessity. Different models meant different agents. There was no way to accomplish the task with one agent because no single model provided all three capabilities at the required quality levels.

The second legitimate reason is context isolation when you genuinely exceed context windows. If your task requires processing 100 documents totaling 5 million tokens and your model's context window is 200k tokens, you cannot fit everything in one agent's context. You need multiple agents: perhaps 25 agents that each process 4 documents and produce summaries, plus one synthesis agent that receives the 25 summaries and produces the final output. In November 2025, a legal discovery platform processed lawsuits involving millions of pages of documents. A single lawsuit might have 10,000 documents totaling 50 million tokens. No context window could hold the full document set, even with models like Claude Opus 4.5 with 200k windows. They used a hierarchical multi-agent architecture: document processing agents at the bottom that each processed 20 to 30 documents and produced summaries, synthesis agents in the middle that each processed 10 summaries and produced higher-level summaries, and a final summarization agent at the top that processed the highest-level summaries and produced the discovery brief. This was dictated by physics, not preference. The context simply did not fit in one agent.

The third legitimate reason is parallelism for performance. If you have independent subtasks that can execute concurrently and your latency requirements demand it, multi-agent with parallel execution can be faster than single-agent sequential execution. In December 2025, a competitive intelligence platform needed to analyze competitor activity across news articles, social media posts, and financial filings. Doing this sequentially with one agent took 45 seconds: 15 seconds to search and analyze news, 18 seconds to search and analyze social media, 12 seconds to search and analyze filings. Running three specialized agents in parallel—news agent, social agent, filings agent—and then synthesizing results with a fourth agent took 16 seconds: 18 seconds for the slowest agent plus 2 seconds for synthesis, minus overlap because all three ran simultaneously. The parallelism was the point. A single agent could not achieve this speedup because it would still execute tool calls sequentially, even if you gave it tools for all three data sources.

The fourth legitimate reason is security isolation. When different parts of your system handle different trust levels of data and compliance requires physical isolation, multi-agent with proper containerization and network policies provides security boundaries that a single agent cannot. In January 2026, a healthcare analytics platform processed both de-identified patient data for research and identified data for billing. HIPAA compliance required that the research system could never access identified data, and they needed to prove this to auditors. They used separate agents running in separate Kubernetes namespaces with separate service accounts and separate credentials. The research agent physically could not access the billing database because its container's network policies blocked that connection and its service account had no IAM permissions for the billing database. A single agent with tools for both research and billing would have required giving one agent access to both databases, creating compliance risk because you would need to rely on the agent's prompt to enforce separation, which auditors would not accept. The physical isolation of multi-agent was the compliance requirement.

The fifth legitimate reason is when different subtasks have wildly different operational characteristics. If one subtask runs in milliseconds and another runs in minutes, and users need the fast subtask to complete quickly without waiting for the slow one, you might separate them into different agents for responsiveness. In February 2026, a data pipeline platform had some tasks that were quick database queries taking 100 to 300 milliseconds and some that were long-running batch jobs taking 5 to 20 minutes. Running them in one agent meant quick queries had to wait in queue behind batch jobs, causing unacceptable latency. Separating them into a fast-response agent for queries and a batch-processing agent for jobs allowed the system to respond to queries in under one second while batch jobs ran asynchronously in the background. The operational characteristics were sufficiently different that separation provided real user value.

These five reasons—model specialization, context limits, parallelism, security isolation, and operational heterogeneity—are legitimate technical drivers for multi-agent architecture. Notice what is not on this list: "separation of concerns," "architectural elegance," "making the system more scalable," "following microservices patterns," or "because it feels more AI-native." These are not technical reasons. They are aesthetic preferences. Aesthetic preferences are valid in architecture, but they should not override practical considerations like latency, cost, debugging complexity, and development velocity unless you have unlimited time and budget, which you do not.

## The Decision Process

How do you decide? Start by trying to design a single-agent system. Write the prompt. List the tools you need. Estimate the context size by adding up your prompt length, tool descriptions, typical input size, and expected reasoning length. Estimate the latency by considering model inference time plus tool call overhead. If the prompt is too complex to maintain—say, over 5000 tokens with convoluted conditional logic that you cannot simplify—consider splitting into multiple agents with simpler prompts. If the context exceeds available windows even with the largest models like Claude Opus 4.5, you must split. If the latency is unacceptable and parallelism would fix it by running independent subtasks concurrently, consider splitting. If you need different foundation models for different capabilities, you must split. If you have security or compliance requirements for physical isolation, you must split. If you have no clear reason from this list to split, do not split. Ship the single-agent version. You can always add agents later if you discover concrete limitations.

In March 2026, a financial analysis company followed this process. They were building a system to analyze earnings calls for investment research. Their initial single-agent design had 18 tools: transcript retrieval from their database, financial statement queries for the company and its competitors, industry data lookup, sentiment analysis of specific quotes, metric calculation for various financial ratios, historical comparison against previous quarters, peer comparison against competitors, and various output formatting tools for charts and tables. Prompt length: 3200 tokens including all tool descriptions and analysis instructions. Context size for a typical earnings call: 45k tokens for the transcript plus 8k for retrieved financial data plus 3k for the prompt plus 6k estimated for reasoning and tool calls, totaling 62k tokens well within Claude Opus 4.5's 200k window. Estimated latency: 8 seconds for model inference plus tool calls. All of these were acceptable for their use case where analysts ran queries interactively and were willing to wait 10 seconds for comprehensive analysis. They shipped the single-agent version. It worked well. Users were happy. The system was maintainable.

Six months later, they added a feature to analyze historical trends across 40 earnings calls spanning 10 years for deep longitudinal analysis. This required processing 40 transcripts totaling 1.8 million tokens, which exceeded any available context window. At that point, they added a second agent specifically for multi-call synthesis. The workflow became: a document processing agent analyzed each earnings call individually and produced structured summaries, then a synthesis agent processed the 40 summaries and identified trends. They evolved from single-agent to multi-agent when they hit a concrete limitation, not preemptively. This is the right approach. Start simple. Add complexity when forced to by real constraints.

The opposite approach—designing multi-agent first and consolidating later—is much harder. Once you have built coordination logic, message schemas, multiple prompts, and inter-agent workflows, consolidating back to single-agent requires rewriting substantial portions of the system. In April 2026, a document automation company built a five-agent system for contract generation: template selection agent, clause population agent, legal review agent, formatting agent, and finalization agent. After six months in production, they realized they did not need the complexity. Most contracts were simple and did not benefit from the specialized agents. The coordination overhead made the system slow and hard to maintain. They tried to consolidate to single-agent and found that they had to redesign almost everything. The coordination logic had shaped their entire data model. Message schemas were baked into their storage layer. The five prompts had different styles and assumptions that were hard to merge. They ended up maintaining the multi-agent architecture because the consolidation cost was estimated at three engineer-months, which they could not justify. They were stuck with unnecessary complexity because of an early architectural decision.

## The Asymmetry of Complexity

This is the key asymmetry: going from single-agent to multi-agent is a bounded addition of complexity. You extract one capability into a new agent, add coordination logic for that one interaction, and the rest of the system continues working as before. You add an orchestration layer, define a message format for the new agent, update your deployment to include the new agent, and you are done. Going from multi-agent to single-agent is a complete redesign because the coordination is woven throughout the system. You need to merge prompts, eliminate message passing, flatten the orchestration into sequential tool calls, redesign error handling, and often rethink your data model. The reversibility is not symmetric. Therefore, the safe default is single-agent. Add agents only when justified by concrete technical requirements.

Hybrid architectures are common in practice and often represent the best of both approaches. You have one primary agent that handles the main workflow and 90 percent of requests, and it can delegate to specialized agents for specific capabilities that genuinely require separate models or security isolation. The user interacts with one agent most of the time. That agent has tools for common tasks. For uncommon or specialized tasks, it delegates to specialist agents. In May 2026, a customer service platform used this pattern. Their primary agent handled account questions, order status checks, basic troubleshooting, returns processing, and general inquiries using 24 tools. For complex technical diagnostics that required reading system logs and running diagnostic scripts, it delegated to a specialized technical support agent that had elevated permissions and specialized tools. For billing disputes that involved payment system access and required PCI compliance isolation, it delegated to a specialized billing agent running in a separate security zone. Most users never triggered the specialist agents. They got fast responses from the primary agent. Power users with complex needs got specialized attention when needed. This captured the simplicity of single-agent for common cases while allowing specialization for edge cases.

## Operational and Testing Overhead

Operational complexity is a massive hidden cost of multi-agent systems that is often underestimated during design. Deploying a single agent means one deployment artifact, one scaling policy, one health check, one service to monitor. Deploying five agents means five deployment artifacts, five scaling policies, five health checks, five services to monitor, plus orchestration infrastructure like Temporal or Step Functions, message queue infrastructure like RabbitMQ or Amazon SQS, service mesh configuration if you are using something like Istio, and inter-agent networking rules in your Kubernetes cluster or cloud VPC. In June 2026, a startup compared the operational burden of their single-agent prototype and multi-agent prototype for the same application. Single-agent: 150 lines of Kubernetes YAML for deployment, monitored with basic service metrics like request rate and error rate and latency, maintained by half of one engineer's time for deployments and incident response. Multi-agent: 1100 lines of Kubernetes YAML plus 300 lines of Temporal workflow definitions, required setting up a RabbitMQ cluster for message passing and a Temporal cluster for orchestration, monitored with distributed tracing, service mesh metrics, queue depth metrics, and inter-agent interaction success rates, required one full-time engineer plus 30 percent of another engineer's time for operational support. The operational tax was 3X in engineering time, not counting the infrastructure costs for the message queue and orchestrator.

Testing is also simpler with single agents. Testing a single-agent system means providing inputs and asserting on outputs. You test each tool individually with unit tests. You test the agent's ability to use tools correctly with integration tests that provide various inputs and verify the agent calls the right tools in the right order. You test end-to-end flows with system tests. Testing a multi-agent system means testing each agent individually, testing each inter-agent interaction path to verify messages are formatted correctly and handled correctly, and testing end-to-end flows across all agents to verify the full system behavior. In July 2026, a QA team measured test suite size and execution time for single-agent and multi-agent versions of the same application. Single-agent: 120 test cases covering tool logic and agent behavior, 4 minutes to run the full suite. Multi-agent: 120 agent-level test cases plus 67 interaction test cases that verified message passing between specific agent pairs plus 15 end-to-end test cases that ran full workflows across all agents, totaling 202 tests, 14 minutes to run. The test complexity and maintenance burden were significantly higher for multi-agent. Every time they changed a message format, they needed to update interaction tests. Every time they added an agent, they needed to add interaction tests for all its connection points.

Performance optimization is more tractable with single agents. If your single-agent system is slow, you profile it: which tool calls take longest? Which parts of the prompt cause unnecessary reasoning? Which context elements are unused and could be removed? You optimize the slow parts. If your multi-agent system is slow, the bottleneck might be in agent execution, or in coordination overhead, or in message serialization and deserialization, or in queue latency, or in sequential dependencies that could be parallelized if you restructured the agent graph. In August 2026, a performance engineering team spent two weeks profiling a slow multi-agent system. They found the bottleneck was not agent execution, which was fast, but orchestrator scheduling delays caused by resource contention in their Kubernetes cluster. The orchestrator was queuing agent invocations because too many agents were trying to run simultaneously and hitting pod limits. This type of issue does not exist in single-agent systems. You cannot have orchestrator scheduling delays when there is no orchestrator.

Version management is simpler with single agents. Updating a single-agent system means updating the prompt or tools and deploying the new version. You can use blue-green deployment or canary deployment to minimize risk. Updating a multi-agent system requires considering version compatibility: if you change Agent A's output format, will Agent B still understand it? Do you need to update both simultaneously? Can you deploy them independently without breaking in-flight requests that span both agents? In September 2026, a SaaS platform tried to update one agent in their three-agent system. They changed the message schema that the agent produced to include additional fields for a new feature. They deployed the updated agent. Downstream agents that consumed those messages immediately started failing because they expected the old schema and could not parse the new fields. They had to coordinate a synchronized deployment of all three agents, which required a maintenance window because they could not safely update them one at a time. Single-agent systems do not have this problem because there are no inter-agent message schemas to maintain compatibility for. You just deploy the new version.

## Error Handling and Recovery

Error handling is dramatically simpler with single agents. When a tool call fails in a single-agent system, the agent can retry with different parameters, try a different tool, or fail gracefully with a clear error message to the user. The agent sees the error, reasons about it, and decides how to respond. When Agent A encounters an error after it has already sent messages to Agent B and Agent C, what happens? Does Agent A send cancellation messages to B and C? Do Agents B and C detect the failure and roll back any partial work they did? Does the orchestrator handle cleanup by terminating B and C? All of these approaches have been tried, and all add significant complexity. In October 2026, an engineer at a payment processing company estimated that 40 percent of their multi-agent orchestration code was error handling and compensation logic for failures during coordination. They had to handle cases like: Agent A fails before sending to Agent B, Agent A sends to Agent B but Agent B fails, Agent A and Agent B succeed but Agent C fails and now you need to compensate the work of A and B, Agent A times out and you do not know if it completed or not. Error handling for the equivalent single-agent version was less than 10 percent of the code because errors were localized to one agent's execution.

The compensation problem is particularly thorny in multi-agent systems. If Agent A debited an account and Agent B was supposed to credit a different account but failed, you need to credit the original account to roll back the debit. This is the distributed transaction problem, which is notoriously hard. Single-agent systems executing tool calls sequentially do not have this problem. The agent calls the debit tool, sees that it succeeded, calls the credit tool, sees that it failed, and calls a rollback tool to reverse the debit. All of this is visible in one trace and controlled by one agent's reasoning.

## The 40-Tool Heuristic

The strongest general heuristic is this: if you can accomplish your task with one agent and 40 or fewer tools without exceeding context windows or latency requirements, use one agent. If you need more than 40 tools, or you exceed context windows, or you need parallelism, or you need different models, or you need security isolation, then consider multiple agents. The number 40 is not magic, but research in September 2025 by Anthropic and OpenAI found that agent tool-selection accuracy starts degrading noticeably above 40 tools, and prompt complexity for describing 40 tools pushes the limits of maintainability. Below 40 tools, single-agent is almost always the right choice. Above 40 tools, you might benefit from splitting into specialized agents that each have smaller tool sets, but you should verify that tool count is actually the problem before adding that complexity.

In November 2025, a data integration platform had 65 tools for connecting to various data sources and transforming data. They initially built one agent with all 65 tools. Tool selection accuracy was poor: the agent frequently chose the wrong connector or transformation tool. They split into three agents: one for data extraction with 25 tools, one for transformation with 22 tools, one for loading with 18 tools. Tool selection accuracy improved significantly. This was a legitimate case where tool count justified multi-agent. But they first tried single-agent, measured the problem, and only added complexity when they had data showing it was necessary.

## Technology Evolution

The final consideration is that agent frameworks and LLM capabilities are evolving rapidly. In December 2025, Claude Opus 4.5 was released with support for 128 tools in parallel calls, up from previous models' limits of 64 tools, and with significantly better tool selection accuracy even with large tool sets. This expanded the viability range of single-agent systems. What required multi-agent with earlier models worked fine as single-agent with Opus 4.5. In January 2026, a new orchestration framework called LangGraph 2.0 reduced multi-agent coordination overhead by 60 percent through optimized message passing and persistent state management. This reduced the performance penalty of multi-agent, making it more competitive with single-agent for latency-sensitive applications. The trade-offs shift as technology improves. What required multi-agent in 2024 might work fine as single-agent in 2026. What had unacceptable coordination overhead in 2025 might be performant in 2027.

Choose based on current constraints, but know that these constraints are moving targets. Do not over-invest in architectural decisions that might be obsoleted by model improvements in six months. Keep your architecture flexible enough to evolve. If you start with single-agent and later need to split, that is usually a manageable refactor. If you start with multi-agent and later realize you did not need it, the consolidation is painful.

## Making the Decision

You are making an architectural decision with long-term consequences. Single-agent is simpler to build, simpler to debug, simpler to deploy, cheaper to run, and faster to iterate on. Multi-agent enables parallelism, model specialization, context isolation beyond what single context windows support, and security boundaries that single-agent cannot provide. The decision should be driven by your specific technical requirements, not by what seems architecturally sophisticated or what blog posts and conference talks showcase. The legal technology company that spent five months building unnecessary multi-agent complexity learned this the hard way. Their beautiful architecture diagram did not ship value to customers. Their fast, simple single-agent replacement did.

Start simple. Add complexity only when you have concrete evidence that simplicity is insufficient. Most of the time, simplicity is sufficient. When you do need multi-agent, add the minimum number of agents necessary to solve your specific constraint. If you need parallelism for three independent data sources, use three specialized agents plus one synthesis agent, not ten agents because more feels more sophisticated. If you need security isolation for two trust zones, use two agents, not five. Every agent you add multiplies complexity. Make that trade consciously, with clear justification, not as a default.

The decision tree is simple. Can one agent with available tools handle your task within context window limits and latency requirements without exceeding 40 tools? Yes: use one agent. No because you need different models: use multiple agents, one per model. No because you exceed context windows: use hierarchical agents with summarization. No because you need parallelism: use specialized agents for parallel subtasks plus synthesis. No because you need security isolation: use separate agents in separate security zones. No because operational characteristics differ wildly: consider separate agents for fast and slow paths. For everything else: use one agent. This decision tree will serve you well for most applications you build in 2026 and beyond.
