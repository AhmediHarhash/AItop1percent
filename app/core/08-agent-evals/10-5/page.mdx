# 10.5 — Agent Infrastructure: Compute, Storage, and Network Requirements

According to infrastructure post-mortems from 2025, 68% of production agent failures in the first 30 days stem not from model errors or logic bugs but from resource exhaustion. Compute saturation, storage overflow, and network egress cost explosions share a common root cause: teams provision agent infrastructure using rules from stateless web applications. Agents are orchestration systems with persistent state, sequential decision loops, and tool call cascades. Their resource profiles differ categorically from APIs, and sizing infrastructure for agents requires modeling workloads the industry has not seen at scale before 2024.

Agent infrastructure requirements differ categorically from conventional application infrastructure because agents are orchestration systems that maintain state, make sequential decisions, and coordinate external dependencies. Your infrastructure sizing determines not just performance but feasibility. Underprovisioned compute creates cascade failures where queue depth grows faster than processing capacity. Undersized storage forces you to truncate agent memory, degrading task success rates. Insufficient network capacity throttles tool execution, turning what should be parallel operations into sequential bottlenecks. This subchapter teaches you how to size compute, storage, and network resources for agent workloads, accounting for the specific characteristics that make agents resource-intensive: multi-turn inference, persistent state, tool orchestration, and trace instrumentation. You will learn to model resource consumption across agent execution phases, calculate infrastructure costs before deployment, and architect systems that scale economically without sacrificing reliability.

## Compute Requirements: Inference, Orchestration, and Tool Execution

Agent compute consumption splits into three distinct phases, each with different resource profiles. First is **inference compute**, the CPU or GPU time consumed by the language model generating responses. Second is **orchestration compute**, the runtime overhead of your agent framework managing state, parsing tool calls, executing control flow logic, and maintaining conversation context. Third is **tool execution compute**, the resources consumed running the actual tools your agent invokes, whether that is database queries, API calls with processing overhead, or local computations like data transformations. Most teams provision only for inference, discovering too late that orchestration and tool execution can consume more resources than the model itself.

Inference compute depends on model size, context length, and output token generation. A single GPT-5 call with 8,000 input tokens and 1,500 output tokens takes approximately 2.3 seconds of wall-clock time when using OpenAI's hosted API, but that latency masks the actual compute consumption. If you are running models on your own infrastructure using Llama 4 70B or a fine-tuned Claude derivative, you need GPU memory equal to roughly 1.5 times the model parameter count in bytes when using 16-bit precision. A 70 billion parameter model requires 140 gigabytes of GPU memory, typically split across multiple A100 or H100 GPUs. Inference throughput scales with GPU count and memory bandwidth, but you cannot simply multiply GPUs linearly. A single H100 can serve approximately 12 concurrent requests with 4,000-token contexts at 2-second latency. Adding a second H100 does not double throughput to 24 requests because memory bandwidth and inter-GPU communication create overhead. Realistic scaling efficiency is 85% for two GPUs, 72% for four, and 60% for eight. Your compute provisioning must account for this sub-linear scaling when sizing infrastructure for agent concurrency targets.

Orchestration compute is the invisible cost that surprises teams. An agent framework like LangChain, Semantic Kernel, or a custom orchestration layer consumes CPU cycles for parsing model outputs, validating tool call syntax, maintaining conversation history in memory, executing retry logic, and managing concurrency primitives like locks and queues. For a simple agent that makes three tool calls per task, orchestration overhead averages 140 milliseconds of CPU time per turn, split between JSON parsing at 35 milliseconds, state serialization at 45 milliseconds, and control flow logic at 60 milliseconds. This sounds negligible until you scale to 1,000 concurrent agents, each running five turns per task. That is 5,000 turns, consuming 700 seconds of CPU time, or nearly 12 full CPU cores at 100% utilization just for orchestration. If your compute nodes have 8 cores and you are also running inference or tool execution on those same nodes, orchestration overhead can saturate available CPU before you hit your expected throughput.

Tool execution compute varies wildly by tool type. A simple API call to a weather service consumes minimal local compute, just enough to serialize the request and deserialize the response, perhaps 10 milliseconds of CPU time. But a tool that queries a database with a complex join across three tables, filters 200,000 rows, and returns aggregated results can consume 600 milliseconds of database CPU time, which if you are running your own database instance translates directly into your compute costs. A tool that processes a PDF document, extracting text and running OCR on embedded images, can consume 4.5 seconds of CPU time and require 2 gigabytes of memory for the document buffer and image processing libraries. Your infrastructure sizing must account for the worst-case tool execution profile, not the average. If your agent can invoke any of 20 tools and three of them are compute-intensive, you must provision for scenarios where multiple agents invoke those heavy tools concurrently.

The compounding effect comes from concurrency. If you target 500 concurrent agents, each running an average of four turns with three tool calls per turn, you have 6,000 tool executions in flight at steady state. If 10% of those tools are database queries averaging 400 milliseconds and another 5% are document processing tasks averaging 3 seconds, your infrastructure must sustain 600 concurrent database queries and 300 concurrent document processing tasks. Database queries require connection pooling, which on most managed databases caps at 100 to 500 connections depending on instance size. Document processing requires memory, which if you allocate 2 gigabytes per task means 600 gigabytes of memory must be available across your compute nodes just for tool execution buffers. Provisioning only for average concurrency leads to resource exhaustion during peak load. You need compute capacity that handles the 95th percentile concurrency, not the mean, because agent workloads are bursty. Morning hours might see 200 concurrent agents while afternoon hours spike to 800.

## Storage Requirements: Conversation State, Tool Outputs, and Trace Logs

Agent storage falls into three categories: **conversation state** for maintaining context across turns, **tool outputs** that must be persisted for replay or auditing, and **trace logs** that capture execution details for debugging and evaluation. Conversation state is your primary storage driver because it grows with every turn and must be retrieved quickly to maintain low latency. Tool outputs can become unexpectedly large when agents invoke services that return rich data structures. Trace logs are the silent killer of storage budgets because comprehensive instrumentation generates megabytes of data per agent session, and most teams do not realize this until the monthly bill arrives.

Conversation state storage scales with conversation length, message size, and retention duration. A typical agent conversation with six turns, where each user message is 200 tokens and each agent response is 800 tokens, totals 6,000 tokens or approximately 24 kilobytes of text when stored as JSON with metadata. If you retain full conversation history for 90 days and serve 100,000 agent sessions per day, your storage requirement is 100,000 sessions times 24 kilobytes times 90 days, which equals 216 gigabytes. This is manageable, but the calculation changes dramatically when you include tool calls. Each tool call adds the tool name, parameters, and response to the conversation history. A tool call with a structured parameter object of 150 tokens and a response payload of 600 tokens adds 750 tokens, roughly 3 kilobytes. If your agent averages three tool calls per turn and six turns per session, that is 18 tool calls adding 54 kilobytes per session. Now your 100,000 daily sessions with 90-day retention require 486 gigabytes. Double your tool call rate to six per turn and storage jumps to 972 gigabytes, nearly a terabyte.

Tool output persistence depends on whether you store raw outputs or just references. Storing raw outputs means every API response, every database query result, every file retrieval is written to storage and associated with the conversation. This is invaluable for debugging and replay, but it explodes storage consumption. An agent that retrieves customer records from a database gets back a JSON array of 50 records, each 1.2 kilobytes, totaling 60 kilobytes. An agent that fetches a product catalog gets back 500 items at 800 bytes each, totaling 400 kilobytes. If your agent makes 10 tool calls per session and 30% of those return large payloads averaging 100 kilobytes, you are adding 300 kilobytes per session just for tool outputs. At 100,000 sessions per day with 90-day retention, that is 2.7 terabytes. Alternatively, you can store only references, like database query IDs or API request URLs, keeping payloads in external systems. This reduces storage but makes replay impossible unless external systems retain their own history, which often they do not.

Trace logs are where storage costs spiral out of control. A comprehensive agent trace includes timestamps for every operation, input and output tokens for every inference call, latency measurements for every tool execution, error messages with stack traces, and decision points with reasoning. For a single agent session with six turns and 18 tool calls, a well-instrumented trace log can easily reach 200 kilobytes when stored as JSON or 150 kilobytes when compressed. At 100,000 sessions per day, that is 15 gigabytes of trace data per day or 450 gigabytes over 30 days. Most teams retain traces for 90 days for compliance or evaluation purposes, pushing storage to 1.35 terabytes just for trace logs. The problem compounds when you add distributed tracing metadata. If your agent orchestration spans multiple services and you are using OpenTelemetry or a similar framework, each trace includes span hierarchies, service names, resource attributes, and propagation context, easily doubling trace size to 300 kilobytes per session. Now you are at 2.7 terabytes for 90 days of traces.

Storage performance matters as much as capacity. Conversation state must be retrieved in under 50 milliseconds to avoid adding latency to every agent turn. This rules out cold storage solutions like Amazon S3 Glacier or Azure Archive, which have retrieval latencies measured in minutes or hours. You need hot storage with low-latency access, typically a managed database like PostgreSQL, DynamoDB, or MongoDB. Database storage costs 10 to 50 times more per gigabyte than object storage like S3, but it provides the query performance and transactional consistency required for agent state management. Tool outputs and trace logs can use tiered storage strategies. Recent data from the last seven days stays in hot storage for fast access during debugging. Data older than seven days moves to warm storage like S3 Standard, where retrieval takes 100 to 500 milliseconds but cost drops to one-tenth of database storage. Data older than 90 days archives to cold storage like S3 Glacier, where retrieval takes hours but cost is one-hundredth of hot storage. Implementing tiered storage requires lifecycle policies and access patterns that distinguish between operational queries needing low latency and analytical queries that can tolerate delays.

## Network Requirements: API Latency, Egress Costs, and Bandwidth Saturation

Agent network consumption is driven by three factors: **API call frequency** to external services, **payload sizes** moving in and out of your infrastructure, and **egress costs** charged by cloud providers for data leaving their network. API call frequency determines how many concurrent connections your infrastructure must sustain. Payload sizes determine bandwidth requirements. Egress costs are the hidden expense that catches teams off guard because cloud providers charge nothing for data ingress but levy substantial fees for data egress, often 10 to 20 cents per gigabyte depending on region and volume tiers.

API call frequency for agents far exceeds typical application workloads. A web application might make one or two API calls per user request, perhaps fetching user profile data and rendering a page. An agent makes API calls on every tool invocation, often in parallel, and agents operate in loops that can execute dozens of turns. An agent handling customer support tickets might invoke six tools per ticket: retrieving customer history from a CRM, fetching order details from an order management system, checking inventory from a warehouse API, querying knowledge base articles from a search service, validating account status from an authentication service, and logging the interaction to an audit system. If you process 50,000 tickets per day and each ticket triggers six API calls, that is 300,000 API calls per day or roughly 3.5 calls per second on average. Peak load during business hours can triple this to 10 calls per second. Each API call requires a TCP connection, SSL handshake, HTTP request, and response handling. Your infrastructure must support connection pooling to avoid exhausting socket limits and must handle SSL termination efficiently to avoid CPU bottlenecks in cryptographic operations.

Payload sizes vary by tool type but trend larger than typical API responses because agents often request rich data structures to inform their reasoning. A tool that queries a customer record might return 1.2 kilobytes of JSON. A tool that fetches order history returns 15 kilobytes for a customer with 20 orders, each with line items and shipping details. A tool that retrieves a knowledge base article returns 40 kilobytes of markdown text. If your agent averages six tool calls per session with an average response size of 12 kilobytes, that is 72 kilobytes of ingress data per session. For 50,000 sessions per day, that is 3.6 gigabytes of ingress. Cloud providers do not charge for ingress, so this is free from a cost perspective, but it consumes bandwidth. If your infrastructure runs in a single availability zone with 10 gigabit network interfaces, 3.6 gigabytes per day is negligible. But agents also send requests, and request payloads can be large when tool parameters include structured data. An agent invoking a database query tool might send a 2-kilobyte JSON object specifying filters, joins, and projections. An agent submitting a document for processing sends the entire document, which could be 500 kilobytes for a PDF. If 10% of your tool calls include large request payloads averaging 50 kilobytes, that is 1.5 gigabytes of egress per day just for tool requests.

Egress costs hit hardest when agents retrieve data from external services hosted in different cloud regions or outside your cloud provider's network. If your agent infrastructure runs on AWS in us-east-1 and invokes an API hosted on Google Cloud in us-central1, every byte of response data is egress from AWS's perspective. AWS charges 9 cents per gigabyte for the first 10 terabytes of egress per month. If your agents pull 3.6 gigabytes of data per day from external APIs, that is 108 gigabytes per month or $9.72 in egress fees just for API responses. This sounds trivial, but the calculation changes when payload sizes grow. An agent system that processes documents, retrieving PDFs from an external document management system, might pull 50,000 documents per day averaging 800 kilobytes each. That is 40 gigabytes per day or 1.2 terabytes per month. At 9 cents per gigabyte, egress costs reach $108 per month. If the external service is in a different region, costs double because inter-region egress is more expensive. Teams often discover these costs only when the monthly bill arrives, having assumed that API calls cost only the per-request fees charged by the API provider.

Bandwidth saturation becomes a bottleneck when agent concurrency scales. If you target 500 concurrent agents and each agent makes three parallel tool calls every 10 seconds, you have 150 tool calls per second at steady state. If average response size is 12 kilobytes, that is 1.8 megabytes per second of ingress bandwidth or 14.4 megabits per second. This is well within the capacity of even modest network interfaces. But peak load during bursts can spike to 2,000 concurrent agents making five parallel tool calls, yielding 1,000 tool calls per second or 96 megabits per second. If your infrastructure runs on instances with 10 gigabit interfaces, you have headroom. But if you are running on smaller instances with 1 gigabit interfaces, you are approaching 10% utilization, which is manageable but leaves little room for spikes. Network saturation manifests as increased latency, not dropped packets, because TCP congestion control slows transmission rates to avoid packet loss. Your agents experience tool call latencies that grow from 200 milliseconds to 600 milliseconds, degrading task success rates and user satisfaction without any explicit error messages.

## Memory Requirements: Context Buffers, Tool State, and Framework Overhead

Memory consumption for agent systems splits into four pools: **context buffers** holding conversation history and model inputs, **tool state** maintaining data structures for tool execution, **framework overhead** from libraries and runtime environments, and **caching layers** that store frequently accessed data to reduce latency. Context buffers scale with conversation length and must fit in RAM for fast access. Tool state can grow unbounded if tools accumulate data across turns. Framework overhead is fixed per agent instance but multiplies with concurrency. Caching trades memory for latency, and sizing cache layers requires understanding access patterns and hit rates.

Context buffers hold the full conversation history that gets passed to the language model on every turn. For a conversation with six turns, each turn containing 1,000 tokens of combined user and agent messages plus tool calls, the context buffer holds 6,000 tokens. At four bytes per token when stored as UTF-8 text in memory, that is 24 kilobytes. If you run 500 concurrent agents, context buffers consume 12 megabytes, which is negligible. But context size grows when you include system prompts, few-shot examples, and retrieved context from vector databases. A system prompt of 2,000 tokens, few-shot examples adding 3,000 tokens, and retrieved context adding 4,000 tokens brings total context to 15,000 tokens per turn. Over six turns, with context accumulating, you might reach 60,000 tokens or 240 kilobytes per agent. Now 500 concurrent agents consume 120 megabytes for context buffers. Add in the fact that agent frameworks often keep multiple versions of context in memory for rollback and branching logic, and memory usage can triple to 360 megabytes. This still sounds manageable, but enterprise agents with longer conversations and richer context can reach 200,000 tokens per session, or 800 kilobytes per agent. At 2,000 concurrent agents, that is 1.6 gigabytes just for context buffers.

Tool state accumulates when tools return data that subsequent tools or reasoning steps reference. An agent executing a multi-step research task might invoke a search tool that returns 20 article summaries totaling 40 kilobytes, then invoke a retrieval tool that fetches full text for five articles totaling 200 kilobytes, then invoke an analysis tool that extracts key quotes totaling 30 kilobytes. The agent framework must hold all this data in memory so the language model can reference it in subsequent turns. If you do not implement memory management strategies like context pruning or external storage for large tool outputs, tool state can balloon to several megabytes per agent. An agent processing documents might accumulate 10 megabytes of document content and metadata across its execution. At 500 concurrent agents, that is 5 gigabytes of memory just for tool state. The problem compounds when agents operate in teams, sharing state across multiple agent instances. A three-agent team where each agent accumulates 2 megabytes of tool state requires 6 megabytes per team, and if you run 200 concurrent teams, memory usage reaches 1.2 gigabytes.

Framework overhead is the fixed cost of running your agent orchestration layer. LangChain, when loaded with its dependency tree including vector database clients, API SDKs, and utility libraries, consumes approximately 85 megabytes of memory per Python process. Semantic Kernel running on .NET consumes roughly 60 megabytes. Custom frameworks built on lightweight runtimes can reduce this to 20 megabytes, but you sacrifice the rich library ecosystem. If you run one agent per process to isolate failures and avoid concurrency bugs, framework overhead scales linearly with agent count. Running 500 concurrent agents with LangChain means 42.5 gigabytes of memory just for framework libraries. The solution is process pooling, where a single process runs multiple agents sequentially or uses async concurrency to handle multiple agents in parallel. A Python process using asyncio can handle 50 concurrent agents with only a single 85-megabyte framework load, reducing total overhead to 850 megabytes for 500 agents. But process pooling introduces failure coupling. If one agent triggers a bug that crashes the process, all agents in that process fail. You must balance memory efficiency against fault isolation.

Caching layers trade memory for reduced latency and API costs. A cache that stores the results of tool calls avoids redundant API requests when multiple agents invoke the same tool with identical parameters. If 30% of your tool calls are cache hits and each cached response averages 15 kilobytes, you need cache capacity equal to 15 kilobytes times the number of unique tool call signatures. For an agent system invoking 50 distinct tools, with each tool called with 10 common parameter sets, you have 500 unique signatures. Caching all of them requires 7.5 megabytes. This is trivial, but cache hit rates depend on request diversity. If tools are called with highly variable parameters, hit rates drop below 10%, making large caches wasteful. A better strategy is adaptive caching that tracks access frequency and evicts rarely used entries. A least-recently-used cache with 100 megabytes of capacity can serve a 40% hit rate for a diverse workload, reducing API call volume by 40% and cutting latency for cached responses from 300 milliseconds to under 10 milliseconds.

## Infrastructure Modeling: Sizing for Peak Load and Cost Targets

Infrastructure modeling requires calculating resource consumption across all dimensions—compute, storage, network, memory—for your target workload, then adding headroom for peak load and failure scenarios. The goal is not to provision for absolute worst case, which wastes resources, but to provision for the 95th percentile with autoscaling that handles the remaining 5%. This section teaches you how to build a resource model that inputs your agent design parameters and outputs infrastructure requirements and monthly costs.

Start with **workload definition**: How many agent sessions per day, how many turns per session, how many tool calls per turn, and what is the distribution of tool types. A customer support agent system processing 100,000 tickets per day, averaging five turns per ticket with four tool calls per turn, executes 2 million tool calls per day. If tool types break down as 40% database queries, 30% API calls to external services, 20% knowledge base searches, and 10% document processing, you have 800,000 database queries, 600,000 external API calls, 400,000 knowledge base searches, and 200,000 document processing tasks per day. Divide by 86,400 seconds per day to get per-second rates: 9.3 database queries per second, 6.9 external API calls per second, 4.6 knowledge base searches per second, and 2.3 document processing tasks per second. These are average rates. Peak load during business hours is typically 3 to 5 times the average. Assume 4x peak, yielding 37 database queries per second, 28 external API calls per second, 18 knowledge base searches per second, and 9 document processing tasks per second.

Next, assign **resource profiles** to each operation type. A database query consumes 50 milliseconds of database CPU time and returns 8 kilobytes on average. An external API call consumes 10 milliseconds of local CPU for request serialization and response parsing, incurs 200 milliseconds of network latency, and returns 12 kilobytes on average. A knowledge base search consumes 150 milliseconds of search service CPU and returns 30 kilobytes. A document processing task consumes 3 seconds of CPU time and 2 gigabytes of memory. At peak load, database queries require 37 times 50 milliseconds equals 1.85 seconds of database CPU per second, or 1.85 CPU cores. External API calls require 28 times 10 milliseconds equals 280 milliseconds of local CPU per second, or 0.28 cores. Knowledge base searches require 18 times 150 milliseconds equals 2.7 seconds, or 2.7 cores. Document processing requires 9 times 3 seconds equals 27 seconds, or 27 cores. Total CPU requirement at peak is roughly 32 cores. Add inference compute: if each of the 100,000 daily sessions runs five inference calls averaging 2 seconds, that is 500,000 seconds of inference per day or 5.8 seconds per second on average, or 5.8 inference cores. At 4x peak, inference needs 23 cores. Combined, you need 55 cores for steady-state peak load.

Memory requirements sum context buffers, tool state, and framework overhead. If you run 500 concurrent agents at peak, each with 300 kilobytes of context buffer, 1 megabyte of tool state, and 1.7 megabytes of framework overhead per agent when using process pooling of 50 agents per process, total memory is 500 times 1.3 megabytes equals 650 megabytes plus framework overhead of 85 megabytes times 10 processes equals 850 megabytes, totaling 1.5 gigabytes. Add 2 gigabytes per document processing task, with nine concurrent tasks at peak, adding 18 gigabytes. Total memory requirement is roughly 20 gigabytes.

Storage requirements combine conversation state, tool outputs, and trace logs. At 100,000 sessions per day, each with 50 kilobytes of conversation state, 200 kilobytes of tool outputs, and 150 kilobytes of trace logs, total storage per day is 400 kilobytes per session times 100,000, equaling 40 gigabytes per day. For 90-day retention, that is 3.6 terabytes. Use tiered storage: keep 7 days in hot database storage at $0.20 per gigabyte per month, totaling 280 gigabytes costing $56 per month. Keep 30 days in warm object storage at $0.023 per gigabyte per month, totaling 1.2 terabytes costing $28 per month. Keep remaining 53 days in cold storage at $0.004 per gigabyte per month, totaling 2.12 terabytes costing $8 per month. Total storage cost is $92 per month.

Network egress is 100,000 sessions times four tool calls per turn times five turns per session times 12 kilobytes average response size, equaling 24 gigabytes per day or 720 gigabytes per month. At $0.09 per gigabyte, egress costs $65 per month. Compute costs depend on instance type. Running 55 cores continuously at $0.05 per core-hour equals $2,035 per month. Memory-optimized instances to support 20 gigabytes might cost $0.08 per core-hour, raising compute to $3,256 per month. Add inference costs: if you use OpenAI's API at $0.01 per 1,000 input tokens and $0.03 per 1,000 output tokens, with 100,000 sessions per day, five inference calls per session, averaging 8,000 input tokens and 1,500 output tokens per call, input costs are 100,000 times 5 times 8 times $0.01 equals $40,000 per month and output costs are 100,000 times 5 times 1.5 times $0.03 equals $22,500 per month, totaling $62,500 per month. Infrastructure cost is negligible compared to inference API costs, but underprovisioned infrastructure degrades success rates, costing you more in rework and customer churn than you save on compute.

## Cost Optimization: Efficiency Strategies Without Sacrificing Reliability

Cost optimization for agent infrastructure balances efficiency against reliability. Aggressive cost cutting through underprovisioning leads to queue depth explosions, task failures, and user dissatisfaction. Overprovisioning wastes budget on idle resources. The optimal strategy is right-sizing for typical load with autoscaling for peaks, combined with efficiency techniques that reduce per-task resource consumption without degrading quality. This section covers six cost optimization strategies that maintain reliability while reducing infrastructure spend by 30 to 60 percent.

First is **request batching** for inference calls. If your agent orchestration can buffer multiple agents awaiting inference and send their prompts as a batch to the language model API, you reduce per-request overhead and may qualify for volume discounts. OpenAI's batch API offers 50% cost reduction for requests that can tolerate 24-hour latency, but this is unsuitable for real-time agents. A more practical approach is micro-batching, where your orchestration layer collects inference requests over a 200-millisecond window and submits them together. If you have 20 agents waiting for inference in that window, you send 20 prompts in a single batch request. Some model APIs process batches more efficiently, reducing per-token cost by 10 to 15 percent. The downside is added latency equal to the batching window, which for latency-sensitive agents is unacceptable. Micro-batching works best for background agents processing queued tasks where 200 milliseconds of additional latency is negligible compared to overall task duration.

Second is **context pruning** to reduce input token counts. Agents accumulate conversation history that grows with every turn, but not all history is relevant to the current reasoning step. A naive agent sends the entire conversation to the model on every turn, causing input token counts to grow quadratically with conversation length. A six-turn conversation with 1,000 tokens per turn sends 6,000 tokens on the sixth turn. A twelve-turn conversation sends 12,000 tokens. A twenty-turn conversation sends 20,000 tokens. Context pruning uses heuristics or lightweight models to identify which prior turns are relevant to the current task and truncates the rest. A simple heuristic is keeping only the most recent three turns plus the system prompt. A more sophisticated approach uses embedding similarity to identify semantically relevant prior turns. If pruning reduces average input tokens from 8,000 to 5,000, you cut input costs by 37.5 percent. The risk is discarding information the agent needs, causing task failures. Evaluation on your specific workload is essential to tune pruning aggressiveness without degrading success rates.

Third is **tool output summarization** to reduce tool state memory and context size. When a tool returns a large payload, like 200 kilobytes of search results, the agent does not need the full payload in every subsequent turn. Instead, invoke a lightweight summarization step that extracts key information, reducing the payload to 10 kilobytes. Store the summary in conversation context and keep the full payload in external storage, retrievable if the agent explicitly requests it. This reduces memory consumption and context token counts without losing information. The tradeoff is the latency and cost of the summarization step, which might add 500 milliseconds and $0.001 per summarization. If you reduce context tokens by 3,000 per session and save $0.03 in inference costs, the summarization pays for itself 30 times over.

Fourth is **caching tool results** to avoid redundant API calls. If multiple agents invoke the same tool with identical parameters, cache the result and serve it from memory or fast storage. A cache hit eliminates the API call latency, reduces external API costs, and frees up compute resources. For a workload where 25% of tool calls are cache hits, you reduce tool API costs by 25% and improve average latency by eliminating 300 milliseconds of network round-trip time on those calls. Cache storage costs are minimal—100 megabytes of cache capacity costs under $0.02 per month in Redis or Memcached—but cache invalidation is critical. If the cached data becomes stale, agents make decisions on outdated information. Implement time-based expiration, where cache entries expire after a period like 10 minutes for rapidly changing data or 24 hours for stable data. Monitor cache hit rates and adjust expiration policies to balance freshness against efficiency.

Fifth is **autoscaling compute and memory** based on real-time load metrics. Instead of provisioning for peak load continuously, run infrastructure that handles average load and dynamically adds capacity when queue depth or CPU utilization crosses thresholds. Autoscaling reduces costs by 40 to 60 percent because peak load typically occurs for only 4 to 8 hours per day. During off-peak hours, you scale down to minimal capacity. The challenge is autoscaling latency. Cloud providers take 3 to 10 minutes to provision new instances, so your infrastructure must tolerate queue depth growth during the scaling window. Implement queue-based autoscaling that triggers when queue depth exceeds 50 pending tasks, giving you time to add capacity before queue depth becomes unmanageable. Pair this with aggressive scale-down policies that remove instances when queue depth stays below 10 for five consecutive minutes. Monitor scaling events closely during the first month to tune thresholds and avoid thrashing, where instances repeatedly scale up and down.

Sixth is **spot instances or preemptible VMs** for non-critical agent workloads. Cloud providers offer compute capacity at 60 to 80 percent discounts in exchange for the ability to reclaim instances with minimal notice, typically 30 seconds to 2 minutes. Spot instances are suitable for background agents processing queued tasks where interruption is tolerable because the task can be retried. They are unsuitable for real-time agents serving user requests because interruptions cause user-visible failures. A hybrid strategy runs real-time agents on on-demand instances for reliability and background agents on spot instances for cost savings. Implement checkpointing so interrupted agents can resume from their last saved state instead of restarting from the beginning. If your background workload consumes 30 compute cores continuously and you migrate it to spot instances, you save $1,200 per month on a $2,000 per month compute bill, a 60% reduction for that workload segment.

---

Your infrastructure determines whether your agent system is feasible at scale. Compute, storage, network, and memory requirements for agents differ categorically from traditional applications because agents orchestrate multiple external dependencies, maintain persistent state, and generate verbose instrumentation. The teams that succeed at agent deployment are those that model infrastructure requirements before launch, size resources for the 95th percentile with autoscaling for peaks, and implement cost optimization strategies that reduce spend without sacrificing reliability. The next subchapter, Agent Scaling: Horizontal, Vertical, and Auto-Scaling, explores how to grow your agent infrastructure from hundreds to tens of thousands of concurrent agents while maintaining low latency and high availability.
