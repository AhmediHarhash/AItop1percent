# 9.4 â€” Agent Debugging Tools: LangSmith, Braintrust, Langfuse, Arize

In mid-2025, a financial services company built a sophisticated agent to assist wealth management advisors with portfolio analysis and client communication. The agent integrated with seven different data sources, called twelve different APIs, and generated personalized investment recommendations based on client profiles, market data, and regulatory constraints. During internal testing, the agent worked well. Three days after launch, advisors began reporting inconsistent recommendations: the same client query would sometimes generate conservative recommendations and sometimes aggressive ones, with no clear pattern. The engineering team spent two weeks trying to reproduce the inconsistency. They added extensive logging, captured inputs and outputs, and ran hundreds of test cases, but they could not isolate the root cause. The logs showed what the agent decided but not how it decided. They could see that the agent called the risk assessment API with different parameters on different runs, but they could not see why the parameters differed or what reasoning led the agent to choose those parameters. After burning $47,000 in engineering time and delaying a major client rollout, the team adopted LangSmith. Within four hours of integrating LangSmith and inspecting traces, they identified the issue: the agent was using conversation history to inform risk tolerance, but the history was being truncated inconsistently depending on token limits, causing the agent to sometimes include a client's mention of conservative preferences and sometimes exclude it. The fix took twenty minutes. The lesson was clear: without purpose-built agent debugging tools, you are debugging blind.

Agent debugging tools are specialized platforms designed to capture, visualize, and analyze the execution of language model agents. These tools provide trace collection, interactive trace visualization, performance profiling, error tracking, prompt versioning, evaluation integration, and production monitoring. They turn agent debugging from a manual, time-consuming process into a structured, efficient workflow. In 2026, the leading agent debugging platforms are LangSmith, Braintrust, Langfuse, and Arize. Each has strengths and trade-offs. LangSmith is the most mature and feature-rich, tightly integrated with LangChain. Braintrust focuses on evaluation and experimentation with excellent prompt versioning. Langfuse is open-source and privacy-focused, ideal for teams with strict data residency requirements. Arize is an observability platform that extends beyond agents to cover the full ML lifecycle, including model monitoring and drift detection. Choosing the right tool depends on your framework, your privacy requirements, your budget, and your team's workflow. This subchapter provides a detailed comparison of these tools and guidance on how to integrate them into your agent development process.

## LangSmith: The Most Comprehensive Agent Debugging Platform

LangSmith is developed by LangChain and is the most widely adopted agent debugging tool in 2026. It provides full-stack observability for LangChain agents, including trace collection, visualization, evaluation, dataset management, and production monitoring. LangSmith is designed to integrate seamlessly with LangChain, but it also supports other frameworks through its tracing API. If you are building agents with LangChain, LangSmith is the default choice. If you are using another framework, LangSmith is still worth considering for its mature feature set and strong community support.

LangSmith's core feature is its trace visualization. When you enable LangSmith tracing in your LangChain agent, every execution is automatically captured as a trace. The trace includes every chain invocation, every LLM call, every tool call, every retrieval step, and every prompt template rendering. You access traces through the LangSmith web interface, where you see a list of recent runs with metadata like input, output, latency, cost, and status. You click on a run to open the trace viewer, which displays the execution as an interactive tree. Each node in the tree represents a step: a chain, a model call, a tool invocation, or a retrieval query. You expand nodes to see detailed metadata: the full prompt, the model parameters, the output, the token counts, the latency, and the cost.

LangSmith's trace viewer includes several advanced features that make debugging efficient. First, it highlights errors in red and warnings in yellow, making it easy to identify failed steps. If a tool call throws an exception, the node is red and the error message is displayed. If a model call has high latency or low confidence, the node is yellow. Second, it provides a timeline view that shows the temporal dimension of the trace. You can see which steps ran sequentially and which could be parallelized. Third, it allows you to search and filter traces by input text, output text, metadata tags, or error messages. If you want to find all traces where the agent failed to retrieve a specific document, you search for the document name and LangSmith returns all matching traces. Fourth, it supports trace comparison. You select two traces and LangSmith displays them side by side, highlighting the differences. This is invaluable for debugging non-deterministic behavior.

LangSmith also includes evaluation and dataset management. You create datasets by saving traces as examples. Each example includes the input, the expected output, and metadata. You then run evaluations by executing your agent on the dataset and comparing the actual outputs to the expected outputs. LangSmith supports custom evaluators written in Python, as well as built-in evaluators for common metrics like exact match, semantic similarity, and hallucination detection. Evaluation results are displayed in a dashboard that shows pass rates, failure modes, and example-level details. You can drill into individual failures, inspect the trace, and diagnose the issue. This tight integration between traces and evaluations makes it easy to iterate on your agent: you identify a failure in production, save it as a test case, fix the agent, re-run the evaluation, and confirm that the failure is resolved.

LangSmith's production monitoring features include real-time dashboards, alerts, and analytics. You configure LangSmith to sample traces from production at a specified rate, such as 5% of all requests. The sampled traces are stored and analyzed. The dashboard shows aggregate metrics like request volume, average latency, error rate, and cost per request. You set up alerts to notify you when error rates spike, latency exceeds thresholds, or cost anomalies occur. LangSmith also provides cohort analysis, allowing you to segment traces by user, session, or metadata tags and compare behavior across cohorts. If you want to understand whether your agent performs better for premium users than for free users, you tag traces with user tier and LangSmith shows you the performance breakdown.

LangSmith's pricing is based on trace volume. The free tier includes 5,000 traces per month, which is sufficient for small projects and local development. The paid tiers start at $39 per month for 50,000 traces and scale up to enterprise plans with millions of traces, dedicated support, and custom integrations. For most teams, the cost is negligible compared to the engineering time saved by efficient debugging. The primary limitation of LangSmith is that it is a hosted service operated by LangChain. Your traces are stored on LangSmith's servers, which may not be acceptable for teams with strict data residency requirements or privacy policies. LangSmith offers enterprise deployments with on-premises or private cloud options, but these require custom contracts.

## Braintrust: Evaluation-First Debugging with Prompt Versioning

Braintrust is an evaluation and experimentation platform that includes agent tracing as a core feature. While LangSmith is trace-first, Braintrust is evaluation-first. The platform is designed around the workflow of running experiments, comparing prompt versions, and measuring performance across datasets. If your primary focus is prompt engineering and evaluation, Braintrust is an excellent choice. If you need deep production monitoring and real-time alerting, Braintrust is less mature than LangSmith or Arize.

Braintrust's key strength is prompt versioning. You create prompts in the Braintrust interface, tag them with version numbers, and reference them in your agent code by ID. When you modify a prompt, Braintrust saves the old version and creates a new version. You can then run experiments that compare multiple prompt versions on the same dataset. Braintrust runs your agent with each prompt version, captures traces, evaluates the outputs, and displays the results in a comparison dashboard. You see which prompt version has the highest accuracy, the lowest latency, or the best user satisfaction score. This makes it easy to iterate on prompts systematically rather than guessing whether a change improved performance.

Braintrust's trace visualization is comparable to LangSmith's but with a stronger focus on evaluation context. When you inspect a trace in Braintrust, you see not only the execution steps but also the evaluation metrics for that run. If the agent failed an evaluation, the trace shows which evaluator failed and why. If the output was semantically incorrect, the trace highlights the reasoning step where the error occurred and shows the expected output for comparison. This tight coupling between traces and evaluations makes it easy to understand not just what the agent did, but whether what it did was correct.

Braintrust also excels at dataset management. You upload datasets as CSV or JSON files, or you create them by logging production traces. Each dataset includes inputs, expected outputs, and metadata. You organize datasets into folders and tag them with labels like "regression suite," "edge cases," or "high-priority customers." You then run experiments by selecting a dataset, selecting one or more prompt versions, and clicking "Run." Braintrust executes your agent on every example in the dataset, captures traces, runs evaluators, and stores the results. You can compare experiments side by side to see which prompt version performed best. This workflow is particularly powerful for teams that treat prompt engineering as an experimental science rather than an ad-hoc process.

Braintrust's pricing is competitive with LangSmith. The free tier includes 10,000 evaluations per month, and paid tiers start at $50 per month for 100,000 evaluations. Like LangSmith, Braintrust is a hosted service, which may be a constraint for teams with strict data residency requirements. Braintrust offers enterprise plans with private deployments, but the default option is cloud-hosted.

The primary limitation of Braintrust is its production monitoring capabilities. While Braintrust supports logging traces from production, it does not have the same real-time dashboard, alerting, or anomaly detection features as LangSmith or Arize. Braintrust is designed for development and evaluation, not for ongoing production observability. If you need both evaluation and production monitoring, you may need to use Braintrust for development and a separate tool like Arize for production.

## Langfuse: Open-Source and Privacy-Focused Observability

Langfuse is an open-source agent observability platform that you can self-host or use as a managed service. It is designed for teams that need full control over their data, whether for privacy, compliance, or cost reasons. Langfuse provides trace collection, visualization, evaluation, and analytics, all running on your own infrastructure. If you have strict data residency requirements, if you operate in a highly regulated industry, or if you want to avoid vendor lock-in, Langfuse is the best choice.

Langfuse's architecture is straightforward. You deploy Langfuse as a Docker container or Kubernetes service in your cloud environment. You configure your agent to send traces to your Langfuse instance using the Langfuse SDK. Traces are stored in a PostgreSQL database that you control. You access the Langfuse web interface to view traces, run evaluations, and analyze performance. Because Langfuse runs on your infrastructure, your trace data never leaves your environment. This is critical for compliance with GDPR, HIPAA, or contractual data processing agreements.

Langfuse's trace visualization is similar to LangSmith's and Braintrust's. You see an interactive tree of execution steps, with metadata for each step. You can expand nodes, search traces, filter by tags, and compare runs. Langfuse also supports custom metadata and tags, allowing you to annotate traces with business context like user ID, session ID, transaction amount, or customer tier. This makes it easy to segment and analyze traces by business dimensions.

Langfuse's evaluation features are less mature than Braintrust's but growing rapidly. You define evaluators as Python functions and register them with Langfuse. You create datasets by uploading examples or logging traces. You run evaluations by selecting a dataset and a set of evaluators, and Langfuse executes your agent on the dataset and displays the results. Langfuse also supports human-in-the-loop evaluation, where you can flag traces for manual review and assign labels or scores. This is useful for cases where automated evaluators are insufficient and you need domain expert judgment.

Langfuse's analytics dashboard provides aggregate metrics like request volume, latency, error rate, and cost. You can create custom charts and filter by metadata tags. Langfuse also supports cohort analysis and trend detection. If you want to see whether your agent's latency has increased over the past week, you create a chart that plots average latency by day. If you want to compare performance across customer segments, you filter traces by customer tier and compare metrics.

The primary advantage of Langfuse is control. You own the infrastructure, the data, and the code. You can customize Langfuse to fit your workflow, integrate it with your existing tools, and scale it to handle your trace volume. You can audit the source code, contribute features, and deploy updates on your own schedule. This flexibility is invaluable for teams with unique requirements or strict compliance mandates.

The primary limitation of Langfuse is operational overhead. You need to deploy and maintain Langfuse yourself, which requires infrastructure expertise and ongoing effort. You need to monitor the PostgreSQL database, scale the web service, handle backups, and apply security patches. For large teams with dedicated platform engineering resources, this is manageable. For small teams without infrastructure expertise, the overhead may outweigh the benefits. Langfuse offers a managed cloud service that removes the operational burden while still allowing you to configure data residency and access controls, but this reduces some of the cost and control advantages of self-hosting.

## Arize: Production Observability Beyond Agents

Arize is a machine learning observability platform that includes agent tracing as part of a broader suite of ML monitoring tools. Arize is designed for teams that operate production ML systems at scale and need to monitor not only agents but also traditional ML models, embeddings, and data pipelines. If you already use Arize for model monitoring, adding agent observability is a natural extension. If you are starting from scratch and only need agent debugging, Arize may be more complex and expensive than LangSmith, Braintrust, or Langfuse.

Arize's agent tracing features include trace collection, visualization, and performance analytics. You instrument your agent with the Arize SDK, and traces are sent to Arize's platform. The trace viewer is similar to LangSmith's and Langfuse's, with an interactive tree of execution steps and detailed metadata. Arize also provides advanced filtering and search capabilities, allowing you to query traces by input text, output text, latency, cost, or custom tags.

Where Arize differentiates itself is in production observability and anomaly detection. Arize continuously monitors your agent's performance and uses machine learning to detect anomalies. If your agent's error rate suddenly spikes, if latency increases, if cost per request jumps, or if output quality degrades, Arize sends an alert. Arize also provides root cause analysis, showing you which dimensions of your data or which execution paths are associated with the anomaly. If your agent's error rate is high for a specific customer segment, Arize identifies that segment and surfaces example traces. This makes it much faster to diagnose and resolve production issues.

Arize also supports embedding and retrieval observability. If your agent uses vector search, Arize captures the embeddings, the queries, the retrieved documents, and the relevance scores. Arize then analyzes the embedding space to detect drift, outliers, or clusters. If your agent starts retrieving irrelevant documents, Arize shows you that the retrieval quality has degraded and provides example queries where retrieval failed. If your embedding model is updated and the embedding space shifts, Arize detects the shift and alerts you. This level of observability is unique to Arize and is extremely valuable for agents that rely heavily on retrieval.

Arize's pricing is based on trace volume and model monitoring volume. The platform is designed for enterprise customers and is more expensive than LangSmith, Braintrust, or Langfuse. Arize does not offer a free tier for production use, and pricing starts at several hundred dollars per month. For large teams with complex production systems, the cost is justified by the advanced observability and anomaly detection features. For small teams or early-stage projects, Arize may be overkill.

The primary limitation of Arize is complexity. Arize is a comprehensive ML observability platform with features for model monitoring, data quality, drift detection, bias analysis, and more. If you only need agent debugging, much of this functionality is irrelevant. The interface is correspondingly complex, with many menus, dashboards, and configuration options. The learning curve is steeper than LangSmith or Braintrust. For teams that already use Arize for other ML systems, this complexity is acceptable. For teams that only need agent debugging, a simpler tool is likely a better fit.

## Choosing the Right Tool for Your Team

The right agent debugging tool depends on your framework, your privacy requirements, your budget, and your team's workflow. Here is a decision framework to guide your choice.

If you are using LangChain and you have no strict data residency requirements, start with LangSmith. It is the most mature, feature-rich, and well-documented option. The free tier is sufficient for development, and the paid tiers are affordable for most teams. LangSmith's integration with LangChain is seamless, and the trace visualization, evaluation, and monitoring features cover the full agent development lifecycle.

If your primary focus is prompt engineering and evaluation, and you want excellent prompt versioning and experiment comparison, choose Braintrust. Braintrust's evaluation-first workflow makes it easy to iterate on prompts systematically and measure performance rigorously. The free tier is generous, and the paid tiers are competitive. Braintrust is framework-agnostic and works with LangChain, LlamaIndex, or custom agent implementations.

If you have strict data residency requirements, if you operate in a highly regulated industry, or if you want full control over your observability infrastructure, choose Langfuse. Self-host it in your cloud environment and retain complete ownership of your trace data. Langfuse is open-source, so you can audit the code, customize the platform, and contribute features. The operational overhead is higher than LangSmith or Braintrust, but the control and compliance benefits are significant.

If you already use Arize for model monitoring, if you need advanced anomaly detection and root cause analysis, or if you operate large-scale production ML systems, extend your Arize deployment to include agent observability. Arize's ML-native features like embedding drift detection and cohort analysis are powerful for complex agents. The cost and complexity are higher, but for teams that need enterprise-grade observability, Arize is the right choice.

You can also use multiple tools. A common pattern is to use Braintrust for development and evaluation, Langfuse for production observability, and LangSmith for ad-hoc debugging. You instrument your agent to send traces to multiple platforms, and you use each platform for its strengths. This requires more integration effort, but it gives you the best of all worlds.

## Integrating Debugging Tools into Your Agent Development Workflow

Integrating a debugging tool into your agent development workflow is straightforward but requires intentional setup. The first step is to instrument your agent to send traces to the platform. Most platforms provide SDKs for Python and JavaScript that integrate with LangChain, LlamaIndex, and other frameworks. You install the SDK, configure your API key or endpoint, and enable tracing. For LangSmith, you set an environment variable with your API key and tracing is automatic. For Braintrust, you wrap your agent code with a Braintrust context manager. For Langfuse, you initialize the Langfuse client and call logging methods at each step. For Arize, you use the Arize SDK to log traces and metadata.

The second step is to tag traces with metadata that makes them searchable and analyzable. You add custom tags for user ID, session ID, request ID, customer tier, feature flags, or any other business context that helps you segment and filter traces. You add tags in your agent code by passing metadata to the tracing SDK. For example, in LangSmith, you pass metadata as a dictionary when you invoke your chain. In Braintrust, you log metadata with each experiment run. In Langfuse, you attach tags to trace objects. This metadata makes it easy to find specific traces, compare cohorts, and diagnose issues.

The third step is to integrate debugging tools with your CI pipeline. When you run automated tests, you enable tracing and store traces for failed tests. You configure your test runner to include links to traces in failure reports. Engineers can click the link, inspect the trace, and debug the failure without reproducing it locally. This requires integrating your testing framework with the tracing SDK. For example, in a pytest setup, you configure a fixture that enables tracing for each test and logs the trace URL when the test fails.

The fourth step is to set up monitoring and alerting for production. You configure your agent to sample traces from production and send them to your debugging platform. You define alert rules based on error rate, latency, cost, or custom metrics. When an alert fires, you receive a notification with a link to the relevant traces. You inspect the traces, diagnose the issue, and deploy a fix. This closes the loop between production incidents and development, making your agent more reliable over time.

Agent debugging tools are essential infrastructure for professional agent development. They turn debugging from a frustrating guessing game into a systematic, efficient process. They make it possible to understand agent behavior, optimize performance, and build confidence in reliability. Choose the tool that fits your needs, integrate it into your workflow, and invest in learning how to use it effectively. The time you spend on tooling setup will be repaid many times over in faster debugging, fewer production incidents, and higher-quality agents. Next, we will examine how to build a comprehensive observability strategy that combines traces, metrics, logs, and user feedback into a unified view of agent health and performance.
