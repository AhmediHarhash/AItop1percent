# 9.13 — Observability-Driven Agent Improvement Loops

In mid-2025, a customer support automation company spent seven months building what they believed was a high-performing agent. Their pre-deployment eval suite showed 89% task success across 240 synthetic test cases. They launched to 15% of support tickets, expecting gradual refinement. Within three weeks, their customer satisfaction scores dropped 11 points, average resolution time increased by 34%, and their support team was escalating 40% more tickets than before the agent existed. The executive team demanded answers. The engineering team ran their eval suite again—it still showed 89% success. They examined the failed production tickets manually and discovered something disturbing: the agent was failing on interaction patterns that never appeared in their test cases. Customers were asking compound questions, referencing previous conversations the agent couldn't retrieve, and using colloquial product names the agent didn't recognize. The test suite measured what they had built, not what customers actually needed. They had no systematic way to turn production failures into improved capability because they had built evaluation as a pre-launch gate, not as a continuous learning system.

The failure revealed a fundamental misunderstanding of how agent systems improve. Most teams treat observability as operational monitoring—uptime, latency, error rates—and evaluation as a pre-deployment quality gate. They instrument enough to detect when things break, then run evals before each release to confirm nothing regressed. This approach works for relatively static systems where requirements are fully known upfront. It fails catastrophically for agents, where the true distribution of user needs, edge cases, and failure modes only becomes visible in production, and where capability gaps emerge gradually as users discover what the agent can and cannot do. You need observability that feeds directly into evaluation, and evaluation that continuously updates based on what observability reveals. The improvement loop is not a phase of your project—it is the operating model for the entire lifecycle of your agent.

## The Observability-to-Evaluation Pipeline

The connection between observability and evaluation is not aspirational integration you build someday. It is the primary mechanism by which your agent becomes better at the actual job users need it to do. Every production interaction generates signals about whether your agent is meeting user needs, and every signal is a potential test case for your eval suite. The pipeline has four stages: capture, triage, synthesis, and incorporation. Most teams build the first stage—logging interactions—and stop. The value is in the other three.

Capture means recording production interactions with sufficient fidelity to reconstruct what happened and why. This is not just saving the final output. You need the full context the agent received, the tools it called and in what order, the reasoning traces it generated if your architecture exposes them, the user's response or follow-up action, and any explicit or implicit feedback signals. Explicit feedback is ratings, thumbs up or down, escalation requests. Implicit feedback is whether the user accepted the agent's output, whether they rephrased and tried again, whether they abandoned the interaction, whether they immediately contacted a human afterward. Many teams log the agent's output and the user's initial query but not the tool calls, not the reasoning trace, not the follow-up behavior. When they try to debug a failure three weeks later, they have no idea what the agent was thinking or what information it had access to. You cannot learn from interactions you cannot reconstruct.

Triage means deciding which captured interactions warrant deeper investigation. You cannot manually review every production interaction—if you are handling thousands or tens of thousands of agent sessions per day, you need automated filtering to surface the highest-value learning opportunities. Triage is not just flagging errors. Errors are obvious failure signals, but they are not the only learning signal. You also want to surface interactions where the user expressed dissatisfaction even though the agent technically completed the task, where the agent took an unusually long time or an unusually high number of tool calls, where the agent's confidence scores were inconsistent with the user's acceptance, and where the interaction pattern does not match any existing test case in your eval suite. The last criterion is critical and often overlooked: novelty is a signal. If your agent successfully handled a request using a strategy you have never seen before, that interaction should become a test case to ensure future changes do not regress that capability. Triage rules evolve as your understanding of failure modes matures, but the initial set should include explicit negative feedback, task abandonment, high latency, high tool call count, low confidence with high user acceptance, and high confidence with low user acceptance.

Synthesis means converting a triaged interaction into a reusable evaluation case. This is not copy-pasting the production input into your test suite. Production interactions are often messy, context-dependent, and entangled with user-specific data you cannot use in evals. Synthesis is the process of extracting the underlying pattern, anonymizing or generalizing the specifics, and defining the success criteria that should have been met. If a user asked "Why did my payment to Acme Corp get declined yesterday?" and the agent failed because it could not access the transaction history, the synthesized test case is not that exact query about Acme Corp—it is a generalized pattern: "User asks about a recent transaction by merchant name and timeframe; agent must retrieve transaction history, identify the relevant transaction, and surface the decline reason." You write a test case that checks whether the agent can retrieve transaction history by merchant name and time range, not whether it can answer one specific user's question. The synthesized case becomes a permanent artifact in your eval suite, tagged with the production interaction ID it came from, the date it was added, and the capability gap it represents.

Incorporation means adding synthesized cases to your eval suite and re-running evals to measure whether your agent currently passes or fails them. This is where most teams hit friction. They treat their eval suite as a frozen baseline—adding new cases feels like moving the goalposts or penalizing the team for things they did not know about originally. This mindset is backwards. Your eval suite is not a contract you signed at project kickoff. It is a living representation of what your users actually need your agent to do, and it grows as your understanding of those needs becomes more complete. When you add a synthesized case from production and your agent fails it, you have not created a new problem—you have made an existing problem visible. The problem already existed in production; now you can measure and fix it. Incorporation happens on a regular cadence—daily, weekly, or after each significant batch of production traffic—and every new case is tracked as either a regression, a novel capability gap, or a confirmation of existing capability. Regressions mean your agent used to pass similar cases and now fails. Capability gaps mean this is a need you never tested for. Confirmations mean your agent already handles this pattern, and the new case increases coverage.

## Mining Production Data for Systematic Gaps

Not all production failures are random one-off edge cases. Many represent systematic gaps—categories of requests your agent consistently mishandles because of missing capabilities, incorrect assumptions, or undertrained reasoning patterns. Observability-driven improvement means identifying these systematic gaps and prioritizing them by impact. Impact is not the same as frequency. A failure mode that affects 2% of interactions but causes 40% of escalations is higher impact than a failure mode that affects 10% of interactions but users work around without complaint.

You identify systematic gaps by clustering production failures along multiple dimensions: intent category, tool usage pattern, input characteristics, and outcome type. Intent clustering groups interactions by what the user was trying to accomplish, even if they phrased it differently. If 200 users ask some variation of "Where is my order," "Track my package," and "When will I receive my shipment," those are the same intent. If your agent fails 35% of those interactions, you have a systematic gap in order tracking capability, not 70 unrelated failures. Tool usage clustering groups interactions by which tools the agent called and in what sequence. If your agent consistently fails when it needs to call the inventory tool followed by the pricing tool, but succeeds when it calls them in the reverse order, you have a systematic gap in multi-tool orchestration. Input characteristic clustering groups interactions by input length, ambiguity, specificity, or domain complexity. If your agent fails on queries longer than 40 words at twice the rate it fails on shorter queries, you have a systematic gap in handling complex compound requests. Outcome clustering groups interactions by how they ended—task completed, user escalated, user abandoned, user rephrased and retried. Different outcome types suggest different root causes. Escalation often means the agent recognized it could not help; abandonment often means the agent thought it helped but the user disagreed.

Once you have clustered failures into systematic gaps, you prioritize by impact. Impact is a function of frequency, severity, and user visibility. Frequency is how often the gap occurs. Severity is the consequence when it does—revenue loss, compliance risk, user frustration, brand damage. User visibility is whether the failure is obvious to the user or silent. A failure that quietly provides incorrect information is higher severity than a failure that tells the user "I don't know" and escalates. You assign each gap a priority score and work through them in order, not by how easy they are to fix or how interesting the engineering problem is. The highest-impact gaps get translated into focused eval suites—collections of test cases that measure one specific capability dimension—and into targeted capability improvements. You do not try to fix everything at once. You fix the gaps that matter most, measure the improvement, deploy, observe whether the fix worked in production, and repeat.

## Continuous Eval Suite Evolution and Versioning

Your eval suite is not static. It grows as you discover new user needs, shrinks as you retire obsolete test cases, and evolves as your understanding of success criteria becomes more precise. This evolution must be tracked and versioned, or you lose the ability to compare agent performance over time. Teams often realize six months into production that they cannot answer the question "Is our agent better now than it was in March?" because their eval suite in March was completely different from their eval suite now, and they did not version the changes.

Eval suite versioning means treating your test cases as a dataset with a schema, version history, and change log. Every test case has metadata: a unique ID, the date it was added, the source it came from—synthetic design, production synthesis, adversarial generation—the capability or intent it measures, the success criteria, and the version range in which it is active. When you add a new test case, you increment the suite version and log what changed. When you retire a test case because it no longer reflects real user needs or because the capability it tested is now handled by a different part of your system, you mark it as deprecated in the current version rather than deleting it. This allows you to re-run historical versions of your eval suite against current agent builds to measure progress, and to re-run current versions of your eval suite against historical agent builds to understand when regressions were introduced. Most teams never build this versioning discipline and end up with eval suites that are append-only lists of test cases with no provenance, no retirement policy, and no way to isolate what changed between two measurement points.

Eval suite evolution also means recognizing when your success criteria were wrong and updating them based on production feedback. You might have written a test case that checks whether the agent provides an answer within 200 words, but production data shows users are more satisfied with longer, more detailed answers. You update the success criterion to remove the length constraint and add a completeness check. Or you might have written a test case that checks whether the agent retrieves a specific data field, but production usage reveals that users rarely care about that field and often care about a different field you did not include. You update the test case to reflect actual user priorities. These updates are not failures of your original design—they are the natural result of learning what users actually value as opposed to what you predicted they would value before launch. The teams that improve fastest are the ones that update their success criteria based on evidence, not the ones that stubbornly defend their original assumptions.

## Feedback Loop Latency and Iteration Speed

The value of observability-driven improvement is proportional to how quickly you can complete the loop from production failure to updated eval to capability fix to redeployment. If it takes you four weeks to notice a systematic gap, two weeks to write test cases, three weeks to implement a fix, and two weeks to deploy, you are operating on an 11-week improvement cycle. Your users experience the failure for 11 weeks. Your competitors who run the same loop in two weeks will outpace you. Feedback loop latency is the sum of detection latency, synthesis latency, fix latency, and deployment latency. You optimize each independently.

Detection latency is how long it takes to identify that a systematic gap exists. If you review production interactions once a month, your detection latency is at least a month. If you have automated triage rules that flag high-impact failures within hours and surface them in a daily review, your detection latency is one day. Reducing detection latency means investing in automated anomaly detection, real-time dashboards that surface interaction patterns rather than just aggregate metrics, and alert rules that fire when failure rates cross thresholds or when new failure modes appear that do not match known patterns. Many teams monitor aggregate success rates—"87% of interactions succeeded today"—but do not monitor per-intent success rates, per-tool-chain success rates, or per-input-characteristic success rates. Aggregate metrics hide systematic gaps. You need segmented metrics that make gaps visible as soon as they emerge.

Synthesis latency is how long it takes to turn a detected gap into actionable test cases. If synthesis is a manual process that requires an engineer to read through production logs, write test cases by hand, and get them reviewed and merged, synthesis latency is days or weeks. If you have tooling that semi-automates synthesis—extracting the interaction, suggesting a generalized test case, pre-filling the success criteria based on templates—you can reduce synthesis latency to hours. Full automation is difficult because synthesis requires judgment about what the underlying pattern is and what success should look like, but partial automation is tractable. You can automatically generate draft test cases from production interactions and have a human review and approve them, rather than starting from scratch every time.

Fix latency is how long it takes to implement a capability improvement once you have test cases that measure the gap. This depends on the nature of the gap. If the gap is a missing tool, fix latency is however long it takes to build and integrate that tool. If the gap is a reasoning failure—your agent has the tools but does not call them in the right order—fix latency is however long it takes to add examples, update prompts, or retrain a component. If the gap is a policy ambiguity—your agent does not know how to handle a category of request because you never gave it guidance—fix latency is however long it takes to define the policy and encode it. You reduce fix latency by building modular architectures where capabilities can be added or updated independently, by maintaining prompt and policy libraries that can be versioned and deployed without full system rebuilds, and by running continuous evals so you catch regressions immediately rather than discovering them in the next release cycle.

Deployment latency is how long it takes to get a fix into production once it is built. If you deploy once a month, deployment latency is up to a month. If you deploy daily or continuously, deployment latency is hours. Continuous deployment for agents is riskier than for traditional software because agent behavior is less predictable, so most teams use staged rollouts—deploy to 5% of traffic, measure impact, expand to 25%, expand to 100%. Staged rollouts add deployment latency but reduce the risk of large-scale regressions. You balance speed and safety by automating the rollout process, defining clear rollback criteria, and running production evals in parallel with user traffic so you can detect problems before they affect a large population.

## Organizational Discipline and Ownership

Observability-driven improvement only works if someone is responsible for running the loop and if the organization values continuous learning over shipping and forgetting. Many teams build excellent observability tooling, capture rich production data, and generate insightful analyses, but nothing changes because no one has the authority or the time to act on what the data reveals. The loop breaks down in the gap between insight and action.

Ownership means assigning a person or team the responsibility for monitoring production performance, synthesizing learnings into eval updates, prioritizing capability gaps, and driving fixes to completion. This is not the same as assigning ownership of the observability platform or ownership of the eval framework. Those are infrastructure responsibilities. The loop owner is accountable for whether the agent is getting better at its job over time, not whether the dashboards are up or whether the test suite runs cleanly. In many organizations, this ownership sits with a product manager or an engineering lead who has end-to-end accountability for agent performance. In some organizations, it sits with a dedicated agent operations team that bridges product, engineering, and data science. The key is that someone wakes up every day thinking about whether yesterday's production interactions revealed new gaps and what the team is doing about them.

Organizational discipline means treating eval suite updates and capability improvements as first-class work, not as technical debt or nice-to-have backlog grooming. When a high-impact systematic gap is identified, it gets prioritized against feature work and infrastructure work using the same impact framework. If the gap affects 12% of users and causes a 20-point drop in satisfaction, that is higher priority than a new feature that will delight 3% of users. If the gap creates compliance risk, it is higher priority than performance optimization. Many teams struggle with this prioritization because feature work is visible to executives and customers, while closing capability gaps feels like fixing problems that should not have existed in the first place. You reframe the narrative: closing capability gaps is not cleanup, it is growth. Every gap you close expands the range of requests your agent can handle well, which expands your addressable use cases, which drives adoption and retention.

You also build rituals that keep the loop running. A weekly review where the team examines the highest-impact production failures from the past week, discusses what they reveal about systematic gaps, and decides which ones warrant test case synthesis. A monthly retrospective where the team reviews how many new eval cases were added, how many gaps were closed, and whether the agent's production performance improved as measured by user satisfaction, task success rate, and escalation rate. A quarterly planning cycle where eval suite growth and capability roadmap are informed by production learnings, not just by roadmap aspirations. These rituals ensure that observability data is not just collected but acted upon, and that the loop completes rather than stopping at insight.

The best agent teams view their eval suite as a living contract with their users. Every test case represents a promise: "Our agent will handle this type of request well." Every production failure that becomes a new test case is a promise added based on what you learned your users actually need. The eval suite grows in fidelity and coverage over time, and the agent's performance against that suite becomes the primary metric of progress. You are not measuring whether your agent performs well on an arbitrary static benchmark. You are measuring whether it performs well on the evolving real-world distribution of what your users ask it to do. That measurement drives everything—prioritization, resourcing, architecture decisions, and ultimately whether your agent succeeds or becomes another cautionary story about overpromising and underdelivering.

Your observability infrastructure and your evaluation framework are not separate systems. They are two halves of a closed loop that turns production experience into improved capability, and the speed and discipline with which you run that loop determine whether your agent stagnates or continuously earns the trust of the people who depend on it. The next challenge is ensuring that the improvements you make in test environments actually translate to production gains, which requires confronting the realism gap between how you simulate agent behavior and how agents behave in the wild.
