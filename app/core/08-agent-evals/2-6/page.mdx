# 2.6 — Self-Ask and Decomposition Patterns

In March 2024, a legal research startup called Precedent AI lost a major law firm client after their AI assistant failed to answer a complex multi-jurisdictional liability question. The query was straightforward enough: "Under what circumstances can a Delaware C-corp be held liable for environmental damages caused by a wholly-owned subsidiary operating in California?" The agent retrieved relevant case law, generated a confident three-paragraph answer, and completely missed the interaction between federal environmental statutes, state piercing-the-corporate-veil doctrines, and recent Supreme Court precedent on parent company liability. The answer wasn't just incomplete—it was dangerously wrong, the kind of wrong that could cost a client millions if followed. The law firm's associate who had used the tool caught the error during review, flagged it to her partners, and within a week the firm had canceled their contract. Precedent AI's engineering team discovered the problem wasn't with their retrieval system or their base model—it was that their agent treated every question as a single lookup-and-answer task, never breaking complex questions into the sub-questions that human legal researchers instinctively ask themselves.

You're building agents that need to handle questions that don't have simple answers. The kind of questions where the path to an answer requires answering other questions first, where you need to establish foundational facts before you can reason about higher-level implications, where a direct answer attempt will miss critical nuances that only become visible when you decompose the problem. This is where the Self-Ask pattern becomes essential—not as a curiosity from academic research papers, but as a production pattern for building agents that can tackle genuinely complex queries.

## What Self-Ask Actually Does

The Self-Ask pattern, introduced in a 2022 paper by researchers at Google and Carnegie Mellon, gives agents a deceptively simple capability: the ability to recognize when they need to answer a sub-question before they can answer the main question. When an agent encounters a complex query, instead of immediately attempting to generate an answer or retrieve relevant documents, it pauses and asks itself: "What do I need to know first?" It generates explicit sub-questions, answers each one sequentially, and then uses those answers to construct a response to the original question. This transforms complex reasoning into a structured process of incremental knowledge building.

The pattern looks like this in practice. Your agent receives the question "Is the CEO of the company that makes the iPhone older than the CEO of the company that makes the Galaxy phone?" A naive agent might try to retrieve information about "CEO ages iPhone Galaxy" and hope for the best. A Self-Ask agent recognizes the decomposition structure: it needs to first ask "Who makes the iPhone?" then "Who is the CEO of that company?" then "How old is that person?" then repeat for the Galaxy phone, then compare. Each sub-question gets answered before moving to the next, building a chain of intermediate results that leads to a final answer. The decomposition makes explicit what was implicit in the original question.

This isn't just prompt engineering theater. The decomposition fundamentally changes how the agent interacts with its knowledge sources. Instead of a single complex retrieval query that might miss key facts, you get a series of targeted queries that are individually simple and verifiable. Instead of asking the language model to hold multiple reasoning steps in a single generation, you externalize the intermediate steps, making the reasoning transparent and debuggable. Instead of hoping the model "figures out" the implicit structure, you make the structure explicit. This structural clarity is what enables reliable performance on complex queries.

The critical insight is that decomposition converts a hard problem into a series of easier problems. Human experts do this instinctively—legal researchers break questions into jurisdictional sub-questions, financial analysts break investment decisions into sequential risk assessments, medical diagnosticians break symptoms into body-system hypotheses. You're giving your agent the same capability, the ability to recognize when a question has hidden structure and to expose that structure before attempting an answer. This metacognitive ability—knowing when you don't know enough to answer directly—is what separates robust agents from brittle ones.

## Decomposition as Structure Discovery

The magic of Self-Ask isn't in the decomposition itself—it's in teaching the agent to recognize when decomposition is necessary and what decomposition structure makes sense. This is harder than it sounds. Not every question benefits from decomposition. "What is the capital of France?" doesn't need sub-questions; it's a direct factual lookup. But "What geopolitical factors led to Paris becoming France's capital?" absolutely does—you need to understand medieval French geography, the Capetian dynasty's power base, the centralization of royal authority, the relationship between political power and economic centers. The agent needs to develop intuition about question complexity.

You build this intuition through few-shot examples that demonstrate decomposition patterns. Show the agent examples where complex questions get broken down: multi-hop reasoning questions that require chaining facts, comparison questions that require establishing facts about each entity before comparing, causal questions that require establishing temporal sequences before inferring causation, counterfactual questions that require establishing what actually happened before reasoning about alternatives. The pattern library becomes the agent's reference for recognizing structural complexity. The more patterns you show, the better the agent gets at pattern matching new queries to appropriate decomposition strategies.

The decomposition process itself has a pattern. First, the agent identifies what type of question it's facing—is this a factual lookup, a comparison, a causal analysis, a prediction? Second, it maps that question type to a decomposition template—comparisons need parallel fact-gathering about each entity, causal analyses need temporal sequencing, predictions need establishing current state then extrapolating. Third, it generates specific sub-questions using that template. Fourth, it answers each sub-question in sequence, using previous answers to inform later sub-questions. Fifth, it synthesizes the sub-answers into a final response. Each step builds on the previous one.

The synthesis step is where many implementations fail. They collect sub-answers but don't effectively integrate them. The agent needs to be explicitly prompted to use its sub-answers as context when generating the final response, to cite which sub-answers support which parts of its conclusion, to identify when sub-answers conflict and resolve those conflicts. The final answer shouldn't just restate the sub-answers—it should use them as building blocks for a higher-level synthesis that addresses the original question directly. Good synthesis demonstrates understanding, not just aggregation.

Another failure mode is over-decomposition: breaking questions down beyond the point where decomposition helps. If you decompose "What is the population of Tokyo?" into "What is Tokyo?" and then "What does population mean?" you've added complexity without adding value. The agent needs guidance on granularity: decompose until sub-questions are answerable directly through retrieval or straightforward reasoning, but no further. This requires showing examples of good stopping points and bad ones, teaching the agent when it has reached atomic questions.

## Self-Ask Versus Chain-of-Thought

You need to understand the relationship between Self-Ask and chain-of-thought reasoning because they're often confused but serve different purposes. Chain-of-thought prompting asks the model to show its reasoning steps before giving an answer—"think step by step" is the canonical example. Self-Ask asks the model to generate explicit sub-questions and answer them. The difference is subtle but important, and understanding it helps you choose the right pattern for the right task.

Chain-of-thought keeps the reasoning internal to a single model generation. The model generates its reasoning in one pass, using its own generated tokens as context for subsequent tokens, but the entire reasoning chain is part of one language model call. This works well for problems where the reasoning structure is relatively straightforward and all the necessary knowledge is already in the model's parameters or the provided context. A math word problem is a perfect chain-of-thought task—the model needs to extract numbers, identify operations, perform calculations, but it doesn't need to look up external facts between reasoning steps.

Self-Ask externalizes the reasoning structure and explicitly allows for information retrieval between sub-questions. Each sub-question can trigger a separate retrieval operation, and the answer to one sub-question informs what the next sub-question should be. This makes Self-Ask more powerful for knowledge-intensive tasks where you can't assume the agent has all necessary information upfront. The legal liability question from our opening story needs Self-Ask, not just chain-of-thought, because answering each sub-question requires retrieving specific case law or statutes that inform the next question. The structure emerges from the data, not just from reasoning.

The practical implication is that you use chain-of-thought when the bottleneck is reasoning clarity and you use Self-Ask when the bottleneck is knowledge access structure. If your agent is making logical errors even when it has the right information, chain-of-thought helps by making the reasoning explicit. If your agent is missing information because it's not retrieving the right facts in the right order, Self-Ask helps by structuring the retrieval process. Often you want both—Self-Ask to structure the information gathering, chain-of-thought within each sub-question to ensure clear reasoning.

The other key difference is debuggability. Chain-of-thought gives you a reasoning trace, but it's still a single model generation—you can see what the model was thinking, but you can't easily intervene or correct mid-reasoning. Self-Ask gives you discrete decision points where you can inspect the sub-questions, verify the sub-answers, and even override incorrect intermediate results. This makes Self-Ask more suitable for high-stakes applications where you need auditability and human-in-the-loop correction opportunities.

Latency characteristics differ significantly. Chain-of-thought adds latency proportional to the length of the reasoning trace, but it's still one inference call. Self-Ask involves multiple inference calls—one for decomposition, one for each sub-question, one for synthesis. If you have five sub-questions, you might have seven total model calls. This means Self-Ask is slower unless you can parallelize independent sub-questions. The latency tradeoff is acceptable when accuracy matters more than speed, but it's a real consideration for production systems with strict latency budgets.

## When Decomposition Helps and When It Hurts

Decomposition isn't free. Each sub-question is an additional language model call, an additional retrieval operation, an additional point where errors can compound. You need to develop intuition about when the benefits outweigh the costs. Decomposition helps when questions have genuine structural complexity, when the sub-questions are individually simpler than the original question, when intermediate answers can be verified independently, when the decomposition reduces the retrieval search space, when the task requires combining information from multiple specialized sources.

Multi-hop reasoning is the canonical case where decomposition helps. Questions like "Where was the founder of the company that acquired Instagram born?" require chaining facts—identify the acquiring company, identify the founder, identify the birthplace. Attempting this as a single retrieval query often fails because no document contains all three facts in sequence. Decomposing into "Who acquired Instagram?", "Who founded that company?", "Where was that person born?" lets you retrieve targeted facts sequentially. Each retrieval is simple and likely to succeed; the complexity is in the chaining, which the agent handles explicitly.

Comparison questions benefit from parallel decomposition. "How does the revenue of Microsoft compare to the revenue of Apple?" becomes "What is Microsoft's revenue?" and "What is Apple's revenue?" answered in parallel, then synthesized. This is cleaner than trying to retrieve comparative information directly, especially when the comparison requires normalizing across different reporting periods or currency conversions. The decomposition lets you gather normalized facts independently and then perform the comparison with clean data.

Temporal reasoning questions benefit from sequential decomposition that mirrors the timeline. "What events led to the 2008 financial crisis?" becomes a sequence of "What happened to housing prices in 2005-2006?", "How were mortgage-backed securities structured?", "What triggered the initial subprime defaults?", "How did those defaults propagate through the financial system?" Each question establishes context for the next, building a causal narrative. The sequential structure matches the causal structure, making the reasoning transparent.

But decomposition hurts when questions require holistic understanding that gets lost in fragmentation. Literary analysis questions like "What are the major themes in Moby Dick?" suffer from over-decomposition. Breaking this into "What is the first chapter about?", "What is the second chapter about?" misses the thematic threads that span the entire work. The question requires synthetic reading, not analytical decomposition. Similarly, questions that require weighing multiple factors simultaneously—"Should we invest in this startup?"—can be damaged by decomposition that forces sequential consideration of factors that should be weighed together.

Decomposition also hurts when the sub-questions are as hard as or harder than the original question. If your agent decomposes "Why did Rome fall?" into sub-questions like "What were the economic factors in Rome's decline?" and "What were the military factors in Rome's decline?", you haven't simplified the problem—you've just reframed it. Each sub-question is itself a complex historical question requiring extensive knowledge and interpretation. Effective decomposition creates sub-questions that are meaningfully simpler, that can be answered with more targeted retrieval or more straightforward reasoning.

Another failure mode is when decomposition creates artificial sequencing for questions that should be answered in parallel or holistically. If you decompose a request for a product recommendation into "What is the user's budget?" then "What features do they need?" then "What brands do they trust?", you've created an artificial sequence where parallel information gathering would be better. The agent should collect all the user preferences at once, not one at a time. The decomposition adds latency without adding value.

## Recursive Decomposition and Its Dangers

The natural extension of Self-Ask is recursive decomposition—when a sub-question itself requires decomposition, let the agent decompose it further. This sounds elegant in theory and is often a disaster in practice. Recursive decomposition without bounds leads to agents that fragment questions into increasingly granular sub-questions until they're answering things like "What is the definition of 'company'?" instead of solving the original problem. The agent disappears into a fractal abyss of sub-questions that never bottom out.

The issue is that language models don't have a natural stopping condition for decomposition. Given the instruction to decompose complex questions, they'll keep decomposing until you explicitly tell them to stop. You need to impose structural constraints: maximum decomposition depth, minimum complexity threshold for sub-questions, explicit signals that a question is "answerable directly" versus "needs decomposition." Without these constraints, you get runaway decomposition that wastes resources and obscures the original question.

A practical pattern is two-level decomposition. The agent can decompose the original question into sub-questions, and it can decompose those sub-questions one level deeper if needed, but it stops there. This gives you the benefits of structured reasoning without the runaway recursion. For the legal liability question from our opening story, first-level decomposition might give you "What is the corporate structure relationship between parent and subsidiary?", "What environmental damages occurred?", "What is the legal standard for parent company liability?" Second-level decomposition of the legal standard question might give you "What does Delaware corporate law say?", "What does California environmental law say?", "What is the federal standard?" But you don't decompose further—these are answerable directly through retrieval.

Another constraint is answer verification at each level. After the agent answers a sub-question, it should assess whether that answer is sufficient to proceed or whether it needs to decompose further. This gives you adaptive depth based on answer quality rather than arbitrary depth limits. If the agent asks "Who acquired Instagram?" and gets a clear answer "Facebook", it proceeds. If it gets an ambiguous answer or low-confidence retrieval results, it might decompose further: "What company did Instagram merge with in 2012?" This makes decomposition depth responsive to actual information needs rather than following a fixed template.

The risk you're managing is that decomposition becomes a way for the agent to avoid answering hard questions. Instead of engaging with complexity, the agent fragments it into pieces so small that the synthesis becomes impossible. You've seen this in human behavior—people who respond to difficult questions by asking endless clarifying questions, never actually taking a position. Your agent can do the same thing, hiding behind sub-questions instead of leveraging its knowledge to construct answers. The solution is to make synthesis mandatory and evaluable—the agent must produce a final answer that integrates its sub-answers, and you evaluate the quality of that synthesis, not just the sub-answers.

Cost control for recursive decomposition requires explicit budgets. Set a maximum number of total sub-questions across all levels—say, ten sub-questions for any single user query. This forces the agent to be selective about what to decompose and how deep to go. When the agent hits the budget, it must synthesize from what it has rather than continuing to decompose. This constraint prevents runaway costs while still allowing meaningful decomposition for genuinely complex questions.

## Production Patterns for Bounded Decomposition

Building production Self-Ask systems requires making decomposition tractable and cost-effective. Unconstrained decomposition can turn a single user query into dozens of language model calls and retrieval operations, blowing your latency and cost budgets. You need patterns that preserve the benefits while controlling the explosion. These patterns are what separate experimental implementations from production-ready systems.

First, implement a decomposition budget. Set a maximum number of sub-questions the agent can generate for any single query—typically three to five for first-level decomposition. This forces the agent to identify the most important sub-questions rather than exhaustively listing every possible thing it might need to know. The constraint actually improves decomposition quality because it pushes the agent toward higher-level, more meaningful sub-questions. You want the agent thinking "What are the three most critical things I need to establish?" not "What are all the possible facts I could gather?"

Second, use parallel execution wherever possible. If sub-questions are independent—like the CEO age comparison example—execute their retrieval and answering in parallel rather than sequentially. This cuts your latency from the sum of sub-question times to the maximum single sub-question time. Your orchestration layer needs to identify independence relationships and schedule parallel execution. Most agentic frameworks support this through async task execution or dependency graphs. The trick is teaching the agent to mark sub-questions as independent or dependent during decomposition.

Third, cache intermediate results aggressively. Sub-questions often overlap across different user queries. "Who is the CEO of Apple?" is a common sub-question for many technology company queries. Cache the answer with an appropriate TTL—maybe twenty-four hours for frequently-changing information like stock prices, maybe seven days for slower-changing information like CEO identities, maybe thirty days for historical facts. When the agent generates a sub-question, check the cache before executing retrieval and reasoning. This turns your decomposition pattern into a progressively smarter system that builds up a library of answered sub-questions over time.

Fourth, learn decomposition patterns from successful queries. Track which decomposition structures led to correct final answers and use those as few-shot examples for future queries. If your legal research agent successfully decomposed a multi-jurisdictional liability question and got a verified correct answer, save that decomposition pattern as a template for similar questions. Over time, you're building a library of proven decomposition strategies that the agent can pattern-match against new queries. This is like building a case law database, but for reasoning patterns rather than legal precedents.

Fifth, implement confidence-based decomposition triggers. Don't decompose every question—only decompose when the agent's initial confidence is below a threshold or when the question matches patterns that historically benefit from decomposition. Let the agent attempt a direct answer first, assess its confidence, and only trigger Self-Ask when needed. This keeps simple questions fast while reserving decomposition for genuinely complex queries. You might find that eighty percent of questions don't need decomposition, which means you save significant latency and cost by being selective.

The monitoring you need is different from standard agent monitoring. Track decomposition depth, sub-question count, sub-answer quality, synthesis quality, cache hit rate on sub-questions. Alert when decomposition depth exceeds thresholds—this indicates either genuinely complex questions or runaway recursion. Monitor the relationship between decomposition complexity and final answer correctness—you want to see that more complex decompositions lead to better answers, otherwise you're adding cost without value. Track which question types benefit most from decomposition to inform your triggering logic.

## The Synthesis Challenge

The hardest part of Self-Ask isn't decomposition—it's synthesis. You can train agents to generate reasonable sub-questions relatively easily through few-shot examples. Getting them to effectively combine sub-answers into coherent, correct final answers is much harder. Synthesis requires reasoning about how facts relate, resolving apparent contradictions, identifying which sub-answers are relevant to which parts of the final answer, and constructing a narrative that's more than just concatenation. This is where most Self-Ask implementations fail in production.

The failure mode you see constantly is collage answers—the agent just lists its sub-answers in sequence without integration. "The company that makes the iPhone is Apple. The CEO of Apple is Tim Cook. Tim Cook was born on November 1, 1960, making him 65 years old. The company that makes the Galaxy phone is Samsung. The CEO of Samsung is Han Jong-hee. Han Jong-hee was born on June 12, 1963, making him 62 years old. Therefore, the CEO of Apple is older than the CEO of Samsung." Technically correct, but it's just sub-answers stapled together, not synthesis. The user can see that you decomposed the question; they can't see that you understood it.

Good synthesis identifies the essential information from each sub-answer and constructs a direct response. "Yes, Tim Cook, CEO of Apple, which makes the iPhone, is 65 years old, making him older than Samsung CEO Han Jong-hee, who is 62." Same facts, but integrated into a coherent answer to the original question. The difference is that the agent had to identify which facts matter for the comparison and structure the response around that comparison rather than around the sub-question sequence. The synthesis demonstrates understanding of what the user actually wanted to know.

You improve synthesis through explicit synthesis prompts. After collecting sub-answers, prompt the agent specifically for synthesis: "Using the above sub-answers, provide a direct answer to the original question. Integrate relevant facts rather than listing sub-answers. Identify any contradictions and resolve them. Cite which sub-answers support your conclusion." This makes synthesis a distinct step with its own quality criteria. The agent knows it's being evaluated on integration quality, not just factual correctness.

Another technique is to show the agent examples of bad synthesis alongside good synthesis. Give it the same set of sub-answers and show both the collage answer and the integrated answer, with annotation about why the integrated version is better. "Bad: lists sub-answers sequentially. Good: integrates facts into a direct answer to the original question, citing only the relevant parts of sub-answers." This helps the model learn the distinction between reporting sub-answers and actually using them to construct new understanding.

The deepest synthesis challenge is handling sub-answer conflicts. When different sub-questions yield contradictory information, the agent needs conflict resolution strategies. Sometimes one source is more authoritative or more recent—if one sub-answer comes from an official company filing and another from a news article, trust the filing. Sometimes the contradiction is only apparent—the facts are consistent when you understand nuance. Sometimes the contradiction is real and the right answer is to flag uncertainty: "Sources disagree about X; according to source A it's Y, but source B says Z." Teaching agents these meta-reasoning skills is where Self-Ask transitions from a pattern to an architecture.

Source weighting is a critical synthesis capability. Not all sub-answers deserve equal weight in the final answer. If you asked "What is the current stock price of Apple?" as a sub-question and got answers from three different sources with slight variations, you should weight real-time data sources higher than cached or delayed sources. If you asked about a legal precedent and got answers from both primary sources—court rulings—and secondary sources—legal commentary—the primary sources should dominate the synthesis. Teaching the agent these source hierarchy principles requires showing examples with explicit reasoning about source authority.

## Self-Ask as Agent Foundation

The Self-Ask pattern isn't just a technique for improving question-answering accuracy—it's a foundation for building agents that can tackle open-ended tasks. Once you have agents that can decompose questions, you can extend that capability to decomposing goals, plans, and research agendas. The same machinery that breaks "complex question" into "series of sub-questions" can break "build market analysis" into "series of research sub-tasks." The abstraction is the same; only the domain changes.

The transition from Self-Ask for questions to Self-Ask for tasks requires generalizing the pattern. Instead of "what do I need to know to answer this question?", the agent asks "what do I need to do to accomplish this goal?" The decomposition structure is the same—identify necessary sub-tasks, execute them sequentially or in parallel, synthesize results—but the execution might involve actions beyond retrieval and generation. The agent might need to call APIs, run code, process data, not just look up facts. The decomposition becomes a planning layer that structures all agent behavior.

This is where agent orchestration patterns start to converge. Self-Ask gives you task decomposition. ReAct gives you the observe-act-reason loop. Together they give you agents that can break complex goals into sub-goals, take actions toward each sub-goal, observe outcomes, and integrate results. The Self-Ask pattern becomes the planning layer that structures what the ReAct loop executes. You might decompose a complex research task into five sub-tasks using Self-Ask, then use ReAct to execute each sub-task adaptively based on what you find.

What you're building toward is agents that have genuine problem-solving capability, not just question-answering capability. The legal research agent from our opening story should be able to handle not just "what is the liability standard?" but "research the liability risks for this corporate structure and draft a preliminary risk assessment memo." That requires decomposing the task into research sub-tasks—identify relevant jurisdictions, find applicable statutes, find relevant case law, identify risk factors—executing each sub-task through retrieval and synthesis, and then assembling the memo. It's Self-Ask all the way down, from the top-level task to the individual question each paragraph needs to answer.

The production readiness question is whether your decomposition and synthesis machinery is reliable enough to build on. If your Self-Ask implementation produces good sub-questions seventy percent of the time and good synthesis sixty percent of the time, your compound success rate for complex multi-level tasks drops rapidly. Seventy percent times sixty percent is forty-two percent—less than half your complex tasks succeed. You need each layer to be solid before you build the next layer. Start with reliable question decomposition, prove that it improves answer accuracy, then extend to task decomposition. The pattern scales, but only if the foundation is sound.

## Decomposition for Different Question Types

Different question types require different decomposition strategies, and teaching your agent to recognize and apply the right strategy is critical for production quality. Factual multi-hop questions follow a chain pattern: each sub-question builds on the previous answer. "Where did the founder of OpenAI go to college?" decomposes to "Who founded OpenAI?" then "Where did that person go to college?" The second question can't be formulated until you have the first answer, so execution must be sequential.

Comparison questions follow a parallel pattern: decompose into independent fact-gathering about each entity being compared, then synthesize the comparison. "Which is larger, the economy of Germany or the economy of Japan?" becomes "What is the GDP of Germany?" and "What is the GDP of Japan?" in parallel, then "Compare these values." The sub-questions are independent, so parallel execution reduces latency. The synthesis step performs the actual comparison.

Causal questions follow a temporal pattern: decompose into establishing what happened in chronological order, then reason about causal relationships. "What caused the rapid rise in GPU prices in 2024?" decomposes to "What was the GPU price trend in early 2024?", "What events happened in the AI industry in early 2024?", "What was the demand pattern for GPUs?", then "Which of these factors contributed to price increases?" The temporal sequencing matters because causation flows forward in time.

Definitional questions with context follow a clarification pattern: first establish the context, then get the definition within that context. "What is a transformer in the context of machine learning?" decomposes to "What field are we asking about?" then "What does transformer mean in that field?" This prevents the agent from returning definitions from the wrong domain—transformers in electrical engineering versus transformers in natural language processing.

Counterfactual questions follow an establishment-then-variation pattern: first establish what actually happened, then reason about the alternative. "What would have happened if GPT-4 had been released in 2020 instead of 2023?" decomposes to "What was the AI landscape in 2020?", "What was the AI landscape when GPT-4 actually released in 2023?", "What changed between 2020 and 2023?", then "Reason about how GPT-4 would have interacted with the 2020 landscape." You need the actual timeline before you can reason about the counterfactual.

Normative questions—questions asking what should be done—follow a constraint-gathering pattern: decompose into identifying relevant constraints, stakeholder interests, tradeoffs, then synthesize a recommendation. "Should we deploy this model to production?" becomes "What are the accuracy requirements?", "What are the latency requirements?", "What are the cost constraints?", "What are the risk tolerances?", then "Given these constraints, evaluate the deployment decision." The decomposition surfaces all the factors that should inform the decision.

## Teaching Decomposition Through Examples

The quality of your agent's decomposition depends heavily on the examples you show it. Generic instructions like "break complex questions into sub-questions" don't work—the agent needs concrete demonstrations of what good decomposition looks like for different question types. Your few-shot examples become the agent's reference library for recognizing and applying decomposition patterns.

A good example set includes the original question, the decomposition into sub-questions, why this decomposition makes sense, the sub-answers, and the final synthesis. For a multi-hop question: "Question: Where was the founder of Tesla born? Decomposition: 1) Who founded Tesla? 2) Where was that person born? Reasoning: This is a multi-hop factual question requiring chaining. Sub-answers: 1) Elon Musk, 2) Pretoria, South Africa. Final answer: Elon Musk, who founded Tesla, was born in Pretoria, South Africa." The annotation explains the pattern, making it generalizable.

You should also show failure cases: decompositions that don't help or make things worse. "Bad decomposition for 'What is 2 plus 2?': 1) What is 2? 2) What does plus mean? 3) What does 2 mean when it appears twice? This over-decomposes a simple question. Good: No decomposition needed, answer directly: 4." This teaches the agent to recognize when decomposition adds complexity without adding value.

Show edge cases where the obvious decomposition doesn't work. "Question: How many countries are in Africa? Obvious decomposition: List all countries in Africa, then count them. Problem: This decomposition doesn't help because listing is as hard as counting. Better: Retrieve 'number of countries in Africa' directly." This teaches the agent to evaluate whether decomposition actually simplifies the problem.

Show examples where multiple decomposition strategies could work but one is better. "Question: Compare the populations of the ten largest cities in the world. Decomposition A: 1) What are the ten largest cities? 2) What is the population of each? Decomposition B: 1) What are the ten largest cities by population? The second decomposition is better because it combines ranking and fact-gathering in one step, reducing total sub-questions." This teaches optimization, not just correctness.

## When Self-Ask Becomes Essential

Self-Ask transitions from optional optimization to essential architecture when your agent's task complexity exceeds what can be handled in a single reasoning step. If users ask questions where the answer depends on information that can only be obtained by first answering other questions, direct answering fails and decomposition becomes necessary. This is common in research domains, legal analysis, financial modeling, medical diagnosis, complex technical support, and anywhere domain expertise requires building understanding incrementally.

The pattern becomes essential when you need auditability and explainability. Showing users a decomposition tree—"To answer your question, I first determined X by asking sub-question A, then determined Y by asking sub-question B, then combined X and Y to conclude Z"—is far more transparent than showing a single complex reasoning trace. Regulated industries often require this level of explainability. Self-Ask gives you a natural audit trail.

The pattern becomes essential when you need to enable human oversight at intermediate steps. If a domain expert needs to verify reasoning before committing to expensive actions, decomposition creates natural checkpoints. The agent can present sub-questions for approval, wait for sub-answer verification, then proceed to synthesis. This collaborative structure is difficult to achieve with monolithic reasoning.

The pattern becomes essential when different sub-questions require different specialized tools or knowledge sources. If answering a question requires looking up financial data, legal precedent, and technical specifications from three different systems, decomposition into sub-questions lets you route each sub-question to the appropriate tool. This is cleaner than trying to combine multiple tool calls in a single reasoning step.

Understanding when Self-Ask is essential versus when it's overkill is the key to building production systems that balance accuracy, cost, and latency appropriately. Simple questions should stay simple. Complex questions should get the full decomposition treatment. Your system needs the intelligence to tell the difference, which brings you to the next challenge in agent orchestration: building routing and planning layers that can decide which patterns to apply to which tasks.
