# 9.14 — Simulation Realism: Avoiding False Wins in Test Environments

In late 2024, a legal technology company built an agent to draft contract amendments based on client requests and existing contract terms. They spent five months developing a sophisticated test harness that simulated contract databases, mock client requests, and expected amendment language. Their agent scored 94% success in the test environment across 800 simulated scenarios. The product team was confident. They launched to a pilot group of 30 law firms. Within two weeks, 19 firms had suspended use. The agent was generating amendments that contradicted existing clauses, missing jurisdiction-specific requirements, and hallucinating section references that did not exist in the actual contracts. The engineering team was baffled—the exact same request types worked perfectly in testing. They investigated and found the problem: their test environment used clean, well-structured contract templates with consistent formatting and complete metadata. The real contracts their customers worked with were scanned PDFs with OCR errors, inconsistent section numbering, handwritten annotations, and clauses that referenced external documents not in the database. The agent had never seen messy real-world data during development. It had been trained and evaluated on an idealized simulation that bore little resemblance to production conditions. Every passing test was a false win.

The failure illustrates a problem that plagues agent development more severely than traditional software: the realism gap. Traditional software is deterministic and context-independent—if a function works on test data, it works on production data as long as the schema matches. Agents are probabilistic and context-dependent. Their behavior depends not just on the input schema but on the content, the formatting, the ambiguity, the presence of conflicting signals, and the statistical patterns they learned during training. If your test environment simplifies or sanitizes any of these factors, your agent will perform better in testing than in production, and you will not know until users tell you. Closing the realism gap is not optional polish. It is the difference between an eval suite that gives you confidence and one that gives you false confidence.

## The Five Dimensions of Simulation Realism

Realism is not a binary property. Your test environment can be realistic along some dimensions and unrealistic along others, and the dimensions that matter most depend on your agent's task and architecture. The five critical dimensions are data realism, interaction realism, infrastructure realism, temporal realism, and adversarial realism. Most teams focus exclusively on data realism and ignore the other four, which is why their agents pass tests and fail in production.

Data realism means your test data matches the statistical distribution, formatting quirks, noise level, and edge case frequency of production data. This is not the same as using production data directly—using production data in testing raises privacy and compliance concerns, and production data is often too large to iterate on quickly. Data realism means your test data is **representative**. If 15% of production contracts have OCR errors, 15% of your test contracts should have OCR errors. If 8% of production user queries contain typos, 8% of your test queries should contain typos. If 3% of production transactions have missing or null fields, 3% of your test transactions should have missing or null fields. Many teams build test datasets by hand-crafting ideal examples or by generating synthetic data from templates. Hand-crafted examples are always cleaner than reality. Synthetic data from templates inherits the template's structure and lacks the organic messiness of real user behavior. You achieve data realism by sampling production data, anonymizing or perturbing it to remove sensitive information, and using that anonymized sample as your test set, or by analyzing production data to extract statistical properties—distribution of field lengths, frequency of null values, prevalence of formatting variations—and generating synthetic data that matches those properties.

Interaction realism means your test interactions match the patterns, complexity, and ambiguity of production interactions. If your agent handles multi-turn conversations, your test interactions should include multi-turn conversations with context shifts, clarifications, and corrections, not just single-query single-response pairs. If your agent handles compound requests—"Book a flight to Seattle, rent a car, and find a hotel near the convention center"—your test interactions should include compound requests with dependencies and ambiguities, not just single-task requests. If users in production frequently rephrase queries when the agent misunderstands, your test interactions should include rephrasing and recovery patterns. Many teams test their agents with scripted interactions that follow a happy path: user asks, agent responds, task succeeds. Production interactions are rarely that clean. Users interrupt agents mid-response, provide incomplete information and expect the agent to ask follow-ups, change their mind halfway through a task, and express frustration when things go wrong. If your test interactions never include these patterns, you have no idea whether your agent handles them gracefully.

Infrastructure realism means your test environment uses the same infrastructure, latency profile, rate limits, and failure modes as production. If your production agent calls external APIs that have rate limits, timeout behavior, and occasional downtime, your test environment should simulate those constraints. If your production agent retrieves data from a vector database that can take 200 to 800 milliseconds depending on query complexity and load, your test environment should simulate that latency variance. If your production tool calls occasionally fail and require retries, your test environment should inject failures at a realistic rate. Many teams test against mock APIs that return instantly, never fail, and never hit rate limits. The agent learns to rely on perfect infrastructure. When it encounters real infrastructure—slow responses, transient errors, partial results—it does not have fallback strategies because it never needed them in testing. Infrastructure realism also includes scale realism. If your production agent will handle 10,000 concurrent sessions, you need to test at or near that concurrency to surface race conditions, resource contention, and throughput bottlenecks that do not appear in single-threaded test runs.

Temporal realism means your test environment reflects how data, user behavior, and external conditions change over time. Many agents operate on datasets that evolve—product catalogs get updated, policies change, user preferences drift. If your test environment uses a static snapshot of data from six months ago, your agent is not being tested against current reality. Temporal realism also means testing the agent's ability to handle delayed information, stale cache entries, and eventual consistency. If your agent retrieves user profile data that might be several minutes out of date, your test environment should simulate that staleness. If your agent depends on external content that updates hourly, your test environment should reflect update cycles and version skew. Temporal realism is often ignored because it is hard to operationalize—most test harnesses are designed for repeatability, which means static inputs and deterministic outputs. You introduce temporal realism by versioning your test datasets, running tests against multiple time slices, and injecting scenarios where the agent must reconcile conflicting information from different data sources with different freshness.

Adversarial realism means your test environment includes inputs deliberately designed to break your agent—malicious prompts, edge cases, boundary conditions, and rare but high-stakes scenarios. This is not the same as fuzzing or random noise injection. Adversarial realism is targeted exploration of known vulnerability classes: prompt injection attempts, requests that try to extract training data or system instructions, inputs that exploit tool misuse, queries designed to maximize hallucination risk, and scenarios that test policy boundaries. If your agent is supposed to refuse certain requests, adversarial realism means testing refusal behavior with creative variations and social engineering tactics. If your agent handles financial transactions, adversarial realism means testing double-spend attempts, currency mismatch exploits, and precision edge cases. Many teams write adversarial test cases only after something breaks in production. The teams that ship robust agents write adversarial cases **before** production, informed by threat models, security reviews, and lessons learned from other agents in their domain.

## Measuring and Closing the Realism Gap

You cannot manage what you do not measure. The realism gap is the difference between your agent's performance in testing and its performance in production on the same distribution of tasks. If your agent scores 91% success in testing and 74% success in production, your realism gap is 17 percentage points. A large realism gap means your test environment is giving you false confidence. A small realism gap means your tests are predictive. You measure the gap by running the same eval framework in both environments and comparing the results, but comparison is only useful if you can trace discrepancies to specific realism dimensions and fix them.

The first step is establishing a shared task set that runs in both test and production. You define a set of canonical evaluation tasks that represent core capabilities your agent must have. You run those tasks in your test environment using simulated data and infrastructure. You also run those tasks in production—either with real user traffic by sampling and labeling interactions, or with synthetic traffic that you inject into production and monitor. You compare success rates, failure modes, and reasoning traces between the two environments. If tasks that pass in testing fail in production, you analyze the failures to identify which realism dimension is responsible. If the production failures are caused by malformed input data, the gap is in data realism. If they are caused by timeout errors or rate limiting, the gap is in infrastructure realism. If they are caused by unexpected multi-turn interaction patterns, the gap is in interaction realism. You prioritize closing the gaps that cause the most divergence.

Closing data realism gaps usually means improving your synthetic data generation or expanding your test dataset with more production samples. If your synthetic data lacks OCR errors and production data is full of them, you add OCR error simulation to your data generator. If your test dataset has no examples of missing fields and production data frequently has missing fields, you sample production records with missing fields, anonymize them, and add them to your test set. If your test queries are grammatically perfect and production queries contain typos and slang, you analyze production query logs to extract common error patterns and inject them into test queries at the same frequency. Data realism improvements are often the easiest to implement because they do not require infrastructure changes—just better test data.

Closing interaction realism gaps means enriching your test cases with multi-turn dialogs, context shifts, ambiguity, and recovery patterns. If your production logs show that 30% of interactions involve clarification exchanges—user asks, agent requests more information, user provides it—you add clarification scenarios to your test suite at the same frequency. If users frequently change their mind or correct the agent mid-task, you add correction and revision scenarios. If users escalate to a human after the agent fails, you add escalation scenarios that test whether the agent hands off gracefully with sufficient context. Interaction realism often requires rethinking how you structure test cases. Many eval frameworks assume one input, one output, one judgment. Multi-turn interaction testing requires session-level evaluation where you assess the entire conversation, not just individual turns.

Closing infrastructure realism gaps means making your test environment behave more like production infrastructure. If production APIs are slow, you add latency injection to your test environment. If production APIs fail at a 2% rate, you configure your mock APIs to fail 2% of the time. If production databases can return partial results when under load, your test database should do the same. Infrastructure realism is harder to implement because it requires test harness sophistication—controllable fault injection, latency simulation, rate limit emulation—and because it makes tests slower and flakier. You balance realism and developer experience by running infrastructure-realistic tests as part of a pre-deployment validation suite rather than on every local code change, and by using percentile-based success criteria rather than requiring 100% pass rates. If your agent is designed to handle 95% of requests successfully in the presence of infrastructure hiccups, your infrastructure-realistic tests should pass if the agent hits 95%, not if it hits 100%.

Closing temporal and adversarial realism gaps requires ongoing investment. Temporal realism improves as you version your test datasets and run regression tests against multiple time slices. Adversarial realism improves as you expand your adversarial test case library based on security reviews, red team exercises, and production incidents. Both dimensions require maintenance—temporal test datasets need to be updated as production data evolves, and adversarial test cases need to be refreshed as new attack vectors are discovered. Many teams treat these as one-time efforts and wonder why their realism gap grows over time. Realism is not a static achievement. It is a discipline you maintain as your agent and its operating environment change.

## False Negatives and Overcorrection Risk

While most realism discussions focus on false positives—tests that pass but should fail—false negatives are also a risk. False negatives happen when your test environment is **more adversarial** than production, causing tests to fail on scenarios that rarely or never occur in real use. If you inject so much noise, so many infrastructure failures, and so many adversarial inputs that your test environment is harder than production, you will spend engineering time solving problems your users will never encounter, and you will reject agent designs that would work fine in practice.

Overcorrection happens when teams discover a realism gap and overcompensate. They find out that 8% of production queries have typos, so they inject typos into 40% of test queries to "be safe." They discover that production APIs occasionally time out, so they configure test APIs to time out 20% of the time, far more than production. They see one instance of a prompt injection attack and add 50 adversarial prompt injection cases to their eval suite. The test environment becomes a stress test rather than a realistic simulation, and the agent's test performance underestimates its production performance. This is less dangerous than false positives, but it is still wasteful. You spend resources hardening the agent against scenarios that are statistically insignificant, and you may make architectural trade-offs that optimize for adversarial resilience at the cost of user experience in common cases.

You avoid overcorrection by grounding your test environment design in **measured production distributions**. If 8% of production queries have typos, inject typos into 8% of test queries, not 40%. If production APIs time out 0.5% of the time, configure test APIs to time out 0.5% of the time, not 20%. If you have observed three prompt injection attempts in six months of production traffic, you do not need 50 prompt injection test cases—you need three to five carefully chosen cases that cover the variations you have seen and the variations you consider plausible based on threat modeling. Your test environment should be **as realistic as possible**, which means matching production difficulty, not exceeding it. The exception is adversarial testing, where you deliberately exceed production frequency for high-consequence low-probability events—security vulnerabilities, compliance violations, catastrophic failures. You test those more aggressively than their production frequency would suggest because the cost of missing one in production is unacceptable, but you test them in a **separate adversarial suite**, not mixed into your standard eval suite. This separation makes it clear which tests measure typical performance and which measure worst-case resilience.

## Staged Realism and Iterative Hardening

You do not need perfect realism from day one. Realism is expensive—realistic test data is harder to generate, realistic infrastructure is more complex to set up, realistic interactions require more sophisticated harnesses. You adopt a staged approach where you start with simplified test environments to validate core logic, then progressively increase realism as the agent matures. The key is knowing which stage you are in and not mistaking early-stage results for production-ready validation.

Stage one is **logic validation**. You test whether the agent can perform the task at all under ideal conditions—clean data, simple interactions, perfect infrastructure. This stage uses hand-crafted test cases and mock infrastructure. The goal is not to predict production performance but to confirm that the basic reasoning and tool usage patterns work. If your agent cannot pass logic validation, there is no point adding realism—you have deeper problems. Stage one is where you spend the first weeks or months of development, iterating rapidly on prompts, architectures, and tool designs without worrying about edge cases or noise.

Stage two is **distributional realism**. You introduce realistic data distributions, input noise, and interaction complexity, but you still use simplified infrastructure. The goal is to measure how well the agent handles the messiness of real user requests and data without the added complexity of infrastructure failures and scale issues. This is where you sample production data, analyze its statistical properties, and build test datasets that match those properties. This is where you expand single-turn test cases into multi-turn dialogs and add ambiguity and context shifts. Stage two is where most teams should spend the majority of their pre-launch eval effort, because this is where you discover whether your agent works on realistic inputs, not just idealized ones.

Stage three is **full realism**. You introduce infrastructure realism, temporal realism, and adversarial realism. You run tests against production-like infrastructure with realistic latency, failure rates, and scale. You test across multiple data snapshots to ensure temporal robustness. You run adversarial suites to stress-test security and policy boundaries. Stage three is where you validate production readiness. You should not launch until your agent performs acceptably in stage three testing, but you also should not wait until stage three to start testing—by then, if you discover fundamental capability gaps, it is too late to fix them without major rework.

The staged approach prevents two common failure modes: launching too early because you only ran stage one tests and mistook logic validation for production readiness, and never launching because you tried to achieve full realism before you had validated basic logic. You move through the stages as the agent matures, and you continue to run tests from earlier stages as regression checks. A passing stage one test suite confirms your agent still has basic capability. A passing stage two test suite confirms it handles realistic inputs. A passing stage three test suite confirms it is ready for production. You need all three.

## Realism Debt and Technical Debt

Just as codebases accumulate technical debt—shortcuts and compromises that make future work harder—eval suites accumulate realism debt. Realism debt is the gap between your test environment and production reality that you have not yet closed. Early in a project, high realism debt is acceptable. Late in a project or after launch, high realism debt is dangerous. It means your eval suite is not telling you what you need to know about production performance, and you are flying blind.

Realism debt compounds. If your test data is unrealistic and you build your agent to perform well on unrealistic data, your agent's architecture and prompts are now optimized for the wrong distribution. When you later introduce realistic data, the agent performs worse, and you have to rework components that you thought were finished. If your test interactions are single-turn and you build an agent that works well on single-turn interactions, introducing multi-turn testing later may reveal that your agent cannot track context across turns. You have to refactor session state management and prompt design. The longer you wait to close realism gaps, the more expensive the fixes become, because you have built more of the system on top of unrealistic assumptions.

You manage realism debt the same way you manage technical debt: by tracking it explicitly, prioritizing it against other work, and paying it down before it becomes unmanageable. Every known divergence between your test environment and production is a realism debt item. You log it, estimate the impact—how much it is likely to inflate your test performance relative to production—and decide when to address it. High-impact realism debt gets paid down before launch. Medium-impact realism debt gets paid down in the first months after launch as you observe production behavior and validate which gaps matter most. Low-impact realism debt may never get paid down if it turns out the divergence does not affect outcomes. The teams that ship robust agents are not the ones with zero realism debt—they are the ones who know what their realism debt is, understand the risk it creates, and address it before it causes production failures.

The hardest part of simulation realism is accepting that you will never fully close the gap. Production is infinitely more complex than any test environment you can build. There will always be edge cases you did not anticipate, interaction patterns you did not simulate, and infrastructure behaviors you did not model. The goal is not perfection. The goal is to make your test environment realistic enough that agent performance in testing is a **useful predictor** of agent performance in production, so that when you ship, you are confident rather than hopeful. That confidence comes from disciplined investment in the five realism dimensions, staged hardening as your agent matures, and continuous comparison of test and production results to validate that your eval suite still reflects the reality your users experience. The final discipline in building robust agent testing is ensuring you have a comprehensive library of scenarios that stress-test the boundaries of your agent's capabilities, particularly the adversarial and long-tail cases that rarely occur but define whether your agent is trustworthy or fragile.
