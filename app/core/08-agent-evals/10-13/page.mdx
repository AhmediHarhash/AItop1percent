# 10.13 — Agent Caching Strategies: Prompt, Semantic, and Result Caching

In mid-2025, a legal technology company running an AI agent for contract analysis hit a wall at scale. The agent processed vendor agreements for enterprise customers, extracting obligations, indemnification clauses, and liability caps. Each contract analysis cost eighteen dollars in API calls. The team had optimized the agent itself—better prompts, tighter tool selection, faster retrieval—but the cost per run stayed stubbornly high. When a Fortune 500 customer uploaded 4,000 contracts in a single batch, the bill came to seventy-two thousand dollars. The customer paid it once, then demanded the company cut costs by half or lose the renewal. The engineering team had two months.

The root cause was repetition at three levels. First, every agent run resent the same 8,000-token system prompt defining contract law concepts, output schemas, and chain-of-thought instructions. Second, the agent re-analyzed semantically identical clauses across contracts—standard force majeure language appeared in 80% of vendor agreements, yet the agent reprocessed it every time. Third, the agent recomputed identical tool outputs: the same retrieval queries over the same legal knowledge base returned the same precedents, but the team paid for fresh embeddings and reranking on every invocation. The company had built a stateless agent in a stateful world. The solution was not smarter prompting. It was caching at every layer where repetition occurred.

## Why Caching Matters More for Agents Than for Single-Shot LLM Calls

Single-shot LLM applications—chatbots, summarizers, one-off classifiers—often process unique inputs and produce unique outputs. Caching helps, but the hit rate stays low because every user query is different. Agents are different. Agents execute repetitive workflows: the same system instructions, the same tool definitions, the same retrieval corpora, the same reasoning templates. Even when the user's request changes, large portions of the agent's execution trace remain identical. A customer support agent uses the same knowledge base, the same escalation logic, the same tone guidelines across thousands of tickets. A code review agent uses the same linting rules, the same style guide, the same repository structure across every pull request. The system prompt alone can be 10,000 tokens, resent on every invocation.

Caching converts this repetition into cost savings and latency reduction. When you cache the system prompt, you pay once to process it, then reuse it across thousands of agent runs for pennies. When you cache tool outputs, you avoid recomputing expensive retrieval or database queries. When you cache semantic embeddings, you avoid re-encoding the same knowledge base paragraphs every time the agent searches. The legal tech company that faced the seventy-two-thousand-dollar invoice implemented three caching layers. Prompt caching saved 60% on input token costs. Semantic caching saved 40% on retrieval latency. Result caching saved 25% on redundant tool calls. Combined, they cut cost per contract from eighteen dollars to six dollars and fifty cents, well below the customer's threshold. The renewal closed. The agent scaled to twelve enterprise customers without hiring additional infrastructure engineers.

You need to understand three caching strategies and when to apply each. Prompt caching stores processed prefixes of the input context so the LLM provider does not reprocess them. Semantic caching stores embeddings or similarity hashes of inputs and returns cached outputs when a new input is sufficiently similar. Result caching stores the outputs of deterministic tool calls keyed by input parameters. All three operate at different layers of the agent stack. All three compound when used together. The legal tech company's 64% total cost reduction came from stacking all three, not from picking one. This subchapter teaches you how to design, implement, and measure each caching strategy, and how to combine them without introducing stale data or incorrect outputs.

## Prompt Caching: Reusing Processed System Instructions and Static Context

Prompt caching is the simplest and most universally applicable caching strategy. LLM APIs like Anthropic's Claude and OpenAI's GPT-4o process input tokens in order: system message, user message, prior turns, function definitions, retrieved documents. Processing tokens costs time and money. If the first 10,000 tokens are identical across requests—your agent's system prompt, tool schemas, and static knowledge—you are paying to reprocess the same content thousands of times. Prompt caching lets you mark a prefix of the input as cacheable. The provider processes it once, stores the internal representation, and reuses it on subsequent calls. You pay full price for the first request, then a reduced cache read price on subsequent requests. Anthropic charges 90% less for cached tokens. OpenAI's prompt caching reduces costs by 50-80% depending on cache hit rates.

The legal contract agent's system prompt was 8,200 tokens. It included definitions of contract law terms, examples of well-formed obligation extractions, chain-of-thought templates for analyzing indemnification clauses, and output schemas for structured JSON. Every contract analysis began with this prompt. Before caching, the company paid full input token rates on 8,200 tokens per contract. At 4,000 contracts per batch, that was 32.8 million input tokens. At three dollars per million input tokens for Claude Opus 4.5, the system prompt alone cost ninety-eight dollars per batch. After enabling prompt caching, the first contract in each batch paid full price, and the remaining 3,999 contracts paid the cache rate of thirty cents per million tokens. The cost dropped to three dollars and sixty cents per batch—a 96% reduction on that component.

You enable prompt caching by structuring your input so static content comes first and variable content comes last. The cache key is the exact byte sequence of the prefix. If you append user-specific data before the system prompt, the cache key changes and you get no hits. The legal tech team restructured their agent's input order: system prompt first, tool definitions second, retrieved legal precedents third, user contract text fourth. The first three blocks were identical across contracts from the same customer, so they cached together. The user contract text varied, but it appeared after the cached prefix, so it did not invalidate the cache. This ordering change alone raised the cache hit rate from 22% to 94%.

Prompt caching works best when your agent has a large, stable system prompt and processes many requests in a short time window. If your agent handles one request per hour, the cache expires between requests and you gain nothing. If your agent handles one hundred requests per minute, every request after the first hits the cache. Cache TTLs vary by provider. Anthropic's prompt cache lasts five minutes of inactivity. OpenAI's lasts ten minutes. If your batch processing job runs for thirty minutes straight, the cache stays warm. If your agent handles sporadic requests with five-minute gaps, the cache expires and you pay full price every time. The legal tech company batched contract uploads into thirty-minute processing windows instead of processing contracts one by one as they arrived. This kept the cache warm and maximized hit rates.

You must also account for cache invalidation when your system prompt changes. Every time you update instructions, tool definitions, or static examples, the cache key changes and the next request pays full price. If you deploy prompt updates every hour, you lose most caching benefits. If you deploy updates once per day, you get thousands of cache hits before invalidation. The legal tech team adopted a prompt versioning policy: system prompt changes were batched into a single daily deploy at 2 AM UTC, between batch processing windows. This gave them twenty-three hours of stable caching per day. When they needed emergency fixes, they deployed immediately but accepted the one-time cache miss.

Prompt caching applies to all agent types: coding agents, customer support agents, data extraction agents, research agents. Any agent with a system prompt over 2,000 tokens and a request rate above ten per minute should use prompt caching. The cost savings are mechanical, not probabilistic. You are not guessing whether caching helps—you are eliminating redundant work the LLM provider was already doing. The only reasons not to use prompt caching are if your provider does not support it, or if your agent's system prompt is so small that the savings are negligible. For everyone else, this is free money.

## Semantic Caching: Deduplicating Similar Inputs Without Exact Matches

Prompt caching requires exact prefix matches. If a single character changes in your system prompt, the cache misses. Semantic caching is different. It caches outputs based on semantic similarity of inputs. When a new request comes in, you compute its embedding, search for similar past requests in a cache store, and if you find a match above a similarity threshold, you return the cached output instead of calling the LLM. This works when your agent handles variations of the same question: "What is the refund policy?" and "How do I get my money back?" are semantically identical, but prompt caching would not help because the character sequences differ. Semantic caching maps both to the same cached answer.

The legal contract agent used semantic caching for clause analysis. Standard force majeure clauses appear in thousands of contracts with minor wording variations: "acts of God, war, terrorism, or natural disaster" versus "war, natural disasters, acts of God, or terrorism." The meaning is identical, but the text is not. Without caching, the agent re-analyzed each variation. With semantic caching, the team embedded each clause using a 768-dimensional sentence transformer model, stored embeddings in a vector database with their corresponding agent outputs, and queried for matches with cosine similarity above 0.95. When the agent encountered a clause that matched a cached embedding, it returned the cached analysis instead of calling Claude Opus 4.5. The hit rate was 68% on force majeure clauses, 71% on indemnification clauses, and 54% on liability caps. This cut LLM calls by 60% on clause-level analysis tasks.

You implement semantic caching with five components: an embedding model, a similarity threshold, a vector store, a cache invalidation policy, and a verification mechanism. The embedding model must be fast and cheap. The legal tech team used a local SBERT model that processed 1,000 clauses per second on CPU. The similarity threshold controls precision versus recall. A threshold of 0.99 produces few false positives but low hit rates. A threshold of 0.90 produces high hit rates but occasional mismatches. The legal tech team started at 0.95, measured false positive rates, and tuned down to 0.93 after validating that mismatches were semantically harmless. The vector store needs millisecond-latency search. They used Pinecone for managed vector search, though Redis with vector extensions or in-memory FAISS also work.

Cache invalidation is critical. If your agent's behavior changes—updated instructions, new tool definitions, different reasoning templates—cached outputs become stale. The legal tech team versioned their cache by appending a version hash to the cache key. When they deployed a new agent version, the hash changed and the cache effectively reset. They also set a time-to-live of seven days on cached entries. If a clause had not been queried in seven days, it expired. This kept the cache size under control and ensured that rarely-seen clauses did not consume storage indefinitely.

Verification means auditing cache hits to ensure outputs remain correct. The legal tech team sampled 500 cache hits per day, compared them to fresh LLM outputs, and measured divergence. In the first week, they found a 2% divergence rate—cached outputs that were semantically close but factually different due to updated legal precedents. They lowered the similarity threshold from 0.95 to 0.93 and added a secondary check: if the cached output was more than thirty days old, recompute it. This reduced divergence to 0.3%, within acceptable bounds for contract review workflows where a human attorney reviewed all outputs anyway.

Semantic caching shines when your agent processes high-volume, low-diversity inputs: customer support tickets, product reviews, contract clauses, code snippets. It fails when every input is unique: creative writing prompts, personalized medical advice, novel research questions. The legal tech company also ran a semantic cache for their customer support agent, which handled vendor onboarding questions. Common questions like "How do I upload contracts?" and "What file formats are supported?" hit the cache 80% of the time. Rare questions like "Can I integrate with SAP Ariba?" missed the cache but represented only 5% of volume. The cache cut support agent costs by 74% while maintaining answer quality.

You should not use semantic caching when your agent's outputs must be deterministic and traceable. If you are building a financial compliance agent where every decision must be auditable, semantic caching introduces risk: you cannot prove that the cached output was generated from the exact input the auditor is reviewing. In those cases, stick to prompt caching and result caching, both of which are deterministic. Semantic caching is for high-volume, low-stakes, or human-reviewed workflows where approximate matches are acceptable.

## Result Caching: Storing Tool Outputs for Deterministic Functions

Result caching stores the outputs of tool calls keyed by input parameters. If your agent calls a retrieval tool with the query "indemnification clauses in vendor agreements," and you have already executed that query this hour, return the cached results instead of hitting the vector database again. This is simpler than semantic caching because the cache key is exact: same function name, same parameters, same output. It is also more reliable because the outputs are deterministic. If the retrieval corpus has not changed, the results will not change either.

The legal contract agent used result caching for three tools: legal precedent retrieval, clause type classification, and obligation extraction templates. The precedent retrieval tool queried a vector database of 50,000 legal cases. The same queries appeared across contracts: "force majeure case law," "indemnification limits," "liquidated damages." The team cached retrieval results keyed by query text and corpus version. When the agent called the tool with a cached query, it skipped the vector search and returned cached precedents in under ten milliseconds instead of 400 milliseconds. The hit rate was 52%, which eliminated 52% of vector database costs and improved agent latency by 30%.

Result caching requires careful invalidation. If the underlying data changes—new legal precedents added, case law updated, knowledge base reindexed—cached results become stale. The legal tech team versioned their retrieval corpus. Every time they added new cases, they incremented the version number and appended it to the cache key. Old cache entries expired after twenty-four hours. This ensured that agents always retrieved the latest precedents while still benefiting from caching within a processing batch.

You implement result caching with a key-value store: Redis, Memcached, or DynamoDB. The cache key is a hash of the function name and serialized parameters. The legal tech team used Redis with a five-minute TTL for retrieval caching and a one-hour TTL for classification caching. Retrieval results changed more frequently, so the shorter TTL reduced stale data risk. Classification results were stable, so the longer TTL maximized hit rates. You should also cache negative results—queries that returned no matches. The agent often searched for rare clause types that did not exist in the corpus. Caching the empty result prevented repeated expensive searches.

Result caching works for any deterministic tool: database queries, API calls, file system reads, computation-heavy functions. It does not work for nondeterministic tools: LLM calls, random sampling, real-time data feeds. The legal tech company also cached the output of their clause extraction template tool, which returned a structured schema for extracting obligations from a given clause type. The schema was deterministic given the clause type, so caching it saved redundant LLM calls. The hit rate was 91% because there were only fifteen clause types and every contract contained at least five of them.

You should monitor cache hit rates and eviction rates. A low hit rate means your cache is too small, your TTL is too short, or your workload is too diverse. A high eviction rate means your cache is undersized. The legal tech team started with a 2 GB Redis instance and hit a 40% eviction rate during peak batch processing. They scaled to 8 GB, which reduced evictions to under 5% and raised hit rates from 48% to 52%. The additional cost was twelve dollars per month. The cost savings from higher hit rates were 1,800 dollars per month. This is a 150x return on cache infrastructure spend.

Result caching compounds with prompt caching and semantic caching. Prompt caching reduces input token costs. Semantic caching reduces LLM call volume. Result caching reduces tool execution costs. The legal tech company's agent stack used all three. Prompt caching saved 60% on system prompt tokens. Semantic caching saved 68% on clause analysis LLM calls. Result caching saved 52% on retrieval tool costs. The combined effect was a 64% reduction in total cost per contract, from eighteen dollars to six dollars and fifty cents. That savings was not additive—it was multiplicative, because each layer reduced the workload for the layers below.

## Cache Coherence and Invalidation: Keeping Cached Data Fresh Without Breaking Agent Logic

Caching introduces staleness risk. If you cache an output and the underlying data changes, the cached output becomes incorrect. If your agent retrieves a product price from a cached API call, but the price changed five minutes ago, the agent gives the user outdated information. If your agent extracts obligations from a cached contract clause, but the legal team updated the clause interpretation rules last week, the cached extraction is wrong. Cache invalidation is not optional. It is the difference between a cost optimization and a correctness failure.

You need a versioning strategy for every cached layer. The legal tech company used semantic versioning for their agent: major version for breaking changes to system prompts or tool definitions, minor version for non-breaking updates, patch version for bug fixes. Every cache key included the major and minor version. When they deployed a minor version update, the cache reset for that version but older versions remained cached. This let them A/B test new agent versions without invalidating the entire cache. When they deployed a patch fix, the cache remained valid because patch fixes did not change behavior, only correctness.

Time-based expiration is the simplest invalidation policy but the least precise. You set a TTL on every cache entry: five minutes, one hour, twenty-four hours. When the TTL expires, the entry is evicted. This works when your data changes on a predictable schedule: daily knowledge base updates, hourly API refreshes. The legal tech company's precedent database updated once per day at midnight UTC. They set a twenty-four-hour TTL on retrieval caches, aligned to midnight. This ensured that every agent run used precedents from the current day without requiring manual invalidation.

Event-based invalidation is more precise but more complex. You listen for events that change underlying data—database writes, file uploads, API webhooks—and invalidate affected cache entries immediately. The legal tech company used event-based invalidation for contract uploads. When a customer uploaded a new contract, the system published an event to a message queue. A cache invalidation worker consumed the event and purged any cached clauses from older versions of that contract. This ensured that analysts always reviewed the latest contract text without waiting for TTL expiration.

You must also handle partial invalidation. If your agent caches the output of a multi-step workflow, and step three's logic changes, you need to invalidate caches for step three and all downstream steps, but not upstream steps. The legal tech company's agent had a five-step pipeline: clause extraction, clause classification, obligation analysis, risk scoring, report generation. When they updated the risk scoring model, they invalidated caches for risk scoring and report generation but left clause extraction and classification caches intact. This required tagging cache entries with dependency metadata: which step produced them, which steps consumed them. The invalidation worker walked the dependency graph and purged affected entries.

Cache coherence also matters in distributed systems. If you run ten agent workers behind a load balancer, all sharing the same cache, and one worker updates a cache entry, the others must see the update immediately. Redis and Memcached provide strong consistency for single-key operations, but if your cache logic involves read-modify-write patterns, you need locking or versioning. The legal tech company used Redis with optimistic locking: each cache entry had a version number, and workers checked the version before writing. If the version changed between read and write, the worker retried. This prevented race conditions where two workers cached conflicting outputs for the same input.

You should also monitor cache invalidation rates. If you invalidate 80% of your cache every hour, your effective hit rate is much lower than your measured hit rate, and you are paying for cache infrastructure with minimal benefit. The legal tech company tracked invalidation rates per cache type. Retrieval caches invalidated 8% per day. Clause analysis caches invalidated 15% per week. Prompt caches never invalidated except on deploys. This helped them tune TTLs and decide where to invest in more sophisticated invalidation logic.

## Measuring Cache Performance: Hit Rates, Cost Savings, and Latency Reduction

You cannot optimize what you do not measure. Every caching layer needs instrumentation: hit rate, miss rate, eviction rate, invalidation rate, cost saved, latency reduced. The legal tech company added cache performance metrics to their agent observability stack. They logged every cache lookup with the result: hit, miss, or stale. They logged every cache write with the size and TTL. They logged every invalidation event with the reason. They aggregated these logs into dashboards showing hit rates per cache type, cost savings per day, and latency reduction per request.

Hit rate is the percentage of lookups that returned a cached value. A 90% hit rate means nine out of ten requests avoided the expensive operation. The legal tech company's prompt cache hit rate was 94%. Their semantic cache hit rate was 68%. Their result cache hit rate was 52%. These numbers are not directly comparable because they apply to different layers. A 50% hit rate on expensive retrieval tool calls saves more money than a 95% hit rate on cheap classification calls. You need to weight hit rates by the cost and latency of the underlying operation.

Cost savings is the total dollars saved by cache hits. For each hit, calculate the cost of the operation you avoided, then sum across all hits. The legal tech company calculated prompt cache savings as the number of cache hits times the difference between full input token cost and cache read cost. For 4,000 contracts with a 94% hit rate on an 8,200-token prompt, that was 3,760 hits times ninety cents per hit equals 3,384 dollars saved per batch. They calculated semantic cache savings as the number of avoided LLM calls times the cost per call. For clause analysis, that was 2,720 avoided calls times twelve cents per call equals 326 dollars saved per batch. They calculated result cache savings as the number of avoided retrieval queries times the database query cost. For legal precedent retrieval, that was 2,080 avoided queries times four cents per query equals 83 dollars saved per batch. Total savings per batch was 3,793 dollars. Total cost before caching was 5,850 dollars per batch. Total cost after caching was 2,057 dollars per batch. That is a 64.8% reduction.

Latency reduction is the total time saved by cache hits. For each hit, calculate the latency of the operation you avoided, then sum across all hits. The legal tech company measured end-to-end agent latency per contract. Before caching, median latency was 47 seconds. After caching, median latency was 31 seconds. The 16-second reduction came almost entirely from result caching, which eliminated 400-millisecond vector searches. Prompt caching reduced LLM processing time slightly but not wall-clock latency because the LLM was already parallelizing token processing. Semantic caching reduced LLM call volume but each call was already fast due to prompt caching, so the latency win was modest. This taught them that cost savings and latency savings do not always correlate. Prompt caching saved the most money but reduced latency the least. Result caching saved the least money but reduced latency the most.

You should also measure cache overhead: the latency added by checking the cache, computing embeddings for semantic caching, and serializing results. The legal tech company found that their semantic cache added 35 milliseconds per request: 20 milliseconds for embedding computation and 15 milliseconds for vector search. On cache hits, this overhead was worth it because they saved a 3-second LLM call. On cache misses, it was pure overhead. At a 68% hit rate, the expected latency change was 0.68 times negative 3 seconds plus 0.32 times positive 35 milliseconds, which equals negative 2.03 seconds. The cache was worth it. But if the hit rate had been 20%, the expected latency change would have been positive, meaning the cache made the agent slower on average.

Cache performance degrades over time. As your workload changes, your hit rate drops. As your data grows, your cache eviction rate rises. As your cache fills, your lookup latency increases. The legal tech company set up alerts for cache hit rate drops below 50%, eviction rate above 10%, and lookup latency above 50 milliseconds. When an alert fired, they investigated. In one case, a hit rate drop from 68% to 34% was caused by a new customer uploading contracts with uncommon clause types. They expanded the cache size and added more training data for those clause types. In another case, an eviction rate spike from 5% to 22% was caused by a batch job that processed 10,000 contracts in one hour, overwhelming the cache. They throttled the batch job and staggered uploads over three hours.

You should also A/B test caching strategies. The legal tech company ran a two-week experiment: 50% of traffic used all three caching layers, 50% used only prompt caching. The all-layers group saved 64% on cost and reduced latency by 34%. The prompt-only group saved 60% on cost and reduced latency by 8%. The incremental benefit of semantic and result caching was only 4% on cost but 26% on latency. This informed their roadmap: they prioritized result caching improvements because latency mattered more to users than cost. They also tested different similarity thresholds for semantic caching: 0.90, 0.93, 0.95, 0.97. The 0.93 threshold maximized hit rate without increasing error rate, so they made it the default.

## Designing a Multi-Layer Caching Architecture for Production Agents

When you stack prompt caching, semantic caching, and result caching, you need a coherent architecture. Each layer operates at a different point in the request flow. Prompt caching happens at the LLM API boundary. Semantic caching happens before the LLM call, in the agent orchestration layer. Result caching happens at the tool execution layer. You need to design the flow so that each layer checks the cache in the right order and falls back gracefully on misses.

The legal tech company's agent flow worked like this. First, the orchestrator received a contract analysis request. Second, it checked the semantic cache: had the agent analyzed this exact contract before, or a semantically identical one? If yes, return the cached analysis and skip all downstream steps. If no, proceed. Third, the orchestrator constructed the LLM input: system prompt, tool definitions, retrieved precedents, contract text. Fourth, it called the LLM API with prompt caching enabled. The API checked its cache: had it processed this system prompt prefix before? If yes, reuse the processed prefix and charge the cache rate. If no, process the full input and cache the prefix for future requests. Fifth, the LLM generated tool calls. Sixth, the orchestrator executed each tool, first checking the result cache. If the tool call parameters matched a cached entry, return the cached result. If not, execute the tool and cache the result. Seventh, the orchestrator fed tool outputs back to the LLM. Eighth, the LLM generated the final analysis. Ninth, the orchestrator stored the final analysis in the semantic cache, keyed by the contract's embedding.

This flow has three cache lookups per request: semantic cache on entry, prompt cache at the LLM API, result cache per tool call. The semantic cache has the highest return: if it hits, you skip the entire agent execution and save 100% of the cost. The prompt cache has the second highest return: if it hits, you save 60% of input token costs. The result cache has the lowest per-call return, but it fires multiple times per request, so it adds up. The legal tech company measured the contribution of each layer to total cost savings: semantic cache contributed 48%, prompt cache contributed 35%, result cache contributed 17%. But all three were necessary to hit the 64% total reduction target.

You need to handle cache misses gracefully. If the semantic cache misses, the agent proceeds normally. If the prompt cache misses, the agent pays full price but still works. If the result cache misses, the agent executes the tool. There is no failure mode. The cache is purely additive: it makes the agent faster and cheaper when it hits, and invisible when it misses. The legal tech company's cache miss path was identical to their pre-caching path. This meant they could enable caching in production without risk. If the cache failed entirely—Redis went down, vector database crashed—the agent would slow down and cost more, but it would not break.

You should also design cache warming strategies. If your cache is empty, the first request pays full price. If you process 4,000 contracts in a batch, the first contract warms the cache, and the remaining 3,999 benefit. But if you process contracts one by one over eight hours, the cache expires between requests and you get no hits. The legal tech company used batch-aware cache warming. When a customer uploaded a batch of contracts, the system pre-warmed the prompt cache by sending a no-op request with just the system prompt and tool definitions. This cost two cents and ensured that all 4,000 contracts hit the cache. They also pre-warmed the semantic cache by embedding all uploaded contracts before processing them, so that duplicate contracts were detected immediately.

Cache partitioning is also important. If you run agents for multiple customers, you need to isolate their caches. Customer A's contract analysis should not hit the cache for customer B's contracts, even if the text is similar. The legal tech company partitioned caches by customer ID. Each cache key included a customer prefix: customer-123-clause-embedding-abc. This prevented cross-customer cache hits, which would have been a data leak. They also partitioned caches by agent version, so that A/B tests did not pollute each other's caches. When they ran experiments comparing Claude Opus 4.5 against GPT-4o, each model version had its own cache namespace. This isolation meant that switching models mid-experiment did not corrupt cached results or leak outputs from one model into another's cache space.

You should design your caching architecture with observability from the start. Every cache lookup, hit, miss, write, and invalidation should be logged. The legal tech company used structured logs with fields for cache type, key, hit status, latency, and cost saved. They ingested these logs into their observability platform and built dashboards showing hit rates, cost savings, and latency distributions per cache type. They also set up anomaly detection: if hit rates dropped below historical trends, an alert fired. This caught issues like cache eviction storms, corpus version mismatches, and TTL misconfigurations. One incident revealed that a batch job was uploading contracts with randomized clause ordering, which broke semantic cache hits even though the content was identical. They normalized clause ordering before embedding, which restored the 68% hit rate.

The observability infrastructure also enabled retroactive analysis. When the legal team updated a contract interpretation guideline in December 2025, they wanted to know which cached analyses were affected. The engineering team queried their logs for all cache hits involving indemnification clauses in the thirty days before the guideline change, found 12,000 potentially stale results, and reprocessed them with the updated guidelines. This audit took two hours and cost 800 dollars in reprocessing, but it prevented legal compliance issues that could have cost millions. Without structured logging, the audit would have been impossible. The team would have had to invalidate the entire cache or risk serving stale legal advice. Neither option was acceptable.

Cache sizing and capacity planning matter at scale. The legal tech company started with an 8 GB Redis instance and projected cache growth based on customer acquisition rates and contract upload patterns. They modeled three scenarios: conservative growth at 20% quarter-over-quarter, moderate growth at 40%, and aggressive growth at 80%. For each scenario, they calculated cache storage requirements, eviction rates, and infrastructure costs. Conservative growth kept the 8 GB instance viable for twelve months. Moderate growth required scaling to 16 GB by month six. Aggressive growth required 32 GB by month four and 64 GB by month eight. They provisioned for moderate growth and set alerts for when cache utilization exceeded 75%, which gave them two weeks of runway to scale up before hitting capacity.

They also designed for cache failure modes. What happens if Redis goes down? If the semantic cache is unavailable, does the agent fall back to always computing fresh results, or does it fail fast and reject requests? The legal tech company chose graceful degradation: if any cache layer fails, the agent bypasses it and runs without caching. This increased costs and latency temporarily but maintained availability. They instrumented cache bypass rates and set alerts for sustained bypass above 5%, which indicated a cache infrastructure problem. When Redis had a partial outage in October 2025, cache bypass rates spiked to 60% for seventeen minutes. The agent continued processing contracts, but cost per contract increased from six dollars and fifty cents to fourteen dollars during the outage. The team replayed the incident logs, identified Redis connection pool exhaustion as the root cause, scaled the connection pool from 50 to 200, and eliminated the failure mode.

The multi-layer caching architecture the legal tech company built in two months became their primary cost optimization lever. When they onboarded new customers, they scaled the cache infrastructure first, then the agent workers. When they launched new agent features, they designed them cache-friendly from the start: deterministic tools, stable system prompts, versioned corpora. When they optimized for cost, they tuned cache thresholds and TTLs before touching the agent logic. Caching was not an afterthought. It was a first-class architectural component, co-designed with the agent itself. That is the approach you need to take if you want to run agents at scale without burning cash or sacrificing latency.

## Caching Edge Cases: Multi-Tenancy, PII, and Compliance Constraints

Caching becomes more complex when you operate in regulated industries or multi-tenant environments. The legal tech company faced three constraints that shaped their caching architecture: customer data isolation, personally identifiable information handling, and audit trail requirements. Each constraint ruled out naive caching strategies and forced architectural decisions that balanced performance with compliance.

Customer data isolation was non-negotiable. In a multi-tenant SaaS platform, one customer's contracts must never appear in another customer's cache results. The obvious solution—partitioning caches by customer ID—works until you consider cross-customer deduplication. The legal tech company observed that many customers used the same standard vendor agreement templates from Microsoft, Oracle, and SAP. These templates appeared in thousands of contracts across hundreds of customers. Caching them per customer meant duplicating identical embeddings and analysis results hundreds of times. Caching them globally meant risking cross-customer leaks if the partitioning logic had a bug.

They solved this with a two-tier cache: a customer-scoped cache for contract-specific analysis and a global cache for standard clause templates. The global cache stored only publicly available clause templates extracted from open-source contract repositories and vendor-published standard forms. Every global cache entry was reviewed by the legal team before publication to ensure it contained no customer-specific information. The customer-scoped cache stored everything else. This hybrid architecture reduced storage costs by 40% compared to pure customer-scoped caching while maintaining perfect data isolation. The tradeoff was operational complexity: the team maintained two cache invalidation policies, two observability dashboards, and two access control systems.

Personally identifiable information introduced a second constraint. Contracts often contained party names, addresses, contact information, and financial terms. Caching clause text meant caching PII. Under GDPR and CCPA, the legal tech company had to support right-to-erasure requests: if a user requested deletion of their data, the company had to purge it from all systems, including caches. The naive approach—flushing the entire cache on every deletion request—would destroy cache hit rates. The surgical approach—tracking which cache entries contained which user's PII and deleting only those entries—required indexing cache entries by user ID, which added storage overhead and query complexity.

They implemented a compromise. Every cache entry included metadata tags indicating whether it contained PII and which user IDs appeared in it. Deletion requests triggered a background job that queried the cache for entries tagged with the deleted user ID and purged them. Non-PII cache entries—clause type classifications, obligation extraction templates, standard legal precedents—were tagged as PII-free and exempted from deletion. This reduced the blast radius of deletion requests. When a user requested deletion, the system purged an average of twelve cache entries instead of flushing the entire cache. The PII tagging added three percent storage overhead and five milliseconds per cache write, but it was worth it to maintain compliance without sacrificing performance.

Audit trail requirements introduced a third constraint. The legal tech company's enterprise customers required audit logs showing exactly which data informed every agent decision. If the agent recommended authorizing a 50 million dollar contract based on cached analysis of an indemnification clause, the audit log had to show when that analysis was generated, which model version produced it, and what inputs it was based on. This ruled out opaque caching strategies where cache keys were hashed and cache values were black boxes.

They implemented cache provenance tracking. Every cache entry included metadata: creation timestamp, agent version, model version, input hash, and a link to the full input and output stored in an immutable audit log. When the agent returned a cached result, it also returned the provenance metadata, which flowed through to the audit log. If an auditor questioned a contract authorization decision six months later, the team could trace it back to the specific cached clause analysis, retrieve the original input and output from the audit log, and rerun the analysis with the same model version to verify consistency. This made caches auditable, but it doubled cache storage requirements because every cache entry carried rich metadata.

These compliance constraints pushed the legal tech company's cache hit rate down from 68% to 61% and increased infrastructure costs by 15%. But the alternative—not caching at all due to compliance concerns—would have increased costs by 300%. Caching with compliance guardrails was still a massive win. If you operate in healthcare, finance, legal, or government sectors, you will face similar constraints. You need to design your caching architecture with compliance as a first-class requirement, not a bolt-on afterthought.

## When Not to Cache: Anti-Patterns and Failure Modes

Caching is not always the right optimization. In some scenarios, caching adds complexity, increases costs, or degrades correctness. You need to recognize when caching is the wrong tool and choose alternative optimizations.

Do not cache when your workload is too diverse. If every input is unique, cache hit rates will be near zero and you will pay cache infrastructure costs for no benefit. The legal tech company analyzed their customer support agent's workload before implementing semantic caching. They embedded 10,000 historical support tickets and measured pairwise similarity. Only 12% of tickets had a semantically similar match above 0.93 similarity. This suggested that semantic caching would achieve at most a 12% hit rate, which would not justify the infrastructure and operational costs. They skipped semantic caching for the support agent and optimized elsewhere: faster LLM models, prompt compression, and batched processing.

Do not cache when your data changes too frequently. If the underlying data updates every minute, cache TTLs must be under one minute, which means most cache entries expire before they are reused. The legal tech company considered caching real-time payer policy lookups for authorization requests. Payer policies updated multiple times per day as insurance companies adjusted coverage rules, drug formularies, and prior authorization requirements. A cached policy could become stale within hours. They measured policy update frequency: 200 updates per day across 50 payers, or four updates per day per payer. With 2,000 authorization requests per day and 50 payers, each payer was queried 40 times per day. The expected number of queries between policy updates was 40 divided by 4 equals 10. A cache with a six-hour TTL would achieve at most a 10% hit rate. They decided the complexity was not worth it and fetched policies in real time on every request.

Do not cache when correctness is more important than performance. If your agent makes high-stakes decisions where stale data could cause harm, cache invalidation bugs could lead to catastrophic failures. The legal tech company built a medical prior authorization agent that determined whether insurance would cover surgical procedures. A cached policy that was outdated by one day could lead to incorrect authorization, delayed surgery, and patient harm. The stakes were too high to risk cache invalidation bugs. They disabled caching entirely for policy lookups and absorbed the higher latency and cost. For that agent, correctness mattered more than cost.

Do not cache when cache infrastructure costs exceed savings. The legal tech company ran a cost analysis for their customer support agent. Semantic caching would require a 4 GB vector database for storing 500,000 embedded tickets, a 2 GB Redis instance for metadata, and ongoing maintenance labor. Total infrastructure cost was 400 dollars per month. Expected hit rate was 12%, which would save 120 LLM calls per day at ten cents per call, or 360 dollars per month. The infrastructure cost exceeded the savings. They skipped caching and instead optimized the prompts to reduce input token counts, which saved 500 dollars per month with zero infrastructure cost.

Do not cache when debugging and iteration speed matter more than cost. Early in development, when you are iterating on prompts, tools, and agent logic daily, caching adds friction. Cached outputs from yesterday's agent version pollute today's testing. You spend more time debugging cache invalidation bugs than you save from caching. The legal tech company disabled all caching layers during the first two months of agent development. Engineers could iterate freely without worrying about stale caches. Once the agent stabilized and entered production, they enabled caching. This sequencing—build first, optimize later—prevented premature optimization and let the team move fast during the high-uncertainty phase.

Caching is a powerful optimization, but it is not free. It adds operational complexity, infrastructure costs, and potential correctness risks. You need to measure the benefit—hit rate, cost savings, latency reduction—against the cost—infrastructure, maintenance, compliance overhead—and make an informed tradeoff. The legal tech company's 64% cost reduction from caching was not inevitable. It was the result of careful analysis, thoughtful architecture, and disciplined execution. If they had cached naively without measuring hit rates, they would have spent thousands on infrastructure for minimal benefit. If they had over-cached in compliance-sensitive areas, they would have violated regulations. If they had cached too early, they would have slowed down development. The discipline is knowing when to cache, what to cache, and how much to invest in caching infrastructure.

In the next subchapter, we will examine how to measure the return on investment of your agent by comparing the value delivered to users against the total cost incurred, including LLM calls, infrastructure, human review, and operational overhead.
