# 3.3 â€” Dynamic Re-Planning: Adapting When Plans Fail or Context Changes

In August 2025, a logistics optimization agent at a European freight company executed what should have been a routine rerouting task. A snowstorm had closed three mountain passes, and the agent needed to reroute 47 shipments through alternative corridors. The agent generated an excellent initial plan: reroute through southern routes, coordinate with alternative carriers, update delivery estimates. It began execution methodically, completing the first twelve reroutes successfully. Then the context shifted. A traffic accident closed the primary southern corridor. The agent detected the failure, noted the changed conditions, and faced a critical decision: should it scrap the entire plan and start over, or adapt incrementally? It chose to re-plan completely. It discarded all in-progress work, regenerated a full routing plan from scratch, and began execution again. Fifteen minutes later, another route closure. Another complete re-plan. By the time the agent had re-planned for the fourth time, it had burned through its daily reasoning budget and completed only 19 of the 47 reroutes. The remaining shipments were delayed by two days, costing the company 840 euros in customer compensation and delayed delivery fees. The company's logistics director reviewed the execution logs and found the core problem: the agent had never learned when to re-plan versus when to adapt locally. It treated every new piece of information as a reason to throw away all prior work and start fresh. The cost of this re-planning thrash was not just financial but strategic. Customers lost confidence. Operations teams lost trust in the agent. The technology was sound, but the architecture was missing a critical capability: disciplined, bounded re-planning that preserves progress while adapting to change.

When should an agent throw away its current plan and start over versus adapting locally? Get this decision wrong and you either waste resources re-planning constantly or waste resources executing bad plans. Dynamic re-planning is the ability of an agent to recognize when its current plan is no longer viable and to generate a new plan without losing all progress. This is distinct from error recovery, which deals with individual tool failures, and from plan repair, which makes local modifications to an existing plan. Re-planning is a more fundamental decision: the agent determines that its current plan structure is no longer appropriate for the changed context and that it needs to rethink its approach from a higher level. In production systems, the decision to re-plan is one of the most consequential choices an agent makes, because re-planning carries significant costs in computation, time, and potentially discarded work. An agent that re-plans too eagerly wastes resources and may never make meaningful progress. An agent that re-plans too reluctantly continues executing a flawed plan and produces poor outcomes. The art of dynamic re-planning lies in recognizing the right moments to re-plan, choosing the appropriate scope of re-planning, and preserving as much completed work as possible. This is not a theoretical concern. This is the difference between agents that handle real-world messiness gracefully and agents that collapse when reality does not match their initial assumptions.

This chapter addresses the hardest questions in re-planning: how does an agent recognize when its plan has failed catastrophically enough to warrant re-planning rather than local repair? How does it decide between full re-planning that starts from scratch and incremental re-planning that preserves viable portions of the current plan? How does it avoid the thrashing behavior where it spends more time re-planning than executing? And how does it preserve the progress it has made so that re-planning builds on completed work rather than discarding it? These questions have clear answers grounded in production experience from thousands of deployments across industries in 2024 and 2025. The patterns that work are known. The failure modes are documented. Your job is to implement these patterns in your agent architecture before your agent encounters its first dynamic environment.

## Recognizing When Plans Fail

The first challenge in dynamic re-planning is recognizing that re-planning is necessary. Not every setback requires a new plan. A single tool failure might be recoverable through retry with exponential backoff. An unexpected intermediate result might still lead to the desired outcome through a slightly different path. The agent needs to distinguish between minor deviations that can be absorbed within the existing plan structure and fundamental changes that invalidate the plan's core assumptions. This distinction requires the agent to understand its plan at a conceptual level, not just as a sequence of steps. It must know which steps are critical path, which assumptions underlie each step, and which intermediate results are load-bearing for downstream steps.

In production systems, re-planning triggers typically fall into several categories, each requiring different evaluation logic. Tool failures that block critical paths are the most common trigger. When an agent calls a tool and receives an error, it must decide whether this error is transient and retryable, whether it can be worked around using alternative tools, or whether it fundamentally blocks the current plan. A rate limit error from an API is usually retryable after a delay and does not require re-planning. The agent can wait thirty seconds and try again. A permission denied error might require escalation or alternative approaches but might not invalidate the overall plan structure. The agent might switch to a different data source or request elevated permissions. However, a tool that returns "resource not found" when the plan assumed that resource existed represents a violated precondition that often requires re-planning. If the plan was built around analyzing data from a specific database table and that table does not exist, retrying will not help and switching tools will not help. The plan's foundation is gone.

The sophistication of this decision-making separates production agents from prototypes. A naive agent re-plans on every error, wasting time and resources regenerating plans when simple retries would suffice. A prototype agent might give up on the first error, failing tasks that were actually recoverable. A production agent evaluates whether the error invalidates the plan's critical path or whether execution can continue through alternative routes. This evaluation requires the agent to maintain a mental model of plan dependencies. It must know that if step three depends on the output of step two, and step two failed, then step three is blocked. But if step three has no dependency on step two, the failure of step two does not block step three. The agent can skip step two and continue.

Unexpected results represent a more subtle re-planning trigger. The agent calls a tool expecting a certain category of response and receives something entirely different. A search tool expected to return multiple relevant documents returns zero results. A classification tool expected to categorize an item into one of five known categories returns a sixth category the plan did not account for. A calculation tool expected to return a value between zero and one hundred returns negative three thousand. A data extraction tool expected to find structured fields returns unstructured text. These unexpected results often signal that the agent's mental model of the problem space is incorrect. The plan was built on assumptions about the data's structure, the environment's state, or the task's scope, and those assumptions do not hold.

When the gap between expected and actual results is large enough, continuing with the existing plan becomes increasingly unlikely to succeed. The agent is navigating with a map that does not match the territory. A research agent planning to synthesize information from ten sources expected each source to provide data in a certain format. If the first source returns data in a completely different format, the agent must consider whether the remaining nine sources will have the same problem. If the format assumption is wrong, the entire analysis approach might be wrong. This is a re-planning trigger. Conversely, if the first source is simply less informative than expected but the format is correct, the agent can continue with the plan and compensate by gathering more sources. This is not a re-planning trigger.

New information arriving during execution creates a different re-planning dynamic. The agent begins with a certain understanding of the task, builds a plan based on that understanding, and then during execution learns something that changes the problem fundamentally. A customer service agent plans to resolve a refund request by processing a return, then during conversation learns the customer never received the item in the first place, which transforms the task from a return to a lost package investigation. A data analysis agent plans to process quarterly sales data, then discovers that the data format changed mid-quarter and the first six weeks use one schema while the last six weeks use another, requiring a completely different parsing strategy. A content generation agent plans to write a comparison of two products, then discovers during research that one of the products was discontinued last month, requiring a shift to a historical comparison or a substitution of a different competing product.

This category of re-planning trigger is particularly challenging because the new information is not an error or failure but rather an expansion or transformation of the problem space that the original plan did not account for. The plan was not wrong given what the agent knew when it planned. It is wrong given what the agent knows now. The agent must recognize that the new information is significant enough to change the approach, not just a detail to be incorporated into the existing approach. A heuristic that works in production systems: if the new information would have changed the high-level structure of the plan had it been known during planning, it is a re-planning trigger. If it would have only changed the details within the existing structure, it is not.

Constraint violations emerge when the agent's plan, while theoretically sound, cannot be executed within the practical limits of the system. The agent planned a sequence of twenty API calls but realizes after the fifteenth call that it will exceed its rate limit before completion. The agent planned a multi-step research task but realizes after consuming eighty percent of its token budget that it will not have enough budget left to produce the final output. The agent planned a set of file operations but realizes the total data size will exceed available memory. The agent planned a workflow that would take two hours but the task has a thirty-minute timeout. These violations often become apparent only during execution, not during planning, because the agent did not have sufficient visibility into resource consumption rates during the planning phase.

When a constraint violation makes plan completion impossible, re-planning becomes necessary to find an approach that fits within the available resources. This might mean reducing scope, using more efficient tools, parallelizing steps that were planned sequentially, or finding alternative approaches that trade quality for resource consumption. The re-planning is triggered not by external changes but by the agent's growing awareness of its own limitations. Production systems handle this by instrumenting resource consumption and checking it at phase boundaries. After each phase of execution, the agent evaluates remaining budget and compares it to the estimated cost of remaining phases. If the remaining budget is insufficient, re-planning is triggered to find a more efficient approach for the remaining work.

## The Cost of Re-Planning

Re-planning is expensive in ways that are not immediately obvious until you operate agents at scale. The most visible cost is the computation required to generate a new plan. The agent must invoke its reasoning model, potentially with extensive context about the current state, the original plan, the reasons for plan failure, and the constraints under which the new plan must operate. For complex tasks, this planning step can consume significant tokens and time. A re-planning operation for a multi-phase workflow might require five thousand tokens and twenty seconds of latency. If the task budget is fifteen thousand tokens total, a single re-plan consumes thirty percent of the budget. If the task must complete within two minutes, a twenty-second re-plan is significant overhead. Production systems must account for this re-planning overhead in their budget models, reserving a portion of the budget for potential re-planning rather than allocating all budget to initial planning and execution.

The less visible but often more significant cost is discarded work. When an agent re-plans, it often cannot reuse the results of actions already completed under the previous plan. A data processing agent that has filtered and transformed fifteen of twenty files might re-plan to use a different transformation strategy, rendering the fifteen completed transformations worthless. They were optimized for the original plan's downstream steps, which are no longer part of the new plan. A research agent that has gathered ten sources on a topic might re-plan to focus on a different aspect of the question, making the gathered sources irrelevant or only partially relevant. An agent building a report might have drafted three sections under the original plan, then re-plan with a different structure that does not include those sections. The discarded work represents both the computational cost of the original actions and the opportunity cost of time spent on a path that ultimately did not contribute to the solution.

In the logistics example that opened this chapter, each complete re-plan discarded the routing decisions already made, forcing the agent to reconsider shipments it had already successfully rerouted. The twelfth shipment was routed three separate times under three different plans, wasting computation and creating confusion in the operational logs. A human reviewing the logs saw conflicting routing decisions and had to manually verify which was the final valid decision. This is a downstream cost of re-planning that goes beyond the agent's direct resource consumption.

The risk of re-planning loops represents a third cost category. An agent re-plans, executes the new plan, encounters another setback, re-plans again, and enters a cycle where it spends more time planning than executing. This thrashing behavior is particularly common when re-planning triggers are too sensitive or when the problem space is genuinely unstable. An agent monitoring a rapidly changing system might observe new information every few seconds, each piece potentially justifying a re-plan, leading to continuous re-planning that never allows enough execution time to make progress. An agent operating in a high-failure environment might attempt a step, fail, re-plan, attempt the new plan's first step, fail, re-plan again, and never complete even a single phase of work.

Production systems need guardrails that prevent infinite re-planning loops. The most common guardrail is a hard limit on the number of re-planning iterations. After three re-plans, the agent must either complete with the current plan or fail the task entirely. This forces the agent to commit to a plan even in the face of continued uncertainty. Another guardrail is cooling-off periods that force the agent to execute for some minimum duration or number of steps before re-planning again. If the agent re-plans, it must execute the new plan for at least five steps or thirty seconds before it is allowed to re-plan again. This prevents rapid oscillation between planning and execution and ensures that each plan gets a fair chance to work before being abandoned. These mechanisms trade off adaptability for progress. An agent with a strict re-planning budget might persist with a suboptimal plan when a better approach exists, but it will at least make forward progress rather than thrashing indefinitely.

## Incremental Re-Planning Versus Full Re-Planning

The scope of re-planning is as important as the decision to re-plan. Full re-planning discards the entire current plan and generates a new one from scratch, treating the situation as a fresh problem with the current state as the starting point. Incremental re-planning, also called plan repair or local re-planning, identifies which portions of the current plan are still viable and regenerates only the portions that are affected by the new information or failure. The choice between these approaches represents a fundamental tradeoff between thoroughness and efficiency. Full re-planning is more thorough because it reconsiders all aspects of the approach with the new information in mind, but it is expensive and discards all planning work. Incremental re-planning is more efficient because it preserves the viable portions of the plan, but it risks missing opportunities for better global optimization now that the context has changed.

Full re-planning is appropriate when the changed circumstances invalidate the core strategy of the original plan. If the agent planned to gather data from Source A, analyze it with Tool B, and produce a report in Format C, but Source A is no longer accessible, the entire plan structure might need rethinking. Perhaps Source D has different data formats that require Tool E for analysis. Perhaps the lack of Source A data means the report scope must change to Format F that does not require the missing data. A full re-plan allows the agent to reconsider all aspects of the approach with the new constraints in mind. It can find a globally optimal solution to the new problem rather than trying to patch the old solution. This thoroughness comes at the cost of potentially discarding parts of the plan that were still viable and wasting the reasoning that went into their construction.

Incremental re-planning is appropriate when the changed circumstances affect a specific portion of the plan but leave the overall structure intact. The agent identifies which steps are downstream of the failure or new information and regenerates just those steps. If a plan consists of phases A, B, C, D, and E, and phase C fails, the agent might regenerate phases C, D, and E while preserving the work already completed in phases A and B. This approach is more efficient and avoids discarded work, but it requires the agent to have a sophisticated understanding of plan dependencies. It must correctly identify which downstream phases are actually affected by the change and which can safely remain unchanged.

A naive incremental re-planner might preserve phases that are no longer consistent with the new context, leading to execution failures later. For example, if phase C was supposed to produce a user ID that phase D would use, and the re-planned phase C produces a session token instead, phase D will fail when it tries to use the session token as a user ID. A sophisticated incremental re-planner must update not just the failed phase but all downstream phases that have dependencies on the failed phase's output. This dependency tracking is complex and error-prone. It requires the plan representation to explicitly encode dependencies, not just a sequence of steps.

In production systems, the most effective approach is often a hybrid: local re-planning with escalation to full re-planning. The agent first attempts to repair the plan locally, regenerating only the affected phases. If this local repair fails or if the agent determines that too much of the plan is affected, it escalates to a full re-plan. This escalation decision can be based on heuristics that balance the risk of partial re-planning with the cost of full re-planning. If more than fifty percent of the remaining phases are affected by the change, full re-planning becomes more efficient than incremental repair. If the failure occurred very early in plan execution, before much work has been completed, full re-planning has less downside because there is little completed work to discard. If the same local repair has failed multiple times, it suggests the plan structure itself is flawed and full re-planning is warranted.

Another heuristic is based on the severity of the change. Minor changes like a single tool failure or a missing optional data field trigger incremental re-planning. Major changes like a fundamentally different problem type or a hard constraint violation trigger full re-planning. Medium changes trigger incremental re-planning with close monitoring: the agent attempts local repair but checks after a few steps whether the repair is working. If the repaired plan continues to encounter problems, it escalates to full re-planning rather than persisting with a failing repair.

## Preserving Completed Work During Re-Planning

The key to efficient re-planning is preserving as much completed work as possible. When an agent re-plans, the new plan should ideally build on the current state rather than ignoring it. If the agent has already retrieved five documents, the new plan should incorporate those documents rather than re-retrieving them or discarding them. If the agent has already applied a transformation to a dataset, the new plan should use the transformed data as a starting point if the transformation is still relevant. If the agent has already validated certain assumptions or gathered certain facts, the new plan should treat those as givens rather than re-validating or re-gathering them. This requires the agent to have visibility into what work has been completed and to reason about whether that work remains useful under the new plan.

The challenge is that completed work often carries assumptions from the previous plan. A document retrieved under the old plan was retrieved because the old plan deemed it relevant to a specific question. If the new plan has a different focus, that document might or might not still be relevant. The agent must reason about this relevance explicitly. One approach is to tag completed work with its original purpose or justification, then during re-planning evaluate whether that purpose remains valid. A document tagged "retrieved for background on pricing models" might still be useful if the new plan still involves pricing analysis, even if the specific question has changed. A calculation tagged "computed average transaction value for Q3" might not be useful if the new plan has shifted to analyzing Q4 data instead. The agent evaluates the tag against the new plan's goals and decides whether to preserve or discard each piece of completed work.

State checkpointing provides a mechanism for preserving work across re-planning boundaries. The agent periodically saves its current state, including completed actions and their results, in a structured format. When re-planning occurs, the new plan can reference this state as given facts rather than actions to be repeated. The new plan might include a step "using the five documents already retrieved" rather than "retrieve relevant documents." This approach requires careful state management. The agent must track what state is available, ensure the state is consistent, and reason about how to incorporate it into new plans. State that is inconsistent with the new plan's assumptions must be either discarded or explicitly reconciled.

A concrete pattern used in production systems: the agent maintains a context object that accumulates verified facts, gathered resources, and completed transformations as execution proceeds. Each action that produces a reusable result adds to this context object with a description of what was done and why. When re-planning is triggered, the re-planning prompt includes the full context object, allowing the new plan to reference existing facts and resources. The prompt explicitly instructs the agent to reuse context where possible to avoid redundant work. The agent reasons about which context elements are still relevant to the new plan and incorporates them.

Partial result reuse is particularly valuable in data processing pipelines. An agent processing a large dataset through multiple transformation stages might fail at stage three. Rather than re-running stages one and two, the re-planned approach should start from the output of stage two if that output is still valid. This requires the agent to understand transformation dependencies and data lineage. If stage two's transformation was parameterized based on an assumption that turned out to be false, its output might not be valid as input to a revised stage three. The agent must reason about these dependencies during re-planning. A safe approach is to checkpoint the output of each stage along with metadata about the assumptions and parameters that produced it. During re-planning, the agent evaluates whether those assumptions still hold. If they do, the checkpointed output is reused. If not, the stage is re-executed with updated parameters.

## The Risk of Thrashing and How to Prevent It

Thrashing is the pathological case where an agent spends more time re-planning than executing. It observes a setback, re-plans, executes a few steps, observes another setback, re-plans again, and never makes significant progress toward the goal. This failure mode is common in environments with high uncertainty or rapid change, where the agent's mental model of the problem space is constantly being invalidated by new information. It is also common when re-planning triggers are too sensitive, causing the agent to re-plan in response to minor setbacks that could have been absorbed within the existing plan. The result is an agent that appears busy, generates extensive logs, consumes significant resources, but delivers minimal value because it never completes a meaningful workflow.

The logistics agent from the opening story exhibited classic thrashing behavior. Each route closure triggered a complete re-plan, even though most of the routing decisions were unaffected by the closure. A more sophisticated approach would have re-routed only the shipments directly impacted by each closure, preserving the routing decisions for unaffected shipments. Instead, the agent treated each closure as a reason to reconsider all 47 shipments from scratch, leading to wasted computation and minimal progress. The thrashing was not caused by the environment's instability alone. The environment was unstable, yes, with multiple route closures. But the thrashing was caused by the agent's inability to distinguish between changes that required global re-planning and changes that could be handled with local adaptation.

Production systems combat thrashing through several mechanisms, all focused on forcing the agent to commit to a plan long enough to make progress before abandoning it. Re-planning budgets limit the number of times an agent can re-plan for a single task. A common setting is three re-planning iterations maximum. After the third re-plan, the agent must either complete with the current plan or fail the task entirely with an explanation of why it could not succeed. This forces the agent to commit to a plan even in the face of uncertainty. An agent that has already re-planned three times knows it cannot re-plan again, so it must make the current plan work even if conditions are not ideal. This is analogous to a human who has revised a strategy multiple times and must now execute the current strategy with discipline because there is no more time for revision.

Cooling-off periods require the agent to execute for a minimum duration or number of steps before it is allowed to re-plan again. If the agent re-plans, it must execute the new plan for at least five steps or sixty seconds before re-planning is allowed again. This prevents rapid oscillation between planning and execution. Each plan gets a fair chance to work before being abandoned. The cooling-off period can be adaptive: the first re-plan has a short cooling-off period, but each subsequent re-plan has a longer cooling-off period, forcing the agent to commit more strongly with each iteration. This discourages thrashing while still allowing re-planning when truly necessary.

Re-planning hysteresis introduces different thresholds for triggering the first re-plan versus triggering subsequent re-plans. It might take a significant setback to trigger the first re-plan, such as three consecutive failures or a critical path blockage. But once the agent has re-planned once, the threshold for the second re-plan is even higher, requiring five consecutive failures or multiple critical path blockages. The threshold increases with each re-plan. This creates a bias toward commitment. The agent is allowed to re-plan when there is strong evidence the current plan is failing, but it requires increasingly strong evidence for each subsequent re-plan. This prevents the agent from abandoning plans prematurely while still allowing adaptation when the evidence is overwhelming.

Another anti-thrashing technique is to distinguish between reactive adaptation and full re-planning. Not every change requires discarding the plan. Many changes can be handled through reactive adaptation within the plan's structure. The agent encounters a tool failure, tries an alternative tool, and continues. This is not re-planning. This is execution-time adaptation. Re-planning is reserved for structural failures where the plan's high-level approach is no longer viable. By clearly separating reactive adaptation from re-planning, the system reduces the frequency of re-planning and the associated costs. Reactive adaptation is cheap and fast. Re-planning is expensive and slow. The agent should default to reactive adaptation and escalate to re-planning only when adaptation is insufficient.

## Production Patterns for Bounded Re-Planning

In production agent systems, re-planning must be bounded and predictable. An agent that re-plans unpredictably or without limits creates operational challenges: unpredictable latency, unpredictable costs, and unpredictable outcomes. Business stakeholders cannot rely on agents whose behavior varies wildly based on runtime conditions. Operations teams cannot budget for agents whose costs might be ten dollars or one hundred dollars depending on how many times they re-plan. Production patterns for bounded re-planning establish clear rules about when and how re-planning occurs, making the behavior predictable even when the environment is not.

The retry-repair-replan escalation ladder is a common pattern. When a tool fails, the agent first attempts retry with exponential backoff. A rate limit error triggers a retry after thirty seconds. If retry succeeds, execution continues without re-planning. If retry fails after three attempts, the agent attempts local repair, substituting an alternative tool or adjusting parameters. A failed search API call might be retried with a different search engine or different query parameters. If local repair succeeds, execution continues. Only if both retry and local repair fail does the agent escalate to re-planning. This ladder ensures that re-planning is a last resort, used only when cheaper alternatives have been exhausted. Most transient failures are resolved by retry. Most tool-specific failures are resolved by repair. Only fundamental plan failures require re-planning.

Budgeted re-planning allocates a portion of the overall task budget specifically for re-planning operations. If a task has a budget of ten thousand tokens, the agent might reserve two thousand tokens for potential re-planning. If re-planning is needed, it draws from this reserve. If the reserve is exhausted, no further re-planning is allowed, and the agent must either complete with the current plan or fail. This ensures that re-planning does not consume resources needed for execution and final output generation. The re-planning budget can be dynamic, adjusted based on task complexity or uncertainty. A task flagged as high-uncertainty might receive a larger re-planning budget, say thirty percent of total budget instead of twenty percent. A routine task might receive a smaller re-planning budget or none at all, forcing it to execute the initial plan without revision.

Checkpoint-based re-planning divides the plan into phases separated by checkpoints. At each checkpoint, the agent evaluates whether re-planning is needed before proceeding to the next phase. Re-planning is allowed only at checkpoints, not during phase execution. This prevents the agent from abandoning a plan mid-phase and provides natural points for state preservation. If phase one completes successfully, its results are checkpointed, and phase two begins. If phase two encounters a problem, the agent can re-plan phases two, three, and four while preserving the phase one checkpoint. The checkpointed state becomes the foundation for the new plan. This pattern is particularly effective for long-running tasks where the problem space might shift during execution. The checkpoints provide structure and predictability, and they ensure that at least some progress is preserved even if later phases require re-planning.

Confidence-gated re-planning makes the re-planning decision based on the agent's confidence in its current plan. The agent maintains a confidence score that reflects how well the plan is matching expectations. As execution proceeds, the confidence score updates based on whether intermediate results match predictions. If the agent expected a search to return ten results and it returns twelve, confidence remains high. If the agent expected ten results and it returns zero, confidence drops. If confidence drops below a threshold, say 0.5 on a scale of zero to one, re-planning is triggered. If confidence remains above 0.7, re-planning is suppressed even if minor setbacks occur. This approach requires the agent to have good calibration: its confidence scores must accurately reflect the likelihood of plan success. Poorly calibrated confidence leads to either excessive re-planning when confidence is too low or insufficient re-planning when confidence is too high despite evidence of plan failure. Calibration can be improved through training or through post-hoc adjustment based on observed success rates at different confidence levels.

The meta-planning pattern treats re-planning as a planned activity. The initial plan includes explicit decision points where re-planning might occur based on observed conditions. "After phase one, if retrieval returns fewer than three documents, re-plan the analysis approach. Otherwise, proceed with standard analysis." The plan anticipates its own potential failures and builds in structured re-planning opportunities. This makes re-planning predictable and bounded. The agent knows in advance where re-planning might occur and under what conditions. This pattern works well for tasks with known uncertainty points where the best approach depends on information that will only be available during execution. The plan encodes this uncertainty explicitly rather than assuming everything will go smoothly.

## Re-Planning in Multi-Agent Systems

When multiple agents coordinate on a shared task, re-planning becomes more complex because one agent's re-plan might invalidate another agent's plan. Agent A re-plans its approach and changes its output format. Agent B, which depends on Agent A's output, now has a plan that assumes the old format. Agent B's plan is invalidated by Agent A's re-plan, forcing Agent B to re-plan as well. This cascade of re-planning can propagate through the entire multi-agent system, creating widespread disruption. Production multi-agent systems need coordination mechanisms that manage re-planning in a way that minimizes cascades.

One approach is hierarchical re-planning. A coordinator agent owns the overall plan and is the only agent allowed to trigger re-planning. Individual worker agents can report failures or unexpected conditions to the coordinator, but they cannot re-plan on their own. The coordinator evaluates the report, decides whether re-planning is necessary, and if so, generates a new overall plan and distributes updated sub-plans to the workers. This centralizes re-planning authority and prevents uncoordinated re-planning cascades. The downside is that the coordinator becomes a bottleneck and must have sufficient context to make good re-planning decisions for all workers.

Another approach is contract-based re-planning. Agents declare their output contracts when they plan: what format they will produce, what guarantees they make. If an agent needs to re-plan in a way that changes its output contract, it must negotiate with dependent agents before proceeding. It asks: if I change my output format from X to Y, can you still proceed? Dependent agents evaluate whether they can adapt to the new contract. If they can, the re-plan proceeds. If they cannot, the re-planning agent must either find a different re-plan that preserves the contract or escalate to the coordinator for a global re-plan. This distributed approach allows local re-planning while preventing contract violations that would break dependencies.

The transaction-based re-planning pattern treats re-planning as an atomic operation across multiple agents. When Agent A determines it needs to re-plan, it proposes a new plan to all dependent agents. Each dependent agent evaluates whether it can work with the proposed changes. If all agents agree, the re-plan commits and all agents update their plans atomically. If any agent objects, the re-plan is rolled back and Agent A must either find an alternative re-plan or escalate to global coordination. This pattern ensures consistency across the multi-agent system but requires sophisticated negotiation protocols and can add significant overhead to re-planning operations.

## When to Give Up Instead of Re-Planning

Sometimes the right decision is not to re-plan but to give up. If the agent has re-planned multiple times and continues to encounter fundamental obstacles, the task might be infeasible given current resources or conditions. Continuing to re-plan wastes resources and delays the inevitable failure. Production systems need clear criteria for when to give up. A common criterion is the re-planning budget: after three re-plans, the agent must either succeed or fail. Another criterion is convergence: if the last three re-plans were all different and all failed, the agent is not converging on a viable solution and should give up. Another criterion is resource exhaustion: if the agent has consumed ninety percent of its token budget or time budget and still has not made significant progress, it should give up rather than attempting another re-plan that will likely fail due to insufficient remaining budget.

Giving up is not a failure of the agent. It is a recognition of reality. Some tasks are not solvable with current tools, current data, or current resources. An agent that gives up gracefully, explains why it could not succeed, and provides partial results or diagnostic information is more valuable than an agent that thrashes indefinitely or produces incorrect output because it forced a plan to completion despite overwhelming evidence it would not work. Production systems should treat clean failure as a success case, not a bug. The agent recognized its limitations, communicated them clearly, and did not waste resources on futile attempts.

The decision to give up should be accompanied by rich diagnostic information that helps human operators understand what went wrong and what would be needed to succeed. The agent should explain which assumptions failed, which constraints could not be satisfied, and what alternative approaches might work if different resources or data were available. This diagnostic information transforms a failed task into a learning opportunity for both the system and the humans operating it. In the logistics example, if the agent had given up after three re-plans and explained that the remaining shipments required routes that were all blocked by closures, the operations team could have escalated to manual routing or explored alternative carriers that the agent did not have access to. Instead, the agent continued thrashing, wasted budget, and provided no clear diagnosis of why it could not complete the task.

## Learning From Re-Planning Patterns

Production deployments reveal patterns in when and why agents re-plan, and these patterns provide valuable insights for improving both agent architecture and task design. An agent that consistently re-plans at the same phase of similar tasks indicates that the initial planning is systematically missing information that only becomes available during that phase. The fix is not to make re-planning more efficient but to improve initial planning by gathering that information earlier or by deferring detailed planning for that phase until the information is available. An agent that frequently re-plans due to resource exhaustion indicates that cost estimation during planning is systematically wrong. The fix is to improve cost models or to build more conservative initial plans that leave buffer for unexpected resource consumption.

Tracking re-planning frequency, cost, and success rate provides metrics that guide architectural improvements. If re-planning consumes more than twenty percent of total task budget on average, the system is spending too much time adapting and not enough time executing. This suggests that either planning quality needs improvement or the environment is too volatile for the current planning approach. If re-plans succeed less than fifty percent of the time, meaning they encounter the same failures as the original plan, the re-planning process is not effectively incorporating lessons from failures. This suggests that the re-planning prompts need better diagnostic information about why the original plan failed and what constraints the new plan must satisfy.

A mature agent system maintains a re-planning taxonomy that categorizes each re-plan by trigger type, scope, cost, and outcome. This taxonomy reveals which types of changes require full re-planning versus incremental adaptation, which phases of tasks are most prone to re-planning, and which task types have the highest re-planning rates. This data-driven understanding guides decisions about where to invest in better planning, where to add more flexibility to plans, and where to build specialized re-planning strategies for high-risk task phases. The logistics company from the opening story analyzed their re-planning logs after the incident and discovered that route closures during winter storms were a recurring trigger. They adjusted their agent architecture to build more robust initial routing plans during winter months, with built-in backup routes and pre-planned fallback options, reducing re-planning frequency by sixty percent in subsequent winter operations.

## Designing Plans That Anticipate Change

The best way to handle re-planning is to need it less often. Plans that anticipate likely changes and build in flexibility require less re-planning when the environment shifts. This does not mean building maximally flexible plans that handle every possible contingency. That approach leads to bloated, inefficient plans that waste resources preparing for changes that never occur. Instead, identify the most likely change scenarios based on historical data and task characteristics, and build targeted flexibility for those scenarios.

A routing agent operating during winter months should plan with the expectation that some routes might close due to weather. Rather than building a single optimal route and re-planning when it fails, build a primary route and one or two backup routes during initial planning. When the primary route closes, switch to backup without re-planning. This costs slightly more during planning but saves significantly during execution. A research agent working with external APIs should plan with the expectation that some sources might be unavailable. Rather than building a plan that depends on specific sources and re-planning when they fail, build a plan that specifies desired information types and maintains a ranked list of sources for each type. When the top source fails, move to the next source without re-planning.

Conditional plans include decision points where the agent evaluates conditions and chooses between pre-planned branches based on observed state. "If retrieval returns more than five high-quality documents, proceed with comprehensive analysis. If retrieval returns two to five documents, proceed with focused analysis. If retrieval returns fewer than two documents, escalate to human researcher." These decision points allow the agent to adapt to varying conditions without re-planning, because the adaptation is already planned. The tradeoff is that conditional planning requires more upfront reasoning to anticipate decision points and plan branches, but it reduces the need for expensive re-planning during execution.

Robust plans include explicit buffers and margins that make them resilient to minor variations in execution. A plan that allocates exactly enough budget for expected costs will require re-planning if any step costs slightly more than expected. A plan that allocates twenty percent buffer for unexpected costs can absorb variations without re-planning. A plan that schedules tasks with no slack time will require re-planning if any task takes longer than expected. A plan with time buffers between phases can absorb delays without restructuring. The art is calibrating buffer sizes: too little buffer and you re-plan frequently, too much buffer and you waste resources on over-provisioning. Historical data on task execution provides the empirical basis for setting appropriate buffers.

## The Role of Human Oversight in Re-Planning

For high-stakes tasks, re-planning decisions should not be fully autonomous. The agent should detect that re-planning is necessary, propose a new plan, and request human approval before proceeding. This approval gate ensures that re-planning decisions are reviewed by someone with broader context and authority to make tradeoff decisions. The human reviewer can evaluate whether the proposed re-plan is appropriate, whether giving up would be better than re-planning, or whether the task requirements should be revised rather than the plan.

The human-in-the-loop re-planning pattern is particularly important when re-planning might involve significant cost, might change the task scope, or might affect other dependent systems. A logistics agent proposing to re-route forty shipments and delay deliveries by two days should not make that decision autonomously. A content generation agent proposing to shift from a technical audience to a general audience should not make that decision autonomously. A data processing agent proposing to skip a validation phase to meet budget constraints should not make that decision autonomously. These are business decisions, not just technical decisions, and they require human judgment.

The agent's role in human-in-the-loop re-planning is to provide rich information that enables informed human decision-making. It should explain why re-planning is necessary, what alternatives it considered, what the proposed new plan will accomplish, what the expected costs and tradeoffs are, and what the risks are if re-planning is denied and execution continues with the current plan. This diagnostic richness transforms the human from a gatekeeper who simply approves or denies into an informed partner who can make nuanced decisions about how to proceed. In some cases, the human might approve the re-plan. In others, they might suggest modifications. In still others, they might decide to abort the task entirely or escalate to more senior decision-makers. The agent enables these decisions by providing transparency into its reasoning and clear presentation of options.

## Re-Planning Metrics and Observability

Measuring and monitoring re-planning behavior is essential for operating production agent systems at scale. You cannot improve what you do not measure, and re-planning is a high-impact behavior that warrants dedicated instrumentation. The metrics you track should answer several critical questions: how often are agents re-planning, what triggers re-planning most frequently, how expensive is re-planning in terms of time and tokens, and how successful are re-plans compared to original plans. These metrics guide both immediate operational responses and long-term architectural improvements.

Re-planning frequency is the foundational metric. Track how many times each task triggers re-planning and aggregate this across task types, users, and time periods. A sudden spike in re-planning frequency indicates environmental changes or degraded planning quality. A gradual increase over time might indicate model drift or changing user behavior. Baseline re-planning rates vary by domain: a research agent operating with unpredictable external data sources might re-plan on thirty percent of tasks, while a structured workflow agent operating with reliable internal systems might re-plan on only five percent of tasks. The absolute number matters less than trends and outliers. If your baseline is ten percent and it suddenly jumps to twenty-five percent, investigate immediately.

Re-planning cost tracking measures the computational and time overhead of re-planning operations. For each re-plan, log the tokens consumed, the wall-clock time spent, and the percentage of total task budget consumed. Track both per-incident costs and aggregate costs across all tasks. If re-planning consumes an average of thirty percent of task budgets, you are spending nearly a third of your resources on adaptation rather than execution. This might be necessary for highly uncertain tasks, but it might also indicate that initial planning is systematically poor and should be improved. Cost tracking also reveals when re-planning becomes prohibitively expensive. A re-plan that consumes eight thousand tokens when the remaining task budget is only three thousand tokens indicates that the agent will likely fail even after re-planning, suggesting that giving up might be more efficient than attempting the re-plan.

Re-planning trigger classification categorizes each re-plan by what caused it: tool failure, unexpected result, new information, constraint violation, or other. This taxonomy reveals which types of changes most commonly force re-planning. If tool failures trigger sixty percent of re-plans, focus on improving error handling and tool reliability. If unexpected results trigger sixty percent of re-plans, focus on improving the agent's mental model of the problem space during initial planning. If constraint violations trigger sixty percent of re-plans, focus on improving cost estimation and resource allocation during planning. The trigger classification transforms re-planning from a black box into a diagnosable system with specific failure modes that can be addressed systematically.

Re-planning success rate measures whether re-plans actually solve the problem or simply delay inevitable failure. Track whether tasks that re-plan ultimately succeed or fail, and compare this to the success rate of tasks that never re-plan. If tasks that re-plan succeed at the same rate as tasks that never re-plan, re-planning is effective. If tasks that re-plan succeed at significantly lower rates, re-planning might be a symptom of fundamentally intractable tasks, and the agent should learn to detect and escalate these tasks earlier rather than wasting resources on futile re-planning. If tasks that re-plan twice succeed less often than tasks that re-plan once, subsequent re-plans might not be adding value, and you should consider more aggressive limits on re-planning iterations.

Re-planning scope tracking distinguishes between full re-plans that regenerate the entire plan and incremental re-plans that modify only affected portions. Log which type of re-plan occurred and how much of the original plan was preserved versus discarded. This metric reveals whether your agents are appropriately scoping their re-planning or defaulting to expensive full re-plans when incremental repairs would suffice. If ninety percent of re-plans are full re-plans even when failures affect only specific phases, your agents might lack the sophistication to do incremental re-planning, or your plan representation might not encode dependencies clearly enough to support it. Conversely, if incremental re-plans frequently fail and escalate to full re-plans, your agents might be attempting incremental repairs in situations where full re-planning is actually needed.

Work preservation metrics track how much completed work is reused versus discarded during re-planning. For each re-plan, log how many prior actions remain relevant to the new plan versus how many become irrelevant. High work preservation rates indicate efficient re-planning that builds on progress. Low work preservation rates indicate wasteful re-planning that repeatedly discards completed work. If your agents consistently discard eighty percent of completed work during re-planning, either improve work preservation mechanisms or question whether re-planning is the right response to the triggering conditions. Perhaps reactive adaptation or escalation to humans would preserve more value than re-planning.

Alerting rules based on re-planning metrics provide real-time operational visibility into agent behavior. Alert when re-planning frequency exceeds thresholds, when re-planning costs spike, when re-planning success rates drop, or when individual tasks re-plan more than the configured maximum. These alerts enable rapid response to both systemic issues affecting many agents and individual tasks that are thrashing or stuck. A financial services company operating compliance review agents set alerts for any task that re-planned more than twice, triggering human review to determine whether the task should be aborted, whether additional resources should be allocated, or whether the task requirements were unclear. This prevented situations where agents burned through budgets on unrecoverable tasks.

Dashboard visualization of re-planning metrics makes patterns visible to both engineers and business stakeholders. Show re-planning frequency over time, breakdown by trigger type, cost distribution, and success rates. Overlay these with environmental factors like API availability, model changes, or traffic patterns to identify correlations. A logistics company discovered through dashboard analysis that re-planning spiked every Monday morning due to weekend route changes that their agents did not anticipate during Friday planning. They adjusted their planning logic to account for weekend volatility, reducing Monday re-planning by forty percent. These insights are only possible when re-planning behavior is instrumented, measured, and visualized systematically.

## Re-Planning as a Design Surface

The most sophisticated agent architectures treat re-planning not as a failure mode to be minimized but as a design surface to be optimized. These systems recognize that in complex, uncertain environments, no initial plan will be perfect, and the ability to adapt gracefully is as important as the ability to plan well. This perspective shifts the design question from "How do we avoid re-planning?" to "How do we make re-planning efficient, predictable, and value-adding?"

Design for re-planning starts with plan representation. Plans that are easy to re-plan have explicit structure, clear dependencies, and modular phases that can be modified independently. A plan represented as a flat sequence of steps is hard to re-plan because changes ripple unpredictably. A plan represented as a dependency graph with explicit inputs and outputs for each phase is easier to re-plan because you can identify precisely which phases are affected by a change and which can be preserved. A plan represented as a hierarchical strategy with high-level goals decomposed into tactical steps is even easier to re-plan because you can re-plan at the appropriate level of abstraction. If a tactical step fails, re-plan the tactics while preserving the strategy. If the strategy becomes invalid, re-plan the strategy while preserving any tactical work that remains relevant to the new strategy.

Design for re-planning includes explicit replanning hooks in the plan structure. At designated checkpoints, the plan includes logic for evaluating whether re-planning is needed and branching to a re-planning phase if conditions warrant it. These hooks make re-planning predictable and testable. You can simulate various failure scenarios and verify that the re-planning hooks trigger appropriately and produce valid new plans. Without explicit hooks, re-planning is purely reactive and difficult to reason about. You cannot predict when it will occur or whether it will work correctly until it happens in production. With explicit hooks, re-planning becomes part of the plan's normal control flow, as predictable as any other conditional branch.

Design for re-planning considers the cognitive load on the agent during re-planning. Generating a new plan requires reasoning about the current state, the original goals, the failures or changes that invalidated the original plan, the constraints on the new plan, and the available options. This is cognitively demanding, especially if the context is large. Reducing cognitive load improves re-planning quality and reduces token consumption. One technique is to summarize completed work before re-planning, distilling detailed execution logs into key facts and outcomes that remain relevant. Another technique is to scope re-planning prompts to focus only on the affected portions of the plan, providing full context for those portions but only summaries for unaffected portions. A third technique is to pre-compute alternative strategies during initial planning and store them as fallback options. When re-planning is triggered, the agent can evaluate pre-computed alternatives rather than generating entirely new plans from scratch, reducing both cognitive load and re-planning latency.

Design for re-planning also means designing evaluation frameworks that account for re-planning behavior. Traditional agent evaluation focuses on final outcomes: did the agent complete the task correctly? Advanced evaluation tracks the path taken: how many steps did the agent take, how many errors did it encounter, how long did it take? Re-planning-aware evaluation additionally tracks adaptation behavior: how many times did the agent re-plan, were the re-plans appropriate given the circumstances, did the re-plans improve outcomes compared to persisting with the original plan? These metrics distinguish between agents that succeed because they planned perfectly from the start and agents that succeed because they adapted effectively when conditions changed. Both are valuable, but they represent different capabilities and thrive in different environments. An agent that never re-plans might excel in stable, predictable environments and fail in volatile, uncertain ones. An agent that re-plans frequently might handle volatility well but waste resources in stable environments. Understanding these tradeoffs requires evaluation frameworks that measure adaptation as explicitly as they measure outcomes.

Finally, design for re-planning means designing organizations and processes that support effective re-planning. When an agent re-plans during a customer-facing task, who is responsible for reviewing the re-plan? When re-planning costs spike unexpectedly, who investigates and addresses the root cause? When re-planning patterns reveal systematic planning failures, who prioritizes the architectural improvements? These are not technical questions but organizational questions. The most sophisticated agent architectures in production at major companies in 2025 and 2026 include not just technical mechanisms for re-planning but operational playbooks for managing re-planning at scale, with clear ownership, escalation paths, and continuous improvement processes. Re-planning is not an implementation detail to be hidden inside the agent. It is a visible, measured, managed aspect of system behavior that requires ongoing attention from both engineering and operations teams.

Dynamic re-planning is essential for robust agent systems that operate in uncertain or changing environments. The ability to recognize when a plan is no longer viable, to generate a new plan efficiently, and to preserve completed work across re-planning boundaries separates production agents from brittle prototypes that collapse when reality deviates from their initial assumptions. But this capability must be bounded and controlled. An agent that re-plans too freely becomes unpredictable, expensive, and inefficient, thrashing between plans without making progress. An agent that never re-plans becomes inflexible and fragile, executing flawed plans to completion even when evidence clearly indicates failure. The balance lies in clear re-planning triggers that distinguish between minor setbacks and fundamental plan failures, scoped re-planning that preserves viable work where possible, hard limits that prevent thrashing, and the wisdom to give up when re-planning will not help. These are not theoretical niceties. These are the practical disciplines that determine whether your agent handles real-world messiness or becomes another case study in why agents are not ready for production. The patterns are known. The failure modes are documented. Implement them before your agent encounters its first unstable environment.

The next subchapter examines how agents plan within hard constraints, building plans that respect budget, time, and safety limits from the start rather than discovering constraint violations during execution and triggering expensive re-planning.
