# 7.2 â€” Approval Workflows: Pausing Agents for Human Review

In March 2025, a fintech startup called Ledger.ai lost forty-three thousand dollars in a single afternoon because their invoice payment agent did not pause for human review before executing a wire transfer. The agent had been trained to process vendor invoices automatically, matching purchase orders to invoices and initiating payments through their banking API. On that Tuesday, a vendor submitted a fraudulent invoice with inflated line items, the agent classified it as matching an existing purchase order due to similar product descriptions, and within eleven minutes the money was wired to an offshore account. The engineering team had discussed adding approval checkpoints but decided against it because they wanted to maintain the agent's promise of same-day payment processing. The forty-three thousand dollars bought them a very expensive lesson about when automation should pause and wait for human judgment.

You are building agents that will make decisions with real consequences. Some of those decisions should never execute without explicit human approval. The challenge is designing workflows where agents can pause gracefully, present enough context for humans to make informed decisions quickly, and then continue execution based on that approval or rejection. This is not about adding a simple confirmation button. This is about architecting state machines that can suspend mid-execution, serialize their context, wait for asynchronous human input that might take seconds or hours, and then resume exactly where they left off. The approval checkpoint pattern transforms your agent from a fully autonomous system into a semi-autonomous one that knows its own limits.

## The Approval Checkpoint Pattern

The fundamental pattern is deceptively simple. Your agent executes a series of actions, reaches a decision point that requires human review, proposes that action to a human reviewer, pauses execution while maintaining its complete state, waits for approval or rejection, and then either continues with the approved action or takes an alternative path based on the rejection. In practice, this pattern requires careful orchestration of four distinct components: the agent logic that identifies when to pause, the state persistence layer that freezes execution context, the approval interface that presents the decision to humans, and the resumption mechanism that continues execution after human input.

Consider an agent that processes customer refund requests. It receives a refund request, analyzes the purchase history, checks the return policy, calculates the refund amount, and then reaches a decision point. For refunds under fifty dollars, it proceeds automatically. For refunds between fifty and five hundred dollars, it pauses and requests approval from a customer service manager. For refunds over five hundred dollars, it pauses and requires approval from both a manager and a finance director. The agent does not simply log a message saying "approval needed" and continue running. It serializes its entire execution state including the customer data, the calculated refund amount, the reasoning for that amount, the relevant policy sections, and the conversation history. It writes this state to durable storage, sends notifications to the appropriate approvers, and then terminates its execution thread. When a human approves or rejects, a new execution thread starts, loads the serialized state, and continues from the exact checkpoint.

This is fundamentally different from a traditional application where a user fills out a form and clicks submit. The agent has already done significant work before reaching the checkpoint. It has gathered data, made intermediate decisions, possibly called external APIs, and built up context. All of that context must be preserved perfectly. If you lose any piece of state, the resumed execution will fail or make incorrect decisions. I have seen teams implement approval workflows where they only saved the final proposed action, thinking that was sufficient. When the agent resumed after approval, it tried to execute that action but did not have the intermediate data it needed, so it either crashed or re-executed all the prior steps, sometimes producing different results the second time because external data had changed.

## Designing Context-Rich Approval Interfaces

The human reviewer needs to see everything necessary to make an informed decision, but nothing extraneous that slows them down. This is a difficult balance. Show too little context and reviewers will approve things they should not because they cannot assess the risk. Show too much context and reviewers will spend minutes parsing through data for a decision that should take ten seconds. The best approval interfaces are ruthlessly focused on the decision at hand, presenting the proposed action clearly, showing the key data that led to that proposal, highlighting any risk factors or anomalies, and providing quick access to deeper context if the reviewer wants to investigate further.

For the refund processing agent, the approval interface should show the customer's name and account, the purchase being refunded and its date, the calculated refund amount with a breakdown of how that amount was determined, any flags such as multiple recent refund requests or account age less than thirty days, and the agent's confidence score in its recommendation. The reviewer should be able to approve or reject with a single click if everything looks normal. They should also be able to expand sections to see the full conversation transcript, the complete order history, or the specific policy clauses that apply. You are optimizing for the ninety percent case where approval takes ten seconds, while still supporting the ten percent case where reviewers need to dig deeper.

I worked with a healthcare company whose prior authorization agent showed reviewers a twenty-page report for every approval request. The report included the patient's complete medical history, every code from the billing system, the full text of the insurance policy, and screenshots of every document the agent had processed. Reviewers were overwhelmed. They started pattern-matching on superficial signals rather than actually reviewing the authorization logic. Approval times averaged eight minutes and the error rate where reviewers approved incorrect authorizations was fourteen percent. We redesigned the interface to show only the proposed procedure, the estimated cost, the reason the agent recommended approval or denial, and any conflicting information from the patient's history. Approval time dropped to ninety seconds and the error rate fell to three percent. The full detailed report was still available, just hidden behind a "show details" expandable section that reviewers used only when something looked suspicious.

## Handling Approvals and Rejections

When a reviewer approves, the agent continues execution with the approved action. This seems straightforward but requires careful handling of the state transition. You load the serialized state, verify that the external world has not changed in ways that invalidate the approval, execute the approved action, and continue the agent's workflow. The verification step is crucial. If the approval sat in a queue for six hours, the customer might have canceled their account, the vendor might have withdrawn the invoice, or the price might have changed. You need to re-validate key assumptions before proceeding. If validation fails, you should not silently proceed anyway. You should either pause again for a new approval based on the changed context, or automatically reject and notify the original approver that circumstances changed.

When a reviewer rejects, you need a well-defined fallback path. Rejection is not always a dead end. Sometimes rejection means "do not proceed with this specific action, but try an alternative." Sometimes it means "escalate to a higher level of approval." Sometimes it means "abort the entire workflow and notify the user." Your approval workflow design must specify what rejection means for each checkpoint. For the refund agent, rejection of a two hundred dollar refund might mean the agent offers a fifty dollar store credit instead, or it might mean the agent sends a message to the customer explaining that the refund does not qualify under their policy. The rejection should include a reason code or free-text explanation that the agent can use to determine the appropriate next action.

Some teams implement rejections as simple boolean flags. The reviewer clicks "reject" and the agent just stops. This creates terrible user experiences because the customer is left in limbo, not knowing whether their request was denied or is still being processed. Better implementations treat rejection as input that flows back into the agent's decision logic. The agent receives the rejection, parses any explanation or reason codes, and decides what to do next based on its programming. Maybe it tries a different approach. Maybe it notifies the customer. Maybe it escalates to a different human. The key is that rejection is not an exception that crashes the agent, it is a normal input that the agent handles gracefully.

## Timeout Handling and Default Actions

Human reviewers do not always respond promptly. Sometimes they are in meetings, sometimes they are on vacation, sometimes they simply miss the notification. Your approval workflow must define what happens when a request sits unapproved for too long. The timeout policy is a critical design decision that balances risk against throughput. You can auto-approve after the timeout, auto-reject after the timeout, escalate to a backup reviewer, or leave the request in pending state indefinitely. Each choice has different implications for your system's behavior and user experience.

Auto-approval after timeout is risky but maintains throughput. If the refund agent waits four hours for approval and receives no response, it might proceed with the refund automatically. This ensures customers get timely responses even when reviewers are unavailable. The risk is that you lose the safety benefit of the approval checkpoint. You are essentially saying "we wanted a human to review this, but if they do not, we will do it anyway." This only makes sense for decisions where the consequence of proceeding is acceptable. A seventy-five dollar refund that auto-approves after four hours might be fine. A five thousand dollar refund should not auto-approve.

Auto-rejection after timeout is safer but harms user experience. The customer waited hours for a decision and then received an automatic rejection, not because a human reviewed their case and decided against it, but because no one reviewed it at all. This feels unfair and generates complaints. I have seen customer service teams spend more time handling complaints about auto-rejected requests than it would have taken to just review the original requests. Auto-rejection makes sense only when the default safe state is to deny, such as security-sensitive operations where you would rather block legitimate requests than allow risky ones.

Escalation to backup reviewers is the most robust approach but requires more complex organizational design. If the primary reviewer does not respond within two hours, the system sends the approval request to a secondary reviewer. If the secondary does not respond within two more hours, it escalates to a manager. This ensures someone eventually reviews the request while maintaining the safety of human oversight. The challenge is that you need to maintain an escalation hierarchy in your system, handle cases where the backup is also unavailable, and avoid creating resentment among team members who feel like they are constantly cleaning up after colleagues who ignore approval requests.

Some systems leave requests in pending state indefinitely, with escalating notifications. The first notification is a gentle email. After two hours, it sends a Slack message. After four hours, it pages the on-call engineer. This works for systems where pending is an acceptable state, such as document review workflows where the document does not expire. It does not work well for real-time operational decisions where users are waiting for outcomes.

## Batch Approvals Versus Individual Approvals

When your agent generates ten approval requests in an hour, should each request go through individual approval, or should the system batch them for more efficient review? Batch approvals improve reviewer productivity by letting them see multiple similar requests at once, spot patterns, and approve or reject in bulk. Individual approvals provide more careful scrutiny for each decision and better audit trails. The right choice depends on the nature of your decisions and your organization's risk tolerance.

A fraud detection agent might flag fifty transactions per day as suspicious. If a human reviewer must open each transaction individually, examine it, and approve or reject, that is fifty separate context switches and fifty separate decisions. Reviewer fatigue sets in. Quality degrades. If instead the system presents all fifty transactions in a single interface where the reviewer can sort by amount, filter by transaction type, and bulk-approve all transactions under a certain threshold, the reviewer can process all fifty in ten minutes with better pattern recognition. They can see that thirty of the flagged transactions are all from the same merchant during a sale event, approve those in bulk, and then focus attention on the twenty genuinely unusual transactions.

The risk with batch approvals is that reviewers stop thinking carefully about each individual item. They glance at the summary statistics, see that most items look normal, and approve the entire batch without noticing the one genuinely risky item buried in the middle. You can mitigate this by designing batch interfaces that highlight outliers, require reviewers to explicitly acknowledge high-risk items even in batch mode, and limit batch sizes so that reviewers cannot approve more than twenty items at once. Some systems use hybrid approaches where routine items can be batch-approved but high-risk items are automatically pulled out for individual review even if they arrived in the same batch.

## Managing the Approval Queue

When you have multiple agents generating approval requests, and multiple human reviewers consuming those requests, you need queue management logic. The approval queue is not just a FIFO data structure. It is a prioritization and routing system that matches requests to appropriate reviewers, ensures high-priority items are handled first, prevents requests from languishing forever, and provides visibility into backlog and throughput.

Priority classification is essential. A request to refund five hundred dollars to an angry customer who is threatening to post a negative review should not sit behind fifty requests to process routine invoice payments. Your system should assign priority scores to approval requests based on factors like monetary amount, customer status, time sensitivity, and business impact. High-priority items jump to the front of the queue and trigger more aggressive notifications. Low-priority items can wait in queue longer before timing out.

Routing requests to appropriate reviewers improves efficiency and quality. If your organization has specialized teams, route invoice approvals to the finance team and refund approvals to customer service. If you have a tiered reviewer structure, route requests requiring specialized knowledge to senior reviewers and routine requests to junior reviewers. Dynamic routing can also balance load, sending new requests to whichever reviewer currently has the smallest queue. Some advanced systems learn reviewer expertise over time and route requests to reviewers who have historically made good decisions on similar items.

Queue visibility prevents requests from being forgotten. A dashboard showing all pending approvals, how long each has been waiting, who is assigned to review, and which items are approaching timeout gives management insight into bottlenecks. If one reviewer has twenty pending items and everyone else has two, you can redistribute the load. If approval times suddenly spike from one hour to six hours, you can investigate what changed. I worked with a logistics company whose approval queue dashboard revealed that forty percent of approvals were waiting on a single reviewer who had been on vacation for three days. No one had noticed because there was no visibility. Once they saw the backlog, they reassigned those requests and implemented an automatic vacation mode that redirected approvals when reviewers were out of office.

## State Persistence During Approval Waits

The technical challenge of approval workflows is preserving complete execution state across potentially long pauses. Your agent might be midway through a complex multi-step process when it hits an approval checkpoint. It has local variables, intermediate API responses, partially constructed objects, and execution context. All of this must be serialized to durable storage, then deserialized later when approval comes through, with the agent resuming exactly as if the pause never happened.

The naive approach is to pickle or serialize the entire agent runtime state. This rarely works well because agent runtimes include non-serializable elements like open network connections, file handles, thread references, and callback objects. Even if you manage to serialize everything, deserializing into a new runtime instance is fragile. Instead, design your agents to maintain explicit checkpointable state. Define a state object that contains only the data needed to resume execution, separate from the runtime machinery. When hitting an approval checkpoint, the agent serializes this state object to a database, terminates cleanly, and records the checkpoint location. When resuming, a fresh agent instance loads the state object, jumps to the checkpoint location in the workflow, and continues.

This requires discipline in how you structure agent code. Every approval checkpoint must be a well-defined point in the execution flow with clear entry and exit conditions. You cannot checkpoint in the middle of a function or midway through a loop iteration. Checkpoints are at decision boundaries where the agent has completed one logical phase and is about to start another. The state object contains the results of all completed phases, so the next phase can execute independently without needing to re-run prior work.

Some teams use workflow orchestration frameworks like Temporal or Cadence that handle state persistence automatically. These frameworks let you write agent code that looks like normal procedural code, with the framework transparently checkpointing state at each activity boundary. When an approval is needed, you call a framework-provided approval activity that pauses the workflow, stores state, and waits for external input. When approval arrives, the framework resumes the workflow with all local variables and execution position intact. This is powerful but locks you into the framework's programming model and operational requirements.

Other teams build custom checkpointing using database records. Each approval request is a database row with a JSON blob containing the serialized state. When the agent needs approval, it inserts a row, sends notifications, and exits. A separate approval service reads rows from the database, presents them in the approval UI, and updates rows when decisions are made. A resumption service polls for approved or rejected rows, loads the state, and launches new agent executions. This approach is more flexible and easier to debug but requires more manual state management.

The critical requirement is idempotency and consistency. If an approval is granted but the resumption service crashes before processing it, the request should not be lost. If the resumption service processes the same approval twice due to a retry, the agent should not execute the approved action twice. You need transaction boundaries and idempotency keys to ensure exactly-once processing of approvals.

## Building Trust Through Transparency

Approval workflows are not just technical mechanisms. They are trust-building exercises. When you pause an agent for human review, you are explicitly acknowledging that the agent is not fully trusted to make this decision alone. The human reviewer needs to understand what the agent is proposing and why, and they need to trust that the agent will faithfully execute their decision. Transparency in how the agent reached its proposal, clarity about what will happen after approval, and reliable execution of the approved action are all essential to building that trust.

Show the agent's reasoning. Do not just present a proposed action and expect reviewers to approve it blind. Explain how the agent arrived at that proposal. Which data sources did it consult? Which rules or policies did it apply? What alternatives did it consider? If the refund agent proposes a two hundred dollar refund, show that it calculated this amount based on the original purchase price of two hundred twenty dollars minus a ten percent restocking fee. Show that it checked the return policy and confirmed the return was within the thirty-day window. Show that it considered offering store credit instead but determined a cash refund was appropriate because the product was defective. This transparency helps reviewers make informed decisions and helps them catch errors in the agent's logic.

Clarify post-approval actions. If the reviewer approves, what exactly will happen? Will the refund be processed immediately or in the next batch? Will the customer receive an email notification? Will any other systems be updated? Ambiguity about post-approval actions creates anxiety for reviewers who are not sure what they are authorizing. Make it explicit. "If you approve, this agent will initiate a wire transfer of two hundred dollars to the customer's original payment method within fifteen minutes and send a confirmation email to the customer." The reviewer knows exactly what their approval means.

Follow through reliably. If the reviewer approves and the agent fails to execute the approved action, or executes it incorrectly, trust is destroyed. Reviewers will start second-guessing the system, approving more cautiously, or rejecting things that should be approved. You need monitoring and alerting that ensures approved actions are executed correctly and promptly. If an execution fails, notify the approver so they know their decision was not carried out and can take manual action if needed.

The approval workflow is your agent's admission that it needs help. Design it to make that help-seeking graceful, efficient, and trustworthy. When done well, approval workflows let you deploy agents in high-stakes domains where pure autonomy would be too risky, while still capturing most of the efficiency benefits of automation. When done poorly, they become bureaucratic bottlenecks that frustrate humans and slow down systems without actually improving decision quality. The difference lies in thoughtful design of checkpoints, interfaces, timeouts, and state management that respects both the capabilities of the agent and the judgment of the humans who oversee it.
