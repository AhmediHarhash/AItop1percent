# 9.8 — Drift Detection: When Agent Behavior Changes Over Time

Your agent's behavior today is not its behavior tomorrow. Model providers update APIs, switch default versions, and retrain checkpoints to improve quality—and those changes propagate silently to your production system unless you pin versions and monitor drift. A seven-point increase in escalation rate sounds modest until you calculate that it means 140 additional tickets per week landing in a backlogged queue, requiring one additional full-time employee to clear. The model did not degrade. It changed calibration: more conservative, more likely to flag uncertainty, more likely to route borderline cases to human review. You were monitoring overall success rate, which stayed flat. You were not monitoring behavioral distributions: routing percentages, confidence scores, response lengths, escalation patterns. Without drift detection, you discover changes only when they cause operational problems, and by then the backlog has grown and customer wait times have increased.

The root cause was not the model update. Providers update models to improve quality, fix bugs, and incorporate new training data. The root cause was the absence of drift detection. The team was monitoring overall success rate, which had not changed. They were monitoring error rate, which had not changed. They were not monitoring behavioral distributions: the percentage of tickets routed to each queue, the percentage of responses flagged as uncertain, the average confidence score, the distribution of response lengths. Without drift detection, they could not identify the change until it caused an operational problem. By the time they noticed, the backlog had grown by 420 tickets, and customer wait times had increased by 30%.

## What Drift Means in Agent Systems

Drift is the phenomenon where an agent's behavior changes over time, even when inputs, prompts, and code remain constant. Drift manifests as shifts in output distributions, shifts in decision patterns, shifts in confidence scores, shifts in latency, or shifts in error types. Drift can be gradual or sudden. It can be intentional, caused by a model update or a prompt change you deployed, or unintentional, caused by a provider-side model swap, a dependency update, or a subtle change in upstream data. Drift is distinct from failure. A drifted agent may still succeed on most tasks, but the way it succeeds, the edge cases it handles, and the tradeoffs it makes have changed.

Drift detection means instrumenting your system to measure behavioral stability over time and alerting when distributions shift beyond acceptable thresholds. You track output distributions, decision distributions, and metadata distributions. You compare current behavior to a baseline, which is usually the most recent stable period, and you flag deviations. Drift detection is not about preventing change. It is about making change visible so you can decide whether the change is acceptable, whether it requires intervention, and whether it signals a deeper issue.

You care about drift because agents are embedded in workflows where downstream systems, users, and processes depend on consistent behavior. If your triage agent routes 12% of tickets to escalation, your staffing model assumes 12%. If that shifts to 19%, your staffing model is wrong, and your SLA is at risk. If your summarization agent produces 200-token summaries and shifts to 350-token summaries, your UI may break, your storage costs may increase, and your users may complain about verbosity. If your code review agent flags 8% of pull requests as high-risk and shifts to 14%, your developers may stop trusting the signal because too many false positives reduce credibility. Drift cascades. Detecting it early contains the damage.

## Types of Drift to Monitor

You monitor several types of drift. Output distribution drift tracks how the distribution of outputs changes. If your classification agent assigns labels A, B, C, D, and the proportion of A-labeled outputs shifts from 40% to 52%, that is drift. You are not monitoring individual predictions. You are monitoring the aggregate distribution. You expect some variance due to input changes, but if the input distribution is stable and the output distribution shifts, the agent has drifted.

Decision distribution drift tracks how the agent makes decisions at key choice points. If your routing agent sends 60% of tasks to Agent A and 40% to Agent B, and that shifts to 70%-30%, that is drift. If your approval agent approves 85% of requests and that shifts to 78%, that is drift. If your escalation agent escalates 12% of cases and that shifts to 19%, that is drift. Decision distributions reflect the agent's internal thresholds, calibration, and heuristics. When they shift, the agent's behavior has changed.

Confidence distribution drift tracks how the agent reports uncertainty. If your agent returns confidence scores between 0 and 1, you track the distribution of those scores. If the median confidence was 0.89 in December and 0.81 in January, the agent is less certain. If the percentage of predictions with confidence below 0.7 was 8% and shifts to 15%, the agent is flagging more uncertainty. Confidence drift often precedes decision drift. A model that is less confident may route more cases to fallback paths, increasing escalation rates or retry rates.

Latency distribution drift tracks how response times change. If your agent's p50 latency was 1.2 seconds and shifts to 1.9 seconds, something changed. Maybe the model got slower. Maybe the provider's infrastructure changed. Maybe your agent started making more tool calls due to a prompt change. Latency drift impacts user experience and system capacity. A 60% increase in latency reduces throughput by 37% if you are request-limited.

Error type distribution drift tracks how failure modes change. If your agent failed 4% of tasks in December with 70% of failures due to timeouts and 30% due to malformed outputs, and in January failures remain at 4% but 60% are malformed outputs and 40% are timeouts, the failure profile has shifted. This suggests the model is struggling with output formatting, possibly because a provider update changed response structure or because your validation logic is stricter.

Token distribution drift tracks how input and output token counts change. If your agent's median input token count was 850 and shifts to 1,100, either inputs got longer or your prompt got longer. If output token counts shift from 220 to 310, the agent is generating more verbose responses. Token drift impacts cost and latency. A 40% increase in output tokens increases cost by 40% and may increase latency by 25%.

## Baseline Selection and Drift Measurement

You measure drift by comparing current behavior to a baseline. The baseline is a reference period where behavior was stable and acceptable. Most teams use a rolling baseline: the most recent 7 or 30 days, excluding the current day. You compute baseline distributions from historical data, then compare today's distribution to the baseline. If today's distribution deviates significantly, you alert.

You define significance using statistical tests or heuristic thresholds. For distribution drift, you use tests like Kolmogorov-Smirnov, chi-squared, or Jensen-Shannon divergence to measure how different two distributions are. If the divergence exceeds a threshold, you flag drift. For single-metric drift like median confidence or p50 latency, you use percent change thresholds: if median confidence drops by more than 10%, you alert. If p50 latency increases by more than 25%, you alert. Thresholds are task-specific. High-stakes agents require tighter thresholds. Low-stakes agents tolerate more variance.

You do not compare every task individually. You aggregate by time window: hourly, daily, or weekly. You compute distributions for each window and compare adjacent windows or compare each window to the baseline. If Monday's output distribution differs from the 30-day baseline by a divergence score above 0.15, you flag Monday as drifted. If the drift persists on Tuesday and Wednesday, you escalate. If it resolves on Thursday, you log it as a transient anomaly and investigate whether it correlates with external factors like weekend traffic patterns or batch processing jobs.

You version your baseline. When you intentionally deploy a prompt change, a model switch, or an agent update, you reset the baseline. You do not want to alert on drift caused by changes you made. You mark the deployment timestamp in your drift detection system, and you start a new baseline period after the deployment. You compare post-deployment behavior to the new baseline, not the old one. This allows you to measure whether your deployment had the intended effect and whether it introduced unintended side effects.

You also track drift over multiple timescales. Short-term drift, measured hourly or daily, catches sudden changes like model swaps or API outages. Long-term drift, measured weekly or monthly, catches gradual shifts like input distribution changes or slow model degradation. A shift that is invisible day-to-day may be obvious when you compare January 2026 to October 2025. You run both short-term and long-term drift detection and alert on either.

## Instrumentation for Drift Detection

You instrument drift detection at the same layer where you log task outcomes and telemetry. For every task, you log outputs, decisions, confidence scores, latencies, token counts, and timestamps. You store these logs in a queryable system: a data warehouse, a time-series database, or a structured logging platform. You run periodic jobs, hourly or daily, that aggregate logs into distributions and compute drift metrics.

You track output distributions by counting occurrences of each output value or output category. If your agent outputs one of five labels, you count how many times each label appeared in the past 24 hours and compute a distribution: 42% A, 28% B, 18% C, 8% D, 4% E. You compare this to the baseline distribution: 40% A, 30% B, 20% C, 7% D, 3% E. You compute a divergence metric. If divergence is high, you investigate which labels shifted and by how much.

You track decision distributions by logging decisions at key choice points. If your agent decides whether to escalate, you log a binary decision: escalate or not. You aggregate over 24 hours: 140 escalations out of 1,200 tasks is 11.7%. Baseline is 12.0%. Drift is minimal. If aggregation shows 19.2%, drift is significant. You log not just the final decision but also intermediate decisions if your agent is multi-step. If an agent decides to fetch context, then decides to call a tool, then decides to escalate, you log all three decisions. If tool call rate shifts from 35% to 48%, that is drift in tool usage, which may propagate to downstream decisions.

You track confidence distributions by logging confidence scores for every task. You compute percentiles: p10, p25, p50, p75, p90. You compare current percentiles to baseline percentiles. If p50 drops from 0.89 to 0.81, you flag it. If p10 drops from 0.62 to 0.51, the low-confidence tail has gotten worse, which may indicate the agent is struggling with harder inputs or the model has become less calibrated.

You track latency distributions by logging task start time and end time. You compute p50, p95, p99 latencies for each day. You compare to baseline. If p95 latency was 3.2 seconds and shifts to 4.8 seconds, you investigate. Latency drift often correlates with provider-side changes, increased tool call counts, or increased input sizes.

You track token distributions by logging prompt tokens and completion tokens for every LLM call. You aggregate per task: total input tokens, total output tokens. You compute daily medians and compare to baseline. If median output tokens shift from 220 to 310, you investigate whether prompts changed, whether the model got more verbose, or whether task inputs got more complex.

You store drift metrics in a time-series database or append them to a monitoring dashboard. You create charts showing each metric over time with baseline ranges shaded. When current values exit the baseline range, the chart highlights the anomaly. You also create alerts that trigger when drift metrics exceed thresholds, sending notifications to Slack, PagerDuty, or email.

## Investigating Drift When It Occurs

When drift is detected, you investigate root cause before taking action. Drift can have many causes: model updates, prompt changes, input distribution shifts, dependency updates, infrastructure changes, or external factors like holidays or seasonal events. You start by correlating the drift event with recent changes.

You check your deployment log. Did you deploy a new prompt, a new model, or a new agent version around the time drift started? If yes, the drift is likely caused by your change. You review the change, compare pre- and post-deployment evals, and decide whether the drift is acceptable. If the new model is more conservative and escalates 7 points more, you decide whether that aligns with your goals. If it does, you accept the drift and update your baseline. If it does not, you roll back or tune the model.

You check provider release notes. Did your model provider update the model version, change the API, or deprecate a feature? OpenAI, Anthropic, Google, and other providers periodically update models. Sometimes they notify developers in advance. Sometimes updates are silent. If you are using a model alias like gpt-4o instead of a pinned version like gpt-4o-2024-11-20, you may get automatically upgraded. You check the provider's changelog and model version logs. If a new version was released around the drift date, you test the new version against the old version using your eval suite. If the new version performs worse on your tasks, you pin to the old version or file a support ticket with the provider.

You check input distribution. Did the types of tasks, the length of inputs, or the content of inputs change? If your support ticket volume shifted from 60% billing questions to 40% billing and 20% technical questions, the input distribution has changed, and output distributions may shift accordingly. Input drift is not a problem with the agent. It is a reflection of real-world changes. You update your baseline to account for the new input distribution, or you segment drift detection by input type so you can monitor each type independently.

You check dependency updates. Did you update a library, a tool, a database, or an API that the agent depends on? If your agent calls a search API and that API changed its response format, the agent's behavior may change. If you updated a parsing library and it now extracts different fields from documents, the agent's inputs change, and outputs may drift. You review recent dependency updates and test whether rolling back the dependency eliminates the drift.

You check infrastructure changes. Did latency increase because your infrastructure scaled down, because network conditions changed, or because a regional outage affected your provider's API? Latency drift may not reflect agent behavior changes but rather environmental changes. You correlate drift timestamps with infrastructure logs, cloud provider status pages, and network monitoring tools.

You check external factors. Did traffic patterns change due to a holiday, a product launch, a marketing campaign, or a seasonal event? If your agent processes customer inquiries and a product launch caused a spike in technical questions, input distribution has shifted, and output distribution may shift. You segment data by time period or event type and compare like to like.

## Responding to Drift

Once you have identified the root cause, you decide how to respond. If the drift is caused by an intentional change you deployed, you evaluate whether the change achieved its goals. If your goal was to reduce false positives and escalation rate increased, you check whether false positive rate decreased. If it did, the tradeoff is acceptable. If false positive rate stayed the same, the change failed, and you roll back or iterate.

If the drift is caused by a provider-side model update, you decide whether to accept the new model, pin to the old model, or switch providers. You run evals comparing the old and new models on your tasks. If the new model is better on 70% of tasks but worse on 30%, you decide based on which tasks matter most. If the 30% are high-stakes tasks, you pin to the old model. If the 30% are low-stakes, you accept the new model. If the provider deprecated the old model and you cannot pin, you tune prompts or add guardrails to mitigate the new model's weaknesses.

If the drift is caused by input distribution changes, you update your baseline or segment your monitoring. You do not penalize the agent for responding appropriately to different inputs. You adjust your expectations and your staffing models to account for the new input mix. You may also update your prompts to handle the new input types more effectively.

If the drift is caused by a dependency update, you roll back the dependency, fix the integration, or update the agent to handle the new dependency behavior. You treat this as a bug fix, not a tuning problem.

If the drift is transient, caused by a temporary infrastructure issue or a one-time event, you log it and monitor for recurrence. You do not overreact to single-day anomalies. You escalate when drift persists for multiple days or when it crosses critical thresholds that impact SLAs or user experience.

## Continuous Drift Monitoring and Governance

Drift detection is not a one-time analysis. It is continuous monitoring that runs every hour or every day, alerting you when behavior changes. You integrate drift detection into your operational dashboard alongside latency, error rate, and cost. You create runbooks for responding to drift alerts: what to check, what to log, who to notify, how to escalate.

You assign ownership for drift investigation. If an alert fires, a designated on-call engineer or agent reliability engineer investigates within four hours. They document findings in a shared log, decide on a response, and update the baseline or roll back changes as needed. Drift investigation is not deferred to the next sprint. It is treated as an operational incident, because drift that goes unaddressed compounds into user-facing degradation.

You version baselines and track baseline changes over time. Every time you reset the baseline due to a deployment, you log the reset event, the reason, and the expected behavior change. This creates an audit trail that explains why behavior changed and whether the change was intentional. When you review drift six months later, you can trace it back to specific deployments or provider updates.

You integrate drift metrics into your quarterly reviews. You report how often drift occurred, what caused it, how quickly it was detected, and how it was resolved. You track mean time to detection and mean time to resolution. If drift goes undetected for days, you tighten alert thresholds or increase monitoring frequency. If resolution takes too long, you improve runbooks or assign more resources to drift response.

You use drift detection as an input to model and prompt versioning decisions. If a model drifts frequently, you consider pinning to specific versions instead of using aliases. If a prompt is sensitive to small model changes, you version it tightly and regression-test it after every provider update. Drift is a signal that your system is fragile, and fragility must be addressed with tighter version control, better evals, or architectural changes.

Drift detection is the operational discipline that keeps agents stable in a world where models, providers, dependencies, and inputs change constantly. It is the counterbalance to the velocity of AI development. Without it, you deploy agents and hope they keep working. With it, you deploy agents and know when they stop working, why they stopped, and how to fix them. The cost is instrumentation and ongoing vigilance. The benefit is reliability, trust, and the ability to operate agents at scale without constant manual oversight.
