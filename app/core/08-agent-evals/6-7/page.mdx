# 6.7 — Context Window Management for Long-Running Agents

In December 2025, a software engineering team deployed an agent designed to help developers refactor a large legacy codebase over a multi-week period. The agent was built on a model with a one-hundred-twenty-eight-thousand token context window, which seemed more than sufficient for tracking the refactoring work. The first few days went smoothly: the agent remembered which files had been modified, which patterns had been deprecated, and which new abstractions had been introduced. But by day seven, strange things started happening. The agent began forgetting decisions it had made just hours earlier. It would recommend refactoring a file it had already refactored. It would contradict its own guidance from earlier in the day. When developers asked why, they discovered the problem: the agent's context window was full. Seven days of continuous conversation, code diffs, file contents, and tool outputs had filled the entire one-hundred-twenty-eight-thousand token buffer. As new information came in, old information was silently evicted. Critical architectural decisions from day two had fallen out of context by day six. The team had assumed the large context window meant they could run the agent indefinitely without managing state. They were wrong. They spent the next week implementing checkpointing, selective context retention, and summarization to make the agent viable for long-running work.

You will encounter this same cliff if you build agents that run for hours, days, or weeks without resetting their state. The context window is the agent's working memory—the information immediately available to the model when it generates its next response. Unlike human working memory, which is limited to a handful of items, model context windows can hold hundreds of thousands of tokens. But they are not infinite, and they do not scale with task duration. A one-hundred-twenty-eight-thousand token context window sounds enormous until you fill it with a few hours of detailed conversation, code, and tool outputs. Once it is full, something must be evicted to make room for new information. If you do not control what gets evicted, you lose critical context and your agent becomes unreliable.

Context window management is the art of deciding what information stays in the agent's immediate working memory and what gets pushed out to external storage or discarded entirely. It requires understanding the context window as a scarce resource, implementing strategies to maximize its value, and building infrastructure to handle the inevitable moment when you run out of space. This is not a luxury concern for exotic use cases. Any agent that runs for more than a few hours will face context window pressure. How you handle that pressure determines whether your agent remains coherent or devolves into confusion.

## The Context Window as Working Memory

The context window is fundamentally different from long-term memory systems like vector stores or databases. Long-term memory is large, persistent, and searchable, but it requires explicit retrieval. The model does not see what is in the vector store unless you query it and inject the results into the context. The context window, in contrast, is small, ephemeral, and automatic. Everything in the context window is immediately visible to the model on every generation. The model does not need to search for it or decide whether to retrieve it—it is simply there. This automatic visibility is what makes the context window so powerful and so dangerous.

This makes the context window extremely powerful for maintaining coherence and continuity. If a user asks a follow-up question, the model can refer back to the previous exchange because it is still in context. If the agent performs a multi-step task, the intermediate results remain accessible without explicit retrieval. The context window is the reason conversational agents feel conversational rather than transactional. Without it, every user message would be treated as a fresh start, with no memory of what came before. The agent would have to retrieve all relevant prior context from external storage every turn, which is slow, error-prone, and expensive.

But this power comes with constraints. Context windows have a fixed maximum size, measured in tokens. For current models in 2026, sizes range from eight thousand tokens for older or smaller models to two million tokens for the largest frontier models. Token consumption depends on the verbosity of the conversation, the size of the data being processed, and the number of tool outputs being accumulated. A single code file can consume thousands of tokens. A detailed conversation can consume tens of thousands. Tool outputs from web scraping or database queries can consume hundreds of thousands. You can fill even a very large context window faster than you expect, especially if your agent is handling rich, complex tasks that involve large documents, code repositories, or detailed multi-turn conversations.

Once the context window is full, the system must evict old content to make room for new content. Different model providers handle this differently. Some use a sliding window: the oldest tokens are dropped first, preserving the most recent context. Some allow you to specify which parts of the context to keep and which to evict. Some simply fail or truncate input arbitrarily. You need to know how your model handles context overflow and design your agent accordingly. If your model silently truncates, you might not even realize that critical information has been lost until the agent starts behaving incoherently.

The worst-case scenario is silent eviction without awareness. The agent continues to operate, but it has lost access to information it previously relied on. It may contradict itself, forget user preferences, or lose track of task state. From the user's perspective, the agent has amnesia. From the agent's perspective, the past simply no longer exists. This is what happened to the refactoring agent in December 2025: critical decisions fell out of context, and the agent had no way to know they were missing. It confidently recommended actions based on incomplete information, leading to confusion and wasted work.

## Strategies for Context Management: Sliding Window, Summarization, Selective Retention

You have several strategies for managing context window consumption in long-running agents. The right strategy depends on the nature of your task, the importance of different types of information, and the capabilities of your model. Most production agents use a combination of strategies rather than relying on a single approach.

The simplest strategy is a sliding window: keep only the most recent N tokens in context and drop everything older. This works well for tasks where recent context is much more important than distant past. In a casual conversation, the last few turns are usually sufficient to maintain coherence. In a debugging session, the most recent error messages and code changes are more relevant than what happened an hour ago. Sliding window is easy to implement—most model APIs support it natively—and it provides predictable, bounded memory consumption. You set a threshold, say sixty thousand tokens, and once you exceed it, the oldest content is automatically dropped.

The downside of sliding window is that it treats all information equally and discards based purely on age. A critical piece of information from early in the session—a system constraint, a user preference, a key decision—will eventually be evicted even if it remains relevant. If the user provided important requirements in the first conversation turn and you are now on turn fifty, those requirements are likely gone. The agent may proceed in a direction that violates early constraints because it no longer remembers them. Sliding window is appropriate for ephemeral tasks where the distant past truly does not matter, but it is dangerous for long-running tasks where foundational decisions remain relevant throughout.

Summarization is a more sophisticated strategy: periodically compress old context into a summary and replace the detailed history with the summary. Instead of keeping the full text of the last thirty conversation turns, you might keep the full text of the last five turns and a summary of the previous twenty-five. This preserves the gist of old context while freeing up space for new details. Summarization is particularly effective for long conversations where the overall narrative arc matters but the verbatim details do not. The user does not need you to remember every word they said, but they do need you to remember the decisions made and the direction chosen.

The challenge with summarization is deciding when to summarize and ensuring the summary preserves the right information. If you summarize too frequently, you lose too much detail. If you summarize too infrequently, you run out of context before summarization kicks in. A common heuristic is to summarize when context consumption exceeds a threshold—say seventy-five percent of the maximum. You take the oldest half of the context, generate a summary, replace those turns with the summary, and continue. This keeps context consumption bounded while preserving high-level continuity. You might trigger summarization every ten thousand tokens consumed, or every twenty turns, or whenever context pressure exceeds a threshold.

Summary quality is critical. A bad summary that omits key details or introduces errors is worse than no summary. You should use a strong model for summarization, provide clear instructions about what to preserve, and validate the summary before replacing the original content. In some cases, you might want to keep the original content in external storage so you can retrieve it if needed, even if it is no longer in the active context window. This hybrid approach—summarize for context efficiency, archive originals for fallback retrieval—provides both speed and safety.

Selective retention is the most powerful but most complex strategy: explicitly decide which parts of the context are high-value and must be retained, and which are low-value and can be evicted. High-value content might include the system prompt, the task description, critical user requirements, key decisions, and recent conversational turns. Low-value content might include verbose tool outputs, intermediate debugging steps, or redundant exchanges. By prioritizing high-value content, you maximize the utility of the limited context space. This requires building infrastructure to tag, score, and rank content, but it produces the best results for complex, long-running tasks.

Implementing selective retention requires tagging or scoring each piece of content based on its expected future value. You might assign priorities at ingestion time: system prompts are priority one, user messages are priority two, tool outputs are priority three. When context is full, evict the lowest-priority content first. Alternatively, you might use heuristics to score content dynamically: content that has been referenced recently is high-value, content that has not been referenced in the last ten turns is low-value. You might even use a small model to predict which content is likely to be needed in future turns and retain that. This predictive approach is expensive but highly effective for agents that handle long, complex tasks.

Selective retention is especially important for agents that use tools extensively. Tool outputs can be extremely verbose—a web scrape might return fifty thousand tokens, a database query might return ten thousand tokens. If you keep all tool outputs in context, you will quickly run out of space. But some tool outputs are critical—the results of a calculation, the status of a deployed service, the contents of a configuration file. You need to distinguish between tool outputs that must remain in context and those that can be summarized or discarded. One approach is to have the agent explicitly mark tool outputs as "retain" or "discard" based on whether it expects to reference them again. Another is to keep tool outputs in context for a fixed number of turns and then evict them unless they are referenced. Yet another is to summarize tool outputs immediately, keeping only the essential information in context while archiving the full output externally.

## Priority-Based Context: Keeping High-Value Information

Priority-based context management treats the context window as a priority queue. Each piece of information has a priority, and when space is needed, the lowest-priority items are evicted first. This ensures that the most important information stays in context as long as possible. The challenge is defining priorities in a way that matches the semantic importance of information rather than just its recency.

Defining priorities requires domain knowledge. In a customer support agent, the current issue description and recent user messages are high-priority. Historical issues from months ago are low-priority. In a coding agent, the current file being edited and the task plan are high-priority. Old code snippets from files that are no longer relevant are low-priority. In a research agent, the research question and key findings are high-priority. Intermediate search results that have already been synthesized are low-priority. You cannot use a universal priority scheme; priorities must reflect the specific semantics of your task.

You can implement priority as a numerical score from zero to ten, or as categorical labels like "critical," "important," "normal," "low." The system maintains a running count of tokens consumed by each priority level. When the context window exceeds a threshold, the system evicts content starting with the lowest priority until space is available. If all low-priority content is evicted and space is still needed, the system moves to the next priority level. This cascading eviction ensures that the most critical information is retained until the very last moment.

Dynamic priority adjustment can improve retention quality. If a piece of information is referenced frequently, its priority should increase. If it has not been referenced in many turns, its priority should decrease. This ensures that the agent retains the information it actually uses and evicts information that is no longer relevant. Implementing dynamic adjustment requires tracking access patterns—which parts of the context the model attends to during generation—and updating priorities accordingly. Some model APIs provide attention weights or citation metadata that can inform this, though most do not, requiring you to use heuristics like explicit references in the agent's output.

Another consideration is priority inheritance. If a high-priority message references a low-priority document, should the document's priority be boosted? If the task plan is high-priority and it mentions a specific code file, should that file be retained even if it would otherwise be evicted? Priority inheritance can prevent context fragmentation—retaining high-priority content that refers to missing low-priority content renders the high-priority content less useful. If the task plan says "refactor the authentication module" but the authentication module code has been evicted from context, the task plan is nearly useless. However, inheritance can also prevent necessary evictions if too many items get boosted. You need to balance inheritance with hard limits on total context consumption, ensuring that inheritance improves coherence without defeating the purpose of eviction.

## The Context Cliff: What Happens When Critical Information Falls Out

The context cliff is the moment when critical information is evicted from the context window and the agent loses access to it. This is not a graceful degradation—it is an abrupt failure. One turn, the agent knows the user's requirements. The next turn, it does not. The user's experience shifts from "this agent understands me" to "this agent is confused." The context cliff is one of the most common failure modes for long-running agents, and it is invisible until it happens.

The severity of the context cliff depends on what was evicted and whether the agent has a way to recover it. If the evicted information is in a long-term memory store and the agent can retrieve it, the cliff is shallow: there may be a slight latency increase or a brief moment of confusion, but the agent can recover. If the evicted information is not stored anywhere else, the cliff is steep: the information is gone, and the agent cannot recover it without asking the user to repeat it. This is frustrating for users and destroys trust. A user who provided detailed requirements at the start of a session should not have to repeat them an hour later because the agent forgot.

To mitigate the context cliff, you need to ensure that anything evicted from context is preserved somewhere else. Before evicting content, write it to a memory store, a database, or a log file. Tag it with metadata—timestamp, source, relevance score—so it can be retrieved later if needed. This way, even if the agent loses immediate access, it can recover the information through retrieval. The retrieval may be slower and may require the agent to explicitly decide to search for the information, but it is better than permanent loss.

You should also make the agent aware of its own context limits. If the agent knows it is approaching the context window limit, it can take proactive action: summarize the current state, checkpoint progress, warn the user that some information may become inaccessible. Some agents implement a "context pressure" signal—a numerical value indicating how full the context window is—and use it to trigger consolidation or checkpointing behavior. When context pressure exceeds eighty percent, the agent might automatically generate a summary of the conversation so far and store it in memory. When it exceeds ninety percent, it might prompt the user to confirm the current task and key requirements before continuing. This proactive approach prevents the context cliff by consolidating information before it is lost.

User-facing transparency can also help. If the agent is about to lose access to important information, it can inform the user: "We are approaching the limit of my working memory. Let me summarize what we have decided so far so I do not lose track." This sets expectations and gives the user an opportunity to correct or confirm the summary before information is evicted. It also builds trust—the user understands that the agent is managing a real constraint, not just being forgetful. Transparency turns a mysterious failure into an understandable limitation.

## Checkpointing and Restoration for Long-Running Tasks

For agents that run for hours, days, or weeks, checkpointing is essential. A checkpoint is a snapshot of the agent's state at a particular moment: the current context, the task progress, the decisions made, the tools invoked, the results obtained. If the agent crashes, if the session is interrupted, or if the context window overflows, you can restore from the most recent checkpoint and continue where you left off. Checkpointing is the foundation of fault tolerance and indefinite runtime.

Checkpointing serves several purposes. First, it provides fault tolerance. If the agent process dies, you do not lose all progress—you can resume from the last checkpoint. Second, it enables context rotation. You can checkpoint the current state, clear the context window, and start fresh with a summary of what happened before. This allows the agent to run indefinitely without hitting context limits. Third, it enables session persistence. A user can close their session and return hours or days later, and the agent can restore the context from the checkpoint and continue the conversation. This is critical for collaborative, long-term work where sessions span multiple days.

A checkpoint typically includes several components. The conversation history: all user and agent messages up to the checkpoint. The task state: the current goal, the plan, the progress so far. The memory state: any facts, preferences, or decisions that have been learned. The tool state: the results of tool invocations that are still relevant. The context itself: the actual tokens in the context window, or a compact representation of them. Some checkpoints also include metadata: the model used, the timestamp, the user ID, the session ID. This metadata is essential for debugging and for understanding checkpoint provenance.

Checkpoints can be stored in various formats. A simple approach is to serialize the entire context as JSON or a similar structured format and write it to a file or database. A more compact approach is to store only the high-level state—task, plan, key facts—and regenerate the full context on restore by replaying the conversation or re-invoking tools. The tradeoff is space versus restore time: full checkpoints are large but fast to restore, compact checkpoints are small but slow to restore. For most use cases, compact checkpoints are preferable unless restore time is critical.

Checkpoint frequency is a design decision. Checkpointing too often wastes resources—writing large state snapshots to disk is expensive. Checkpointing too rarely risks losing significant progress if a failure occurs. A common heuristic is to checkpoint at natural breakpoints: after completing a subtask, after a significant decision, after a long tool invocation, or after a certain amount of time has elapsed. You might also checkpoint proactively when context pressure is high, using the checkpoint as an opportunity to clear the context and continue with a fresh window. This dual-purpose checkpointing—both for fault tolerance and for context management—is highly effective.

Restoration from a checkpoint requires more than just loading the state. You need to reinitialize the agent's context with the checkpoint data, verify that the state is consistent, and handle any changes that occurred since the checkpoint was created. If the user modified external state—edited a file, changed a configuration, deployed a service—the checkpoint may be stale. The agent should detect staleness and either refresh the relevant parts of the state or warn the user that the checkpoint may be out of date. Staleness detection can be implemented using timestamps, version numbers, or checksums. If the checkpoint includes a file at version X but the file is now at version Y, the checkpoint is stale.

## How Different Models Handle Context Differently

Not all models treat context windows the same way. Some models have fixed-size windows with hard cutoffs. Some have variable-size windows that degrade performance as they fill. Some have hierarchical attention mechanisms that treat recent context differently from distant context. Understanding these differences is critical for designing robust context management. What works for one model may fail catastrophically for another.

Fixed-size models enforce a strict token limit. If you exceed the limit, the API either truncates the input or returns an error. Truncation might remove the oldest tokens, the newest tokens, or tokens from the middle, depending on the provider. You need to know the truncation behavior and design around it. If the model truncates from the beginning, put your system prompt and critical instructions at the end. If it truncates from the end, put them at the beginning. If it truncates from the middle, avoid putting critical information there. Some providers allow you to specify which parts of the context are protected from truncation, which is highly valuable.

Variable-size models allow inputs up to a maximum size but may experience performance degradation as the context grows. Latency increases because the model must process more tokens. Quality may decrease because attention is diluted across a larger input. Some models exhibit the "lost in the middle" phenomenon: information in the middle of a very long context is less likely to influence the output than information at the beginning or end. If you are using a variable-size model, monitor performance metrics—latency, quality, correctness—as context size grows, and implement proactive context management before degradation becomes severe. You might set a soft limit at seventy percent of the maximum and trigger consolidation at that point, even though the model could technically handle more.

Some frontier models in 2026 support structured context or hierarchical attention. These models can treat different parts of the context differently: a system prompt might have permanent high-priority attention, recent messages might have medium attention, and older messages might have low background attention. This allows the model to handle much larger effective contexts without the same degradation. If your model supports structured context, use it. Tag different parts of your input with priority or role metadata and let the model's attention mechanism optimize accordingly. This is far more efficient than manual eviction because the model can use its learned attention patterns to focus on relevant content.

Model-specific features can also influence context strategy. Some models support caching: if you send the same prefix multiple times, the provider caches the processed state and reuses it, reducing latency and cost. This is ideal for agents with a fixed system prompt and a growing conversation. You can cache the system prompt and the early conversation, and only pay for processing new turns. Caching is especially valuable for high-throughput agents where the same prefix is reused across many sessions. Other models support prompt compression: they automatically compress old context to free up space for new input. This is convenient but opaque—you do not control what gets compressed or how—so it is best used as a safety net rather than a primary strategy.

Always consult your model provider's documentation and test context behavior empirically. The official documentation may describe theoretical limits and behaviors, but real-world performance can differ. Run experiments with long contexts. Measure latency, cost, and quality. Identify the point at which performance degrades and design your context management to stay below that threshold. Do not assume that a model with a two-hundred-thousand token context window will perform well with two hundred thousand tokens in use. The practical limit may be much lower.

## Designing for Indefinite Runtime

If you want your agent to run for hours, days, or weeks without resetting, you need to treat context window management as a first-class design problem, not an afterthought. This means building infrastructure for summarization, checkpointing, selective retention, and memory integration from the start. It means monitoring context consumption in production and alerting when thresholds are approached. It means testing your agent under long-running scenarios to identify where context limits cause failures. Context management is not something you bolt on after the agent is built; it is a fundamental architectural concern.

Start by establishing a context budget. Allocate a portion of the context window to fixed content: system prompts, task descriptions, high-priority instructions. Allocate another portion to dynamic content: conversation turns, tool outputs, retrieved memories. Reserve a buffer for new input. If your total context window is one hundred thousand tokens, you might allocate ten thousand to fixed content, seventy thousand to dynamic content, and reserve twenty thousand for new input. Monitor consumption in each category and trigger consolidation when dynamic content exceeds its budget. This budget-based approach ensures that you never run out of space for new input, which would cause the agent to fail.

Implement automated context management policies. When dynamic content exceeds its budget, automatically summarize the oldest content, checkpoint the state, or evict low-priority items. Do not rely on manual intervention—by the time a human notices the context is full, the agent may have already failed. Automation ensures the agent can run unattended for extended periods. You might implement a background process that monitors context pressure every N turns and triggers consolidation when pressure exceeds a threshold. This process should be invisible to the user—context management should happen seamlessly in the background without interrupting the conversation.

Build observability into your context management. Log every eviction, every summarization, every checkpoint. Track metrics like context utilization percentage, eviction rate, checkpoint frequency, and restoration success rate. These metrics help you understand how your agent uses context in production and where improvements are needed. If you see frequent evictions of high-priority content, you need to increase your context budget or improve your retention strategy. If you see checkpoints failing to restore correctly, you need to debug your checkpoint logic. Observability turns context management from a black box into a transparent, debuggable system.

Finally, educate your users about context limits. If your agent is designed for long-running tasks, make it clear that the agent has working memory constraints and that it uses checkpointing and summarization to manage them. Set expectations that the agent may not remember every detail of every past interaction verbatim but that it will preserve the essential information. Give users tools to review the agent's current state, to inspect checkpoints, and to manually provide context when needed. Transparency about limitations builds trust and prevents users from blaming the agent for what are actually fundamental constraints of the underlying technology.

Context window management is the difference between an agent that runs for minutes and an agent that runs for days. It is the difference between an agent that gracefully handles long tasks and an agent that collapses into confusion as context overflows. The decisions you make about sliding windows, summarization, selective retention, priority-based eviction, checkpointing, and model-specific optimizations will determine whether your agent can sustain long-running tasks or whether it hits the context cliff and fails. Treat the context window as the scarce, valuable resource it is. Manage it carefully, monitor it constantly, and design your agent to thrive within its limits. The agents that master context management will be the agents that deliver real value on complex, long-running tasks that span hours, days, or weeks. The agents that ignore context management will fail mysteriously and frustrate their users, no matter how sophisticated their other capabilities are.

## Token Budgeting: Allocating Context Across Components

Context window management becomes significantly easier when you treat it like a financial budget. You have a fixed amount of tokens available—say one hundred thousand—and you need to allocate that budget across different components of your agent system. The system prompt needs tokens. The task description needs tokens. Conversation history needs tokens. Retrieved memories need tokens. Tool outputs need tokens. User messages need tokens. If you do not allocate explicitly, some component will consume the entire budget and starve the others.

Token budgeting starts with establishing fixed allocations. The system prompt might get five thousand tokens. The task description might get two thousand tokens. These are non-negotiable: they must always be in context. Next, you allocate a buffer for user input and agent output: reserve ten thousand tokens for the next user message and the agent's response. This ensures you never run out of space to process new input. The remaining eighty-three thousand tokens are your dynamic budget, allocated to conversation history, tool outputs, and retrieved memories.

Within the dynamic budget, you implement priority-based allocation. High-priority content gets allocated first. If the current task plan is high-priority, allocate tokens for it before allocating for old conversation turns. If a recent tool output contains critical information, allocate tokens for it before allocating for historical tool outputs. If retrieved memories are directly relevant to the current query, allocate tokens for them before allocating for tangential context. This priority-driven allocation ensures that the most valuable information stays in context.

When the dynamic budget is exhausted, you must make eviction decisions. You cannot simply drop everything that does not fit; you need to selectively retain the highest-value content. This is where your retention strategy comes into play. Use the strategies discussed earlier—sliding window, summarization, selective retention—to compress or remove low-priority content until you fit within budget. The budget acts as a hard constraint that forces you to make explicit tradeoffs.

Token budgeting also enables monitoring and alerting. Track how much of your budget each component consumes over time. If conversation history starts consuming sixty percent of your budget, leaving little room for tool outputs and memories, you know you need to summarize more aggressively. If tool outputs are consuming eighty percent of your budget, you know you need to either reduce tool verbosity or implement better tool output summarization. Budget metrics give you visibility into where your context is going and help you optimize allocation.

## Context Compression: Semantic Lossy Compression for Long Context

Traditional data compression—gzip, brotli, LZ4—reduces token count by exploiting patterns and redundancy in text. These methods are lossless: you can decompress and recover the original text exactly. But they provide limited compression ratios for natural language—perhaps two-to-one or three-to-one. For long-running agents, this is insufficient. You need much higher compression ratios, which requires semantic lossy compression: understanding the content and discarding parts that are less important.

Semantic compression is what summarization does, but it can be applied more broadly. Instead of summarizing entire conversations, you can compress individual messages, tool outputs, or code snippets. For example, a tool output might contain a hundred lines of log data, but only three lines are relevant to the current task. You can extract those three lines and discard the rest, reducing the token count by ninety-seven percent while retaining all the essential information. This requires understanding what is relevant, which may require the model itself or a separate classifier.

You can also compress by extracting structure. If a conversation includes a detailed discussion of ten different options, you might compress it into a structured list: option one, pros and cons, option two, pros and cons, and so on. The verbatim discussion is lost, but the key information—the options and their tradeoffs—is preserved in a much more compact form. This structured compression is easier to process and faster to retrieve than the original verbose discussion.

Another technique is reference compression. Instead of keeping the full text of a code file in context, you might keep only the function signatures and a brief description. If the agent needs more detail, it can retrieve the full file from external storage. This allows you to maintain awareness of many files without consuming huge amounts of context. The agent knows the files exist, knows what they contain at a high level, and can drill down when necessary. Reference compression trades some convenience for significant space savings.

Semantic compression requires careful validation. Because it is lossy, you risk discarding information that later turns out to be important. You should test compressed context against uncompressed context on representative tasks and measure whether task performance degrades. If compression causes the agent to make more errors or to ask for clarification more often, the compression is too aggressive. Tune your compression strategy to balance space savings with information preservation.

## The Attention Span Problem: Keeping the Agent Focused

Even when you manage to keep critical information in the context window, there is no guarantee the agent will actually use it. Large language models have an attention span problem: as context grows, the model's ability to attend to all parts of the context degrades. Information at the beginning and end of the context tends to be more influential than information in the middle. Information that is repeated or emphasized tends to be more influential than information mentioned once. If you have a critical requirement buried in the middle of a fifty-thousand-token context, the model may effectively ignore it.

This means context window management is not just about what information is present, but about how that information is positioned and emphasized. You can improve attention by placing critical information at strategic locations. Put the most important instructions and constraints at the beginning of the context, immediately after the system prompt. Put the most recent and actionable information at the end of the context, just before the user's latest message. This leverages the model's natural attention bias toward beginnings and endings.

You can also improve attention by repetition and emphasis. If a requirement is critical, mention it multiple times in different parts of the context. Include it in the system prompt, include it in the task description, and include it in recent conversation turns. This redundancy increases the likelihood that the model will attend to it. You can also use formatting to draw attention: use headings, bullet points, or markers like "IMPORTANT" or "CRITICAL" to signal that certain content should not be ignored. While this does not guarantee attention, it improves the odds.

Another technique is to use explicit instructions that reference earlier context. Instead of assuming the model will remember a requirement from ten thousand tokens ago, explicitly remind it: "As we discussed earlier, the authentication module must support two-factor authentication." This refreshes the model's attention on the requirement without needing to reproduce the entire original discussion. Explicit references are especially valuable for long-running tasks where critical information may have fallen out of immediate attention.

You should also monitor whether the agent is actually using context. If you provide retrieved memories and the agent never references them, they are wasting space. If you keep old conversation turns in context and the agent never refers back to them, they should be summarized or evicted. Attention monitoring is difficult—most model APIs do not expose attention weights—but you can use heuristics like explicit citations in the agent's output. If the agent says "based on the earlier discussion," you know it is attending to old context. If it never references old context, it probably is not attending to it.

## Multi-Turn Context Rotation: Clearing and Rebuilding

For tasks that span dozens or hundreds of turns, even aggressive context management may not be enough. The context window will eventually fill, and you will hit a hard limit. When this happens, you need a strategy for context rotation: clearing the context window and rebuilding it with a fresh, compact representation of the agent's state.

Context rotation works by checkpointing the current state, as discussed earlier, then clearing the context and starting a new session with a summary. The summary contains the task goal, the progress so far, the key decisions made, and any critical information that must be retained. The new session continues from this summary, treating it as the starting point. This allows the agent to run indefinitely: when one context window fills, rotate to a new one.

The challenge is ensuring continuity across rotations. The agent must not feel like it is starting from scratch. The summary must be rich enough that the agent can pick up where it left off without confusion. This requires careful summary construction. You cannot just concatenate the last few conversation turns; you need to distill the entire session into its essential elements. What is the agent trying to accomplish? What has been accomplished already? What is still pending? What constraints or requirements were established? These questions guide summary content.

You should also preserve references to external state. If the agent was working on a specific file, include that filename in the summary. If the agent made a configuration change, include what was changed. If the agent identified a problem, include the problem description. These references allow the agent to reconnect to external state after rotation. Without them, the agent will have a coherent high-level understanding but will lack the concrete details needed to continue work.

Context rotation should be transparent to the user when possible. The user should not need to know that a rotation occurred. The agent's behavior should remain consistent before and after rotation. This requires testing: run long tasks that span multiple rotations and verify that the agent maintains coherence and does not lose track of goals or constraints. If users notice a quality drop after rotation, your summary is insufficient.

## Hybrid Context: Combining Window and Memory Retrieval

The most sophisticated approach to context management combines the context window with external memory retrieval in a hybrid model. Instead of treating the context window as the sole source of information, treat it as a working set: the information the agent is actively using right now. Everything else lives in external memory and is retrieved on demand. This hybrid approach allows the agent to maintain a small, fast context window while having access to a large, slow memory store.

In a hybrid model, the context window contains the system prompt, the current task, the last few conversation turns, and any information that has been explicitly retrieved from memory. When the agent needs information that is not in the working set, it queries external memory, retrieves the relevant results, and injects them into the context. This retrieval happens automatically as part of the agent's reasoning process. The agent might generate a query, execute it against the memory store, receive results, and use those results to inform its next action.

The benefit of this approach is that the context window can remain small and focused, containing only what is immediately relevant. The agent does not need to keep every past conversation in context; it keeps only the current conversation and retrieves past conversations when they are needed. This dramatically reduces context consumption and allows the agent to scale to much longer task durations. The working set might consume only ten thousand tokens, even if the agent has access to millions of tokens worth of memories.

The challenge is retrieval quality. The agent must be able to formulate good queries, and the memory store must return relevant results. If retrieval fails—the query is too vague, the results are irrelevant, the latency is too high—the agent will not have the information it needs and will make mistakes. You need robust retrieval infrastructure, good query generation, and fast memory systems. You also need fallback strategies: if the agent cannot find what it needs in memory, it should ask the user to provide the information rather than proceeding blindly.

Hybrid context also requires the agent to be aware of its own knowledge boundaries. The agent should know what information is in the working set and what information is in external memory. When asked a question, the agent should recognize whether it has the answer in context or whether it needs to retrieve it. This metacognitive awareness is difficult to implement but essential for reliable hybrid systems. Without it, the agent may confidently answer based on incomplete information, not realizing that critical details are in memory but not in context.

## The Future: Infinite Context and Beyond

As of early 2026, model context windows are growing rapidly. Models with two million token context windows are available, and there is ongoing research into models with effectively unbounded context through hierarchical attention, memory-augmented architectures, and retrieval-integrated models. These advances will change the calculus of context window management. If context is effectively infinite, many of the constraints discussed in this subchapter become less pressing.

But infinite context does not eliminate the need for management. Even with a million-token context window, you still need to decide what information to include, how to organize it, and how to ensure the model attends to relevant parts. Attention span problems persist and may even worsen as context grows. A model with access to a million tokens may struggle to use all of them effectively. You still need prioritization, summarization, and structure.

Moreover, infinite context does not come for free. Larger contexts mean longer processing times, higher costs, and greater latency. A model that can handle two million tokens may take minutes to process them, which is unacceptable for real-time conversational agents. You may have the capacity to include everything in context, but you may not want to for performance reasons. Selective inclusion remains valuable even when capacity constraints are lifted.

The future of context management is likely hybrid: models with very large context windows combined with sophisticated retrieval systems and memory architectures. The context window holds the working set, memory retrieval provides access to broader knowledge, and intelligent orchestration decides what goes where. The agents that master this hybrid approach will be the ones that thrive in the era of long-running, complex, multi-session tasks. The agents that treat context as an infinite dumping ground will waste resources and deliver poor performance. Context management will remain an essential skill, even as the constraints evolve.
