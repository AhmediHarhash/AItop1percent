# Chapter 9 — Agent Testing, Debugging, and Observability

Agents fail in ways that traditional software does not. They make twenty decisions before producing an output. They adapt to feedback mid-execution. They loop, retry, and sometimes hallucinate success. You cannot debug them with print statements.

Testing and observability for agents means capturing the entire decision graph: what the agent planned, what tools it called, what it observed, how it adapted, and why it stopped. It means replaying sessions to reproduce bugs. It means understanding cost attribution across multi-step workflows. It means detecting drift before it becomes a production incident.

This chapter teaches you how to build a testing and observability stack for agents: from local replay to production tracing, from manual debugging to automated drift detection. You'll learn how to use tools like LangSmith, Braintrust, Langfuse, and Arize, when to rely on OpenTelemetry, and how to close the loop from observation to improvement.

---

## What This Chapter Covers

- **9.1** — Agent Testing Strategy: Unit, Integration, and End-to-End
- **9.2** — Test Environments for Agentic Systems
- **9.3** — Trace Analysis: Reading Agent Decision Graphs
- **9.4** — Debugging Tools: LangSmith, Braintrust, Langfuse, Arize
- **9.5** — Production Observability for Agents
- **9.6** — Latency Profiling: Where Time Is Spent in Multi-Step Workflows
- **9.7** — Cost Attribution Across Agent Episodes
- **9.8** — Drift Detection: Behavior Changes Over Time
- **9.9** — Incident Response for Agent Failures
- **9.10** — Replay and Simulation for Debugging
- **9.11** — OpenTelemetry for Agent Tracing
- **9.12** — Observability Anti-Patterns That Hide Failures
- **9.13** — Closing the Loop: From Observation to Improvement
- **9.14** — Simulation Realism: Making Test Environments Representative
- **9.15** — Scenario Banks for Regression Testing

---

*Let's start with the testing strategy that catches failures before they reach production.*
