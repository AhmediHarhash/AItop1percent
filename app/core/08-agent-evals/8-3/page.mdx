# 8.3 — Planning Guardrails: Constraining What Agents Can Decide

In March 2025, a financial services technology company deployed an agentic workflow system designed to help customer support teams resolve account issues. The agent could analyze customer problems, plan multi-step solutions, and coordinate actions across multiple backend systems. Within two weeks, the team discovered that the agent had approved seventeen account closure requests that should have required manual review. The closures were technically valid according to the narrow criteria the agent evaluated, but all seventeen cases involved customers with active loans, pending disputes, or recent fraud alerts. The cleanup required sixty-three hours of manual case review, $47,000 in expedited reinstatement costs, and regulatory reporting to state banking authorities. The root cause was not a bug in the agent's execution logic. The agent did exactly what it was designed to do: it planned solutions, evaluated options, and selected actions. The problem was that no one had constrained what kinds of decisions the agent was allowed to make. The system treated all plans as equally valid as long as they solved the immediate problem, with no understanding of which decisions required human judgment, regulatory compliance review, or cross-functional approval.

Planning guardrails are the constraints you place on what an agent is allowed to decide during its reasoning process. Unlike output validation, which checks the final result of a completed task, planning guardrails intervene during the agent's decision-making process itself. They prevent the agent from even considering certain actions, routes, or strategies. They define the boundaries of autonomous decision-making and the points at which human judgment becomes mandatory. Without planning guardrails, agents optimize for the wrong objectives, make decisions outside their domain of competence, and create compliance risks that surface only after significant harm has occurred.

## The Three Layers of Planning Constraint

Planning guardrails operate at three distinct layers, each addressing a different type of decision risk. Understanding these layers helps you design constraint systems that catch problems early without creating so much friction that the agent becomes useless.

The first layer is **action-level constraints**, which prevent specific operations regardless of context. These are hard boundaries: the agent cannot delete production databases, cannot modify user permissions, cannot send messages to customers without approval, cannot access systems outside its defined scope. Action-level constraints are binary. You do not give the agent conditional access to dangerous operations. You do not allow deletion "if the agent is confident." You define operations that are always prohibited, and you enforce those prohibitions at the infrastructure layer, not just in the prompt. When a financial services agent plans a workflow, it cannot include "close customer account" as a step it executes autonomously. When a support agent plans a resolution, it cannot include "issue refund over $500" without human approval. When a data analysis agent explores a dataset, it cannot include "drop table" in any query plan. These constraints are not negotiable based on the situation.

The second layer is **domain-level constraints**, which prevent the agent from planning solutions outside its area of expertise or authority. An agent designed to troubleshoot technical issues should not plan solutions that involve billing adjustments, account access changes, or policy exceptions. An agent designed to analyze sales data should not plan solutions that involve modifying customer records or initiating outbound communications. Domain-level constraints recognize that agents have bounded competence. The agent might be excellent at diagnosing API errors, but that does not mean it understands the financial implications of waiving integration fees. The agent might be very good at identifying usage patterns, but that does not mean it should decide which customers to contact about upgrades. Domain constraints prevent scope creep during planning. They force the agent to recognize when it has reached the edge of its expertise and needs to hand off to a different system or human.

The third layer is **consequence-level constraints**, which prevent the agent from planning actions that cross certain impact thresholds, even if those actions are technically within its domain. An agent might be authorized to process refunds, but not refunds that exceed a dollar threshold or refund rates that suggest systemic product issues. An agent might be authorized to modify infrastructure configurations, but not changes that affect production traffic or redundancy guarantees. An agent might be authorized to respond to customer inquiries, but not inquiries that involve legal threats, regulatory complaints, or executive escalations. Consequence-level constraints recognize that authority is not binary. The agent has autonomy up to a certain impact level, beyond which human judgment is required regardless of technical correctness.

These three layers work together. Action-level constraints prevent entire categories of operations. Domain-level constraints prevent planning outside areas of expertise. Consequence-level constraints prevent high-impact decisions even within authorized domains. You need all three. Action constraints alone allow the agent to make terrible decisions using permitted operations. Domain constraints alone allow the agent to cause significant harm within its domain. Consequence constraints alone force you to enumerate every possible bad outcome, which is impossible.

## Expressing Constraints in the Planning Context

The agent needs to know about planning constraints during its reasoning process, not after it has already committed to a plan. This means constraints must be part of the planning context: the information the agent considers when generating candidate solutions.

The most effective approach is a **structured constraint declaration** that appears in the agent's system prompt and is reinforced with every planning request. This declaration lists prohibited actions explicitly, defines domain boundaries clearly, and specifies impact thresholds numerically. For action-level constraints, you provide an explicit list: "You cannot delete resources. You cannot modify user permissions. You cannot access billing systems. You cannot send messages to users without approval." This is not subtle. You do not rely on the agent inferring boundaries from examples. You state the boundaries as direct prohibitions.

For domain-level constraints, you define the agent's scope positively and then explicitly mark adjacent domains as out of bounds. "Your domain is technical troubleshooting of API integration issues. You analyze error logs, test connectivity, verify credentials, and recommend configuration changes. You do not make decisions about billing, account status, contract terms, service level agreements, or customer communication. When an issue involves these domains, you document your findings and hand off to the appropriate team." This framing gives the agent a clear picture of where its authority ends. It reduces the risk that the agent tries to "helpfully" solve problems outside its competence.

For consequence-level constraints, you specify numerical thresholds and escalation triggers. "You can approve refunds up to $200 without human review. Refunds between $200 and $1,000 require approval from the support lead. Refunds over $1,000 require approval from finance. If you identify more than three refund requests in a single day for the same product or issue, escalate to product management before processing any of them." These thresholds are not suggestions. They are hard limits that the agent must check before including high-impact actions in its plan.

You also need to handle constraint violations explicitly in the agent's planning loop. When the agent generates a plan that violates a constraint, the system must detect the violation and provide corrective feedback before execution. This is not a post-execution validation that fails after the fact. This is an in-planning check that says "your proposed plan includes a prohibited action: delete database. Revise your plan to achieve the goal without this action, or escalate if the goal cannot be achieved within your constraints." The agent then regenerates the plan with the constraint violation removed. This iterative constraint enforcement teaches the agent to internalize boundaries over time, especially if you are using few-shot examples that show both constraint violations and successful revisions.

## Detecting Constraint Violations During Planning

Detecting violations requires parsing the agent's plan representation and checking each proposed action against your constraint rules. The implementation depends on how your agent structures its plans.

If your agent outputs plans as structured data—lists of actions, API calls, tool invocations—you can validate each action against a constraint rule set before allowing execution. Each action has a type, parameters, and target resource. You check the action type against your prohibited action list. You check the target resource against domain boundaries. You check the parameters against impact thresholds. If any check fails, you reject the plan and ask the agent to revise. This approach works well with tool-using agents where plans are represented as sequences of tool calls. Each tool call is validated against constraints before invocation.

If your agent outputs plans as natural language reasoning—paragraphs explaining what it will do and why—you need a secondary model or rule-based parser to extract proposed actions from the prose and validate them. This is less reliable because natural language is ambiguous, but it is often necessary with agents that use chain-of-thought planning. You can prompt a second model to read the plan and answer: "Does this plan include any of the following prohibited actions: deleting resources, modifying user permissions, accessing billing systems, sending messages to users? Does this plan involve decisions outside the agent's defined domain of technical troubleshooting? Does this plan propose actions that exceed the following thresholds: refunds over $200, configuration changes affecting production, customer communications involving legal or regulatory issues?" The secondary model acts as a constraint classifier. It is not perfect, but it catches most violations before execution.

You can also implement constraint checking as part of the agent's tool definitions. Each tool declares its own constraint metadata: required permissions, domain restrictions, impact thresholds. When the agent proposes using a tool, the tool's constraint metadata is checked against the current context. If the agent tries to use the "delete resource" tool, the tool itself returns an error: "This operation is prohibited by planning constraints. You do not have permission to delete resources. Revise your plan." This pushes constraint enforcement down to the tool layer, which is more robust because it cannot be bypassed by clever prompting or reasoning.

The best implementations use multiple layers of detection. Constraints are declared in the prompt. Constraints are checked by a secondary validation model. Constraints are enforced by the tools themselves. This defense-in-depth approach ensures that even if one layer fails—the agent ignores prompt instructions, the validation model misses a violation—the tool layer still prevents execution. You do not rely on the agent's obedience. You enforce constraints structurally.

## Handling Constraint Conflicts and Edge Cases

Planning guardrails inevitably create situations where the agent cannot achieve its goal without violating a constraint. The agent is asked to resolve a customer issue, but the resolution requires a billing adjustment that is outside its domain. The agent is asked to optimize a database query, but the optimal solution involves schema changes that are prohibited. The agent is asked to respond to a customer inquiry, but the inquiry involves a legal complaint that exceeds its consequence threshold. How the agent handles these conflicts determines whether your guardrails are effective or just frustrating.

The correct behavior is **escalation with context**. The agent does not attempt to work around the constraint. It does not produce a suboptimal plan that technically complies but fails to solve the problem. It does not simply report "I cannot do this" without explanation. Instead, it documents what it tried to do, explains which constraint prevented the solution, and provides enough context for a human or specialized system to take over. "I analyzed the customer's issue and determined that the root cause is an incorrect billing tier assignment. Resolving this requires changing the customer's subscription plan, which is outside my domain of technical troubleshooting. I am escalating this issue to the billing team with the following context: customer ID, current plan, expected plan, date of incorrect assignment, and steps I have already taken to verify the technical integration is functioning correctly."

This escalation-with-context pattern requires the agent to recognize constraint boundaries proactively. You train this behavior by including examples in the agent's few-shot prompt: "Here is an example of a situation where the optimal solution violated domain constraints. Notice how the agent identified the constraint violation, documented its analysis, and escalated with full context rather than attempting a workaround." You also provide explicit instructions: "If you determine that the best solution to a problem requires an action outside your domain or above your authority threshold, do not attempt a partial solution. Instead, escalate to the appropriate team with a clear explanation of why the escalation is necessary and what information the receiving team needs to resolve the issue."

You also need to handle situations where constraints are genuinely too restrictive and the agent cannot operate effectively. If your agent escalates every single request because your domain boundaries are too narrow, you have not built an autonomous agent—you have built an expensive ticket classifier. This is a sign that your constraints do not match the actual scope of problems the agent is supposed to solve. The fix is not to weaken the constraints. The fix is to narrow the agent's responsibilities to match its constrained authority, or to expand its authority to match its responsibilities. If you want the agent to resolve customer billing issues end-to-end, it needs domain authority over billing actions. If you cannot grant that authority, the agent should not be responsible for end-to-end resolution—it should be responsible for triage and escalation only.

Similarly, if your impact thresholds are so low that the agent escalates even trivial decisions, you have overconstrained the system. A $50 refund threshold might make sense for a high-volume consumer product, but it is absurd for an enterprise SaaS platform where average contract values are in the tens of thousands. Thresholds must be calibrated to the actual risk profile of the domain. You set thresholds by analyzing historical data: what percentage of actions at each impact level have resulted in problems? What is the cost of human review versus the cost of occasional errors? You adjust thresholds until escalation rates are sustainable—high enough to catch genuinely risky decisions, low enough that humans can actually review escalated cases.

## Constraint Auditability and Refinement

Planning constraints are not static rules you set once and forget. They are hypotheses about where autonomous decision-making becomes too risky. You refine these hypotheses based on operational data: how often constraints are violated, how often they prevent actual harm, how often they block legitimate actions.

Every constraint violation should be logged with full context: what the agent was trying to do, which constraint was triggered, what the agent did instead, what the eventual outcome was. This log becomes your primary source of truth for evaluating whether your constraints are well-calibrated. You review these logs weekly, looking for patterns. Are you seeing repeated violations of the same constraint by agents attempting the same task? That suggests the task requirements do not match the agent's authority—you need to either change the task or expand the authority. Are you seeing constraint violations that did not actually prevent harm because the proposed action was fine in context? That suggests your constraint is too broad—you need to refine it with additional context or conditions. Are you seeing harmful actions that did not trigger any constraint violation? That suggests a gap in your constraint coverage—you need to add a new constraint.

You also track escalations separately from violations. An escalation is not a failure—it is the agent correctly recognizing the boundary of its authority. But escalation patterns tell you whether your boundaries are in the right place. If 60 percent of customer support cases escalate because they involve billing questions, and billing questions are out of domain, you either need to give the agent billing domain authority or stop routing billing questions to it. If 40 percent of refund requests escalate because they exceed your $200 threshold, and manual review approves 95 percent of them, your threshold is too conservative. If only 5 percent of refund requests escalate and manual review rejects 30 percent of them, your threshold is well-calibrated.

Constraint refinement is a collaborative process between the team that operates the agent and the team that owns risk and compliance. Operators know which constraints create friction and which are ignored in practice. Risk and compliance teams know which constraints prevent actual harm and which are theoretical. You bring both perspectives together in a regular constraint review meeting. You look at violation logs, escalation rates, and harm incidents. You propose changes: tighten this constraint because we saw three incidents, loosen this one because it blocks 90 percent of legitimate actions, add this new constraint because we identified a gap. You test changes in a staging environment or with a small percentage of traffic before rolling them out broadly.

You also version your constraints and tie them to the agent deployment version. When you change a constraint, you deploy it as a new configuration version, and you can roll back if the change causes problems. This is especially important for consequence-level thresholds, which you might need to adjust frequently as your product, customer base, or risk tolerance evolves. The $200 refund threshold that made sense when you had 500 customers might not make sense when you have 50,000. You need the ability to update thresholds without redeploying the entire agent system.

## Communicating Constraints to Stakeholders

Planning guardrails are technical controls, but they are also policy decisions. They encode judgments about what the organization is willing to let an automated system decide. This means non-technical stakeholders—product managers, compliance officers, legal teams, executives—need to understand what constraints exist and why.

You document constraints in a **guardrail policy document** that is separate from the agent's technical implementation. This document lists each constraint, explains its rationale, specifies who approved it, and defines the process for changing it. For action-level constraints, the document explains which operations are prohibited and why: "Agents cannot delete production resources because accidental deletion has caused multiple outages in the past, and recovery requires significant engineering effort and customer communication." For domain-level constraints, the document explains the boundaries of agent authority and the rationale for those boundaries: "Support agents can troubleshoot technical issues but cannot make billing decisions because billing decisions require understanding of contract terms, payment histories, and revenue recognition rules that are outside the scope of technical support training." For consequence-level constraints, the document specifies thresholds and explains how they were calibrated: "Refund approvals over $200 require human review because historical data shows that 15 percent of refunds in this range involve disputes, fraud, or policy exceptions that require judgment beyond the agent's training."

This document is not technical. It does not describe prompt engineering or validation logic. It describes what the agent is and is not allowed to decide, in terms that product managers and compliance officers understand. It becomes the shared reference point for discussions about expanding or tightening constraints. When a product manager asks why the agent cannot handle a certain class of requests, you point to the guardrail policy: "That request type involves decisions outside the agent's domain, as defined in section 3.2." When a compliance officer asks how you prevent the agent from making high-risk decisions, you point to the consequence-level constraints: "All decisions above these thresholds require human approval, as documented in section 4.1."

You also communicate constraint violations and escalations to stakeholders through regular reporting. Every week, you send a summary: how many planning requests the agent handled, how many triggered constraint violations, how many escalated, how many escalations were approved versus rejected by humans, how many incidents involved constraint failures. This reporting builds trust. It shows that the guardrails are actually working, that the agent is respecting boundaries, and that you have visibility into edge cases. It also surfaces opportunities for improvement. If stakeholders see that 40 percent of requests escalate for the same reason, they are more likely to support expanding the agent's authority or redesigning the workflow to route those requests elsewhere from the start.

Planning guardrails are how you give an agent autonomy without giving it unlimited authority. They let you deploy agents in high-stakes environments where mistakes have real consequences, because you have constrained the decisions the agent can make. The next challenge is ensuring that even within those constraints, the tools the agent uses are safe to execute—which requires a different set of guardrails around tool invocation and parameter validation.
