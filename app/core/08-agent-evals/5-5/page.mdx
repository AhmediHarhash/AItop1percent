# 5.5 â€” Agent Communication: Message Passing and Shared State

Shared state means agents implicitly coordinate by reading and writing common data, which works beautifully in testing and collapses into race conditions at scale. In March 2025, a fintech startup called Meridian Flow deployed a multi-agent system to process loan applications across twelve different specialized agents. Each agent was a separate Claude instance with specific prompting and tool access. The system worked beautifully in testing, processing applications in under ninety seconds with impressive accuracy. On launch day, they onboarded their first major client, a regional bank bringing over 2,400 applications in the first week. Within three hours, the system ground to a halt. Applications were stuck in limbo, some agents were processing the same application multiple times, and the engineering team discovered agents were overwriting each other's intermediate results in their shared PostgreSQL database.

Multi-agent systems present a fundamental architectural choice that shapes everything about how your system behaves, scales, and fails. When you have multiple AI agents working together, they need to coordinate their work, share information, and understand what the others are doing. This coordination happens through communication, and you have two primary paradigms to choose from: message passing and shared state. Message passing means agents send structured messages to each other, explicitly communicating their intent, results, and status through discrete packets of information. Shared state means agents read from and write to a common data store, implicitly coordinating by observing and modifying shared data structures. Neither approach is universally better; each carries distinct tradeoffs that become brutally apparent at scale.

The difference between these paradigms is like the difference between a group of people collaborating via email versus collaborating on a shared Google Doc. With email, every communication is explicit and traceable; you know who said what and when, but you end up with complex threading and potential message overload. With a shared document, everyone can see the current state instantly, but you lose clarity about who changed what, when, and why. The same dynamics play out in multi-agent systems, amplified by the fact that AI agents operate at machine speed with machine volumes.

## The Case for Message Passing

Message passing creates explicit communication channels between agents. When Agent A completes credit scoring, it does not just write the score to a database and hope Agent B notices; it sends a structured message directly to Agent B saying "credit score complete, score is 720, confidence 0.89, proceed to fraud detection." This explicitness brings clarity and traceability. You can log every message, inspect message flows, understand exactly what information moved between agents, and replay sequences to debug problems. Message passing decouples agents from each other; Agent A does not need to know where Agent B stores its data or what database schema it uses. It just sends a message to an address, and the messaging infrastructure handles delivery.

The typical implementation uses a message broker or queue system like RabbitMQ, AWS SQS, Google Pub/Sub, or Azure Service Bus. Each agent subscribes to specific message topics or queues, receives messages, processes them, and publishes new messages for downstream agents. The message itself is a structured data packet, typically JSON, containing several key elements. First, the message type, which identifies what kind of communication this is: task assignment, result notification, status update, error report, or coordination signal. Second, the payload, which contains the actual data being communicated: the credit score, the fraud assessment, the extracted entities. Third, metadata about the message itself: unique message ID, timestamp, source agent ID, destination agent ID, correlation ID linking this message to a specific workflow instance, priority level, and expiration time if the message becomes stale.

A well-designed message might look like this in structure: type is "CREDIT_SCORE_COMPLETE", payload contains application ID 847392, applicant name, score value, score confidence, score factors, and raw credit bureau data. Metadata includes message ID, creation timestamp, source agent "credit-scoring-agent-03", destination agent "fraud-detection-agent", correlation ID linking back to the original application submission, priority level normal, and no expiration. This structure gives the receiving agent everything it needs to process the message in context.

Message passing handles several critical challenges gracefully. First, decoupling: agents can be developed, deployed, and scaled independently as long as they maintain message format compatibility. You can upgrade your fraud detection agent without touching your credit scoring agent. Second, asynchrony: agents do not need to wait for each other synchronously; they send messages and move on to the next task, improving throughput and resource utilization. Third, resilience: if an agent crashes, messages wait in the queue until the agent recovers or a replacement instance picks them up. Fourth, observability: every message is an observable event that you can log, trace, and analyze to understand system behavior.

But message passing introduces its own complexity. You now have a distributed system with all the failure modes that entails. Messages can be delayed, duplicated, or lost entirely despite best efforts by the messaging infrastructure. Your agents must handle duplicate messages idempotently, meaning processing the same message twice produces the same result as processing it once. They must handle out-of-order messages; if Agent C expects messages from both Agent A and Agent B, those messages might arrive in any order, and Agent C needs logic to wait for both before proceeding. They must handle lost messages with timeouts and retry logic; if Agent D expects a response within thirty seconds and does not receive it, what should it do?

The message format itself becomes a contract between agents that you must manage carefully. When you evolve your system and need to change message schemas, you face backwards compatibility challenges. If you add a new field to the credit score message, older versions of the fraud detection agent might not recognize it. You need versioning strategies: include a schema version in every message, maintain backwards compatibility for N versions, or run blue-green deployments where you upgrade all agents simultaneously. None of these approaches is easy.

Message ordering is particularly tricky. Most message brokers guarantee messages from a single producer to a single consumer will arrive in order, but they do not guarantee global ordering across multiple producers and consumers. If Agent A sends messages M1 and M2 in sequence, and Agent B sends messages M3 and M4 in sequence, Agent C might receive them as M1, M3, M2, M4. If the messages have dependencies, this creates problems. Your options are to use sequence numbers and have receiving agents buffer and reorder messages, use a broker that supports strict ordering at the cost of performance, or design your system so message order does not matter, which requires careful thinking about state transitions.

Message loss is rare but real. Even with durable queues and replication, a sufficiently catastrophic infrastructure failure can lose messages. Your system needs to detect and recover from this. Common patterns include acknowledgment messages where Agent B confirms receipt of Agent A's message, and if Agent A does not receive acknowledgment within a timeout, it resends. Or end-to-end workflow tracking where a coordinator agent monitors the entire flow and detects when expected messages do not arrive. Or idempotent operations where losing a message is equivalent to the operation not being requested, and higher-level retry logic handles it.

Message duplication is more common. Network issues, agent restarts, and retry logic all conspire to deliver the same message multiple times. Every agent must implement idempotency, which typically means recording processed message IDs in durable storage and checking each incoming message against this record. If you have already processed message ID 847392-credit-score, you acknowledge it again but do not reprocess it. This requires persistent storage, which brings you partway back to shared state.

## The Case for Shared State

Shared state takes a fundamentally different approach: agents coordinate by reading from and writing to common data structures. In the Meridian Flow example, they used a PostgreSQL database where each agent queried for applications ready for its processing stage, performed its work, wrote results back to the database, and updated status fields to indicate completion. This feels simpler at first glance because there is no message broker to operate, no message schemas to version, and no message loss or duplication to handle. Agents just interact with data directly.

The canonical shared state implementation is a database, often relational like PostgreSQL or MySQL, sometimes document-oriented like MongoDB, occasionally key-value stores like Redis or DynamoDB. The schema represents the workflow state: an applications table with columns for every intermediate result credit score, fraud assessment, income verification status, and so on plus status columns indicating which stages are complete. Each agent runs a loop: query for applications where my prerequisites are complete and my stage is not, process one, write my results and update my status flag, repeat.

This works well for simple workflows with light load. It is easy to understand, easy to debug by looking at database contents, and easy to add new agents by adding new columns and status flags. The database provides consistency; if two agents query the same application simultaneously, database transactions ensure they do not corrupt each other's updates. The database provides durability; if an agent crashes mid-processing, the partial state is saved, and you can implement recovery logic based on status flags.

But shared state creates problems that grow exponentially with scale. The first problem is contention. Every agent is querying and updating the same database, creating lock contention and serialization bottlenecks. When Meridian Flow hit 2,400 applications, they had twelve agent types running four instances each, all hammering the applications table. PostgreSQL's row-level locking meant agents blocked each other, throughput collapsed, and database CPU hit 100 percent. The second problem is polling overhead. Agents continuously query "are there applications ready for me" even when the answer is usually no, wasting database resources and agent time. The third problem is implicit coordination. When Agent A updates the credit score column and sets credit-score-complete to true, Agent B only discovers this when it next polls the database. There is no explicit notification, just eventual discovery through repeated queries.

The fourth problem is data model coupling. Every agent needs to understand the entire database schema, or at least the portions relevant to its work. When you add a new agent type or change how an existing agent works, you often need schema migrations that affect the entire system. This tight coupling makes independent development and deployment difficult. The fifth problem is race conditions. Even with database transactions, complex workflows create subtle races. If Agent A and Agent B both need to update different parts of the same application record, they might read stale data, make decisions based on it, and overwrite each other's changes. Optimistic locking with version numbers helps but adds complexity.

Despite these problems, shared state has legitimate use cases. For workflows with low concurrency, where agents process serially rather than in parallel, shared state is simpler and works fine. For workflows where every agent needs access to the complete current state of an entity, shared state provides that naturally while message passing would require complex state reconstruction from message histories. For workflows where humans need to inspect and modify in-flight work, a database provides a natural interface while message queues are opaque.

A middle-ground approach is the blackboard pattern, borrowed from classical AI research. A blackboard is a shared data structure where agents post partial results and observations, and other agents read these to inform their processing. Unlike a relational database with a fixed schema, a blackboard is more free-form, often implemented as a key-value store or document database. For the loan application workflow, the blackboard might be a Redis hash or a MongoDB document where the credit scoring agent writes credit-score-result with its score object, the fraud detection agent writes fraud-assessment with its assessment object, and so on. Agents subscribe to notifications when relevant keys are updated, avoiding polling overhead.

The blackboard pattern preserves the simplicity of shared state while addressing some of its problems. Agents do not need to understand the full schema, just the keys they care about. Notifications reduce polling overhead. Fine-grained key-level operations reduce contention compared to monolithic database rows. But you still have coupling agents must agree on key names and value formats and you still have potential consistency issues if multiple agents update related keys.

## Event-Driven vs Polling Patterns

Layered on top of message passing and shared state is the question of how agents learn about new work: event-driven notification or polling. In event-driven systems, when something happens that an agent cares about, that agent is immediately notified and can react. In polling systems, agents periodically check whether something happened and react if it did. Event-driven is more responsive and more efficient, but requires more infrastructure and complexity. Polling is simpler but wastes resources and adds latency.

With message passing, event-driven is natural: agents subscribe to message queues or topics, and the message broker pushes messages to them as they arrive. The agent sits idle until a message arrives, then springs into action. This is maximally efficient; no CPU is wasted polling, and reaction time is milliseconds. The challenge is handling message bursts when messages arrive faster than the agent can process them, queues fill up, and you need backpressure mechanisms to avoid overwhelming downstream agents.

With shared state, event-driven requires additional infrastructure. Relational databases offer triggers and notifications for example, PostgreSQL's LISTEN/NOTIFY that can alert agents when relevant data changes. Document databases offer change streams MongoDB change streams, DynamoDB streams that emit events when documents are modified. Key-value stores offer pub/sub systems Redis pub/sub for notifications. These mechanisms let you build event-driven systems on shared state, getting the simplicity of a database with the responsiveness of message passing. The downside is complexity: you are now running both a database and an event notification system, and you need to ensure consistency between them.

Polling is the fallback when event-driven is not feasible or is not worth the complexity. Agents run a loop: sleep for N seconds, query for new work, process any work found, repeat. The sleep interval is a tradeoff between responsiveness and overhead. Sleep for one second and you get near-real-time reaction but constant database queries. Sleep for sixty seconds and you get low overhead but potential minute-long delays. Adaptive polling adjusts the interval based on workload: if you found work last time, poll again immediately; if you did not, back off exponentially up to some maximum interval. This balances responsiveness during busy periods with efficiency during quiet periods.

A hybrid approach uses long polling, where the query for new work does not return immediately if there is no work but instead waits up to some timeout for work to appear. This is how SQS works: you make a ReceiveMessage call with a wait time of twenty seconds, and if no messages are available, the call blocks for up to twenty seconds waiting for messages to arrive. If messages arrive during that time, they are returned immediately. If the timeout elapses with no messages, the call returns empty, and you immediately make another long-polling call. This gives you event-driven responsiveness with polling simplicity; no separate notification infrastructure is needed.

## Message Formats and Protocols

When you commit to message passing, the structure and semantics of your messages become critical. A message format is more than just JSON or Protocol Buffers; it is a contract that defines how agents communicate. The format must be expressive enough to convey all necessary information, stable enough to avoid constant breaking changes, and flexible enough to evolve over time. The best message formats strike a balance between human readability for debugging and machine efficiency for processing.

Every production message passing system uses versioned schemas. The simplest approach is to include a schema version field in every message. When Agent A sends a message, it includes "schema_version: 2" in the metadata. When Agent B receives it, it checks the version and uses the appropriate parsing and validation logic. This allows you to evolve message formats without breaking older agents. You can maintain parsing code for multiple versions simultaneously, giving you time to upgrade all agents gradually. A logistics company in 2025 maintained three concurrent schema versions for their shipment coordination messages, allowing them to roll out changes across 40 agents over two weeks without downtime.

Message types create semantic structure. Instead of every message being a generic blob of JSON, you define explicit types: TaskAssignment, TaskComplete, TaskFailed, StatusUpdate, DataRequest, DataResponse. Each type has a defined schema and semantic meaning. When Agent A receives a TaskAssignment message, it knows exactly what fields to expect and what action to take. Type safety prevents entire classes of bugs. A healthcare company in early 2026 used TypeScript interfaces to define their message types and generated validation code that rejected malformed messages at the boundary, preventing bad data from propagating through the system.

Correlation IDs are essential for observability in multi-agent systems. When a user request triggers a workflow that involves five agents, all messages related to that request share the same correlation ID. This lets you trace the entire workflow through your logs. When something goes wrong, you search for the correlation ID and see every message sent, every decision made, and every error encountered. Without correlation IDs, debugging multi-agent systems is nearly impossible. An e-commerce company in 2025 implemented correlation ID tracking and reduced their mean time to resolution for production incidents from four hours to thirty minutes.

Message priorities allow critical work to bypass queues. You assign each message a priority level: critical, high, normal, low. Agents process critical messages first, even if there are hundreds of normal messages waiting. This prevents urgent work from being delayed by backlogs of routine tasks. A fintech company used priority messages for fraud alerts; when a high-risk transaction was detected, a critical-priority message bypassed the normal processing queue and triggered immediate review, reducing fraud response time from minutes to seconds.

Expiration times prevent stale messages from being processed. Some messages become irrelevant after a certain time. A stock trading system might send a message about a price alert, but if that message sits in a queue for five minutes, the price has changed and the alert is meaningless. By including an expiration timestamp, agents can check message age and discard stale messages rather than wasting resources processing obsolete information. A market data company in 2026 set five-second expirations on price alert messages, preventing their agents from acting on outdated market conditions.

Message acknowledgments create reliability. When Agent B receives a message from Agent A, it sends back an acknowledgment message confirming receipt. If Agent A does not receive the acknowledgment within a timeout, it knows something went wrong and can retry or escalate. This turns unreliable messaging into reliable messaging through application-level confirmation. Some message brokers provide this automatically, but implementing it at the application level gives you more control and visibility.

## State Consistency in Multi-Agent Systems

When agents share state, either through a database or through accumulated message histories, maintaining consistency becomes a challenge. Consistency means all agents have a coherent view of the world. If Agent A believes a customer's status is "approved" and Agent B believes it is "pending," your system has a consistency problem that will eventually cause user-visible errors.

The gold standard is strong consistency, where all reads reflect the most recent write. If Agent A writes a value and Agent B immediately reads it, Agent B sees the new value. Relational databases with ACID transactions provide this. When Agent A commits a transaction updating a customer record, that change is immediately visible to all other agents. Strong consistency makes reasoning about system behavior easier; there is one source of truth and everyone sees the same truth. But strong consistency is expensive. It requires coordination between database replicas, which adds latency. It limits scalability because you cannot partition data arbitrarily. A payments company in 2025 used strongly consistent PostgreSQL for account balances, accepting the latency cost because consistency was critical for financial correctness.

Eventual consistency is the alternative: all agents will eventually see the same value, but there may be temporary disagreements. If Agent A writes a value, Agent B might read the old value for a few milliseconds or seconds until the change propagates. Eventual consistency enables higher performance and better scalability. You can replicate data to multiple regions and let agents read from the nearest replica without coordinating writes globally. But eventual consistency requires careful system design. You need to handle cases where Agent B makes a decision based on stale data. A social media company in 2026 used eventual consistency for user profile updates. If a user changed their name, different agents might see the old name for up to one second. This was acceptable because profile updates were not time-critical and the system converged quickly.

Optimistic concurrency control helps manage consistency without locking. Each record has a version number. When Agent A reads the record, it notes the version. When it writes back, it specifies the expected version. If another agent has modified the record in the meantime, the version has changed and the write fails. Agent A detects the conflict and retries. This works well when conflicts are rare. A customer support company in 2025 used optimistic concurrency for ticket updates. Agents read tickets, updated them, and wrote back with version checks. In 99 percent of cases there was no conflict. The 1 percent that conflicted simply retried and succeeded on the second attempt.

Pessimistic locking provides stronger guarantees. Agent A acquires a lock before reading, preventing other agents from modifying the record until the lock is released. This eliminates conflicts but reduces concurrency. If many agents need to access the same record, they queue up waiting for locks. An inventory management system in 2025 used pessimistic locking for stock updates during flash sales. When thousands of orders per second competed for the same inventory items, optimistic concurrency led to excessive retries and timeouts. Pessimistic locking serialized updates and ensured correctness at the cost of throughput.

Consensus protocols like Raft or Paxos provide strong consistency in distributed systems. Multiple agent instances coordinate to elect a leader, and only the leader processes writes. Reads can go to followers. If the leader crashes, followers elect a new leader. This provides fault tolerance and consistency. But consensus protocols are complex to implement and operate. Most teams use managed services like etcd or Consul that implement consensus for you. A real-time bidding system in 2026 used etcd to maintain consistent state about active auctions across twenty agent instances distributed globally.

## Practical Recommendations

When designing agent communication, start with your consistency and latency requirements. If you need strong consistency and can tolerate higher latency, lean toward shared state with ACID transactions. If you need low latency and can tolerate eventual consistency, lean toward message passing with asynchronous communication. If you need auditability and are willing to invest in infrastructure, consider event sourcing where all state changes are captured as immutable events.

For systems with fewer than five agents and predictable load patterns, shared state with polling is often simplest. Use a relational database, have agents poll for work every few seconds, and rely on database transactions for consistency. Instrument your database performance carefully and be prepared to migrate to message passing if you hit scaling limits.

For systems with five to twenty agents, dynamic workloads, or high concurrency, invest in message passing. Use a managed message broker like SQS or Pub/Sub to avoid operating infrastructure. Design clear message schemas with versioning from day one. Implement idempotency in every agent. Use correlation IDs religiously for observability.

For systems with more than twenty agents or complex coordination requirements, consider an orchestration framework like Temporal or Apache Airflow that handles message passing, retries, and state management for you. These frameworks let you focus on agent logic rather than infrastructure.

Regardless of architecture, instrument everything. Log every message sent and received, every database transaction, every state transition. Use distributed tracing to visualize workflows across agents. Build dashboards showing message throughput, queue depths, processing latencies, and error rates. Set up alerts for anomalies. In multi-agent systems, observability is not optional; it is the only way to understand what is happening and debug problems.

Test under realistic load before production. Multi-agent systems have emergent behaviors that only appear at scale. Run load tests with hundreds or thousands of concurrent workflows. Inject failures: kill agents randomly, delay messages, simulate database slowdowns. Verify your system recovers gracefully. A document processing company in 2025 ran weekly chaos experiments where they randomly terminated agent instances during peak load. They discovered and fixed three critical bugs that would have caused production outages.

Finally, remember that communication architecture is not permanent. As your system evolves, your communication needs will change. Build flexibility into your design so you can migrate from shared state to message passing, or from synchronous to asynchronous communication, without rewriting everything. The teams that succeed with multi-agent systems are the ones that treat communication architecture as an ongoing design problem, not a one-time decision.

Communication is the foundation that determines whether your agents work together smoothly or trip over each other. Get it right and your system scales gracefully, recovers from failures, and delivers consistent user experiences. Get it wrong and you end up like Meridian Flow, racing to rebuild while your customers walk away.
