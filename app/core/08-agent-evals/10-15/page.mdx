# 10.15 â€” Resume and Replay Safety: No Duplicate Actions After Restart

Resume is not replay. The agent that restarts mid-execution and faithfully re-executes every action in its history will send duplicate emails, charge duplicate payments, and create duplicate records. If your resume logic does not include idempotency keys for every side-effecting operation, you are one deployment away from a duplicate action incident.

Resume and replay safety is the discipline of ensuring that when an agent session pauses and resumes, whether due to deployment, crash, timeout, or deliberate suspension, no side-effecting action executes more than once unless explicitly intended. This is not a theoretical edge case. Agent systems restart constantly: container orchestration platforms like Kubernetes reschedule pods, serverless functions time out and restart, deployment pipelines roll out new versions, and long-running agents hit resource limits and suspend. If your resume logic replays the action log without guarding against duplicate execution, you will send duplicate emails, charge duplicate payments, create duplicate records, and trigger duplicate alerts. Resume safety requires three architectural commitments: idempotent action execution, deterministic replay with effect isolation, and checkpoint-based resumption that distinguishes between state reconstruction and action re-execution.

## The Replay Idempotency Requirement

Every tool call that produces a side effect must be idempotent at the infrastructure level, not just the agent level. Idempotency means that executing the same operation multiple times produces the same result as executing it once. The agent generates an idempotency key for every action before execution. That key is passed to the tool executor and stored alongside the action result. If the agent restarts and replays the action log, the tool executor checks whether the idempotency key has already been processed. If it has, the executor skips the actual operation and returns the cached result. If it has not, the executor performs the operation, stores the result with the key, and returns the result. This pattern guarantees that even if the agent logic replays the action, the external system sees the operation exactly once.

Consider an agent that books a meeting room and sends a calendar invite. The agent calls book_room with arguments room_id equals conference_a and time_slot equals 2pm. The agent runtime generates an idempotency key based on the session ID, the action sequence number, and a hash of the arguments: session_7x3k_action_042_hash_8f4e. The tool executor checks the idempotency store, sees no record for that key, executes the booking, records the confirmation ID, and stores the result keyed by session_7x3k_action_042_hash_8f4e. Later, the agent runtime restarts and replays the action log. When it reaches action 042, it calls book_room again with the same arguments. The runtime regenerates the same idempotency key, passes it to the executor, and the executor finds the existing record. The executor returns the cached confirmation ID without re-executing the booking. The calendar invite send follows the same pattern. The agent never double-books the room, never sends duplicate invites, and the user sees no disruption.

Idempotency keys must be stable across restarts. The key generation algorithm must depend only on information that survives the restart: session ID, action sequence number, tool name, and argument values. Do not include runtime-specific data like process IDs, timestamps, or random nonces. If the key changes after restart, the idempotency check fails and the operation executes again. Hash the tool name and arguments to produce a deterministic fingerprint. Store that fingerprint alongside the session ID and sequence number. This ensures that identical calls generate identical keys, and non-identical calls generate different keys even if they target the same tool.

Action sequence numbers must be strictly monotonic and assigned before execution, not after. If you assign sequence numbers after execution, a restart between execution and number assignment will cause the next action to reuse the same number, breaking idempotency. Assign the sequence number when the agent decides to take the action, persist the decision to the checkpoint, then execute the action. The sequence number becomes part of the durable record before the side effect occurs. On replay, the agent reads the sequence number from the checkpoint and skips re-deciding, proceeding directly to execution with the already-assigned number. This eliminates the re-assignment hazard.

## Deterministic Replay with Effect Isolation

Replay logic must distinguish between deterministic operations that can safely re-execute and non-deterministic or side-effecting operations that must not. Deterministic operations include state transitions, variable assignments, conditional branches based on existing data, and pure computations. Non-deterministic operations include external API calls, random number generation, timestamp retrieval, and any operation whose result depends on external state that may have changed since the original execution. Side-effecting operations include writes to external systems, sends of messages or notifications, creation of records, and any operation that changes the world outside the agent session.

The replay engine categorizes each action in the log. Pure state operations replay directly because they produce the same result given the same inputs. External reads replay conditionally: if the external state might have changed, the replay engine may choose to re-execute the read to get current data, or it may use the cached result depending on the replay policy. External writes never replay as actual executions; they replay as lookups against the idempotency store. The replay engine reconstructs the agent's internal state by replaying pure operations and reloading cached results for external operations. This ensures that the agent resumes with the correct state without re-executing side effects.

Consider an agent that retrieves user preferences, computes a personalized recommendation, and sends the recommendation via email. The action log contains three entries: get_user_prefs, compute_recommendation, and send_email. On restart, the replay engine processes each entry. For get_user_prefs, the engine checks whether user preferences might have changed. If the policy is strict consistency, the engine re-executes the call to fetch current preferences. If the policy is session consistency, the engine uses the cached result from the original execution. For compute_recommendation, the engine replays the computation deterministically using the preferences from the previous step. For send_email, the engine looks up the idempotency key in the execution store, finds the cached send confirmation, and returns that result without re-sending the email. The agent resumes with the correct recommendation and the knowledge that the email was sent, without actually sending it again.

Replay policies vary by tool and by deployment context. Read-only tools with stable data can replay from cache safely. Read-only tools with volatile data should re-execute to avoid stale state. Write tools must always use idempotency keys and never re-execute the actual write. Configuration tools that change system state should be treated as writes even if they appear read-only; re-applying a configuration change is a write. The agent platform provides a tool registry where each tool declares its replay policy: pure, cacheable_read, volatile_read, or idempotent_write. The replay engine consults the registry and applies the appropriate logic for each action.

## Checkpoint-Based Resumption

Checkpoints are the foundation of safe resumption. A checkpoint is a durable snapshot of the agent's state at a specific point in the action sequence. The state includes the conversation history, the variable bindings, the action log, the tool call results, and the next action pointer. The agent runtime writes a checkpoint after every state-changing operation: after receiving a user message, after executing a tool call, after generating a response, and after updating variables. The checkpoint is stored in durable storage with a sequence number and a timestamp. If the runtime crashes, the resume logic loads the most recent checkpoint and continues from that point.

The checkpoint must include enough information to resume without re-deciding. This means storing not just the result of each action, but also the decision metadata: which tool was chosen, which arguments were selected, which branches were taken, and which reasoning steps led to the decision. If the checkpoint only stores results, the agent must re-run the decision logic on resume, which may produce different decisions if the model has changed or if non-deterministic sampling produces different outputs. Storing the decision metadata allows the agent to replay the exact sequence of actions without re-invoking the language model for decisions that were already made.

Some agent architectures use event sourcing, where the checkpoint is the entire action log from session start to the current point. The resume logic replays the log from the beginning, applying the replay policies described above. This approach provides a complete audit trail and enables time-travel debugging, but it requires careful replay logic to avoid re-executing side effects. Other architectures use state snapshots, where the checkpoint is a serialized copy of the agent's internal state at a specific moment. The resume logic deserializes the snapshot and continues from that state without replaying the log. This approach is faster for long sessions but requires careful serialization to capture all relevant state, including in-flight operations and pending decisions.

Hybrid approaches combine event sourcing with periodic full snapshots. The agent writes a full snapshot every N actions and writes incremental action deltas between snapshots. On resume, the runtime loads the most recent snapshot and replays the deltas since that snapshot. This balances the auditability of event sourcing with the speed of snapshot resumption. The snapshot frequency depends on the action rate and the cost of replay. For agents with high action rates, snapshot every ten to twenty actions. For agents with low action rates, snapshot every session boundary or every user turn.

## Handling Partial Executions and In-Flight Operations

Resume safety becomes complex when the agent crashes mid-operation. Consider an agent that calls a tool, and the tool begins execution but crashes before returning a result. The tool may have completed the side effect, or it may have failed midway. The agent has no record of the result because the crash happened before the result was written to the checkpoint. On resume, the agent sees the action in the log with no result. The naive approach is to retry the operation, but if the tool already completed the side effect, the retry will duplicate it. The safe approach is to use idempotency keys and tool-side result persistence.

Every tool execution begins by writing a pending entry to the execution store with the idempotency key and a status of in_progress. The tool performs the operation and updates the entry to completed with the result, or failed with the error. If the tool crashes mid-execution, the entry remains in_progress. On resume, the agent checks the execution store for the idempotency key. If the entry is completed, the agent uses the cached result. If the entry is in_progress, the agent queries the external system to determine whether the operation completed. For example, if the operation was sending an email, the agent queries the email service to see whether the email was delivered. If it was, the agent marks the entry as completed and caches the delivery confirmation. If it was not, the agent retries the operation.

This pattern requires tools to be queryable: the external system must provide a way to check whether a specific operation completed. Email services provide delivery logs keyed by message ID. Payment processors provide transaction status APIs keyed by transaction ID. Database writes provide transaction logs keyed by transaction ID. If the external system does not provide a query mechanism, the tool must implement its own tracking layer. For example, a file write tool writes a marker file alongside the target file with the idempotency key and the write timestamp. On resume, the tool checks for the marker file. If it exists, the write completed. If it does not, the write did not complete and the tool retries.

Some operations are fundamentally non-idempotent and non-queryable. A random number generator cannot reproduce the same number on retry. A timestamp retrieval returns a different value each time. A web scrape retrieves different content if the page changed. For these operations, the agent must cache the result before using it for decision-making. The agent calls the tool, writes the result to the checkpoint, then proceeds with the decision. On replay, the agent uses the cached result instead of re-executing the tool. This transforms a non-idempotent operation into a deterministic replay by freezing the result at the moment of first execution.

## Distributed Agents and Cross-Session Idempotency

In multi-agent systems, resume safety extends across agents. If Agent A calls Agent B, and Agent B calls a side-effecting tool, and Agent A crashes and resumes, Agent A may retry the call to Agent B. Agent B must recognize the retry and return the cached result without re-executing the tool. This requires cross-agent idempotency keys. Agent A includes its session ID and action sequence number in the call to Agent B. Agent B treats the call as a tool invocation and generates an idempotency key based on the caller's session ID and sequence number. Agent B checks its execution store, finds the cached result from the first call, and returns it without re-executing. This pattern ensures that agent collaboration is resume-safe even when agents restart independently.

Cross-session idempotency applies to agents that span multiple user sessions. A customer support agent may handle a ticket over several days, with the user and agent exchanging messages asynchronously. The agent session persists across these interactions. If the agent runtime restarts between messages, the resume logic must ensure that responses sent in previous sessions do not resend. The agent tracks sent messages by message ID and user session ID. When composing a response, the agent checks whether it has already sent a message with the same content to the same session. If it has, the agent skips the send and moves to the next action. This prevents duplicate notifications when the agent resumes after a long pause.

Multi-region deployments introduce additional complexity. If an agent session runs in one region, checkpoints to a global store, and resumes in a different region, the idempotency store must be globally consistent. The new region's runtime must see the same execution history as the old region. This requires a distributed idempotency store with strong consistency guarantees. DynamoDB with global tables, Cloud Spanner, or CockroachDB provide the necessary consistency. Eventually consistent stores like S3 or replicated Redis do not guarantee that the new region sees all writes from the old region immediately, which can cause duplicate executions during the consistency window. Use strong consistency for idempotency stores in multi-region deployments.

## Testing Resume and Replay Scenarios

Resume safety testing requires simulating crashes at every possible point in the action sequence. The test harness runs an agent session and injects a crash after each action. The harness then resumes the session and verifies that the agent continues correctly without re-executing side effects. For a session with ten actions, the harness runs ten crash-resume cycles and checks that each external tool was called exactly once despite multiple resumes. This exhaustive testing catches edge cases where the idempotency logic fails.

Chaos engineering techniques apply well to resume safety. A chaos agent randomly terminates agent processes during normal operation and monitors for duplicate actions. The chaos agent tracks external API calls using distributed tracing and flags any duplicate call that does not match an idempotency key. This continuous validation catches resume bugs in production before they cause user-visible duplicates. The chaos agent runs in a shadow traffic mode, terminating shadow sessions while leaving production sessions untouched, so that testing does not disrupt real users.

Replay verification tests load a checkpoint and replay the action log, then compare the resulting state to the original state after the checkpoint was written. The states should match exactly. If they do not, the replay logic is non-deterministic or the checkpoint is incomplete. This test catches state drift bugs where the resume logic produces different state than the original execution. Run replay verification tests on every checkpoint written to production. Store the post-checkpoint state hash alongside the checkpoint. On resume, replay the log, compute the state hash, and compare to the stored hash. A mismatch indicates a replay bug.

Idempotency store durability tests verify that idempotency keys persist across failures. The test writes an idempotency record, crashes the store, restarts it, and checks that the record is still present. This ensures that the idempotency store does not lose data during restart. Test both single-node and distributed store configurations. For distributed stores, test network partitions where the write succeeds in one region but not another, and verify that the agent runtime handles the inconsistency safely by retrying or failing the operation rather than assuming success.

## Operational Monitoring for Resume Safety

Production monitoring tracks resume frequency and duplicate action rates. High resume frequency indicates infrastructure instability or aggressive timeout policies. A sudden spike in resumes may signal a deployment issue or a resource exhaustion problem. Track resumes by cause: deployment, timeout, crash, manual suspension, or resource limit. Each cause has different remediation. Deployment-triggered resumes are expected and should cause no duplicates if idempotency is working. Crash-triggered resumes indicate a bug in the agent code or infrastructure. Resource-triggered resumes indicate scaling problems.

Duplicate action detection monitors external systems for repeated operations with the same arguments within a short time window. If the same email sends twice to the same recipient within ten seconds, flag it as a potential duplicate. If the same database write executes twice with the same transaction ID, flag it. Correlate these duplicates with recent resumes. If a duplicate follows a resume, the idempotency logic failed. Alert the on-call engineer and pause further resumes until the issue is resolved. This prevents a small idempotency bug from cascading into thousands of duplicate actions.

Checkpoint lag measures the time between an action execution and the checkpoint write. High lag increases the window of vulnerability: if the runtime crashes during the lag, the action result is lost and the agent may retry. Target checkpoint lag below 100 milliseconds. Asynchronous checkpoint writes help, but they introduce a risk that the checkpoint write fails after the action succeeds. Use two-phase commit or write-ahead logging to ensure that the checkpoint write completes before acknowledging the action to the user. This guarantees that the user never sees an action result that is not yet checkpointed.

Idempotency store hit rate tracks how often resumed actions find existing records in the idempotency store versus executing new operations. A healthy system shows high hit rates after resumes, indicating that most resumed actions are lookups rather than re-executions. A low hit rate suggests that idempotency keys are not stable or that the store is losing data. Monitor hit rate by tool and by session. Tools with low hit rates may have bugs in their key generation logic. Sessions with low hit rates may have non-deterministic argument generation that breaks key stability.

The next subchapter examines the full lifecycle of long-running agents, including suspend, resume, and cancel operations, and the user experience patterns that make these operations safe and predictable.
