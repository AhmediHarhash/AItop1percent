# 1.5 â€” The Agent Execution Loop: Observe, Reason, Plan, Act, Reflect

Every agent follows the same execution loop: observe the environment, reason about what the observation means, plan what to do, act on that plan, and reflect on whether the action achieved the intended outcome. Skip any one of these steps and your agent becomes dangerously unreliable. In August 2025, a logistics company called FreightWise deployed an agent to optimize delivery routes in real time based on traffic, weather, and customer priorities. The agent had access to live traffic APIs, weather services, the customer priority database, and the route planning system. In testing with simulated scenarios, it performed brilliantly, rerouting trucks around accidents and weather delays to maintain on-time delivery rates above ninety-five percent. The product team celebrated, the operations team approved for production, and the agent went live managing routes for a fleet of forty-three trucks serving a major metropolitan area. Six hours later, operations pulled the plug after seventeen trucks had been sent on unnecessary detours that collectively wasted forty thousand dollars in fuel and driver hours.

The failure was subtle and insidious. The agent would receive a traffic update indicating a highway closure, correctly reason that affected routes needed replanning, generate a new plan for impacted deliveries, execute route updates in the dispatch system, and log success. The reasoning was sound, the plans were logically coherent, and the actions executed without errors. But the agent never reflected on whether the new routes were actually better than the original routes. It would reroute a truck to avoid a closure on Highway 101, sending it on a detour that added forty minutes, without noticing that the closure was only affecting northbound lanes and the truck was traveling southbound on the same highway. The observation was correct, the closure existed. The reasoning was correct, closures require rerouting. The plan was executable. The action succeeded. But the action made things worse, and the agent never checked.

The engineering team had built the agent loop with four phases instead of five. They observed the environment, reasoned about what the observations meant, planned actions, and executed those actions. They assumed that if the reasoning and planning phases were sound, the actions would be correct. This assumption works for deterministic systems where correct logic guarantees correct outcomes. It fails for agents operating in complex real-world environments where observations can be incomplete, reasoning can be flawed despite appearing sound, and actions can have effects that differ from intentions. Without reflection, the agent had no feedback mechanism to catch misunderstandings, incorrect assumptions, or actions that failed to achieve their intended effects.

By the time the operations team noticed the pattern, they had lost a quarter of their daily margin to unnecessary routing. The failure was not in the agent's capabilities, the model was GPT-5 and reasoning quality was high. The failure was architectural. The loop was incomplete. The agent never asked "did that action improve the situation or make it worse?" It never measured progress toward the goal. It never compared the new route time to the old route time. It just acted based on its reasoning and moved on. The fix required adding a reflection phase that evaluated whether each rerouting decision actually reduced expected delivery time, but by then the damage to the operations team's trust in AI automation was done.

The canonical agent execution loop in 2026 has five phases: observe, reason, plan, act, and reflect. Every production agent implements some version of this loop, though the details vary by archetype and use case. Understanding each phase in depth, knowing what can go wrong in each phase, and designing your agent architecture to handle failures at each stage is the difference between agents that work reliably in production and agents that produce expensive surprises. This loop is not a theoretical model, it is an engineering framework derived from thousands of production deployments and their failure patterns.

## Observation: What the Agent Perceives and How

Observation is the phase where the agent receives inputs and perceives its environment. For a research agent, observation might be receiving a user question and reading the results of search queries. For a workflow agent, observation might be reading the current state of records in multiple systems to determine what has been completed and what remains. For a coding agent, observation might be reading files from a codebase and seeing the output of test runs or compiler errors. For a decision support agent, observation might be retrieving data from business intelligence systems and reading external market information. Observation is not passive data reception, it is active perception with filtering, attention, and interpretation.

The critical insight that shapes observation architecture is that agents do not observe raw reality, they observe representations mediated by tools and APIs. A coding agent does not see your actual codebase as a living evolving system, it sees the text content of files you allow it to read through a file reading tool. A workflow agent does not see the actual state of your CRM as it exists in memory and disk, it sees the JSON responses from API calls that expose a particular view of that state. A research agent does not see the web, it sees search result snippets and retrieved documents. The quality of observation is fundamentally limited by the quality of these representations. If your tools provide incomplete, stale, ambiguous, or misleading information, the agent's observations will be flawed no matter how sophisticated its reasoning.

Observation fails in several predictable ways that you must design against. The agent misses relevant information because it did not know to look for it or because the tools do not expose it. A workflow agent might observe that a customer record exists but miss that it is marked as inactive because the API endpoint it called does not include status fields. The agent reasons and acts based on incomplete information. The agent receives information but fails to parse or interpret it correctly. A coding agent sees an error message that is formatted inconsistently with its training data and interprets it as a success message. The agent observes stale data and treats it as current. A decision support agent retrieves sales figures from a cache that is three hours old and makes recommendations based on outdated conditions.

The agent is overwhelmed by too much information and cannot identify what matters. A research agent retrieves fifty documents in response to a query and tries to read all of them in depth, exceeding context limits or spending so much time on reading that it cannot synthesize. The agent observes error states or edge cases that were not represented in its training data and defaults to ignoring them or misinterpreting them. An API returns a 429 rate limit error and the agent treats it as a permanent failure rather than a temporary condition that should trigger backoff and retry.

The architecture mitigations for observation failures start with tool design. Your tools must provide complete, accurate, and timely information with clear semantics. If a tool returns a timestamp, the agent must be able to interpret what that timestamp means. Is it creation time, modification time, or access time? Is it in UTC or local time? If a tool returns an error code, the error code must be documented in a way the agent can understand, ideally with natural language descriptions embedded in the response. Many observation failures trace back to tools that were designed for human operators who bring contextual knowledge and error-handling instincts the agent lacks.

A well-designed tool for agents returns structured data with explicit schemas, includes metadata like timestamps and data freshness indicators, provides clear success and error signals, and documents edge cases and error conditions in a format the agent can parse. A poorly designed tool returns ambiguous unstructured text, omits critical metadata, uses error codes that require looking up external documentation, and assumes the caller has implicit knowledge about the domain. If you are building agents on top of existing APIs designed for humans, you often need to wrap them in an agent-friendly layer that adds structure and clarity.

Attention mechanisms are critical in observation. Agents have finite context windows and finite processing capacity. They cannot deeply analyze every piece of information they receive. You must design your observation phase to highlight what matters and filter what does not. This might mean tools that return summarized views rather than raw data dumps, or prompting strategies that direct the agent to focus on specific fields or signals. The FreightWise agent observed every traffic update with equal weight, treating minor slowdowns and major closures identically. A better observation design would have flagged severity and relevance, perhaps including fields like "severity: high" and "affects route: yes/no" so the agent could prioritize appropriately.

Attention can be implemented through several mechanisms. You can design tools to pre-filter and rank information, returning only the most relevant items. You can use retrieval techniques that match observations to the agent's current goal and surface what is relevant. You can prompt the agent to explicitly identify what to focus on before diving into details. Many production agents use a two-pass observation strategy where the first pass scans for high-level signals and the second pass dives deep into the items flagged as relevant. This prevents the agent from spending all its capacity on low-value information.

Temporal dynamics in observation are often overlooked and cause production failures. Most environments change while the agent is executing. A research agent might observe a search result, spend thirty seconds synthesizing information from that result, and by the time it decides to cite the source, the webpage has been updated or removed. A workflow agent might observe system state, spend ten seconds planning actions, and by the time it executes, another process or another agent has modified the state. Your observation design must account for staleness and race conditions.

Some architectures re-observe before each action to verify assumptions still hold. The agent plans to update a record, then immediately before executing the update, re-reads the record to check that it still exists and has the expected state. This adds latency but prevents acting on stale observations. Other architectures use optimistic concurrency control where the agent acts on its observations and handles conflicts when they occur. The update API includes the expected previous state as a precondition, and if the state has changed, the update fails and the agent retries with fresh observations. The right approach depends on failure tolerance and action latency. For high-stakes actions, re-observation is safer. For high-volume low-stakes actions, optimistic execution with conflict handling is more efficient.

The output of the observation phase is a representation of the current situation that the agent will reason about. This representation should be structured, complete for the agent's purpose, and grounded in verifiable data. Many teams make the mistake of passing raw tool outputs directly into reasoning without structuring them. The agent receives a blob of JSON from an API call and is expected to parse and interpret it within the reasoning phase. This works in simple cases but breaks down as complexity increases and the agent must integrate information from multiple sources or track state across iterations.

Better observation architectures include a formatting or structuring step that transforms raw tool outputs into a consistent representation optimized for reasoning. You might have a component that takes API responses from multiple systems, extracts relevant fields, resolves inconsistencies, and produces a structured situation summary. For a workflow agent, this might be "current state: customer record created, welcome email pending, credentials not yet provisioned." For a research agent, this might be "retrieved twelve sources, five highly relevant, three moderately relevant, four low relevance, two sources contradict on key claim." The structured representation makes reasoning easier and more reliable.

## Reasoning: Interpreting What Observations Mean

Reasoning is the phase where the agent interprets its observations and decides what matters. Given the current situation as represented by observations, what is relevant to the agent's goal? What can be inferred? What is uncertain? What are the implications? Reasoning is where the LLM's strengths are most apparent. The agent must handle ambiguity, apply domain knowledge, and make judgments that are not purely logical deduction. Reasoning is the bridge between perception and action.

The critical distinction that clarifies reasoning architecture is between epistemic reasoning and practical reasoning. Epistemic reasoning establishes what the agent believes to be true about the world. Practical reasoning generates candidate actions or directions based on those beliefs. Both happen in this phase but they serve different purposes. Epistemic reasoning builds a coherent mental model of the situation. Practical reasoning uses that model to consider options. Many agent architectures blur these together, leading to muddled outputs where factual claims and action proposals are interleaved and it is hard to tell what the agent believes versus what it thinks it should do.

Better architectures separate epistemic and practical reasoning explicitly. The agent first performs epistemic reasoning to establish its understanding. "The traffic closure affects northbound lanes on Highway 101 between exits 15 and 18. Truck 23 is currently southbound on Highway 101 approaching exit 12. The closure does not affect truck 23's route." Then it performs practical reasoning based on that understanding. "No action needed for truck 23. Monitor for updates that might affect southbound traffic." The separation makes reasoning more auditable and makes it easier to catch errors. If the epistemic reasoning is wrong, you can see where the misunderstanding occurred. If the practical reasoning is wrong, you can see that the agent misapplied correct beliefs.

Reasoning fails when the agent makes incorrect inferences, when it ignores relevant information, when it fixates on irrelevant details, when it applies inappropriate heuristics, or when it exhibits biases from training data. A common failure mode is reasoning that is locally coherent but globally flawed. The agent makes a series of reasonable-sounding inferences that lead to an absurd conclusion. This happens because LLMs are trained to produce locally plausible text, not to maintain global logical consistency. Each sentence follows from the previous one, but the chain of reasoning drifts away from truth.

Another failure mode is premature closure where the agent settles on the first plausible interpretation and does not consider alternatives. The agent observes that a customer has not responded to an email and reasons that they are not interested, without considering that the email might be in spam, or they are on vacation, or they need more time to evaluate. The agent jumps to a conclusion that fits the observation but is not the only explanation. This happens especially when the agent is prompted to be decisive or when the task structure rewards quick answers over thorough analysis.

The architecture mitigations for reasoning failures include chain-of-thought prompting to make reasoning explicit, structured reasoning templates that force the agent to address specific aspects of the situation, and adversarial checks where the agent generates counterarguments to its own reasoning. Chain-of-thought prompting asks the agent to show its work, to explain its reasoning step by step. This makes errors more visible and also tends to improve reasoning quality because the agent cannot skip steps. Structured templates provide a scaffolding for reasoning. You might prompt the agent with "First list relevant observations. Then list possible interpretations. Then evaluate each interpretation against the observations. Then select the most plausible interpretation." This structure prevents the agent from jumping to conclusions.

Adversarial checks ask the agent to argue against its own reasoning. After the agent reaches a conclusion, you prompt it with "what evidence would contradict this conclusion? what alternative explanations fit the observations?" This forces the agent to consider whether its reasoning is robust or brittle. Many production agents in 2026 use multi-step reasoning where the agent first generates candidate interpretations, then evaluates those interpretations, then selects the best one. This is slower than single-step reasoning but more reliable because it creates space for comparison and self-correction.

Uncertainty handling in reasoning is critical and often neglected. Real-world observations are noisy and incomplete. The agent should reason not just about what is likely true but about what is uncertain and what additional observations would reduce uncertainty. A research agent that observes conflicting information from different sources should reason that the truth is uncertain and plan to gather more evidence, not pick one source arbitrarily and proceed as if it were certain. A workflow agent that observes an unexpected system state should reason that its mental model might be wrong and plan to verify, not assume the system is broken and proceed with corrective actions that might make things worse.

Uncertainty can be represented explicitly in reasoning outputs. Instead of "the customer is not interested," the agent might reason "the customer has not responded, which could indicate lack of interest, but could also indicate the message was not received or they need more time. Confidence: low. Recommended action: send a follow-up with different framing and check delivery logs." This makes uncertainty visible to downstream phases and to humans reviewing the reasoning.

Context and domain knowledge play a huge role in reasoning quality. An agent reasoning about medical diagnoses needs different background knowledge than an agent reasoning about software bugs or financial markets or logistics. The LLM's pretraining provides broad knowledge but not deep domain expertise. You must supplement the agent's reasoning with domain-specific context, whether through retrieval-augmented generation that pulls in relevant documentation or examples, few-shot prompting that provides examples of correct reasoning in the domain, or structured knowledge bases that encode domain rules and heuristics.

The FreightWise agent failed partly because it lacked transportation domain knowledge. It did not understand that highway closures often affect only one direction, or that detours are only worthwhile if they save significant time relative to waiting or taking a slightly longer route that does not require backtracking. A human dispatcher has this knowledge implicitly from experience. The agent needed it provided explicitly, either in prompts or in tool outputs that flagged relevant domain constraints. "Highway 101 closure affects northbound only, truck 23 is southbound, no rerouting needed" would have prevented the failure.

The output of the reasoning phase is a structured interpretation of the situation that will feed into planning. This should include what the agent believes to be true, what is uncertain, what the key constraints are, and what options seem worth considering. Many teams output reasoning as unstructured text, which makes it hard for the planning phase to leverage and makes evaluation difficult. Better architectures use structured outputs with explicit fields for beliefs, uncertainties, constraints, and candidate directions. "Beliefs: customer record exists and is active, email sent successfully yesterday, no response yet. Uncertainties: whether email was delivered, whether customer has opened it, whether they are interested. Constraints: cannot resend email within 24 hours per company policy. Candidate directions: wait another day, try alternative contact method, escalate to human for manual follow-up."

## Planning: Formulating Next Steps

Planning is the phase where the agent decides what to do next. Given its understanding of the situation from reasoning, what actions or sequences of actions will move toward the goal? Planning translates reasoning into executable steps. It is where the agent commits to a course of action. Planning can range from detailed multi-step plans that map out an entire workflow to simple next-action decisions where the agent picks the immediate best move. The appropriate level of planning depends on the agent archetype, task complexity, and environment predictability.

The fundamental tension in planning is between commitment and flexibility. Detailed plans provide structure, enable parallel execution, and make dependencies explicit. But they become invalid when assumptions change. If the agent plans five steps based on its current understanding, and the environment changes after step two, steps three through five might no longer make sense. Minimal plans preserve flexibility by deciding only the next action, but they provide less guidance and can lead to wandering or thrashing. The agent tries one thing, then another, without a coherent strategy.

Different agent archetypes resolve this tension differently. Workflow agents benefit from detailed upfront planning because the process structure is usually known or can be inferred from the goal. If the goal is "onboard new customer," the steps are create account, send welcome email, provision access, schedule onboarding call. The agent can plan all of this upfront and execute sequentially with checkpoints. Research agents benefit from minimal planning because the information landscape is unpredictable. The agent cannot know in advance how many searches it will need or what follow-up queries will be relevant until it sees initial results. Planning too far ahead locks the agent into a search strategy that might turn out to be unproductive.

Coding agents often use hierarchical planning where high-level steps are planned upfront and low-level implementation details are planned just-in-time. The high-level plan might be "read existing authentication code, identify where to add new validation, implement validation function, update tests, run tests." Each high-level step is then decomposed into specific actions when the agent reaches it. This balances structure with adaptability. The agent has a roadmap but can adjust tactics as it encounters specifics.

Planning fails when the agent generates infeasible plans, when it plans actions that do not address the goal, when it plans inefficiently, or when it commits to a plan that becomes invalid but does not replan. A classic failure is the agent generates a plan that looks reasonable in the abstract but violates real-world constraints. "Plan: send email to customer, wait for response, proceed based on response." Humans know email is asynchronous and responses are not instant. The agent might not model that constraint unless explicitly told. It might plan to wait five seconds and then assume no response means the customer is not interested.

Another common planning failure is circular plans. The agent plans to do A, then B, then A again because it did not realize that B undoes A. Or the agent plans to gather information it already has because it did not check its observations carefully. Or the agent plans actions in an order that violates dependencies. "Plan: provision user access, then create user account." The provisioning will fail because the account does not exist yet. These failures are often caught during execution, but they waste time and create confusion.

The architecture mitigations for planning failures include feasibility checking before plan execution, explicit constraint modeling, and replanning triggers. Before executing a plan, the agent checks whether the planned actions are actually possible given current state and tool capabilities. Does the agent have access to the tools the plan requires? Are the preconditions for each step satisfied? Are there timing or sequencing constraints that the plan violates? Feasibility checking can be implemented as a separate validation step or integrated into the planning phase where the agent explicitly checks each planned action against known constraints.

Constraints like timing, dependencies, and resource limits should be represented explicitly and checked against. If there is a business rule that emails cannot be sent more than once per day to the same customer, that constraint should be encoded in a form the agent can check when planning email actions. If certain API calls have rate limits, those limits should be part of the tool specification so the agent can plan within bounds. Explicit constraints prevent the agent from generating plans that will fail predictably.

Replanning triggers are conditions that indicate the plan is no longer valid and the agent should generate a new plan rather than continuing with the current one. The environment changed in a way that invalidates key assumptions. A workflow agent planned to update a record, but before executing, observes that the record was deleted by another process. An expected resource became unavailable. A research agent planned to retrieve documents from a database that just went offline. The agent encountered a failure that suggests the approach is wrong. A coding agent tried to fix a bug with three different approaches and all failed, indicating the bug is more complex than initially understood.

Without replanning triggers, agents blindly continue executing invalid plans, wasting resources and potentially causing harm. With triggers, the agent can detect when to cut losses and start over with fresh reasoning and planning. The triggers must be specific enough to catch real invalidity but not so sensitive that the agent replans constantly in response to minor variations.

Plan granularity is a key design decision. Fine-grained plans with many small steps provide more control and observability but increase overhead and brittleness. Each step is a point where the agent must verify success and decide whether to continue. Coarse-grained plans with few large steps reduce overhead but make it harder to detect and recover from failures mid-plan. If a coarse step fails, you might not know which part failed or how far the agent got before the failure.

The right granularity depends on task complexity and failure tolerance. For high-stakes workflow tasks where partial failures can be expensive, fine-grained planning with checkpointing after each step is appropriate. The agent plans "create CRM record," executes it, verifies success, checkpoints, then plans "send welcome email," executes, verifies, checkpoints. If anything fails, the agent knows exactly where it failed and what succeeded before the failure. For low-stakes research tasks where partial progress has little value, coarse-grained planning is sufficient. The agent plans "search for information on topic X," executes multiple queries, retrieves results, and only checks overall progress at the end.

Plan representation matters for downstream execution and for humans reviewing the agent's work. Some agents represent plans as natural language descriptions of steps. "First, I will search for recent papers on battery thermal management. Then I will read the abstracts to identify the most relevant ones. Then I will retrieve the full text of the top five papers." This is readable and flexible but ambiguous. What does "recent" mean? How is relevance determined? What if there are not five papers? Other agents use structured formats like lists of tool calls with parameters. "search query equals 'battery thermal management' AND year greater than 2023, limit equals 20" followed by "retrieve documents with IDs from top 5 results ranked by relevance score." This is precise and executable but rigid and harder for humans to understand at a glance.

Still other agents use formal representations like state machines or dependency graphs that explicitly encode preconditions, postconditions, and branches. These enable sophisticated analysis and optimization but require more upfront modeling and infrastructure. Most production agents in 2026 use structured formats with some natural language explanation. The plan is a list of actions with parameters that can be executed directly, annotated with rationale that explains why the agent chose this sequence. This balances precision with interpretability.

The output of the planning phase is an executable plan or a next action decision. For plan-then-execute architectures, this is a sequence of actions with preconditions, expected outcomes, and success criteria. For reactive architectures like ReAct, this is a single next action with parameters and a rationale for why this action is appropriate. The plan must be specific enough to execute reliably but abstract enough to tolerate minor variations in execution outcomes. A plan that specifies every detail is brittle. A plan that leaves everything to execution-time judgment is not really a plan. The balance is domain-dependent and learned through iteration and failure analysis.

## Action: Executing Through Tools

Action is the phase where the agent executes its plan by calling tools that interact with external systems. For a research agent, actions are search queries and document retrievals. For a workflow agent, actions are API calls that create, update, or delete records. For a coding agent, actions are file writes and command executions. For a decision support agent, actions might be data queries and computation calls. Action is where the agent affects the world outside its own processing. Everything before action is internal to the agent. Action is the boundary crossing.

The critical property of actions is that they have effects that persist beyond the agent's execution context. A failed reasoning step is a local error that affects only the agent's internal state and can be corrected in the next iteration with no external consequences. A failed action can leave external systems in incorrect or inconsistent states that require cleanup or manual intervention. This asymmetry means action failures have higher stakes and require more careful handling than failures in earlier phases.

Actions fail when tools are unavailable, when tool calls have incorrect parameters, when tools succeed but produce unexpected results, or when tools have side effects the agent did not anticipate. Tool availability failures are usually transient, caused by network issues, service outages, or rate limits. They are annoying but recoverable through retries. Parameter errors are usually bugs in the agent's action generation or tool calling logic. The agent passed a string where an integer was expected, or forgot a required parameter, or used a deprecated parameter name. These errors are often caught by tool validation before execution.

Unexpected results are more insidious. The tool executed successfully from the tool's perspective, but the outcome was not what the agent expected. The agent called an API to create a record and the API returned success, but the record was created with default values instead of the values the agent specified because the agent misunderstood the API schema. Or the agent called a search API expecting the top result to be most recent, but the API ranks by relevance not recency. The action succeeded but did not achieve the intended effect. These failures often go undetected without explicit verification.

Unanticipated side effects are the most dangerous because they are often silent until they cause downstream problems. The agent calls an API to update one field in a record, and the API has a trigger that sends an email to the customer, but the agent did not know about the trigger. Or the agent deletes a file thinking it is a temporary cache, but the file was a critical configuration that other services depend on. These failures are architectural. The agent's model of the tool did not match the tool's actual behavior.

The architecture mitigations for action failures are parameter validation, idempotency, retries, and transaction boundaries. Before executing an action, the agent validates that parameters conform to the tool's schema and constraints. If the tool expects an email address, the agent checks that the parameter is a valid email format. If the tool expects a positive integer, the agent checks that the value is not negative or zero. Validation catches many errors before execution and provides clear error messages that the agent can use for self-correction.

Tools are designed to be idempotent where possible so that retries do not cause duplicate effects. An API call to create a record with a specific ID should succeed if the record does not exist and return success if the record already exists with the same ID. This allows the agent to safely retry the call if it does not receive a response without worrying about creating duplicate records. Not all actions can be made idempotent, sending emails is inherently non-idempotent, but when idempotency is achievable, it dramatically simplifies error handling.

Transient failures trigger automatic retries with exponential backoff. If a tool returns a temporary error like a timeout or rate limit, the agent waits and tries again, with increasing delays between retries. This handles transient failures without agent intervention. Permanent failures are surfaced immediately so the agent can replan or escalate. The distinction between transient and permanent errors must be clear from the tool's error responses. A rate limit error should be marked as transient, a "resource not found" error should be marked as permanent.

Related actions are grouped into transactions so that partial failures can be rolled back atomically. If a workflow requires creating a record in system A, updating a record in system B, and sending an email, those actions should be grouped such that if the email fails, the changes to systems A and B are undone. This prevents inconsistent states where some parts of the workflow succeeded and others failed. Transaction boundaries are hard to implement across heterogeneous systems, which is why workflow agents often use compensating transactions or idempotency to achieve similar guarantees.

Action observability is critical and often neglected. After executing an action, the agent must observe the result to verify success. This means tools must return clear success or failure signals and provide enough information to diagnose failures. A tool that returns a generic "error" message is much less useful than one that returns "error: customer ID not found in database" or "error: insufficient permissions to update this field." The agent's action phase must include result verification, not just blind execution. For actions with side effects, verification often requires a separate observation step. The agent creates a record, then queries to confirm the record exists and has the expected values.

Concurrency in actions introduces complexity but is sometimes necessary for performance. Some agent architectures execute actions sequentially, waiting for each to complete before starting the next. This is simple and safe but slow when actions are independent. Other architectures execute independent actions in parallel to reduce latency. A research agent might issue five search queries concurrently instead of sequentially, cutting the overall retrieval time from five times the per-query latency to roughly the single-query latency. Parallel execution improves performance but introduces race conditions and ordering dependencies that the agent must reason about.

The FreightWise agent executed route updates sequentially, which was safe but slow. During high traffic periods, the agent might take several minutes to process all the required reroutes, and by the time it finished, the traffic conditions had changed and the reroutes were stale. An agent that tried to update multiple routes in parallel might have been faster but would need to handle conflicts if routes shared resources like trucks or drivers. If truck 15 is assigned to two routes and both routes are updated concurrently, the updates might conflict. Proper concurrency control requires reasoning about dependencies at planning time or using locking mechanisms during execution.

The output of the action phase is a set of observations about what happened when the tools were called. These observations feed back into the observation phase for the next iteration of the loop. The agent observes whether actions succeeded, what results they produced, and how the environment state changed. This feedback is essential for the reflection phase to evaluate whether progress was made. Without capturing action results as observations, the agent has no way to learn from its actions or correct course when actions fail to achieve intended effects.

## Reflection: Evaluating Outcomes and Progress

Reflection is the phase where the agent evaluates whether its actions achieved their intended effects and whether it is making progress toward the goal. This is the phase the FreightWise agent was missing. Without reflection, agents execute actions blindly without checking if those actions helped. Reflection closes the loop. It is the agent's self-assessment and course correction mechanism. Reflection asks several questions. Did the action succeed or fail? If it succeeded, did it produce the intended effect? Is the current state closer to the goal than before? Are there unexpected consequences that need to be addressed? Should the agent continue with the current plan, replan, or escalate?

Reflection is metacognitive. The agent is reasoning about its own performance. This is harder than object-level reasoning because it requires the agent to maintain a mental model not just of the environment but of its own goals, plans, and execution history. Reflection requires comparing the current state to a goal state or to a previous state and making a judgment about whether things are improving. This is straightforward for quantifiable goals but challenging for qualitative goals or goals where progress is not monotonic.

Reflection fails when the agent cannot measure progress, when it uses the wrong success criteria, when it is overconfident about success, or when it gets stuck in local optima. A common failure mode is the agent judges success based on whether the tool call succeeded rather than whether the goal is closer. A workflow agent might successfully create a database record and mark the task complete without verifying that the record contains correct data or that creating the record actually moved the workflow forward. The action succeeded in a narrow technical sense, but the outcome is wrong. Reflection must evaluate outcomes, not just outputs.

Another failure mode is using proxy metrics that do not align with the true goal. A research agent might measure progress by the number of sources retrieved and consider the task successful once it has ten sources, without checking whether those sources actually answer the question or whether they are all saying the same thing. The agent optimized for a metric that does not capture what the user cares about. Reflection must evaluate against the actual goal, not convenient proxies.

Overconfidence in reflection leads to premature termination. The agent thinks it succeeded when it actually failed or only partially succeeded. A coding agent runs tests and sees that three of five tests pass, but reports success because it focused on the passing tests and ignored the failures. Or a decision support agent generates a recommendation and considers its job done without checking whether the recommendation is grounded in the data it retrieved or addresses the user's actual question. The agent stops too early because it did not critically evaluate its own output.

Getting stuck in local optima is a failure of reflection to recognize when the current approach is not working. The agent tries the same action repeatedly with minor variations, each time achieving a small improvement or no improvement, without recognizing that it is stuck and a fundamentally different approach is needed. A coding agent tries five variations of the same fix and none of them work, but it keeps trying variations because reflection sees each attempt as making progress toward a solution. Better reflection would recognize the pattern of repeated failures and trigger replanning.

The architecture mitigations for reflection failures include explicit success criteria, progress metrics, and multi-level evaluation. Before acting, the agent establishes what success looks like for that action. For a workflow agent creating a customer record, success criteria might be "record exists with correct customer name, email, and account type, and record ID is returned." After acting, the agent evaluates whether those criteria were met. It does not just check that the API returned a 200 status code, it verifies the record exists and has correct values.

Progress metrics provide a quantitative or qualitative measure of how close the agent is to the overall goal. For a research agent, progress might be measured by coverage of key topics or number of high-quality sources found. For a coding agent, progress might be measured by number of tests passing. For a workflow agent, progress might be measured by percentage of workflow steps completed. The agent maintains this metric across iterations and uses it to assess whether actions are helping. If the metric is not improving, reflection should trigger replanning.

Multi-level evaluation means assessing at different granularities. Did this specific action succeed? Did this sub-goal get achieved? Is the overall goal closer? These evaluations catch failures at different levels. A workflow agent might successfully complete a step, satisfying the action-level success criteria, but realize that the sub-goal is not achieved because the step produced an unexpected result. Or the agent might achieve a sub-goal but realize through reflection that the overall goal is still far away because the sub-goal turned out to be less important than expected.

Reflection should also identify when to stop. Agents need termination conditions, not just for success but for futility. If the agent has tried several approaches and made no progress, continuing indefinitely is wasteful and potentially harmful. Reflection should detect when the agent is stuck, when it is looping without progress, or when it has exceeded reasonable resource budgets, and trigger appropriate termination or escalation. Many production agent failures in 2026 involve agents that ran until they hit hard timeout limits rather than recognizing earlier that they were stuck and escalating to humans.

Self-correction based on reflection is a key capability that distinguishes reliable agents from brittle ones. When reflection reveals that an action failed or did not help, the agent should adjust its approach. This might mean retrying with different parameters, choosing a different tool, replanning the entire approach, gathering additional information to reduce uncertainty, or seeking human assistance. The loop is not observe-reason-plan-act-reflect-done. It is observe-reason-plan-act-reflect-adjust-repeat. The reflection phase feeds back into reasoning and planning for the next iteration.

The FreightWise agent would have caught its failures in reflection if it had compared new route times to original route times. Reflection would ask "I rerouted truck 23, did this improve expected delivery time?" The answer would be no, the new route is forty minutes longer than the original. Reflection would then ask why the rerouting happened and whether the reasoning was sound. This would surface the flaw in the agent's understanding of traffic closures and trigger corrective learning or replanning.

The output of the reflection phase is a decision about what to do next. Continue with the current plan because progress is being made. Modify the plan because the approach is mostly right but needs adjustment. Replan from scratch because the current approach is not working. Escalate to a human because the agent has determined it lacks the capabilities or information to succeed. Terminate successfully because the goal is achieved. Terminate with failure because the agent has exhausted reasonable efforts and cannot proceed. This decision determines whether the loop continues and if so how. Many agent architectures make this decision implicitly through prompt engineering or hardcoded logic. Better architectures make it explicit and auditable so you can understand why the agent chose to continue or stop.

## Loop Structures: Tight Versus Loose Coupling

The five phases can be composed into different loop structures that make different tradeoffs between adaptability and efficiency. The two dominant patterns in 2026 are tight loops and loose loops. Tight loops execute all five phases for each individual action. The agent observes the current state, reasons about what to do next, plans a single action, executes it, reflects on the result, and repeats. This is the ReAct pattern and it is maximally adaptive but also maximally slow because every action requires a full reasoning cycle with an LLM call.

Tight loops are appropriate when the environment is highly unpredictable and each action's outcome significantly affects what should happen next. A coding agent debugging a failing test might use a tight loop because each action, reading a file or running a test, provides information that shapes the next action. The agent cannot plan five actions in advance because it does not know what it will find. Each observation triggers fresh reasoning.

Loose loops separate planning from execution. The agent observes the overall situation, reasons about what needs to happen, plans a sequence of actions, executes all of them, then reflects on the aggregate outcome. This is the plan-then-execute pattern and it enables parallel execution and reduces LLM calls but is less adaptive because the plan can become stale during execution. If the environment changes or actions produce unexpected results, the agent continues with the plan rather than adjusting in real time.

Loose loops are appropriate when the environment is predictable and actions are mostly independent. A research agent can often plan several search queries upfront because the results of one query do not usually invalidate the others. A workflow agent can plan the steps of a well-understood process because the steps are defined and dependencies are known. The agent gains efficiency by batching planning and execution.

The choice between tight and loose loops depends on environment predictability and action latency. In predictable environments where actions usually succeed and produce expected results, loose loops are efficient. In unpredictable environments where every action might require adaptation, tight loops are safer. When actions are fast, the latency of tight loops is tolerable. When actions are slow, the overhead of making an LLM call before each action becomes prohibitive and loose loops with checkpointing are better.

Many production agents use hybrid structures that balance adaptability and efficiency. Plan at a coarse grain, execute a batch of actions, reflect on the batch outcome, replan if necessary, execute the next batch. This is plan-then-execute with checkpointing. The agent is not replanning after every action, but it is not committing to a full workflow upfront either. The granularity of batching is a tuning parameter. Smaller batches provide more frequent reflection at the cost of more LLM calls. Larger batches reduce overhead but increase the risk of executing stale plans or missing failures until late in the process.

Loop termination is critical and surprisingly hard to get right. Agents must have clear conditions for when to stop. Success conditions are obvious. The goal was achieved, verification confirms the desired state is reached, no further actions are needed. Failure conditions are less obvious but equally important. The agent has exceeded time or cost budgets. The agent has tried multiple approaches without progress. The agent has detected that it lacks necessary capabilities or information. The agent has encountered unsafe or ambiguous situations that require human judgment.

Without proper termination logic, agents either stop prematurely before completing the task or run indefinitely consuming resources and potentially causing harm. Premature termination happens when success criteria are poorly defined or when the agent mistakes partial success for complete success. Indefinite execution happens when failure conditions are not specified and the agent keeps trying variations without recognizing futility. Your loop structure must explicitly encode both success and failure termination conditions and evaluate them in the reflection phase.

## Cascading Failures and Defensive Design

Understanding phase-specific failures helps you design robust agents, but the worst failures are cascading failures where a failure in one phase causes failures in subsequent phases. An observation failure leads to flawed reasoning, which leads to a bad plan, which leads to harmful actions, which the agent fails to recognize in reflection because reflection is based on the same flawed observations. The agent is confidently wrong at every phase because the initial error propagates and compounds.

Breaking cascades requires defensive design at each phase. Observations must be validated against schemas and freshness requirements. The agent does not just accept tool outputs, it checks that timestamps are recent, that required fields are present, that values are within expected ranges. Reasoning must explicitly consider uncertainty and alternative interpretations. The agent does not just settle on the first plausible explanation, it generates alternatives and evaluates them. Plans must be checked for feasibility before execution. The agent does not just generate a sequence of actions, it verifies that the actions are possible and ordered correctly.

Actions must be verified after execution. The agent does not just call a tool and assume it worked, it checks the result and confirms the intended effect occurred. Reflection must use ground truth measurements not derived from the agent's own observations when possible. If the agent is optimizing delivery times, reflection should measure actual delivery times from GPS data, not estimated times from the agent's routing calculations. If the agent is generating code, reflection should run independent tests, not just check that the code matches the agent's specification.

The FreightWise agent's failure was a reflection failure that propagated backward. Because the agent never checked whether rerouting improved outcomes, it never corrected its flawed reasoning about which routes needed changes. If it had reflected by comparing new route times to original route times using independent GPS data, it would have detected that many reroutes were unnecessary. This would have fed back into reasoning, causing the agent to refine its understanding of when rerouting is appropriate. The reflection failure prevented learning and allowed the flawed reasoning to persist.

In 2026, production agents are designed with the assumption that every phase will fail sometimes. The question is not whether failures will happen but how quickly they will be detected and corrected. Your agent architecture must make failures visible through logging and observability, provide recovery mechanisms at each phase through retries and replanning, and ensure that phase-level failures do not cascade into system-level disasters through defensive validation and independent verification. The five-phase loop is not just a conceptual model for understanding how agents work, it is an engineering framework for building reliable autonomous systems that degrade gracefully when things go wrong.

The next step after understanding the execution loop is studying the failure modes that occur when loops go wrong over multiple iterations. Agents do not just fail in individual phases, they exhibit systemic failure patterns that emerge from the interaction of phases over time. Infinite loops where the agent repeats the same actions without progress. Tool cascades where the agent calls dozens of tools without synthesizing information. Hallucinated tool arguments where the agent invents plausible but wrong parameters. Premature termination where the agent stops before the job is done. Goal drift where the agent loses track of the original objective and pursues tangents. These are the production failure modes that separate teams who have successfully shipped agents from teams who are still debugging in staging environments.

