# 9.9 â€” Agent Incident Response: Debugging Failures in Production

At 2:47 PM on April 14th, an agent began rejecting every small business loan application with the same reason: insufficient tax documentation. It processed 183 applications over four hours, denying every single one, including applications with complete tax returns already uploaded. No alert fired. No monitoring dashboard flagged the anomaly. The pattern went unnoticed until a relationship manager manually escalated a high-value client case at 6:30 PM and discovered that forty-one consecutive applications had been incorrectly rejected. By then, the agent had denied applications representing eleven million dollars in loan volume, costing the company $470,000 in remediation and three lost enterprise accounts. The failure was not visible in standard observability metrics: the agent reported success on every run, rejection rate was scoped per-product rather than globally, and the small business line represented only eight percent of total volume. The incident response playbook you need is not the one you use for API outages. It is the one designed for silent, category-specific agent failures that manifest as correctness degradation, not error spikes.

The root cause was a configuration change deployed at 2:43 PM that altered the document classification model's confidence threshold for one document type. The change was unrelated to loan processing, but the agent's retrieval logic had a hidden dependency on that threshold. When the threshold shifted, the retrieval step began returning an empty set for tax documents, and the agent interpreted empty results as missing documentation. The failure was invisible in logs because the agent reported success on every run, the retrieval step returned valid responses with zero results, and no alert fired because the rejection rate metric was scoped per-product and the small business line represented only 8% of total volume. The team had monitoring, logging, and eval coverage, but none of it was designed to surface this specific failure mode. They discovered the issue only through human escalation, four hours after onset.

This is the agent incident response problem. When an agent fails in production, you need a systematic process to detect the failure, isolate the cause, assess the scope, stop the damage, and restore correct behavior. Unlike traditional software incidents where a stack trace or error log points you to the bug, agent failures are often silent, gradual, and contextual. The agent continues running, the infrastructure reports healthy, and the outputs look syntactically valid. The failure manifests as subtle drift in decision quality, unexplained changes in behavior, or localized breakage in edge cases that your metrics did not anticipate. You need an incident response framework designed specifically for agent systems, one that treats output correctness as the primary signal and trace-level behavior as the diagnostic surface.

## The Agent Incident Lifecycle

Agent incidents follow a different progression than traditional software incidents. In a web service failure, you see error rates spike, latency degrade, or requests fail with HTTP 500 codes. The signal is immediate and binary. In agent incidents, the failure often appears as a slow drift in output quality, a localized pattern affecting a subset of inputs, or a change in decision distribution that looks normal in aggregate but catastrophic in a specific segment. You need to structure your incident response around this reality.

The lifecycle has five phases: detection, triage, isolation, mitigation, and resolution. Detection is recognizing that something is wrong. Triage is determining severity and scope. Isolation is identifying the root cause. Mitigation is stopping or containing the damage. Resolution is fixing the underlying issue and validating the fix. Each phase requires different tools and different decision-making processes than you would use for traditional software incidents.

Detection for agent systems relies on a combination of automated monitoring, eval-based alerts, and human escalation. You monitor output distributions, decision patterns, eval scores, and user feedback signals. You set alerts on metrics like task success rate, refusal rate, tool use frequency, and eval performance across slices. But you also design escalation paths that allow humans to flag anomalies that metrics miss. The financial services company had monitoring, but their alerts were scoped too broadly. They tracked rejection rate globally, not per-product, and they had no alert on decision uniformity. A single-reason rejection pattern affecting 183 consecutive cases should have triggered an alert within minutes, but the system was not designed to detect it.

Triage is the first decision point. You need to quickly assess whether this is a P0 incident requiring immediate mitigation, a P1 incident requiring investigation within hours, or a P2 issue that can wait for the next sprint. The assessment depends on three factors: scope, impact, and trend. Scope is how many cases are affected. Impact is the severity of incorrect outputs. Trend is whether the issue is growing, stable, or resolved. If scope is large, impact is high, or trend is expanding, you escalate to P0. If the failure affects a small slice, produces low-severity errors, or appears contained, you triage to P1 or P2. You make this decision in minutes, not hours, because delayed mitigation in agent systems compounds damage with every additional run.

Isolation is the diagnostic phase. You reconstruct what the agent did, why it made the decisions it made, and what changed to trigger the failure. This requires trace-level logs, input/output pairs, retrieval results, tool calls, and model responses. You compare traces from failing cases to traces from successful cases. You diff configuration changes, model versions, data updates, and external dependencies. You replay the failing inputs to confirm reproduction. You systematically eliminate hypotheses until you identify the root cause. The financial services team isolated their issue by pulling traces for all 183 rejections, discovering that every trace showed zero documents returned in the retrieval step, then tracing that behavior back to the threshold change deployed four minutes before onset.

Mitigation is stopping the damage. You have three options: rollback, disable, or patch. Rollback reverts the change that caused the failure. Disable stops the agent from running until you fix the issue. Patch applies a targeted fix to restore correct behavior without a full rollback. You choose based on confidence, risk, and urgency. If you have high confidence in the root cause and rollback is low-risk, you roll back immediately. If the issue is widespread and you cannot identify a safe rollback target, you disable the agent and route traffic to a manual process or fallback system. If you can isolate the failure to a specific code path or configuration, you deploy a patch. The key is speed. Every minute of delay extends the damage window.

Resolution is the full fix. You deploy a permanent solution, validate that it resolves the issue across all affected cases, and confirm that your monitoring and eval coverage now detect this failure mode. You run regression evals to ensure the fix did not introduce new issues. You update your incident postmortem with root cause analysis, timeline, impact assessment, and prevention measures. You treat the incident as a learning opportunity and invest in tooling, alerts, or process changes that prevent recurrence.

## Building the Incident Response Runbook

Your team needs a written runbook that defines roles, decision trees, escalation paths, and diagnostic procedures for agent incidents. The runbook is not a generic SRE guide adapted for agents. It is a purpose-built playbook that accounts for the unique failure modes, diagnostic surfaces, and mitigation strategies of agent systems.

The runbook starts with incident classification. You define severity levels based on scope and impact. P0 incidents are those affecting more than 10% of traffic, producing high-severity errors, or impacting critical business processes. P1 incidents affect 1% to 10% of traffic or produce medium-severity errors. P2 incidents affect less than 1% of traffic or produce low-severity errors. You set response SLAs for each level. P0 requires immediate acknowledgment within five minutes and mitigation within thirty minutes. P1 requires acknowledgment within thirty minutes and mitigation within four hours. P2 can be triaged during business hours.

You assign roles. The incident commander owns the response, makes mitigation decisions, and coordinates communication. The diagnostics lead investigates the root cause. The mitigation engineer implements the fix. The communications lead updates stakeholders. For small teams, one person may hold multiple roles, but you still define the responsibilities explicitly. The financial services company had no defined roles. When the incident surfaced, three engineers independently started investigating, two product managers began drafting user communications, and no one made a mitigation decision for forty minutes because no single person owned the call.

You define escalation triggers. When does an on-call engineer escalate to the team lead? When does the team lead escalate to executive leadership? You set clear thresholds. Escalate to team lead if mitigation is not complete within thirty minutes, if the issue affects a critical customer, or if the root cause is unknown after one hour of investigation. Escalate to executive leadership if the incident will result in customer data exposure, regulatory violation, financial loss exceeding a defined threshold, or reputational damage. Write these triggers into the runbook so that engineers do not hesitate or second-guess escalation decisions under pressure.

You create diagnostic checklists. When an incident is declared, the diagnostics lead follows a systematic procedure: pull traces for the last one hundred affected cases, compare to one hundred successful cases from the same time window, check for recent deployments or configuration changes, verify that external dependencies are healthy, inspect retrieval results and tool outputs, review model response patterns, and check eval scores for the affected segment. The checklist ensures that you do not skip steps or chase red herrings when you are operating under time pressure.

You document common failure modes and their signatures. Configuration drift shows up as sudden changes in behavior after a deployment, with traces revealing altered retrieval results or different tool call patterns. Model degradation appears as gradual increases in eval failure rates, with outputs becoming less coherent or less accurate over days or weeks. Data quality issues manifest as localized failures in specific input segments, with traces showing unexpected retrieval results or missing context. Dependency failures surface as tool call errors, empty responses from external APIs, or timeout patterns. By cataloging these signatures, you give your team pattern-matching shortcuts that accelerate diagnosis.

You define mitigation playbooks for each common failure mode. For configuration drift, the playbook is: identify the deployment timestamp, compare configuration before and after, roll back to the last known good configuration, validate with a sample of recent inputs, and monitor for thirty minutes before closing the incident. For model degradation, the playbook is: check model version, compare outputs to the last known good version, roll back to the previous model if performance is measurably worse, or disable the agent and route to fallback if no good version exists. For data quality issues, the playbook is: identify the affected data source, validate data freshness and correctness, refresh or backfill the data, and replay affected cases. You make these playbooks executable, with specific commands, dashboards to check, and validation steps.

## Real-Time Trace Analysis During Incidents

When an incident is active, you need the ability to pull and analyze traces in real time, not hours later when batch logs are processed. This requires infrastructure that streams trace data to a queryable store with sub-minute latency, a query interface that allows filtering by time range, input characteristics, output patterns, and trace structure, and visualization tools that let you compare traces side by side.

Your trace store indexes every agent run by timestamp, session ID, input hash, output hash, task type, user ID, and eval outcome. During an incident, you query for all runs in the failure window, filter to the affected segment, and pull the full trace for a sample of cases. You compare these traces to a baseline sample from before the failure window. You look for differences in retrieval results, tool call sequences, model response lengths, decision points, and final outputs. These diffs point you toward the root cause.

For the financial services incident, the diagnostics lead queried for all small business loan applications processed between 2:47 PM and 6:30 PM. The query returned 183 runs. She pulled full traces for ten of them and compared to ten successful runs from the morning. Every failing trace showed the same pattern: the document retrieval step returned an empty list, and the agent proceeded to the rejection logic with a reason template filled as "insufficient documentation for tax verification." The morning traces showed the retrieval step returning three to seven documents per application. This diff immediately narrowed the investigation to the retrieval step.

She then queried the retrieval service logs for the same time window and discovered that the document classification model had been redeployed at 2:43 PM with a higher confidence threshold for tax document classification. Documents that previously scored 0.78 confidence and passed the threshold now scored 0.78 but failed the new 0.80 threshold. The retrieval service filtered them out, returned empty results, and the agent interpreted this as missing documentation. The entire causal chain was reconstructed in twelve minutes because the trace store allowed real-time querying and side-by-side comparison.

You also use trace analysis to assess scope. Once you identify the root cause, you query for all cases that exhibit the failure signature and count them. You determine how many users were affected, what the financial impact is, and whether the failure is still ongoing. This assessment informs your mitigation decision and your stakeholder communication. If the scope is contained and the issue is no longer producing new failures, you may defer mitigation and focus on remediation. If the scope is expanding, you mitigate immediately.

Your trace analysis tooling should support ad hoc queries without requiring engineering support. Product managers, trust and safety operators, and on-call engineers should all be able to pull traces, filter by attributes, and export results. This democratization of diagnostic access reduces time to triage and eliminates bottlenecks where one engineer becomes the gatekeeper for all incident data.

## Automated Rollback and Canary Mitigation

For many agent incidents, the fastest mitigation is rollback: revert to the last known good configuration, model version, or code deployment. But rollback in agent systems is more complex than in traditional services, because agents depend on configurations, prompts, models, tools, data sources, and external APIs. A single deployment may change multiple components, and rolling back one component may not fully restore correct behavior if other components have also changed.

You need automated rollback tooling that tracks the full dependency graph of each deployment and can revert all related components atomically. When you deploy a new prompt template, the system records the model version, tool definitions, retrieval configuration, and eval thresholds active at deployment time. If you need to roll back the prompt, the system reverts all associated components to their state at the time of the last stable deployment. This prevents partial rollbacks that leave the agent in an inconsistent state.

You also use canary deployments to contain risk. When you deploy a configuration change, you route 5% of traffic to the new version and 95% to the stable version. You monitor eval scores, output distributions, and user feedback for both cohorts. If the canary cohort shows degraded performance, you halt the rollout and investigate before exposing more users. If the canary performs as well or better, you gradually increase traffic to 10%, 25%, 50%, and finally 100%. This staged rollout limits the blast radius of failures and gives you early signal to abort bad deployments.

During an incident, you can use canary infrastructure for rapid mitigation testing. You deploy a candidate fix to 5% of traffic, monitor for ten minutes, and validate that the fix resolves the issue without introducing new failures. If validation succeeds, you promote the fix to full traffic. If validation fails, you roll back the candidate and try a different fix. This approach is faster and safer than deploying a fix to all traffic and hoping it works.

The financial services company did not have automated rollback. When they identified the configuration change as the root cause, an engineer manually reverted the threshold in the document classification config, redeployed the service, waited for propagation, and then tested with a sample input. The entire process took nineteen minutes. With automated rollback, the mitigation would have taken ninety seconds: identify the deployment ID, trigger rollback, validate with automated tests, and confirm resolution.

## Incident Communication and Stakeholder Updates

Agent incidents often have business impact that extends beyond engineering. If the agent makes customer-facing decisions, incorrect outputs may require customer outreach, remediation, or compensation. If the agent processes sensitive data, failures may trigger compliance or legal review. If the agent supports a critical business process, downtime may halt operations. You need a communication plan that keeps stakeholders informed without over-communicating or under-communicating.

Your communication plan defines who gets notified at each severity level, what information they receive, and how often they receive updates. For P0 incidents, you notify executive leadership, affected business units, customer success teams, and legal or compliance if applicable. You send an initial notification within fifteen minutes of incident declaration, stating that an issue has been detected, impact is being assessed, and mitigation is in progress. You send updates every thirty minutes with current status, estimated time to mitigation, and any changes to scope or impact. You send a final resolution notification when the issue is fully mitigated and validated.

For P1 incidents, you notify the relevant product and engineering leads and affected business units. You send an initial notification within one hour and updates every two hours. For P2 incidents, you document the issue in your incident tracker and notify stakeholders during the next business day.

The content of your updates matters. You avoid technical jargon and focus on business impact. Instead of "the document classification model's confidence threshold was increased, causing retrieval to return empty sets," you write "a configuration change caused the system to incorrectly classify loan applications as missing required documentation, resulting in incorrect rejections. We have identified the cause and are deploying a fix." You state the number of affected cases, the time window, the customer impact, and the expected resolution time. You update these estimates as new information becomes available.

You also communicate remediation plans. If customers were impacted, you state how they will be contacted, what remediation is offered, and when they can expect resolution. If internal processes were disrupted, you state how work will be re-processed or compensated. You treat this as part of incident response, not an afterthought.

## Post-Incident Review and Prevention

Every agent incident, regardless of severity, receives a written postmortem. The postmortem documents the timeline, root cause, impact, mitigation steps, and lessons learned. It also defines action items to prevent recurrence. These action items are not vague aspirations like "improve monitoring." They are specific, assigned, and tracked: "Add an alert on decision uniformity that fires when more than 70% of cases in a fifteen-minute window share the same rejection reason," owned by the observability lead, due in two weeks.

You conduct a blameless postmortem review meeting within forty-eight hours of resolution. The team walks through the timeline, discusses what went well, what went poorly, and what could be improved. You focus on systems and processes, not individual mistakes. The goal is organizational learning, not accountability assignment. The financial services company's postmortem identified three prevention measures: deploy a decision uniformity alert, add per-product rejection rate monitoring, and implement automated rollback for configuration changes. All three were completed within ten days.

You also update your eval suite to cover the failure mode. If the incident was caused by a configuration change that broke retrieval, you add an eval case that tests retrieval correctness under configuration drift. If the incident was caused by model degradation, you add an eval that tracks output quality over time and alerts on regressions. You treat every incident as a gap in your eval coverage and close that gap before moving on.

You share incident learnings across teams. If you are operating multiple agents or multiple deployments, the failure mode that hit one agent may be latent in others. You review the root cause analysis with other teams, audit their systems for similar dependencies, and proactively fix issues before they trigger incidents. This cross-pollination of learnings prevents repeated failures across the organization.

## The Incident Response Culture

Agent incident response is not just a technical process. It is a cultural commitment to treating output correctness as a production reliability concern, responding to failures with urgency and rigor, and learning from every incident. Many organizations treat agent failures as product issues to be logged in a backlog, not production incidents requiring immediate response. This mindset leads to slow detection, delayed mitigation, and compounding damage.

You build a culture where any team member can declare an incident without fear of overreaction. You reward early detection and rapid mitigation, not perfect uptime. You measure your incident response effectiveness by time to detection, time to mitigation, and recurrence rate, not by the number of incidents. You celebrate postmortems as learning artifacts, not failure markers.

You also invest in making incident response as low-friction as possible. You automate trace querying, rollback procedures, and stakeholder notifications. You build dashboards that surface the most common diagnostic queries in one click. You write runbooks in plain language with step-by-step instructions. You reduce the cognitive load on engineers during high-stress incidents so they can focus on diagnosis and mitigation, not tool-fighting.

The financial services company revised their incident response process after the April incident. They appointed an incident commander for all P0 and P1 agent issues, created a dedicated Slack channel for incident coordination, automated their rollback tooling, and ran quarterly incident response drills where they simulated agent failures and practiced the runbook. Six months later, they detected and mitigated a similar configuration drift issue in eight minutes, before it affected any customer applications.

When you treat agent incidents with the same rigor you apply to infrastructure outages, you minimize damage, accelerate resolution, and build organizational muscle for operating production AI systems. The next subchapter covers replay and simulation, the diagnostic techniques that allow you to reproduce agent behavior, test fixes, and validate that your mitigation actually resolved the issue.
