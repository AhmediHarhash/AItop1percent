# 8.19 — Compliance by Autonomy Tier: What Must Be Logged and Retained

In March 2025, a logistics company operating autonomous inventory management agents across 14 warehouses received a regulatory inquiry from the Federal Trade Commission regarding pricing decisions that appeared to violate anti-competitive pricing laws. The FTC requested a complete audit trail of all automated pricing decisions made by the company's agents over the preceding 18 months, including the data inputs used, the reasoning process, the human approvals obtained, and any overrides or interventions. The company's engineering team discovered they had retained detailed logs of agent actions for only 30 days, with high-level summary metrics stored for 90 days beyond that. They had no record of which human operators had reviewed which decisions, no structured audit trail of when agents had been overridden, and no way to reproduce the agent's reasoning for decisions made more than a month ago. The company could not demonstrate compliance with their own stated policy that all pricing decisions would be reviewed by a human manager before implementation. The FTC investigation resulted in a $2.8 million fine, a consent decree requiring the implementation of comprehensive audit systems, and two years of quarterly reporting on agent decision-making processes. The root cause was not malicious behavior—the agents had largely made reasonable pricing decisions—but the company could not prove it because they had not designed their logging and retention architecture to match the compliance requirements of the autonomy level at which their agents operated. They had treated the agents as low-stakes assistive tools and logged them accordingly, when in fact the agents were making high-stakes autonomous decisions that required full auditability.

Compliance obligations for agent systems are not uniform—they vary based on the level of autonomy the agent operates at, the domain it operates in, and the potential impact of its actions. An agent that drafts email responses for human review has different logging requirements than an agent that autonomously approves credit applications. An agent that retrieves documents for a support representative has different retention requirements than an agent that makes medical treatment recommendations. The autonomy tier determines what must be logged, how long it must be retained, who must be able to audit it, and what level of reasoning transparency is required. Failing to match your logging and retention architecture to your agent's autonomy tier is not just an operational oversight—it is a compliance failure that can result in regulatory penalties, litigation exposure, and inability to defend your system's behavior when challenged.

## Autonomy Tiers and Their Compliance Profiles

Autonomy tiers describe the level of independence and decision-making authority an agent has. The most common framework divides agent autonomy into five tiers: assistive, advisory, semi-autonomous, autonomous with human oversight, and fully autonomous. Each tier has distinct compliance requirements.

Assistive agents operate at the lowest autonomy tier. They perform tasks under direct human control, with the human making all consequential decisions. An assistive agent might retrieve information, format documents, or generate draft text, but it does not take actions that have legal, financial, or operational consequences without human initiation. The compliance requirement for assistive agents is minimal: log what the agent did, when it did it, and which user initiated the action. Retain these logs long enough to support debugging and basic accountability, typically 30 to 90 days. You do not need to log the agent's reasoning process or decision criteria because the agent is not making decisions—it is executing human instructions.

Advisory agents provide recommendations or analysis that inform human decisions, but do not execute actions themselves. An advisory agent might suggest which support ticket to prioritize, recommend a discount amount for a sales negotiation, or flag a transaction as potentially fraudulent. The human reviews the recommendation and decides whether to act on it. Compliance requirements for advisory agents are higher than for assistive agents because the recommendations, even if not directly executed, influence human behavior and may need to be audited if the resulting decisions are challenged. You must log the recommendation, the data inputs used to generate it, the confidence or score associated with it, and whether the human accepted, rejected, or modified the recommendation. Retention periods are typically 90 days to one year, depending on the domain. If the recommendations relate to regulated decisions—hiring, lending, medical treatment—retention may extend to three to seven years to match the statute of limitations for discrimination or malpractice claims.

Semi-autonomous agents execute actions within predefined boundaries, with human approval required for actions that exceed those boundaries. An example is an agent that automatically responds to common customer service inquiries but escalates unusual requests to a human. Another example is an agent that places routine supply orders below a certain dollar threshold but requires approval for larger purchases. Compliance requirements for semi-autonomous agents depend on the significance of the actions they can take autonomously. For low-stakes actions, log the action, the triggering condition, and the outcome. For higher-stakes actions, also log the decision criteria, the data inputs, and the confidence level. Retention periods range from six months to three years, with longer retention required if the actions have financial or legal consequences. You must also log escalations—when the agent determined it could not act autonomously and why, along with the human decision that followed.

Autonomous agents with human oversight operate independently but are subject to audit, review, or intervention by human supervisors. An example is an agent that approves mortgage applications autonomously, with a random sample reviewed by human underwriters for quality assurance. Another example is an agent that adjusts server capacity autonomously, with an operations team monitoring for anomalies. Compliance requirements are stringent. You must log every decision the agent makes, the data inputs used, the decision criteria applied, the outcome, and any human reviews or interventions. You must be able to reproduce the agent's reasoning process if a decision is challenged. Retention periods are determined by regulatory and litigation risk, typically three to seven years for financial and legal decisions, and potentially longer if the decisions involve protected classes or regulated industries. Under the EU AI Act, high-risk AI systems in this category must maintain logs sufficient to enable post-market monitoring and regulatory audits.

Fully autonomous agents operate without human oversight or intervention, making consequential decisions in real time. These are rare in practice as of 2026, and where they exist, they are subject to the most rigorous compliance requirements. Examples include autonomous trading systems in financial markets, autonomous medical diagnostic systems, and autonomous content moderation systems at scale. Compliance requirements include complete decision auditability, real-time monitoring, incident reporting, and the ability to halt or override the system immediately if it behaves unexpectedly. Logs must include every decision, every data input, every external system interaction, and a complete reasoning trace. Retention periods are often ten years or more, determined by industry-specific regulations and the potential for long-tail litigation. In financial services, for example, records related to trading decisions must be retained for at least six years under SEC rules, and longer in some jurisdictions.

## What to Log at Each Tier

The logging requirements vary not just in volume but in the type of information captured. For assistive agents, log the user ID, the task requested, the timestamp, the tool calls made, and the result status. You do not need to log intermediate reasoning steps or model outputs unless they are directly presented to the user. The goal is to support basic accountability: if a user claims the agent did something unexpected, you can verify what actually happened.

For advisory agents, log the recommendation or analysis provided, the data inputs that informed it, the model or rule set used to generate it, the confidence score or probability, and the user's response. If the agent provides multiple recommendations ranked by priority or confidence, log all of them, not just the top recommendation. This allows you to audit whether the agent's ranking was reasonable and whether the human followed the agent's advice or deviated from it. If the human deviates—for example, approving a loan application the agent recommended rejecting—log the reason for the deviation if the user provides one. This creates a feedback loop that can be used to improve the agent and also provides evidence of human judgment if the decision is later challenged.

For semi-autonomous agents, log every autonomous action with the same level of detail you would log if a human had taken the action. Include the triggering event or condition, the data retrieved to inform the action, the decision logic applied, the action taken, and the immediate outcome. If the agent performed a risk assessment or confidence check before acting, log the assessment result. If the agent considered multiple possible actions and selected one based on criteria, log the alternatives considered and why the selected action was chosen. For escalations, log the triggering condition, the reason the agent could not act autonomously, and the human decision that resolved the escalation. This level of detail is necessary to demonstrate that the agent operated within its authorized boundaries and that humans were appropriately involved in edge cases.

For autonomous agents with oversight, log everything required for semi-autonomous agents, plus data necessary to reproduce the decision. This includes the exact input data used, the model version or rule set version applied, the intermediate reasoning steps if available, and the final decision with confidence or probability scores. If the agent accessed external APIs or databases, log the queries sent and the responses received. If the agent applied business rules or regulatory constraints, log which rules were evaluated and whether they were satisfied. This logging standard enables post-hoc audits where a regulator or auditor can examine a specific decision and verify that it complied with applicable policies and regulations. In some regulated industries, you must also log negative decisions—cases where the agent determined not to take an action—because the absence of action may itself be subject to scrutiny.

For fully autonomous agents, log every state change, every decision point, every external interaction, and every internal reasoning step. This is comprehensive observability, not just logging. The goal is to create a complete forensic record that can be analyzed if the agent causes harm, violates policy, or behaves in an unexpected way. Include not just the agent's outputs but its inputs, context, and any feedback it received from the environment or other systems. Log errors and exceptions with full diagnostic information, since failures in fully autonomous systems can have serious consequences. This level of logging generates significant data volume and storage costs, but it is not optional if you are operating truly autonomous systems in consequential domains.

## Retention Periods and Regulatory Drivers

Retention periods are driven by regulatory requirements, litigation risk, and operational needs. The baseline is to retain logs long enough to satisfy the longest applicable statute of limitations or regulatory audit window. In the United States, employment discrimination claims have a statute of limitations of 180 to 300 days, but many organizations retain employment-related data for three to seven years to account for discovery delays and tolling provisions. In lending, the Equal Credit Opportunity Act requires creditors to retain records for 25 months, but in practice many retain them longer to defend against fair lending challenges. In healthcare, HIPAA requires retention of audit logs for six years. Under the EU AI Act, high-risk systems must retain logs for a period appropriate to the risk level, with guidance suggesting at least six months for lower-risk high-risk systems and several years for higher-risk applications.

Financial services have some of the longest retention requirements. The SEC requires broker-dealers to retain records related to trades, communications, and customer accounts for three to six years depending on the record type. Dodd-Frank regulations require swap data repositories to retain records for five years. The UK's Financial Conduct Authority requires seven years for most records. If your agent makes trading decisions, credit decisions, or handles customer funds, plan for retention periods of at least six years and potentially longer if you operate in multiple jurisdictions.

Healthcare and life sciences also have extended retention requirements. Medical records must be retained for the period specified by state law, which ranges from five to ten years for adult patients and longer for pediatric patients. If your agent is involved in diagnosis, treatment recommendations, or clinical decision support, logs must be retained for the same period as the underlying medical records. The FDA requires clinical trial data to be retained for two years after the drug or device is approved or discontinued, and if the agent was used in trial data analysis or patient monitoring, the agent's logs are part of that data set.

Beyond regulatory minimums, consider litigation risk. If your agent makes decisions that could be challenged in court—hiring, termination, credit denial, insurance claims, content moderation—retain logs long enough to defend those decisions through the full litigation lifecycle, which can extend five to ten years from the date of the decision. Even if you win at trial, appeals can add years to the timeline, and you need logs to support your defense at each stage.

Retention is not just about keeping data—it is about keeping data in a usable, auditable format. Logs stored in compressed archives that take days to extract and analyze are not useful for responding to regulatory inquiries or litigation discovery. Your retention architecture must support efficient retrieval and analysis of historical logs, filtered by user, time period, decision type, or outcome. This typically requires indexing, searchable storage, and query tooling that remains compatible with log formats as your system evolves. In 2025, a healthcare company faced sanctions during litigation because they had retained the required logs but could not extract the relevant records in the format requested by the court. The logs were stored in a proprietary format that required a version of the parsing software that was no longer supported, and it took four months to reconstruct the tooling needed to read the data.

## Reproducing Agent Decisions for Audit

One of the most challenging compliance requirements is decision reproducibility: the ability to take a historical decision, examine the inputs and logic that produced it, and verify that the decision complied with applicable policies. Reproducibility is difficult because agent decisions are influenced by context that changes over time—model versions, prompt templates, retrieved documents, external API responses, business rules, and feature flag states. A decision made in March 2025 used a model version that was updated in April, a knowledge base that has since been expanded, and business rules that were revised in May. If you try to reproduce the decision in January 2026, you get a different result even with the same user input, because the underlying system has changed.

Reproducibility requires versioning and immutability. Every component that influences the agent's decision must be versioned, and the versions used for a given decision must be logged. This includes the model version or checkpoint, the prompt template version, the tool implementation version, the knowledge base snapshot, the business rule set version, and the feature flags active at the time. When you log a decision, log these version identifiers. When you need to reproduce the decision, reconstruct the environment with the same versions. This is analogous to reproducible builds in software engineering, where the goal is to produce identical output from the same source code and build environment.

In practice, full reproducibility is expensive and often impractical. Model inference is not deterministic, especially if you use sampling-based generation with non-zero temperature. Retrieved documents may be ranked differently if the retrieval index has been updated. External API responses are not guaranteed to be stable. A pragmatic approach is to log the actual inputs and outputs used in the decision, rather than attempting to reproduce the decision by re-running the agent. Log the retrieved documents, the API responses, the intermediate tool call results, and the final agent output. This provides a forensic record of what the agent saw and what it decided, even if you cannot perfectly reproduce the decision by re-executing the agent.

For higher-autonomy tiers and regulated domains, consider deterministic replay systems that capture the complete execution trace of the agent—every function call, every API response, every model output—and allow you to replay the trace exactly as it occurred. This is computationally expensive and generates large trace files, but it provides the highest level of reproducibility. Some agent frameworks are beginning to support trace capture and replay as a first-class feature, though as of 2026 this remains an emerging capability.

## Human-in-the-Loop Logging and Approval Trails

For agents that require human approval or oversight, compliance depends on logging not just what the agent decided but what the human decided and when. If your agent recommends an action and a human approves it, the approval must be logged with a user ID, timestamp, and any justification or comments the human provided. If the human overrides or modifies the agent's recommendation, log the original recommendation, the human's modification, and the reason for the change if provided. This creates an audit trail that demonstrates human accountability and allows you to evaluate whether humans are rubber-stamping agent recommendations or exercising independent judgment.

A common compliance failure is implementing human-in-the-loop workflows where the human's approval is not logged or is logged in a way that cannot be linked back to the specific agent decision being approved. For example, a human clicks an "approve batch" button that approves 50 agent recommendations at once, but the system only logs that the human approved a batch, not which specific recommendations were in the batch or whether the human reviewed them individually. This does not satisfy compliance requirements for meaningful human oversight. Each approval must be linked to a specific decision, and the system must capture enough information to verify that the human had the opportunity to review the relevant context before approving.

In semi-autonomous and autonomous-with-oversight tiers, also log instances where the human did not intervene, if non-intervention is itself a form of approval. For example, if your agent makes decisions that are subject to review within 24 hours, and a human reviewer does not override the decision within that window, the decision is considered approved by default. Log the decision, the review window, and the fact that no override occurred. This demonstrates that the human had the opportunity to intervene and chose not to, which is legally significant in some contexts.

## Incident Reporting and Regulatory Disclosure

Some agent failures trigger mandatory reporting obligations. Under the EU AI Act, providers of high-risk AI systems must report serious incidents to national authorities. A serious incident is one that leads to death, serious health damage, serious property damage, or serious disruption to critical infrastructure. If your agent operates in healthcare, transportation, financial services, or critical infrastructure, you must have processes to detect serious incidents, assess whether they meet reporting thresholds, and file reports within the required timeframes. This requires real-time monitoring, automated incident detection, and escalation workflows that alert compliance and legal teams immediately when an incident occurs.

In financial services, certain trading anomalies, compliance breaches, or system failures must be reported to regulators within 24 to 72 hours. If your agent causes or contributes to such an event, the incident report must include what the agent did, what inputs it was responding to, and what controls failed to prevent the incident. This requires detailed logs and the ability to extract and analyze them under time pressure.

In healthcare, adverse events involving AI systems used in diagnosis or treatment may trigger FDA medical device reporting requirements. If your agent provides clinical decision support that influences patient care, and a patient is harmed, you may need to report the event and provide a detailed analysis of the agent's role. This requires logs that include the clinical data the agent processed, the recommendations it made, and the actions taken by the clinician based on those recommendations.

Beyond mandatory reporting, consider voluntary disclosure obligations. If you discover that your agent has been making systematically biased or erroneous decisions, you may have an ethical and legal obligation to notify affected individuals, even if no regulation explicitly requires it. In 2025, a hiring software company discovered that their resume screening agent had been systematically downranking candidates with names associated with certain ethnic groups due to biased training data. The company voluntarily notified the affected candidates and offered them the opportunity to reapply without the biased screening. This proactive disclosure mitigated reputational damage and demonstrated good faith, reducing the risk of class action litigation.

## Balancing Compliance and Operational Burden

Comprehensive logging and long retention periods create operational burden. Logs consume storage, logging infrastructure has availability and performance costs, and log analysis for audits requires tooling and expertise. You must balance compliance requirements against operational feasibility. The key is to log selectively based on the autonomy tier and decision significance, and to use tiered storage with automated lifecycle management.

For low-stakes decisions, use sampled logging: log a random subset of decisions at full detail, and log only summary statistics for the rest. This provides enough data to monitor agent behavior and detect anomalies while reducing storage costs. For high-stakes decisions, log everything. Use automated classification to determine which decisions are high-stakes based on criteria like decision amount, user impact, or regulatory domain.

Implement tiered storage where recent logs are stored in fast, expensive storage for real-time analysis, and older logs are moved to cheaper, slower storage for compliance retention. After the retention period expires, logs are automatically purged. This lifecycle management reduces storage costs by an order of magnitude compared to keeping all logs in high-performance storage.

Use log aggregation and summarization to reduce the volume of data that must be reviewed in audits. Instead of handing a regulator 10 million raw log lines, provide them with an aggregated report showing decision volumes, approval rates, override rates, and error rates by category, along with the ability to drill down into specific decisions if they identify anomalies. This makes audits more efficient and demonstrates that you have operational control over your agent's behavior.

The goal is not to log everything that could possibly be useful—that is neither feasible nor necessary. The goal is to log what is required to demonstrate compliance, support incident investigation, and defend decisions when challenged, at a level of detail proportional to the autonomy tier and risk level of your agent.

This concludes the examination of compliance logging and retention requirements by autonomy tier, which together with the earlier discussions of data minimization and secret-adjacent attacks, forms a comprehensive framework for agent safety, guardrails, and runtime control—ensuring that your agent operates within acceptable boundaries, protects sensitive information, and produces auditable, defensible outcomes in production environments.
