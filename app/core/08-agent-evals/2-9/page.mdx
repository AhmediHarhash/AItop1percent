# 2.9 â€” Choosing the Right Pattern: Task Characteristics and Tradeoffs

In July 2024, a fintech startup burned through forty-seven thousand dollars in API costs in a single weekend. Their customer support agent, designed to handle complex refund disputes, was running a Tree of Thoughts pattern on every single incoming ticket. When a customer asked "Where is my refund?" the agent would explore multiple reasoning paths, backtrack through alternatives, evaluate candidate solutions against each other, and eventually arrive at an answer that a simple database lookup could have provided in two hundred milliseconds. The engineering team had read the ToT paper, been impressed by its performance on complex reasoning tasks, and deployed it everywhere.

Using Tree of Thoughts to answer "Where is my refund?" is like hiring a management consultant to make you coffee. Technically it works, but you're paying a thousand times more than the task warrants. By Monday morning, when the finance team flagged the spend anomaly, the damage was done. Forty-seven thousand dollars in API calls that could have been handled for under three hundred dollars with basic ReAct routing and a few database lookups. The CTO mandated a full audit of every agent pattern in production, and what they found was worse than the weekend spike. Across their entire agent fleet, seventy-two percent of tasks were using patterns at least one level more complex than necessary. The company had been hemorrhaging money for months, just not at weekend-spike volumes.

The pattern selection problem sits at the heart of agent engineering. You have five major orchestration patterns at your disposal: ReAct for simple iterative reasoning, Plan-and-Execute for complex multi-step workflows, Reflection for quality-critical outputs, Tree of Thoughts for exploration-heavy problems, and ReWOO for latency-sensitive operations. Each pattern represents a different set of tradeoffs along four critical dimensions: output quality, response latency, computational cost, and system debuggability. The art of agent engineering lies not in mastering one pattern but in developing the judgment to match patterns to task characteristics.

Most teams get this wrong in predictable ways. They either pick one pattern and use it for everything, creating the PayStream problem, or they prematurely optimize for edge cases, building systems so complex that nobody can understand or maintain them. The correct approach is both simpler and more nuanced: start with the simplest pattern that might work, measure whether it actually works, and upgrade only when you have specific evidence that a more complex pattern would solve a specific problem.

## The Task Characteristic Framework

The first dimension to evaluate is task complexity, but not in the vague sense of "this seems hard." What matters is the number of sequential dependencies in the task. If you need to do A before you can determine what B should be, and B before C, you have sequential dependencies. A customer support query like "I was charged twice for my subscription" typically has low sequential complexity: check the billing records, identify the duplicate charge, process a refund. Each step is straightforward once you know what the previous step revealed.

Compare this to "Design a marketing campaign for our new product launch targeting millennials in urban areas with household income above seventy-five thousand dollars." This task has high sequential complexity: you need to research the target demographic before you can identify effective channels, understand channel characteristics before you can draft messaging, test messaging variants before you can allocate budget. The number of steps where each step depends on learning from the previous step determines your task complexity.

The second dimension is output quality sensitivity. Some tasks have clear right answers: "What is the account balance?" has one correct value in the database. Other tasks have a wide range of acceptable outputs: "Write a welcome email for new users" has hundreds of versions that would work fine. Quality sensitivity is not about importance. A mission-critical task can have low quality sensitivity if any of many solutions would work. What matters is how much value you get from incremental quality improvements. If going from an eighty-percent-quality output to a ninety-five-percent-quality output would change user behavior or business outcomes, you have high quality sensitivity. If users would not notice the difference, you have low quality sensitivity.

This distinction determines whether patterns like Reflection or Tree of Thoughts, which trade latency and cost for quality, make economic sense. A SaaS company in late 2025 ran a controlled experiment on their contract generation agent. They tested single-shot generation against three-round reflection. The reflected contracts scored twelve percent higher on legal review quality metrics. But when they measured downstream impact, the difference in contract disputes was statistically insignificant. The quality improvement was real but did not matter for business outcomes. They were paying three times more for generation that did not move any metric their business cared about.

The third dimension is exploration requirement. Some tasks have well-defined solution spaces: "Generate a SQL query to find users who signed up last week" has one obvious approach and maybe two or three variations. Other tasks have vast, poorly-mapped solution spaces: "Suggest novel approaches to reduce customer churn" could go in dozens of directions, each worth exploring. Exploration requirement is about uncertainty in the solution approach, not uncertainty in the solution details. If you know roughly how to solve a problem but need to figure out specific parameters, that is low exploration. If you genuinely do not know which of several fundamentally different approaches might work best, that is high exploration.

The fourth dimension is latency tolerance. This is straightforward but often misunderstood. Latency tolerance is not about what users prefer in the abstract. Everyone prefers faster responses. Latency tolerance is about the point where additional latency changes user behavior or business outcomes. A chatbot responding to "What are your hours?" has millisecond-level latency tolerance: users expect instant answers to factual queries, and a five-second delay feels broken. An agent analyzing a ten-page legal contract has hour-level latency tolerance: users expect the task to take time, and whether it takes thirty minutes or ninety minutes rarely matters. Measure latency tolerance by observing actual user behavior, not by asking users what they want.

These four dimensions create a decision framework. Map your task along each dimension, and the appropriate pattern becomes clear.

## Pattern Matching: When Each Pattern Wins

ReAct dominates the space of simple, low-latency tasks with moderate quality requirements. If your task has sequential dependencies but each step is relatively straightforward, if users expect responses in seconds not minutes, and if getting to a good-enough answer quickly matters more than finding the optimal answer slowly, ReAct is your default choice. Customer support queries, simple research tasks, basic code generation, and routine data analysis all fall into this category.

ReAct's strength is its simplicity: thought, action, observation, repeat until done. The model decides what to do next based on what it learned in the previous step. No complex planning, no backtracking, no parallel exploration. This simplicity makes ReAct fast, cheap, and debuggable. When a ReAct agent fails, you can read through the thought-action-observation chain and see exactly where it went wrong. In production systems running GPT-5 or Claude Opus 4.5, a typical five-step ReAct chain completes in under ten seconds and costs less than two cents per invocation. For high-volume customer support operations handling thousands of tickets per day, this cost profile is sustainable.

The latency characteristics matter more than teams realize. ReAct generates one reasoning step at a time, serially. For a task with five steps, you make five sequential LLM calls. If each call takes two seconds, that is ten seconds end-to-end. For many tasks, this is perfectly acceptable. For a user waiting for a chatbot response, ten seconds feels eternal. Know your latency requirements before you commit to ReAct.

Plan-and-Execute excels at complex, multi-step tasks where understanding the full scope upfront enables better execution. If your task involves coordinating multiple sub-tasks, some of which could be parallelized, or if execution errors are expensive and planning reduces them, Plan-and-Execute is worth the overhead. Marketing campaign creation, complex data pipeline construction, multi-file code refactoring, and research report generation all benefit from explicit planning.

The pattern's strength is separating "what to do" from "how to do it," which reduces thrashing and enables parallelization. The plan becomes a contract that execution can follow, and when execution fails, you can revise the plan without starting over. The tradeoff is upfront latency. You pay for a planning phase before you do any real work. For a task that turns out to be simpler than expected, this planning overhead is pure waste. For a task that is genuinely complex, planning saves time overall by preventing false starts.

Reflection suits quality-critical tasks where the cost of poor output is high and you have latency budget to invest in iteration. Legal document generation, technical writing, code review, and strategic analysis all benefit from reflection. The pattern's strength is systematic quality improvement: generate a candidate output, critique it, revise based on the critique, repeat until quality criteria are met. This mirrors how humans produce high-quality work. The tradeoff is multiplicative cost: if you do three reflection rounds, you pay roughly three times as much as single-shot generation.

The latency implications are severe. Three reflection rounds on a task that takes thirty seconds per round means ninety seconds minimum, and that assumes you know when to stop. Unbounded reflection can spiral into minute-long response times. You need clear termination criteria: a quality threshold, a maximum iteration count, or a time budget.

Tree of Thoughts makes sense for exploration-heavy tasks with high quality sensitivity and substantial latency tolerance. Novel problem-solving, creative generation, strategic planning, and research hypothesis development all benefit from explicit exploration. The pattern's strength is systematic coverage of the solution space: generate multiple candidate approaches, evaluate them against each other, expand the most promising, backtrack from dead ends. This is expensive but effective for tasks where finding the right approach is more important than executing quickly.

The latency and cost implications make ToT unsuitable for high-volume production systems. You are paying for speculation: most of the branches you explore will not end up in the final solution. This is worthwhile only when the value of finding a better solution outweighs the cost of the search. For most production tasks, it does not. The debugging challenges are also substantial: when a ToT agent produces a result, understanding why it chose that path over alternatives requires examining the entire search tree. This is feasible for occasional failure analysis but impractical at scale.

ReWOO fits latency-sensitive tasks with moderate complexity where sub-tasks can be identified upfront and executed in parallel. Structured data retrieval, multi-source research, parallel API orchestration, and batch processing all benefit from ReWOO's approach. The pattern's strength is minimizing serial LLM calls: plan all the actions upfront, execute them in parallel, reason over the combined results. For tasks with five independent sub-tasks, ReWOO can be five times faster than ReAct. The tradeoff is rigidity: you commit to your action plan before seeing any results. If the results reveal that you asked the wrong questions, you cannot adapt without starting over.

## The Start-Simple Upgrade-When-Necessary Strategy

The single most important principle in pattern selection is this: start with ReAct. Not because ReAct is the best pattern for every task, but because ReAct is the simplest pattern that handles a surprisingly wide range of tasks acceptably well. You can implement a basic ReAct agent in an afternoon. You can debug it by reading logs. You can iterate on it quickly. Most importantly, you can measure whether it works before you invest in anything more complex.

Teams that start with Plan-and-Execute or Tree of Thoughts often spend weeks building infrastructure before they discover whether their task is even suitable for agent-based solutions. A healthcare technology company in early 2025 spent three months building a Plan-and-Execute system for their clinical documentation agent before realizing that ninety-one percent of documentation tasks were simple enough for ReAct. They had over-engineered the system for the nine percent of complex cases and made the common case slower and more expensive.

The upgrade path becomes clear through measurement. Deploy ReAct, instrument it thoroughly, and watch where it struggles. If you see the agent thrashing, starting tasks and abandoning them, repeatedly trying the same failed approaches, that suggests planning would help. Upgrade to Plan-and-Execute for those specific task types. If you see the agent producing outputs that work but are not good enough, and you have latency budget to invest in iteration, that suggests Reflection would help. Add reflection loops to those output types. If you see the agent picking the first solution it thinks of and missing better alternatives, and you have cost budget for search, experiment with Tree of Thoughts on a subset.

This measurement-driven upgrade path prevents premature optimization. You do not pay for complexity you do not need. It also builds understanding: by starting simple and adding complexity only when you see specific failures, you develop intuition for which task characteristics require which patterns. This intuition generalizes to future projects. The teams that build the best agents are not the teams with the most sophisticated patterns. They are the teams with the best judgment about when sophistication is warranted.

The migration strategy matters as much as the initial choice. When you upgrade from ReAct to Plan-and-Execute, you do not replace your entire agent at once. You add planning to the subset of tasks that need it and leave the rest on ReAct. This requires orchestration logic that routes tasks to appropriate patterns, but that orchestration is simpler than forcing all tasks through the same pattern. When you add Reflection, you add it to specific output types where quality matters most, not to every agent response.

## Cost Modeling: Making Pattern Selection Data-Driven

Pattern selection without cost modeling is pattern selection by intuition, and intuition is unreliable. You need concrete numbers for each pattern applied to your specific tasks. This means measuring four things: the average number of LLM calls per task, the average tokens consumed per call, the average latency per task, and the success rate.

A mid-sized e-commerce company did this analysis in September 2025 for their product recommendation agent. ReAct averaged 4.2 LLM calls per recommendation, consuming about twelve thousand tokens total, at an average latency of 6.8 seconds and a success rate of eighty-nine percent. Plan-and-Execute averaged 6.1 calls, twenty-one thousand tokens, 11.4 seconds latency, and ninety-three percent success. Reflection averaged 8.7 calls, thirty-four thousand tokens, 19.2 seconds, and ninety-six percent success. The numbers made the decision obvious. Going from ReAct to Reflection tripled their cost and tripled their latency for a seven-percentage-point improvement in success rate. For a recommendation system where "good enough" recommendations still drive purchases, that tradeoff was not worth it. They stayed with ReAct and invested the saved compute budget in handling more concurrent users.

The cost model changes when you factor in the cost of failures. If a failed recommendation means nothing happens and the user sees a generic fallback, the cost of an eleven-percent failure rate is low. If a failed legal document generation means a lawyer has to rewrite the entire document from scratch at three hundred dollars per hour, even a four-percent failure rate is expensive. The effective cost of a pattern is the direct compute cost plus the failure rate multiplied by the cost per failure. When you model it this way, expensive patterns with low failure rates often beat cheap patterns with higher failure rates for high-stakes tasks.

Build a cost model for every agent task type you operate. Update it monthly as model pricing changes, as your task distribution shifts, and as you optimize your prompts. In 2026, model pricing is volatile. GPT-5 costs dropped forty percent between January and September 2025. Claude Opus 4.5's pricing changed twice in 2025. A pattern that was too expensive six months ago might be cost-effective today. A pattern that was cheap might become expensive if your traffic grows and you hit volume pricing tiers.

## Quality Versus Latency Versus Cost Versus Debuggability

Every pattern selection is a tradeoff. You cannot optimize for quality, latency, cost, and debuggability simultaneously. Understanding which dimension matters most for each task drives better decisions.

For a customer-facing chatbot, latency often dominates: a perfectly accurate answer that takes thirty seconds is worse than a pretty-good answer that takes three seconds, because users will not wait thirty seconds. You sacrifice quality and cost efficiency for speed. For a legal document generator, quality dominates: getting the contract right matters infinitely more than whether it takes five minutes or fifty minutes. You sacrifice latency and cost for quality. For a high-volume data processing pipeline, cost often dominates: processing ten million records per day means small per-record cost differences compound into large budget impacts. You sacrifice quality and latency for cost efficiency.

The debuggability dimension is the most commonly neglected and the most painful when ignored. Sophisticated patterns like Tree of Thoughts produce better outputs than simple patterns like ReAct on complex tasks, but when they fail, understanding why is vastly harder. For tasks you will run millions of times, where even a small failure rate creates hundreds or thousands of failures per day, debuggability becomes paramount. You need to understand failures quickly and fix them systematically. This often means choosing simpler patterns even when more complex patterns would produce better outputs, because the operational cost of debugging the complex pattern outweighs the quality benefit.

## The Production Distribution of Patterns in 2026

In practice, most production agents end up using ReAct for the majority of tasks, Plan-and-Execute for a minority of complex workflows, Reflection for a small set of quality-critical outputs, and Tree of Thoughts almost never. ReWOO sees use in specific latency-critical scenarios where parallelization is feasible. This distribution reflects the actual characteristics of production workloads: most tasks are relatively simple, most users care about latency, most businesses care about cost, and most teams care about being able to debug failures.

Industry data from enterprise agent deployments shows that the vast majority of production agent tasks use simple ReAct or single-step tool calling. Plan-and-Execute accounts for a smaller portion, primarily in document generation and multi-system workflow automation. Reflection is used in a small fraction of tasks, concentrated in legal, compliance, and content generation. Tree of Thoughts and other exploration patterns account for an even smaller percentage, almost exclusively in research and creative applications. These numbers reflect hard-won production experience, not theoretical preference.

The sophisticated patterns have their place, but their place is smaller than academic papers suggest. The correct mental model is not "which pattern is best" but "which pattern's tradeoffs align with this task's requirements." Build the capability to use multiple patterns, instrument heavily to understand where each pattern works, and develop the organizational discipline to match patterns to tasks based on evidence rather than enthusiasm.

The teams that do this well do not have the most sophisticated agents. They have the most reliable agents, the most cost-efficient agents, and the agents that ship features quickly because the engineering team can understand and modify them. That is the real goal: not maximum sophistication, but appropriate sophistication applied with precision.

## Pattern Routing: Automating the Selection

As your agent system matures, you will want to automate pattern selection rather than hardcoding it per task type. The simplest approach is a classifier that examines the incoming request and routes it to the appropriate pattern. This classifier can be rule-based initially, using keywords and task metadata to determine complexity. A request mentioning "quick lookup" or "check status" routes to ReAct. A request mentioning "generate report" or "analyze and recommend" routes to Plan-and-Execute.

More sophisticated systems use a lightweight LLM call to classify the task before choosing the pattern. The classification model examines the request, estimates complexity along the four dimensions, and selects the appropriate pattern. This adds a small amount of latency, typically two hundred to five hundred milliseconds, but can dramatically improve cost efficiency and success rates across your task portfolio. An insurance technology company implemented this approach in mid-2025 and reduced their monthly agent compute spend by thirty-eight percent while maintaining the same success rates, simply by routing simple tasks away from expensive patterns.

The key is measuring the router's accuracy. A pattern router that misclassifies ten percent of tasks is worse than no router at all if it sends simple tasks to expensive patterns. Track the router's decisions, compare them against post-hoc analysis of which pattern would have been optimal, and continuously improve the routing logic. Over time, your pattern router becomes one of the most valuable components in your agent infrastructure, silently saving money and improving performance across every task your agents handle.

With pattern selection understood, the next step is learning how to combine multiple patterns within a single agent system, which is where hybrid orchestration comes in.
