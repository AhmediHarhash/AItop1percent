# 1.6 â€” Agent Failure Modes: A Production Taxonomy

What kills agents in production is not model failures or prompt injection attacks. It's the systematic, predictable failure modes that appear when reasoning loops meet real-world messiness: infinite loops, tool misuse, context loss, goal drift, and resource exhaustion. In November 2025, a legal tech startup called CaseBuilder deployed an agent to draft discovery requests for civil litigation. The agent had access to case management databases, legal research tools, document templates, and a file system for saving drafts. During the first week of production, a partner at one of their pilot law firms submitted a request for discovery documents in a complex antitrust case. The agent began its work at nine AM on a Tuesday. By five PM, it had made four thousand tool calls, consumed six hundred dollars in API credits, and still had not produced a draft.

The engineering team pulled the logs and discovered the agent was stuck in what they called a "research rabbit hole." It would identify a legal precedent, retrieve the case opinion, find a citation to another case in that opinion, retrieve that case, find another citation, and continue recursively. The agent was executing a perfectly logical research strategy, each individual tool call was justified, but the aggregate behavior was an unbounded search through case law with no termination condition. It had visited over eight hundred cases, many only tangentially related to the original query, and showed no signs of stopping. The agent was exhibiting infinite loop behavior, one of the most common and most expensive failure modes in production agent systems.

Agents fail in predictable patterns. After two years of production deployments across thousands of companies, the industry has developed a comprehensive taxonomy of agent failure modes. These are not hypothetical edge cases, they are the actual reasons agents fail in the field. Understanding this taxonomy, recognizing the symptoms of each failure mode, knowing the root causes, and implementing the proven mitigations is essential for shipping reliable agent products. You will encounter every one of these failure modes. The question is whether you will detect and fix them in development or discover them in production.

## Infinite Loops and Runaway Execution

Infinite loops occur when the agent enters a cycle of actions that never terminates and never makes progress toward the goal. The CaseBuilder agent was in an infinite loop of case retrieval. Other common patterns include retry loops where the agent repeatedly tries an action that keeps failing, refinement loops where the agent makes minor edits to outputs without reaching a satisfactory state, and search loops where the agent explores options without ever committing to one.

The root cause of infinite loops is usually missing or ineffective termination conditions. The agent has a strategy that makes sense locally at each step but lacks a global stopping rule. Research agents loop when they lack completeness criteria, they do not know when they have gathered enough information. Workflow agents loop when they lack progress detection, they retry failed actions indefinitely without recognizing futility. Coding agents loop when they lack convergence criteria, they refine code forever without recognizing when it is good enough.

The symptoms are runaway resource consumption, very high tool call counts, low diversity in tool calls over time, and execution times far exceeding reasonable bounds. An agent that makes hundreds of similar tool calls in sequence is likely looping. An agent that runs for hours on a task that should take minutes is likely looping. An agent that uses ten times the expected API budget is likely looping.

The mitigations are hard limits, progress tracking, and loop detection. Hard limits impose maximum iteration counts, maximum execution time, or maximum cost budgets. When any limit is hit, the agent stops regardless of whether it completed the task. This is a safety net, not a solution, but it prevents infinite resource consumption. Progress tracking requires the agent to measure whether it is getting closer to the goal and stop if progress stalls. Loop detection identifies repetitive patterns in tool calls or state and triggers termination. Many production agents in 2026 use all three mitigations in defense-in-depth.

A subtler variant is pseudo-termination where the agent stops but declares success despite not completing the task. This happens when the agent has a flawed success condition and genuinely believes it succeeded. The CaseBuilder agent could have been designed to stop after retrieving fifty cases, it would have terminated but still failed to produce the required discovery draft. The mitigation is validating termination conditions against ground truth, not just accepting the agent's self-assessment.

## Tool Call Cascades and Error Amplification

Tool call cascades occur when one incorrect or suboptimal tool call triggers a chain of subsequent tool calls that compound the error. An agent makes a wrong first move, then makes additional moves to try to fix the consequences, each move creating more problems that require more fixes. The cascade continues until the agent has made dozens of tool calls and left multiple systems in inconsistent states.

A canonical example is a workflow agent that creates a record with incorrect data, then tries to update it but uses the wrong identifier, creating a second record instead, then tries to delete the duplicate but deletes the original, then tries to restore the original by creating a third record with slightly different data, and so on. Each action is a reasonable attempt to fix the previous error but the aggregate effect is chaos.

The root cause is lack of verification and rollback. The agent executes actions optimistically without checking results before proceeding. When an action has an unexpected outcome, the agent does not recognize it and compounds the error. If the agent had verified the first record creation succeeded with correct data, it would have caught the error before the cascade began.

The symptoms are high tool call counts concentrated in a short time window, multiple calls to the same tool with varying parameters, state churn where the same entities are created, updated, and deleted repeatedly, and error messages appearing later in the sequence. An agent that creates three customer records in ten seconds is probably in a cascade. An agent whose tool calls show a pattern of create-update-delete-create is probably in a cascade.

The mitigations are verification after each action, rollback on errors, and circuit breakers. After each tool call, the agent explicitly checks the result matches expectations before proceeding. If an action fails or produces an unexpected result, the agent does not try to patch it with more actions, it rolls back if possible or escalates. Circuit breakers track error rates per tool and stop calling tools that are consistently failing. This prevents cascades from propagating across many actions.

Transaction boundaries help contain cascades. If a sequence of actions is grouped into a transaction, a failure mid-sequence can roll back the entire transaction atomically. The agent tries again from the beginning rather than trying to patch a partially completed sequence. This is standard in database systems but less common in agent architectures where actions span multiple independent systems. Even without formal transactions, logical grouping of actions with all-or-nothing semantics reduces cascade risk.

## Hallucinated Tool Arguments

Hallucinated tool arguments occur when the agent generates tool calls with parameters that look plausible but are factually incorrect. The agent calls a function with a customer ID that does not exist, or a file path that is not accessible, or a date in an invalid format. The tool call fails and the agent either retries with the same hallucinated argument, tries a different hallucinated argument, or gives up.

The root cause is the LLM's tendency to generate plausible-sounding text without grounding in actual data. If the agent needs to reference a customer ID and does not have it in context, it might generate something like "CUST-001234" because that looks like a plausible ID format. If the agent needs a file path, it might generate "/data/reports/2025/summary.pdf" because that looks like a reasonable path structure. These hallucinations are not malicious or random, they are the model's best guess at what a valid argument would look like.

The symptoms are tool call failures with "not found" or "invalid argument" errors, repeated tool calls with slightly different but still invalid arguments, and tool arguments that do not appear in the agent's context. If the agent calls a database API with a record ID that was never mentioned in prior observations, it is likely hallucinated. If the agent tries three different customer IDs in sequence and all three return "not found," they are likely hallucinated.

The mitigations are constrained generation, argument validation, and retrieval before action. Constrained generation uses tool schemas with strict types and enums to limit what arguments the agent can generate. If a tool parameter must be a customer ID from a specific set, the schema should enumerate valid IDs or reference a lookup table. Argument validation checks arguments against ground truth before executing the tool. If the agent generates a customer ID, validate it exists before calling the tool. Retrieval before action means the agent explicitly retrieves valid argument values through observation before planning actions, so arguments are grounded in real data rather than generated from patterns.

Some agent frameworks in 2026 use structured outputs where tool arguments must be selected from a provided set rather than generated freely. The agent receives a list of valid customer IDs and must choose from that list. This eliminates hallucination at the cost of increased context size and reduced flexibility. The tradeoff is often worth it for critical parameters like identifiers and foreign keys.

## Premature Termination

Premature termination occurs when the agent stops before completing the task. The agent was supposed to generate a report, analyze data, execute a workflow, or solve a problem, but it declares success or gives up partway through. The user receives incomplete results and often does not realize they are incomplete until they try to use them.

The root cause is usually flawed success conditions or inadequate task decomposition. The agent was told to "create a customer onboarding report" but not told what must be included in the report. It generates a report with some relevant information and decides it is done. Or the agent was told to "fix the failing tests" and fixed one test but missed the others because it did not enumerate all failing tests upfront. The agent genuinely believes it succeeded because its success condition was too vague or narrow.

The symptoms are outputs that are shorter or less complete than expected, tasks that finish much faster than reasonable, agent logs showing the agent stopped after early successes without attempting remaining work, and user complaints about missing functionality or incomplete results. If a task that should take twenty tool calls finishes in five, premature termination is likely. If the agent's final output is half the length of reference examples, premature termination is likely.

The mitigations are explicit checklists, completeness verification, and reference examples. Before starting, the agent generates a checklist of what must be accomplished. Before terminating, it verifies every checklist item is complete. This forces the agent to decompose the task upfront and track progress against all components. Completeness verification uses heuristics or rules to check outputs are sufficiently detailed. A report must have at least N sections, a code file must implement all required functions, a workflow must complete all defined steps. Reference examples calibrate the agent's sense of what "complete" looks like for the task type.

Conversation flow design matters for interactive agents. If an agent is helping a user through a multi-step process, it should explicitly confirm at each step that the user is satisfied before moving on. Premature termination often happens when the agent assumes the user is satisfied based on minimal feedback. Better flow design requires explicit confirmation or validates progress against task requirements independent of user feedback.

## Goal Drift and Context Erosion

Goal drift occurs when the agent gradually shifts away from the original objective as it executes. The agent was supposed to research battery technology but ends up researching electric vehicle market trends. The agent was supposed to fix a bug but ends up refactoring unrelated code. The agent was supposed to draft a contract but ends up researching case law with only tangential relevance. Each individual step seems related to the previous step, but the cumulative effect is deviation from the goal.

The root cause is weak goal maintenance and context window limitations. The agent starts with a clear goal but as it takes actions and receives results, the context fills with information related to those actions. The original goal fades to the background. The agent starts following interesting threads in the new information, threads that are related to recent context but not to the original goal. This is especially common in long-running tasks where the agent's context window is dominated by recent actions and observations.

The symptoms are outputs that are loosely related but not directly responsive to the request, execution paths that wander through many topics or code areas, and tool calls whose relevance to the original goal is unclear. If you read the agent's logs chronologically and the early actions are about topic A but the late actions are about topic B, goal drift has occurred. If the agent was asked to do X and delivered Y, where Y is interesting but not X, goal drift occurred.

The mitigations are goal reinforcement, periodic re-grounding, and relevance checking. Goal reinforcement means the original task description is included in every prompt to the agent, not just the initial prompt. As the context fills with observations and actions, the goal remains visible. Periodic re-grounding means the agent is prompted every N iterations to explicitly check whether its current actions serve the original goal. Relevance checking means before each action, the agent briefly justifies how that action contributes to the goal. If it cannot articulate a clear connection, the action is probably drift.

Hierarchical task decomposition helps by creating sub-goals that guide progress. Instead of one vague goal like "research electric vehicles," the agent decomposes it into specific sub-goals like "identify leading manufacturers," "analyze battery technology trends," "review regulatory environment," each with clear scope. The agent works on one sub-goal at a time and explicitly closes it before moving to the next. This provides structure that resists drift.

## Context Window Overflow and Information Loss

Context window overflow occurs when the agent's conversation history exceeds the model's context window and information starts getting truncated. The agent loses access to earlier observations, forgets what it has already tried, or loses critical context about the task. This leads to repetitive behavior, forgotten constraints, or actions that contradict earlier decisions.

The root cause is unbounded context accumulation. Every observation and action adds to the context. In long-running tasks or tasks with verbose tool outputs, the context grows until it hits the model's limit. Most LLM APIs handle overflow by truncating from the beginning, which means the agent loses its earliest memories first. This often includes the original task description and early important observations.

The symptoms are repetitive tool calls where the agent tries actions it already tried, contradictory decisions where later actions violate earlier constraints, failure to use information from early in the conversation, and errors indicating the agent has forgotten parts of the task. If the agent asks for information it already received, context overflow has likely occurred. If the agent violates a constraint that was stated early but followed initially, context overflow is likely.

The mitigations are context summarization, selective retention, and external memory. Context summarization means after every N actions, the agent generates a summary of progress so far and important information to retain. The detailed history is discarded and replaced with the summary, freeing context space. Selective retention means not all tool outputs are kept in context verbatim. Verbose outputs are summarized or filtered to keep only relevant parts. External memory means important information is written to external storage and retrieved when needed rather than kept in context continuously.

Some agent frameworks use a memory hierarchy where recent context is kept in full, older context is summarized, and very old context is archived externally and only retrieved if relevant to current actions. This mimics human memory where recent events are vivid, older events are remembered in summary, and distant events require conscious recall effort.

Tool design impacts context consumption. Tools that return large JSON blobs or verbose text consume context faster than tools that return concise structured data. If your tools tend to produce kilobytes of output, you will hit context limits quickly. Better tool design returns only information the agent needs in a compact format. If details are needed, provide tools for drilling down rather than dumping everything upfront.

## Cost Explosion and Budget Overruns

Cost explosion occurs when the agent's execution costs far exceed expectations, consuming budgets, hitting rate limits, or making the product economically unviable. An agent that was supposed to cost fifty cents per execution actually costs fifty dollars. An agent that was supposed to handle a thousand requests per day hits API rate limits after a hundred requests. An agent that was economically feasible in testing becomes a money pit in production.

The root cause is usually unbounded execution combined with expensive operations. The agent makes far more tool calls than expected, or calls expensive tools repeatedly, or runs much longer than expected. This happens when the agent lacks cost awareness, treats all tools as equally cheap, and has no budget constraints. An agent that does not know or care that calling a particular API costs one dollar per call will happily make a hundred such calls to solve a task worth ten dollars.

The symptoms are invoice surprises, rate limit errors appearing in production, execution costs that vary wildly across similar requests, and economic models that do not close. If your agent bill is ten times projections, cost explosion has occurred. If you hit rate limits despite moderate traffic, cost explosion has occurred. If some agent executions cost a hundred times more than others for similar tasks, you have cost explosion on the high-cost executions.

The mitigations are cost budgets, tool cost awareness, and caching. Cost budgets give each agent execution a maximum spend. The agent tracks costs as it executes and stops if it exceeds the budget. This prevents individual executions from running away but requires tools to expose cost information. Tool cost awareness means the agent's planning considers the relative cost of different tools and prefers cheaper options when they suffice. Caching means repeated identical tool calls return cached results instead of hitting the API again. This is especially effective for read-only tools like search or retrieval.

Tiered tool sets help by providing cheap and expensive versions of capabilities. A research agent might have a cheap web search tool and an expensive specialized database tool. The agent tries cheap tools first and escalates to expensive tools only when necessary. This requires the agent to understand tool trade-offs and plan accordingly, which works better for sophisticated agents than simple ones.

Cost monitoring and anomaly detection catch explosions early. If an agent execution is tracking toward ten times the median cost, flag it and potentially terminate it before it completes. This is a blunt instrument but prevents worst-case outcomes. Better is designing the agent to avoid explosion in the first place through budgets and cost-aware planning.

## Stale State and Race Conditions

Stale state failures occur when the agent acts on information that was true when observed but is no longer true when the action executes. The agent reads a value, plans an action based on that value, and by the time it executes, the value has changed. A workflow agent reads that a database record is in status A, plans to transition it to status B, but by the time it executes the transition, another process has already moved it to status C. The action fails or produces an incorrect state.

The root cause is time delay between observation and action in environments that change during execution. Most production environments are concurrent systems where multiple processes modify shared state. Agents observe state snapshots but those snapshots go stale as time passes. The longer the agent takes to plan and act, the higher the chance the environment has changed.

The symptoms are action failures with "precondition not met" or "conflict" errors, state inconsistencies where the agent's updates are based on stale assumptions, and race conditions where the agent's actions interfere with concurrent processes. If the agent frequently encounters "record already updated" errors, stale state is likely. If the agent's actions sometimes succeed and sometimes fail on identical requests, races are likely.

The mitigations are optimistic concurrency control, re-verification before action, and transactional semantics. Optimistic concurrency control means actions include a version or timestamp from the observation they are based on. The system rejects the action if the version has changed since observation, forcing the agent to re-observe and re-plan. Re-verification means the agent re-checks critical assumptions immediately before executing actions. If assumptions no longer hold, the agent replans. Transactional semantics mean related actions are grouped so they either all succeed or all fail, preventing partial updates based on stale state.

Minimizing time between observation and action helps. Tight loops where the agent observes and acts quickly reduce staleness windows. Loose loops where the agent observes, plans extensively, then acts much later increase staleness risk. For environments with high concurrency, tight loops are safer despite their overhead.

Some agent architectures use event-driven designs where the agent subscribes to state changes and reacts immediately rather than polling. This reduces staleness because the agent always works with fresh notifications. The tradeoff is increased complexity in handling event streams and ensuring the agent does not miss events or process them out of order.

## Hallucinated Tool Existence and Capability Overestimation

Hallucinated tool existence occurs when the agent attempts to use tools that do not exist or calls tools with capabilities they do not have. The agent decides it needs to do X, assumes there must be a tool for X, and generates a call to a plausible-sounding but nonexistent tool. Or the agent has a tool that does Y but assumes it can also do X because X is related to Y, and generates a call with parameters or expectations the tool does not support.

The root cause is the LLM's world knowledge including information about common tools and APIs that might not be present in your specific agent system. The model knows that most systems have tools for sending email, so when your agent needs to send email, it might generate a call to "send_email" even if you never provided such a tool. Or the model knows that database tools often support complex queries, so it generates a sophisticated query for your simple database tool that only supports basic lookups.

The symptoms are tool call failures with "tool not found" or "unsupported operation" errors, tool calls that look reasonable but do not match your tool definitions, and parameters that make sense conceptually but are not in your tool schemas. If the agent calls "get_current_weather" and you never defined that tool, it hallucinated tool existence. If the agent calls your database tool with a SQL query and your tool only accepts table and key parameters, it overestimated capability.

The mitigations are strict tool schemas, validation before execution, and explicit capability documentation. Tool schemas define exactly what tools exist and what parameters they accept. The agent's outputs are validated against schemas before execution. Any tool call that does not match a schema is rejected. Capability documentation means your system prompt clearly lists what tools are available and what each tool can and cannot do. This reduces hallucination by grounding the agent's planning in actual capabilities rather than general knowledge.

Some frameworks use tool call validation layers that intercept generated tool calls, check them against registered tools, and return errors to the agent if they do not match. The agent sees the error and can try again with valid tools. This is more forgiving than hard failures but requires the agent to handle tool errors gracefully.

## Negative Side Effects and Unintended Consequences

Negative side effects occur when the agent's actions achieve the intended goal but cause unintended harm. A workflow agent successfully completes a data migration but accidentally deletes backup copies in the process. A coding agent fixes a bug but introduces a performance regression. A research agent finds the requested information but violates rate limits on a third-party API and gets your account banned. The agent was not malicious and did not fail at its task, but the consequences were net negative.

The root cause is incomplete modeling of action effects. The agent reasons about primary effects, what the action is intended to do, but not secondary effects, what else might happen. This is partly a limitation of the agent's world model and partly a specification problem where the task description did not include constraints about what must be avoided.

The symptoms are successful task completion accompanied by user complaints, unexpected state changes in systems the agent touched, degraded performance or availability of services the agent used, and violations of policies or quotas. If the agent reports success but users are unhappy, negative side effects likely occurred. If monitoring shows anomalies correlated with agent execution, negative side effects likely occurred.

The mitigations are conservative action design, impact analysis, and guardrails. Conservative action design means actions have minimal scope and reversibility. Instead of deleting files, move them to trash. Instead of overwriting data, create new versions. Impact analysis means before executing actions with significant scope, the agent estimates what will be affected and checks against constraints. Guardrails are hard limits on what the agent can do, enforced by the infrastructure not the agent's judgment. The agent cannot delete more than N records, cannot call expensive APIs more than M times, cannot modify production databases.

Pre-flight checks help by simulating actions in safe environments before executing for real. A workflow agent can execute its plan against a test instance of systems to see what happens before touching production. A coding agent can run tests and performance benchmarks before committing changes. This catches many negative side effects before they impact production.

## Detecting and Diagnosing Failure Modes in Production

Recognizing failure modes requires comprehensive logging and monitoring. Every agent execution should log observations, reasoning, plans, actions, and reflection outputs. Tool calls should log parameters, results, and timing. The agent's context and token usage should be tracked. With this telemetry, you can diagnose failures post-hoc by analyzing patterns.

Infinite loops show up as high iteration counts and low action diversity. Tool cascades show up as bursts of tool calls to the same tool with varying parameters. Hallucinated arguments show up as tool failures with "not found" errors. Premature termination shows up as low iteration counts and short outputs. Goal drift shows up in logs where early and late actions are about different topics. Context overflow shows up when repetitive behavior starts after a certain iteration count. Cost explosion shows up in aggregated cost metrics that spike on certain executions. Stale state shows up as precondition failures or conflict errors.

Automated detection is possible for many failure modes. Set up alerts for executions that exceed iteration thresholds, cost thresholds, or time thresholds. Flag executions with high tool error rates or repeated identical tool calls. Compare execution metrics to historical baselines and alert on anomalies. Build dashboards that visualize agent behavior over time so patterns become visible.

User feedback is critical for detecting failures that do not show up in metrics. Premature termination and goal drift often produce outputs that look fine to automated checks but are wrong to humans who understand the intent. Negative side effects might not trigger any agent errors but cause user-facing problems. Systematic feedback collection and analysis catches these.

## Building Resilience Into Agent Architecture

The common thread across these failure modes is that they emerge from the interaction of autonomous decision-making with complex environments. You cannot eliminate them through better prompting or fine-tuning alone. You must build architectural mitigations that detect and correct failures as they happen.

Defense in depth is the principle. Hard limits prevent runaway resource consumption. Verification catches errors before they cascade. Schemas prevent hallucination. Checklists prevent premature termination. Re-grounding prevents goal drift. Summarization prevents context overflow. Budgets prevent cost explosion. Optimistic concurrency prevents stale state failures. Guardrails prevent negative side effects. No single mitigation is perfect, but layers of mitigations catch most failures.

Fail-safe defaults matter. When the agent is uncertain, what does it do? When it hits a limit, does it escalate or terminate? When verification fails, does it retry or abort? Design your defaults to minimize harm. Escalating to humans is safer than guessing. Aborting is safer than retrying indefinitely. Asking for clarification is safer than making assumptions.

The production agent systems that work reliably in 2026 are not the ones with the most sophisticated reasoning or the most powerful models. They are the ones with the most robust architectures, the ones that assume failure and build recovery into every phase of execution. Understanding the failure mode taxonomy is step one. Designing your agent to detect and mitigate each mode is step two. Testing that your mitigations actually work under production conditions is step three. The teams who do all three ship agents that run in production for months without disasters. The teams who skip these steps are still debugging their first production incident.
