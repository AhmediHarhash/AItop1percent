# 1.12 â€” The Agent Team: Who Builds, Who Evaluates, Who Operates

In August 2025, a healthcare startup shipped an agent that helped clinicians review patient records and suggest diagnostic pathways. The engineering team was exceptional: three senior software engineers with ML backgrounds, two prompt engineers who had worked at major AI labs, and a tech lead who had shipped production LLM features at scale. They built a technically sophisticated system with elegant prompt chains, robust error handling, and impressive performance on their evaluation suite. Four weeks after launch, the agent was making dangerous mistakes that the evaluation suite never caught.

The problem was not technical capability. It was team composition. The builders knew how to build agents. They did not know how to evaluate medical reasoning, what edge cases mattered in clinical settings, or how to operate a system where failures could harm patients. They built what they thought would work. They tested what they knew how to test. They missed what they did not know to look for. The missing capabilities were not engineering skills. They were domain expertise, evaluation mindset, and operational experience. The team structure assumed building was sufficient. It was not.

Agents are not just software. They are reasoning systems that make decisions in complex domains with real consequences. Building them requires software engineering and ML expertise. Evaluating them requires domain knowledge and adversarial thinking. Operating them requires reliability engineering and cost management. These are different skill sets that rarely exist in the same person. Successful agent development requires cross-functional teams where each role contributes distinct perspectives and capabilities.

## The Builder Role

Builders translate requirements into working agent systems. The role includes prompt engineers who design the agent's instructions and reasoning patterns, software engineers who build the tools and infrastructure the agent uses, and ML engineers who handle model selection, fine-tuning when needed, and performance optimization. Each sub-role requires different expertise, and the balance depends on system complexity.

Prompt engineers are the agent architects. They design how the agent thinks about problems, what reasoning steps it follows, how it uses tools, and how it handles edge cases. Good prompt engineering is part psychology, part linguistics, part software design. You must understand how models interpret instructions, how to structure information for maximum clarity, and how to guide reasoning without over-constraining it. The prompt engineer turns vague requirements like "help users analyze contracts" into specific instructions that reliably produce useful behavior.

The challenge in prompt engineering is that small changes can have large effects, and those effects are often non-obvious until you test extensively. A prompt that works perfectly in development might fail in production because real inputs have different characteristics than test inputs. A prompt optimized for one model might work poorly with a different model. Prompt engineers need both creativity to design effective approaches and discipline to validate that their designs work across the full range of expected inputs.

Software engineers build the infrastructure that agents run on. They design tool interfaces that agents call to take actions or retrieve information. They implement safety guardrails that prevent agents from taking dangerous actions. They build logging and monitoring systems that capture agent behavior. They create evaluation frameworks that measure agent performance. They handle integration with existing systems and services. The software engineering work for agents overlaps with traditional backend development but has unique challenges.

Agents make unpredictable sequences of tool calls, which means infrastructure must handle arbitrary call patterns gracefully. Agents might retry operations that should not be retried or fail to retry operations that should be retried, which means tool implementations need careful idempotency and error handling design. Agents might attempt to call tools in ways you did not anticipate, which means tool interfaces need robust validation. The software engineering mindset must shift from "build APIs for human developers who will read docs and use them correctly" to "build APIs for non-deterministic reasoning systems that will use them in creative and sometimes baffling ways."

ML engineers handle model-level concerns. They select which models to use for different tasks, considering capability, cost, latency, and reliability trade-offs. They fine-tune models when pre-trained models are insufficient, though fine-tuning is less common for agents than for narrow classifiers. They optimize inference performance, perhaps using quantization, caching, or batching strategies. They monitor model behavior for issues like distribution shift or quality degradation.

The ML engineering work for agents is lighter than for traditional ML products because you rely primarily on pre-trained foundation models rather than training custom models. The focus shifts from model training to model integration and optimization. You need to understand model capabilities and limitations well enough to architect systems that play to strengths and avoid weaknesses. You need to recognize when a model is not the right tool and when engineering solutions around model limitations are preferable to trying to fix the model.

The builder team must work closely together because the boundaries between roles are porous. Prompt design affects tool design affects model selection. A capability gap might be addressed through better prompts, better tools, or a stronger model. The decision depends on understanding trade-offs across all three areas. Teams where prompt engineers, software engineers, and ML engineers work in silos produce agents that are suboptimal and fragile.

## The Evaluator Role

Evaluators determine whether agents work correctly and safely. The role includes quality engineers who design test suites and run systematic evaluations, domain experts who assess whether agent outputs are correct and appropriate in specific contexts, and red teamers who try to make agents fail or misbehave. These roles represent fundamentally different mindsets from building.

Quality engineers bring systematic testing discipline. They design comprehensive test suites that cover happy paths, edge cases, error conditions, and adversarial inputs. They implement automated evaluation pipelines that run tests continuously as the agent evolves. They track quality metrics over time to detect regressions. They design experiments to compare different agent versions or configurations. The quality engineering mindset is skeptical and thorough. Builders ask "does this work." Quality engineers ask "what are all the ways this could fail, and have we verified it handles each one correctly."

The challenge in agent quality engineering is that traditional software testing techniques are necessary but not sufficient. You can test that tools work correctly and that safety guardrails activate when they should. You cannot exhaustively test that the agent's reasoning is correct because the space of possible inputs and reasoning paths is too large. Quality engineers must develop intuition for which test cases are most revealing, which failure modes are most important to catch, and how to design tests that stress-test reasoning capabilities.

Domain experts assess whether agent outputs are actually correct and useful in real-world contexts. For the healthcare startup, domain experts would be clinicians who understand medical reasoning, diagnostic processes, and clinical workflows. They review agent outputs not just for technical correctness but for appropriateness, completeness, and safety in clinical settings. They identify edge cases that matter in practice but would not occur to engineers. They provide ground truth for evaluation datasets.

The domain expert role is critical and frequently missing. Teams assume that if the agent produces outputs that look plausible to engineers, the outputs are correct. This assumption fails constantly. Legal documents that look fine to engineers might have critical flaws that lawyers immediately recognize. Financial analyses that seem reasonable might violate accounting principles. Customer service responses that appear helpful might use language that damages brand trust. Domain experts catch these failures.

The challenge is that domain experts are expensive and their time is limited. You cannot have a doctor review every output from a medical agent. The practical approach is to use domain experts strategically: to design evaluation datasets, to review samples of production outputs, to investigate failures, and to validate that automated evaluation metrics actually correlate with real quality. The goal is to compress domain expertise into artifacts that the broader team can use: evaluation datasets, rubrics for assessing quality, and documented patterns of what good and bad outputs look like.

Red teamers adopt an adversarial mindset. Their goal is to make the agent fail, to bypass safety measures, to find inputs that cause harmful outputs. They try prompt injection attacks where malicious instructions are hidden in user inputs. They try to extract training data or proprietary information. They test whether the agent can be manipulated into taking actions it should refuse. They explore the boundaries of the agent's capabilities looking for dangerous edge cases.

Red teaming is most critical for agents with safety implications, but every agent benefits from adversarial testing. Even low-risk agents can be abused in ways that create cost, reputational damage, or security vulnerabilities. Red teamers find these issues before adversaries do. The red team mindset is creative and slightly paranoid. They assume the agent has vulnerabilities and their job is to discover them.

The evaluator team must work independently from the builder team to maintain objectivity. If the people who built the agent are also the only people evaluating it, they bring unconscious biases about how it should work and what failures matter. Independent evaluation catches issues that builders miss because evaluators approach the system with fresh perspectives and different priorities.

## The Operator Role

Operators keep agents running reliably in production. The role includes site reliability engineers who monitor system health and respond to incidents, on-call engineers who debug failures and make emergency fixes, and cost managers who track spending and optimize resource usage. Operating agents is different from operating traditional software because agents are non-deterministic, they consume expensive API resources, and their behavior can degrade in subtle ways.

Site reliability engineers apply SRE practices to agent systems. They define service level objectives for availability, latency, and correctness. They build monitoring dashboards that track key metrics. They set up alerting for anomalies. They conduct incident response when things break. They run post-mortems to understand failures and prevent recurrence. They implement gradual rollouts and automated rollbacks to reduce deployment risk.

The SRE mindset for agents must account for non-determinism. Traditional software is deterministic: given the same inputs, it produces the same outputs. Agents are stochastic: the same input might produce different outputs. This makes debugging harder because you cannot always reproduce failures. It makes monitoring harder because you must distinguish between normal variation and actual problems. SREs need statistical intuition to set alert thresholds that catch real issues without triggering on noise.

On-call engineers handle production incidents. When an agent starts behaving incorrectly, on-call engineers investigate. They review logs to understand what the agent was attempting to do. They check whether recent changes correlated with the issue. They determine whether the problem is prompt-related, tool-related, model-related, or infrastructure-related. They implement fixes or mitigations. For critical issues, they might disable the agent temporarily while investigating.

The on-call experience for agents is often frustrating because root causes are unclear. An agent that was working fine might start failing because input distribution shifted, because a model update changed behavior, because an external API changed its response format, or because rare edge cases became more common. The debugging process requires hypothesis generation and testing rather than deterministic root cause analysis. On-call engineers need tolerance for ambiguity and systematic debugging skills.

Cost managers track agent spending and optimize resource usage. Agent costs include model API calls, tool executions that might hit external services, storage for logs and memory, and infrastructure for orchestration. These costs can be substantial, especially for agents that make long chains of reasoning or that operate at high volume. Cost managers establish budgets, monitor spending, identify cost drivers, and implement optimizations.

The cost management challenge is balancing spending with value. An agent that costs $10,000 per month but generates $100,000 in value is fine. An agent that costs $100 per month but generates no value is waste. Cost managers need visibility into both spending and value to make good decisions. They work with product teams to understand value and with engineering teams to understand cost drivers and optimization opportunities.

## How These Roles Interact

The three role groups interact in defined patterns. Builders create new agent versions. Evaluators assess whether those versions meet quality and safety requirements. Operators deploy approved versions and provide feedback on production behavior. The feedback loops in both directions. Evaluators identify failure modes that guide builders' improvements. Operators identify production issues that guide evaluators' test design and builders' reliability work.

The interaction pattern varies by development phase. During initial development, builders dominate. They are rapidly iterating on prompts, tools, and architecture. Evaluators are designing test suites but may not be running comprehensive evaluations on every iteration. Operators are not yet involved because there is nothing in production. The risk is that builders move too fast for evaluators to keep up, and quality issues accumulate.

During pre-launch validation, evaluators dominate. Builders have a candidate version they believe is ready. Evaluators run comprehensive tests, domain experts review outputs, red teamers probe for vulnerabilities. The evaluator feedback drives builder iterations. This phase can be frustrating for builders who want to ship, but it is essential. The issues found in pre-launch validation are much cheaper to fix than the same issues found in production.

During production operation, operators dominate. They monitor agent behavior, respond to incidents, and optimize performance. Builders and evaluators remain involved but in supporting roles. Builders make improvements based on production feedback. Evaluators validate that improvements do not introduce regressions. The risk is that operational concerns like availability and cost overshadow quality concerns. An agent that is highly available but frequently wrong is not actually succeeding.

The interaction requires clear communication channels and shared artifacts. Everyone needs access to logs, metrics, and evaluation results. Everyone needs visibility into incident reports and post-mortems. Everyone needs to understand current priorities and trade-offs. The communication overhead is substantial, which is why small teams struggle to cover all roles adequately.

## Common Organizational Mistakes

The most common mistake is builder-only teams. A group of talented engineers builds an agent, evaluates it themselves using test cases they wrote, and deploys it without dedicated evaluation or operations capability. This works for small experiments. It fails at scale. Builders are optimistic about their work. They test what they thought about during development. They miss what they did not think about. They lack domain expertise to recognize subtle errors. They lack adversarial mindset to probe for vulnerabilities.

The healthcare startup made this mistake. Their engineering team was excellent, but they needed clinicians on the evaluation team and healthcare operations experts on the operations team. Without those perspectives, they shipped an agent that worked technically but failed clinically. The fix required hiring domain experts and restructuring the team to give evaluation and operations distinct identities.

The second common mistake is no dedicated evaluation. Evaluation happens ad hoc when builders test their changes or when operators notice production issues. There is no systematic evaluation process, no comprehensive test suite, no domain expert review. This creates a false sense of quality. The agent passes all the tests you ran, but you did not run the tests that mattered. Dedicated evaluation requires dedicated people whose primary responsibility is determining whether the agent works correctly.

The third common mistake is no operations planning. Teams build agents assuming they will operate like traditional software. They discover too late that agent operations require specialized skills and infrastructure. They have inadequate monitoring, no runbooks for common failure modes, no cost controls, and no clear ownership of production reliability. When incidents happen, response is chaotic because nobody prepared for operations.

The fourth common mistake is role confusion. The same person is builder, evaluator, and operator. They make changes, test their own changes, and deploy their changes. This creates conflicts of interest and blind spots. The person who built a feature is not the best person to test it adversarially or to wake up at 3 AM when it breaks in production. Role separation creates checks and balances that improve quality and reliability.

## The Minimum Viable Agent Team

The minimum viable team depends on agent complexity and risk. For a low-risk experimental agent, a single person might cover all roles adequately. For a production agent with meaningful business impact, you need at least three people: a builder who develops the agent, an evaluator who tests it, and an operator who runs it. The separation does not need to be full-time. The same three people might work on multiple agents. The separation needs to be conceptual. When evaluating, you are not in builder mindset. When operating, you are not in evaluator mindset.

For medium-risk agents, you need a team of five to eight: multiple builders with different specialties, dedicated quality engineering, domain expert access for evaluation, and operations capability with on-call rotation. The team can share some roles. A software engineer might contribute to both building and operating. The critical roles like domain expert evaluation cannot be skipped.

For high-risk agents, you need a larger team with dedicated specialists: separate prompt engineering, software engineering, and ML engineering on the builder side; separate quality engineering, domain expert review, and red teaming on the evaluator side; separate SRE, on-call, and cost management on the operator side. You also need program management to coordinate across roles and executive oversight to ensure risk is being managed appropriately.

The team scaling is not linear with agent complexity. A moderately complex agent needs disproportionately more evaluation and operations capability than a simple agent because the failure modes multiply and the operational challenges intensify. The team composition must match the actual risk and complexity, not the desired team size or available headcount.

## Evolving Team Structure

Team needs evolve as agents mature. Early stage agents need heavy builder investment to achieve basic functionality. Mid-stage agents need heavy evaluator investment to achieve production quality. Late-stage agents need heavy operator investment to maintain reliability at scale. The team composition should shift to match these phases.

Early stage is builder-dominated. You might have four builders and one evaluator. The evaluator's main job is preventing builders from shipping obvious mistakes, not running comprehensive validation. The focus is on rapid iteration toward something that works at all. The risk is spending too long in this phase, where quality discipline is loose.

Mid-stage is evaluator-dominated. You might have three builders, three evaluators, and one operator. The builders are refining the system based on evaluation feedback. The evaluators are running comprehensive tests and finding failure modes. The operator is preparing infrastructure and runbooks. The focus is on achieving the quality bar for production. The risk is evaluators finding endless issues and the agent never launching.

Late-stage is operator-dominated. You might have one builder, one evaluator, and three operators. The agent is stable and feature-complete. Most work is operational: monitoring, incident response, cost optimization, and incremental improvements based on production data. The builder handles occasional enhancements. The evaluator validates changes. The risk is neglecting quality as operational concerns consume attention.

The team transition requires active management. You cannot just assume that people will naturally shift focus as the phase changes. You need to explicitly reallocate effort, change meeting structures, and adjust incentives. A team optimized for building will resist shifting to operations. The transition is when many projects stall.

## Cross-Functional Collaboration Patterns

Effective agent teams develop collaboration rituals that bring roles together. Weekly demos where builders show new capabilities and evaluators ask hard questions about failure modes. Bi-weekly evaluation reviews where the full team examines test results and discusses implications. Monthly production reviews where operators present metrics, incidents, and cost data. Quarterly roadmap planning where all roles contribute to priorities.

The collaboration rituals serve multiple purposes. They create shared understanding of system state. They surface issues before they become crises. They build mutual respect across roles. They force teams to confront trade-offs explicitly rather than letting different roles optimize locally. A builder who attends production reviews understands operator constraints. An operator who attends evaluation reviews understands quality requirements.

The collaboration requires cultural norms that value all roles equally. In many engineering organizations, builders have higher status than evaluators or operators. This is toxic for agent development. If evaluators are seen as blockers who slow down shipping, they will not be empowered to enforce quality standards. If operators are seen as people who just keep things running, they will not be empowered to push back on unsustainable designs. The team culture must celebrate finding problems before shipping them as much as building new capabilities.

## Looking Forward

As agent capabilities grow and deployment scales, team structures will professionalize further. Current practice is ad hoc team formation where engineers take on multiple roles. Emerging practice is specialized roles with dedicated career paths. Prompt engineering, agent quality engineering, and agent operations are becoming recognized disciplines with specific skills and expertise.

The organizations that succeed with agents will be those that invest in cross-functional teams early, that create clear role boundaries while fostering collaboration, that scale team composition to match agent risk, and that avoid the trap of builder-only teams. The healthcare startup eventually restructured. They hired two clinicians to lead evaluation, a healthcare operations expert to lead production operations, and reorganized the engineering team to separate building from operating. The new team structure was slower to ship features. The features they shipped actually worked. The agent is now used across 47 hospitals. Team structure was not a nice-to-have. It was the difference between failure and success.
