# 9.7 â€” Agent Cost Attribution: Tracking Spend by Task, Agent, and User

You cannot optimize what you do not measure, and you cannot govern what you do not attribute. When your monthly API bill arrives at three times the projected budget and leadership asks which agents are expensive, which tasks drove costs, and which users need guardrails, you will discover whether you treated cost tracking as an operational capability or as an afterthought. Most teams choose the latter. They log every API call but store cost data in an aggregated billing dashboard that provides one number per month. When a single task consumes six days of compute processing 14,000 files instead of 200, no alert fires, no one notices, and the user has no idea they just spent $23,000 on a workflow they thought would cost $180. Cost attribution is not financial reporting. It is a core operational requirement that must be instrumented from the first agent you deploy.

The root cause was not the expensive task itself. Document review at scale is legitimate work. The root cause was the absence of cost attribution infrastructure. The team treated API costs as an abstract operational expense, like bandwidth or storage, rather than a precise metric tied to every task execution, every agent invocation, and every user action. Without attribution, you cannot budget, you cannot optimize, and you cannot govern. Cost attribution is not a financial reporting feature. It is a core operational capability that must be instrumented from the first agent you deploy.

## Cost Attribution as a First-Class Dimension

Cost attribution means decomposing total spend into attributable components: which task consumed how many tokens, which agent performed how many calls, which user triggered how many executions, which model was invoked at what price, and which time period saw what volume. You instrument this at the same layer where you instrument latency, error rates, and success metrics. Every span in your trace carries cost metadata. Every log event records token counts and model identifiers. Every agent execution emits a cost summary. You treat cost as a first-class dimension of observability, not a billing afterthought.

Most teams start by logging token counts. Prompt tokens, completion tokens, reasoning tokens if using models like o1 or o3. You log these at the LLM call level, then roll them up to the task level, the session level, the user level. You join token counts with model pricing: GPT-5 at $2.50 per million input tokens and $10.00 per million output tokens as of early 2026, Claude Opus 4.5 at $3.00 and $15.00, GPT-5.1 at higher rates. Pricing changes monthly. You do not hardcode prices in your instrumentation. You maintain a pricing table keyed by model name and date, updated whenever providers publish new rates. Your cost calculation queries this table at analysis time, not at logging time. This allows you to recompute historical costs when pricing changes or when you discover you were using the wrong rate.

You log model names exactly as returned by the API: gpt-4o-2024-11-20, claude-3-5-sonnet-20241022, o1-2024-12-17. You do not abbreviate or alias. Model names encode version, which encodes pricing tier and capability. You also log whether the call used caching, whether it triggered reasoning tokens, whether it invoked tools. Caching reduces input token costs by 50% to 90% depending on provider. Reasoning tokens cost more than output tokens. Tool calls add extra completion tokens. All of these factors change the per-call cost, and all must be captured in your logs.

You tag every LLM call with hierarchical identifiers: task ID, agent ID, user ID, session ID, workflow ID, tenant ID if you run multi-tenant infrastructure. These tags propagate through your tracing system. When you query for total cost by user, you sum all calls tagged with that user ID. When you query for cost by agent, you sum all calls tagged with that agent ID. When you query for cost by task type, you sum all calls tagged with that task identifier. The same event stream supports all three queries because the tags are orthogonal. You do not create separate cost logs for each dimension. You create one enriched event stream and query it flexibly.

## Per-Task Cost Tracking

Per-task cost tracking means knowing how much each distinct task execution cost, in dollars, attributed to the specific inputs and configuration of that execution. A document summarization task that processes a 50-page PDF costs more than one that processes a 2-page email. A code review task that analyzes 30 changed files costs more than one that reviews a single-file hotfix. You log input characteristics alongside cost data so you can model cost as a function of input size, complexity, and task parameters.

You emit a task cost summary at the end of every task execution. This summary includes total tokens consumed, total API calls made, total cost in dollars, breakdown by model if multiple models were used, breakdown by agent if multiple agents were invoked, and a timestamp. You also include task metadata: input token count before any LLM call, output token count after final response, number of tool calls, number of retries, number of validation failures that triggered rework. This metadata lets you analyze cost efficiency. If a task cost $2.40 but required six retries due to malformed outputs, that is a quality problem masquerading as a cost problem. Fixing output formatting might cut cost by 70% by eliminating retries.

You track cost per task type over time. If document summarization tasks averaged $0.80 in October 2025 but $1.30 in December 2025, you investigate. Did input documents get longer? Did you switch models? Did you add a validation layer that requires extra calls? Did prompt changes increase output verbosity? Cost drift per task type is a leading indicator of process changes, scope creep, or infrastructure issues. You set up alerts when per-task cost exceeds historical averages by more than 30%, triggering a review before the month closes.

You also compute cost percentiles. The median document review task might cost $0.12, but the 95th percentile might cost $4.50. That distribution tells you where risk lives. If one in twenty tasks costs 37 times the median, you need guardrails to catch those cases early. You might impose a budget cap per task: if projected cost exceeds $5.00 based on input size, require manager approval before execution. You might split large tasks into smaller chunks and process incrementally, giving users a cost estimate and a cancel option after the first chunk. You might route high-cost tasks to cheaper models or hybrid workflows where cheap models handle bulk work and expensive models handle edge cases.

## Per-Agent Cost Tracking

Per-agent cost tracking means attributing costs to each agent in your system, whether that agent is a single-purpose tool like a web search agent, a reasoning agent that orchestrates multi-step workflows, or a supervisor agent that delegates to specialists. Different agents have different cost profiles. A web search agent might make one LLM call per query at $0.02 per call. A code generation agent might make five calls per task at $1.20 per call. A research agent might make forty calls per task at $6.80 per call. You need to know which agents are expensive, which are cheap, and which are inefficient relative to the value they deliver.

You tag every LLM call with the agent ID that initiated it. If Agent A calls a tool that invokes Agent B, Agent B's costs are attributed to Agent B, not Agent A, but you also log the parent-child relationship so you can trace delegation costs. If a supervisor agent delegates 80% of its work to specialist agents, the supervisor's direct cost is low, but its total cost including delegated work is high. You track both. You track direct cost, the cost of LLM calls made by the agent itself, and total cost, the cost of all LLM calls made by the agent and any agents it invoked recursively.

You analyze cost per agent over time. If your web search agent's median cost was $0.02 per invocation in November 2025 but $0.09 in January 2026, you investigate. Did the search tool change its behavior and return longer snippets? Did you add a summarization step? Did you switch from GPT-5-mini to GPT-5? Small changes compound when an agent runs thousands of times per day. A $0.07 increase per call times 5,000 calls per day is $350 per day, $10,500 per month. Cost per agent is a proxy for agent efficiency, and efficiency directly impacts your ability to scale.

You also compare cost per agent to success rate per agent. If Agent X costs $2.40 per task and succeeds 94% of the time, and Agent Y costs $0.60 per task but succeeds only 72% of the time, Agent Y is not cheaper. Failed tasks require retries, manual fallback, or user escalation, all of which have hidden costs. You compute cost per successful task, not just cost per task. If Agent Y's cost per successful task after accounting for retries is $0.83, it is still cheaper than Agent X. But if retries push it to $2.10, you switch to Agent X or invest in improving Agent Y's reliability.

You track agent cost by model. If an agent uses GPT-5 for 80% of calls and Claude Opus 4.5 for 20%, you log that split. If Claude calls are failing more often, you might route all traffic to GPT-5 and accept the cost increase. If GPT-5 calls are more expensive but no more accurate, you might route all traffic to Claude. Cost attribution by model within each agent reveals optimization opportunities that aggregate metrics hide.

## Per-User Cost Tracking

Per-user cost tracking means knowing how much each user or team has spent over a given period, which tasks they ran, which agents they used, and whether their usage patterns are normal or anomalous. This is not about punishing high-cost users. This is about understanding usage, setting budgets, providing feedback, and catching runaway processes before they consume five-figure budgets.

You tag every task execution with a user ID or team ID. When a user submits a task, you log their identifier, the task type, the timestamp, and the projected cost if you can estimate it from input size and historical data. When the task completes, you log actual cost. You aggregate these logs into per-user cost summaries: total spend this month, total tasks run, cost per task, breakdown by task type, breakdown by agent. You expose this data in a dashboard that users can view. Transparency drives accountability. If a user sees they have spent $340 this month and their peer has spent $80, they ask why. Maybe they are processing larger datasets. Maybe they are running redundant tasks. Maybe they are using an expensive agent when a cheaper one would suffice.

You set budget caps per user or per team. A team with a $500 monthly budget gets alerts at $400, $450, and $490. At $500, you either block further tasks or require manager approval. This prevents the scenario where one user accidentally runs a $47,000 job. Budget caps are not arbitrary restrictions. They are guardrails that force users to review high-cost workflows and decide whether the value justifies the expense. If a user needs to exceed their budget for legitimate reasons, they request an increase, and the request is logged and audited. This creates a decision trail that protects both the user and the organization.

You also track cost per user over time. If a user's monthly spend was $60 in October, $75 in November, and $220 in December, you investigate. Did their workload increase? Did they start using a more expensive agent? Did they change their workflow in a way that triggers more LLM calls? Cost creep is often invisible to users because they do not see the per-call cost, only the task outcome. Showing them a cost trend over time makes the pattern visible and actionable.

You segment users by role and usage pattern. Power users who run 200 tasks per month are different from occasional users who run five. Junior analysts who summarize documents are different from senior researchers who run complex multi-agent workflows. You compute cost per user per role and flag outliers. If junior analysts average $40 per month but one spends $190, you review their tasks. Maybe they are using the wrong agent. Maybe they are processing malformed inputs that trigger excessive retries. Maybe they need training on how to scope tasks to avoid unnecessary work.

## Cost Modeling and Forecasting

Cost modeling means building a predictive model that estimates task cost based on input characteristics, task type, agent selection, and historical data. You use this model to show users an estimated cost before they run a task, to set budget caps, and to forecast monthly spend based on projected task volume.

You start with historical data. For each completed task, you have input size, task type, agent used, model used, total tokens, total cost. You fit a regression model or a simple heuristic model that predicts cost from input size and task type. For document summarization, cost might be 0.0012 times input token count plus a fixed overhead of $0.05. For code review, cost might be 0.0018 times changed lines plus 0.0024 times context lines plus $0.08. These models do not need to be precise. They need to be directionally correct and fast to compute.

You expose cost estimates in the UI. When a user uploads a 40-page document for summarization, you estimate input tokens, query your cost model, and display an estimate: approximately $1.20. The user can proceed or cancel. If the estimate exceeds their budget or their expectations, they reconsider whether the task is necessary. This shifts cost awareness from retrospective analysis to prospective decision-making.

You use cost models to forecast monthly spend. If your team runs an average of 1,200 tasks per month, and your cost model predicts an average of $0.85 per task, you forecast $1,020 per month. If you are tracking $780 after three weeks, you are on pace for $1,040, within forecast. If you are tracking $1,350, you are on pace for $1,800, and you investigate immediately. Forecasting turns cost tracking from a lagging indicator into a leading indicator.

You update cost models monthly. As task patterns change, as input sizes grow, as you add new agents or switch models, cost dynamics shift. A model trained on October 2025 data may underpredict costs in January 2026 if you switched from GPT-5 to GPT-5.1 or if users started submitting longer inputs. You retrain on the most recent 90 days of data and compare predicted costs to actual costs. If prediction error exceeds 20%, you investigate model drift or feature drift.

## Cost Optimization Levers

Cost attribution is useful only if it drives action. Once you know which tasks, agents, and users are expensive, you optimize. You have several levers: model selection, prompt optimization, caching, batching, output length control, retry logic, and task scoping.

Model selection is the highest-leverage lever. If a task works equally well on GPT-5-mini at $0.15 per million input tokens and GPT-5 at $2.50 per million input tokens, you route to mini and cut cost by 94%. You do not guess which tasks work on cheaper models. You run evals comparing task success rates across models and measure cost per successful task. If mini succeeds 91% of the time and GPT-5 succeeds 96%, you compute the cost per successful outcome after retries. Often, the cheaper model wins because the small accuracy gap does not justify the 16x price premium.

Prompt optimization reduces token counts without sacrificing quality. A verbose prompt with 1,200 tokens can often be rewritten to 400 tokens with no loss in output quality. Cutting prompt tokens by 66% cuts input cost by 66%, and if you cache the prompt, you cut cost by even more because cache hits are nearly free. You version prompts and track cost per version. If prompt v3 costs 30% less than prompt v2 with the same success rate, you deploy v3 and monitor for regressions.

Caching is critical for agents that reuse context across calls. If an agent sends the same 8,000-token system prompt on every call, caching reduces input cost by 90% after the first call. Both OpenAI and Anthropic support prompt caching as of early 2026. You enable caching for all static context: system prompts, schemas, examples, knowledge base excerpts. You log cache hit rates and measure cost savings. If caching saves $3,200 per month, you factor that into your model selection and budgeting.

Batching reduces overhead. If you have 50 independent classification tasks, you batch them into a single LLM call instead of 50 separate calls. The per-call overhead, both in latency and sometimes in token padding, is eliminated. Batching works for tasks that share context and do not require sequential reasoning. You do not batch tasks that depend on prior outputs.

Output length control reduces completion token costs. If your summarization prompt says "provide a summary" and the model returns 800 tokens when 200 would suffice, you revise the prompt to "provide a summary in under 200 tokens" or you set a max tokens parameter. Cutting unnecessary verbosity cuts cost. You measure output token distributions per task type and set length targets based on what users actually read and use. If users only read the first 150 tokens of a 600-token summary, you cap output at 200 tokens and save 66% on completion costs.

Retry logic should be cost-aware. If a task fails due to a malformed output, you retry. But if you retry blindly ten times, you spend 10x the cost for a task that may never succeed. You limit retries to three, and you use cheaper models for retries if appropriate. You also log retry costs separately and alert when retry costs exceed 15% of total costs, because that signals a quality problem upstream.

Task scoping is the most impactful lever. If users submit 14,000-file repositories when they meant to submit 200-file folders, you add guardrails. You show input size estimates, you require confirmation for tasks above a size threshold, you split large tasks into smaller ones and process incrementally. You educate users on how to scope inputs to avoid unnecessary work.

## Cost Governance and Accountability

Cost governance means creating policies, approvals, and guardrails that ensure spend aligns with value and budget. You do not govern by restricting access. You govern by making costs visible, setting expectations, and requiring decisions when costs exceed norms.

You publish cost benchmarks: median cost per task type, 95th percentile cost, cost per user, cost per team. You share these benchmarks with users and managers. Teams that exceed benchmarks are not penalized. They are asked to explain their usage and decide whether it is justified. Often, high costs reflect high-value work. Sometimes, they reflect inefficiency or misunderstanding. The review conversation surfaces the difference.

You require manager approval for high-cost tasks. If a task is projected to cost more than $10, the system sends an approval request to the user's manager with context: what task, what inputs, what estimated cost, what user. The manager approves or denies. This is not bureaucracy. This is a forcing function that makes cost-value tradeoffs explicit. If the task is worth $10, approval is instant. If the task is exploratory or low-priority, the user reconsiders.

You review cost trends in monthly retrospectives. You show total spend, breakdown by task type, breakdown by agent, breakdown by team, month-over-month change, and variance from forecast. You identify the top ten most expensive tasks and the top ten most expensive users. You discuss whether the costs were expected and whether the value justified the spend. You identify optimization opportunities and assign owners. This monthly rhythm keeps cost top of mind and prevents runaway spend from accumulating silently.

You integrate cost data into your eval pipeline. When you compare two agent architectures, you compare not just accuracy and latency but also cost per successful task. Architecture A might have 2% higher accuracy but 3x higher cost. Whether that tradeoff is acceptable depends on the task value. For high-stakes legal or medical tasks, higher accuracy may justify higher cost. For internal tooling or low-risk workflows, lower cost may win. Cost-aware evals make these tradeoffs explicit.

## Instrumentation and Tooling

You instrument cost attribution at the same layer where you instrument latency and errors. Every LLM call logs model, prompt tokens, completion tokens, reasoning tokens if applicable, cache hit status, and timestamp. You compute cost at query time by joining logs with a pricing table. You do not compute cost at write time because pricing changes and you need to recompute historical costs when rates update.

You use your existing observability stack. If you trace with OpenTelemetry, you add cost attributes to spans: span.setAttribute for model, input tokens, output tokens. If you log with structured JSON, you add cost fields to every log event. You do not build a separate cost tracking system. You extend your existing instrumentation to include cost as a dimension.

You expose cost data in dashboards. You create views for cost per task type, cost per agent, cost per user, cost over time, cost by model. You create alerts for anomalies: cost spike above 2x daily average, per-user spend above budget cap, per-task cost above 95th percentile. You send alerts to Slack or email with context and a link to logs.

You provide cost APIs that users and tools can query. A CLI tool might query total cost for a user over the past 30 days. A budget enforcement service might query current month spend and block tasks if the cap is reached. A forecasting tool might query historical spend and project next month's budget. Cost data is not locked in dashboards. It is queryable, exportable, and integrable.

You version your cost attribution schema. As you add new fields, new agents, or new models, your schema evolves. You use backward-compatible changes so historical data remains queryable. You document schema changes in a changelog so downstream consumers know when new fields are available.

Cost attribution is not optional. It is infrastructure you build before agents reach production, and it is telemetry you maintain with the same rigor you apply to uptime and latency. Without it, you cannot budget, you cannot optimize, and you cannot govern. With it, you turn cost from an abstract liability into a precise, attributable, and actionable dimension of agent operations. The next step is detecting when agent behavior changes over time, even when costs remain stable, which we address in drift detection.
