# 10.12 — Model Tiering for Agents: Routing Steps to Cheaper Models

A legal technology company launched a contract review agent in June 2025 using Claude Opus 4.5 for all agent steps. The agent worked exceptionally well, identifying non-standard clauses, flagging regulatory risks, and generating redline suggestions that matched senior associate quality. The product team celebrated the launch. The finance team did not. Each contract review cost between $8 and $14 in API fees, far exceeding the $3 target that made the product economically viable at their planned pricing. The engineering team analyzed token consumption and discovered that 65% of agent steps were routine tasks: extracting contract metadata, splitting documents into sections, reformatting dates, checking whether clauses were present or absent. These steps required no advanced reasoning. They were being routed to the most expensive model in the stack because the agent used a single-model architecture. The team estimated that routing low-complexity steps to GPT-5-mini and mid-complexity steps to Claude Opus 4.5 while reserving Opus 4.5 for high-complexity analysis would reduce per-review costs to $2.80 without measurable quality loss. But re-architecting the agent to support model tiering would take six weeks, and the business could not wait. They suspended new customer onboarding and operated at a loss for two months while engineering rebuilt the system. The root cause was not model selection or poor prompting. It was the lack of **model tiering**—the architectural pattern of routing agent steps to different models based on task complexity, optimizing the cost-quality trade-off at the step level rather than defaulting to a single high-cost model for everything.

You cannot run cost-effective production agents using frontier models for every step. The marginal cost difference between running a tool parameter extraction step on GPT-5-mini versus Claude Opus 4.5 is 40 times. Multiply that by a hundred steps per session and a thousand sessions per day, and you are burning tens of thousands of dollars on tasks that could be handled by a model costing one-fortieth the price. Model tiering is not about cutting corners or degrading quality. It is about matching the model capability to the task requirement. A frontier model for frontier reasoning, a mid-tier model for moderate complexity, and a lightweight model for structured extraction and routing. This subchapter covers how to classify agent steps by complexity, define tiering rules, implement dynamic routing, and validate that tiering preserves quality while reducing costs.

## Classifying Agent Steps by Reasoning Complexity

The first step in model tiering is understanding which agent steps require advanced reasoning and which do not. You cannot tier models effectively if you treat all steps as equally complex. You decompose your agent workflow into discrete steps and assess the cognitive load of each one. This is not a subjective exercise. You use specific criteria to classify complexity and map those classifications to model tiers.

Low-complexity steps are deterministic or near-deterministic tasks that involve pattern matching, extraction, formatting, or simple classification. Examples include parsing structured data from a document, extracting entities like dates and names, checking whether a clause exists in a contract, splitting text into sections based on headings, reformatting output into a specific schema, and answering yes-or-no questions based on explicit text. These steps require high accuracy but minimal reasoning. They do not benefit from frontier model capabilities. A model like GPT-5-mini, Claude Opus 4.5 Haiku, or Gemini 2.0 Flash handles these tasks with 98%-plus accuracy at a fraction of the cost. You route low-complexity steps to the cheapest models in your stack.

Mid-complexity steps require moderate reasoning: comparing multiple pieces of information, generating summaries, identifying implications, making judgment calls based on guidelines, or applying domain knowledge to ambiguous situations. Examples include summarizing a document section, comparing two contract clauses to identify differences, determining whether a customer inquiry matches a known support category, generating a list of action items from meeting notes, or drafting a preliminary response that requires tone and context awareness. These steps benefit from stronger reasoning but do not require frontier capabilities. Models like GPT-5, Claude Opus 4.5, or Gemini 2.0 Pro handle mid-complexity tasks well. You route these steps to mid-tier models that balance cost and capability.

High-complexity steps require advanced reasoning, synthesis across multiple contexts, handling of edge cases, creative generation, or domain expertise. Examples include analyzing a contract for subtle regulatory risks, generating a legal argument that requires multi-step reasoning, debugging code by understanding implicit dependencies, writing a comprehensive strategic analysis that synthesizes data from six sources, or making high-stakes decisions that require balancing competing priorities. These steps justify frontier model costs because cheaper models produce measurably worse results. You route high-complexity steps to models like Claude Opus 4.5, GPT-5.1, or the best available reasoning model in your stack.

You document the complexity classification for every step in your agent workflow. For the contract review agent, you create a step inventory: extract metadata (low), split into sections (low), identify clause types (low), check for required clauses (low), summarize each section (mid), compare clauses to standard templates (mid), flag regulatory risks (high), generate redline suggestions (high), synthesize final report (mid). Out of nine steps, three are low-complexity, four are mid-complexity, and two are high-complexity. If you route the six low- and mid-complexity steps to cheaper models, you reduce the per-session cost by 60% without touching the quality of the high-complexity reasoning that defines the product's value.

You validate your complexity classifications with evaluation data. You run a sample of tasks through both the frontier model and the proposed cheaper model and compare results. If the cheaper model achieves 95%-plus agreement with the frontier model on a low-complexity step, your classification is correct. If agreement drops below 90%, the step is more complex than you thought, and you either reclassify it or improve the prompt for the cheaper model. You do not guess at complexity—you measure it.

## Defining Tiering Rules and Model Selection Policies

Once you have classified steps by complexity, you define explicit tiering rules that map complexity levels to specific models. These rules are not suggestions—they are policy enforced by your agent framework. Every step is routed according to the policy, and deviations are logged and reviewed. The policy balances cost, latency, and quality based on your system's priorities.

A basic tiering policy has three tiers. Tier 1 uses the cheapest models: GPT-5-mini, Claude Opus 4.5 Haiku, Gemini 2.0 Flash. These models cost between $0.10 and $0.40 per million input tokens as of January 2026. You route all low-complexity steps to Tier 1 models. Tier 2 uses mid-range models: GPT-5, Claude Opus 4.5, Gemini 2.0 Pro. These models cost between $2.50 and $7.50 per million input tokens. You route mid-complexity steps to Tier 2 models. Tier 3 uses frontier models: Claude Opus 4.5, GPT-5.1, and future best-in-class reasoning models. These models cost between $15 and $75 per million input tokens. You route high-complexity steps to Tier 3 models. The cost ratio between Tier 1 and Tier 3 is often 50 to 1 or more, which is why tiering has such dramatic cost impact.

You also define fallback rules. If a Tier 1 model fails to complete a step—times out, returns an error, or produces output that fails validation—you automatically retry with a Tier 2 model. If the Tier 2 model fails, you escalate to Tier 3. This ensures robustness while still optimizing for cost. You track fallback frequency to identify steps that are misclassified. If a supposedly low-complexity step requires Tier 2 fallback 30% of the time, it is not actually low-complexity, and you reclassify it.

Your policy includes latency considerations. Some Tier 1 models have higher throughput and lower latency than Tier 3 models because they are smaller and faster to execute. If a step is latency-sensitive and low-complexity, you prioritize Tier 1 models even if a Tier 2 model might marginally improve quality. If a step is not latency-sensitive, you optimize purely for cost and quality. A batch summarization job that processes overnight can tolerate higher latency, so you route all steps to the cheapest viable model. A real-time customer support agent needs sub-second responses, so you choose the fastest model in each tier.

You also implement override rules for specific contexts. If a user is on an enterprise plan with contracted SLAs, you route all their steps to Tier 2 or Tier 3 models regardless of complexity, because the business has committed to premium service quality. If a task is flagged as high-stakes—like a contract review for a million-dollar deal—you route all steps to Tier 3 models to minimize risk. If a user is on a free trial, you route all steps to Tier 1 models to control costs. These overrides are configured in your system, not hard-coded per agent, so you can adjust policies without redeploying code.

You version your tiering policies and track changes over time. When a new model is released, you evaluate it against your existing tiers and update the policy if it offers better cost-quality trade-offs. When model pricing changes, you re-run cost projections and adjust tier assignments if necessary. Model tiering is not static. It evolves as the model landscape evolves, and your infrastructure must support policy updates without requiring re-architecture.

## Implementing Dynamic Routing in Agent Frameworks

Model tiering requires dynamic routing infrastructure. Your agent framework must classify each step at runtime, select the appropriate model based on tiering policy, invoke the model, and handle the response. This is not a single conditional statement—it is a routing layer that integrates with your step execution engine, observability stack, and cost tracking system.

Your agent's step executor receives a step definition that includes a complexity hint: "extract_metadata, complexity: low" or "analyze_risk, complexity: high." The executor looks up the complexity level in the tiering policy and retrieves the assigned model. For low complexity, it gets GPT-5-mini. For high complexity, it gets Claude Opus 4.5. The executor then formats the step's prompt and input according to the selected model's API requirements—different models have different message formats, parameter names, and token limits—and invokes the API. The model returns a response, which the executor validates and passes to the next step.

You implement routing rules as configuration, not code. Your tiering policy lives in a configuration file or database, not embedded in Python or TypeScript. This allows you to update routing rules without redeploying your agent application. If you discover that a particular step performs better on a different model, you update the configuration, and the change takes effect immediately for new sessions. You version the configuration and track changes in your deployment log so you can correlate model changes with cost and quality metrics.

You also implement runtime complexity detection for steps where complexity varies based on input. A summarization step might be low-complexity if the input is 200 words but mid-complexity if the input is 5,000 words. Your routing logic checks the input length and adjusts the tier accordingly: "if input_length less than 500 words, use Tier 1; if input_length between 500 and 2,000 words, use Tier 2; if input_length greater than 2,000 words, use Tier 3." This is dynamic tiering based on runtime state, not static classification based on step type.

You track model usage per step type in your observability system. Every time a step executes, you log which model was used, how many tokens were consumed, the latency, and whether the step succeeded or required fallback. You aggregate this data daily and review it weekly. You identify steps where the tiering policy is suboptimal—steps that consistently require fallback, steps where a cheaper model would suffice, or steps where cost has crept up due to input size growth. You adjust the policy based on this data, continuously optimizing the cost-quality trade-off.

Your routing infrastructure also handles model availability and rate limits. If your preferred Tier 2 model is temporarily unavailable or rate-limited, your framework automatically falls back to a Tier 3 model to maintain service continuity. You do not fail the task because a specific model is unavailable. You complete the task using the next-best option and log the fallback so you can investigate the root cause. If fallbacks become frequent, you either add redundancy by supporting multiple models per tier or negotiate higher rate limits with your model providers.

You implement A/B testing for model changes. Before updating your tiering policy to route a step from GPT-5 to Claude Opus 4.5 Haiku, you run an A/B test where 10% of traffic uses the new model and 90% uses the old model. You compare quality metrics, cost, and latency between the two groups. If the new model performs equivalently at lower cost, you roll out the change to 100% of traffic. If quality degrades, you revert the change and investigate why. You do not make tiering changes based on intuition—you validate them with production data.

## Validating Quality Preservation Across Model Tiers

Model tiering only works if cheaper models actually preserve quality for the steps they handle. If routing a step from Claude Opus 4.5 to GPT-5-mini reduces cost by 95% but also reduces accuracy from 98% to 78%, you have not optimized—you have broken the product. Quality validation is mandatory before deploying tiered routing to production.

You start with offline evaluation. You collect a representative dataset of inputs for each step type—hundreds or thousands of examples covering typical cases, edge cases, and known failure modes. You run each example through both the current model and the proposed cheaper model, generating outputs from both. You compare the outputs using automated metrics where applicable: exact match for structured extraction, semantic similarity for summaries, classification agreement for categorization tasks. For steps where automated metrics are insufficient, you use human evaluation. Reviewers score outputs from both models on quality dimensions like accuracy, completeness, and appropriateness.

You set quality thresholds for each step type. For low-complexity extraction steps, you require 95%-plus agreement between the cheaper model and the current model. For mid-complexity steps, you require 90%-plus agreement. If the cheaper model meets the threshold, you approve it for tiering. If it falls short, you either improve the prompt, choose a different model in the same price range, or accept that this step requires a more expensive model. Not every step can be cost-optimized—some tasks genuinely require frontier capabilities.

You also validate quality in production using shadow mode. Before switching a step to a cheaper model, you run both models in parallel for a sample of production traffic. The cheaper model processes the step, but its output is not used—it is logged for comparison. The current model's output is used in the live workflow. You compare the logged outputs and measure agreement. If agreement is high over a week of production traffic, you promote the cheaper model to primary. If agreement is low or inconsistent, you investigate why production inputs differ from your evaluation dataset and either expand the evaluation set or adjust the tiering policy.

You implement quality monitoring post-deployment. After switching a step to a cheaper model, you track downstream metrics for degradation. If the step is part of a contract review agent, you track whether the rate of missed clauses or false positives increases. If the step is part of a support agent, you track whether resolution time or customer satisfaction scores change. If you detect degradation, you revert to the previous model and conduct a detailed analysis to understand what went wrong. You do not tolerate silent quality losses in the name of cost savings.

You also validate that model tiering does not introduce bias or fairness issues. If you route simple customer inquiries to a cheap model and complex inquiries to an expensive model, and complexity correlates with customer demographics, you may inadvertently provide lower-quality service to certain customer segments. You audit routing decisions for disparate impact, ensuring that model tier assignments are based on task characteristics, not user characteristics. If you discover bias, you adjust routing rules or apply equalization strategies, like routing a random sample of all users' requests to Tier 3 models to ensure everyone receives premium service occasionally.

## Optimizing Multi-Step Workflows with Hybrid Tiering

The full power of model tiering emerges in multi-step workflows where different steps have different complexity. You design hybrid workflows that route each step to the optimal model, creating an end-to-end pipeline where cost is minimized and quality is preserved by concentrating expensive model calls only where they deliver value.

Consider a customer support agent with five steps: classify inquiry category (low), retrieve relevant knowledge base articles (low), generate a draft response (mid), refine response for tone and accuracy (mid), finalize and format (low). In a single-model architecture using Claude Opus 4.5, this workflow costs approximately $0.18 per session based on January 2026 pricing. In a tiered architecture, you route classify and retrieve to GPT-5-mini, draft and refine to GPT-5, and finalize to GPT-5-mini. The tiered workflow costs approximately $0.04 per session—a 78% cost reduction. Quality is preserved because the steps that define response quality—drafting and refinement—still use a capable model, while the steps that are purely mechanical use the cheapest viable option.

You optimize costs further by analyzing token consumption per step. If the finalize step consumes only 800 tokens on average, you batch multiple finalize steps into a single API call to amortize the fixed overhead of the API request. If the retrieve step returns large documents that consume 15,000 tokens of context, you implement a summarization sub-step using a Tier 1 model to compress the context before passing it to the Tier 2 draft model. You are not just tiering models—you are redesigning workflows to minimize token waste and concentrate spend where it matters.

You also implement conditional tiering based on intermediate results. If the classify step assigns an inquiry to a high-stakes category like "billing dispute" or "service outage," you automatically upgrade all subsequent steps to Tier 3 models because the business impact of a poor response is high. If the classify step assigns the inquiry to a low-stakes category like "FAQ request," you keep all subsequent steps in Tier 1 or Tier 2. The workflow adapts to the task, not to a static policy.

You track end-to-end costs and quality for tiered workflows. You measure total cost per session, cost per step, quality metrics for each step, and overall task success rate. You compare these metrics to the baseline single-model architecture and validate that tiering delivers the expected cost savings without degrading outcomes. If a tiered workflow underperforms the baseline, you investigate which step is the weak link and either upgrade that step's model or re-engineer the prompt.

You document tiering decisions and rationale in your agent design documentation. For each step, you record the assigned tier, the reasoning behind the assignment, the evaluation results that validated the assignment, and any fallback rules. This documentation ensures that future engineers understand why specific routing decisions were made and can update the policy intelligently as models and requirements evolve.

## Managing Model Drift and Tiering Policy Updates

Model tiering is not a one-time optimization. Models change—providers release new versions, deprecate old ones, adjust pricing, and shift performance characteristics. Your tiering policy must evolve in response to these changes, and your infrastructure must support updates without disrupting production.

You monitor model provider announcements and release notes. When OpenAI announces GPT-5.1-mini or Anthropic releases Claude Opus 5, you evaluate the new models against your tiering policy. You run your evaluation datasets through the new models and measure performance relative to the models currently in each tier. If a new Tier 1 model outperforms your current Tier 2 model at a fraction of the cost, you promote it and demote the old Tier 2 model to Tier 1 or remove it entirely. If a Tier 3 model is deprecated, you select a replacement and validate quality before the deprecation deadline.

You also monitor model drift—changes in model behavior over time even without version updates. Providers sometimes update models in place, and these updates can affect accuracy, latency, or token consumption. You track quality metrics continuously and set up alerts for degradation. If a step that historically had 96% accuracy drops to 89% over two weeks, you investigate whether a model update caused the change. If so, you either adjust prompts to compensate, switch to a different model, or escalate to the provider for support.

Pricing changes also trigger policy updates. If your Tier 1 model's price doubles due to a provider's pricing restructure, it may no longer be the cheapest option. You re-run cost comparisons across all available models and update the tier assignments. You communicate these changes to stakeholders so they understand why per-session costs might shift. You do not absorb cost increases silently—you either pass them to customers, optimize further, or adjust service levels.

You implement gradual rollouts for tiering policy changes. When you update a tier assignment, you do not switch 100% of traffic immediately. You route 10% of traffic to the new model, monitor for a day, then expand to 50%, then 100% if no issues emerge. This phased approach limits the blast radius of a bad policy change. If the new model underperforms, you roll back before impacting all users.

You version your tiering policy and track changes in source control. Every policy update is a commit with a clear description of what changed and why. You tag releases so you can correlate policy versions with cost and quality trends. If costs spike or quality drops, you can quickly identify which policy change might be responsible and revert if necessary.

Model tiering is foundational infrastructure for cost-effective agent operations. Without it, you pay frontier prices for routine tasks and bleed money on workflows that could be handled by cheaper models. With it, you match model capability to task complexity, optimize cost at the step level, and preserve quality by concentrating expensive models only where they deliver irreplaceable value. You classify steps by reasoning complexity, define tiering rules based on cost and quality trade-offs, implement dynamic routing that enforces policy at runtime, and validate quality rigorously before and after deployment. You adapt to model changes, monitor for drift, and continuously optimize tiering policies as the model landscape evolves. This is how you run high-quality agents at sustainable cost, routing every step to the right model for the job.
