# 9.10 â€” Replay and Simulation: Reproducing Agent Behavior for Analysis

In October 2025, a healthcare technology company discovered that their patient intake agent had been incorrectly routing cardiology referrals for three weeks. The agent was supposed to classify patient symptoms, determine urgency, and route to the appropriate specialty. Cardiology referrals with chest pain indicators were being routed to general internal medicine instead of cardiology, delaying specialist evaluation by an average of eleven days. The company identified 127 affected patients. When the engineering team attempted to debug the issue, they could not reproduce it. They ran the same inputs through the current version of the agent, and every test case routed correctly to cardiology. They pulled logs from the failure window, but the logs showed only final routing decisions, not the intermediate reasoning steps. They had no way to reconstruct what the agent had been thinking when it made the incorrect decisions.

The team spent four days attempting to reproduce the failure. They tested different model versions, prompt variations, and configuration states. They manually replayed inputs through previous deployments. Nothing produced the incorrect routing pattern. Finally, a senior engineer discovered that the issue occurred only when the agent processed cases during a six-hour window each day when a specific external clinical database was under maintenance and returning cached data instead of live data. The agent's retrieval step was pulling outdated protocol definitions from the cache, and those outdated protocols had different urgency classification rules. The failure was reproducible only if you replayed the exact input at the exact time with the exact external dependency state. Without replay infrastructure that captured the full execution environment, the team could not reconstruct the failure and could not validate their fix.

This is the replay and simulation problem. When an agent produces incorrect outputs, you need the ability to reproduce the exact conditions that led to the failure, re-run the agent step by step, inspect intermediate states, modify variables to test hypotheses, and validate that your fix actually resolves the issue. Unlike traditional software debugging where you can set breakpoints and step through code, agent debugging requires replaying model calls, retrieval results, tool outputs, and external API responses. You need infrastructure that captures execution context, allows deterministic replay, and supports counterfactual simulation.

## The Three Modes of Agent Replay

Agent replay operates in three modes: full deterministic replay, partial replay with live dependencies, and counterfactual simulation. Each mode serves different diagnostic and validation purposes, and you need tooling that supports all three.

Full deterministic replay reconstructs the exact execution environment from a historical run. You capture not just the input and output, but every intermediate state: the model responses, retrieval results, tool call outputs, timestamps, configuration values, and external API responses. When you replay the run, you do not call the model or external dependencies again. Instead, you play back the recorded responses. This guarantees that the replay produces identical behavior to the original run, allowing you to inspect the decision path, identify where the agent went wrong, and validate root cause hypotheses without introducing variability from live model calls or external APIs.

Partial replay with live dependencies re-runs the agent with the same input but allows some components to execute live. You might replay with the current model version and live retrieval but use recorded tool outputs. Or you might replay with live model calls but recorded external API responses. This mode lets you test whether a specific component change would have prevented the failure. If you suspect that a model upgrade fixed the issue, you replay with the new model and recorded tool outputs to isolate the model's contribution to the fix. Partial replay introduces controlled variability to test hypotheses about which components are causally responsible for the failure.

Counterfactual simulation re-runs the agent with modified inputs, prompts, or configurations to test what-if scenarios. You take a failing case, change one variable, and observe whether the agent now produces the correct output. You might modify the input to remove ambiguity, adjust the prompt to add explicit constraints, or change a retrieval parameter to return more context. Counterfactual simulation helps you understand the boundaries of the failure mode, identify which input characteristics trigger the issue, and design targeted fixes or eval cases that cover the edge case.

The healthcare company needed full deterministic replay to reproduce the cardiology routing failure. Once they built the infrastructure to capture and replay external API responses, they could replay cases from the failure window with the exact cached protocol definitions that the clinical database had returned. The replay reproduced the incorrect routing every time, confirming that the issue was caused by outdated cache data. They then used counterfactual simulation to test whether adding a cache freshness check to the retrieval step would prevent the failure. The simulation showed that the fix worked for all 127 affected cases.

## Building Replay Infrastructure

Replay infrastructure requires capturing execution traces at a level of detail far beyond typical application logging. You need to record every non-deterministic operation: model API calls and responses, retrieval queries and results, tool invocations and outputs, external API requests and responses, timestamps, random seeds, and configuration snapshots. You serialize this data into a replay log that can be stored, indexed, and loaded for replay.

Your trace format includes the full request and response for every model call. You record the prompt, the model version, the temperature, the max tokens, the stop sequences, and the full text of the model's response. You also record the response metadata: the token count, the finish reason, and the latency. This allows you to replay the model call without actually calling the model. When the agent reaches the point in execution where it would normally call the model, your replay harness instead loads the recorded response from the trace and returns it as if the model had just responded.

For retrieval steps, you record the query, the retrieval parameters, the document IDs returned, the scores, and the full text of the retrieved chunks. You also record the state of the retrieval index: the index version, the total document count, and any filtering or ranking configuration. This allows you to replay the retrieval step deterministically. If the agent retrieved documents X, Y, and Z during the original run, the replay returns the same documents with the same content, even if the index has since been updated or those documents have been modified.

For tool calls, you record the tool name, the input parameters, the output, the execution time, and any errors or warnings. If the tool is a database query, you record the query, the result set, and the database schema version. If the tool is an external API call, you record the request, the response, the status code, and the response headers. This ensures that tool behavior is deterministic during replay, even if the underlying systems have changed.

You also record the execution environment: the agent version, the code commit hash, the configuration file snapshot, the environment variables, and the system timestamp. This allows you to reconstruct the full deployment state at the time of the original run. If a configuration change or code deployment caused the failure, the replay environment reflects that state.

Your replay logs are indexed by session ID, run ID, timestamp, and input hash. You store them in a durable, queryable data store with retention policies that match your compliance and debugging needs. For high-stakes domains like healthcare or finance, you may retain replay logs for years. For lower-stakes applications, you may retain them for weeks or months. The key is that when an incident occurs, you can retrieve the replay logs for affected runs within seconds.

## Deterministic Replay for Root Cause Isolation

When you identify a failing case, you load its replay log and re-execute the agent in replay mode. Your agent runtime has a replay harness that intercepts all non-deterministic operations and substitutes recorded responses. The agent executes its decision logic step by step, but instead of calling the model, it reads the recorded model response. Instead of querying the retrieval index, it reads the recorded retrieval results. Instead of calling external APIs, it reads the recorded API responses.

As the agent executes, your replay harness logs every decision point, condition evaluation, and state transition. You can set breakpoints at specific steps, inspect variable values, and trace the control flow. You identify the exact line of logic where the agent's reasoning diverged from the correct path. For the healthcare company, the replay showed that the agent's urgency classification logic checked whether the retrieved protocol definition included the term "emergent chest pain" in the urgency criteria. The cached protocol definition from the outdated database did not include this term. The agent classified the case as non-urgent and routed to general medicine instead of cardiology.

You can also replay multiple failing cases side by side and compare their traces. If ten cases failed, you replay all ten and look for commonalities. Do they all fail at the same decision point? Do they all retrieve the same incorrect documents? Do they all call the same tool with unexpected outputs? These patterns point you to the root cause faster than analyzing cases individually.

Deterministic replay is also critical for validating fixes. Once you identify the root cause and deploy a candidate fix, you replay the failing cases with the new code or configuration. If the fix is correct, all cases now produce the correct output. If some cases still fail, you know the fix is incomplete. This validation happens in seconds, not hours, because you are not waiting for live traffic to exercise the edge case. You are replaying the exact failure conditions with the new logic.

You also use deterministic replay for regression testing. When you deploy a change, you replay a sample of recent production runs to ensure that the change does not introduce new failures. You select a diverse set of cases that cover different input patterns, task types, and edge cases. You replay them with the new deployment and compare outputs to the original outputs. Any divergence triggers a review. This proactive regression detection prevents incidents caused by unintended side effects of seemingly unrelated changes.

## Partial Replay for Component Attribution

Partial replay allows you to test which components of your agent are responsible for a failure or a success. You replay a case with one component updated to its current version and all other components held at their historical state. You observe whether the output changes. If it does, you have identified the component that caused the difference.

For example, you suspect that a model upgrade from GPT-4o to GPT-4.5 improved performance on a specific task type. You take a sample of cases that failed under GPT-4o and replay them with GPT-4.5 model responses but with all retrieval results, tool outputs, and configurations held constant from the original run. If the cases now succeed, you have confirmed that the model upgrade was causally responsible for the improvement. If they still fail, the improvement came from some other component change.

Partial replay is particularly useful when you deploy multiple changes simultaneously. You release a new prompt template, a new retrieval strategy, and a new model version in the same deployment. Performance improves. Which component drove the improvement? You replay a sample of cases with each component updated individually and measure the impact. You discover that the new retrieval strategy accounted for 80% of the performance gain, the new prompt template accounted for 15%, and the model upgrade accounted for 5%. This attribution informs future optimization priorities.

You also use partial replay to test counterfactual scenarios. If you had deployed a different model version, would the incident have occurred? You replay the failing cases with the alternative model version and observe whether the failures persist. If they do not, you consider switching to that model version or adding model version as a configurable parameter that you can toggle during incidents.

The healthcare company used partial replay to test whether upgrading to a newer version of their retrieval library would have prevented the cardiology routing failure. They replayed the 127 affected cases with the new retrieval library, which included a cache freshness check, and observed that all cases routed correctly. This confirmed that the library upgrade was a sufficient fix and they did not need to implement custom cache validation logic.

## Counterfactual Simulation for Robustness Testing

Counterfactual simulation tests how the agent behaves under modified conditions. You take a real input, perturb it in specific ways, and observe whether the agent's output changes. You test input robustness by adding typos, rephrasing the input, or changing the order of information. You test retrieval robustness by modifying the retrieved documents, removing key context, or introducing noise. You test tool robustness by simulating errors, timeouts, or unexpected responses. These simulations reveal fragility in your agent's logic and help you design more robust decision paths.

For input perturbations, you start with a case where the agent produced the correct output. You then introduce variations: change capitalization, add irrelevant sentences, rephrase the request, swap the order of clauses. You replay the agent with each variation and check whether the output remains correct. If small perturbations cause the output to degrade, you have identified a robustness gap. You add these perturbed cases to your eval suite and iterate on the prompt or model to improve stability.

For retrieval perturbations, you modify the retrieved documents to test how sensitive the agent is to context quality. You remove the most relevant chunk, inject a misleading chunk, or reduce the retrieval count. You replay the agent and observe whether it still produces the correct output or whether it degrades gracefully. If the agent fails catastrophically when one key document is missing, you know you need to add redundancy or fallback logic. If the agent gracefully handles missing context by requesting clarification or deferring to a human, you have validated that your robustness design works.

For tool perturbations, you simulate failures in external dependencies. You replace a successful tool response with an error message, a timeout, or an empty result set. You replay the agent and check whether it retries, falls back to an alternative tool, or surfaces the error to the user. If the agent crashes or produces nonsensical output when a tool fails, you have identified a gap in error handling. You add try-catch logic, timeout handling, or fallback strategies and validate the improvement with additional simulations.

You also use counterfactual simulation to test boundary cases. If the agent succeeded with a retrieval count of five documents, does it still succeed with three? With ten? You replay the case with different retrieval counts and measure the impact on output quality. You identify the minimum retrieval count that preserves correctness and use that as a default. If the agent succeeded with a model temperature of 0.7, does it still succeed at 0.3? At 1.0? You replay with different temperatures and assess output consistency. These simulations help you tune hyperparameters with empirical evidence rather than guesswork.

The healthcare company used counterfactual simulation to test how their agent behaved when the clinical database was unavailable. They replayed patient intake cases with the database tool returning an error instead of cached data. The agent crashed. They added error handling logic that deferred to a human operator when the database was unavailable, replayed the simulations, and confirmed that the agent now handled the failure gracefully.

## Replay-Driven Debugging Workflow

Your debugging workflow integrates replay as a first-class tool. When a failure is detected, the first step is to pull the replay log for the affected run. You load the log into your replay harness and execute a full deterministic replay to confirm that you can reproduce the failure. If the replay succeeds, you have a stable reproduction environment. If the replay does not reproduce the failure, the issue is non-deterministic or the trace capture is incomplete, and you need to investigate why.

Once you have a stable reproduction, you inspect the trace to identify the decision point where the agent diverged from correct behavior. You examine the inputs to that step, the model response or tool output, and the subsequent logic. You form a hypothesis about what went wrong. You then test the hypothesis with partial replay or counterfactual simulation. If you suspect the model response was incorrect, you replay with a different model version. If you suspect the retrieval results were insufficient, you replay with modified retrieval parameters. You iterate until you confirm the root cause.

You then design a fix. The fix might be a prompt change, a configuration update, a code modification, or a dependency upgrade. You apply the fix in a staging environment and replay the failing cases to validate that the fix resolves the issue. You also replay a broader sample of recent production cases to ensure that the fix does not introduce regressions. Once validation passes, you deploy the fix to production and monitor for recurrence.

You document the debugging process in your incident postmortem. You include the replay log IDs, the hypotheses tested, the counterfactual simulations run, and the validation results. This documentation serves as a reference for future incidents and helps other engineers learn your debugging methodology.

## Replay Infrastructure Design Patterns

Your replay infrastructure follows several design patterns that make replay fast, reliable, and scalable. First, you separate trace capture from trace replay. Trace capture happens in the production agent runtime with minimal performance overhead. You serialize trace data to a message queue or log stream asynchronously, so that capturing a trace does not add latency to the agent's response time. Trace replay happens in a separate replay runtime that loads traces from storage and executes them in a controlled environment.

Second, you use content-addressable storage for large replay artifacts like retrieved documents or tool outputs. Instead of storing the full text of a document in every trace, you store a hash and a reference to the document in a shared content store. When you replay, you load the document by hash. This reduces storage costs and allows you to deduplicate common artifacts across traces.

Third, you version your trace schema. As you add new components to your agent or change logging formats, you increment the trace schema version and write migration logic that can read old traces and convert them to the new format. This ensures that you can replay traces from months or years ago even as your agent evolves.

Fourth, you build replay acceleration tools. For cases where you need to replay thousands of traces, you parallelize replay across multiple workers. You distribute the traces to workers, replay them concurrently, and aggregate the results. This allows you to validate a fix across a large sample of production cases in minutes rather than hours.

Fifth, you provide a replay UI that allows non-engineers to trigger replays and inspect results. Product managers, domain experts, and trust and safety operators can load a failing case, replay it step by step, and see the intermediate states in a human-readable format. This democratizes debugging and allows domain experts to contribute to root cause analysis without needing engineering support.

## Replay for Continuous Validation

Replay is not just a debugging tool. It is also a continuous validation tool. You run daily or weekly replay jobs that take a sample of recent production cases and replay them with the current agent version. You compare the replayed outputs to the original outputs. Any divergence triggers an alert. This proactive replay detects regressions caused by dependency changes, data drift, or configuration drift that your eval suite did not catch.

You also use replay to validate that your agent is stable over time. You select a set of canonical cases that represent core functionality and replay them weekly. If the outputs remain consistent, you have evidence that the agent is not drifting. If the outputs start to diverge, you investigate whether the drift is intentional or a regression. This stability monitoring is particularly important for agents that operate in regulated domains where consistency and auditability are compliance requirements.

You integrate replay into your deployment pipeline. Before promoting a deployment to production, you replay a regression test suite of historical cases and validate that outputs are consistent with expectations. If replay detects unexpected changes, the deployment is blocked until the changes are reviewed and approved. This prevents accidental breakage from reaching production.

## Replay as a Learning and Training Tool

Replay is also valuable for onboarding new engineers and training domain experts. You curate a library of interesting or instructive replay logs: cases where the agent succeeded under challenging conditions, cases where it failed and was subsequently fixed, and cases that illustrate key decision patterns. New team members work through these replays to understand how the agent reasons, what failure modes exist, and how debugging and validation work.

You also use replay logs in design reviews. When proposing a change to the agent's logic, you replay a diverse set of cases with the proposed change and present the results to the team. You show how the change improves performance on some cases, has no effect on others, and potentially degrades performance on edge cases. The team reviews the replays, discusses trade-offs, and makes an informed decision about whether to proceed with the change.

Replay also supports domain expert review. You select cases where the agent made surprising or borderline decisions and replay them for domain experts. The experts watch the agent's reasoning step by step and provide feedback on whether the logic aligns with domain standards. This feedback loop helps you refine the agent's decision criteria and catch subtleties that automated evals miss.

The healthcare company built a replay library of fifty representative patient intake cases covering common symptoms, edge cases, and rare conditions. They used this library for training new clinical operations staff, validating protocol updates, and demonstrating the agent's capabilities to hospital partners. The replay library became a living artifact that evolved as the agent learned new patterns and handled new edge cases.

When you invest in replay infrastructure, you transform debugging from a time-consuming investigation into a systematic, reproducible process. You validate fixes with confidence, test robustness with counterfactual simulations, and continuously monitor for regressions. Replay becomes the foundation of reliable agent operations, enabling you to operate production systems with the rigor and discipline that high-stakes domains demand.
