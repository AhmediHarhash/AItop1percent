# 1.2 — The Agentic Spectrum: From ReAct Loops to Autonomous Systems

In July 2025, an enterprise software company spent nine months building what they called a "fully autonomous sales intelligence agent." The system was supposed to monitor prospect behavior, identify buying signals, research company backgrounds, draft personalized outreach, and schedule meetings—all without human intervention. The team invested heavily in sophisticated planning algorithms, multi-model orchestration, and a complex state machine to track prospect interactions across weeks.

They launched to their sales team in April 2026. Within three days, they had to pull it offline. The system had sent 1,847 emails, many of them to the same prospects multiple times, some with hallucinated information about the prospect's company, and several that were flagged as spam. The "autonomous" agent had no way for salespeople to review outreach before it went out, no way to pause when it was uncertain, and no way to escalate decisions that required judgment. The sales team was furious. Prospects were confused. The company's email deliverability score tanked.

The failure was not that the system could not execute its tasks. It could research, draft emails, and send them. The failure was that the team built a fully autonomous system for a problem that needed assisted autonomy. Salespeople did not want the agent to operate independently—they wanted it to do the research and drafting, then present recommendations for their approval. They wanted a system at the reactive end of the spectrum, not the autonomous end.

The team spent another four months rebuilding the system with human-in-the-loop approval gates, much simpler state management, and single-episode execution instead of continuous operation. The new version was a third of the code, significantly cheaper to run, and actually got adopted. The difference was understanding where on the agentic spectrum the system needed to be.

## The Five Tiers of Autonomy

Autonomy is not binary. There is a spectrum from zero independence to full autonomy, and each tier has distinct characteristics, capabilities, risks, and appropriate use cases. Most teams fail because they build for the wrong tier—usually too autonomous for what the problem requires.

**Tier 0: Single LLM Call.** The system takes input, generates output, returns it to the user. There is no iteration, no tool use, no state. This is a chatbot responding to a message, a completion API generating text, or a classification model labeling data. Capabilities are limited to what can be accomplished in one inference pass. Risks are limited to the quality of that single output. This tier is appropriate for any task that can be framed as input-to-output transformation: translation, summarization, classification, extraction, simple question-answering.

**Tier 1: Tool-Augmented Chat.** The system can call tools to retrieve information or perform actions, but only in direct response to user requests and only within a single turn. The user asks a question, the system calls a tool, incorporates the result, and returns a response. There is no multi-step planning. The system does not iterate. It observes the tool result only to format it for the user, not to decide what to do next. This is ChatGPT with plugins when you ask it a weather question, a customer support bot that looks up account info, or a code assistant that runs a test when you ask it to. Capabilities extend to tasks that require external data or simple actions. Risks include tool misuse in the single call and incorrect interpretation of tool results. This tier is appropriate for tasks where one piece of external information or one action is sufficient to respond to the user.

**Tier 2: ReAct Loops.** The system executes multi-step workflows by iterating through a cycle of reasoning, acting via tool calls, and observing results. Each observation informs the next reasoning step. The system can adapt based on intermediate outcomes. It has termination logic to stop when the goal is achieved or determined unachievable. The loop is user-initiated and runs until completion. This is the classic ReAct pattern: the user asks a research question, the agent searches, reads results, decides what is missing, searches again, synthesizes findings, and returns a summary. Capabilities extend to tasks that require multiple steps, information gathering from multiple sources, and adaptation to partial information. Risks include infinite loops, compounding errors, and failure to terminate. This tier is appropriate for research tasks, complex question-answering, multi-step troubleshooting, and data gathering workflows.

**Tier 3: Plan-and-Execute Agents.** The system generates an explicit plan before execution, then executes the plan step by step, adapting the plan when steps fail or new information emerges. Planning and execution are separate phases. The agent can replan when the environment changes or when it discovers that the original plan is not working. User initiates the task, the agent plans and executes, then reports back. This is a coding agent that analyzes requirements, generates a plan (write function A, write test for A, implement feature B, integrate A and B), executes each step, and adjusts the plan when tests fail. Capabilities extend to tasks where upfront planning improves outcomes, where the task is too complex for pure reactive loops, and where you want to inspect or approve the plan before execution. Risks include plans that become stale, over-commitment to bad plans, and complexity in replanning. This tier is appropriate for software tasks, complex multi-step workflows, and tasks where plan transparency is valuable.

**Tier 4: Goal-Directed Autonomous Agents.** The system operates continuously or over extended periods, pursuing a defined goal without per-task user initiation. It observes the environment, takes actions when conditions warrant, and adapts its strategy based on outcomes. It has autonomy over when to act, not just how. The user sets goals and constraints, and the agent operates within them. This is a monitoring agent that watches system metrics, detects anomalies, investigates root causes, and takes corrective action like restarting services or scaling resources. It is a scheduling agent that coordinates tasks across a team, adjusting schedules when deadlines change or resources become unavailable. Capabilities extend to continuous operation, proactive action, and long-term goal pursuit. Risks include unintended actions, divergence from user intent, and difficulty in stopping or correcting the agent mid-execution. This tier is appropriate for monitoring, orchestration, continuous optimization, and tasks where speed of response is critical and human approval would be a bottleneck.

**Tier 5: Fully Autonomous Systems.** The system operates independently with minimal human oversight, making high-stakes decisions in real time. It has authority to take consequential actions: financial transactions, infrastructure changes, communication on behalf of the organization. It operates in environments where errors can be costly, but speed and scale justify the risk. This is an autonomous trading system, an autonomous vehicle, or a fully autonomous infrastructure management system. Capabilities are maximal: the system can do anything within its defined scope without asking permission. Risks are maximal: the system can cause significant harm through errors, misalignment, or unforeseen interactions. This tier is appropriate only when the value of autonomy is very high, the cost of mistakes is manageable or mitigatable, and you have mature evaluation, monitoring, and intervention systems.

Most production agent systems in 2026 sit at Tier 2 or Tier 3. Tier 1 is extremely common but often mislabeled as agentic. Tier 4 is emerging in specific domains like DevOps, monitoring, and workflow orchestration. Tier 5 is rare, high-risk, and reserved for specialized applications where autonomy is essential and the organization can tolerate the risk.

## Characteristics of Each Tier

Each tier has distinct architectural and operational characteristics that determine what you need to build and how you evaluate success.

**Tier 0** is stateless. Every request is independent. You do not need to track what the system did before. You do not need termination logic because execution ends when the LLM returns output. Error handling is simple: if the LLM call fails, you return an error. Latency is one round trip to the model. Cost is one inference. Evaluation is input-output pair testing.

**Tier 1** adds tool execution but remains single-turn. You need tool definitions, function calling, and error handling for tool failures. You need to handle cases where the tool returns unexpected data or fails. But you still do not need state management across turns. Each user request is independent. Latency is one LLM call plus tool execution time. Cost is one inference plus tool costs. Evaluation is input-output testing with tool result simulation.

**Tier 2** introduces iteration and state. You need to track what the agent has tried, what it has learned, and what remains to be done. You need termination logic: maximum iterations, goal achievement detection, failure detection. You need loop detection to prevent infinite cycles. You need error recovery: if a tool call fails, the agent must adapt, not just crash. Latency is multiple round trips, possibly dozens. Cost is multiple inferences plus multiple tool calls. Evaluation is episode-based: did the agent achieve the goal, how efficiently, what errors occurred?

**Tier 3** adds explicit planning. You need a planner that can generate structured plans from goals. You need plan representation: a data structure that captures steps, dependencies, and status. You need plan execution logic that interprets the plan and calls tools accordingly. You need replanning: when steps fail or conditions change, the agent must update the plan. You need plan validation: can this plan actually achieve the goal? Latency includes planning time upfront plus execution time. Cost includes planning inferences plus execution inferences and tool calls. Evaluation includes plan quality assessment, not just outcome quality.

**Tier 4** adds continuous operation. You need persistent state across sessions, not just within episodes. You need scheduling: when should the agent check conditions and decide whether to act? You need event-driven triggers: the agent reacts to environment changes, not just user requests. You need resource management: the agent must operate within compute and cost budgets over time. You need richer termination logic: pause, resume, cancel, escalate. Latency becomes less meaningful because the agent operates in the background. Cost is ongoing, not per-task. Evaluation includes long-term behavior: does the agent maintain goals over days or weeks, does it adapt to changing conditions, does it respect constraints consistently?

**Tier 5** adds high-stakes decision authority. You need extensive safety mechanisms: pre-action validation, impact assessment, rollback capabilities. You need monitoring and alerting that can detect and stop harmful behavior in real time. You need auditability: every action must be logged with reasoning. You need fallback to human oversight when the agent encounters situations outside its training or policy. Latency must be real-time for time-critical decisions. Cost includes not just execution but also safety infrastructure. Evaluation includes adversarial testing, worst-case scenario simulation, and continuous monitoring in production.

Understanding these characteristics helps you choose the right tier and build the right infrastructure. If you are building a Tier 2 agent, you do not need the scheduling and event infrastructure of Tier 4. If you are building a Tier 4 agent, you cannot skip the state management and error recovery of Tier 2—it is a prerequisite.

## Capabilities and Limitations at Each Tier

Each tier has a ceiling on what it can accomplish. Trying to push a lower-tier system to do higher-tier work leads to brittle hacks and unpredictable failures.

**Tier 0** can handle any task that reduces to a single input-output transformation. Classification, extraction, translation, summarization, generation of creative content, simple question-answering. It cannot handle tasks that require external information, multiple steps, or adaptation. If you need to look something up, you need at least Tier 1. If you need to iterate based on results, you need Tier 2 or higher.

**Tier 1** can handle tasks that require one external lookup or action per user request. Fetching data, running a simple computation, executing a single API call. It cannot handle tasks that require iteration: if the first search does not return what you need, the system cannot search again with a different query. If a tool call fails, the system cannot try an alternative. It cannot gather information from multiple sources and synthesize it. For that, you need Tier 2.

**Tier 2** can handle tasks that require multi-step information gathering, adaptation based on intermediate results, and synthesis across sources. Research, troubleshooting, complex question-answering, data analysis that requires looking at multiple datasets. It cannot handle tasks that require long-term continuity: if the task takes hours or days, the system cannot persist state across sessions. It cannot operate proactively: it waits for user initiation. For continuous or proactive operation, you need Tier 3 or 4.

**Tier 3** can handle complex tasks that benefit from upfront planning, tasks where the plan itself is valuable to inspect, and tasks where the structure of the solution matters. Software engineering, complex workflow execution, multi-phase projects. It still requires user initiation and runs to completion within a session. It cannot operate continuously in the background or respond proactively to events. For that, you need Tier 4.

**Tier 4** can handle continuous monitoring, proactive response to events, long-term goal pursuit, and orchestration across extended time periods. It can operate without per-task user initiation, adapting to changing conditions and maintaining state across sessions. It cannot make high-stakes decisions without approval unless you move to Tier 5, which adds the authority and safety mechanisms for consequential autonomy.

**Tier 5** can handle anything within its scope, with the authority to take consequential actions in real time. The limitation is not capability—it is risk. Tier 5 systems require mature safety infrastructure, extensive testing, and organizational readiness to handle failures. Most organizations are not ready for Tier 5, even if the technology is.

## Risks at Each Tier

Risk increases with autonomy. Higher tiers introduce new failure modes and amplify the consequences of errors.

**Tier 0** risks are limited to output quality: hallucination, bias, toxicity, incorrectness. The system can generate bad output, but it cannot take actions that change the world. Mitigation is output validation, content filtering, and user awareness that the output may be wrong.

**Tier 1** risks include incorrect tool use: calling the wrong tool, passing malformed parameters, misinterpreting tool results. The system can take one action, so the blast radius is one API call, one write operation, one message sent. Mitigation is tool input validation, permission scoping, and user confirmation for high-stakes actions.

**Tier 2** risks include infinite loops, compounding errors, resource exhaustion, and failure to terminate. The system can iterate, so a bad decision in step 3 can cascade through steps 4, 5, 6. A loop that does not terminate consumes resources until you kill it. Mitigation is loop detection, maximum iteration limits, anomaly detection, and observability into the agent's state and reasoning.

**Tier 3** risks include over-commitment to bad plans, stale plans that no longer match the environment, and plan execution failures. The system generates a plan and executes it, but if the plan is wrong, the entire episode fails. Mitigation is plan validation, replanning on failure, and human review of plans before execution for high-stakes tasks.

**Tier 4** risks include unintended actions, goal drift, and difficulty stopping the agent. The system operates continuously, so it can take many actions before you notice something is wrong. Mitigation is extensive monitoring, circuit breakers, anomaly detection, and clear escalation paths for unusual situations.

**Tier 5** risks include all of the above, amplified, plus the risk of catastrophic errors in high-stakes domains. The system has authority to make decisions with significant consequences, so a single error can be very costly. Mitigation requires mature safety infrastructure: pre-action validation, impact assessment, rollback capabilities, real-time monitoring, and the ability to intervene instantly.

Teams often underestimate the risk jump between tiers. A Tier 2 agent that loops a few extra times is annoying but not dangerous. A Tier 4 agent that loops a few extra times might send dozens of unwanted emails or make dozens of unnecessary API calls before you notice. A Tier 5 agent that makes one wrong decision might execute a financial transaction you cannot reverse or take down a production system.

## Appropriate Use Cases for Each Tier

Choosing the right tier means matching the level of autonomy to the task requirements and organizational tolerance for risk.

**Use Tier 0 when** the task is a pure transformation, no external data or actions are needed, and one LLM call is sufficient. Content generation, summarization, classification, extraction from provided text, translation. This is the default starting point for any LLM application.

**Use Tier 1 when** you need one external lookup or one simple action per user request. Question-answering with retrieval, simple tool use (calculate, convert, fetch data), single-step task execution. This is appropriate for most chatbot use cases, customer support bots, and information retrieval assistants.

**Use Tier 2 when** the task requires multiple steps, information gathering from multiple sources, or adaptation based on partial results. Research, complex question-answering, troubleshooting, data analysis, exploratory workflows. This is the sweet spot for many production agent systems: enough autonomy to handle complex tasks, not so much that the risk becomes unmanageable.

**Use Tier 3 when** upfront planning improves outcomes, the task structure is complex enough to benefit from a plan, or you want transparency into the agent's intended approach before execution. Software engineering agents, complex workflow orchestration, project planning. The added complexity of planning is worth it when tasks are long-running, expensive, or high-stakes enough that you want to see the plan first.

**Use Tier 4 when** the task requires continuous operation, proactive response, or long-term goal pursuit, and human-in-the-loop approval for every action would be a bottleneck. Monitoring agents, scheduling agents, optimization agents, alerting and incident response. The value of speed and continuous operation outweighs the risk of occasional errors.

**Use Tier 5 when** the value of full autonomy is very high, the cost of errors is manageable, and you have the safety infrastructure to mitigate risks. Autonomous trading in constrained scenarios, autonomous infrastructure management with rollback capabilities, autonomous vehicle control. This tier is not appropriate for most organizations or use cases. If you are unsure whether you need Tier 5, you do not.

## The Autonomy-Trust Tradeoff

There is a fundamental tradeoff between autonomy and trust. The more autonomous the system, the more you must trust it to make good decisions without oversight. The less you trust it, the more you must constrain its autonomy.

In Tier 0 and Tier 1, trust requirements are low. The system generates output or takes one action. You can review the output before using it. You can undo the action if it was wrong. Errors are contained.

In Tier 2, trust requirements increase. The system will take multiple actions before you see the result. You trust it to iterate responsibly, to terminate when appropriate, and to avoid getting stuck in loops. If that trust is misplaced, the system wastes resources or produces bad results, but the damage is limited to the episode.

In Tier 3, trust requirements include trusting the system to plan well. You are giving it latitude to design a multi-step approach, and you trust that the plan will be reasonable. If the plan is bad, the entire episode is wasted.

In Tier 4, trust requirements are high. The system operates without per-task approval. You trust it to act proactively, to make correct decisions about when to act and when to wait, and to stay within the bounds of its authority. If that trust is violated, the system can take many unwanted actions before you intervene.

In Tier 5, trust requirements are maximal. You are trusting the system to make high-stakes decisions in real time. If the trust is misplaced, the consequences can be severe.

This tradeoff means you cannot just dial up autonomy to get better performance. You must earn the right to deploy higher-tier systems by building trust: through rigorous evaluation, extensive testing, observability, safety mechanisms, and demonstrated reliability at lower tiers. Teams that jump straight to Tier 4 or Tier 5 without proving their systems at Tier 2 and Tier 3 are taking risks they do not understand.

## Why Most Production Systems Should Sit in the Middle

The extremes of the spectrum—Tier 0 and Tier 5—are well understood. Tier 0 is standard LLM application development. Tier 5 is highly specialized, high-risk work that most teams will never do.

The middle tiers—Tier 2, Tier 3, Tier 4—are where most production agent systems live, and they represent the right balance for most use cases. Tier 2 and Tier 3 provide enough autonomy to handle complex tasks without the risk and infrastructure burden of continuous autonomous operation. Tier 4 is appropriate for specific domains where continuous operation is essential.

The mistake teams make is building Tier 4 or Tier 5 systems when Tier 2 or Tier 3 would suffice. Autonomy is expensive. It requires more infrastructure, more evaluation, more monitoring, and more organizational process. Every tier up the spectrum adds complexity and risk. If you can solve the problem at a lower tier, you should.

The sales intelligence team that opened this chapter learned this the hard way. They built a Tier 5 system—fully autonomous, continuous operation, high-stakes actions—when the problem required Tier 2: user-initiated, multi-step research and drafting, with human approval before sending. The Tier 2 version was simpler, cheaper, safer, and actually got used.

When in doubt, start at the lowest tier that can handle the task. If Tier 1 suffices, do not build Tier 2. If Tier 2 suffices, do not build Tier 3. Prove the system works at the lower tier, build organizational trust, and only then consider moving up the spectrum if the use case justifies it.

## Placing Your System on the Spectrum

To place your system correctly, ask these questions:

**Does the task require iteration, or can it be done in one step?** If one step suffices, you need Tier 0 or Tier 1, not an agent. If iteration is required, you need Tier 2 or higher.

**Does the task require external data or actions?** If no, Tier 0. If yes, Tier 1 or higher.

**Does the task require adapting based on intermediate results?** If no, Tier 1 might suffice. If yes, Tier 2 or higher.

**Does the task benefit from upfront planning, or is reactive iteration sufficient?** If reactive works, Tier 2. If planning adds value, Tier 3.

**Does the task require continuous operation, or is user initiation acceptable?** If user initiation works, stay at Tier 2 or Tier 3. If continuous operation is essential, consider Tier 4.

**Does the task require real-time high-stakes decisions without human approval?** If no, stay at Tier 4 or below. If yes, and you have the safety infrastructure and organizational readiness, consider Tier 5.

**What is the cost of errors?** If errors are low-cost (wasted compute, annoying output), higher tiers may be acceptable. If errors are high-cost (financial loss, reputational damage, safety incidents), you need either lower tiers or extensive safety mechanisms.

**What is your organizational trust in the system?** If trust is low, constrain autonomy. If trust is high because you have proven the system extensively, you can consider higher tiers.

Honest answers to these questions will place your system on the spectrum. Most teams will land at Tier 2 or Tier 3. Some will need Tier 4. Very few will need Tier 5.

The spectrum is a design tool. It gives you language to talk about autonomy, a framework to make architectural decisions, and a reality check when you are tempted to over-engineer. Agents are powerful, but power comes with cost and risk. The right tier is the one that solves your problem with the minimum necessary autonomy.
