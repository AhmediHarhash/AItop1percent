# 2.8 — Agentic RAG: When Agents Decide When and How to Retrieve

In February 2024, an enterprise knowledge management company called ContextIQ was demonstrating their AI research assistant to a potential client—a pharmaceutical company doing drug discovery research. The demonstration query was sophisticated: "Summarize recent research on KRAS inhibitors, focusing on clinical trial results from the past two years, and compare efficacy data across different cancer types." ContextIQ's system was a state-of-the-art RAG pipeline with advanced embedding models, hybrid search, and reranking. It retrieved thirty relevant papers from their biomedical knowledge base, generated a comprehensive summary, and presented it to the pharma company's research director. She read for thirty seconds and then said, "This is interesting, but you're missing the Nature Medicine paper from November 2023 that showed completely different results. Also, you didn't include any data from ongoing trials registered on ClinicalTrials.gov, which would give us the most current picture." ContextIQ's engineer checked their system—the Nature Medicine paper wasn't in their knowledge base because their ingestion pipeline had missed it, and they didn't index ClinicalTrials.gov at all. The system had no idea these gaps existed. It retrieved from what it had, generated confidently from incomplete information, and never indicated that it might be missing critical sources. The pharma company thanked them politely and never followed up.

You're building RAG systems that need to do more than execute a fixed retrieve-then-generate pipeline. They need to reason about what information they need, where to find it, whether what they retrieved is sufficient, and what to do when it's not. They need to understand the boundaries of their knowledge, recognize when they should search multiple sources, adapt their retrieval strategy based on what they find, and critically evaluate retrieval quality before generating answers. This is agentic RAG—where the agent becomes an active participant in the retrieval process rather than a passive consumer of whatever the pipeline returns.

## The Limits of Static RAG Pipelines

Traditional RAG follows a fixed sequence: take the user query, convert it to embeddings, search a vector database, retrieve top-k documents, stuff them into context, generate an answer. This pipeline assumes you know in advance what to retrieve, where to retrieve from, and how many documents you need. These assumptions break constantly in real applications.

The what-to-retrieve assumption breaks when the user query isn't the optimal retrieval query. A user asks "Why did our revenue drop last quarter?" The literal query retrieval will find documents mentioning revenue and quarters, but might miss documents about market conditions, competitor actions, or operational changes that explain the drop. A human researcher wouldn't just search for "revenue drop"—they'd think about what factors could cause revenue drops and search for those factors. Your agent needs the same capability.

The where-to-retrieve assumption breaks when information is distributed across multiple sources with different characteristics. You have a SQL database for structured business metrics, a vector store for documentation, a graph database for relationships, and external APIs for real-time data. Which source should you query for which parts of the user's question? A static pipeline picks one source or naively queries all of them. An agentic approach reasons about which sources are relevant and queries strategically.

The how-many assumption breaks when information needs vary by query complexity. Simple factual questions need one or two highly relevant documents. Complex analytical questions need dozens of documents providing different perspectives. Current events questions might need zero documents from your static knowledge base and instead need real-time web search. A fixed top-k parameter can't adapt to these different needs.

The deeper limitation is that static RAG can't learn from retrieval results to improve retrieval. If the first retrieval attempt returns low-quality results, the pipeline just proceeds anyway, generates from poor context, and gives the user a poor answer. There's no feedback loop where the system recognizes "these documents don't actually answer the question" and tries a different retrieval strategy. The pipeline is blind to its own retrieval quality.

## The Agent as Query Planner

Agentic RAG starts with query planning—before executing retrieval, the agent reasons about what information it needs and how to obtain it. This adds a reasoning step before retrieval but dramatically improves retrieval effectiveness. The agent analyzes the user query, identifies information requirements, decomposes complex information needs into sub-queries, and plans a retrieval strategy.

Query analysis identifies the query type and information characteristics. Is this a factual lookup, a comparison, a causal explanation, a prediction? Does it require recent information or stable historical information? Is it narrow and specific or broad and exploratory? The answers determine retrieval strategy. Factual lookups can use direct semantic search. Comparisons need parallel retrieval for each entity being compared. Causal explanations might need temporal ordering of events. Predictions need recent trends.

Decomposition breaks complex queries into sub-queries that are individually retrievable. "Compare the performance of our three product lines across demographics and identify which demographic segments show the strongest growth" becomes multiple sub-queries: "What are our three product lines?", "What demographic data do we have?", "What are the sales by product and demographic?", "Which segments show growth?" Each sub-query can be mapped to specific data sources and retrieval methods.

Source selection chooses which knowledge bases to query for each sub-query. The agent maintains a model of what information lives where—structured metrics in SQL, unstructured text in vector stores, relationships in graph databases, real-time data from APIs. Given a sub-query, it identifies the appropriate sources and constructs source-specific queries. This might mean converting a natural language question to SQL for database queries, using semantic search for document retrieval, or formulating API calls for external data.

The output of query planning is a retrieval plan: a structured representation of what to retrieve, where to retrieve from, in what order, and how to combine results. This plan might be sequential—retrieve A, then use results from A to inform retrieval of B. It might be parallel—retrieve A and B simultaneously, then merge. It might be conditional—retrieve A, check if it's sufficient, retrieve B only if needed. The plan makes the retrieval strategy explicit and executable.

## Adaptive Retrieval Strategy Selection

Different retrieval strategies work better for different queries and content types. Dense retrieval using embeddings excels at semantic similarity—finding documents that discuss the same concepts even if they use different words. Sparse retrieval using keyword search excels at exact matching—finding documents that mention specific entities, technical terms, or identifiers. Hybrid approaches combine both. Your agent needs to select the right strategy for each query.

The selection can be learned from query characteristics. Queries with specific named entities, product codes, or technical identifiers benefit from keyword search—you want exact matches for "Model X-2000" not semantically similar "Model Y-3000." Queries with conceptual terms and natural language benefit from semantic search—"documents about customer satisfaction" should match documents discussing "user happiness" even if they never say "customer satisfaction." Queries that blend both benefit from hybrid search with appropriate weighting.

The agent can also adapt strategy based on initial results. Start with the default strategy, evaluate result quality, and switch strategies if quality is poor. If semantic search returns many documents but none are highly relevant—you're in the similarity-but-not-relevance zone—try adding keyword constraints. If keyword search returns too few results, relax to semantic search. This adaptive approach treats the first retrieval as a probe that informs the second attempt.

Another dimension is retrieval scope—how broadly to search. Narrow retrieval searches a small, specific subset of your knowledge base. Broad retrieval searches everything. The tradeoff is precision versus recall. Narrow retrieval gives you highly relevant results if you correctly identified the subset to search. Broad retrieval gives you higher coverage but more noise. The agent can start narrow and expand if necessary, or start broad and filter aggressively, depending on the query characteristics.

Time-based scoping matters for queries that reference recency. "Latest research on topic X" should constrain retrieval to recent documents. "Historical context for event Y" should include older documents. The agent needs to parse temporal references from queries and apply appropriate filters. This is especially important for knowledge bases that accumulate information over time—searching everything gives you a mix of current and outdated information, and the agent needs to distinguish when old information is historical context versus when it's just stale.

## Multi-Source Retrieval and Synthesis

Real knowledge doesn't live in a single vector database. It's distributed across structured databases, unstructured documents, graph relationships, external APIs, and real-time data streams. Agentic RAG systems need to retrieve from multiple heterogeneous sources and synthesize information across them. This is architecturally complex but essential for comprehensive answers.

The multi-source challenge starts with query translation. The same information need might map to a SQL query for structured data, a vector search for documents, a graph traversal for relationships, and an API call for external data. The agent needs to understand each source's query language and capabilities. A query like "Who are the decision-makers at our top five customers?" might translate to a SQL query to identify top customers by revenue, a graph query to find contacts and their roles at those companies, and potentially an external API call to enrich contact information.

Parallel retrieval is essential for latency management. If you're querying three sources sequentially and each takes half a second, you've added 1.5 seconds to response time. Query them in parallel and you pay only the maximum latency, usually under a second. Your orchestration layer needs to issue retrieval calls concurrently, collect results as they arrive, and handle partial failures where some sources return data and others time out.

The synthesis challenge is combining information from different sources with different structures. SQL returns rows and columns. Vector search returns documents with relevance scores. Graph queries return nodes and edges. APIs return JSON with arbitrary schemas. The agent needs to understand these different formats, extract the relevant information from each, and integrate them into a coherent representation that can be used for generation.

Conflict resolution matters when sources disagree. Your internal database says a customer has fifteen employees, but LinkedIn says fifty. Your documentation says a feature was released in January, but your git history shows February. The agent needs strategies for handling conflicts: prefer more recent information, prefer more authoritative sources, flag conflicts explicitly in the generated response, or use majority voting across sources. The right strategy depends on source reliability and information type.

The ContextIQ failure from our opening story happened because the system only had one source and no awareness of source limitations. An agentic approach would have recognized that the query asked for "recent research," evaluated whether the internal knowledge base was current and comprehensive, identified potential external sources like PubMed and ClinicalTrials.gov, retrieved from those sources, and integrated external information with internal knowledge. The agent would have known what it didn't know.

## Evaluating Retrieval Quality Before Generation

The critical capability that distinguishes agentic RAG is retrieval evaluation—before generating an answer from retrieved documents, the agent assesses whether those documents are sufficient to answer the question. This prevents the classic RAG failure mode of confidently generating nonsense from irrelevant context.

Relevance scoring is the simplest evaluation. For each retrieved document, score how relevant it is to the original query. This can be done with a cross-encoder model that takes the query and document as input and outputs a relevance score, or with a prompted language model that judges relevance. Low relevance scores indicate retrieval problems—you're not finding the right information. The agent should escalate or reformulate rather than proceeding with poor context.

Coverage assessment asks whether the retrieved documents collectively contain information necessary to answer all parts of the query. If the query asks about three product features and your retrieved documents only discuss two of them, you have a coverage gap. The agent can detect this by decomposing the query into information requirements and checking which requirements are satisfied by retrieved content. Unsatisfied requirements trigger additional retrieval targeted at the gaps.

Confidence calibration uses the language model's own uncertainty about its ability to answer from the retrieved context. Before generating the full answer, ask the model "Based on these documents, how confident are you that you can answer the user's question?" and parse a confidence score. Low confidence suggests either retrieval quality issues or genuinely uncertain information, both of which the agent should surface rather than hiding behind a confident-sounding but wrong answer.

Factual grounding checks whether specific facts needed to answer the query are present in retrieved documents. For queries requiring numerical data, specific dates, or named entities, the agent can verify that these facts appear in the context. If the query asks "What was our revenue growth in Q3 2024?" and none of the retrieved documents mention Q3 2024 revenue figures, the agent knows it doesn't have the necessary information.

The decision point is whether to proceed with generation or trigger additional retrieval. You need thresholds calibrated to your application's accuracy requirements. For high-stakes applications like medical or legal research, use high thresholds—require strong relevance and high confidence before generating. For exploratory research or casual questions, use lower thresholds—generate from imperfect context and indicate uncertainty in the response. The agent adapts its caution level to the stakes.

## Adaptive Retrieval Depth

Not all queries need the same amount of information. Simple questions can be answered from one or two documents. Complex analytical questions might need dozens. Current systems either fix retrieval depth at a constant value or require users to specify it. Agentic RAG adapts retrieval depth dynamically based on query complexity and retrieval quality.

The adaptation pattern starts with an initial retrieval of a small number of documents—typically three to five. The agent evaluates these documents and decides whether they're sufficient. If relevance is high and coverage is complete, stop—you have enough information. If relevance is medium or coverage has gaps, retrieve more documents, perhaps expanding the search or reformulating the query. If relevance is low, retrieval strategy is likely wrong and you need to reformulate rather than just retrieve more.

Iterative deepening is a specific pattern where you retrieve in rounds of increasing depth. Round one retrieves the top three most relevant documents, round two retrieves the next five, round three retrieves the next ten. After each round, evaluate whether you have sufficient information. Most simple queries are answered in round one. Moderate complexity queries need round two. Only the most complex queries need round three. This pattern saves retrieval cost for simple queries while ensuring complex queries get the depth they need.

Confidence-driven retrieval uses answer confidence as the trigger for deeper retrieval. Generate a preliminary answer from initial retrieval, evaluate confidence, and retrieve more documents if confidence is below threshold. The intuition is that low confidence indicates either missing information or conflicting information, both of which benefit from additional retrieval. High confidence suggests you already have what you need. This makes retrieval depth proportional to answer uncertainty.

The cost tradeoff is between retrieval depth and latency. Each additional round of retrieval adds time. For latency-sensitive applications, you might limit to two rounds maximum. For accuracy-critical applications, you might allow deeper retrieval. The agent needs to understand the latency budget and work within it, choosing between "fast approximate answer from limited retrieval" and "slower comprehensive answer from deep retrieval" based on application requirements.

## Query Reformulation and Expansion

When initial retrieval fails to find good information, the agent needs to reformulate the query rather than just accepting poor results. Query reformulation is one of the highest-leverage techniques in agentic RAG because small changes to query phrasing can dramatically change retrieval results.

Reformulation strategies include expansion—adding synonyms, related terms, or context to the query. If "customer satisfaction metrics" retrieves poorly, try "customer satisfaction metrics user happiness scores NPS ratings." This broadens the semantic space and catches documents that use different terminology. Expansion helps recall—finding more relevant documents—but can hurt precision by introducing noise.

Abstraction moves to a higher-level concept. If "KRAS G12C inhibitor clinical trial results" retrieves poorly, try "KRAS inhibitor research" or "targeted cancer therapy trials." This helps when your specific query is too narrow or uses terminology that doesn't match your document collection. Abstraction trades specificity for coverage.

Specification adds constraints or context. If "revenue trends" retrieves too many irrelevant documents, try "revenue trends Q3 2024 North America market." This narrows the search space and improves precision. Specification helps when your query is too broad and retrieves many documents that aren't actually relevant to your specific need.

Decomposition breaks the query into sub-queries. If "compare product performance across regions" retrieves poorly as a single query, decompose to "product A performance," "product B performance," "regional sales data," and retrieve for each. This is especially effective when complex queries confuse the retrieval system or when different parts of the query require different retrieval strategies.

The agent selects reformulation strategy based on the failure mode. If retrieval returned no results, expansion or abstraction helps. If retrieval returned many irrelevant results, specification helps. If retrieval returned partial information, decomposition helps. The agent can also learn which reformulation strategies work for which query types by tracking which reformulations led to successful answers in the past.

## The Evaluation Challenge of Agentic RAG

Evaluating agentic RAG is harder than evaluating static RAG because the retrieval behavior varies by query and depends on intermediate decisions. You can't just measure "did it retrieve the right documents?" because the "right documents" depend on the query plan, source selection, and adaptive retrieval decisions the agent made. You need evaluation metrics that capture the full retrieval strategy, not just the final output.

First, evaluate query planning quality. For a test set of queries, do the generated retrieval plans make sense? Do they identify the right sources, construct reasonable sub-queries, sequence retrieval steps logically? This requires human judgment or using a strong model to evaluate the plan. You're assessing the agent's reasoning about retrieval before execution.

Second, evaluate retrieval efficiency. How many retrieval operations did the agent perform? How many were actually useful? Are you seeing unnecessary redundant retrievals, or is each retrieval adding new information? Track the ratio of unique information gained to retrieval operations performed. High ratios indicate efficient retrieval. Low ratios indicate wasted effort.

Third, evaluate adaptive decisions. When the agent decides to reformulate, retrieve deeper, or switch sources, does that decision improve results? Track the quality delta between pre-adaptation and post-adaptation. If reformulation consistently improves retrieval quality, your adaptation logic is working. If it doesn't, you're adapting without benefit.

Fourth, evaluate source coverage. For queries that require multi-source information, is the agent successfully retrieving from all necessary sources? Track source utilization patterns and correlate with answer quality. You might discover that answers lacking external source enrichment are systematically worse, indicating you need more aggressive external retrieval.

Fifth, evaluate the relationship between retrieval confidence and answer correctness. Your agent is using confidence scores to decide when to retrieve more. Are those confidence scores well-calibrated? Do low-confidence retrievals actually correlate with incorrect answers? Do high-confidence retrievals correlate with correct answers? Calibration analysis tells you if your evaluation function is trustworthy.

The end-to-end metric is still answer quality, but you need to decompose answer quality into retrieval quality and generation quality. Some wrong answers happen because retrieval failed—you didn't find the right information. Others happen because generation failed—you had the right information but misused it. Attribute failures correctly so you know whether to improve your retrieval planning or your generation prompts.

## Production Architecture for Agentic RAG

Building production agentic RAG requires infrastructure beyond what standard RAG needs. You need a query planning layer that can reason about retrieval strategy, a multi-source orchestration layer that can query heterogeneous data sources, a retrieval evaluation layer that can assess result quality, and an adaptation layer that can modify strategy based on intermediate results.

The query planning layer is typically a prompted language model with few-shot examples of good query plans. Given a user query and descriptions of available data sources, it generates a structured retrieval plan. This plan specifies what to retrieve, where to retrieve from, what order to execute retrievals, and what conditions trigger additional retrieval. The plan becomes the execution blueprint for the orchestration layer.

The orchestration layer executes the retrieval plan by dispatching queries to appropriate sources, managing parallel execution, handling timeouts and errors, and collecting results. It needs connectors for each data source type—SQL adapters for databases, API clients for external services, vector search clients for embedding stores. These connectors translate the plan's abstract retrieval requests into source-specific queries.

The evaluation layer runs after each retrieval operation to assess quality. It computes relevance scores, checks coverage, estimates confidence, and determines whether additional retrieval is needed. This layer needs to be fast—ideally under 100ms—because it's in the critical path. Using small specialized models for relevance and coverage checking keeps latency down while maintaining evaluation quality.

The adaptation layer decides what to do when evaluation indicates problems. It has strategy libraries for query reformulation, source switching, retrieval deepening, and query decomposition. Given an evaluation result—"low relevance"—it selects an appropriate adaptation strategy, updates the retrieval plan, and triggers re-execution. This layer prevents the system from getting stuck with poor retrieval.

The monitoring infrastructure needs to track the full retrieval pipeline. Log every retrieval plan, every source query, every evaluation score, every adaptation decision, and correlate these with final answer quality. This gives you visibility into where the system succeeds and fails. You might discover that SQL queries consistently fail due to schema mismatches, or that external API calls succeed but their information is rarely relevant, or that query reformulation helps for some query types but hurts for others.

## The Boundary Between Agent Orchestration and RAG Internals

This chapter covers agentic RAG from the orchestration perspective—how agents decide when and how to retrieve. Section 7 of this book covers RAG pipeline internals—embedding models, chunking strategies, indexing, reranking, all the machinery that executes once you've decided what to retrieve. The boundary between these topics is that this chapter treats retrieval as a tool the agent can invoke, while Section 7 treats retrieval as an internal system to optimize.

From the agent perspective, retrieval is a tool with inputs—a query string, a source identifier, optional parameters like top-k and filters—and outputs—a list of documents with relevance scores. The agent reasons about when to invoke this tool, what parameters to pass, and whether the outputs are satisfactory. It doesn't care about embedding models or chunk sizes—those are implementation details of the retrieval tool.

From the RAG pipeline perspective, the focus is on making each individual retrieval operation as effective as possible given the query. What chunking strategy preserves semantic coherence? What embedding model captures domain terminology? What reranking model improves top-k precision? These optimizations happen within the retrieval tool implementation, invisible to the orchestration layer.

The separation matters because you can improve them independently. You can make your agent smarter about retrieval planning without changing your embedding model. You can upgrade your embedding model without changing your agent's orchestration logic. The clean interface between them—structured queries in, ranked documents out—enables modular improvement.

But the separation isn't absolute. Agent orchestration and pipeline internals inform each other. If your agent frequently reformulates queries due to poor initial retrieval, maybe your embedding model isn't capturing query semantics well. If your pipeline consistently retrieves documents with high embedding similarity but low relevance, maybe your agent needs better query planning to add specificity. The metrics from one layer inform optimization of the other.

## Agentic RAG as Competitive Advantage

The companies building the best AI products in 2026 are distinguishing themselves through agentic RAG. They're not just building better embedding models—though they do that too. They're building agents that reason about information needs, that know what they know and what they don't know, that search strategically across multiple sources, that evaluate their own retrieval quality, that adapt when initial approaches fail. This isn't just incrementally better than static RAG—it's categorically different.

The ContextIQ failure from our opening story happened because they treated RAG as a pipeline to optimize rather than as an agent capability to develop. They focused on better embeddings, better reranking, better chunking—all important, but not sufficient. What they needed was an agent that could recognize "I'm being asked about recent clinical trials, my knowledge base only goes through October 2023, I should search external sources for November onward." That level of reasoning about retrieval doesn't come from better vector similarity—it comes from treating the agent as an active participant in knowledge gathering.

You're building systems that will be judged not just on whether they can answer questions from their knowledge base, but on whether they can find answers when their knowledge base is incomplete. The agent that says "I searched my internal documents and found X, but I also searched PubMed and found Y which provides more recent data" is qualitatively more useful than the agent that says "based on my knowledge base, the answer is X" and doesn't know what it missed. This self-awareness of knowledge boundaries is the hallmark of agentic RAG.

The technical investment is significant. You're building query planning, multi-source orchestration, retrieval evaluation, adaptive strategy selection, all layered on top of already complex RAG pipelines. But the payoff is agents that can handle the messy, incomplete, multi-source reality of enterprise knowledge. That's not a nice-to-have—it's table stakes for production AI systems that people actually trust with important questions.
