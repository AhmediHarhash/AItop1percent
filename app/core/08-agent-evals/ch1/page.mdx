# Section 8 — Agent Evaluation Systems

## Chapter 1

### Plain English

Agent evaluation answers this question:

**"Did the AI behave correctly over time while acting in the world?"**

Not just:
- "Was the answer correct?"

But:
- Did it plan well?
- Did it choose the right actions?
- Did it recover from mistakes?
- Did it stop when it should?
- Did it avoid harmful side effects?

Agents are **systems**, not messages.
So they must be evaluated as **systems**, not text.

---

### Why Agent Evaluation Is Different (and Hard)

Traditional evals assume:
- one input
- one output
- one judgment

Agents break all of that.

Agents:
- reason across multiple steps
- call tools
- maintain state
- adapt to feedback
- act in the world
- sometimes fail safely (by design)

Evaluating only the final output is **misleading**.

Elite teams evaluate:
- the journey
- the decisions
- the outcomes
- the failures

---

### What an "Agent" Means in 2026

An agent is a system that:
- receives a goal
- plans actions
- executes tools
- observes results
- updates state
- decides when to continue or stop

Agents may:
- loop
- branch
- retry
- escalate
- abandon goals safely

Agents are **decision engines**, not chatbots.

---

### Core Agent Quality Dimensions

Agent evaluation focuses on **behavioral dimensions**, not just text quality.

Key dimensions include:

- Goal completion
- Action correctness
- Planning quality
- Tool usage quality
- Recovery behavior
- Safety & constraint adherence
- Efficiency (steps, cost, time)
- Stability (no infinite loops)
- Termination correctness

Each dimension captures a different failure mode.

---

### 1) Goal Completion

**Did the agent accomplish the intended goal?**

Important nuance:
- partial success vs full success
- acceptable failure vs unacceptable failure

Example:
- booking a meeting but with wrong date = failure
- failing gracefully with explanation = acceptable failure

Goal completion is **binary or categorical**, not a score.

---

### 2) Action Correctness

**Were the actions taken appropriate?**

Evaluates:
- correct tool selection
- correct parameters
- correct order of actions

An agent can succeed accidentally.
You still count incorrect actions as failures.

This protects against brittle systems.

---

### 3) Planning Quality

**Did the agent reason sensibly before acting?**

Signals include:
- reasonable decomposition
- avoidance of unnecessary steps
- awareness of constraints
- adaptation when plan fails

Planning quality is evaluated by:
- step trace analysis
- comparison to expected plans
- human or LLM-judge review

Bad planning predicts future failure.

---

### 4) Tool Usage Quality

**How well does the agent use tools?**

Includes:
- choosing the right tool
- passing valid arguments
- interpreting tool outputs correctly
- retrying intelligently
- not abusing tools

Tool misuse is one of the most common agent failure modes in 2026.

---

### 5) Recovery and Error Handling

**What happens when something goes wrong?**

Elite agents:
- detect failure
- adjust strategy
- retry safely
- escalate when needed
- stop when stuck

Bad agents:
- loop endlessly
- repeat the same mistake
- hallucinate success

Recovery behavior is a **core differentiator**.

---

### 6) Safety and Constraint Adherence

**Did the agent stay within bounds?**

Includes:
- permission constraints
- data access rules
- business logic constraints
- safety policies

An agent that "gets results" by breaking rules is a failure.

Safety violations are **hard stops**, not averages.

---

### 7) Efficiency Metrics

**How expensive was success?**

Measured via:
- number of steps
- number of tool calls
- latency
- token cost

Two agents that both succeed are not equal.

Efficiency matters at scale.

---

### 8) Stability and Loop Detection

**Did the agent avoid pathological behavior?**

Includes:
- infinite loops
- oscillation between states
- repeated failed actions

Stability metrics often include:
- max step count
- repeated action detection
- timeout enforcement

This protects infrastructure and cost.

---

### 9) Termination Correctness

**Did the agent stop at the right time?**

Failures include:
- stopping too early
- continuing after goal completion
- hallucinating completion
- failing to escalate

Termination is a decision, not an afterthought.

---

### Evaluation Units for Agents

You do NOT evaluate agents per message.

You evaluate:
- full episodes
- action traces
- decision graphs
- state transitions

Each evaluation unit represents **one attempted task**.

---

### Types of Agent Evaluations

#### 1) Scenario-Based Evals

Predefined scenarios with:
- initial state
- tools available
- success criteria

Used for:
- regression testing
- release gates

---

#### 2) Trace-Based Evals

Evaluating:
- the sequence of actions
- intermediate decisions
- recovery behavior

Often reviewed by:
- humans
- LLM judges
- hybrid systems

---

#### 3) Outcome-Based Evals

Evaluating:
- final state
- side effects
- user-visible result

Useful but insufficient alone.

---

#### 4) Adversarial Agent Evals

Designed to:
- break planning
- trigger loops
- exploit edge cases
- test safety boundaries

Critical before enterprise release.

---

### Automating Agent Evaluation (Carefully)

Automation can detect:
- step count anomalies
- tool misuse
- loops
- invalid states
- policy violations

Automation struggles with:
- intent alignment
- subtle planning quality
- business appropriateness

Best practice:
- automate detection
- sample episodes for human review

---

### Agent Evals in Production

In live systems, you monitor:
- success rate
- recovery rate
- escalation rate
- loop incidents
- cost per task

Agent evals continue **after launch**.

---

### Enterprise Expectations

Enterprises require:
- predictable agent behavior
- auditability of decisions
- explainable failures
- safe defaults

"Autonomous" does not mean "unaccountable."

---

### Founder Perspective

For founders, agent evals:
- prevent silent disasters
- protect trust
- enable safe autonomy
- reduce support burden

Most failed agent startups failed on evaluation, not modeling.

---

### Common Failure Modes

- evaluating only final output
- ignoring action traces
- allowing agents to self-certify success
- optimizing for speed over safety
- skipping adversarial testing

These failures scale catastrophically.

---

### Interview-Grade Talking Points

You should be able to explain:

- why agent evals focus on behavior
- how goal completion differs from correctness
- how recovery is evaluated
- why efficiency matters
- how agent evals differ from chat evals

This is **Principal-level knowledge**.

---

### Completion Checklist

You are done with this section when you can:

- design an eval for any agent workflow
- explain how to detect loops and failures
- define success vs acceptable failure
- explain how agents are monitored post-launch
- explain tradeoffs between autonomy and safety

If this is clear, you're ahead of most teams in 2026.

---

### What Comes Next

Now that agents are covered, the next challenge is:

**How do we evaluate systems that retrieve and reason over external knowledge?**

That is Section 09 — RAG Evaluation Systems.
