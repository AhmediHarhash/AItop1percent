# 6.12 â€” Memory Frameworks: Mem0, Letta, LangGraph Store, and Bedrock Memory

In 2026, building agent memory infrastructure from scratch is professional negligence unless you have requirements that no framework can meet. In August 2025, a healthcare startup spent six weeks building a custom memory system, then watched a competitor launch the same functionality in forty-eight hours using Mem0, a managed memory layer that provided all the same features out of the box. The startup's CEO asked why they had spent six weeks building something that already existed as a service. The CTO had no good answer. By January 2026, the startup had ripped out their custom memory system, migrated to Letta, and moved two engineers to higher-value projects.

Memory frameworks have matured rapidly. Two years ago, if you wanted agent memory, you built it yourself. You chose a vector database, wrote extraction logic, implemented retrieval, and maintained everything. Today, you have multiple production-ready frameworks that handle the entire memory lifecycle. They extract memories from conversations. They store them in managed databases. They retrieve them based on semantic similarity and metadata filters. They handle versioning, expiration, and conflict resolution. They provide APIs for reading and writing memory programmatically. They integrate with major agent frameworks. The question is no longer whether to use a memory framework. The question is which one.

## Mem0: Managed Simplicity

Mem0 is the simplest option. It is a managed memory layer that sits between your agent and your application. You send conversation transcripts to Mem0. It extracts facts, entities, and relationships automatically. It stores them in a managed vector database. When your agent needs context, you query Mem0 with the user ID and the current conversation, and it returns relevant memories. You do not configure extraction rules. You do not tune retrieval parameters. You do not manage infrastructure. Mem0 handles it all. The trade-off is control. You cannot customize how memories are extracted or how retrieval is ranked. You get a black box that works well for common use cases but may not fit specialized requirements.

Mem0 is best for teams that want to add memory to an existing agent with minimal engineering effort. A customer support agent, a sales assistant, a personal productivity agent. These are use cases where memory requirements are standard. You need to remember user preferences, conversation history, and domain facts. You do not need exotic retrieval strategies or custom memory schemas. Mem0 gives you memory in a few hours. You integrate their SDK, send them conversation data, and start querying for memories. Pricing is based on memory storage and retrieval volume. You pay per gigabyte stored and per thousand retrievals. For most applications, the cost is negligible compared to model inference costs.

The integration pattern for Mem0 is straightforward. After each conversation turn, you send the conversation history to Mem0's API. Mem0 extracts relevant information and stores it. At the start of the next conversation, you query Mem0 for memories related to the user. Mem0 returns a list of facts, which you include in your agent's context. The agent now has access to information from previous conversations. The entire integration is typically fifty lines of code. You import the SDK, configure your API key, call the store method after conversations, and call the retrieve method before generating responses.

The main limitation of Mem0 is that it is optimized for conversational memory. It works well for agents that interact with users over multiple sessions and need to remember user-specific information. It works less well for agents that need to remember complex domain knowledge or maintain structured state. For example, a coding agent that needs to remember the structure of a codebase, the dependencies between files, and the history of changes. Mem0 can store this information as unstructured memories, but retrieval becomes inefficient as the memory grows. You end up retrieving dozens of memories for every query because the codebase structure is fragmented across many memory entries. For these use cases, you need a framework that supports structured memory.

Mem0's automatic extraction uses a language model to identify facts worth remembering from conversations. The extraction model is trained to recognize common patterns like user preferences, stated facts, important decisions, and action items. It works well for general conversations but may miss domain-specific information. If your agent operates in a specialized domain like legal document analysis or medical diagnosis, Mem0's extraction may not capture the right details. You cannot train or configure the extraction model. You get what Mem0 provides. For most applications, this is sufficient. For specialized domains, it is a limitation.

Retrieval in Mem0 is based on semantic similarity between the current conversation context and stored memories. When you query Mem0, it embeds the current conversation, searches for semantically similar memories, and returns the top results. The similarity threshold and ranking algorithm are managed by Mem0. You can filter by user ID, session ID, or custom metadata tags, but you cannot control the core retrieval logic. This simplicity is Mem0's strength for standard use cases. It is also a weakness when you need custom retrieval behavior, such as preferring recent memories over older ones or weighting certain types of memories more heavily.

## Letta: Self-Editing Tiered Memory

Letta, formerly MemGPT, is a framework for self-editing memory with tiered storage. The core idea is that the agent maintains a working memory in context and a long-term memory in a database. The working memory is small, typically a few thousand tokens, and contains the information the agent is actively using. The long-term memory is large, potentially gigabytes, and contains everything the agent has learned over time. The agent decides when to move information from working memory to long-term memory and when to load information from long-term memory back into working memory. This gives the agent control over its own memory management.

Letta agents edit their memory by generating special commands in their output. For example, the agent might output a command to store a new memory, retrieve a memory by keyword, or delete an outdated memory. The Letta runtime intercepts these commands, executes them, and updates the agent's memory accordingly. The agent can also request a summary of its long-term memory or search for specific information. This makes memory management explicit and debuggable. You can see exactly when the agent decided to store or retrieve a memory and why.

The advantage of Letta is flexibility. The agent can implement custom memory strategies based on the task. A research agent might store detailed notes in long-term memory and periodically summarize them to keep working memory focused. A coding agent might store function signatures in long-term memory and load them into working memory only when needed. A customer service agent might store customer history in long-term memory and load it at the start of each conversation. The agent adapts its memory strategy to the context.

The disadvantage of Letta is complexity. You have to train the agent to manage memory effectively. If the agent stores too much, it wastes storage and slows down retrieval. If it stores too little, it forgets important information. If it retrieves too aggressively, it clutters working memory. If it retrieves too conservatively, it misses relevant context. Finding the right balance requires experimentation and tuning. You also have to handle cases where the agent issues malformed memory commands or tries to retrieve memories that do not exist. Letta provides guardrails, but you still need robust error handling.

Letta is best for teams building agents with complex memory requirements that cannot be handled by automatic extraction. A legal research agent that needs to remember case law, statutes, and legal arguments. A scientific research agent that needs to remember experimental results, hypotheses, and literature references. A multi-step planning agent that needs to remember plans, progress, and outcomes. These agents benefit from explicit control over memory. They need to decide what to remember, when to remember it, and how to organize it. Letta gives them that control.

The engineering investment for Letta is significantly higher than Mem0. You need to design the memory command syntax, train the agent to use it correctly, implement error handling for malformed commands, and build tooling to inspect and debug memory operations. You also need to operate your own vector database for long-term memory storage. Letta does not provide managed infrastructure. It is a framework you deploy on your own infrastructure. This gives you control but also operational responsibility. You manage backups, scaling, monitoring, and incident response for the memory layer.

Prompt engineering for Letta agents requires teaching the agent when and how to use memory commands. You provide examples of conversations where the agent correctly stores important information and retrieves it when needed. You show the agent how to summarize long-term memory to avoid cluttering working memory. You teach the agent to delete outdated or incorrect memories. This prompt engineering is iterative. You test the agent, observe memory management failures, update the prompts to address those failures, and repeat. Over time, the agent learns effective memory strategies, but the initial tuning period is substantial.

## LangGraph Store: Structured State Persistence

LangGraph Store is the memory solution for LangGraph agents. LangGraph is a framework for building agents as stateful graphs, where nodes represent actions and edges represent transitions. LangGraph Store provides persistent state for these graphs. You define a state schema, and LangGraph Store persists it across agent executions. When the agent runs, it loads state from the store, executes actions, updates state, and persists it back. This is not semantic memory. It is structured state management. You do not embed memories or retrieve by similarity. You define state fields and access them by key.

LangGraph Store is best for agents that need to maintain structured state over time. A workflow automation agent that tracks the status of tasks, the results of previous steps, and the next actions to take. A data processing agent that maintains a queue of items to process, the processing status of each item, and error logs. A multi-agent system where each agent maintains its own state and shares state with other agents. LangGraph Store handles persistence, versioning, and concurrent access. You focus on defining the state schema and the logic for updating it.

The state schema in LangGraph Store is defined using Python data classes or TypeScript interfaces, depending on your language. You specify the fields, their types, and any validation rules. LangGraph Store serializes the state to a backing store, which can be a relational database, a key-value store, or a cloud storage service. When the agent needs to access state, LangGraph Store deserializes it and provides it to the agent. When the agent updates state, LangGraph Store persists the changes. This provides a simple programming model where state looks like ordinary variables but is automatically persisted.

The limitation of LangGraph Store is that it does not provide semantic retrieval. You can store arbitrary data, but you retrieve it by key, not by similarity. If you need to retrieve information based on semantic similarity to a query, you have to integrate a separate vector database. Some teams use LangGraph Store for structured state and Mem0 or Letta for semantic memory, combining both in the same agent. This works but adds complexity. You have to manage two memory systems and decide which information goes into which system.

Versioning in LangGraph Store allows you to track how state changes over time. Each state update creates a new version. You can query historical versions to understand how the agent's knowledge evolved or to debug issues. If the agent makes a bad decision based on corrupted state, you can inspect previous versions to identify when the corruption occurred. Versioning also enables rollback. If a state update causes problems, you can revert to a previous version. This is particularly useful for agents that modify their own state in complex ways where bugs can corrupt the state irreparably.

Concurrency control in LangGraph Store handles cases where multiple agents or processes try to update the same state simultaneously. Without concurrency control, updates can conflict and cause data loss or corruption. LangGraph Store provides optimistic concurrency control where each update includes a version number. If two updates conflict, one succeeds and the other is rejected with a conflict error. The rejected update can retry with the new state. This ensures consistency without locking, which improves performance for concurrent agents.

## Amazon Bedrock Memory: Zero-Integration Convenience

Amazon Bedrock Memory is AWS's managed memory service for agents built on Bedrock. It integrates tightly with Bedrock's agent runtime. You enable memory for an agent, and Bedrock automatically extracts and stores memories from conversations. Memories are stored in a managed vector database. Retrieval is automatic. When the agent responds to a user query, Bedrock retrieves relevant memories and includes them in the prompt. You do not write any memory management code. It is entirely handled by the Bedrock runtime.

Bedrock Memory is the easiest option if you are already using Bedrock for agent inference. It requires zero integration effort. You flip a switch in the Bedrock console, and your agent has memory. The trade-off is vendor lock-in. Your memories are stored in Bedrock's managed database. You cannot export them to another system without writing custom migration code. You also cannot customize extraction or retrieval. Bedrock decides what to remember and when to retrieve it. For teams that prioritize ease of use over control, this is acceptable. For teams that need fine-grained control or multi-cloud portability, it is a dealbreaker.

Bedrock Memory pricing is based on storage and retrieval, similar to Mem0. You pay per gigabyte of memory stored and per thousand retrievals. AWS offers volume discounts for large-scale deployments. The cost is typically lower than building and operating your own vector database, especially when you factor in engineering time and infrastructure overhead. However, retrieval latency can be higher than self-hosted solutions because memories are retrieved over the network from AWS's managed database. For latency-sensitive applications, this can be a problem.

The automatic extraction in Bedrock Memory is similar to Mem0's approach. Bedrock uses a language model to identify important facts from conversations and stores them as memories. The extraction model is part of Bedrock's infrastructure and is not customizable. It works well for general-purpose agents but may not capture domain-specific information effectively. If your agent operates in a specialized domain, Bedrock Memory's extraction may miss critical details. You cannot configure the extraction logic or train it on domain-specific examples.

Integration with other AWS services is Bedrock Memory's strength. If your agent uses other AWS services like DynamoDB for user data or S3 for document storage, Bedrock Memory integrates seamlessly. Memories can reference data stored in other AWS services, and access control policies can be unified across services using IAM. This tight integration reduces operational complexity for teams already invested in the AWS ecosystem. For multi-cloud or cloud-agnostic architectures, the tight coupling to AWS is a disadvantage.

## Choosing the Right Framework

Comparing these frameworks requires understanding your requirements. If you need simplicity and fast integration, use Mem0 or Bedrock Memory. If you need control over memory management, use Letta. If you need structured state persistence, use LangGraph Store. If you need high customization and do not mind managing infrastructure, build on top of a vector database like Pinecone, Weaviate, or Qdrant. There is no universal best choice. The right framework depends on your use case, your team's expertise, and your tolerance for operational complexity.

One dimension to consider is managed versus self-hosted. Managed frameworks like Mem0 and Bedrock Memory handle infrastructure for you. You do not provision databases or manage backups. You do not worry about scaling or availability. The framework guarantees uptime and performance. The downside is cost and control. You pay for the convenience, and you cannot optimize the infrastructure for your specific workload. Self-hosted frameworks like Letta and LangGraph Store give you full control. You deploy them on your own infrastructure. You optimize for your workload. You pay only for compute and storage, not for a managed service. The downside is operational overhead. You manage backups, scaling, monitoring, and incident response.

Another dimension is feature set. Mem0 provides automatic extraction and retrieval but no support for structured state or multi-tier memory. Letta provides self-editing memory and tiered storage but requires explicit memory commands. LangGraph Store provides structured state persistence but no semantic retrieval. Bedrock Memory provides automatic extraction and retrieval but only within the Bedrock ecosystem. No framework does everything. You either choose the framework that best matches your primary requirement or combine multiple frameworks to cover all requirements.

Pricing varies significantly across frameworks. Mem0 charges based on storage and retrieval volume. A typical deployment with one million memories and one hundred thousand retrievals per month costs around two hundred dollars. Letta is open source and self-hosted, so you pay only for infrastructure. A deployment on AWS with a managed vector database and compute for the agent runtime costs around five hundred dollars per month for moderate scale. LangGraph Store is part of LangChain's commercial offering, with pricing based on state storage and transaction volume. A typical deployment costs around three hundred dollars per month. Bedrock Memory pricing is bundled with Bedrock agent usage, typically adding ten to twenty percent to your total Bedrock bill.

Migration strategies between frameworks are important to consider upfront. Memory is stateful. Once you store millions of memories in a framework, migrating to a different framework is expensive. You have to export memories from the old system, transform them to the new schema, and import them into the new system. Embeddings may not be compatible, so you have to recompute them. Metadata schemas may differ, so you have to map old metadata to new metadata. Retrieval behavior may change, so you have to revalidate that the agent retrieves the right memories. Some teams budget a full sprint for memory migration. Others avoid migration entirely by choosing the right framework upfront.

## Evaluation and Selection Process

To evaluate frameworks for your use case, start with a prototype. Build a minimal agent that uses memory for a representative task. Integrate one framework and test it. Measure extraction quality, retrieval precision, latency, and cost. Then integrate a second framework and repeat the measurements. Compare the results. The framework that performs best on your specific workload is the one you should use. Do not rely on benchmarks or marketing claims. Different frameworks excel at different workloads. The only way to know which is best for you is to test them on your data.

One team I worked with evaluated Mem0, Letta, and a custom solution built on Pinecone for a financial advisory agent. They tested all three on the same set of conversations and measured retrieval precision. Mem0 had the highest precision for recent conversations but struggled with historical data older than six months. Letta had lower precision overall but allowed the agent to explicitly retrieve historical data when needed. The custom Pinecone solution had the best precision but required the most engineering effort. They chose Letta because the ability to explicitly control retrieval outweighed the lower automatic precision. Six months later, they had tuned the agent's memory commands to the point where Letta matched Mem0's precision while retaining explicit control.

Evaluation metrics should cover multiple dimensions. Retrieval precision measures what fraction of retrieved memories are relevant to the current context. Retrieval recall measures what fraction of relevant memories are retrieved. Extraction coverage measures what fraction of important information from conversations is stored. Extraction noise measures what fraction of stored memories are actually useful. Latency measures how long retrieval and storage operations take. Cost measures the total expense of memory operations including storage, compute, and API calls. You need to measure all these dimensions to make an informed decision.

Long-term maintenance costs are often overlooked in framework selection. A managed service like Mem0 or Bedrock Memory has low ongoing maintenance because the vendor handles infrastructure, scaling, and updates. A self-hosted framework like Letta requires continuous operational effort. You monitor the vector database for performance issues. You apply updates and patches. You scale capacity as memory volume grows. You debug infrastructure problems when they occur. Over multiple years, the operational cost can exceed the initial development cost. Factor this into your decision if you are choosing between managed and self-hosted.

## When to Build Custom

When to build custom versus use a framework is a question of marginal value. Building custom gives you maximum control and optimization potential. Using a framework gives you fast time to value and lower maintenance burden. The crossover point depends on your scale and your requirements. If you have fewer than one million memories and standard retrieval needs, use a framework. The engineering time saved outweighs any performance gains from custom optimization. If you have more than ten million memories or exotic retrieval requirements, consider building custom. At that scale, even small optimizations in retrieval latency or storage efficiency translate to significant cost savings.

However, most teams overestimate their need for customization. They assume their requirements are unique when they are actually common. A customer service agent needs to remember customer information, conversation history, and product details. This is not unique. Thousands of customer service agents have the same requirements. A framework designed for this use case will work well. The team that built custom memory for their healthcare intake agent learned this the hard way. They thought healthcare was too specialized for a generic framework. They were wrong. Mem0 worked just as well and saved them five weeks of engineering time.

The healthcare startup's migration from custom to Letta took two weeks. They exported their custom memory database to JSON, wrote a script to convert each memory to Letta's format, and imported the data. They retrained embeddings using Letta's embedding model. They updated their agent code to issue Letta memory commands instead of direct database queries. They ran tests to verify that retrieval behavior was consistent. The migration was painful but successful. Afterward, their memory system required one-tenth the maintenance effort. They no longer had to patch vector database bugs or optimize query performance. Letta handled it. The CTO told me the migration was one of the best decisions they made. They had learned that infrastructure is not a competitive advantage. Building features that users care about is the competitive advantage. Memory infrastructure is a commodity. Treat it like one.

Customization should be reserved for cases where frameworks genuinely cannot meet your needs. Examples include highly specialized retrieval algorithms like graph-based memory traversal, extreme-scale deployments with billions of memories requiring custom sharding strategies, strict data residency requirements that prevent using managed services, or integration with proprietary systems that frameworks do not support. For these cases, building custom is justified. For everything else, use a framework.

## Hybrid Approaches and Integration Patterns

Some teams use multiple memory frameworks in the same agent, combining their strengths. A common pattern is to use LangGraph Store for structured state like task status and workflow progress, and Mem0 for conversational memory like user preferences and conversation history. The agent queries both systems and combines the results. Structured state provides precise, reliable information for task execution. Conversational memory provides context and personalization. Together, they give the agent a complete knowledge base.

The integration complexity of hybrid approaches should not be underestimated. You have to manage two separate systems with different APIs, data models, and operational characteristics. You have to decide which information goes into which system, a decision that is not always clear-cut. You have to handle consistency when information exists in both systems. You have to debug problems that span both systems. The operational burden doubles. Use hybrid approaches only when a single framework genuinely cannot meet your needs.

Another pattern is to use a framework for production memory and a custom solution for specialized retrieval. For example, you might use Mem0 to store memories but build a custom retrieval layer that combines semantic similarity with business logic specific to your domain. You query Mem0 for candidate memories, then apply custom ranking and filtering. This gives you the convenience of managed storage with the flexibility of custom retrieval. The trade-off is added complexity in the retrieval logic.

## The Build-vs-Buy Decision Framework

In 2026, you have mature memory frameworks that handle the hard parts of agent memory. Use them. Do not build memory infrastructure from scratch unless you have proven that no framework meets your needs. Spend your engineering time on the parts of your agent that differentiate you in the market, not on reinventing memory management. The teams that win are the ones that ship features fast, and you cannot ship fast if you are building infrastructure that already exists.

The build-vs-buy decision should follow a clear process. First, document your memory requirements in detail. What data needs to be remembered? How will it be retrieved? What scale do you expect? What latency requirements do you have? What compliance constraints apply? Second, evaluate available frameworks against these requirements. Can any framework meet all your needs? Can a framework meet most needs with minor customization? Third, estimate the cost of building custom. How many engineer-months to build? How many engineers to operate? What is the opportunity cost of not building features instead? Fourth, compare the total cost of ownership over three years for build versus buy. Include development, operation, and opportunity cost. The option with lower total cost wins.

Most teams who go through this process rigorously choose to buy rather than build. The engineering effort to build a production-grade memory system is substantial. You need extraction logic, embedding generation, vector database integration, retrieval ranking, metadata management, versioning, access control, monitoring, and debugging tools. This is months of work for a small team. Frameworks provide all this out of the box. The cost of a managed framework is usually less than the salary of one engineer, and you get a system built and maintained by a team of specialists.

The exceptions are teams with truly unique requirements or extreme scale. If you are building an agent that will store billions of memories and serve millions of queries per second, you need custom infrastructure optimized for that scale. If you are building an agent with specialized retrieval requirements that no framework supports, you need custom logic. If you operate in a regulatory environment that prohibits using third-party services, you need to self-host everything. For these cases, build. For everything else, buy.

The final consideration is team expertise. If your team has deep expertise in vector databases, embedding models, and distributed systems, building custom may be feasible. If your team's expertise is in product development, domain modeling, and user experience, use a framework. Play to your strengths. Building memory infrastructure when you lack the expertise is a recipe for low-quality systems and operational failures. Use a framework built by experts, and focus your expertise on the parts of the agent that create user value.

In 2026, memory frameworks have reached maturity. They are production-ready, well-documented, and widely adopted. The risk of using a framework is low. The risk of building custom is high. Choose the framework that best fits your requirements, integrate it quickly, and move on to building features that differentiate your agent. Memory is essential, but it is not a differentiator. Your unique value is in how your agent uses memory to solve user problems, not in the memory infrastructure itself. Invest your engineering effort accordingly. The next chapter explores agent observability: how to monitor, debug, and optimize agent behavior in production.
