# 10.17 — Step Budgets and Loop Caps: Max Steps, Max Retries, Max Tool Calls

In October 2025, a legal technology company deployed an agent system designed to extract structured data from case files and generate preliminary research summaries. The agent performed well in testing, typically completing tasks in three to seven steps. In production, however, the engineering team began receiving alerts about jobs that had been running for over twenty minutes, consuming thousands of API tokens per request. When they examined the logs, they discovered that a subset of requests—approximately 8% of the total—were entering pathological loops. The agent would attempt to parse a document, fail to find the expected section, decide it needed to search for alternative headings, invoke a document navigation tool, re-read the same content, fail again, and repeat. One request logged 147 tool calls before the operations team manually killed the process. The incident cost the company $3,400 in API charges in a single afternoon and delayed critical deliverables for three clients. The root cause was not a bug in the agent's logic but the absence of circuit breakers: the system had no maximum step count, no retry cap, and no tool call limit. The agent was free to loop indefinitely until external intervention.

This failure illustrates a fundamental principle of agent deployment: autonomy without bounds is a liability. Agents, by design, make decisions dynamically based on intermediate results. This flexibility is their strength, but it also means they can enter runaway states that single-turn models cannot. A completion model generates one output and stops. An agent generates an output, evaluates it, decides on next actions, and continues. If that decision logic encounters an edge case—ambiguous instructions, malformed inputs, tool errors that look recoverable—the agent may retry indefinitely, spiral into correction loops, or explore exponentially branching paths. Step budgets and loop caps are the control mechanisms that prevent these runaway behaviors. They define hard limits on how many actions an agent can take, how many times it can retry a failed operation, and how many tool calls it can invoke in a single session. These limits are not arbitrary restrictions; they are engineered constraints that balance autonomy with predictability, allowing agents to operate flexibly within a safety envelope. You implement step budgets not because you expect agents to hit them routinely, but because you need guaranteed termination in the worst case.

## Why Agents Need Hard Limits on Execution

Agents operate in a fundamentally different execution model than traditional software. A web service processes a request, returns a response, and terminates. An agent processes a request, generates intermediate steps, evaluates outcomes, and decides whether to continue. This iterative structure introduces two risks that do not exist in single-turn systems: infinite loops and exponential branching. Infinite loops occur when an agent's decision logic does not converge. For example, an agent tasked with verifying that a document contains a specific field might search for the field, fail to find it, decide to search again with a slightly different query, fail again, and repeat. If the field genuinely does not exist, the agent has no stopping condition beyond its own reasoning, which may not recognize futility. Exponential branching occurs when an agent explores multiple paths in parallel or sequentially. An agent tasked with researching a technical question might identify five relevant sources, decide to read each, extract subquestions from each source, and recursively research those subquestions. Without limits, this process can expand from five sources to twenty-five to over a hundred, consuming unbounded compute and API budget.

Hard limits on execution prevent these failure modes by enforcing guaranteed termination. A maximum step count ensures that no agent session exceeds a defined number of reasoning-action cycles, regardless of task complexity. A maximum retry count ensures that transient failures do not become infinite retry loops. A maximum tool call count ensures that an agent cannot invoke hundreds of API calls in a single session, protecting both cost budgets and downstream service rate limits. These limits act as circuit breakers. When an agent hits a limit, the system halts execution, logs the state, and returns a failure signal or partial result to the caller. This controlled failure is vastly preferable to unbounded execution, which can exhaust API quotas, delay time-sensitive tasks, or—in the worst case—incur five-figure cloud bills before anyone notices.

The key insight is that these limits are not penalties for bad agent design; they are safety margins for good agent design operating in unpredictable conditions. Even well-engineered agents encounter edge cases: users provide malformed inputs, external APIs return unexpected errors, documents have unusual formats. In these cases, the agent's reasoning may be sound but inconclusive, leading it to retry or explore alternatives. Without budgets, sound reasoning in an unsolvable case becomes an infinite loop. With budgets, the agent attempts reasonable recovery, hits the limit, and escalates to a human or fallback handler. This is the correct behavior. You are designing for graceful degradation, not unbounded optimism.

## The Three-Layer Budget Framework

Step budgets and loop caps are not a single parameter; they are a layered control framework that addresses different failure modes at different granularities. The first layer is the **maximum step count**, which limits the total number of reasoning-action cycles an agent can execute in a single session. A step is defined as one iteration of the agent loop: the model generates a reasoning trace, selects an action, invokes a tool or produces output, and evaluates the result. In a well-behaved session, an agent might complete a task in four steps: search for information, read a document, extract key points, and format the output. A maximum step count of twenty allows the agent to retry, explore alternatives, and recover from minor errors, while guaranteeing that even the most pathological case terminates within twenty cycles. When the agent reaches step twenty, the orchestrator halts execution, logs the session state, and returns a failure code or the best available partial result.

The second layer is the **maximum retry count per operation**, which limits how many times an agent can retry a specific failed action. Retries are necessary because external systems are unreliable: APIs time out, rate limits trigger, network errors occur. An agent should retry a failed tool call once or twice before concluding that the operation is genuinely unavailable. However, retrying the same operation ten times is almost never productive. Either the failure is persistent—the API is down, the input is malformed—or the agent is misinterpreting the error. A maximum retry count of two or three per operation prevents agents from burning steps on futile retries while still allowing recovery from transient failures. This limit is enforced per tool call, not per session. If an agent invokes a search tool, fails, retries twice, and succeeds, it has used three of its step budget but only two retries for that specific operation. If it later invokes a different tool and that tool fails, the retry count resets.

The third layer is the **maximum tool call count per session**, which limits the total number of external tool invocations an agent can make, regardless of how many steps it takes. Tool calls are expensive: they consume API quota, incur latency, and—if the tool is an external service—may have usage-based pricing. An agent that invokes a search API fifty times in a single session may technically stay within a twenty-step budget if each step involves multiple parallel tool calls, but it still represents runaway behavior. A maximum tool call count of thirty or forty provides a hard budget that prevents cost overruns even if the step logic is misconfigured. This limit is particularly important for agents that use tools with per-call costs, such as web search APIs, database queries, or third-party data enrichment services. One agent session should not consume a day's worth of API quota.

These three layers work together to create a comprehensive safety envelope. The step count prevents infinite loops. The retry count prevents futile repeated attempts. The tool call count prevents cost explosions. You configure all three, and the orchestrator enforces them at runtime. When any limit is reached, the session terminates immediately, and the system logs which limit was hit, how many steps were taken, and what the agent was attempting when it stopped. This logging is critical for diagnosing whether the limits are too restrictive, too permissive, or correctly calibrated.

## Calibrating Budgets from Baseline Task Distributions

Setting step budgets is not guesswork; it is an empirical process based on observed task distributions. You begin by running your agent on a representative validation set—at least 200 to 500 real-world tasks—without any limits. You log every session: how many steps each task took, how many tool calls, how many retries, and whether the task succeeded or failed. You then analyze the distribution. For most well-scoped tasks, the distribution is heavily left-skewed: 70% to 85% of tasks complete in under six steps, 90% to 95% complete in under ten steps, and a long tail of outliers extends to fifteen, twenty, or more steps. The outliers fall into two categories: genuinely complex tasks that require extensive exploration, and pathological cases that should have failed faster. Your budgets should accommodate the former and contain the latter.

A common calibration approach is to set the maximum step count at the 95th or 98th percentile of your baseline distribution, plus a margin. If 95% of tasks complete in nine steps, you set the maximum at fifteen or twenty. This allows normal variance—tasks that take an extra retry, encounter one more ambiguity, or need one more clarification—while still capping runaway cases. The margin accounts for production variance: users in production provide messier inputs, edge cases appear that were not in your validation set, and external APIs behave differently under load. You do not set the maximum at the median, because that would fail half of all normal tasks. You do not set it at the maximum observed value, because outliers in validation are often pathological cases that should fail faster. You set it at a percentile that captures the vast majority of legitimate task variance while excluding runaway loops.

The same logic applies to retry counts and tool call counts. If your baseline data shows that 98% of successful tasks retry a tool call zero or one time, a maximum retry count of two per operation is reasonable. If 95% of successful tasks invoke fewer than twelve tool calls, a maximum tool call count of thirty provides ample headroom. You are not optimizing for the absolute minimum budget; you are optimizing for a budget that allows normal tasks to succeed while preventing runaway tasks from consuming unbounded resources. This calibration process is iterative. You deploy with initial budgets, monitor how often limits are hit, examine the sessions that hit limits, and adjust. If 5% of production sessions hit the step limit and most are legitimate complex tasks, you raise the limit. If 2% of sessions hit the tool call limit and all are pathological loops, you keep the limit or lower it.

The critical mistake is setting budgets based on intuition rather than data. A product manager might assume that no task should ever need more than ten steps, because ten steps "feels like a lot." In practice, legitimate tasks—researching a multi-part question, verifying compliance across several documents, debugging a configuration error—often require twelve to eighteen steps. Setting the budget at ten fails these tasks unnecessarily. Conversely, an engineer might set the budget at one hundred steps to "be safe," which prevents no pathological cases and allows runaway loops to consume massive API quota before terminating. Calibration must be empirical. You measure, you set percentiles, you deploy, and you refine.

## Implementing Budget Enforcement in the Orchestrator

Step budgets and retry limits are enforced by the agent orchestrator, not by the model itself. The model has no awareness of budgets; it generates reasoning and actions based on its prompt and context. The orchestrator tracks execution state: how many steps have been taken, how many retries have occurred for each tool, how many tool calls have been invoked, and whether any budget has been exceeded. Before each iteration of the agent loop, the orchestrator checks these counters. If any limit is reached, the orchestrator halts execution and invokes a termination handler. This separation of concerns is essential. The model focuses on reasoning and action selection; the orchestrator focuses on safety and resource management.

A typical orchestrator maintains four counters: step count, tool call count, and a per-tool retry map. The step count increments after each reasoning-action cycle. The tool call count increments each time a tool is invoked, regardless of success or failure. The retry map tracks how many consecutive failures have occurred for each tool. When a tool call succeeds, its retry count resets to zero. When a tool call fails, its retry count increments. If the retry count for a specific tool reaches the maximum, the orchestrator does not allow the agent to retry that tool again in the current session. Instead, it either skips the action or returns a permanent failure signal to the model, prompting it to choose a different tool or approach.

The termination handler determines what happens when a budget is exceeded. The simplest approach is to halt execution immediately and return a failure response with metadata: which limit was hit, the current step count, and the agent's last action. This is appropriate for batch workflows where partial results are not useful. For interactive or high-value tasks, a better approach is to halt execution but return the best available partial result along with a status code indicating budget exhaustion. For example, if an agent is summarizing a long document and hits the step limit after processing 80% of the content, the system can return the partial summary with a flag that indicates incompleteness. The calling service can then decide whether to accept the partial result, retry with a higher budget, or escalate to a human.

The orchestrator also logs budget exhaustion events with full context: the task ID, the input, the sequence of steps taken, the tool calls invoked, the errors encountered, and the final state when the limit was hit. This log is invaluable for calibration. If you see many sessions hitting the step limit on the same type of task, the task may be underspecified, the agent's prompt may be poorly tuned, or the budget may be too low. If you see sessions hitting the retry limit repeatedly for one specific tool, that tool may be unreliable or misconfigured. The logs tell you whether your budgets are working as intended or need adjustment.

Implementation details matter. The orchestrator should enforce budgets before invoking the model, not after. If the agent has taken nineteen steps and the maximum is twenty, the orchestrator should check this before sending the next prompt. If the check happens after the model generates a response, you have already incurred the cost of the twentieth inference call, which defeats the purpose of the budget. Similarly, retry limits should be checked before invoking a tool. If a tool has already failed twice and the maximum retry count is two, the orchestrator should not invoke the tool a third time; it should immediately signal failure to the model. This eager enforcement prevents wasted API calls and ensures that budgets are strict guarantees, not best-effort suggestions.

## Handling Budget Exhaustion Gracefully

When an agent hits a budget limit, the system has several options, and the right choice depends on task priority and downstream dependencies. For low-priority batch tasks, the correct response is to fail fast: log the failure, return an error code, and move on. The agent attempted the task, exhausted its budget, and did not succeed. This is an expected failure mode, and the system should handle it like any other task failure. The task may be requeued with a higher budget, routed to a human, or logged for later review. The key is that the failure is clean: the agent did not consume unbounded resources, and the system knows exactly why it failed.

For high-priority or time-sensitive tasks, failing fast may not be acceptable. In these cases, the system can implement fallback strategies. One approach is **budget escalation**: if an agent hits its initial budget without completing the task, the system automatically retries with a higher budget. For example, if the default step limit is fifteen and an agent hits that limit, the system might retry the same task with a limit of thirty. This escalation is typically one-time: if the agent fails again, the system fails the task rather than escalating further. Escalation is useful for tasks where complexity is variable. Some questions are simple and take five steps; others are legitimately complex and take twenty-five. A two-tier budget allows the system to handle both without overprovisioning resources for the common case.

Another fallback strategy is **partial result extraction**. If the agent has made meaningful progress before hitting the budget, the system can extract whatever partial output is available and return it with a status flag. For example, if an agent is tasked with summarizing ten documents and completes seven before hitting the step limit, the system can return summaries for the seven completed documents along with a note that three are pending. The calling service can decide whether seven summaries are sufficient or whether the task should be retried. This approach is particularly valuable for decomposable tasks where partial completion has value. It is less useful for atomic tasks where partial results are meaningless, such as "classify this email as spam or not spam." In those cases, partial results are not an option, and the system must either succeed or fail.

A third option is **human escalation**. If an agent hits a budget limit and the task is high-value, the system can route the task to a human operator with full context: the original input, the steps the agent took, where it got stuck, and why it exhausted the budget. The human can either complete the task manually, adjust the input to make it more tractable, or determine that the task is not feasible. This escalation path is common in customer support and legal workflows, where task failure is not acceptable but unbounded agent cost is also not acceptable. The budget limit serves as a trigger for human oversight rather than a hard failure.

The key principle is that budget exhaustion should never be silent. The system must log the event, signal the failure or partial completion to the caller, and provide enough context to diagnose whether the budget was appropriate. If budget exhaustion is rare—less than 1% of tasks—the budgets are well-calibrated. If it is common—more than 5% of tasks—either the budgets are too restrictive, the agent is poorly tuned, or the tasks are underspecified. You use the logs to distinguish these cases and adjust accordingly.

## Differentiating Step Budgets by Task Tier

Not all tasks deserve the same budget. High-priority, high-value tasks can justify higher step limits and more retries because their success is critical. Low-priority or exploratory tasks should run with tighter budgets to prevent resource wastage. This leads to a tiered budget model, where different task types or priorities receive different execution envelopes. For example, a customer-facing support agent responding to a paid enterprise client might operate with a step limit of thirty and a tool call limit of fifty, allowing extensive research and verification. A batch content moderation agent processing user-generated posts might operate with a step limit of ten and a tool call limit of fifteen, prioritizing throughput over exhaustiveness.

Tiering is typically based on task metadata: priority level, user tier, expected complexity, or SLA requirements. When a task enters the system, the scheduler assigns it to a budget tier based on these attributes. The orchestrator then enforces the corresponding limits. This tiering must be transparent and auditable. If a task fails because it hit a tier-one budget, the logs should show that the task was assigned to tier one, what the limits were, and why the assignment was made. This prevents confusion when stakeholders ask why a task failed: the failure was not arbitrary; it was the result of an explicit resource allocation policy.

Tiering also enables cost control at scale. If your agent system processes 100,000 tasks per day, and 80,000 are low-priority batch tasks while 20,000 are high-priority interactive tasks, you can run the batch tasks with tight budgets and the interactive tasks with generous budgets, keeping average cost per task low while maintaining quality for high-value interactions. This is economically rational. You allocate expensive resources—model inference, tool API calls, retry attempts—where they provide the most value, and you constrain them where value is limited.

The common mistake is to use a single global budget for all tasks, either because tiering feels complex or because stakeholders resist the idea that some tasks get more resources than others. In practice, a single global budget is either too permissive for low-priority tasks or too restrictive for high-priority tasks. If you set the global step limit at twenty-five to handle the most complex interactive tasks, you are overprovisioning for batch tasks that need only eight steps, wasting resources. If you set it at ten to optimize batch efficiency, you are underprovisioning for interactive tasks, causing unnecessary failures. Tiering resolves this tension by allowing you to optimize budgets for different use cases independently. You measure baseline distributions per task tier, set percentile-based limits per tier, and enforce them at runtime.

## Monitoring Budget Utilization and Adjusting Over Time

Step budgets are not static. As your agent system evolves—new prompt versions, new tools, new task types—the distribution of steps per task will shift. A prompt change that improves reasoning efficiency might reduce median step count from seven to five. A new tool that enables richer queries might increase the 95th percentile from twelve to eighteen. You must monitor budget utilization continuously and adjust budgets as the system changes. The key metrics are budget exhaustion rate, median and percentile step counts, and the distribution of tasks that hit limits versus tasks that complete successfully.

Budget exhaustion rate is the percentage of tasks that hit a step limit, retry limit, or tool call limit. A healthy exhaustion rate is between 0.5% and 2%. Below 0.5% suggests budgets may be overly generous, allowing runaway cases to consume more resources than necessary before terminating. Above 2% suggests budgets may be too tight, failing legitimate tasks. You track this rate per task tier and per budget type. If step limit exhaustion is at 3% but tool call limit exhaustion is at 0.2%, the step limit may need adjustment while the tool call limit is fine.

Median and percentile step counts show how the task distribution is shifting over time. If the median step count drops from seven to five after a prompt update, you may be able to lower the step limit from twenty to fifteen without affecting success rate, saving cost. If the 95th percentile rises from twelve to seventeen after introducing a new complex task type, you may need to raise the step limit or introduce a higher budget tier for that task type. You track these percentiles weekly or monthly and correlate changes with prompt updates, task mix shifts, or external API reliability changes.

The distribution of tasks that hit limits is equally important. You want to examine the tasks that exhausted budgets and determine whether they were pathological loops or legitimate complex cases. If most tasks that hit the step limit were stuck in retry loops on malformed inputs, the budget is working correctly: it contained runaway cases. If most tasks that hit the step limit were well-formed, high-value tasks that simply required more reasoning, the budget is too restrictive. You examine a sample of limit-hit sessions monthly, categorize them, and use that analysis to inform budget adjustments.

Adjustment is iterative. You do not change budgets based on a single outlier session or a single week of data. You wait until you have statistically significant evidence—hundreds of sessions, multiple weeks—and then adjust incrementally. If exhaustion rate is 3.5% and most limit-hit tasks are legitimate, you raise the step limit by 20%, deploy, and monitor for two weeks. If exhaustion rate drops to 1.8% and success rate improves, the adjustment was correct. If exhaustion rate remains high, you adjust again. This is continuous calibration, not one-time configuration.

## Step Budgets as a Forcing Function for Task Decomposition

An underappreciated benefit of step budgets is that they act as a forcing function for task decomposition. If a task consistently hits the step limit, it may be too complex for a single agent session. Rather than raising the budget indefinitely, the correct response is often to decompose the task into subtasks that each fit within the budget. For example, a task that requires reading and synthesizing information from twenty documents might routinely take thirty-five steps, exceeding a twenty-step budget. Rather than raising the budget to forty, you decompose the task: one agent session per document to extract key points, followed by a final agent session to synthesize the extracted points. Each session stays within the step budget, and the overall workflow is more robust because partial failures are isolated.

This decomposition discipline improves system design. It forces you to think about task granularity, intermediate outputs, and failure isolation. A monolithic agent session that tries to do everything in one loop is fragile: if it fails at step eighteen, you lose all progress. A decomposed workflow where each agent session produces a durable intermediate artifact is resilient: if one session fails, you can retry just that session without reprocessing the entire task. Step budgets make this trade-off explicit. If raising the budget is the only way to complete a task, you are probably not decomposing effectively.

The limit also exposes poorly scoped tasks. If user requests routinely hit the step limit, it may indicate that users are asking questions that are too open-ended or vague. For example, a user might ask "research this company," which is unbounded. The agent starts searching, finds hundreds of sources, and spirals into exploratory loops. A step limit of twenty forces the agent to terminate, but the underlying issue is that the task lacks constraints. The correct response is to prompt the user for more specificity: "What aspect of the company are you interested in? Financials? Leadership? Products?" This turns an unbounded task into a bounded one that fits within the budget. Step budgets surface these scoping issues that might otherwise go unnoticed until they become cost problems.

In this way, step budgets serve a dual purpose: they are runtime safety mechanisms, and they are design feedback loops. They prevent runaway execution in production, and they signal when tasks need better decomposition, scoping, or prompt engineering. You treat budget exhaustion not just as a failure but as diagnostic information. Each time a limit is hit, you ask: was this a pathological case that should fail, or a design issue that should be fixed? The answer informs whether you adjust the budget, refine the task definition, or improve the agent's reasoning prompt.

Step budgets, retry caps, and tool call limits are not optional features for production agent systems; they are foundational controls that define the boundary between flexible autonomy and uncontrolled resource consumption. You implement them early, calibrate them from empirical data, enforce them strictly in the orchestrator, and monitor them continuously as your system evolves. These limits protect your cost budget, guarantee termination, and provide the safety margin that allows you to deploy agents confidently at scale. Without them, you are running agents in an open loop, hoping they self-regulate. With them, you have engineered the constraints that make agent autonomy viable in production.

Next, you will see how to detect when those budgets are being consumed abnormally, identifying cost spikes before they escalate into financial incidents.
