# 3.8 — Backtracking and Rollback: Recovering from Bad Planning Decisions

In November 2024, an e-commerce automation agent at a mid-sized retailer executed what looked like a perfect plan. A customer requested an exchange for a defective product, and the agent created a three-step plan: cancel the original order, issue a refund, and create a new order for the replacement item. The agent executed step one, canceling the original order in the warehouse management system. It executed step two, processing a $127 refund to the customer's credit card. Then at step three, when attempting to create the new order, the agent discovered the replacement item was out of stock. The plan was now broken—the customer had been refunded and had no product coming. The agent attempted to backtrack by un-canceling the original order, but the warehouse system did not support order un-cancellation. The original items had already been flagged for return to inventory. The agent tried to reverse the refund, but the payment processor required manual approval for refund reversals, which would take 24 to 48 hours. What should have been a simple exchange turned into a mess requiring two customer service calls, manual intervention from three different teams, and a frustrated customer who went five days without the product they needed. The agent had made irreversible decisions without checking whether later steps of the plan were feasible. It had no backtracking strategy.

Backtracking is the challenge of recovering when your agent realizes partway through executing a plan that the plan was wrong. The agent took action A, then action B, then discovered that action C is impossible or that the whole plan was based on a wrong assumption. Now it needs to undo what it has done and try a different approach. In systems without side effects—pure reasoning tasks, draft generation, data analysis—backtracking is trivial. You throw away the bad reasoning and start over. In systems with side effects—orders placed, emails sent, records deleted, charges processed—backtracking is hard. Some actions cannot be undone.

## The Backtracking Challenge in Agent Systems

Agents operate in the real world where actions have consequences. When your agent sends an email, that email exists in the recipient's inbox. When it processes a refund, money moves between accounts. When it deletes a record, that data is gone. These actions have side effects that persist beyond the agent's execution context. If the agent later realizes the action was wrong, it cannot simply rewind time and choose differently.

The core challenge is that agent planning happens in an uncertain world. Your agent makes a plan based on what it knows at planning time. But as it executes the plan, it learns new information. A system that was available is now down. An item that was in stock is now out of stock. A user who seemed to want X actually wanted Y. The plan that made sense at the start no longer makes sense three steps in. The agent needs a way to handle this realization without leaving the system in an inconsistent or broken state.

Classical planning systems avoid this problem by assuming complete information upfront. The planner knows the entire state of the world before making a plan, so the plan is guaranteed to be correct given that state. But agent systems rarely have complete information. The agent interacts with external systems, other agents, and humans—all of which can change state in ways the agent did not predict. Planning under uncertainty means accepting that some plans will turn out to be wrong mid-execution.

The severity of the backtracking challenge depends on how far the agent got into the broken plan before realizing it was broken. If the agent realizes on step one that the plan will not work, backtracking is easy—either undo step one or abandon the plan. If the agent realizes on step seven that the plan was wrong, and steps one through six all had side effects, backtracking might be impossible. The damage is already done. The agent's only option might be to mitigate the consequences rather than undo them.

Traditional software handles this with transactions—atomic units of work that either fully complete or fully roll back. If any step fails, the entire transaction is undone, leaving the system in its original state. Agents cannot always use transactions because they interact with systems that do not support them. Email APIs do not have rollback. Payment processors have refund mechanisms, not transaction rollback. Human conversations cannot be unsent. The agent operates in a non-transactional world and needs non-transactional recovery strategies.

## Rollback Strategies for Agents

The simplest rollback strategy is undo actions—taking the inverse of each completed step to restore the original state. If the agent created a record, it deletes the record. If it sent a confirmation email, it sends a cancellation email. If it charged a card, it issues a refund. Undo works well when each action has a clean inverse that restores the prior state without side effects.

The limitation of undo is that many actions do not have clean inverses. Sending an email cannot be unsent—you can send a follow-up clarifying or retracting the first email, but the recipient has already seen the first one. Deleting data cannot always be undeleted—the data might have been overwritten or the backup might not include it. Processing a refund creates a new transaction; it does not erase the original charge. These are compensating actions that mitigate the original action rather than truly undoing it.

Compensating actions are a more realistic rollback strategy. Instead of trying to undo what was done, the agent takes new actions that offset or fix the consequences. The agent sent an order confirmation email for an order that will not be fulfilled? Send a cancellation email with an apology. The agent created an invoice that should not have been created? Issue a credit memo. The agent allocated inventory that is needed elsewhere? Deallocate it and create a backorder. Compensating actions accept that the original action happened and focus on making the current state correct rather than erasing history.

Restart from checkpoint is the strategy of saving state at key decision points so the agent can return to them if a plan fails. Before executing a risky or irreversible action, the agent saves enough state to reconstruct the situation if it needs to try a different approach. If the current plan fails, the agent loads the checkpoint and tries an alternative. This works well for planning tasks where the checkpoint is just the conversation state and plan history. It works poorly for tasks with real-world side effects because the checkpoint does not include external system state.

In practice, most production agents use a combination of strategies. Some actions are undoable, some require compensating actions, some can be checkpointed. Your agent needs to know which strategy applies to each action type and apply the right one when backtracking is needed. This requires explicit modeling of action reversibility in your agent architecture.

## When Backtracking Is Possible vs Impossible

Some actions are fully reversible within the agent's control. Creating a draft document can be undone by deleting the draft. Adding an item to a cart can be undone by removing it. Allocating memory can be undone by freeing it. These actions have clean inverses that the agent can execute without external approval or consequences. Backtracking from these actions is straightforward.

Some actions are reversible but require external approval or have costs. Processing a refund can be reversed by charging the card again, but this requires the customer's approval and might trigger fraud alerts. Deleting a record can be reversed by restoring from backup, but this requires database admin access and might restore stale data. Sending an email can be partially reversed by sending a retraction, but you cannot unsend the original. These actions are theoretically reversible but practically costly.

Some actions are irreversible within any reasonable timeframe. Once your agent publishes content to a public API, that content is visible to anyone consuming the API. You can unpublish it, but some consumers have already cached or stored it. Once your agent executes a database migration that drops a column, the data in that column is gone unless you have backups. Once your agent confirms a shipment, the package is in the carrier's system and cannot be easily recalled. These actions create facts about the world that cannot be erased.

Actions involving humans are especially difficult to reverse. If your agent sends a message to a user and the user reads it, you cannot make them unread it. If your agent schedules a meeting and the user blocks time for it, canceling the meeting wastes their time. If your agent makes a promise to a customer and the customer relies on that promise, breaking the promise damages the relationship. Human actions have trust and expectation costs that compensating actions cannot fully address.

The reversibility of an action often depends on timing. Canceling an order is easy before it ships, hard after it ships, and impossible after it is delivered. Correcting a billing error is straightforward before payment, harder after payment, and very hard after the billing period closes. Your agent needs to understand these time windows and prioritize backtracking when it is still feasible rather than waiting until it is too late.

## Designing Actions to Be Reversible

If you know backtracking will be necessary, you can design your agent's actions to support it. The most powerful pattern is the two-phase commit: plan and confirm. In phase one, the agent formulates a plan and prepares all the actions without executing them. It creates a draft order but does not submit it. It calculates a refund amount but does not process it. It generates an email but does not send it. In phase two, the agent verifies that all steps are feasible and then executes them. This verification step catches plan failures before side effects happen.

Two-phase commit does not eliminate backtracking, but it shifts the failure point to before side effects. If the agent discovers in phase one that step five will fail, it can revise the plan before executing steps one through four. The cost is additional latency—you are doing a dry-run of the entire plan before executing it. For high-stakes operations, this latency is worth it. For low-stakes operations, it might not be.

Idempotent actions are another design pattern that simplifies backtracking. An idempotent action produces the same result whether you execute it once or multiple times. Creating a record with a specific ID is idempotent if the system checks whether the ID already exists and skips creation if it does. Sending a notification is idempotent if the system deduplicates by message ID. Idempotent actions mean the agent can safely retry without creating duplicate side effects, and can execute compensating logic without worrying about double-compensation.

Soft deletes instead of hard deletes make data modifications reversible. When your agent deletes a record, it marks it as deleted rather than removing it from the database. The record is hidden from normal queries but still exists and can be undeleted if needed. This pattern is common in production systems specifically because it supports recovery from mistakes. The tradeoff is increased storage cost and query complexity.

Audit logs enable backtracking by recording what the agent did. If the agent needs to undo a sequence of actions, the audit log tells it exactly what those actions were and in what order. The agent can walk the log backward, executing inverse actions for each logged action. Audit logs also support debugging—when something goes wrong, you can see exactly what the agent did and where the plan diverged from expectations.

Delayed execution is a reversibility pattern where actions are queued rather than executed immediately. The agent plans a sequence of actions and adds them to a queue with a short delay—maybe 30 seconds or 2 minutes. During that delay, the agent can cancel or modify queued actions. Once the delay expires, actions execute. This gives the agent a window to catch planning errors before they become real errors. The cost is the delay itself, which might not be acceptable for time-sensitive operations.

## The Checkpoint Pattern in Agent Systems

Checkpoints are save points in agent execution that capture enough state to resume from that point. Before making a significant decision or taking a risky action, the agent saves a checkpoint. If the decision turns out to be wrong, the agent loads the checkpoint and tries a different decision. This is common in game AI and is increasingly used in agent systems.

A checkpoint in an agent system typically includes the conversation history up to that point, the plan state, the agent's working memory or context, and enough information about external system state to resume coherently. The checkpoint does not include the external world—it cannot undo an email that was sent or a charge that was processed. But it does include the agent's internal state, so the agent can reason about the situation as it existed at checkpoint time.

The value of checkpoints is that they enable plan exploration without commitment. The agent can checkpoint, explore one approach, determine it will not work, rollback to the checkpoint, and explore a different approach. This is particularly useful for planning tasks where multiple strategies might work but the agent is not sure which is best. Rather than committing to the first strategy, the agent can explore several and choose the best.

Checkpoints have costs. Storing checkpoint state consumes memory or storage. Loading a checkpoint discards any work done after the checkpoint, which might include expensive API calls or computation. If the agent creates too many checkpoints, the overhead of managing them becomes significant. The engineering challenge is deciding when checkpoints are worth their cost.

A practical checkpoint pattern is to checkpoint before irreversible actions. Before sending an email, deleting data, or processing a payment, the agent saves enough state to reconsider that action. If the action has unintended consequences, the agent can load the checkpoint and choose differently. This pattern protects against the most costly errors—irreversible actions that should not have been taken.

Another pattern is to checkpoint at plan branches. When the agent faces a decision with multiple reasonable options, it checkpoints before choosing. If the chosen option does not work out, the agent can return to the checkpoint and try another option. This is essentially depth-first search through the plan space, using checkpoints to backtrack when a path fails.

Some teams implement checkpoints as explicit user confirmation points. Before taking a high-stakes action, the agent presents the plan to the user and asks for confirmation. The user can approve, reject, or request modifications. This human-in-the-loop checkpoint ensures that irreversible actions have human oversight. The agent can plan autonomously, but execution of risky steps requires human approval.

## The Cost of Backtracking

Backtracking is expensive in multiple dimensions. The most obvious cost is re-executing steps. If the agent executed five steps, realized the plan was wrong, undid those steps, and started a new plan, it has done 10 steps worth of work to make 5 steps worth of progress. If each step involves API calls, the token cost and latency cost are doubled. At scale, backtracking inefficiency becomes a significant budget item.

The token cost of backtracking is particularly high because the agent needs to reason about what went wrong, how to undo or compensate, and what to do instead. This reasoning is in addition to the original planning and execution tokens. An interaction that would have consumed 2000 tokens without backtracking might consume 5000 tokens with backtracking—the original plan, the execution, the failure analysis, the rollback, and the new plan.

User-facing latency is another cost. From the user's perspective, the agent is taking a long time to complete a simple task. The user does not see the backtracking happening—they just see that the agent has not responded yet. If backtracking adds 15 seconds to a task that should take 5 seconds, the user experience degrades significantly. Some users will abandon the interaction before the agent recovers.

Trust cost is harder to measure but real. If the agent takes an action and then backtracks—"I sent you a confirmation email, actually no I did not, sorry"—the user's confidence in the agent decreases. If this happens repeatedly, users learn they cannot trust the agent's statements about what it has done. Trust erosion affects long-term adoption and satisfaction even if individual backtracking instances are handled correctly.

Backtracking can create inconsistent state across systems. If your agent updates system A, then system B, then realizes the update was wrong and rolls back system A but fails to roll back system B, the systems are now inconsistent. System A believes one thing, system B believes another, and reconciling them requires manual intervention. The risk of inconsistency increases with the number of systems the agent interacts with and the complexity of rollback logic.

Some backtracking failures leave the system in a worse state than if the agent had done nothing. The e-commerce example at the start of this chapter illustrates this: the attempt to exchange a product left the customer with no product and a refund they did not want. The agent's actions created a problem that did not exist before. This is worse than failing to help—it is actively creating harm that requires cleanup.

## When to Backtrack vs Adapt Forward

Not every plan failure requires backtracking. Sometimes the better move is to adapt the plan forward—acknowledging that the original plan will not work and constructing a new plan that starts from the current state rather than trying to return to the original state. The decision between backtracking and forward adaptation depends on the cost and feasibility of each.

Backtracking makes sense when the cost of undoing is lower than the cost of adapting forward. If the agent has taken two actions that are easily reversible, and adapting forward would require five additional complex actions, backtracking is the better choice. You undo two steps and start over rather than compounding the problem with five more steps.

Forward adaptation makes sense when backtracking is costly or impossible. If the agent has taken five actions, three of which are irreversible, attempting to backtrack will fail. The agent should accept the current state as given and plan forward from there. "Given that I have already done X, Y, and Z, and Z cannot be undone, what is the best path forward?" This is often more productive than trying to undo the undoable.

The user's goal should drive the decision. If the user's original request can still be satisfied from the current state, forward adaptation might achieve it faster than backtracking to the original state and trying again. If the current state makes the original goal unachievable, backtracking to a state where the goal is achievable might be necessary.

Some situations call for partial backtracking. The agent undoes the most recent steps that are causing problems but keeps earlier steps that are still valid. This is a middle ground between full backtracking and pure forward adaptation. The agent salvages what it can from the original plan while correcting the part that went wrong.

The time horizon matters. If the agent is early in a long plan when it discovers the plan is wrong, backtracking is relatively cheap—little work has been done, and the cost to restart is low. If the agent is near the end of a long plan when it discovers a problem, backtracking is expensive—most of the work would be discarded. Forward adaptation that completes the task imperfectly might be preferable to backtracking and starting a new long plan.

## Implementing Backtracking in Practice

Practical backtracking requires your agent to maintain an action history—a log of what it has done, in what order, with enough detail to reverse or compensate. This history is essential for rollback. Without it, the agent does not know what to undo. The history should include not just what actions were taken but what their effects were, because the undo or compensating action needs to target those effects.

Each action type should have a defined rollback strategy. Email actions rollback via compensating emails. Database actions rollback via inverse SQL or compensating records. Payment actions rollback via refunds. Your agent needs to know which strategy applies to each action it can take. This knowledge is often encoded in the tool or function definitions—each tool specifies how to undo or compensate for its use.

The agent should assess rollback feasibility before attempting rollback. Not all actions can be rolled back, and attempting to rollback an irreversible action wastes time and might create new errors. Before attempting to undo an action, the agent should check whether undo is possible. If it is not, the agent should immediately shift to compensating actions or forward adaptation rather than trying and failing to rollback.

Rollback should be logged and monitored just like forward actions. When your agent executes a rollback, that fact should be visible in logs and metrics. Frequent rollbacks indicate planning problems—the agent is making plans that fail mid-execution. This is a signal to improve planning, add verification steps, or redesign actions to be more reversible. Rollback frequency is a key metric for agent reliability.

Some teams implement rollback as an explicit agent capability. The agent has a "rollback" tool that takes an action history and attempts to reverse it. This separates rollback logic from forward execution logic and makes rollback behavior easier to test and maintain. The rollback tool knows how to undo each action type and handles errors when undo is not possible.

Others implement rollback as part of error handling. When any action fails or produces an unexpected result, the error handler evaluates whether rollback is appropriate. If so, it executes the rollback strategy for actions taken so far. This ensures that failures do not leave the system in partially-executed states. The downside is that error handlers can become complex when they need to handle rollback logic for many different action types.

## Learning from Backtracking Failures

Every backtracking event is a learning opportunity. The agent made a plan that turned out to be wrong. Why was it wrong? What information would have prevented the bad plan? What checks could have caught the problem earlier? The answers to these questions should feed back into your agent's planning process.

If backtracking happens because the agent lacked information, you can improve planning by gathering that information upfront. The agent planned to exchange a product without checking stock levels. Stock level checking should be added to the planning phase. The agent planned to process a refund without verifying payment method. Payment method verification should be added. Each backtracking instance suggests a planning improvement.

If backtracking happens because the agent made incorrect assumptions, those assumptions should be made explicit and verified. The agent assumed the user wanted X when they actually wanted Y. That assumption should become a clarifying question. The agent assumed system A would accept the update when it had validation rules. Those rules should be checked before planning the update.

Patterns in backtracking reveal systematic planning flaws. If the agent frequently backtracks from the same action type, that action type is harder to plan for than the agent realizes. You might need better documentation of that action's preconditions, better error messages when it fails, or better examples of successful usage. If the agent frequently backtracks when certain conditions are present, those conditions should trigger more careful planning or human confirmation.

Some teams use backtracking frequency as a training signal. In RL-based agent systems, backtracking is a negative reward—the agent is penalized for making plans that require backtracking. Over time, the agent learns to make plans that are more likely to succeed without rollback. This works when you have enough volume to train on and when the planning space is consistent enough that patterns learned from past backtracking apply to future situations.

## The Ideal of No Backtracking

The best backtracking is the backtracking you never have to do. The goal is not to build perfect backtracking mechanisms—it is to build planning processes that do not require backtracking. Every backtracking event represents a planning failure. Perfect planning would never start a plan that cannot be completed.

This ideal is unachievable in practice because agents operate in uncertain environments with incomplete information. You cannot always know upfront whether a plan will work. But you can get closer to the ideal by improving planning quality, adding verification steps, implementing dry-runs, and using two-phase commits for high-stakes actions.

The practical goal is to minimize backtracking frequency and cost. You will never eliminate backtracking entirely, but you can reduce it to rare edge cases rather than routine occurrences. An agent that backtracks on 2 percent of interactions is well-designed. An agent that backtracks on 30 percent of interactions has planning problems that need to be addressed.

When backtracking is necessary, it should be fast, clean, and transparent. The agent should recognize quickly that the plan is not working. It should know exactly what needs to be undone or compensated. It should communicate clearly to the user what happened and what it is doing about it. The user should not be left confused or frustrated by the backtracking process.

Backtracking is ultimately about resilience—the ability to recover from mistakes gracefully. You build backtracking capabilities not because you expect to use them constantly, but because mistakes will happen and your agent needs a way to handle them without leaving broken state behind. The best agents combine good planning that minimizes mistakes with robust backtracking that handles mistakes cleanly when they occur. Both are necessary. Neither is sufficient alone.

