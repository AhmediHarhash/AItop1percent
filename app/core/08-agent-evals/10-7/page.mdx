# 10.7 â€” Agent Queue Management and Concurrency Control

According to industry data from 2025, the median agent system experiences its first production queue saturation incident within eleven days of launch. The pattern is predictable: initial load looks manageable, a traffic spike triggers concurrent execution of dozens of agents, shared resources exhaust, and latency spirals into timeout cascades before anyone realizes the queue depth hit triple digits.

The mistake was treating agent requests like stateless API calls instead of long-running, resource-intensive workflows. Agent systems operate in a fundamentally different resource envelope than traditional services. A single agent session might execute for minutes or hours, consume thousands of tokens across multiple model calls, hold database connections, maintain state in memory, and spawn sub-processes or tool invocations. Without explicit queue management and concurrency control, you allow demand spikes to destabilize the entire system. Queue management is not optional infrastructure you add later when scaling issues emerge. It is foundational architecture you design during framing, before writing a single line of agent logic.

## The Agent Request Lifecycle and Resource Envelope

Every agent request enters your system with unknown resource requirements. You do not know in advance how many model calls it will trigger, how many tool invocations it will require, or how long it will run. A support ticket agent handling a password reset might complete in eight seconds with two model calls. The same agent handling a billing dispute involving subscription changes across three products might run for ninety seconds with fourteen model calls, six database queries, and two API integrations. Your queue management strategy must account for this variability without overprovisioning for worst-case scenarios or underprovisioning and creating bottlenecks.

The resource envelope for a typical agent session includes model API quota measured in tokens per minute, concurrent request slots with your model provider, memory for maintaining conversation history and intermediate state, database connections for reading user data and writing logs, external API calls for tools and integrations, and CPU for orchestration logic and response parsing. Each of these resources has hard limits. Your model provider enforces rate limits measured in requests per minute and tokens per minute. Your database connection pool has a maximum size. Your server instances have finite memory and CPU cores. When demand exceeds capacity across any of these dimensions, the system degrades.

The degradation manifests in three patterns. First, queue buildup: requests wait longer before processing starts, increasing end-to-end latency. Second, throttling errors: agents hit rate limits mid-session, fail requests, and trigger retries that consume more quota. Third, cascading timeouts: long-running agents hold resources, starving new requests, which time out and retry, creating positive feedback loops. You prevent these patterns by implementing explicit queue management that meters request admission based on current system load and resource availability.

## Queue Depth and Admission Control

The most critical metric in agent queue management is queue depth: the number of requests waiting to be processed. Queue depth directly determines wait time for new requests and signals system saturation. A queue depth of zero means the system is underutilized. A queue depth climbing into double digits means you are approaching capacity. A queue depth in the hundreds means you are already failing SLAs and creating bad user experiences.

You set admission control policies that define when to accept new requests, when to queue them, and when to reject them. A basic admission policy uses three thresholds: accept, queue, and reject. When active agent sessions are below the accept threshold, you process new requests immediately. When active sessions are between accept and queue thresholds, you add new requests to the queue. When the queue depth exceeds the reject threshold, you return an error immediately rather than queuing indefinitely. For a system designed to handle twenty concurrent agent sessions, you might set accept at fifteen, queue at thirty total requests, and reject when queue depth exceeds fifty.

These thresholds depend on your resource limits and SLA commitments. If your model provider enforces 500,000 tokens per minute and your average agent session consumes 8,000 tokens over ninety seconds, you can theoretically run sixty-two concurrent sessions at steady state. In practice, you provision for variability and burst traffic. You set your accept threshold at 70 percent of theoretical capacity and your queue limit at 2x accept threshold. This leaves headroom for request variance and prevents queue buildup from becoming unbounded.

Admission control also considers request priority. Not all agent requests have equal business value. A Tier 1 enterprise customer initiating a compliance review should receive priority over a free-tier user asking for report formatting. You implement priority queues with separate admission thresholds per tier. High-priority requests bypass the standard queue when system load is below critical thresholds. You reserve capacity for high-priority traffic by setting aside a portion of your concurrency budget. If you can handle twenty concurrent sessions, you reserve five slots exclusively for high-priority requests. Standard requests compete for the remaining fifteen slots.

## Concurrency Limits and Resource Pooling

Concurrency control defines how many agent sessions can run simultaneously. The correct concurrency limit balances throughput against resource saturation. Setting concurrency too low wastes capacity and creates unnecessary queuing. Setting concurrency too high triggers rate limits, memory exhaustion, and database connection starvation.

You determine your concurrency limit by measuring the resource bottleneck. For most agent systems, the bottleneck is model API quota, specifically tokens per minute. You measure the token consumption profile of representative agent sessions across different task types. A customer support agent averages 6,500 tokens per session over forty-five seconds. A research agent averages 32,000 tokens per session over four minutes. A code review agent averages 18,000 tokens per session over two minutes. You calculate weighted average token consumption based on your traffic mix. If 60 percent of requests are support, 25 percent are research, and 15 percent are code review, your weighted average is 13,100 tokens per session.

With a model provider limit of 500,000 tokens per minute and average session consumption of 13,100 tokens, you calculate maximum safe concurrency. You divide quota by average consumption: 500,000 divided by 13,100 equals thirty-eight sessions. You apply a safety margin of 25 percent to account for variance, yielding a concurrency limit of twenty-eight. You monitor actual token usage in production and adjust the limit based on observed P95 consumption, not just averages.

Resource pooling extends concurrency control to downstream dependencies. Your agent system does not operate in isolation. It calls databases, external APIs, caching layers, and message queues. Each dependency has its own concurrency limits. Your database connection pool might support fifty concurrent connections. Your external API for document storage might enforce ten requests per second. Your message queue for async notifications might handle one hundred messages per second. Your agent concurrency limit must respect the most constrained dependency.

You implement per-resource semaphores that track utilization and block new sessions when resources are saturated. Before starting an agent session, you acquire a database connection from the pool, a model API slot from your rate limiter, and any other required resources. If any resource is unavailable, the request waits in the queue until resources free up. This prevents the scenario where twenty agent sessions start successfully but then fifteen fail mid-execution because the database connection pool is exhausted.

## Backpressure and Queue Overflow Handling

Backpressure is the mechanism that signals upstream systems when your agent queue is saturated. Without backpressure, client applications keep sending requests regardless of your capacity, flooding the queue and creating unbounded latency. Backpressure communicates system load so clients can throttle their own request rates or route traffic elsewhere.

You implement backpressure through HTTP status codes and response headers. When queue depth exceeds your accept threshold but remains below reject threshold, you return HTTP 202 Accepted with a Retry-After header indicating estimated wait time. When queue depth exceeds reject threshold, you return HTTP 503 Service Unavailable with a Retry-After header. Clients that respect these signals back off automatically, reducing load and allowing the queue to drain.

Estimated wait time is calculated from current queue depth and average processing time. If you have forty requests in the queue and average processing time is ninety seconds with concurrency of twenty, you calculate wait time as queue depth divided by concurrency times average duration: forty divided by twenty times ninety equals 180 seconds. You add a safety buffer and return Retry-After: 240 in the response header. Sophisticated clients use exponential backoff, doubling their retry interval after each rejection.

You also expose queue depth metrics via monitoring endpoints. Client applications and load balancers query your metrics endpoint to get real-time queue depth and active session count. Load balancers use this data to route traffic to the least-loaded instance in a multi-instance deployment. Internal dashboards display queue depth trends, allowing your operations team to scale capacity proactively before users experience degraded service.

Queue overflow handling defines what happens when the queue is full and new requests arrive. You have three options: reject immediately, spill to durable storage, or apply load shedding policies. Immediate rejection is appropriate for latency-sensitive workloads where waiting five minutes defeats the purpose. The client receives HTTP 503 and must retry later or route the request to a fallback system. Spilling to durable storage moves overflow requests to a database or message queue, processing them asynchronously when capacity becomes available. This works for batch workloads where users tolerate longer turnaround times.

Load shedding applies business logic to decide which requests to drop when the system is saturated. You define shedding criteria based on request priority, customer tier, or task type. When queue depth exceeds critical threshold, you drop low-priority requests while continuing to queue high-priority requests. You log every dropped request with reason codes, enabling post-incident analysis of how many users were affected and which traffic patterns triggered shedding.

## Dynamic Concurrency Scaling

Static concurrency limits work for stable traffic patterns but fail during demand spikes or when upstream dependencies degrade. Dynamic concurrency scaling adjusts limits in real time based on observed system behavior and success rates.

You implement dynamic scaling using a feedback control loop. Every thirty seconds, you measure key metrics: current queue depth, active session count, success rate over the last minute, P95 latency, and error rate. If queue depth is climbing and success rate remains above 95 percent, you increase concurrency by 10 percent. If error rate exceeds 5 percent or P95 latency crosses SLA thresholds, you decrease concurrency by 20 percent. The asymmetry in scaling rates prevents oscillation. You scale up slowly to test new capacity limits but scale down aggressively when degradation signals appear.

Dynamic scaling responds to rate limit changes and model provider degradation. If your model provider reduces your quota mid-day due to platform-wide load, your agents start hitting 429 errors. The increased error rate triggers concurrency reduction, preventing retry storms. As errors subside, the control loop gradually increases concurrency until it finds the new sustainable limit. You do not need to manually reconfigure thresholds every time your provider adjusts quota.

You also use dynamic scaling to handle cost caps. If your monthly model spend approaches budget limits, you reduce concurrency to throttle token consumption. You calculate remaining budget, days left in the billing period, and current burn rate. If you are on track to exceed budget by 30 percent, you reduce concurrency by 30 percent. This spreads available quota across the month rather than exhausting it in the first two weeks and then rejecting all requests.

Dynamic scaling requires guardrails to prevent runaway behavior. You set absolute minimum and maximum concurrency bounds. Even if the control loop suggests scaling to zero due to high error rates, you enforce a minimum of two concurrent sessions to keep the system responsive. Even if queue depth suggests scaling to one hundred concurrent sessions, you enforce a maximum of fifty to prevent database connection exhaustion. You log every scaling decision with the triggering metrics, enabling post-incident review of whether scaling logic behaved correctly.

## Task-Based Queue Routing

Not all agent tasks have identical resource profiles or latency requirements. A thirty-second FAQ agent should not wait in the same queue as a twenty-minute contract analysis agent. Task-based queue routing segregates requests into separate queues with dedicated concurrency budgets and processing workers.

You define queue routing rules based on task type, expected duration, or customer tier. Short-running tasks under thirty seconds go to a fast queue with concurrency of forty and strict SLA of five-second wait time. Long-running tasks over two minutes go to a batch queue with concurrency of ten and relaxed SLA of five-minute wait time. Medium-duration tasks use the default queue. This prevents long-running tasks from blocking short, latency-sensitive requests.

Each queue has independent admission control and concurrency limits. The fast queue accepts requests until active sessions reach forty, then queues up to eighty additional requests, then rejects. The batch queue accepts requests until active sessions reach ten, queues up to two hundred requests, and never rejects because users tolerate long wait times for complex workflows. The default queue uses intermediate settings. You allocate total system capacity across queues based on traffic distribution. If 70 percent of requests are fast, 20 percent default, and 10 percent batch, you allocate concurrency proportionally.

Queue routing also isolates failure domains. If a bug in the contract analysis agent causes memory leaks and crashes, it only affects the batch queue. Fast-queue requests for FAQ agents continue processing normally. Without queue segregation, a single bad agent would destabilize the entire system. You monitor per-queue metrics separately: queue depth, active sessions, success rate, and P95 latency for each queue. This granular visibility lets you identify which task types are causing bottlenecks or failures.

You route requests to queues using metadata in the initial request. The client specifies task type or expected duration in request headers. If metadata is missing, you apply heuristics. An agent request with a 200-token prompt and no tool definitions likely runs fast. An agent request with a 4,000-token prompt and six tool definitions likely runs long. You classify into queues based on these signals. You also support queue migration: if a request routed to the fast queue exceeds thirty seconds, you pause it, move it to the batch queue, and resume execution. This prevents misclassified requests from violating fast-queue SLAs.

## Graceful Degradation and Circuit Breaking

When system load exceeds capacity despite queue management, you need graceful degradation strategies that maintain partial functionality rather than failing completely. Graceful degradation prioritizes critical agent capabilities and sheds non-essential features.

You define degradation tiers for agent behavior. In normal operation, your support agent uses retrieval-augmented generation with semantic search across your full knowledge base, calls six different APIs to gather customer context, and generates responses with citation links. In degraded mode tier one, you disable citation links and reduce knowledge base search depth. In degraded mode tier two, you skip API calls for non-critical context and rely on cached data. In degraded mode tier three, you disable RAG entirely and respond using only the model's parametric knowledge. Each degradation tier reduces resource consumption while maintaining core functionality.

You trigger degradation tiers based on queue depth and error rate thresholds. When queue depth exceeds fifty and error rate exceeds 3 percent, you activate tier one degradation. When queue depth exceeds one hundred and error rate exceeds 10 percent, you activate tier two. When queue depth exceeds two hundred or error rate exceeds 25 percent, you activate tier three. You broadcast degradation state to all active sessions via a shared state service. Each agent checks degradation state before making expensive calls and adapts behavior accordingly.

Circuit breaking prevents cascading failures when downstream dependencies become unreliable. If your external document retrieval API starts timing out, your agents retry, consume more quota waiting for responses, and tie up concurrency slots. A circuit breaker monitors error rates for each dependency. When error rate for document retrieval exceeds 50 percent over a one-minute window, the circuit breaker opens. All subsequent calls to document retrieval fail immediately without attempting the request. The agent falls back to responding without retrieved context.

The circuit breaker remains open for a cooldown period, typically thirty seconds to two minutes. After cooldown, it enters half-open state, allowing a small percentage of requests through to test if the dependency recovered. If those test requests succeed, the circuit closes and normal operation resumes. If they fail, the circuit reopens and cooldown resets. Circuit breakers prevent retry storms that amplify outages and give dependencies time to recover without continuous bombardment.

You implement circuit breakers per dependency, not globally. A circuit breaker for your database operates independently from circuit breakers for external APIs and model providers. This fine-grained control lets you degrade specific capabilities while preserving others. You log every circuit state transition with triggering metrics and affected request count, enabling post-incident analysis of how degradation contained failures.

## Monitoring Queue Health and Capacity Planning

Queue management is only effective if you monitor queue health in real time and use historical data for capacity planning. You track queue depth over time, active session count, request arrival rate, processing duration by percentile, success rate, error rate by error type, concurrency limit, and admission decisions per minute.

You visualize queue depth as a time series with color-coded thresholds. Green zone indicates queue depth below 25 percent of limit, yellow zone indicates 25 to 75 percent, red zone indicates above 75 percent. Time spent in red zone correlates with user-reported latency issues. You set alerts that fire when queue depth remains in red zone for more than five consecutive minutes, indicating sustained overload requiring capacity increases.

You monitor the relationship between queue depth and error rate. In healthy systems, error rate remains flat as queue depth increases. Errors result from invalid inputs or business logic failures, not resource saturation. When error rate climbs in correlation with queue depth, it signals that queuing itself is causing failures. Long-queued requests might time out before processing starts, or they might hit stale data because context changed during wait time. You investigate correlated error spikes to identify root causes and adjust queue limits or SLA definitions.

You track admission decisions to understand how often the system rejects requests. A rejection rate below 1 percent is normal and indicates effective capacity planning. A rejection rate above 5 percent means you are regularly turning away traffic and need to scale capacity. You break down rejections by priority tier and task type to identify which workloads are most affected by capacity constraints.

Capacity planning uses historical queue metrics to predict future needs. You analyze weekly traffic patterns to identify peak load periods. If every Monday morning sees a 3x spike in agent requests due to weekend email backlog, you pre-scale concurrency limits Monday mornings. You forecast growth based on user acquisition trends. If your user base grows 15 percent per quarter and agent usage per user remains constant, you need 15 percent more capacity each quarter. You model the impact of new features: adding a new agent capability that handles ten additional requests per user per week means a 10x increase in that traffic type.

You run load tests quarterly to validate that your concurrency limits and queue thresholds match actual system capacity. You generate synthetic traffic that mimics production request patterns and ramp up load until queue depth saturates or error rate spikes. The load at failure point is your true system capacity. You compare tested capacity to configured limits and adjust settings accordingly. Load testing in staging environments does not capture production dependencies and traffic variance, so you also run controlled load tests in production during low-traffic windows.

Your monitoring dashboards include runbooks for common queue health issues. When queue depth alert fires, the runbook guides operators through diagnostic steps: check model provider status for outages, review recent deployments for regressions, examine error logs for new failure modes, inspect database connection pool utilization, and verify external API health. The runbook provides commands for emergency mitigation: scale concurrency limits, activate degradation tiers, enable circuit breakers, or route traffic to backup regions. Clear runbooks reduce time to mitigation from twenty minutes to two minutes during incidents.

Queue management and concurrency control are not infrastructure layers you build once and forget. They are operational systems that require continuous monitoring, tuning, and capacity planning. Your queue configuration that works at launch will not work six months later when traffic has tripled and you have added five new agent types. You treat queue management as a living system, reviewing metrics weekly, adjusting thresholds monthly, and load testing quarterly. The next critical piece of operational infrastructure is session management: how you persist agent state, recover from failures, and maintain continuity across long-running workflows.
