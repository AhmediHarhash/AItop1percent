# 7.11 — HITL Security: Preventing Approval Bypasses and Social Engineering

The attacker did not hack the system—they studied the agent's decision logic through trial transactions, identified which features predicted low-risk classification, and engineered fraudulent transfers to match that profile precisely, bypassing human review for 127 wire transfers totaling 4.7 million dollars. In September 2025, a financial services company discovered this manipulation after a three-week period where an attacker had crafted transaction patterns and supporting documentation that triggered the agent's low-risk classification. For the handful of transactions that did reach human reviewers, the attacker had seeded the agent's context with fabricated business relationship history that made the transfers appear legitimate. The attack succeeded not because the HITL workflow was absent, but because it was designed with the assumption that human reviewers were the primary security control, when in reality they had become a secondary check on an AI system that could be manipulated.

Your human-in-the-loop workflows are not just operational controls. They are attack surfaces. Any system that routes agent decisions through human approval introduces the possibility that attackers will attempt to manipulate either the routing logic to avoid review, or the reviewers themselves to approve malicious actions. Security for HITL systems requires defending against both technical bypasses and social engineering attacks, treating human reviewers as components in a security architecture rather than infallible validators of AI outputs.

## Threat Model for HITL Systems

The first category of attacks targets the routing logic that determines which agent actions require human review. If your system only routes high-risk or uncertain actions to reviewers, attackers will attempt to craft inputs or contexts that cause the agent to classify malicious actions as low-risk, bypassing review entirely. This is a form of adversarial machine learning, but the goal is not to fool the agent into making an incorrect decision. The goal is to fool the agent into making the decision appear routine enough that it does not trigger human oversight. The attack surface is the agent's risk scoring or uncertainty estimation mechanism, and the vulnerability is that these mechanisms are trained or tuned on benign data distributions, not adversarially crafted inputs designed to evade detection.

Consider a customer service agent that routes refund requests to human approval only when the refund amount exceeds certain thresholds or when the customer's account history shows anomalies. An attacker who understands these routing rules can structure fraudulent refund requests to stay just below thresholds, spread across multiple accounts to avoid triggering anomaly detection, or include plausible justifications that match patterns the agent associates with legitimate requests. Each individual request bypasses review because it does not meet the criteria for escalation, but in aggregate they represent coordinated fraud. Your HITL routing logic must be designed with the assumption that attackers will reverse-engineer your escalation criteria and optimize their attacks to evade them.

The second category of attacks targets human reviewers through social engineering, manipulating the information they see or the context in which they make decisions to increase the likelihood of approval for malicious actions. The most direct form of this attack is prompt injection, where an attacker embeds instructions in user inputs that cause the agent to present false or misleading information to the reviewer. If your agent generates a summary of a transaction or case for human review, and that summary includes user-controlled text, an attacker can inject prompts that instruct the agent to downplay risk factors, fabricate supporting details, or omit information that would trigger rejection. The reviewer sees what appears to be a coherent, well-supported recommendation without realizing that the agent's output has been manipulated by adversarial input.

A more subtle form of social engineering exploits the cognitive load and decision fatigue of human reviewers. If your HITL workflow routes hundreds of low-risk approvals to reviewers each day, with only occasional genuinely risky cases, reviewers develop a pattern of rapid approval, relying on the agent's assessment as the primary signal. Attackers can exploit this by timing their attacks during high-volume periods when reviewers are most fatigued, or by gradually increasing the frequency of borderline-risky actions to normalize suspicious patterns. The attack does not require fooling the reviewer about any individual case. It relies on statistical manipulation of the review environment to increase approval rates for actions that would be rejected under more careful scrutiny.

The third category of attacks targets the HITL infrastructure itself, attempting to bypass or disable human review through technical exploits. This includes race conditions where the attacker triggers an action and a cancel or modification in rapid succession to exploit timing windows in the approval workflow, privilege escalation where the attacker gains access to reviewer credentials or permissions, and logic flaws in the approval workflow that allow actions to be executed before review is complete or after review expiration. These are traditional application security vulnerabilities, but they are particularly dangerous in HITL contexts because they can allow automated agents to act without the oversight that your risk model depends on.

## Defending Against Routing Bypasses

The first defense is to design your HITL routing logic to be robust against adversarial manipulation. This means not relying solely on the agent's confidence scores or uncertainty estimates to determine when human review is required. Confidence scores are easily gamed by attackers who can manipulate inputs to produce high-confidence incorrect outputs. Instead, use a combination of rule-based triggers and statistical anomaly detection that cannot be bypassed by optimizing individual inputs. For example, require human review for all transactions above a threshold regardless of the agent's confidence, for all actions involving new counterparties or accounts regardless of the agent's risk assessment, and for any case where the agent's decision differs from historical patterns for similar contexts.

Implement meta-review triggers that detect when the agent's decision-making deviates from expected distributions. If your agent normally escalates fifteen percent of refund requests to human review, and in a given hour that rate drops to three percent, this is a signal that something has changed. It could be a shift in user behavior, but it could also be an attacker probing for bypass techniques. Your HITL system should flag these distributional anomalies and route a sample of the low-escalation cases to human review even if they did not individually trigger escalation criteria. This creates a moving target for attackers, because optimizing inputs to avoid review on an individual basis does not prevent detection through aggregate pattern analysis.

You should also implement randomized review, where a percentage of agent actions are routed to human review regardless of risk scores or uncertainty. This serves two purposes. First, it provides ground truth data on the agent's performance on cases it classified as low-risk, allowing you to detect drift or manipulation over time. Second, it creates uncertainty for attackers about which actions will be reviewed, making it riskier to assume that any action below a threshold will bypass oversight. The randomization rate should be high enough to provide meaningful coverage but low enough to be operationally sustainable, typically between five and fifteen percent of total actions depending on your review capacity and risk tolerance.

For high-stakes domains like financial transactions or healthcare decisions, consider implementing dual-control or split-knowledge requirements where certain actions require approval from two independent reviewers or two separate agent evaluations. This makes bypass attacks much more difficult because the attacker must fool two different decision-makers or systems simultaneously. Dual control also defends against insider threats, where a single malicious or compromised reviewer might approve fraudulent actions. The two reviewers should not see each other's assessments until both have completed their review, preventing anchoring bias where the second reviewer simply defers to the first.

## Hardening Review Interfaces Against Manipulation

The information your HITL interface presents to reviewers is a critical attack surface. If that information can be manipulated by user inputs or agent outputs, attackers will exploit it to increase approval rates for malicious actions. The first hardening measure is strict separation between user-controlled content and system-generated metadata in the review interface. User inputs should be clearly marked and sandboxed visually so that reviewers can immediately distinguish between what the user provided and what the system assessed. If your review interface shows a transaction description, the interface should clearly indicate whether that description came from the user, from the agent's interpretation, or from a verified external source.

Implement content sanitization and validation on all text that flows from user inputs through the agent to the review interface. This includes stripping formatting that could be used to obscure or highlight specific information, removing embedded instructions or prompts that could manipulate the agent's summary generation, and validating that structured fields contain expected data types and ranges. If your agent generates a risk summary for reviewer consumption, that summary should be generated from structured data, not from free-text user inputs that could contain adversarial prompts. The agent should extract facts, classify them into predefined categories, and present them to reviewers as structured fields, not as paragraphs of synthesized prose that blend user claims with system assessments.

Your review interface should present information in a way that makes anomalies and inconsistencies salient. If a transaction involves a new counterparty, that should be highlighted prominently, not buried in a list of transaction details. If the agent's risk assessment is lower than the historical average for similar transactions, the interface should surface that discrepancy and prompt the reviewer to consider why. If key fields are missing or the data quality is poor, the interface should require the reviewer to acknowledge those gaps before approving. The design principle is to make risky features and red flags visually and cognitively prominent so that reviewers cannot overlook them even during rapid review sessions.

Implement time-based controls that prevent rubber-stamping. If a reviewer approves a complex case in less than the minimum time required to actually read the information presented, the system should flag this as a potential quality issue and route the case to a second reviewer or a supervisor. This does not mean imposing arbitrary delays, but it does mean tracking review time distributions and treating unusually fast approvals as anomalies that warrant scrutiny. Attackers who rely on reviewer fatigue or inattention benefit from rapid approvals. Forcing a minimum engagement time disrupts that attack vector.

You should also provide reviewers with tools to verify information independently of the agent's assessment. If the agent presents a claim about a customer's account history, the review interface should provide one-click access to the source system where that history is stored, allowing the reviewer to confirm the agent's interpretation. If the agent cites a policy or regulation to support its recommendation, the interface should link to the actual policy document, not just the agent's summary. Making verification easy increases the likelihood that reviewers will spot manipulated or fabricated information, and it reduces their reliance on the agent as the sole source of truth.

## Monitoring for Social Engineering and Reviewer Compromise

Even with hardened interfaces, you must monitor for signs that reviewers are being manipulated or that their decision-making patterns have changed in ways that suggest compromise. The first signal is approval rate drift. If a reviewer's approval rate for a specific category of cases increases significantly over a short time period, this could indicate that they have been socially engineered to lower their standards, that they are under pressure to process volume over quality, or that they have been compromised. Your HITL analytics should track approval rates per reviewer, per decision type, and over time, and should flag statistically significant changes for investigation.

The second signal is decision time compression. If reviewers are spending less time on cases without a corresponding change in case complexity, this suggests they are relying more heavily on the agent's recommendation and conducting less independent verification. This is a vulnerability that attackers can exploit by manipulating the agent's outputs to present malicious actions as routine. Your monitoring should track not just average review time, but the distribution of review times, flagging reviewers who have shifted toward very rapid approvals or who show bimodal distributions suggesting they are triaging cases into quick-approve and careful-review buckets without consistent criteria.

The third signal is clustering of approvals for risky actions. If a single reviewer approves multiple cases that share suspicious features—same counterparty, similar transaction patterns, edge-case interpretations of policy—this could indicate targeted manipulation or collusion. Your analytics should identify clusters of related approvals and surface them for audit, even if each individual approval appeared justified in isolation. Attackers who are sophisticated enough to bypass routing logic are often sophisticated enough to spread their attacks across multiple cases to avoid detection, but clustering analysis can reveal the pattern.

You should also monitor for external indicators of compromise. If a reviewer's credentials are used from an unusual location or device, if their account shows signs of unauthorized access, or if they exhibit behavioral changes like accessing systems outside normal working hours, these are signals that their account may be compromised and that any approvals they grant should be treated as suspect. Integrate your HITL system with your broader security monitoring infrastructure to correlate HITL activity with authentication logs, endpoint security alerts, and insider threat indicators.

When you detect anomalies in reviewer behavior, your response should be calibrated to the severity and confidence of the signal. For low-confidence anomalies like a single instance of rapid approval, route the case to a second reviewer for confirmation. For moderate-confidence anomalies like approval rate drift, schedule a coaching session or audit with the reviewer to understand what has changed. For high-confidence anomalies like credential compromise indicators, immediately suspend the reviewer's approval authority and audit all recent approvals they made. Do not wait for conclusive proof of an attack. The cost of a false positive investigation is much lower than the cost of approving fraudulent actions because you hesitated to act on early warning signals.

## Access Control and Privilege Management for Reviewers

Your HITL system must enforce strict access controls that limit who can approve which types of decisions and under what conditions. Not all reviewers should have the same approval authority. Senior reviewers with more experience and training should be authorized to approve higher-risk or higher-value actions, while junior reviewers are limited to routine cases. This is not just an operational efficiency measure. It is a security control that limits the damage an attacker can do by compromising a single reviewer account.

Implement role-based access control that maps reviewer roles to specific approval scopes. A reviewer authorized to approve refunds up to $500 should not be able to approve refunds above that threshold even if they attempt to override the system. A reviewer authorized for one product line or customer segment should not see or approve cases from other areas. Scope limitations should be enforced at the database and API level, not just in the UI, so that attackers cannot bypass them through direct API calls or SQL injection. Every approval action should include an authorization check that verifies the reviewer's credentials, role, and scope before recording the decision.

Use time-based and context-based access controls for high-risk approvals. For example, require that approvals above certain thresholds can only be performed during business hours when supervisory oversight is available, or require multi-factor authentication for approvals that exceed a reviewer's normal authority. If a reviewer who normally approves fifty transactions per day suddenly attempts to approve two hundred, require step-up authentication to confirm their identity before allowing the additional approvals. These controls make it harder for attackers who have compromised credentials to abuse them at scale.

Implement separation of duties for critical workflows. The person who initiates a transaction or case should not be the same person who approves it. The person who configures the agent's decision logic or routing rules should not have approval authority for cases governed by those rules. Separation of duties prevents both insider fraud and external attackers who compromise a single account from executing end-to-end attacks without needing access to multiple independent accounts. Your HITL architecture should enforce these separations through technical controls, not just policy, so that violations are impossible rather than merely prohibited.

Regularly audit reviewer access and remove permissions that are no longer needed. Employees who change roles, leave the company, or go on extended leave should have their approval authority revoked immediately. Contractors and temporary staff should have time-limited credentials that expire automatically. The principle of least privilege applies to reviewers just as it does to application users. Every reviewer should have the minimum approval authority required for their current responsibilities, and that authority should be reviewed at least quarterly to ensure it remains appropriate.

## Designing for Incident Response and Recovery

Despite your best defenses, you must assume that attackers will eventually succeed in manipulating your HITL system to approve malicious actions. Your architecture must be designed to detect these incidents quickly, contain the damage, and recover without catastrophic impact. The first requirement is comprehensive audit logging of all approval decisions. Every approval action should generate a tamper-evident log record that includes the agent's recommendation, the information presented to the reviewer, the reviewer's identity and credentials, the timestamp, and any notes or overrides the reviewer entered. These logs must be stored in a separate system from the HITL application itself, with access controls that prevent even privileged users from modifying or deleting them.

Implement real-time alerting for high-risk approval patterns. If a reviewer approves multiple high-value transactions in rapid succession, if approvals cluster around unusual times or locations, or if the approval rate for risky cases spikes, your monitoring system should generate alerts to security and compliance teams immediately, not in a daily or weekly report. Real-time detection allows you to halt suspicious activity before attackers complete their objective, potentially preventing additional fraudulent approvals while you investigate.

Your incident response plan must include procedures for revoking or reversing approved actions when you discover that they were manipulated. This is operationally complex because the agent may have already executed the approved action, especially for real-time systems like payment processing or access control. You need compensating controls that allow you to reverse transactions, revoke access, or otherwise undo the effects of fraudulent approvals even after execution. This might involve integration with payment networks for transaction reversal, identity systems for access revocation, or manual intervention workflows for actions that cannot be reversed automatically.

When an incident occurs, conduct a thorough post-incident review that examines not just the technical attack vector but also the human factors that allowed the attack to succeed. If reviewers approved manipulated cases, determine whether the manipulation was subtle enough that any reviewer would have been fooled, or whether there were red flags that should have been caught. If the attack exploited routing bypasses, determine whether the routing logic was flawed or whether the attacker had access to information about escalation criteria that should have been confidential. Use these findings to harden your HITL system against similar attacks in the future, updating routing rules, review interfaces, monitoring thresholds, and training programs based on lessons learned.

Finally, recognize that HITL security is not a one-time design effort. Attackers adapt, probing for new vulnerabilities as you close old ones. Your security program for HITL systems must include regular red team exercises where you simulate attacks against your review workflows to identify weaknesses before real attackers do. It must include threat intelligence monitoring to track emerging attack techniques targeting AI and approval systems in your industry. And it must include continuous improvement cycles where you analyze approval patterns, review quality metrics, and incident trends to identify security gaps and prioritize remediation. Human-in-the-loop workflows are a powerful control for managing AI agent risk, but only if they are designed and operated with the recognition that they are themselves targets of sophisticated attacks.

Human oversight of AI agents introduces security risks that cannot be addressed through traditional application security controls alone. Attackers will attempt to bypass human review through adversarial manipulation of routing logic, to manipulate reviewers through social engineering and information presentation attacks, and to exploit vulnerabilities in the approval infrastructure itself. Defending against these threats requires hardening routing logic with anomaly detection and randomized review, sanitizing information flows to prevent manipulation, monitoring reviewer behavior for signs of compromise, enforcing strict access controls and privilege management, and designing for rapid incident detection and response. Security for HITL systems is an ongoing discipline that must evolve as attackers probe for new vulnerabilities and as your agent capabilities expand into higher-risk domains.
