# 10.3 — Agent A/B Testing: Comparing Patterns and Configurations

Can you trust offline evaluation to predict production success? The answer, for every team that has deployed agents at scale, is no. Test sets represent yesterday's understanding of the problem. Production represents the problem as it exists today, with edge cases, adversarial inputs, and interaction effects that no static benchmark can capture.

The failure wasn't in the new architecture itself. Post-mortem analysis revealed the reflection pattern actually was better for complex multi-issue conversations, but worse for simple single-question interactions that made up 64% of their volume. The offline test set had been biased toward complexity because the team had deliberately oversampled difficult cases to stress-test the system. In production, the new pattern spent extra inference cycles on conversations that didn't need them, degrading both latency and cost while providing no quality benefit. A proper A/B test would have caught this in the first two hours with 95% confidence. Instead, the team learned an expensive lesson about the difference between offline evaluation and online experimentation.

## The Fundamental Difference Between Offline Eval and A/B Testing

Offline evaluation answers whether an agent can perform well on known test cases under controlled conditions. A/B testing answers whether an agent performs better than the current system on real production traffic with real users in real contexts you cannot fully predict. These are not the same question. Offline eval is necessary but not sufficient. You need both.

The distinction matters because production contains distribution shifts, edge cases, adversarial inputs, temporal patterns, user behavior changes, and interaction effects that no static test set captures. Your test set represents your past understanding of the problem. Production represents the problem as it actually exists right now. When a healthtech company tested a new symptom-checking agent in September 2025, their offline eval showed 91% accuracy on their curated test set of 5,000 symptom descriptions. In A/B testing, accuracy dropped to 84% because real users described symptoms in fragmented, multi-turn conversations with frequent clarification needs that the test set's clean single-turn examples didn't reflect. The test set had correct answers. Production had confused people trying to figure out if they should go to the emergency room.

A/B testing also captures metrics offline eval cannot measure. User satisfaction, task completion rate, time to resolution, escalation frequency, repeat contact rate, downstream conversion, long-term retention—these are behavioral outcomes that only manifest in real usage. Your offline precision and recall scores don't tell you whether users actually trust the agent's answers enough to act on them. A financial services company discovered this in November 2025 when their new investment recommendation agent scored 94% on offline relevance metrics but saw 31% lower click-through rates in A/B testing because users found the more concise responses less trustworthy than the existing agent's more detailed explanations. Offline eval measured correctness. A/B testing measured persuasiveness. The company needed both.

The production environment also tests your system's resilience to real-world conditions. Load spikes, database slowness, API timeouts, rate limits, cache misses, concurrent users, regional differences—offline eval runs in clean lab conditions. A/B testing runs in production chaos. A logistics company's new route optimization agent looked flawless in offline testing with 200 millisecond average latency. In A/B testing during peak hours, latency spiked to 3.4 seconds because the new architecture made four sequential LLM calls instead of two parallel calls, and under load the queue depth multiplied delays. Offline eval tested the algorithm. A/B testing tested the system.

You run offline eval continuously during development to catch regressions and validate improvements before they reach production. You run A/B tests to prove that offline improvements actually translate to production value. The offline eval is your unit test. The A/B test is your integration test with reality. You need both to ship agents safely.

## Designing Agent A/B Tests with Statistical Rigor

Agent A/B testing requires larger sample sizes and longer durations than traditional feature A/B tests because agent outputs are high-variance and success metrics are often delayed. A change to a button color shows impact immediately in click rates. A change to an agent architecture might affect user satisfaction that only gets measured in a survey sent three days later, or impact customer retention that takes six weeks to manifest. You must design for this variance and delay.

Start with power analysis to determine minimum sample size. For agent quality metrics like task success rate, you typically need to detect differences of 2-5 percentage points to be meaningful. If your current agent has an 82% success rate, you want to detect whether the new agent achieves 84% or higher. Assuming 80% statistical power and 95% confidence, detecting a 2 percentage point difference at an 82% baseline requires approximately 8,000 samples per variant. At 1,000 agent interactions per day, that's eight days minimum before you can make a decision. Many teams run tests for two weeks to capture weekly seasonality patterns. An e-commerce company testing a new product recommendation agent ran their test for 21 days because weekday and weekend shopping behavior differed significantly, and they needed at least three full weekly cycles to achieve confidence.

You must also account for multiple comparisons if you're testing multiple metrics simultaneously. If you measure task success rate, average handle time, escalation rate, customer satisfaction, and cost per conversation, you're running five hypothesis tests. With a 95% confidence threshold, you have a 23% chance of finding at least one false positive purely by chance. Apply Bonferroni correction by dividing your significance threshold by the number of comparisons. With five metrics, use 99% confidence (0.05 divided by 5 is 0.01) instead of 95% to maintain experiment-wide error control. Alternatively, designate one primary metric as your decision criterion and treat others as secondary guardrails. The e-commerce company made task success their primary metric and required that the new agent not degrade average response time by more than 10% or increase costs by more than 15%, treating those as constraints rather than joint optimization targets.

Randomization unit selection matters enormously for agents. You typically randomize by user session, not by individual request, because agent behavior is stateful and conversational. If you randomize per request, a single user might get different agent versions across turns in the same conversation, creating incoherent experiences and invalidating your measurements. A healthcare chatbot made this mistake in August 2025, assigning agent variants per message instead of per session. Users who asked follow-up questions sometimes got responses from a different agent architecture that lacked context from the previous turn, causing confusion and artificially inflating escalation rates for both variants. The test results were meaningless because the treatment wasn't cleanly applied.

For B2B agents serving organizational accounts, you may need to randomize by company or team rather than by individual user to avoid contamination. If users within the same company compare notes and notice they're getting different agent behaviors, it creates confusion and support burden. A sales enablement company testing a new deal analysis agent randomized by individual sales rep and saw complaints flood in when reps on the same team compared outputs and got different answer formats. The second test randomized by team and ran cleanly.

You also need to decide on traffic allocation. Start conservatively—typically 5% to 10% traffic to the new variant and 90% to 95% control. This limits blast radius if the new agent performs worse. As confidence builds, you can increase allocation. Some teams use dynamic allocation algorithms that shift more traffic to the winning variant as significance emerges, but this requires careful statistical adjustment to avoid biasing your final results. The simpler approach is fixed allocation until you reach a decision threshold, then either roll out fully or kill the variant.

## What to Test: Patterns, Prompts, Models, and Parameters

Agent A/B testing covers a wide range of changes, each requiring different test design considerations. The most common categories are architecture patterns, prompt variations, model upgrades, and parameter tuning. Each has different expected effect sizes and variance characteristics.

**Architecture pattern testing** compares fundamental agent designs—ReAct versus chain-of-thought versus reflection versus tool-use patterns. These are large structural changes that typically show clear differences if they exist, but they also have high variance because different patterns excel on different task types. When testing architecture changes, you need longer test durations and larger samples because the effect may be heterogeneous across your traffic mix. A customer support company tested ReAct versus a new hierarchical planner architecture in December 2025. Overall success rates were statistically identical at 79.2% versus 79.8%. But segmented analysis revealed the planner excelled on multi-step workflows (87% versus 81%) while ReAct performed better on single-question lookups (76% versus 71%). The company ended up deploying both, routing by detected task complexity. This kind of nuanced outcome only emerges with sufficient sample size to power subgroup analysis.

**Prompt testing** compares variations in instructions, examples, formatting, or tone while keeping the architecture constant. Prompt changes can have surprisingly large effects—5% to 15% swings in success metrics are common—but they're also sensitive to specific task distributions. A legal tech company tested three prompt variants for their contract analysis agent: one emphasizing accuracy and thoroughness, one emphasizing speed and conciseness, one balancing both. The thorough prompt won on complex contracts but lost on simple NDAs. The concise prompt won on NDAs but missed critical clauses in employment agreements. The balanced prompt performed second-best on both but had the highest overall consistency. The company chose the balanced prompt because predictability mattered more than peak performance on any single contract type.

Prompt tests typically need smaller samples than architecture tests because the variance is lower—you're not changing reasoning patterns, just guidance. You can often reach 95% confidence with 3,000 to 5,000 samples per variant for prompt changes, versus 8,000 to 12,000 for architecture changes. But you should still run for at least one full week to capture daily and weekly patterns.

**Model upgrade testing** compares different LLM versions or providers while keeping prompts and architecture constant. This is critical when new models release. In January 2026, GPT-5.1 launched with significant improvements in reasoning and instruction-following. Dozens of production agents upgraded immediately without testing, assuming newer meant better. Several teams saw regressions. A financial analysis agent's accuracy dropped 6% because GPT-5.1's more verbose responses triggered downstream parsing failures in the team's output validation logic that expected GPT-5's terser format. A content moderation agent's false positive rate doubled because GPT-5.1 applied stricter interpretation of ambiguous guidelines. Both issues were caught in A/B testing by the teams that ran controlled experiments instead of blind upgrades.

Model tests should measure both quality and cost. Newer models are often more expensive. If GPT-5.1 costs 40% more than GPT-5 but only improves task success by 3%, the cost-benefit trade-off may not favor the upgrade depending on your margins. A customer service company calculated that their 3% success improvement from GPT-5.1 reduced human escalations by 2.1%, saving $0.18 per conversation in support costs, but the model upgrade cost $0.31 more per conversation. The net impact was negative $0.13 per conversation at their scale of 400,000 monthly conversations, costing an extra $52,000 per month. They stayed on GPT-5.

**Parameter tuning** tests configurations like temperature, top-p, max tokens, or timeout thresholds. These are smaller changes with smaller expected effects, typically 1% to 3% differences. You need large samples to detect small effects with confidence. A hiring assistant tested five temperature settings from 0.0 to 1.0 in increments of 0.25. Differences were subtle—response diversity increased with temperature but so did hallucination rate. The optimal setting of 0.4 emerged only after 50,000 samples, showing a statistically significant 1.8% improvement in candidate satisfaction over the baseline 0.7 setting. Small effects require large samples.

## Segmentation and Heterogeneous Treatment Effects

The overall A/B test result often masks important variation across user segments, task types, or contexts. A new agent might perform better overall but worse for your most valuable customers. Or better on weekdays but worse on weekends. Or better in English but worse in Spanish. Segmentation analysis reveals these heterogeneous treatment effects and enables targeted deployment strategies.

Pre-define your key segments before the test begins based on dimensions you believe might moderate the treatment effect. Common segments include user tenure (new versus returning), task complexity (simple versus multi-step), language, geography, device type, time of day, and user demographic or firmographic attributes. A travel booking agent pre-defined segments by trip complexity (domestic versus international), booking timeline (last-minute versus advance), and user status (loyalty member versus guest). The test revealed that the new agent's proactive suggestion feature delighted loyalty members (22% satisfaction increase) but annoyed guests who perceived it as pushy (8% satisfaction decrease). The company deployed the new agent only to loyalty member sessions.

Be cautious about post-hoc segmentation—slicing data into subgroups after seeing results to find where effects are significant. With enough subgroups, you will find spurious patterns by chance. If you segment by 20 different dimensions, you expect one false positive at 95% confidence. Only interpret subgroup effects that you hypothesized in advance or that are large enough to survive multiple comparison correction. A marketing automation company found that their new content generation agent performed 11% better for users in the Pacific time zone compared to Eastern time zone in post-hoc analysis. This made no theoretical sense. Further investigation revealed it was a spurious correlation—the Pacific segment happened to contain more users in the technology industry, and industry was the real moderator. They re-ran the test with industry as a pre-specified segment and confirmed the effect.

Segmentation also enables stratified rollout. Instead of deploying to 100% of traffic or killing the variant entirely, you can deploy to segments where the effect is positive and withhold from segments where it's neutral or negative. This maximizes value while limiting downside. A financial advice agent showed overall neutral results—84.1% success for control versus 84.3% for treatment, not statistically significant. But segmented analysis showed 89% success on retirement planning questions versus 81% control, and 78% success on investment questions versus 86% control. The team deployed the new agent only for retirement planning queries, routing other questions to the original agent. This selective deployment captured most of the value while avoiding the regression.

You can also use segmentation to understand why a treatment worked or failed. If a new agent performs better for complex tasks but worse for simple tasks, that suggests the architecture has higher capability but also higher overhead or latency. If it performs better for experienced users but worse for new users, that suggests the interaction model requires learning or familiarity. These insights inform future iterations. The legal tech company that found their thorough prompt won on complex contracts but lost on NDAs used that insight to build a complexity classifier that routed to different prompts based on contract type, combining the best of both approaches.

## Instrumentation and Metric Collection for A/B Tests

Agent A/B testing requires careful instrumentation to collect both immediate and delayed metrics without biasing results. You need to log treatment assignment, agent outputs, user behaviors, downstream outcomes, and guardrail metrics, all keyed to a consistent experiment ID that survives across sessions and asynchronous events.

Treatment assignment must be deterministic and consistent. Generate a hash of the user ID or session ID and use modulo arithmetic to assign to control or treatment. This ensures the same user always gets the same variant throughout the experiment, preventing crossover contamination. A SaaS company used randomized assignment on each session, causing some users to experience both variants across different days. The users who happened to see both variants had 19% lower satisfaction scores than users who consistently saw one variant, polluting the overall results. Consistent assignment is not optional.

Log the variant assignment, timestamp, user ID, session ID, and any relevant context attributes at the moment of first agent interaction. This is your experiment assignment record. Every subsequent log entry—agent turns, tool calls, outputs, user actions—must include the same session ID so you can join them back to the assignment record during analysis. Use a structured logging format with consistent schema. A hiring platform logged assignment records to one database table and conversation records to another without a shared session key, making it impossible to link treatments to outcomes. They had to re-run the test after fixing instrumentation.

Collect both online metrics (measured during the interaction) and offline metrics (measured after the interaction completes). Online metrics include response latency, number of turns, tool call counts, token usage, error rates, and user engagement signals like message length or dwell time. Offline metrics include task completion (did the user accomplish their goal), satisfaction scores from post-interaction surveys, downstream conversion or retention measured days or weeks later, and human review ratings of conversation quality. A customer support agent collected online metrics in real-time but relied on CSAT surveys sent 24 hours after the interaction. They had to wait for survey responses before analyzing test results, extending experiment duration from one week to two weeks to accumulate sufficient survey completions.

Handle delayed metrics carefully. If you measure seven-day retention, you cannot evaluate users who entered the test in the last seven days. Your effective sample size for retention analysis is smaller than your sample size for immediate metrics. Plan for this in your power analysis. The travel booking agent measured 30-day repeat booking rate as a key metric. Their test ran for 45 days—15 days to accumulate traffic plus 30 days for the retention window to complete for the earliest users. Even then, their retention sample was only users from days 1-15, not the full 45-day traffic, reducing statistical power.

Implement guardrail metrics to catch catastrophic failures early. Monitor error rates, timeout rates, cost per conversation, extreme latency outliers, and safety violations. Set automatic kill switches that stop the experiment if guardrails breach thresholds. A financial advisory agent set a kill switch at 5% error rate, double their normal 2.5% baseline. On day three of testing, the new variant hit 5.2% errors due to a bug in tool call parsing, and the system automatically stopped routing traffic to the treatment and paged the on-call engineer. Without the kill switch, the test would have continued degrading user experience for another four days until the scheduled analysis checkpoint.

## Interpreting Results and Making Deployment Decisions

Once your A/B test reaches sufficient sample size and duration, you must interpret results with appropriate statistical rigor and make a defensible deployment decision. This is not about whether the new agent is better in absolute terms—it's about whether the evidence is strong enough to justify change given the risks and costs.

Start by checking that your test ran cleanly. Verify traffic allocation matches the intended split—if you planned 10% treatment but actually got 8%, investigate why. Check that assignment was balanced across key segments—if your treatment group happened to be 68% mobile users while control was 51% mobile, you have a sampling imbalance that will confound results. Verify there were no outages, deployment issues, or data collection gaps during the test window. A logistics company discovered on day 12 of a 14-day test that their new agent variant had been silently failing to log tool calls due to a schema mismatch, making half their instrumentation data unusable. They had to restart the test.

Calculate your primary metric with confidence intervals, not just point estimates. If task success is 82.3% for control and 84.1% for treatment, the point estimate shows a 1.8 percentage point improvement. But the 95% confidence interval might be -0.2% to 3.8%, meaning the true effect could be anywhere from a slight regression to a moderate improvement. If the confidence interval includes zero, you do not have statistical significance. Do not deploy based on directional point estimates without significance. A sales intelligence agent showed a 2.1% improvement in lead quality scores, but the confidence interval was -1.2% to 5.4%. The team deployed anyway because the point estimate was positive. Three months later, long-term analysis showed the true effect was actually -0.4%, a slight regression. The test had been underpowered, and the positive point estimate was noise.

Check your secondary metrics and guardrails. Even if the primary metric improved, you should not deploy if costs increased by 40%, latency doubled, or safety violations tripled. Define acceptable trade-off thresholds in advance. The e-commerce recommendation agent set a rule that they would deploy if task success improved by at least 2% AND cost per conversation increased by no more than 15% AND latency increased by no more than 200 milliseconds. The new agent hit 3.2% success improvement but 18% cost increase, violating the cost guardrail. They went back to optimize the architecture to reduce costs before re-testing.

Examine subgroup effects to understand where the treatment worked and where it didn't. If the overall effect is positive but driven entirely by one segment, you need to decide whether to deploy broadly or selectively. If the effect is inconsistent across segments with no clear pattern, that suggests the treatment is unreliable and may not generalize. The healthcare symptom checker showed 6% improvement in task success overall, but the improvement was 14% for users over 55 and -2% for users under 30. The company investigated and found that the new agent's more cautious recommendations resonated with older users who wanted thoroughness but frustrated younger users who wanted speed. They deployed the new agent only to users over 45, using age-based routing.

Make a deployment decision with explicit criteria. The options are full rollout, partial rollout to winning segments, continued iteration and re-test, or kill the variant. Full rollout is appropriate when the primary metric improved significantly, guardrails held, and effects are consistent across key segments. Partial rollout is appropriate when effects are heterogeneous but positive in valuable segments. Re-test is appropriate when results are directionally positive but not statistically significant—you may need a longer test or larger sample. Kill is appropriate when the treatment underperformed or when the improvement is too small to justify the operational complexity of maintaining a new architecture.

Document your decision rationale with the data. A written decision memo should include the primary metric result with confidence intervals, secondary metrics and guardrails, key subgroup effects, sample size and test duration, statistical significance levels, and the deployment decision with justification. This creates institutional memory and accountability. When the legal tech company decided to deploy their new contract agent to only complex contracts based on subgroup analysis, they wrote a three-page memo explaining the decision with supporting data. Six months later, when a new PM questioned why they were running two different agent architectures, the memo provided the complete rationale and prevented re-litigating a decision that had been made with rigor.

## Operational Considerations for Running Continuous A/B Tests

Agent A/B testing is not a one-time event—it's a continuous practice. As you iterate on architectures, prompts, models, and features, you should always be running controlled experiments to validate changes before full deployment. This requires operational infrastructure and team processes.

Build a reusable experimentation platform that makes A/B tests easy to launch and analyze. The platform should handle treatment assignment, consistent variant routing, metrics collection, statistical analysis, and dashboards. Many teams use existing tools like Optimizely, LaunchDarkly, or Statsig, configuring them for agent-specific metrics. Others build custom platforms integrated with their agent orchestration layer. A fintech company built an experimentation SDK that let any engineer launch an agent A/B test with a configuration file specifying the variants, traffic allocation, primary metric, guardrails, and test duration. The system automatically handled assignment, logging, analysis, and alerting. This reduced the friction of running tests from a two-day setup process to a 20-minute configuration task, increasing experimentation velocity from one test per quarter to two tests per month.

Establish a review process for experiment proposals. Before launching a test, require a brief proposal document specifying the hypothesis, expected effect size, primary and secondary metrics, segment definitions, sample size calculation, test duration, and deployment criteria. This prevents poorly designed tests from wasting time and traffic. A healthcare company required all A/B test proposals to be reviewed by a data scientist who verified statistical rigor before launch. This caught several underpowered tests that would have run for weeks without reaching significance and several poorly defined metrics that would have been uninterpretable.

Run experiments sequentially, not in parallel, unless you have extremely high traffic and can ensure no interactions between tests. If you're testing both a new architecture and a new prompt simultaneously on overlapping traffic, you cannot isolate which change caused observed effects. A marketing automation company ran three agent tests in parallel on the same user base and saw confusing results—one variant showed improvement, but they couldn't tell if it was the prompt change, the model upgrade, or the interaction between them. They re-ran the tests sequentially, which took longer but produced interpretable results.

Create a knowledge base of past experiments and results. Tag each test with the component tested (architecture, prompt, model, parameters), the result (win, loss, neutral), and key learnings. This prevents redundant testing and builds institutional knowledge about what works. The travel booking company maintained a wiki of all agent experiments with results summarized in one-paragraph takeaways. New team members read through the experiment history as part of onboarding, learning that temperature above 0.6 degraded accuracy, that the reflection pattern worked poorly on simple queries, and that users strongly preferred bulleted output formats over paragraph formats for itinerary summaries. This compressed years of learnings into a few hours of reading.

## The Long-Term Experimental Mindset

The most sophisticated agent teams treat production as a continuous experiment. They don't just test major changes—they test everything. Prompt tweaks, formatting changes, minor model version updates, parameter adjustments—every change goes through A/B testing before full rollout. This discipline prevents silent degradations and builds a culture of evidence-based decision making.

This mindset requires accepting that most experiments will show neutral or negative results. That's not failure—it's learning. A content generation company ran 23 agent experiments in 2025. Eight showed significant improvements and were deployed. Eleven showed neutral results and were shelved. Four showed regressions and were killed. The team celebrated the 15 experiments that prevented bad deployments just as much as the eight that shipped improvements. Each neutral or negative result refined their understanding of what mattered and what didn't.

The experimental mindset also means quantifying intuition. When a product manager says "I think users would prefer more conversational responses," the answer is not yes or no—it's "let's test it." When an engineer says "this new architecture should be faster," the response is "let's measure it in production." Opinions become hypotheses. Hypotheses become experiments. Experiments become data. Data drives decisions. A sales enablement company embedded this so deeply that their internal shorthand for "let's test it" became "science it," as in "we should science that prompt change before shipping it." The language shift reflected a cultural shift toward empiricism.

The next subchapter covers canary deployments and shadow mode testing, techniques that complement A/B testing by reducing risk during the transition from experiment to full production rollout.

