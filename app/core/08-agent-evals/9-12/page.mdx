# 9.12 â€” Agent Observability Anti-Patterns: Over-Logging and Under-Tracing

In December 2025, a healthcare technology company running medical triage agents discovered that their observability costs exceeded their compute costs by 40%. They were writing 2.3 terabytes of logs per day to CloudWatch, spending $47,000 monthly on log ingestion and storage alone. When a critical production incident occurred where agents incorrectly triaged chest pain symptoms, the engineering team faced a different problem: the sheer volume of DEBUG-level logs made it impossible to find the relevant execution trace. After six hours of log searching, filtering, and correlation attempts, they finally identified the failure in a malformed prompt template, but the diagnostic delay meant the issue had impacted 1,100 patient interactions. The root cause was not insufficient observability but pathological observability: the team had instrumented every function entry and exit, every variable assignment, every conditional branch, generating gigabytes of logs per agent turn while never implementing distributed tracing. Three months later, after removing 85% of log statements and adding OpenTelemetry spans at semantic boundaries, they reduced observability costs by $38,000 monthly and cut median debugging time from 90 minutes to 11 minutes.

## The Over-Logging Anti-Pattern

Over-logging happens when teams treat logging as the primary observability mechanism and add log statements reactively every time a bug is hard to diagnose. Each debugging session leaves behind a trail of new log lines intended to prevent that specific bug from being hard to diagnose again. Over months and years, the codebase accumulates thousands of log statements, most of which fire on every request, producing vast volumes of redundant data that obscure the actual signals needed for debugging. The intuition that more logging equals better observability is false. More logging equals more noise, higher costs, slower log queries, and cognitive overload when trying to reconstruct execution flow from unstructured text.

The classic symptom is log files dominated by DEBUG or INFO level messages that document normal operations. You see log lines like "entering function process_user_input", "user input validated successfully", "calling model API", "model API returned 200 OK", "parsing model response", "parsed response successfully", "updating conversation state", "conversation state updated successfully". These messages provide no information during normal operation because success is the default assumption. They become actively harmful during failure investigation because the failure-related logs are buried in thousands of success logs. Searching for "error" in a log file with 100,000 lines where 99,950 lines are routine success messages is slower than searching a file with 100 lines where 50 are errors. The signal-to-noise ratio determines debugging speed more than absolute data volume.

Another manifestation is logging complete data structures at every transformation step. The agent receives user input, logs it. Constructs a prompt, logs it. Calls the model, logs the full prompt and full response. Parses the response, logs the parsed structure. Validates the structure, logs the validation result. Transforms the structure, logs the transformed result. By the end of a single turn, you have logged the same data in six different representations, consuming megabytes of log storage and making it nearly impossible to identify which logged version corresponds to which processing step without reading timestamps and correlating across dozens of log lines. This is not observability; this is data hoarding driven by fear that you might someday need to reconstruct exactly what the prompt looked like before the third transformation step.

Logging sensitive data is both a security risk and an observability smell. If your logs contain full user messages, personal health information, API keys, or proprietary business data, you have violated data governance policies and you have also logged information you cannot effectively use for debugging because it cannot be indexed, searched, or shared with external support teams without triggering compliance reviews. The right approach is structured logging with explicit inclusion of debugging-relevant attributes (conversation ID, model name, token counts, latency, error codes) and explicit exclusion of sensitive content. Logs should answer questions like "which turns took longer than five seconds" and "which model invocations returned errors," not "what did the user say in conversation 47392."

## The Under-Tracing Anti-Pattern

Under-tracing happens when teams adopt distributed tracing as a checkbox feature but fail to instrument critical execution paths, leaving gaps that make traces useless for debugging complex failures. You see traces with a single root span labeled "agent turn" and no child spans showing model invocations, tool executions, or retrieval queries. The trace tells you that the turn took 8.3 seconds and failed with an error, but it does not tell you whether those 8.3 seconds were spent in model inference, waiting for a tool response, retrying a failed API call, or deadlocked in state management. Without child spans, the trace is no better than a single log line with start time, end time, and error status.

A related failure mode is instrumenting only the happy path. The agent creates spans for successful model invocations but not for retries. It creates spans for tool executions that succeed but not for timeouts or rate limit errors. When a trace represents a successful turn, it looks complete and useful. When a trace represents a failed turn, half the execution path is missing because error handling code paths were never instrumented. Debugging failures with incomplete traces requires falling back to logs, defeating the purpose of tracing. The correct approach is instrumenting every code path: the primary path, the retry path, the fallback path, the error path, the timeout path. If a function can fail in three different ways, there should be spans or span events capturing which failure mode occurred.

Insufficient span attributes render traces structurally complete but semantically useless. You create a span for a model invocation, but you do not record which model was called, how many tokens were sent, how many tokens returned, or what the finish reason was. The trace shows that a model invocation took 2.1 seconds, but when you try to correlate latency with model choice or prompt size, the data is not there. You create a span for a tool execution, but you do not record the tool name, input size, output size, or error code. When you query for all traces involving a specific tool, you cannot filter by tool name because it was never recorded as an attribute. Spans without rich attributes are skeleton traces: they show structure but not substance.

Failing to propagate trace context across asynchronous boundaries fragments traces into disconnected segments. The agent publishes a task to a queue and creates a span for the publish operation. The worker consumes the task and creates a new span for processing, but because trace context was not encoded into the message metadata, the worker's span is not linked to the publish span. You have two separate traces with no causal relationship visible in the tracing backend. To correlate them, you must manually search for a shared identifier like conversation ID, defeating the entire purpose of distributed tracing, which is automatic correlation. Proper instrumentation means injecting trace context into every message, HTTP header, database record, or RPC call that crosses an execution boundary, and extracting that context on the receiving side to parent new spans correctly.

## The Logging-as-Metrics Anti-Pattern

Teams sometimes use log aggregation queries as a substitute for proper metrics. They count occurrences of specific log messages (grep for "model invocation succeeded" and divide by time period to compute throughput) or parse latency values from log messages (extract "latency: 320ms" strings and compute percentiles). This approach is slow, expensive, and fragile. Log aggregation queries that scan gigabytes of text to compute a single number take seconds or minutes to execute, making them unsuitable for dashboards that need sub-second refresh rates. Parsing unstructured log text is brittle: if the log message format changes, the query breaks silently and returns incorrect results.

Metrics systems are designed for high-cardinality dimensional aggregation and efficient time-series storage. A counter for "agent_turns_total" labeled by model, status, and error_type costs a few bytes per increment and can answer queries like "how many turns using GPT-4o failed with timeout errors in the last hour" in milliseconds. The equivalent log-based query requires scanning millions of log lines, filtering by text pattern, grouping by extracted fields, and counting matches. The performance difference is orders of magnitude. Using logs for metrics is like using a text file as a database: it works for tiny scales and becomes unusable at production scale.

The correct architecture is metrics for aggregations, logs for unstructured context, and traces for causality. Metrics answer "how many" and "how fast" questions. Logs answer "what happened" and "what did the error message say" questions. Traces answer "why was it slow" and "where did it fail" questions. Trying to make one observability primitive serve all three purposes produces systems that are expensive, slow, and hard to use. The cost of running three specialized systems is lower than the cost of overloading one system with inappropriate use cases.

## The Alert Fatigue Anti-Pattern

Over-instrumented systems produce alert fatigue when teams create alerts for every logged error without understanding error semantics or acceptable error rates. An agent system might log a WARNING every time a tool retry succeeds on the second attempt. Retries are expected behavior in distributed systems. If your SLO tolerates 1% tool failures and you retry failed tools, you will see retries on 1% of tool invocations, generating thousands of WARNING logs per day. If you create an alert that fires on any WARNING log, you will receive thousands of alerts per day, train yourself to ignore them, and miss the one alert per month that represents a real production issue.

The correct approach to alerting is defining service level objectives, instrumenting SLI metrics, and alerting only when SLOs are violated. If your SLO states that 95% of agent turns must complete in under five seconds, instrument a histogram metric for turn latency, compute the 95th percentile over a rolling window, and alert when it exceeds five seconds for more than 10 minutes. This alert fires rarely, is actionable when it fires (you are violating user expectations), and is tied to business impact. Alerting on raw error counts without context is noise. Alerting on SLO violations is signal.

Another source of alert fatigue is alerting on symptoms without providing diagnostic context. An alert that says "turn latency exceeded threshold" without linking to a trace, a dashboard, or a runbook forces the on-call engineer to start debugging from scratch. An alert that says "turn latency exceeded threshold: 95th percentile is 7.2 seconds, up from 3.1 seconds baseline, primarily in GPT-4o invocations, example trace: trace-id-12345" gives the engineer everything needed to start investigating immediately. The alert includes the metric that violated the threshold, the baseline for comparison, the dimensional breakdown showing which component is slow, and a concrete trace exemplifying the problem. This is professional alerting discipline.

## The Missing Runbook Anti-Pattern

Observability without runbooks is data without decision procedures. Your dashboards show that tool timeout rates have doubled in the last hour, but your team does not know whether that is expected behavior during this time of day, whether it is caused by a known upstream service degradation, or whether it requires immediate mitigation. Without a runbook mapping observable conditions to response procedures, engineers either over-react to normal variance or under-react to actual incidents. The on-call engineer pages the team lead at 3 AM for a metric fluctuation that happens every night at this time. The on-call engineer ignores a 10x increase in error rates because they do not know that this specific error type indicates a critical database failure.

Runbooks encode institutional knowledge about system behavior. They document baseline metric values, normal daily and weekly patterns, known causes of metric deviations, and step-by-step debugging procedures. A runbook for "tool timeout rate increased" might say: baseline is 0.3%, normal variance is plus or minus 0.1%, increases above 0.5% warrant investigation. First check the external API status page for the tool's backing service. If the service is degraded, increase timeout threshold and retry count; create an incident; notify stakeholders. If the service is healthy, query traces for example timeouts, look for common tool input patterns, check if a recent deployment changed tool invocation logic. This runbook turns an ambiguous metric spike into a structured investigation.

Runbooks should be tested during chaos drills. Inject a synthetic failure, verify that alerts fire, verify that the runbook's diagnostic steps correctly identify the failure, verify that the runbook's mitigation steps resolve the failure. If the runbook says to check the external API status page but the status page does not actually report the failure you injected, update the runbook to add a fallback diagnostic step. If the runbook says to increase timeout thresholds but your configuration management system does not support dynamic timeout changes, either build that capability or update the runbook to reflect the actual mitigation process. Runbooks that have never been executed in practice are fiction.

## The Instrumentation Drift Anti-Pattern

Instrumentation drift occurs when observability code falls out of sync with application code. A new tool is added to the agent, but no one adds spans for its invocations. A prompt template is refactored, but the span attributes recording prompt metadata are not updated to reflect the new structure. A configuration parameter is added to control retry behavior, but no metrics track how often the new retry logic triggers. Over time, the observable system diverges from the actual system, and debugging sessions reveal that the traces and metrics you rely on no longer correspond to production behavior.

The root cause is treating instrumentation as a one-time setup task rather than a continuous maintenance discipline. Instrumentation should be part of the definition of done for every feature. When you add a new tool, the pull request must include spans for tool invocation, metrics for tool latency and error rates, and updates to dashboards and alerts. When you refactor a prompt, the pull request must verify that span attributes still capture the relevant metadata. When you add a configuration parameter, the pull request must include metric labels or span attributes that make the parameter's effect observable. Instrumentation is not optional metadata; it is the interface through which operators understand system behavior.

Code reviews should include observability review. The reviewer asks: if this code path fails in production, will we have a trace showing where it failed? If this code path is slow, will we have metrics showing how often it is slow? If this configuration change causes a regression, will we have alerts that fire before users complain? If the answer to any of these questions is no, the pull request needs additional instrumentation before merging. This discipline prevents instrumentation drift and ensures that every code path is observable from the day it deploys.

## The Unsampled Trace Storage Anti-Pattern

Storing every trace from a high-traffic agent system is prohibitively expensive and operationally useless. If your agent processes one million turns per day and each trace is 50 kilobytes, you generate 50 gigabytes of trace data daily. At $0.10 per gigabyte for ingestion and $0.05 per gigabyte per month for storage, you spend $5,000 monthly on ingestion and $7,500 monthly on storage after five months. Most of those traces represent successful, fast turns that provide no debugging value. The traces you need are the ones representing errors, slow performance, or unusual execution patterns, which might be 1% of total traffic. Storing the other 99% is waste.

Tail-based sampling solves this by keeping all traces that match error or latency criteria and sampling only a fraction of normal traces. Configure your tracing backend or OpenTelemetry Collector to retain all traces where any span has an error status, all traces where the root span duration exceeds five seconds, and 1% of other traces selected randomly. This ensures you have complete data for debugging problems while controlling storage costs. The 1% sample of normal traces provides baseline data for comparison: when you investigate a slow trace, you can compare it to sampled fast traces to identify what is different.

Retention policies should align with debugging needs. Keep high-resolution trace data for seven days to support active incident investigation. After seven days, downsample to 0.1% of traces and retain for 90 days to support long-term trend analysis and quarterly performance reviews. After 90 days, delete traces or archive them to cold storage. This tiered retention strategy gives you detailed data when you need it and historical context without paying for gigabytes of rarely-accessed traces in hot storage.

## The Missing Error Taxonomy Anti-Pattern

Logging errors without classification makes it impossible to distinguish between routine, expected errors and critical, unexpected errors. An agent system might encounter tool timeouts, model rate limits, malformed user inputs, retrieval query failures, and state store deadlocks. Each of these is an error, but they have different severities, different mitigation strategies, and different business impacts. Logging them all as generic errors with unstructured messages forces humans to read error messages and manually classify them during every debugging session.

A proper error taxonomy assigns error codes, severities, and categories. Tool timeouts are E1001, severity WARN, category transient_tool_failure. Model rate limits are E1002, severity ERROR, category external_service_degraded. Malformed user inputs are E2001, severity INFO, category user_error. Retrieval query failures are E3001, severity ERROR, category internal_service_failure. State store deadlocks are E4001, severity CRITICAL, category data_corruption_risk. With this taxonomy, you can query for all CRITICAL errors in the last 24 hours and immediately see state store deadlocks without wading through thousands of INFO-level user input validation failures.

Error codes should be stable across releases. E1001 always means tool timeout, even if the implementation of tool timeouts changes. This stability allows you to build dashboards, alerts, and historical analyses keyed by error code. If E1001 appears 10x more frequently this week than last week, you have a regression in tool reliability. If you change error codes with every refactor, historical comparisons become impossible.

## The Dashboard Proliferation Anti-Pattern

Teams sometimes create dozens of dashboards, each tailored to a specific debugging session or stakeholder request, without consolidating or deprecating old dashboards. You end up with 40 dashboards, most of which are outdated, unmaintained, and showing metrics that no longer exist or no longer matter. During an incident, engineers waste time searching through the dashboard list trying to find the one relevant dashboard, or they create a new dashboard because finding the existing one is harder than starting from scratch. This is observability debt.

The correct approach is a small set of well-maintained dashboards organized by audience and purpose. One dashboard for real-time operational health (turn latency, error rates, tool availability, model availability) used by on-call engineers. One dashboard for resource utilization (compute, memory, API quotas) used by infrastructure teams. One dashboard for business metrics (turns per hour, escalations, user satisfaction) used by product managers. One dashboard per major subsystem (retrieval, tool orchestration, state management) for deep dives. That is five to ten dashboards total, each with a clear owner responsible for keeping it current.

Dashboards should link to traces and runbooks. A graph showing increased latency should have an annotation linking to a pre-filtered trace query showing example slow traces. A panel showing error rate spikes should link to the runbook for investigating that error category. This turns dashboards into launchpads for investigation rather than passive data displays. The engineer sees a problem on the dashboard, clicks the link, and lands directly in the trace viewer or runbook that helps diagnose it.

## Recovery Through Observability Audits

If you recognize your system in these anti-patterns, recovery requires an observability audit. List every log statement, every trace span, every metric, and every alert. For each one, ask: what question does this answer, how often do we use it, what is the cost of collecting and storing it, and what happens if we remove it? Delete log statements that document normal operations. Delete metrics that no one queries. Delete alerts that fire routinely and are ignored. Consolidate redundant dashboards. Add spans to execution paths that lack instrumentation. Add metrics for new configuration parameters. Update runbooks to reflect current architecture.

Measure before and after. Track observability costs, median time to debug incidents, and percentage of incidents where traces provided the root cause. If your audit successfully removes noise and fills gaps, you should see costs decrease, debugging time decrease, and trace utility increase. If those metrics do not improve, your audit removed the wrong things or failed to address the real gaps. Observability is a feedback loop: instrument, measure effectiveness, refine instrumentation, measure again.

Establish instrumentation standards enforced via CI checks. Require that every new RPC handler creates a span. Require that every span recording latency also records a corresponding histogram metric. Require that every alert links to a runbook. These standards codify the lessons from the audit and prevent regression. Without enforcement, instrumentation drift will recur as new engineers join and add code without understanding observability expectations.

The path from observability chaos to observability excellence is not adding more logs or more traces; it is adding the right logs and the right traces, deleting the wrong ones, and treating observability as a maintained product rather than an emergent side effect of development. Agents instrumented with intention produce telemetry that accelerates debugging, supports continuous improvement, and justifies its cost. Agents instrumented carelessly produce telemetry that obscures problems, drains budgets, and trains engineers to ignore alerts. The difference is discipline.
