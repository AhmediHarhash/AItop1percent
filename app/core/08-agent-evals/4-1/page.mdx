# 4.1 — Tool Orchestration vs Tool Calling: The Agent-Level View

On February 8, 2025, a startup called DataSync launched an AI agent designed to help sales teams research prospects. The agent had access to twelve powerful tools: LinkedIn search, company database lookup, news search, financial data API, technology stack detection, employee headcount estimation, funding history retrieval, competitive intelligence search, social media scanning, web scraping, email finder, and CRM integration. Each tool worked perfectly in isolation—the engineering team had tested every single one extensively. The LinkedIn search returned accurate profiles. The company database returned comprehensive firm data. The news search surfaced relevant articles. Every tool did exactly what it was supposed to do. Yet when the agent was given a simple request—"research Acme Corp and identify the best person to contact about our analytics product"—it failed spectacularly. The agent called LinkedIn search and found 2,400 employees. It called company database and got Acme's industry and size. It called news search and retrieved 47 articles. It called technology stack detection and learned Acme used Google Analytics. Then it stopped. It had gathered four pieces of information using four different tools, but it had not connected them, had not filtered the 2,400 employees to find decision-makers, had not cross-referenced the tech stack with the news to understand Acme's analytics pain points, and had not identified a specific contact. The agent could call tools, but it could not orchestrate them. It had twelve instruments but could not play a symphony.

The difference between tool calling and tool orchestration is the difference between knowing how to use individual tools and knowing how to combine them into effective workflows. Tool calling is a single-step operation: you have a tool, you invoke it with appropriate parameters, you get a result. Tool orchestration is multi-step coordination: you decide which tools to use, in what order, with what parameters, how to route results from one tool to the next, how to handle errors, and how to combine outputs into a coherent whole. Tool calling is a mechanical skill. Tool orchestration is a strategic capability.

## Tool Calling: The Foundation

Tool calling, covered extensively in Section 6, is the fundamental skill that enables agent systems to interact with external services and APIs. An agent that can call tools has the ability to execute functions with parameters, receive structured responses, and use those responses in its reasoning. The model generates a function call in a specific format—typically JSON specifying the function name and argument values—and the framework executes that function, injecting the result back into the model's context.

Tool calling requires several technical capabilities. The model must understand the function's purpose from its description. It must map task requirements to function parameters. It must generate syntactically valid function call specifications. The framework must parse those specifications, execute the calls, handle errors, and format results in a way the model can use. These are well-solved problems as of 2026. GPT-4, Claude, and other frontier models excel at tool calling. The function calling APIs from major providers are robust and reliable.

The quality of individual tool calls depends primarily on tool description quality and parameter clarity. If a tool is described clearly with examples of valid usage, models call it correctly nearly all the time. If the description is vague or the parameters are ambiguous, models struggle. But this is a documentation problem, not a fundamental capability gap. Modern models can call well-described tools reliably.

Tool calling in isolation is not sufficient for agent behavior. An agent that can only call one tool at a time, wait for a result, and then decide what to do next is functionally equivalent to a chatbot with function access. It can be useful—many production chatbots use function calling to retrieve data or perform actions—but it is not agentic in the sense of pursuing multi-step goals autonomously. Agentic behavior requires orchestrating multiple tool calls into coherent workflows.

## Tool Orchestration: The Strategic Layer

Tool orchestration is where agent complexity lives. Given a goal and a set of available tools, the agent must construct a workflow: a sequence of tool calls, each using outputs from previous calls, collectively achieving the goal. This requires understanding tool capabilities, reasoning about information dependencies, planning the call sequence, handling partial failures, and integrating results.

The orchestration challenge is fundamentally different from the calling challenge. Calling is about correctness: did the agent invoke the function with valid parameters? Orchestration is about effectiveness: did the agent choose the right sequence of functions to achieve the goal efficiently? Calling has a clear right answer—the parameters match the schema or they do not. Orchestration has many possible answers with different cost, latency, and quality tradeoffs.

Consider the prospect research task from the opening example. Effective orchestration would look like this: Call company database to get Acme's industry, size, and location. Call news search with filters for the past six months to find recent developments. Call technology stack detection to identify current analytics tools. Call LinkedIn search filtered by Acme Corp, job titles containing "analytics" or "data" or "Chief", seniority level senior or above. Cross-reference the LinkedIn results with the news results to see if any of the identified people are mentioned in articles. Call email finder for the top three candidates. Call CRM integration to check if any of the three are existing contacts. Return the best match with reasoning. This eleven-tool-call workflow combines information from multiple sources, uses outputs from earlier calls to refine later calls, and produces a specific actionable answer.

Poor orchestration might call all twelve tools without filtering or sequencing, generating massive amounts of irrelevant data and wasting budget. Or it might call only one or two tools and return incomplete information. Or it might call tools in the wrong order, performing expensive operations that could have been avoided if cheaper tools were queried first. Or it might fail to propagate information from one tool to the next, treating each call as independent rather than as part of a pipeline.

The quality of orchestration determines the quality of the agent. An agent with access to mediocre tools but excellent orchestration will outperform an agent with excellent tools but poor orchestration. Orchestration is the higher-order capability that multiplies the value of individual tools.

## The Five Core Orchestration Decisions

Every orchestration task involves five fundamental decisions: tool selection, sequencing, parallelization, error handling, and result routing. How your agent makes these decisions determines its effectiveness.

Tool selection is the problem of choosing which tools to use from the available set. If the agent has twenty tools and the task requires three or four, which ones should it pick? The agent must match task requirements to tool capabilities. If the task is "find recent news about company X," the news search tool is an obvious choice. But if the task is "understand company X's competitive position," multiple tools might be relevant—news search, competitive intelligence, financial data, employee reviews—and the agent must decide which combination provides the best information.

Sequencing is the problem of ordering tool calls. If the agent needs to call tools A, B, and C, should it call A then B then C, or B then A then C, or some other order? The order matters when later calls depend on earlier results. If tool B needs information from tool A's output, A must be called first. If tool C filters results from both A and B, both must complete before C. The agent must identify dependencies and sequence calls to respect them.

Parallelization is the problem of identifying tool calls that can execute concurrently. If the agent needs to call tools A, B, and C, and none of them depend on each other, calling them in parallel reduces latency. Instead of waiting for A, then B, then C sequentially, the agent can call all three at once and wait for all to complete. Effective parallelization is the difference between 10-second workflows and 3-second workflows when tool calls have network latency.

Error handling is the problem of deciding what to do when a tool call fails. Should the agent retry? Switch to an alternative tool? Skip the step and continue without that information? Abort the entire workflow? The right answer depends on why the call failed, whether the information is essential, and what alternatives exist. An agent that retries transient errors but skips calls to unavailable services demonstrates sophisticated error handling.

Result routing is the problem of determining which tool outputs feed into which subsequent tool inputs. If tool A returns a list of candidates and tool B analyzes individual candidates, the agent must route each candidate from A's output to B's input, possibly calling B multiple times. If tool C aggregates results from both A and B, the agent must collect those results and pass them to C together. Routing is the data plumbing that connects tools into pipelines.

These five decisions interact. Tool selection affects sequencing because some tools must run before others. Sequencing affects parallelization because sequential dependencies prevent parallel execution. Error handling affects tool selection because robust orchestration includes backup tools when primary tools fail. The orchestration problem is not five independent problems—it is one integrated problem with five dimensions.

## Why Orchestration Is Harder Than Calling

Tool calling is a well-defined interface problem. The tool has a schema. The agent must generate JSON that matches the schema. Success is binary: the call either works or it does not. Failure modes are clear: syntax errors, type mismatches, missing required fields, invalid values. These are straightforward to debug and fix.

Tool orchestration is an open-ended reasoning problem. There is no schema for workflows. There are many valid orchestration strategies for any given task, with different tradeoffs. Success is a spectrum: the agent might complete the task poorly, adequately, or excellently. Failure modes are subtle: the agent might choose suboptimal tools, sequence calls inefficiently, miss parallelization opportunities, or handle errors poorly while still technically completing the task.

Calling is local reasoning: given this tool and this task, what parameters should I use? Orchestration is global reasoning: given this goal, these tools, and this context, what sequence of tool uses will achieve the goal efficiently? Local reasoning is easier because the context is small and the decision space is constrained. Global reasoning is harder because the agent must consider many possibilities and their downstream consequences.

Calling can be tested exhaustively. For a tool with five parameters and well-defined valid ranges, you can test all important parameter combinations and verify correct behavior. Orchestration cannot be tested exhaustively because the space of possible workflows is combinatorially large. An agent with ten tools can orchestrate them in billions of different sequences. You cannot test all sequences—you can only test representative examples and hope the agent generalizes.

Calling errors are usually deterministic and reproducible. If the agent generates invalid JSON for a tool call, it will fail the same way every time with the same inputs. Orchestration errors are often context-dependent and non-deterministic. The agent might choose the right orchestration strategy for one instance of a task and the wrong strategy for a slightly different instance. Debugging orchestration requires understanding not just what the agent did, but why it chose that strategy over alternatives.

Models are explicitly trained for tool calling. The function calling APIs from OpenAI, Anthropic, and others include tool calling in their training data and fine-tuning. Models are optimized to generate valid function call syntax. They are not explicitly trained for orchestration—that capability emerges from general reasoning ability combined with instruction following. Emergence is less reliable than explicit training.

## How Orchestration Quality Determines Agent Quality

An agent's ultimate value is determined not by whether it can call tools, but by whether it can orchestrate them effectively to achieve user goals. Tool calling is table stakes—every production agent system in 2026 can call tools. Orchestration quality is the differentiator.

Poor orchestration leads to failed tasks. The agent gathers some information but not the right information. It calls expensive tools unnecessarily and skips cheap informative tools. It produces partial results that are not actionable. It takes 30 seconds to complete a task that should take 5 seconds. It consumes ten dollars in API costs when two dollars would have sufficed. These failures are not catastrophic in the sense of crashing or producing errors—the agent technically functions—but they make the agent unfit for production use.

Mediocre orchestration leads to functional but inefficient agents. The agent achieves the goal most of the time, but not elegantly. It calls redundant tools. It sequences calls suboptimally, causing unnecessary latency. It does not parallelize when it could. It handles errors conservatively, often aborting when alternatives exist. The agent works well enough to be useful but not well enough to be cost-effective at scale.

Good orchestration leads to production-grade agents. The agent selects the right tools for the task. It sequences calls to minimize latency and cost. It parallelizes aggressively where safe. It handles errors gracefully, using fallbacks and retries appropriately. It routes data efficiently from tool to tool. The agent completes tasks quickly, cheaply, and reliably. This is the level required for commercial deployment.

Excellent orchestration leads to agents that surprise users with their effectiveness. The agent finds creative tool combinations that users did not anticipate. It chains tools in ways that extract more value than each tool individually. It adapts orchestration strategies to the specific characteristics of each task instance. It demonstrates something approaching human-level workflow design. This level is rare in 2026 but represents the frontier.

The gap between mediocre and good orchestration is the difference between an internal prototype and a production system. The gap between good and excellent orchestration is the difference between a useful tool and a competitive advantage. Orchestration quality scales with the sophistication of the tasks you attempt and the value you deliver.

## The Gap Between Can Call Tools and Uses Tools Effectively

Many agent systems in 2026 can call tools but cannot orchestrate them. This gap is responsible for the majority of agent deployment failures. Teams build agents, equip them with tools, verify that tool calling works, deploy to users, and discover that the agent is ineffective at real tasks. The problem is not that the tools do not work—the tools work fine. The problem is that the agent does not know how to combine them into useful workflows.

This gap exists because tool calling and tool orchestration are taught and tested differently. When you test tool calling, you give the agent a task that requires one tool, verify that it calls that tool correctly, and move on. When you test orchestration, you give the agent a task that requires multiple tools, observe what sequence it generates, and evaluate whether that sequence makes sense. The second type of testing is harder and less commonly done.

The gap also exists because orchestration is brittle to small changes in prompts, tool descriptions, and model versions. An agent might orchestrate tools well with one system prompt and poorly with a slightly different prompt. A tool might be selected frequently when its description emphasizes capability X but ignored when the description emphasizes capability Y. A model update might change orchestration patterns in subtle ways. Calling is more robust to these variations because the interface is explicit and constrained.

Closing the gap requires deliberate focus on orchestration. You must design tool descriptions with orchestration in mind, not just calling. You must test multi-tool workflows, not just individual tools. You must evaluate orchestration quality explicitly, using metrics like tool selection accuracy, sequencing optimality, parallelization effectiveness, and error recovery robustness. You must iterate on orchestration prompts and instructions separately from tool calling correctness.

Some teams address the gap by providing orchestration examples in prompts. The system prompt includes examples of good workflows: "When asked to research a company, first call the company database for basic info, then use that info to filter news searches, then use news results to identify key people, then find contact details for those people." These examples teach orchestration patterns that the model can generalize.

Other teams address the gap with explicit orchestration frameworks. Instead of relying on the model to decide orchestration, they implement workflow primitives: pipelines, parallel branches, conditional execution, error handling blocks. The model still decides which tools to use, but the framework manages sequencing, parallelization, and error handling. This reduces the orchestration burden on the model and makes workflows more predictable.

The frontier approach is to train or fine-tune models specifically for orchestration. This is expensive and requires high-quality orchestration examples, but it is the only way to make orchestration as reliable as calling. As of 2026, a few specialized providers offer orchestration-tuned models, but this is not yet common in general-purpose models.

## Practical Implications for Agent Design

When you design an agent system, you must decide how much orchestration responsibility to give the model versus how much to encode in framework logic. Full model responsibility means the model decides everything: which tools, what order, what parameters, how to handle errors. Full framework responsibility means the model just calls tools when prompted and the framework handles all orchestration. Most systems fall somewhere in between.

The right balance depends on task predictability. For predictable tasks with known workflows, encode orchestration in the framework. Define explicit pipelines or state machines that specify the tool sequence. The model fills in parameters but does not decide strategy. This is reliable and efficient. For unpredictable tasks where workflows vary by instance, give orchestration responsibility to the model. The model must adapt to each case. This is flexible but less reliable.

Regardless of where you put orchestration responsibility, make orchestration visible. Log the orchestration decisions your agent makes. Record which tools were selected, in what order, with what parameters, and what the results were. This logging is essential for debugging orchestration failures and for improving orchestration over time.

Measure orchestration quality separately from task completion. An agent might complete a task but do so inefficiently. Track metrics like tools called per task, total latency, total cost, parallelization rate, and error recovery success rate. These metrics tell you whether orchestration is effective even when task completion is high.

Finally, remember that orchestration is a skill that improves with iteration. Your first version will have poor orchestration. Your tenth version will be significantly better. Build feedback loops that let you learn from orchestration failures and successes. Test with real tasks. Observe what works and what does not. Refine tool descriptions, prompts, and framework logic based on what you learn. Orchestration quality is not a one-time achievement—it is an ongoing practice.

## Orchestration Patterns That Emerge in Production

As you deploy agent systems and observe their orchestration behavior in production, certain patterns emerge repeatedly. Recognizing these patterns helps you understand orchestration quality and design better systems. The most common pattern is sequential dependency chains: Tool A produces output that becomes input to Tool B, which produces output for Tool C. This is the simplest orchestration pattern and the easiest for agents to execute correctly.

The fan-out pattern is when one tool call produces information that feeds into multiple parallel tool calls. The agent searches for a company, gets basic information, then fans out to multiple tools: news search for that company, financial data lookup, employee reviews, competitive analysis. All these tools can run in parallel because they depend only on the initial search result, not on each other. Good orchestration recognizes this parallelization opportunity. Poor orchestration executes them sequentially, wasting time.

The fan-in pattern is the inverse: multiple tool calls produce results that are aggregated by a final tool or reasoning step. The agent calls three different news APIs, gets results from each, then combines them into a unified summary. The aggregation step depends on all three completing. Good orchestration waits for all three, then aggregates. Poor orchestration might try to aggregate after each result, producing incomplete summaries.

The conditional branch pattern is when tool selection depends on the result of a previous tool call. If Tool A returns result X, use Tool B. If it returns result Y, use Tool C instead. This is common in error handling and adaptive workflows. The agent calls a primary data source, and if it fails or returns insufficient data, it calls a backup source. Orchestration quality depends on correctly detecting the condition and selecting the appropriate branch.

The iterative refinement pattern is when the agent repeatedly calls the same tool or tool type with progressively refined parameters based on previous results. The agent searches for information, gets too many results, narrows the search parameters, searches again, still too many results, narrows further, searches a third time. This pattern is necessary when the agent does not know the right parameters upfront. Good orchestration limits iterations to prevent runaway costs. Poor orchestration either iterates too many times or gives up too early.

The checkpoint-and-resume pattern is when the agent saves intermediate results and resumes from them if a later step fails. The agent gathers data from five sources, saves the results, attempts analysis, fails due to an error, resumes from the saved data without re-gathering. This pattern is valuable for expensive or slow data gathering operations. Orchestration quality depends on identifying which results are worth checkpointing and implementing the save-and-resume logic reliably.

## How Orchestration Complexity Scales With Tool Count

Orchestration difficulty does not scale linearly with the number of available tools. It scales worse than linearly because the number of possible tool combinations grows combinatorially. With five tools, there are hundreds of possible sequences. With ten tools, there are millions. With twenty tools, the space is vast. Your agent cannot exhaustively reason about all possibilities—it must heuristically select promising sequences and evaluate them.

This scaling problem is why most production agents have relatively small tool sets. A agent with six well-chosen tools that cover the necessary capabilities will outperform an agent with thirty tools that overlap and confuse each other. More tools mean more selection decisions, more potential for choosing wrong tools, more cognitive load on the orchestration reasoning, and more opportunities for errors.

When you must provide many tools, structure them into categories or namespaces. Instead of presenting twenty flat tools, present five categories with four tools each. The agent first selects the category, then selects the tool within the category. This two-stage selection reduces the complexity of each decision. The agent reasons about high-level capability categories before reasoning about specific tools.

Another approach is dynamic tool provisioning where the agent sees only the tools relevant to the current context or task type. A data analysis task sees data tools. A communication task sees communication tools. The tool set changes based on what the agent is doing. This requires task classification or explicit user specification of task type, but it dramatically simplifies orchestration by reducing the decision space.

Some teams implement tool recommendations where the system suggests which tools are most likely to be useful for the current task based on task description or past usage patterns. The agent can still select any available tool, but the recommendations guide it toward good choices. This is particularly helpful for new or complex tasks where the optimal orchestration is not obvious.

## The Role of Examples in Teaching Orchestration

Examples are the most effective way to improve orchestration quality. An example shows the agent a complete workflow: here is a task, here are the tools that were used, here is the sequence, here are the results. The agent learns orchestration patterns by observing successful examples and generalizing them to new tasks.

Effective examples are concrete and diverse. Concrete means they show specific tool calls with actual parameters and actual results, not abstract descriptions. The agent sees that for task X, the correct approach was to call SearchTool with parameter A, then call AnalysisTool with the search results as input B, then call SummaryTool with analysis output as input C. The specificity makes the pattern learnable.

Diverse means the examples cover different task types, different orchestration patterns, and different edge cases. If all your examples show sequential chains, the agent will not learn to parallelize. If all your examples use the same three tools, the agent will over-rely on those tools. Diversity in examples leads to generalization. The agent learns that parallel fan-out is appropriate for independent data gathering, that conditional branching is appropriate when primary sources might fail, that iteration is appropriate when parameters are uncertain.

You can provide examples in system prompts, in tool documentation, or in few-shot learning contexts. System prompt examples are always available and teach high-level patterns. Tool documentation examples teach tool-specific usage. Few-shot examples are provided per-task and teach task-specific orchestration. Each has a role. The key is ensuring the agent sees enough varied examples to learn robust orchestration strategies.

In 2026, some teams are building example libraries: databases of successful agent workflows that can be retrieved and provided as context when similar tasks arise. The agent is given a task, the system retrieves similar past tasks and their orchestration, and the agent uses those examples to guide its orchestration decisions. This is case-based reasoning applied to tool orchestration. It works well when you have enough historical data and good similarity matching.

## Debugging Orchestration Failures

When orchestration fails—the agent selects wrong tools, sequences them poorly, or mishandles errors—debugging is harder than debugging tool calling failures because the problem is strategic rather than mechanical. The tool calls might all be syntactically correct, but the combination is ineffective. How do you diagnose what went wrong?

The first step is to make orchestration decisions visible. Log not just what tools were called, but why the agent chose them. If your framework supports it, have the agent output its reasoning: "I am calling Tool A because the task requires data X and Tool A provides that data." This reasoning helps you understand the agent's decision process and identify where it went wrong.

Compare the agent's orchestration to what a human would do for the same task. If the agent called five tools when a human would call two, the agent is over-orchestrating or selecting wrong tools. If the agent called tools sequentially when a human would parallelize, the agent is missing parallelization opportunities. The human baseline helps you identify specific orchestration deficiencies.

Analyze the failure mode. Did the agent select the wrong tool entirely? Did it select the right tool but sequence it incorrectly? Did it fail to parallelize? Did it handle an error poorly? Each failure mode suggests a different fix. Wrong tool selection means improve tool descriptions or selection prompts. Poor sequencing means add dependency reasoning or examples. Missing parallelization means add examples of parallel workflows. Poor error handling means add error handling examples or make error handling explicit in the framework.

Look for patterns in orchestration failures across multiple tasks. If the agent consistently fails to parallelize fan-out patterns, that is a systematic gap that requires a systematic fix—adding examples, modifying prompts, or implementing explicit parallelization hints. If the agent consistently chooses expensive tools when cheap tools would work, that is a cost-awareness gap. Patterns reveal root causes that individual failures might obscure.

Some teams implement orchestration validators that check generated workflows for common mistakes before execution. The validator checks that dependencies are respected in sequencing, that parallelizable calls are marked for parallelization, that error handling is included for fallible operations. If the validator detects issues, it can either fix them automatically or ask the agent to regenerate the orchestration. This catches mistakes before they cause execution failures.

## The Evolution of Orchestration as Agents Gain Experience

In reinforcement learning-based agent systems, orchestration quality improves over time as the agent learns from successes and failures. The agent tries different tool sequences, observes which lead to task completion and which do not, and adjusts its policy to favor successful patterns. Early in training, orchestration is poor and seemingly random. After thousands of tasks, clear strategies emerge.

Even in non-RL systems, you can implement learning-like behavior through success tracking and prompt engineering. Track which tool sequences succeed for which task types. When a new task arises, check if it matches a known task type, and if so, suggest the tool sequence that worked previously. The agent can follow the suggestion or deviate if the specific task instance requires it. Over time, the system builds up a library of proven orchestration strategies.

Another evolutionary approach is to A/B test orchestration prompts. Run the same task with different orchestration instructions—one emphasizing parallelization, one emphasizing cost minimization, one emphasizing reliability—and measure which produces better outcomes. The winning prompt becomes the default. Re-test periodically as the agent and tasks evolve. This is evolutionary optimization applied to orchestration strategies.

The frontier in 2026 is meta-learning for orchestration: training models specifically to orchestrate tools based on large datasets of successful and failed workflows. These models learn to recognize task patterns, match them to orchestration strategies, and generate effective tool sequences. This is expensive to develop but produces agents with near-human-level orchestration capabilities. A few specialized providers offer orchestration-optimized models, but this is not yet mainstream.

## Why Orchestration Quality Is the Limiting Factor for Agent Value

You can have the best tools in the world, the most powerful models, and the most sophisticated infrastructure, but if your agent orchestrates tools poorly, it will not deliver value. Orchestration quality determines what tasks your agent can complete, how efficiently it completes them, how much they cost, and how reliable the results are. It is the ultimate bottleneck.

Poor orchestration means tasks that should succeed fail. The agent has all the capabilities needed but cannot combine them into working solutions. This is the most frustrating failure mode because the potential is there but unrealized. You know the agent could solve the problem if only it orchestrated correctly. Users lose confidence when agents fail at tasks that seem within reach.

Mediocre orchestration means tasks succeed but inefficiently. The agent uses three times as many tool calls as necessary, takes five times as long, and costs ten times as much as optimal. The task completes, but the cost and latency make the agent impractical for production use. Users abandon the agent in favor of manual processes that are faster and cheaper.

Good orchestration means tasks succeed efficiently. The agent selects the right tools, sequences them optimally, handles errors gracefully, and delivers results quickly at reasonable cost. This is the threshold for production viability. Users trust the agent, rely on it for routine tasks, and the economics work. Most successful agent deployments in 2026 operate at this level.

Excellent orchestration means the agent finds creative solutions that users did not anticipate. It combines tools in novel ways, discovers efficiency shortcuts, and handles edge cases smoothly. This is where agents provide genuine value beyond automation—they augment human capability by seeing possibilities humans miss. This level is rare but represents the aspiration.

Tool calling gets you in the game. Tool orchestration determines whether you win. Every agent can call tools in 2026. The agents that succeed are the ones that orchestrate those tools into effective, efficient, reliable workflows. That is the capability you must build, measure, debug, and continuously improve. Orchestration is not a feature—it is the core competency that makes agents valuable.
