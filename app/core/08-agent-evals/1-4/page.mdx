# 1.4 â€” Agent Product Archetypes: Research, Workflow, Decision Support, Coding

In March 2025, a financial services startup called Meridian Analytics deployed what they called a "universal AI agent" to handle everything from customer research to portfolio rebalancing to generating compliance reports. The agent had access to dozens of tools, from web search to database queries to email sending to trade execution APIs. The founding team was proud of their generalist approach. Why build four specialized agents when one could do it all? The unified architecture seemed elegant, the development cost was lower than building separate systems, and early demos impressed investors who saw the agent seamlessly switch between researching market trends and filing regulatory reports.

Three weeks into production, their Head of Compliance discovered the agent had been filing quarterly 13F reports with data pulled from customer research queries instead of audited financial databases. The agent would receive a request like "generate the quarterly 13F filing," correctly identify that it needed holdings data, evaluate its available tools, and choose the web search tool because it had recently used that tool successfully for a similar-sounding task involving portfolio information. The agent would search for the company name plus "portfolio holdings," find news articles estimating positions based on public filings from previous quarters, extract those estimates, and format them into the regulatory report template. The output looked correct. The formatting was perfect, the positions were plausible, the narratives were coherent. But the data was approximated from stale public sources rather than pulled from the internal portfolio management system that contained authoritative real-time positions.

The agent had the right goal, a logically coherent reasoning process, and access to the correct tools. The failure was architectural. The agent lacked guardrails that prevented category errors. Research tasks and compliance tasks lived in the same execution context with the same tool selection logic. The agent learned during training that web search was useful for gathering financial information, and it generalized that pattern to a context where only audited internal data was acceptable. No prompt engineering could fix this. The architecture itself made the failure inevitable. The resulting SEC inquiry cost the company two hundred thousand dollars in legal fees, six months of remediation work, and a permanent note in their compliance record. The universal agent was decommissioned and replaced with four specialized agents, each with tool access scoped to its archetype.

By late 2025, the production agent landscape had crystallized around four dominant archetypes, each with distinct architecture patterns, tool requirements, evaluation needs, and failure modes. You are building agents in 2026, and the first strategic decision you face is not what model to use or how to tune your prompts, but which archetype your use case maps to. Get this wrong and you will spend months fighting architecture mismatches that no amount of prompt engineering can fix. Get it right and the rest of your design decisions flow naturally from the archetype's constraints. The archetypes are not arbitrary categories, they are patterns that emerged from thousands of production deployments, each representing a fundamentally different relationship between the agent and the world it acts upon.

## Research Agents: Information Synthesis Across Sources

Research agents exist to gather, filter, synthesize, and present information from multiple sources in response to open-ended questions. A user asks "what are the leading approaches to battery thermal management in electric vehicles as of 2026" and the agent searches academic papers, patent databases, industry reports, and technical blogs, then produces a structured synthesis with citations. The agent's value is not in executing actions that change state but in reducing information overload and finding signal in noise. The output is knowledge, not effects. The user's environment is unchanged after the agent runs, only the user's understanding has improved.

The canonical research agent architecture is query expansion followed by parallel retrieval followed by iterative synthesis. The agent receives a question, analyzes it to identify key concepts and dimensions, generates multiple search queries to cover different facets of the topic, executes those searches concurrently across available sources, retrieves the top results from each query, reads and extracts key information, identifies gaps in coverage, generates follow-up queries to fill those gaps, and eventually produces a coherent answer with source attribution. The process is iterative and adaptive. Early retrievals inform later queries. The agent refines its understanding as it gathers information.

The tool set for research agents is dominated by read-only operations. Web search is nearly universal. Document retrieval from internal knowledge bases or external databases is common. API calls to data providers, screen scraping where necessary, file reading for uploaded documents, and sometimes specialized tools for parsing structured formats like PDF tables or academic paper metadata. Research agents almost never write data beyond logging and caching. They do not trigger state changes in external systems. They do not send emails or create records or execute transactions. This constraint is architectural, not just operational. If your research agent has write access to production systems, you have an architecture mismatch.

The evaluation challenge for research agents is correctness and completeness in an unbounded information space. You cannot predefine a gold standard answer because the information landscape changes constantly and questions are open-ended. Instead, you evaluate coverage, citation accuracy, factual consistency, and relevance. Did the agent find the major sources a human expert would find? Are the citations real and correctly attributed? Does the synthesis accurately reflect what the sources say, or does it hallucinate claims not supported by the evidence? Are there major relevant sources the agent missed? These questions are partially automatable through citation verification tools that check URLs and factual consistency models that detect contradictions, but ultimately they require human expert review on a sample basis.

Research agents fail in predictable ways, and understanding these failure modes shapes your architecture. They get stuck in local minima, finding one good source and then semantically similar sources without exploring alternative framings or perspectives. A query about battery thermal management might return ten academic papers all from the same research group, missing alternative approaches from industry or other academic traditions. They hallucinate citations, generating plausible-looking references that do not exist. The agent's training included millions of real citations, and it learns the format and style, but it sometimes fabricates titles, authors, and publication venues that sound right but are invented. They fail to recognize when sources contradict each other and present conflicting claims as compatible. One source says liquid cooling is superior, another says air cooling is sufficient for most use cases, and the agent summarizes both as "cooling is important" without flagging the disagreement.

They retrieve information but fail to synthesize, producing a list of excerpts or bullet points instead of a coherent narrative that integrates the information and answers the question. This happens especially when the information is complex or contradictory and synthesis requires judgment calls the agent is not confident making. They chase tangents, following interesting but irrelevant threads because the relevance boundary is fuzzy. A question about thermal management might lead to tangents about battery chemistry or vehicle aerodynamics, which are related but do not directly answer the question. Each of these failures is common enough that your architecture must explicitly mitigate it.

The architecture mitigations for research agent failures are well-established by 2026. Query diversification techniques force the agent to generate searches from multiple perspectives, different phrasings, different levels of specificity, and different source types. You might require the agent to generate academic queries, industry queries, and news queries for the same topic. Citation verification tools check that every reference corresponds to a real retrievable document. Before finalizing output, the agent passes all citations to a verification step that attempts to fetch each URL or search each title in bibliographic databases. Hallucinated citations are flagged and removed or replaced.

Contradiction detection passes flag when sources disagree on factual claims. The agent is prompted to identify points of disagreement and either resolve them with additional evidence or explicitly note the disagreement in the output. Synthesis quality is enforced through structured output schemas that require claims to be tied to specific sources. You might require the agent to output not just a narrative but a structured representation where each claim has associated citations, forcing the agent to ground its synthesis in retrieved evidence rather than generating plausible-sounding text from its training.

Relevance is maintained through periodic re-grounding prompts that ask the agent to evaluate whether its current search direction serves the original question. After each retrieval batch, the agent is prompted with "given this information and the original question, what gaps remain and what should we search for next?" This prevents unbounded exploration and keeps the agent focused on the user's actual information need.

A critical architectural detail is that research agents are almost always implemented as plan-then-execute rather than tight ReAct loops. The agent generates a research plan upfront, perhaps a set of five to ten queries covering different facets of the topic, executes retrieval in parallel, and then synthesizes. Tight loops where the agent decides on each tool call sequentially are too slow and too prone to getting lost in the search space. Sequential search means each query waits for the previous one to complete, and in a multi-source research task, that latency compounds. You want concurrency in retrieval and you want the search strategy decided before execution begins. The tradeoff is reduced adaptability. The agent cannot adjust its search strategy based on what it finds in real time. But research tasks benefit more from breadth than from deep reactive exploration. You want to cast a wide net, not drill down one path.

Tool choice for research agents is surprisingly constrained despite the apparent complexity of information gathering. You need web search, you probably need specialized database or API access for your domain, and you need document reading capabilities. You do not need write access to external systems, you do not need code execution in most cases, you do not need complex workflow orchestration. The temptation is to give research agents access to tools for "just in case" scenarios. Maybe the agent will need to send an email to request information, or create a spreadsheet to organize findings. This is almost always a mistake. Every additional tool increases the chance of category errors like the Meridian Analytics case. If your agent is doing pure research, its tool set should reflect that constraint. Write access should be limited to logs and caches, never to user-facing or business-critical systems.

Research agent outputs are typically long-form text with citations, structured summaries, or knowledge graphs depending on the use case. The output format shapes synthesis quality. Free-form text allows the agent to produce fluent narratives but makes it easy to hallucinate or drop citations. Structured formats like "claim, evidence, source" tuples enforce grounding but can feel mechanical. Many production research agents use a hybrid approach where the primary output is a narrative but the agent also generates a structured evidence map that can be inspected for verification.

## Workflow Agents: Multi-Step Business Process Execution

Workflow agents exist to execute defined multi-step processes that involve multiple systems and require adaptive decision-making at each step. A user says "onboard this new customer" and the agent creates accounts in the CRM, sends welcome emails, provisions access credentials, schedules onboarding calls, updates tracking spreadsheets, and notifies the relevant teams. The agent's value is in automating processes that are too complex or variable for simple RPA scripts but too routine to require human attention at each step. Unlike research agents that produce knowledge, workflow agents produce effects. They change the state of the world. The systems they interact with are different after the agent runs.

The canonical workflow agent architecture is plan-then-execute with checkpointing and rollback. The agent receives a high-level goal, retrieves or infers the process steps required, generates a step-by-step plan, executes each step, verifies success, and proceeds to the next step. If a step fails, the agent either retries with corrections, escalates to a human, or executes a rollback procedure to undo partial changes and leave the system in a clean state. The plan is not rigid. The agent can adapt if it encounters unexpected conditions, skipping steps that are not applicable or adding steps that become necessary. But there is a clear linear or branching structure to the process. The agent is not exploring an open-ended search space, it is executing a known workflow with variations.

The tool set for workflow agents is dominated by write operations. Creating records in databases, calling APIs that trigger actions, sending messages via email or chat, updating files, moving data between systems. Every tool call has side effects. The agent is not just reading and synthesizing, it is changing the state of production systems. This creates fundamentally different failure consequences. A research agent that retrieves the wrong document wastes time but does not break anything. A workflow agent that calls the wrong API or passes the wrong parameters can create incorrect records, send messages to wrong recipients, or trigger processes that are expensive to undo.

The evaluation challenge for workflow agents is end-to-end process correctness and state consistency. Did the agent complete all required steps? Are the resulting system states correct and consistent? Were there partial failures that left systems in inconsistent states? Evaluation requires not just checking outputs but verifying that side effects occurred correctly. You need integration tests that run the agent against test instances of your production systems, execute complete workflows, and verify the full state transition. Check that the CRM record was created with correct fields, that the welcome email was sent to the right address with the right content, that the access credentials were provisioned in the identity system, that the onboarding call appeared on the right calendars. This is expensive and slow, which is why workflow agent evaluation is often the bottleneck in deployment cycles.

Workflow agents fail when they lose track of process state, when they execute steps out of order, when they fail to detect that a step failed, or when they cannot recover from partial failures. The classic failure mode is the agent completes eight of ten steps, the ninth step fails silently, and the agent reports success. Now you have a customer who is half-onboarded, missing critical setup, and no one realizes it until they try to use the product and encounter errors. The failure is not in reasoning but in observability and verification. The agent did not check whether the ninth step actually succeeded, or it checked but misinterpreted the response, or it detected failure but did not propagate it correctly.

Another common failure is order violations. The agent provisions access credentials before creating the user account, and the provisioning fails because the account does not exist yet. Or the agent sends a welcome email before verifying that the account creation succeeded, and the user receives a welcome to a product they cannot access. These failures happen because the agent reasons about steps independently without modeling dependencies, or because the plan was correct but execution encountered timing issues or race conditions.

Recovery from partial failures is a hard problem. If step nine fails, should the agent retry it? Roll back the previous eight steps? Escalate to a human? The answer depends on the semantics of the workflow and the nature of the failure. Some workflows are naturally transactional, all steps must succeed or none should take effect. Others are incremental, partial progress is acceptable and even desirable. Your architecture must make these semantics explicit and give the agent clear guidance on failure handling.

The architecture mitigations for workflow agent failures are checkpointing, idempotency, and comprehensive logging. After each step, the agent logs what it attempted and what happened, then verifies the result before proceeding. Verification is not just checking that the API call returned success, it is checking that the intended state change occurred. If the agent called an API to create a record, verification means querying to confirm the record exists and has correct values. If the agent sent an email, verification might mean checking a sent folder or an audit log.

Steps are designed to be idempotent where possible so that retries do not cause duplicate actions. Creating a record with a specific ID is idempotent if the system returns success when the record already exists. Sending an email is not naturally idempotent, so you might wrap it in deduplication logic that checks whether an email with the same content was already sent recently. Idempotency allows the agent to safely retry failed steps without checking whether they partially succeeded.

The agent maintains an explicit state machine representation of the workflow so that it can resume after failures or escalations. If a step fails and requires human intervention, the human can fix the issue and tell the agent to resume from that step. This requires the agent to persist not just the plan but the current state, what has been completed, what is in progress, what is pending. Many production workflow agents in 2026 use a hybrid architecture where the high-level plan is generated by an LLM but the execution engine is a traditional workflow orchestrator that handles retries, rollbacks, and state management. The LLM provides adaptability and handles the reasoning required to generate the plan and make step-level decisions. The orchestrator provides reliability and handles the operational concerns like retries, timeouts, and persistence.

A critical decision is how much autonomy to give workflow agents. In high-stakes processes like financial transactions or legal filings, you want human-in-the-loop approval at each critical step. The agent generates the action, presents it to a human, waits for approval, then executes. In high-volume low-stakes processes like data entry or routine provisioning, you want full autonomy with exception-based escalation. The agent executes steps automatically and only involves humans when it encounters failures or ambiguities it cannot resolve. The architecture must support both modes and make it easy to configure approval gates per workflow. Many teams get this wrong initially by building fully autonomous agents for processes that turned out to require human judgment, then retrofitting approval steps in ways that break the agent's mental model of the workflow.

Tool choice for workflow agents is dictated by the systems involved in the process. If your onboarding workflow touches CRM, email, access management, billing, and support ticketing, your agent needs tools for all five. The challenge is ensuring tools are reliable and well-specified. A research agent can often work around a flaky API by trying alternative sources. A workflow agent that encounters a flaky provisioning API either fails the entire workflow or creates inconsistent state. You need higher reliability standards for workflow agent tools, which often means wrapping third-party APIs in your own reliability layer with retries, circuit breakers, and fallbacks. You also need clear error semantics. When a tool call fails, the error response must tell the agent what went wrong in a way it can reason about. Generic errors like "internal server error" are not actionable. Specific errors like "user already exists" or "invalid email format" allow the agent to adapt.

Workflow agent outputs are typically completion reports, summaries of what was done, and pointers to the artifacts created. The agent might return "onboarding complete, CRM record created with ID 12345, welcome email sent to user at specified address, credentials provisioned, onboarding call scheduled for next Tuesday." The output is less important than the side effects. The user cares that the systems are in the right state, not that the agent produced a report. But the report serves as an audit trail and a starting point for debugging if something went wrong.

## Decision Support Agents: Analysis and Recommendation

Decision support agents exist to analyze data, evaluate options, and recommend actions to human decision-makers. A user provides a business scenario, the agent gathers relevant data, performs analysis, considers alternatives, and presents a recommendation with supporting rationale. The agent does not execute the decision, it informs human judgment. The value is in augmenting human expertise with computational analysis at scale and speed. The human retains final authority and accountability, but the agent provides a structured, evidence-based starting point that would take a human analyst hours or days to produce.

The canonical decision support agent architecture is data gathering, quantitative analysis, qualitative reasoning, and structured recommendation generation. The agent identifies what data is needed to inform the decision, retrieves it from available sources, performs calculations or runs models, interprets the results in context, and produces a decision memo with options, tradeoffs, and a recommended course of action. The tool set includes read access to data systems, computation tools like spreadsheet functions or statistical models, and sometimes external APIs for market data or benchmarks. The agent rarely writes data except to logs or temporary analysis files. Like research agents, decision support agents are mostly read-only, but unlike research agents, they produce actionable recommendations, not just information.

The evaluation challenge for decision support agents is recommendation quality, which is inherently subjective and context-dependent. You can evaluate whether the agent's data retrieval was complete, whether its calculations were correct, and whether its reasoning followed sound logic. You cannot definitively evaluate whether the recommendation was "right" without seeing the outcome of the decision, which may take months or years and depends on factors outside the agent's control. This makes decision support agents hard to validate before deployment. You end up relying heavily on expert review of a sample of recommendations and A/B testing in production where feasible. A common evaluation approach is to have the agent generate recommendations for historical decisions where the outcome is known, then check whether the recommendation aligns with what an expert would have chosen or whether following the recommendation would have led to better outcomes.

Decision support agents fail when they use stale data, when they apply inappropriate analytical methods, when they ignore qualitative factors that matter, or when they present recommendations with false confidence. A classic failure is the agent retrieves last quarter's sales data instead of real-time data, performs valid analysis on outdated inputs, and recommends a strategy based on conditions that no longer hold. The analysis is correct given the data, but the data is wrong. Another common failure is overconfidence in quantitative models. The agent runs a regression, gets a coefficient estimate, and presents a precise ROI projection without acknowledging the uncertainty in the model or the assumptions baked into it. The recommendation sounds authoritative but is fragile.

Ignoring qualitative factors is a failure mode that stems from the agent's tool set. If the agent has access to quantitative data but not to qualitative context like team morale, competitive dynamics, or strategic priorities, it will produce recommendations that are analytically sound but strategically naive. A recommendation to cut costs by reducing headcount might be optimal from a financial spreadsheet perspective but disastrous if it destroys team cohesion or erodes critical capabilities. Decision support agents must either have access to qualitative context or explicitly flag the limits of their analysis.

The architecture mitigations for decision support failures are data freshness checks, model calibration, and uncertainty quantification. The agent explicitly checks timestamps on data sources and flags when data is stale. It uses multiple analytical approaches when possible and compares results to check robustness. It quantifies uncertainty in its projections, whether through statistical confidence intervals or qualitative assessments, and communicates confidence levels. It structures recommendations to separate facts from inferences from judgments. "Fact: sales were down fifteen percent last quarter. Inference: if this trend continues, annual revenue will miss target by two million dollars. Judgment: we should prioritize customer retention initiatives over new customer acquisition." This structure makes it clear what is certain, what is likely, and what is opinion.

Many production decision support agents in 2026 use a layered architecture where data retrieval and calculation are handled by deterministic code and only the interpretation and recommendation synthesis use LLM reasoning. This keeps the statistical rigor in the quantitative layer and uses the LLM for what it does well, which is contextual reasoning and communication. The agent might call a Python script to pull sales data and compute growth rates, then feed those results to the LLM along with qualitative context to generate the recommendation narrative. This separation also makes evaluation easier because you can test the quantitative components with unit tests and reserve the harder qualitative evaluation for the synthesis layer.

A critical design choice is how directive to make recommendations. Some decision support agents present three options with pros and cons and leave the choice to the human. "Option A: expand into market X, which has high growth potential but requires significant investment. Option B: optimize operations in existing markets, which has lower upside but also lower risk. Option C: maintain current strategy and invest in product improvements." Others make a clear recommendation and explain why. "Recommendation: expand into market X. Analysis shows the growth potential outweighs the investment risk given our current financial position and competitive advantages." The former feels safer but often frustrates users who want clear guidance. The latter is more useful but requires the agent to have sufficient context and judgment.

In practice, you calibrate this based on domain expertise and stakes. For routine tactical decisions where the agent has good data and the consequences are limited, clear recommendations work well. The agent has analyzed enough similar decisions that it has pattern-matched to a known good answer. For strategic decisions with major consequences or decisions where the agent lacks critical context, options-based presentation is safer. The human needs to apply judgment that the agent cannot replicate. Your architecture should support both styles and select based on decision characteristics.

Tool choice for decision support agents centers on data access and analytical computation. You need reliable connections to your data warehouse, business intelligence tools, and any external data sources relevant to decisions. You often need spreadsheet or numerical computation capabilities for quantitative analysis. You may need access to specialized models like forecasting engines or optimization solvers. You generally do not need tools that trigger actions in external systems, which helps keep the failure blast radius small. A bad recommendation is costly but does not directly break systems. The human is the circuit breaker who decides whether to act on the recommendation.

Decision support agent outputs are structured recommendations with supporting analysis. The format varies widely by domain. A financial decision might produce a multi-page memo with data tables, sensitivity analysis, and risk assessments. A hiring decision might produce a scorecard with candidate evaluations and a ranked list. A marketing decision might produce a campaign plan with projected costs and returns. The key is that the output must be persuasive and actionable. It must give the human decision-maker enough information to understand the reasoning, enough detail to challenge assumptions, and enough clarity to make a confident choice.

## Coding Agents: Software Generation and Debugging

Coding agents exist to write, test, debug, and modify code in response to natural language specifications. A user describes a feature or a bug, the agent reads the codebase context, generates or modifies code, runs tests, debugs failures, and iterates until the tests pass or the agent determines it cannot solve the problem. The agent's value is in automating programming tasks that are well-specified but tedious or time-consuming for human developers. The output is working code that extends or fixes a software system.

The canonical coding agent architecture is context gathering, code generation, execution and testing, error analysis, and iterative refinement. The agent reads relevant parts of the codebase to understand structure and conventions, identifies where changes are needed, generates an initial implementation, runs it to see if it works, analyzes any errors, and refines the code. The loop continues until tests pass or the agent determines it cannot solve the problem, either because the specification is ambiguous, the codebase is too complex, or the task is beyond its capabilities. The tool set includes file reading and writing, code execution in sandboxed environments, test running, linting, and often version control operations. Coding agents are unusual among agent archetypes in that their primary output is structured text that must be syntactically and semantically correct according to formal language rules.

The evaluation challenge for coding agents is functional correctness and code quality. Did the generated code pass all tests? Does it handle edge cases? Is it maintainable and consistent with codebase conventions? Is it secure and performant? Functional correctness is partially automatable through test suites. You give the agent a programming task with a set of tests and measure whether the final code passes all tests. But code quality requires human review or sophisticated static analysis. Many teams evaluate coding agents by having them solve well-specified programming tasks from benchmarks like HumanEval or MBPP, then measuring pass rates and iteration counts. Production evaluation also includes code review by human developers who check for subtle bugs, readability issues, and adherence to team standards.

Coding agents fail when they generate syntactically invalid code, when they misunderstand requirements, when they introduce subtle bugs that tests do not catch, when they get stuck in loops making the same mistake repeatedly, or when they make changes that break other parts of the codebase. A particularly insidious failure mode is the agent passes the narrow test cases provided but introduces regressions or edge case bugs that only surface later in production. The agent satisfies the specification as written but violates implicit requirements or assumptions that were not captured in tests.

Another common failure is context misunderstanding. The agent reads part of the codebase but misses critical context elsewhere, leading to changes that are locally correct but globally inconsistent. The agent might add a function that duplicates existing functionality because it did not see the existing implementation, or use a pattern that is deprecated in the codebase because it read old code and did not see the newer conventions. Coding agents must balance reading enough context to understand the system with not reading so much that they exceed token limits or lose focus.

The architecture mitigations for coding agent failures are sandboxed execution, comprehensive testing, static analysis integration, and iteration limits. Code is always executed in isolated environments where failures cannot affect production systems or the developer's local environment. The agent runs both unit tests and integration tests before considering a solution complete. Static analysis tools like linters and type checkers are integrated into the loop to catch common errors early, before execution. The agent has hard limits on iteration count to prevent infinite refinement loops. If the agent has tried ten variations and tests still fail, it escalates to the human rather than continuing indefinitely.

Many production coding agents in 2026 use a diff-based editing model where the agent proposes changes and a separate verification step checks that the changes are safe before applying them. The agent generates a diff, not a complete new file. This makes changes more auditable and reduces the risk of losing human-written code. The verification step checks that the diff does not delete large amounts of code, does not introduce obvious syntax errors, and is scoped to the relevant parts of the file. This catches many simple mistakes before execution.

A critical distinction is between coding agents that generate new code and those that modify existing code. Greenfield code generation is easier because there are fewer constraints and less context to understand. The agent writes a new function or module from scratch based on a specification. Modifying existing code requires reading and understanding surrounding context, inferring conventions and patterns, and ensuring changes do not break dependencies. The latter is much harder and accounts for most coding agent failures in production. Your evaluation strategy must weight editing tasks heavily because that is where real-world usage concentrates. Most professional development is maintenance and extension of existing systems, not greenfield projects.

Tool choice for coding agents is highly specific to the development environment. You need file system access scoped to the project directory, code execution capabilities with appropriate sandboxing, test runners for the relevant languages and frameworks, and often integration with version control to check diffs and history. You may need package managers to install dependencies, build tools to compile code, and deployment scripts depending on the complexity of the development workflow. The key constraint is ensuring all tools are safe to use programmatically and cannot be exploited to escape the sandbox or access unauthorized resources. A coding agent should not be able to read files outside the project directory, make network calls to arbitrary endpoints, or execute commands that modify the host system.

Coding agent outputs are code changes, typically represented as diffs or modified files, along with explanations of what was changed and why. The agent might return "I added a validation function to check email format, updated the registration handler to call this function before creating the user, and added test cases to verify the validation catches invalid emails." The output must be precise enough for the human to review and decide whether to accept the changes. Many production coding agents integrate with development tools like IDEs or code review platforms, presenting changes in familiar formats with inline explanations.

## Identifying Your Archetype and Avoiding Category Errors

The strategic question you face when designing an agent product is which archetype your use case maps to. This is not always obvious because real use cases often have elements of multiple archetypes. A customer support agent might do research to find relevant documentation, execute a workflow to resolve the issue, and provide decision support on escalation. A data analysis agent might gather information like a research agent, perform computations like a decision support agent, and generate code like a coding agent to process the data. The question is which archetype is primary and which are secondary.

The heuristic is to identify what outcome defines success. If success is producing accurate information, you are building a research agent. If success is completing a multi-step process correctly, you are building a workflow agent. If success is helping a human make a better decision, you are building a decision support agent. If success is generating working code, you are building a coding agent. Your primary archetype determines your architecture, your tool selection, and your evaluation strategy. Secondary archetypes influence specific components but do not drive the overall design.

Many agent products fail because teams try to support multiple archetypes with a single architecture. You build a universal agent with all the tools and all the capabilities, and you end up with an agent that is mediocre at everything and excellent at nothing. The research mode is slower than a dedicated research agent because it has to reason about tools it will never use. The workflow mode is less reliable than a dedicated workflow agent because it lacks the transaction boundaries and state management that workflows require. The decision support mode is less trustworthy because it has write access it should not have. The coding mode is less capable because it cannot commit fully to the development environment.

The better approach is to build specialized agents for each archetype and compose them when necessary. A customer support system might have a research agent that finds documentation, a decision support agent that recommends responses based on the documentation and customer history, and a workflow agent that executes resolution steps like creating tickets or processing refunds. Each agent has a focused architecture optimized for its archetype. The composition layer routes requests to the appropriate agent based on the user's intent and combines results. This is more complex from a system perspective but each component is simpler and more reliable.

The Meridian Analytics failure from the opening story was a failure to recognize archetype boundaries. Generating compliance reports is a workflow task with strict data provenance requirements, idempotency needs, and audit trail obligations. Customer research is a research task with flexibility in sourcing, tolerance for approximate information, and no side effects. Treating them as interchangeable led to category errors. The agent applied research patterns to a workflow task and used web search where only internal databases were acceptable. Had they built separate agents with archetype-appropriate architectures and tool sets, the compliance agent would have had access only to audited internal databases and the research agent would have had access to web search. The failure would have been architecturally impossible. The system would have rejected a compliance report based on web search because the compliance agent would not have had that tool.

In practice, identifying your archetype requires analyzing not just what the agent does but what happens when it fails. Research agents that fail waste user time but do not break systems. Workflow agents that fail can leave systems in inconsistent states. Decision support agents that fail lead to bad decisions but the human is the final check. Coding agents that fail produce code that does not work or breaks the build. The failure consequences shape your architecture priorities. Research agents prioritize coverage and accuracy. Workflow agents prioritize reliability and state management. Decision support agents prioritize explainability and uncertainty quantification. Coding agents prioritize correctness verification and sandbox safety.

In 2026, the production agent landscape is mature enough that you do not need to invent new archetypes for most use cases. Identify which of the four your use case maps to, study the established architecture patterns for that archetype, and adapt them to your specifics. The failure modes are well-known, the mitigations are proven, and the evaluation strategies are documented. Your job is not to be original but to be rigorous in applying what works. If you find yourself designing an agent that does not fit any archetype cleanly, step back and decompose the use case. Chances are it is actually several agents that should be separated, not one agent with a novel architecture.

The next step after identifying your archetype is understanding the execution loop in detail. Agents are not monolithic inference steps, they are iterative loops of observation, reasoning, planning, action, and reflection. The structure of that loop varies by archetype, but the phases are universal across all agent systems. Getting each phase right is what separates agents that work from agents that fail in production, and understanding the loop deeply is the foundation for everything else you will build.
