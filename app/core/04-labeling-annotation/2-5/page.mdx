# 2.5 — Backward Compatibility: Migrating Historical Labels After Schema Changes

In September 2025, a healthcare technology company discovered that six months of trend analysis was completely invalid. The company had changed its medical imaging label schema in March 2025, consolidating four severity levels into three and renaming several anatomical region labels for consistency with updated medical terminology. The annotation team migrated 47,000 historical labels using an automated mapping table that converted old labels to new labels based on simple string matching rules. The migration took three days and appeared successful. Every historical label now used the new schema format. Dashboards rendered without errors. Eval pipelines ran clean.

The problem surfaced when the clinical validation team noticed that model performance trends showed an impossible discontinuity at the March cutoff date. Precision for moderate-severity findings dropped from 0.91 to 0.76 overnight, then immediately recovered to 0.89 the next day. Recall for thoracic imaging jumped from 0.83 to 0.94 in a single day, with no model changes or data distribution shifts. The team spent two weeks investigating before they understood what had happened. The mapping table had merged two old severity levels into a single new level, but the merge was asymmetric. Items that were labeled severe-acute in the old schema mapped to severe in the new schema. Items labeled severe-chronic also mapped to severe. But the clinical characteristics of acute versus chronic severity were fundamentally different, and the model had learned to distinguish them. After migration, all trend analysis treated these as the same category, creating artificial performance changes that had nothing to do with actual model quality. The company had to re-label 31,000 images at a cost of $240,000 and six weeks of annotator time to restore data integrity.

The root cause was treating schema migration as a data formatting problem instead of a data provenance problem. Every historical label carries the context of the schema version that produced it. When you change the schema, you don't just need to convert the label to the new format. You need to preserve the original context, decide whether conversion is semantically valid, and ensure that downstream systems can distinguish migrated data from natively-labeled data. Schema migration is not a one-time transformation. It is an ongoing compatibility commitment that affects every system that consumes your labels for years after the change.

## The Three Migration Strategies

You have three approaches to handling historical labels after a schema change, and the choice determines your data quality, cost, and operational complexity for the next year. The first strategy is re-labeling: sending all historical items back through the annotation pipeline with the new schema, using fresh annotators who have never seen the old labels. This produces the cleanest data. Every label is native to the new schema. There are no mapping artifacts, no semantic ambiguities, no version compatibility issues. Re-labeling is the only strategy that guarantees full backward compatibility with downstream systems because the historical data becomes indistinguishable from new data in terms of schema semantics.

Re-labeling is expensive. If you have 100,000 historical labels and each label costs $2.50 in fully-loaded annotator cost including quality control, re-labeling costs $250,000. If your annotation throughput is 2,000 labels per week, re-labeling takes 50 weeks. For most teams, re-labeling the entire historical dataset is not feasible. But re-labeling a critical subset is often necessary. High-stakes production data, data used in regulatory submissions, data tied to legal obligations, data that anchors your most important eval benchmarks — these subsets justify re-labeling cost because the cost of data integrity failure exceeds the cost of re-labeling.

The second strategy is mapping tables: creating explicit conversion rules that translate old labels to new labels based on semantic equivalence rules. A mapping table says that the old label content-policy-violation-hate-speech maps to the new label policy-hate in the updated schema, and the old label content-policy-violation-violence maps to policy-violence. Mapping tables are pragmatic. They let you convert historical labels in hours instead of months. They work well for simple schema changes like label renaming, hierarchy flattening, or category merging where the semantic mapping is unambiguous.

Mapping tables are dangerous when the semantic relationship between old and new labels is not one-to-one. When you split an old label into multiple new labels, the mapping is ambiguous. The old label unsafe-content might map to either policy-hate or policy-violence or policy-self-harm depending on the specific content of each item, but the mapping table only has access to the label itself, not the original item. If you use a default mapping rule that sends all unsafe-content labels to policy-hate, you've just introduced systematic label noise into your historical data. Every item that was actually violent but labeled unsafe-content is now mislabeled as hate speech. The mapping table transformed a schema change into a data corruption event.

The third strategy is dual-schema periods: running the old schema and new schema in parallel for a transition window, labeling all new items with both schemas, and gradually phasing out the old schema as historical data becomes less relevant to current analysis. Dual-schema periods preserve both the old and new views of your data without forcing an immediate migration decision. Systems that depend on the old schema continue working. Systems that need the new schema get native new-schema labels for recent data. Over time, as old data ages out of your training sets and eval benchmarks, the old schema becomes purely archival and you can retire it without breaking active systems.

Dual-schema periods are operationally complex. You are running two annotation workflows in parallel. Annotators label each item twice, once per schema, which doubles annotation cost during the transition. Your data storage must maintain both label versions for every item. Your dashboards and eval pipelines must support both schemas. Your model training code must handle datasets where some items have old-schema labels, some have new-schema labels, and some have both. The operational overhead is substantial, but for large-scale production systems with many downstream consumers, dual-schema periods are often the only migration path that does not create unacceptable downtime or data continuity gaps.

## When Each Strategy Is Appropriate

Re-labeling is appropriate when data quality is non-negotiable and the historical dataset is small enough or valuable enough to justify the cost. Medical imaging labels used for FDA submissions must be re-labeled when the schema changes because regulatory submissions require native schema compliance. Financial fraud detection labels used in audit trails must be re-labeled when taxonomy changes affect regulatory category definitions. Content moderation labels tied to legal takedown decisions may need re-labeling when policy definitions change to ensure consistency with current enforcement standards.

Re-labeling is also appropriate when the schema change is semantically incompatible with automated mapping. If you replace a binary safe-unsafe label with a ten-category risk taxonomy, there is no mapping table that can reliably convert old labels to new labels. The old labels do not contain enough information to infer the correct new category. You must re-annotate with the full new schema to produce valid labels.

Mapping tables are appropriate when the schema change is a refactoring that preserves semantic meaning but changes structure or naming. Renaming product-recommendation-relevant to recommendation-match is a pure rename. The concept is identical, only the label string changed. A mapping table handles this perfectly. Merging slightly-relevant and moderately-relevant into a single relevant category is a semantic merge, but if the downstream use case does not require the granularity distinction, the merge is valid and a mapping table implements it cleanly.

Mapping tables are also appropriate when the historical data is low-stakes and mapping errors are acceptable. Exploratory research datasets, internal model experiments, early-stage prototypes — these contexts tolerate some mapping noise in exchange for fast migration. The key requirement is that you document the mapping logic explicitly and you version-tag all migrated labels so that downstream consumers know the data has been transformed and can assess whether the transformation affects their use case.

Dual-schema periods are appropriate when you have many downstream consumers with different migration timelines and you cannot coordinate a synchronized schema cutover. A large enterprise with 15 teams consuming the same label data cannot force all 15 teams to update their systems on the same day. A dual-schema period gives each team time to migrate on their own schedule while ensuring that new data is available in both formats.

Dual-schema periods are also appropriate when the schema change is speculative and you are not yet confident that the new schema is better than the old schema. Running both schemas in parallel for three months gives you time to compare model performance, annotation agreement, and downstream consumer feedback before committing fully to the new schema. If the new schema turns out to be worse, you can revert to the old schema without losing any data. If the new schema is better, you have three months of dual-labeled data that lets you validate migration strategies before applying them to historical data.

## Data Integrity Requirements: Preserving Schema Version Context

Every historical label must retain metadata about the schema version that produced it, even after migration. This is not optional. This is a data provenance requirement that underpins every form of trend analysis, model evaluation, and audit trail you will ever run. When you migrate a label from schema version 1.0 to schema version 2.0, the migrated label must carry both the current label value in the new schema and the original label value in the old schema, along with timestamps for when it was originally created and when it was migrated.

The reason is continuity. If you run a trend analysis that spans the schema change cutoff date, you need to know which labels are native to the new schema and which are migrated from the old schema. Native labels reflect annotator decisions made directly in the new schema. Migrated labels reflect annotator decisions made in the old schema, then transformed by your migration logic. These are not the same. If you see a performance drop at the schema cutoff date, you need to know whether the drop is caused by a model regression, a data distribution shift, or a migration artifact. Without schema version metadata, you cannot distinguish these cases.

Schema version metadata is also required for audit compliance under the EU AI Act as of 2026. Article 12 of the Act requires high-risk AI systems to maintain logs that enable traceability of system behavior over time. When your labels change schema, the audit trail must show what changed, when it changed, why it changed, and how historical labels were migrated. If you cannot produce this documentation, you are not compliant. A data governance team at a financial services company spent four months in early 2026 reconstructing schema migration history from Git logs and Slack messages because their label database did not store schema version metadata. The audit cost them $180,000 in consultant fees and delayed a product launch by three months.

The implementation is straightforward. Your label schema must include a schema_version field that records which version of the schema was active when the label was created. When you migrate a label, you add a migration_history field that records the original schema version, the original label value, the migration method, and the migration timestamp. If the same label is migrated multiple times across multiple schema versions, the migration history becomes a chain that documents every transformation. This is not over-engineering. This is the minimum metadata required to operate a production label dataset responsibly.

## Testing Requirements: Validating Migration Integrity

You cannot deploy a schema migration without testing that the migration preserves trend continuity and does not introduce artifacts. The testing process has three phases. First, you validate the mapping logic on a sample dataset. You take 500 to 1,000 representative items, apply your migration logic, and manually review the results to confirm that old labels map to semantically equivalent new labels. You look for edge cases where the mapping is ambiguous or incorrect. You look for patterns where the mapping changes the meaning of the label in subtle ways that will affect downstream analysis.

Second, you run a trend continuity test. You take a time series of model performance metrics that spans at least three months before the schema change cutoff date. You compute metrics like precision, recall, and F1 for each label category using the old schema. Then you apply your migration logic to the historical labels and recompute the same metrics using the new schema. If the migration is semantically preserving, the metrics should be nearly identical before and after migration, with differences explainable by legitimate semantic changes in the schema. If you see large discontinuities, your mapping logic is wrong.

A content moderation team ran this test in late 2025 and discovered that their migration logic had introduced a 12 percentage point recall drop for policy-violence labels. The old schema had separate labels for graphic-violence and threats-of-violence. The new schema merged these into a single violence label. The migration logic mapped both old labels to the new violence label, which should have been correct. But the team had forgotten that their model was trained on the old schema and had learned to detect graphic-violence and threats-of-violence as distinct patterns. After migration, the eval dataset no longer distinguished these patterns, so the recall metric averaged performance across both, which masked the fact that the model was much better at detecting graphic violence than threats. The team reverted the migration and re-trained the model on the new schema before migrating historical labels.

Third, you run a downstream consumer test. You identify every system that consumes your labels — dashboards, eval pipelines, training datasets, reporting tools — and you run those systems on a test dataset that includes both pre-migration and post-migration labels. You verify that every system handles the migrated labels correctly and that no system throws errors or produces unexpected outputs. This sounds tedious, but it is the only way to catch integration issues before they reach production. A recommendation system team skipped this step and discovered three weeks after migration that their A/B test framework was filtering out all migrated labels because the framework expected a label field that no longer existed in the new schema. The framework had been silently dropping 40% of their eval data for three weeks, making all A/B test results invalid.

## Migration Rollout: Phased Deployment and Rollback Plans

You do not migrate all historical labels at once. You migrate in phases, starting with the smallest, lowest-risk subset and expanding only after you have validated that the migration works correctly. The first phase is a pilot migration of 1,000 to 5,000 labels that represent the full diversity of your dataset. You migrate these labels, run all validation tests, and monitor downstream systems for one week. If the pilot reveals issues, you fix them before proceeding. If the pilot is clean, you move to phase two.

Phase two migrates a larger subset, typically 10% to 20% of your historical data. This phase tests whether your migration logic scales and whether you have identified all downstream consumers. You monitor for two weeks. You check that dashboards render correctly, that eval metrics are stable, that training pipelines process migrated labels without errors. You also check migration performance: how long does it take to migrate 100,000 labels? Do you have enough database capacity to store dual versions of every label during the transition? Do you have enough compute capacity to run validation tests at scale?

Phase three is full migration. You migrate the remaining 80% to 90% of historical labels, using the exact same logic and process that worked in phases one and two. You monitor for at least one month after full migration. You run weekly trend reports to verify that performance metrics remain stable. You solicit feedback from downstream consumers to catch edge cases that did not appear in pilot testing.

Every migration phase must have a rollback plan. If you discover a critical issue during phase two or phase three, you need the ability to revert to pre-migration state without data loss. The rollback plan depends on your migration strategy. For mapping table migrations, rollback means switching your query layer back to the old schema and ignoring the migrated labels. For re-labeling migrations, rollback is not possible because you have overwritten the old labels, which is why re-labeling requires even more rigorous validation before deployment. For dual-schema migrations, rollback is trivial: you just keep using the old schema and you stop populating the new schema.

The rollback plan must be tested before you start migration. You cannot assume rollback will work. You must actually execute a test rollback in a staging environment to verify that your systems can revert cleanly. A fintech company learned this in early 2026 when they discovered during a production rollback that their migration script had deleted the original schema version metadata as part of the migration process. Rollback was impossible. They had to restore from backups, losing two days of new labels in the process.

## Communication and Stakeholder Coordination

Schema migration affects every team that consumes your labels, and you cannot migrate successfully without coordinating with all of them. The communication plan starts at least four weeks before migration. You send a detailed migration notice to every stakeholder, including the exact date of the migration, the nature of the schema changes, the migration strategy you are using, and the expected impact on their systems.

The migration notice must include a compatibility matrix that shows which old labels map to which new labels. This is not optional documentation. This is the reference that lets downstream teams update their code, dashboards, and reports to handle the new schema. If you are using a mapping table, you include the full mapping table in the notice. If you are re-labeling, you explain which subsets will be re-labeled and which will remain in the old schema. If you are running a dual-schema period, you specify the start date, end date, and support commitment for the old schema during the transition.

You also hold a migration readiness meeting one week before the migration. Every stakeholder team sends a representative. You walk through the migration plan, answer questions, and confirm that every team has updated their systems to handle the new schema. You ask each team to explicitly confirm their readiness. If any team is not ready, you delay the migration. Forcing a migration when downstream consumers are not ready creates production incidents that damage trust and take weeks to repair.

A data platform team ignored this step in mid-2025 and migrated their label schema on schedule despite knowing that the experimentation team had not yet updated their A/B test framework. The experimentation team discovered the issue two hours after migration when their framework started throwing errors on every test run. The platform team had to roll back the migration, delay it by three weeks, and rebuild their relationship with the experimentation team through a painful series of post-mortem meetings.

## Archive Requirements: Preserving Old Schemas for Compliance

Even after you successfully migrate all historical labels to the new schema, you must preserve the old schema definition and the mapping logic for audit and compliance purposes. The EU AI Act requires that you maintain documentation of any changes to training data or labeling processes for high-risk systems, and schema migration is one of those changes. The archive must include the old schema definition, the new schema definition, the mapping table or re-labeling instructions, the validation test results, and the stakeholder communication records.

The archive is not just a compliance checkbox. It is a practical necessity for long-term data operations. Two years after a schema migration, someone will ask a question that requires understanding how labels were structured before the migration. A legal team will request historical data for a case that spans the migration cutoff date. A research team will want to compare model behavior before and after the schema change. If you do not have the old schema and migration logic archived in a queryable format, you cannot answer these questions.

The archive must be stored in a system that supports long-term retention and is accessible to future teams who were not involved in the original migration. A Git repository works for schema definitions and mapping tables. A data warehouse works for migration metadata and validation test results. A documentation wiki works for stakeholder communication records. The key requirement is that the archive is not siloed in a single person's laptop or a deprecated tool that will be inaccessible in two years.

The next subchapter covers label deprecation: the process of retiring labels that are no longer relevant without breaking the systems that depend on them.

