# Chapter 6 — Inter-Annotator Agreement and Quality Control

If two annotators look at the same output and disagree, your labels are unreliable. Inter-annotator agreement is the single most important quality signal in any labeling program. This chapter covers how to measure it, what to do when it is low, and how to build the quality control infrastructure that keeps labels trustworthy at scale.

---

## What This Chapter Covers

- **6.1** — Why Agreement Matters: The Foundation of Label Reliability
- **6.2** — Measuring Agreement: Kappa, Alpha, and Beyond
- **6.3** — Designing Overlap: How Much Double-Labeling You Need
- **6.4** — Disagreement Analysis: Mining Gold from Annotator Conflicts
- **6.5** — Adjudication as an Operational System: Roles, Evidence, and Decisions
- **6.6** — Decision Notes: Writing Down Why the Final Label Is What It Is
- **6.7** — Disagreement Mining to Guideline Updates: The Feedback Loop
- **6.8** — Golden Sets and Trap Questions: Continuous Quality Assurance
- **6.9** — Quality Dashboards: Real-Time Monitoring of Label Integrity

---

*We begin with the question every labeling program must answer: how do you know your labels are reliable?*
