# 1.4 â€” Task-Specific Labeling: Chat, RAG, Tool Use, Agents, Voice

In late 2025, an enterprise software company built a RAG-based documentation assistant and hired a labeling team to evaluate its quality. They gave annotators a simple binary label: was the answer good or bad? After labeling 2,000 query-answer pairs, they had a dataset showing 74% of answers were good, which seemed promising. When they analyzed failure cases more carefully, they discovered that the dataset was useless. Some annotators marked answers as good if the retrieved documentation was relevant, even if the generated answer was unfaithful to that documentation. Other annotators marked answers as good if the generated text sounded plausible, even if the retrieval had pulled completely irrelevant chunks. Still others marked answers as bad if the documentation itself was outdated, which was not a failure of the RAG system but of the underlying content. The company had spent $85,000 collecting labels that conflated three completely different failure modes: retrieval relevance, answer faithfulness, and citation accuracy. When they tried to use these labels to improve the system, they could not diagnose whether to fix the retrieval component, the generation component, or the documentation. The root cause was treating RAG evaluation as a generic question-answering task instead of recognizing that retrieval-augmented generation requires decomposed, component-specific labeling.

Different AI modalities have fundamentally different architectures, failure modes, and production requirements. A single-turn chat response is a discrete text output that either helps the user or fails to help them. A RAG system involves multiple components that can fail independently: retrieval, reranking, context assembly, generation, and citation. A tool use system makes discrete decisions about which tool to call and what parameters to pass, then must correctly use the tool result. An agent executes multi-step traces where each step can succeed or fail, and where efficiency matters as much as correctness. A voice interface adds latency, tone, interruption handling, and transcription accuracy to the evaluation space. Generic labeling approaches that treat all of these as "rate the quality from 1 to 5" miss the structure of the task and produce labels that cannot diagnose specific failures or guide specific improvements. Task-specific labeling means designing label types, label granularity, and evaluation procedures that match the actual architecture and failure modes of the system you are evaluating.

## Chat Labeling: Turn-Level Versus Conversation-Level

Chat systems generate responses to user messages in single-turn or multi-turn conversations. The simplest chat labeling approach is turn-level evaluation: you show annotators a user message and an assistant response, and you ask them to judge the quality of that response. This works for single-turn question answering or instruction following, where each response is independent. It fails for multi-turn conversations where response quality depends on conversation history, where the assistant needs to maintain context across turns, and where a response might be technically correct but conversationally inappropriate. A response that repeats information from three turns ago is unhelpful. A response that contradicts something the assistant said earlier breaks trust. A response that ignores the user's evolving emotional state shows poor conversation management. Turn-level labels cannot capture these failures because they evaluate each response in isolation.

Conversation-level labeling evaluates the entire dialogue rather than individual turns. You show annotators the full conversation history and ask them to judge whether the conversation as a whole achieved the user's goal, maintained coherence, handled context appropriately, and satisfied quality standards. Conversation-level labels capture emergent properties like helpfulness across multiple exchanges, consistency of the assistant's persona, and successful resolution of complex requests that require clarification. The disadvantage is that conversation-level labels provide no diagnostic signal about which specific turn caused a failure. If annotators rate a conversation as unsuccessful, you know something went wrong, but you do not know whether it was the third turn where the assistant misunderstood the user's intent, the fifth turn where the assistant repeated itself, or the seventh turn where the assistant gave up on a complex request. Conversation-level labels are good for measuring overall system quality but poor for identifying specific failure modes to fix.

The solution used by leading chat evaluation teams in 2026 is **dual-layer labeling**: turn-level labels for individual response quality and conversation-level labels for dialogue success. Turn-level labels evaluate each assistant response on helpfulness, safety, and accuracy as separate dimensions, which we will address shortly. Conversation-level labels evaluate whether the full dialogue achieved the user's goal, whether the assistant maintained appropriate context, and whether the conversation required an inefficient number of turns to reach resolution. This dual structure lets you measure both component quality and system-level outcomes. You can identify that 92% of individual responses are helpful, but only 78% of conversations successfully resolve the user's request, which tells you that the gap is in multi-turn coherence or goal tracking, not in single-response quality.

The critical design choice in turn-level chat labeling is whether to treat helpfulness, safety, and accuracy as a single composite judgment or as separate dimensions. Most teams start with a single "quality" label because it seems simpler. This is a mistake. A response can be helpful but unsafe: an LLM that gives detailed instructions for a sensitive medical procedure is helpful but may be unsafe without appropriate disclaimers. A response can be safe but unhelpful: an overly cautious refusal to answer a legitimate question is safe but frustrating. A response can be helpful and safe but inaccurate: a confident-sounding answer that contains subtle factual errors passes safety and helpfulness filters but fails on accuracy. If you collect single composite quality labels, you cannot distinguish between these failure modes, and you cannot prioritize which dimension to improve.

Best practice for production chat systems in 2026 is to collect separate binary labels for each critical dimension. Is the response helpful in addressing the user's request? Is the response safe according to your content policy? Is the response factually accurate based on verifiable information? These are three independent binary judgments, and annotators can reliably make each one as long as you provide clear definitions and examples. A response receives a profile like "helpful: yes, safe: yes, accurate: no" which immediately tells you the failure mode is factual correctness, not helpfulness or safety. You can compute separate precision and recall metrics for each dimension, track how changes to your system affect each dimension independently, and make informed tradeoff decisions when improvements in one dimension hurt another. Separate dimensional labels cost more annotator time than single composite labels, but they produce far more actionable diagnostic signal.

## RAG Labeling: The Three-Part Evaluation

Retrieval-augmented generation systems retrieve relevant documents or chunks from a knowledge base, then generate answers based on that retrieved context. RAG introduces a fundamental evaluation challenge: you must separately evaluate whether the system retrieved the right information and whether it generated a faithful answer based on that information. A RAG system can fail by retrieving irrelevant documents and generating a plausible-sounding answer anyway, which is a retrieval failure. It can fail by retrieving relevant documents and generating an answer that contradicts or misrepresents those documents, which is a generation failure. It can fail by retrieving relevant documents and generating a faithful answer but failing to cite which document each claim came from, which is a citation failure. Generic question-answering evaluation collapses these three failure modes into one, making it impossible to diagnose where the system broke.

The three-part RAG evaluation framework that has become standard in 2026 is **retrieval relevance, answer faithfulness, and citation accuracy**. Retrieval relevance evaluates whether the chunks or documents retrieved by the system contain information needed to answer the user's query. Annotators see the query and the retrieved chunks, and they label each chunk as relevant, tangentially relevant, or irrelevant. This is a multi-instance labeling task: for a query that retrieves ten chunks, annotators evaluate all ten independently. You compute retrieval precision as the fraction of retrieved chunks that are relevant, and retrieval recall by separately labeling a reference set of all relevant chunks in the knowledge base and measuring what fraction the system retrieved. Retrieval relevance labels tell you whether your embedding model, your retrieval algorithm, and your reranking logic are working correctly.

Answer faithfulness evaluates whether the generated answer accurately represents the information in the retrieved chunks, without adding unsupported claims or contradicting the source material. Annotators see the retrieved chunks and the generated answer, and they label the answer as faithful, partially faithful, or unfaithful. Faithful means every factual claim in the answer is supported by the retrieved chunks. Partially faithful means most claims are supported but some are unsupported or overgeneralized. Unfaithful means the answer contradicts the chunks or makes significant claims with no support. Answer faithfulness is the most cognitively demanding label in the three-part framework because annotators must carefully compare the generated text to the source chunks, identify every factual claim, and verify each one. This is why answer faithfulness labeling typically costs more per example than retrieval relevance labeling.

Citation accuracy evaluates whether the answer correctly attributes claims to specific source documents or chunks. If your RAG system generates inline citations, annotators verify that each citation points to a chunk that actually supports the cited claim. If your system generates a final answer with footnotes or references, annotators verify that the references are accurate and complete. Citation accuracy matters because users need to be able to verify claims and because many enterprise use cases have audit requirements that demand traceability from generated answers back to authoritative source documents. A RAG system with high answer faithfulness but low citation accuracy generates correct answers but gives users wrong information about where those answers came from, which breaks trust and fails compliance requirements.

The labeling workflow for RAG evaluation is necessarily more complex than for simple chat evaluation. For each query-answer pair, annotators first label retrieval relevance for each retrieved chunk. Then they label answer faithfulness by comparing the answer to the relevant chunks. Then they label citation accuracy by verifying each citation. This is three separate labeling tasks per example, which is why high-quality RAG evaluation datasets cost three to five times more per example than chat evaluation datasets. Teams that try to cut costs by skipping one of the three components end up with incomplete diagnostic signal. If you label only answer quality without labeling retrieval relevance, you cannot tell whether answer failures are caused by bad retrieval or bad generation. If you label only retrieval relevance without labeling answer faithfulness, you cannot tell whether your generation component is actually using the retrieved chunks correctly.

The failure mode that many teams hit in 2025-2026 is labeling retrieval relevance but not answer faithfulness, then being surprised when fixing retrieval does not improve end-to-end answer quality. The issue is that modern LLMs are extremely good at generating plausible-sounding answers even when given irrelevant or low-quality retrieval chunks. Your retrieval component might be working poorly, but your generation component compensates by ignoring the bad chunks and generating answers based on the model's parametric knowledge. Improving retrieval does not help because the generation component is already ignoring it. You need answer faithfulness labels to detect this failure mode, because only by checking whether the generated answer is actually grounded in the retrieved chunks can you tell whether the RAG system is functioning as designed or whether it has degraded into an LLM that happens to retrieve irrelevant documents.

## Tool Use Labeling: Decision, Execution, and Integration

Tool use systems, also called function calling or action systems, allow LLMs to invoke external tools, APIs, or functions as part of generating a response. When a user asks "What is the weather in Tokyo?" a tool use system might call a weather API, receive structured data about Tokyo weather, and then generate a natural language response incorporating that data. Tool use labeling must evaluate three separate stages: was the right tool selected, were the tool parameters correct, and was the tool result used properly in the final response? Each stage can fail independently, and each requires different labeling approaches.

Tool selection labeling evaluates whether the system chose the appropriate tool or function for the user's request. If the user asks about weather and the system calls a calendar API, that is a tool selection failure. If the user asks to schedule a meeting and the system calls a weather API, that is a tool selection failure. Tool selection labels are typically multi-class: annotators see the user request and the tool the system selected, and they label it as correct tool, plausible alternative tool, or wrong tool. "Plausible alternative" captures cases where multiple tools could satisfy the request. A query like "What is happening in Tokyo today?" could legitimately call a weather API, a news API, or an events API, depending on interpretation. Binary correct-or-wrong labels would force annotators to make arbitrary choices in these cases, while a three-category label captures the ambiguity.

Tool parameter labeling evaluates whether the system passed the correct parameters to the selected tool. If the user asks "What is the weather in Tokyo tomorrow?" and the system calls a weather API for New York today, that is a parameter failure. Tool parameters are structured data: location strings, dates, numerical values, enumerated options. Parameter labeling requires annotators to verify that each parameter matches the user's request. This is more objective than most labeling tasks because parameters either match or do not match. The challenge is that parameter errors can be subtle. If the user asks "How hot is it in Tokyo?" and the system requests temperature in Celsius when the user expects Fahrenheit, that is a parameter error only if you are serving US users who expect Fahrenheit. Context matters, and annotators need clear guidelines about what counts as correct.

Tool result integration labeling evaluates whether the system correctly incorporated the tool result into its final response. If the weather API returns 22 degrees Celsius and the system tells the user it is 22 degrees Fahrenheit, that is an integration failure. If the API returns an error and the system ignores the error and makes up a weather forecast, that is an integration failure. If the API returns a valid result and the system says it cannot answer the question, that is an integration failure. Integration labeling requires annotators to see the user request, the tool call, the tool result, and the final response, then verify that the response accurately and appropriately uses the result. This is similar to answer faithfulness labeling in RAG, but applied to structured tool outputs rather than unstructured document text.

The compounding complexity in tool use systems is that a single user request may trigger multiple tool calls in sequence or in parallel. The user asks "Schedule a meeting with Alice tomorrow at 2pm and send her the agenda." The system must call a calendar API to schedule the meeting and an email API to send the agenda. If it calls the calendar API successfully but fails to call the email API, that is a partial failure. If it calls both APIs but sends the email before creating the calendar event, causing the email to have no meeting link, that is a sequencing failure. Multi-tool labeling requires annotators to evaluate not just individual tool calls but the entire sequence: were all necessary tools called, were they called in the right order, were results from earlier tools correctly passed to later tools, and was the final response coherent given all the tool interactions?

Best practice for tool use labeling in 2026 is to decompose multi-tool interactions into per-tool labels plus a final integration label. For each tool call in a trace, annotators label tool selection, parameter correctness, and result handling. Then they label the overall response for completeness: did the system fully address the user's request given all the tool results it received? This decomposition lets you diagnose whether failures are happening in tool selection, parameter extraction, result parsing, or response generation. It also lets you measure tool-specific reliability: you might discover that calendar API calls succeed 94% of the time but email API calls succeed only 78% of the time, which tells you where to focus reliability improvements.

## Agent Labeling: Multi-Step Trace Review

Agents are systems that execute multi-step plans to achieve user goals, typically involving repeated cycles of reasoning, tool use, observation, and replanning. An agent asked "Find the cheapest flight from New York to London next week" might search flight APIs, compare prices, check baggage policies, verify departure times, and present a recommendation. This could involve ten or twenty individual steps, each of which could succeed or fail. Agent labeling must evaluate not just the final outcome but the efficiency and safety of the path the agent took to get there.

The first dimension of agent labeling is **goal completion**: did the agent successfully achieve what the user asked for? This is a binary label at the conversation level. The user asked for the cheapest flight next week, and the agent either found it or did not find it. Goal completion labels are necessary but insufficient because they provide no diagnostic signal about what went wrong. An agent might fail to achieve the goal because it searched the wrong APIs, because it misinterpreted search results, because it gave up after encountering an error, or because the goal was impossible to achieve with available tools. Goal completion labels tell you your success rate but not how to improve it.

The second dimension is **step-level correctness**: for each step in the agent's trace, was the action appropriate given the current state? Annotators review the full trace and label each step as correct, suboptimal, or incorrect. A correct step moves the agent closer to the goal using an appropriate tool with appropriate parameters. A suboptimal step moves toward the goal but uses an inefficient approach or redundant tool call. An incorrect step moves away from the goal, calls the wrong tool, or misinterprets results. Step-level labeling is cognitively demanding because annotators must understand the agent's reasoning chain, the state at each step, and what actions were available. This is why agent trace labeling typically requires more senior annotators or domain experts rather than general crowd workers.

The third dimension is **efficiency**: how many steps did the agent take to reach the goal, and was that reasonable? An agent that calls a flight search API twenty times with minor parameter variations before settling on a result is inefficient even if it ultimately succeeds. An agent that searches five different flight APIs when one would suffice is inefficient. Efficiency labels are typically scalar or categorical: annotators label traces as highly efficient, acceptably efficient, or inefficient. The challenge is that efficiency depends on task complexity. A simple query should be resolved in two or three steps. A complex query might legitimately require fifteen or twenty steps. Annotators need reference examples showing what efficient traces look like for different task types.

The fourth dimension is **safety at each step**: did the agent take any actions that violated safety policies, risked user data, or caused unintended side effects? If the agent is searching for flights, calling a flight search API is safe. Calling a booking API without explicit user confirmation is unsafe. If the agent is managing email, reading email is safe. Deleting email without explicit user instruction is unsafe. Sending email to unintended recipients is unsafe. Safety labeling for agents requires annotators to review every action in the trace and flag any action that could cause harm or policy violations. This is particularly critical for agents with write access to production systems, where a single incorrect action can have real consequences.

Agent labeling workflows in 2026 typically use a combination of automated trace analysis and human review. Automated systems check for obvious failures like tool call errors, infinite loops, or policy violations. Human annotators review goal completion, step-level correctness, and efficiency for a sample of traces. The sample is stratified to include successful traces, failed traces, and edge cases. Failed traces get deeper review to diagnose failure modes. Successful traces that took many steps get efficiency review to identify optimization opportunities. This hybrid approach makes agent evaluation practical at scale while maintaining high signal quality for diagnostic purposes.

## Voice Labeling: Transcription, Tone, Latency, and Interruption

Voice interfaces add layers of complexity beyond text-based chat. A voice assistant must transcribe user speech accurately, generate an appropriate response, synthesize that response in a suitable voice, deliver it with acceptable latency, and handle interruptions gracefully. Each of these components can fail independently, and each requires different evaluation approaches. Text-based chat labeling focuses on content quality. Voice labeling must additionally evaluate speech-to-text accuracy, text-to-speech quality, prosody and tone, latency perception, and turn-taking behavior.

Transcription accuracy is the most objective voice labeling task. Annotators listen to user audio and compare it to the system's transcription. They mark transcription errors as substitutions, where the system transcribed the wrong word; insertions, where the system added words that were not spoken; or deletions, where the system missed spoken words. Transcription accuracy is typically reported as word error rate: the number of errors divided by the total number of words. A word error rate below 5% is generally considered acceptable for production systems, but acceptable thresholds vary by domain. Medical or legal domains require near-perfect transcription because a single wrong word can change meaning in critical ways. Customer service domains can tolerate higher error rates as long as the gist is captured. Transcription labeling is time-consuming because annotators must listen carefully to audio, often multiple times, and precisely mark every error.

Tone appropriateness evaluates whether the synthesized voice response matches the intended emotional and social context. A cheerful tone is appropriate when confirming a successful booking. A cheerful tone is inappropriate when delivering bad news like a canceled flight. A formal tone is appropriate for professional contexts. A casual tone is appropriate for entertainment contexts. Tone labeling requires annotators to listen to the synthesized audio and judge whether the prosody, pace, and emotional coloring match the content and context. This is subjective and culturally dependent. What sounds appropriately enthusiastic to one annotator may sound insincere to another. What sounds respectfully formal in one culture may sound cold in another. Tone labeling requires careful annotator selection and calibration, ideally using annotators who match your target user demographics.

Latency perception is how responsive the voice interface feels to users, which depends not just on actual latency but on user expectations and context. A two-second pause before a voice assistant starts speaking feels acceptable if the user asked a complex question. The same two-second pause feels broken if the user asked a simple question like "What time is it?" Latency labeling asks annotators to interact with the voice system or review recordings of interactions, and to judge whether each response felt instantaneous, responsive, slow, or broken. Annotators need reference examples calibrated to specific latency values: 500 milliseconds typically feels instantaneous, 1500 milliseconds feels responsive, 3000 milliseconds feels slow, and anything above 5000 milliseconds feels broken. These thresholds vary by task type and user expectations, and they change over time as users become accustomed to faster systems.

Interruption handling evaluates how well the voice system responds when users interrupt it while it is speaking. Ideally, the system should stop speaking immediately when interrupted, process the interruption, and respond appropriately. Poor interruption handling continues speaking over the user, or stops speaking but fails to process what the user said, or processes the interruption but loses context about what it was saying before. Interruption labeling requires annotators to review or create interactions where users interrupt the system, and to label whether the system stopped promptly, whether it correctly processed the interruption, and whether it maintained conversation context. This is a multi-part label: stop latency (how fast the system went silent), processing accuracy (did it hear and understand the interruption), and context retention (did it lose track of the conversation state).

Voice labeling is more expensive than text labeling because audio review takes longer than text review, because some evaluations require audio playback equipment and quiet environments, and because some dimensions like tone appropriateness require native speakers or domain experts. Teams typically use a tiered approach: automated metrics for transcription accuracy and latency, lightweight human review for tone and interruption handling on a sample of interactions, and deep expert review for critical failures or edge cases. The key is ensuring that each component of the voice pipeline, from speech recognition through synthesis through turn-taking, gets appropriate evaluation coverage so that you can diagnose component-specific failures rather than just measuring end-to-end user satisfaction.

## The Common Mistake: Generic Quality Ratings

The pattern across all these modalities is that generic "rate the quality from 1 to 5" labeling fails because it collapses multiple distinct failure modes into a single score. A RAG system with perfect retrieval and unfaithful generation gets the same low rating as a system with terrible retrieval and faithful generation, even though the fixes are completely different. A tool use system that selects the right tool but extracts wrong parameters gets the same low rating as a system that selects the wrong tool, even though one is a parsing problem and the other is a reasoning problem. An agent that achieves the goal inefficiently gets the same rating as an agent that fails to achieve the goal, even though one needs efficiency optimization and the other needs capability improvements. A voice system with perfect content but poor latency gets the same rating as a system with good latency but poor content, even though the engineering solutions are unrelated.

Task-specific labeling means designing evaluation frameworks that match system architecture. Decompose your system into components. Identify how each component can fail. Design labels that separately measure each component's performance. Collect labels at the appropriate granularity: turn-level for chat, chunk-level for RAG retrieval, step-level for agents, component-level for voice. Use label types that match your decisions: binary for thresholds, multi-class for categorizing failure types, comparative for selecting between alternatives, scalar only when you have objective anchor points. Invest in annotator training so that labelers understand the system architecture and can reliably apply component-specific judgments.

The teams that succeed at labeling in 2026 treat evaluation design as an engineering problem, not an afterthought. They recognize that chat is not RAG is not tool use is not agents is not voice. They build evaluation frameworks specific to each modality, accept the higher cost and complexity of component-level labeling, and use the resulting diagnostic signal to drive targeted improvements. The teams that fail treat evaluation as a generic data collection problem, use one-size-fits-all quality ratings, and wonder why their labeled data does not help them improve their systems.

In the next subchapter, we turn to the operational challenge of who actually does the labeling: building and managing annotation teams, whether to use internal experts or external contractors, and how to balance cost, quality, and speed in staffing decisions.
