# 7.9 â€” Weak Supervision and Programmatic Labeling: Rules, Heuristics, and Labeling Functions

In late 2024, a financial services company building a transaction monitoring system faced a labeling crisis. They needed 200,000 labeled transactions to train a fraud detection model, but expert review cost $12 per transaction and required specialized knowledge of financial regulations that only senior analysts possessed. At $2.4 million for the full dataset, the labeling budget exceeded the entire project budget. The team considered synthetic data, active learning, and transfer learning, but none addressed the fundamental problem: they needed massive labeled datasets for a task where expert labeling was prohibitively expensive. A senior engineer proposed weak supervision: instead of paying experts to label 200,000 transactions manually, they would write labeling functions that encoded expert knowledge into programmatic rules. If a transaction involved a high-risk country, flagged certain keywords, or matched known fraud patterns, the labeling function would mark it as suspicious. If it matched known legitimate patterns, the function would mark it as clean. The functions would be noisy, sometimes wrong, and would conflict with each other, but noise-aware training methods could learn from the aggregate signal. The team wrote 37 labeling functions in three weeks, generated 200,000 noisy labels in four hours, and trained a fraud detection model that achieved 91% precision and 87% recall, comparable to models trained on fully expert-labeled data. The total labeling cost was $43,000 in engineer time to write and validate labeling functions, a 98% cost reduction compared to manual expert labeling. Weak supervision turned an impossible labeling problem into a tractable engineering problem.

Weak supervision generates labels programmatically using rules, heuristics, and labeling functions instead of manual annotation, enabling you to create large labeled datasets quickly and cheaply by encoding domain knowledge into code. This subchapter teaches you how to design labeling functions, handle label noise, combine multiple weak sources, and understand when weak supervision works and when it fails.

## The Weak Supervision Paradigm: Trading Manual Effort for Programmatic Noise

Traditional supervised learning requires clean, manually labeled data: a human expert examines each example and assigns the correct label. Weak supervision replaces manual labeling with programmatic labeling: you write functions that assign labels based on heuristics, rules, patterns, or external knowledge sources. The functions are weak because they are noisy, incomplete, and sometimes wrong. They do not label every example, they conflict with each other, and they make mistakes. But when you combine many weak labeling functions using noise-aware training, the aggregate signal is often strong enough to train effective models.

The core insight is that writing labeling functions is faster and cheaper than manual labeling at scale. Writing a function that labels 10,000 examples takes one engineer two hours. Manually labeling 10,000 examples takes ten annotators 50 hours. The function might be 80% accurate while the manual labels are 97% accurate, but if noise-aware training can extract useful signal from 80% accuracy labels, the 25x speedup and 96% cost reduction justify the quality trade-off. Weak supervision is not about eliminating manual effort entirely. It is about shifting effort from repetitive manual labeling to one-time function writing and validation, then using noise-aware methods to learn from imperfect labels.

Weak supervision works best when you have domain knowledge that can be encoded into rules, access to external knowledge sources that correlate with labels, or patterns that distinguish classes even if imperfectly. It works poorly when the task requires nuanced judgment that cannot be reduced to rules, when the classes are defined by subtle differences that heuristics cannot capture, or when domain knowledge is tacit and cannot be articulated as programmatic logic. The decision to use weak supervision depends on whether your labeling bottleneck is cost, speed, or access to expertise, and whether the task characteristics allow domain knowledge to be encoded into functions.

## Labeling Functions: Heuristics, Rules, and Knowledge Sources

A labeling function is a Python function that takes an unlabeled example as input and returns a label, an abstain decision, or multiple labels with associated confidence. Labeling functions encode domain knowledge, heuristics, keyword patterns, external lookups, or statistical rules that correlate with the target label.

The simplest labeling functions are keyword heuristics. In a sentiment classification task, a function might return positive if the text contains words like "excellent," "amazing," or "loved," return negative if it contains "terrible," "awful," or "hated," and abstain otherwise. The function is noisy because keyword presence does not guarantee sentiment: "not excellent" is negative despite containing "excellent," and "I loved how terrible it was" is ambiguous. But across thousands of examples, keyword heuristics provide useful signal that correlates with true labels.

Pattern-based labeling functions use regular expressions, templates, or structural patterns. In an email classification task, a function might label emails as receipts if the subject line matches the pattern "Receipt from" or the body contains "Order number" followed by digits. It might label emails as newsletters if the sender domain is on a known newsletter domain list or the unsubscribe link pattern is present. Pattern-based functions are more precise than keyword functions because they capture structure, but they are also more brittle because they fail when patterns change slightly.

Knowledge-based labeling functions use external databases, ontologies, or knowledge graphs. In a biomedical entity recognition task, a function might label a term as a disease if it appears in the Disease Ontology database, label it as a drug if it appears in DrugBank, or label it as a gene if it appears in the HUGO Gene Nomenclature database. Knowledge-based functions are high precision when the knowledge source is accurate and comprehensive, but they suffer from coverage gaps when entities do not appear in the knowledge source or when the knowledge source is outdated.

Statistical labeling functions use simple models, pre-trained embeddings, or distant supervision. A function might use a pre-trained sentiment classifier as a labeling function, using its predictions as noisy labels for fine-tuning a task-specific model. Another function might use cosine similarity between example embeddings and prototype embeddings for each class, labeling examples as the nearest prototype class. Statistical functions capture patterns that keyword and pattern functions miss, but they introduce model dependency and require the statistical model to be well-suited to the labeling task.

Crowdsourced labeling functions treat crowd worker labels as weak supervision rather than ground truth. If you collect three crowd labels per example and treat the majority vote as a noisy label rather than truth, you can use noise-aware training to learn from crowd labels without expensive adjudication or expert review. Crowdsourced functions are useful when crowd workers have partial knowledge but are not reliable enough to produce ground truth, and you can collect many cheap crowd labels instead of fewer expensive expert labels.

Each labeling function is independently noisy, but collectively they provide overlapping signals that noise-aware training methods can combine into accurate models. The art of weak supervision is writing diverse labeling functions that capture different aspects of domain knowledge, maximizing coverage and signal while accepting individual function noisiness.

## Noise-Aware Training: Learning from Conflicting, Noisy Labeling Functions

Labeling functions produce noisy, conflicting labels. One function labels an example positive, another labels it negative, and a third abstains. Traditional supervised learning assumes labels are ground truth and cannot handle conflicts or noise directly. Noise-aware training methods aggregate multiple noisy labels into probabilistic training targets, learning from the consensus signal even when individual functions disagree.

The foundational approach is the Snorkel label model. Snorkel treats labeling functions as noisy votes and learns a generative model that estimates the true label distribution for each example based on observed function outputs. The generative model captures function accuracy, correlations between functions, and class balance, using these estimates to produce probabilistic labels for training a discriminative model. If five labeling functions vote on an example and four vote positive while one votes negative, the label model might assign a probabilistic label of 0.87 positive based on the estimated accuracy of each function. The discriminative model is then trained on these probabilistic labels, learning from the aggregate signal rather than individual function outputs.

The label model learns function accuracies without ground truth by analyzing agreement and disagreement patterns across the dataset. If two functions agree 90% of the time, they likely have similar accuracy or are capturing the same signal. If two functions disagree 60% of the time, they are capturing different signals or have different accuracy levels. If a function agrees with the majority vote 95% of the time, it is likely high accuracy. If it agrees only 55% of the time, it is either low accuracy or capturing a rare signal that other functions miss. The label model uses these agreement patterns to estimate function parameters, then uses those parameters to generate probabilistic labels.

The label model also captures correlations between labeling functions. If two functions always agree or always disagree, they are correlated and should not be treated as independent votes. If two functions are uncorrelated, their disagreements provide useful signal for resolving conflicts. Modeling correlations prevents over-counting functions that capture the same signal and ensures that diverse functions contribute more to the final label estimate than redundant functions.

Once the label model produces probabilistic labels, you train a discriminative model using those labels as targets. The discriminative model learns feature representations that predict the true label, generalizing beyond the heuristics encoded in labeling functions. If a labeling function uses the keyword "excellent" as a positive signal, the discriminative model learns not just the keyword but also the broader context and semantic patterns that correlate with positive sentiment, enabling it to generalize to examples that no labeling function covers.

Noise-aware training transforms weak supervision from a heuristic hack into a principled machine learning approach. You write labeling functions to bootstrap the process, the label model aggregates their noisy outputs into useful training signal, and the discriminative model generalizes beyond the labeling functions to learn the underlying task.

## Coverage, Accuracy, and Overlap: Designing Effective Labeling Function Sets

A labeling function set must balance three properties: coverage, accuracy, and overlap. Coverage measures what fraction of examples at least one function labels. Accuracy measures how often functions are correct when they do label. Overlap measures how many functions label each example. Effective labeling function sets maximize coverage and accuracy while ensuring sufficient overlap for the label model to estimate function quality.

Coverage determines how much training data you can generate. If your labeling functions cover 60% of examples, you can generate noisy labels for 60% of your dataset, and the remaining 40% are unlabeled. Higher coverage enables training on more data, but chasing coverage by writing low-quality functions that label everything reduces accuracy and adds noise without adding signal. The coverage target depends on your task: for tasks with abundant unlabeled data, 40% coverage might be sufficient. For tasks with scarce data, you need 80% or higher coverage to generate enough training examples.

Accuracy determines how much noise the label model must handle. If labeling functions are 90% accurate, the label model has clean signal to work with. If they are 60% accurate, the label model must extract signal from high noise, requiring more overlap and more diverse functions to resolve conflicts. The accuracy target depends on how much labeled validation data you have for measuring function quality and how sophisticated your noise-aware training method is. Snorkel-style label models handle functions as low as 55% accuracy if you have enough overlap, but functions below 70% accuracy often add more noise than signal unless carefully designed.

Overlap determines how well the label model can estimate function accuracy and resolve conflicts. If every example is labeled by only one function, the label model cannot estimate which functions are accurate because there are no agreement patterns to analyze. If every example is labeled by five or more functions, the label model has rich agreement data and can accurately estimate function quality. The overlap target is typically three to seven functions per example: enough for the label model to learn function parameters, but not so many that you waste effort writing redundant functions.

Designing a labeling function set starts with brainstorming diverse sources of signal. List domain knowledge that correlates with labels, external knowledge sources you can query, patterns you observe in examples, and heuristics that experts use when labeling manually. Write one labeling function per signal source, focusing on high-precision functions that are correct when they fire even if they abstain often. After writing initial functions, measure coverage, accuracy, and overlap on a small labeled validation set. If coverage is too low, write additional functions that capture different signals or relax thresholds on existing functions to label more examples. If accuracy is too low, remove or refine the lowest-accuracy functions. If overlap is too low, write functions that fire on the same examples using different signals to increase agreement data.

Iterate on the labeling function set until coverage exceeds 60%, accuracy on labeled examples exceeds 70%, and average overlap exceeds three functions per example. This configuration provides enough signal for noise-aware training to produce useful probabilistic labels, and the discriminative model trained on those labels generalizes to examples that no labeling function covered.

## Combining Weak Labels with Ground Truth: Bootstrapping and Refinement

Weak supervision is most powerful when combined with small amounts of ground truth data. Ground truth labels serve as validation data for measuring labeling function accuracy, as training data for the discriminative model alongside weak labels, and as calibration data for refining labeling functions.

The bootstrapping workflow starts with a small labeled validation set, typically 500 to 2,000 examples labeled manually by experts. You write labeling functions, evaluate their accuracy on the validation set, and iterate until the function set achieves acceptable coverage and accuracy. You then apply the labeling functions to the full unlabeled dataset, run the label model to generate probabilistic labels, and train the discriminative model on the combination of weak labels and ground truth labels. The ground truth labels anchor the model to true patterns, while the weak labels provide scale and coverage. The combination often outperforms training on ground truth alone because weak labels add volume, and outperforms training on weak labels alone because ground truth corrects systematic biases in labeling functions.

Refinement loops use discriminative model predictions to improve labeling functions. After training the discriminative model on weak labels, you evaluate its predictions on the validation set and analyze disagreements between model predictions and ground truth. If the model consistently mispredicts a specific class or example type, you investigate whether labeling functions are biased, missing signal, or introducing systematic noise for that class. You refine labeling functions to fix the bias, retrain the label model and discriminative model, and measure whether validation performance improves. Refinement loops turn weak supervision into an iterative development process where model performance guides labeling function improvements.

Another refinement strategy is active learning on top of weak supervision. After training on weak labels, you use uncertainty sampling to select examples where the model is least confident, send those examples to manual labeling, add the manual labels to the training set, and retrain. Active learning prioritizes labeling budget on the examples where human labels add the most value, often achieving target performance with 10x to 50x fewer manual labels than random sampling.

Combining weak supervision with ground truth also provides safety guarantees. You can measure labeling function accuracy on held-out ground truth before deploying them to label production data, catching functions that are biased or systematically wrong. You can compare discriminative model performance on ground truth to performance on weak labels, ensuring that weak supervision is not degrading model quality relative to fully supervised baselines. You can use ground truth as a quality gate: if model performance on ground truth falls below a threshold, you reject the weak supervision approach and fall back to manual labeling.

The combination of weak supervision for scale and ground truth for validation and refinement is more powerful than either alone. Weak supervision provides volume and speed, ground truth provides accuracy and trust, and together they enable training high-quality models at a fraction of the cost of fully manual labeling.

## When Weak Supervision Works and When It Fails

Weak supervision is not a universal solution. It works well for specific task types and fails for others. Understanding when to use weak supervision and when to use manual labeling or alternative approaches is critical for labeling strategy.

Weak supervision works well when labeling requires domain knowledge that can be articulated as rules or patterns. Tasks like spam detection, content classification, entity recognition, and relationship extraction often have clear heuristics: spam emails contain certain keywords, entities appear in known databases, relationships match syntactic patterns. Domain experts can write labeling functions that encode these heuristics quickly, and noise-aware training extracts useful signal even when individual functions are noisy.

Weak supervision works well when you have access to external knowledge sources that correlate with labels. If you are labeling biomedical entities and have access to ontologies, or labeling products and have access to product catalogs, or labeling locations and have access to geographic databases, knowledge-based labeling functions provide high-precision signal at scale. The quality of weak supervision often depends on the quality and coverage of external knowledge sources.

Weak supervision works well when the cost or speed of manual labeling is prohibitive. If expert labeling costs $10 per example and you need 100,000 labels, manual labeling is a $1 million problem. If weak supervision can achieve 85% of manual labeling quality at 5% of the cost, the trade-off is obvious. If manual labeling takes three months and delays product launch, weak supervision that produces labels in one week is worth significant quality trade-offs.

Weak supervision fails when the task requires nuanced human judgment that cannot be reduced to rules. Tasks like evaluating argument quality, assessing creative writing, or judging ethical appropriateness often depend on subjective judgment, context, and cultural norms that are difficult to encode programmatically. Labeling functions for these tasks are either too simplistic to capture the judgment or too complex to write and maintain.

Weak supervision fails when labeling functions are all highly correlated and capture the same signal. If you write ten labeling functions that all use keyword matching with slightly different keyword lists, they provide redundant signal and do not enable the label model to resolve conflicts or estimate accuracy accurately. Effective weak supervision requires diverse functions that capture independent signals, and writing diverse functions is harder than writing redundant functions.

Weak supervision fails when you lack the domain expertise to write accurate labeling functions. If the task is in an unfamiliar domain and you cannot articulate heuristics or access relevant knowledge sources, labeling functions will be random guesses that add noise without signal. In these cases, manual labeling or transfer learning from related domains is more effective than weak supervision.

Weak supervision also fails when your unlabeled data distribution differs significantly from the distribution where labeling functions were designed. If you write labeling functions based on patterns observed in one dataset and apply them to a different dataset with different vocabulary, structure, or class balance, the functions will be poorly calibrated and introduce bias. Labeling function generalization requires careful validation on representative data before deployment.

The decision to use weak supervision requires honest assessment of whether the task characteristics, available knowledge sources, and expertise align with weak supervision strengths, or whether manual labeling or other approaches are more appropriate.

## Practical Weak Supervision: Tools, Workflows, and Best Practices

Implementing weak supervision in production requires tooling, workflows, and practices that go beyond the core algorithms. The most widely used weak supervision framework is Snorkel, an open-source library that provides labeling function abstractions, label model training, and integration with discriminative model training pipelines.

The weak supervision workflow starts with data exploration. Load a sample of unlabeled data, examine examples, identify patterns that correlate with labels, and brainstorm potential labeling functions. If you have a small labeled validation set, use it to guide function design by examining examples where patterns are clear versus ambiguous. If you have no labeled data, manually label 200 to 500 examples to create a validation set before writing labeling functions. Validation data is essential for measuring function accuracy and iterating on function design.

The next step is writing labeling functions. Start with simple keyword and pattern functions that are easy to write and validate. Measure their accuracy on the validation set, keep functions with accuracy above 70%, and discard or refine functions with accuracy below 60%. Add more sophisticated functions based on external knowledge sources, pre-trained models, or statistical patterns. Aim for at least ten labeling functions with diverse signals, though effective sets can range from five functions to over fifty depending on task complexity.

After writing labeling functions, analyze coverage, accuracy, and overlap. Coverage should exceed 60% of the unlabeled dataset. Accuracy on the validation set should average above 70% across all functions. Overlap should average three to seven functions per labeled example. If these targets are not met, write additional functions, refine existing functions, or relax abstention thresholds to increase coverage.

Train the label model on the labeling function outputs. The label model learns function accuracies and correlations, then generates probabilistic labels for all examples covered by at least one function. Inspect the learned function accuracies: if a function you expected to be high accuracy is estimated as low accuracy, investigate whether the function is buggy, biased, or correlated with other functions in unexpected ways.

Train the discriminative model on the probabilistic labels generated by the label model. Use the same model architecture you would use for fully supervised learning, but treat the probabilistic labels as soft targets rather than hard labels. Evaluate the discriminative model on the held-out validation set. If performance meets your target, deploy the model. If performance is below target, iterate on labeling functions, collect more validation data, or combine weak labels with a small set of manual labels to improve model quality.

Best practices include versioning labeling functions in code repositories, writing unit tests for labeling functions to catch bugs, logging function outputs on validation data to monitor accuracy over time, and maintaining a labeling function changelog to track why functions were added or modified. These practices ensure that labeling functions remain maintainable and debuggable as the project evolves.

Weak supervision is not a one-time labeling shortcut. It is an engineering discipline that requires iteration, measurement, and continuous improvement. Teams that treat it as a principled development process achieve significant cost and speed advantages over manual labeling. Teams that treat it as a quick hack produce low-quality models and waste time debugging why weak supervision failed.

## Debugging Weak Supervision: Common Failure Modes and Fixes

Weak supervision fails in predictable ways. Recognizing failure modes and applying systematic fixes turns failures into learning opportunities.

The first failure mode is low coverage. Labeling functions label only 20% of examples, leaving 80% unlabeled and giving the discriminative model too little training data. The fix is writing additional functions that fire on different example types, relaxing abstention thresholds on existing functions, or using a pre-trained model as a catch-all function that labels all examples with low confidence.

The second failure mode is low accuracy. Labeling functions are correct only 55% of the time, introducing so much noise that the label model cannot extract useful signal. The fix is removing the lowest-accuracy functions, refining functions to be more precise even if coverage drops, or collecting more validation data to measure function accuracy more reliably.

The third failure mode is low overlap. Most examples are labeled by only one function, preventing the label model from estimating function accuracy or resolving conflicts. The fix is writing additional functions that cover the same examples using different signals, or using multiple variations of the same function with different thresholds to increase overlap.

The fourth failure mode is correlated labeling functions. All functions use keyword matching with similar keyword lists, providing redundant signal that does not help the label model. The fix is diversifying function types: add pattern-based functions, knowledge-based functions, and statistical functions to complement keyword functions.

The fifth failure mode is biased labeling functions. Functions systematically over-label one class or under-label another, introducing class imbalance that degrades discriminative model performance. The fix is writing class-balancing functions that label under-represented classes, or using class weights during discriminative model training to compensate for labeling function bias.

The sixth failure mode is label model failure. The label model estimates all functions as equally accurate or assigns nonsensical accuracy estimates, producing probabilistic labels that do not improve over individual function outputs. The fix is increasing overlap so the label model has more agreement data, adding ground truth labels to anchor the label model, or simplifying the label model by removing correlation modeling if the dataset is too small to estimate correlations reliably.

Debugging weak supervision requires measuring function-level and model-level metrics, analyzing where performance degrades, and applying targeted fixes rather than indiscriminately adding more functions or more complexity. Most failures are fixable through systematic iteration on labeling functions and validation data.

Weak supervision transforms labeling from a manual data entry bottleneck into a programmatic engineering challenge. By encoding domain knowledge into labeling functions, using noise-aware training to aggregate noisy signals, and combining weak labels with small amounts of ground truth, you can generate large labeled datasets at a fraction of the cost and time of manual labeling. The next subchapter explores how to mix weak labels with expert labels in hybrid workflows, combining the scale of weak supervision with the accuracy of human judgment to build labeling pipelines that deliver both quality and efficiency.
