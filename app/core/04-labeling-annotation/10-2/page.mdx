# 10.2 — Labeling Long-Form and Multi-Step Outputs

In mid-2025, a legal technology company launched an AI assistant designed to help paralegals draft discovery responses. The assistant used a multi-step reasoning process: first analyzing the discovery request, then identifying relevant document categories, then drafting the formal response with supporting citations. The product team designed a labeling workflow where annotators rated each final response on a five-point scale for accuracy and professionalism. After three months and 847 labeled examples, the model's performance on the final response quality metric reached 91%, and the team declared victory. They shipped to production across their client base of 140 law firms. The metric looked strong, the timeline was on track, and the team was confident they had built a production-ready system.

Within six weeks, usage collapsed. Paralegals reported that the assistant frequently "went off the rails halfway through" and produced responses that started strong but ended with irrelevant citations or incomplete arguments. Usage dropped from 78% of paralegals using it daily to 22% using it at all. When the team investigated, they discovered the problem: their labeling workflow only captured whether the final output was good, not where and why the reasoning chain broke down. A response could fail because the initial request analysis was wrong, because the document categorization missed key sources, or because the drafting step introduced errors even though the prior steps were correct. But the label—a single score on the final output—gave no signal about which step failed. The model had no way to learn that step two was the failure point in one case and step four in another. The team had treated a multi-step reasoning process as if it were a single atomic output, and the result was a system that could not learn from its mistakes in any structured way. They spent four months redesigning their labeling workflow to capture segment-level judgments, re-labeling 2,400 examples, and retraining. The cost in lost client trust and engineering time exceeded $1.8 million.

The root cause was a fundamental misunderstanding of what makes long-form and multi-step outputs different from short, atomic responses. When an output has internal structure—reasoning steps, conversation turns, document sections—you cannot treat it as a single indivisible unit and expect your model to learn effectively. This subchapter teaches you how to design labeling workflows that respect the structure of complex outputs and give your models the training signal they need to improve.

## The Atomic Output Illusion

Most labeling systems are designed for atomic outputs: a single classification label, a short generated sentence, a binary yes-or-no answer. The output either succeeds or fails as a whole. This model works well for tasks where the output genuinely has no internal structure worth decomposing. If your task is "classify this support ticket into one of five categories," there is no sub-step to evaluate separately. The label is the entire output. The classification is a single decision, and you can meaningfully label it as correct or incorrect without losing information about how the system arrived at that decision.

The atomic model also works for very short generated outputs where the generation is truly indivisible. A product title, a tweet-length summary, a yes-or-no answer—these are outputs that succeed or fail as units. There is no useful segmentation to extract. You can ask whether the title is accurate and compelling, whether the summary captures the key point, whether the yes-or-no answer is correct, but you cannot meaningfully decompose these judgments into sub-judgments about different parts of the output. When the output is genuinely atomic, label it atomically.

But many modern AI systems produce outputs with rich internal structure. An agent trace might include a planning step, three tool calls, a synthesis step, and a final answer. A multi-turn conversation might span twelve exchanges between user and assistant. A generated report might include an executive summary, four analysis sections, and a recommendations list. These outputs are not atomic. They have parts, and those parts can succeed or fail independently. When you label them as if they were atomic, you lose the ability to give your model fine-grained feedback about what went wrong and where.

The legal tech case is a clean example. The discovery assistant's output was a sequence: analyze request, identify documents, draft response. Each step depended on the prior step, but each step also had its own failure modes. If you only label the final response, you cannot distinguish between a failure in step one that cascaded forward and a failure in step three that occurred despite perfect execution in steps one and two. The model sees only "this output was bad" and has no way to learn which part of its reasoning to fix. The training signal is diffuse and uninformative. The model might learn to avoid the surface features of bad final outputs, but it will not learn to fix the underlying reasoning errors that caused them.

You see the same problem in multi-turn conversation labeling. A customer support conversation might have ten turns. The assistant might handle the first eight turns perfectly, then make a critical error in turn nine that ruins the entire interaction. If you label the conversation as a whole with a single quality score, the model gets negative signal on an interaction that was 80% correct. It has no way to know that turns one through eight were fine and turn nine was the problem. It might learn to avoid patterns that appeared anywhere in the conversation, even the good parts. The training signal penalizes correct behavior along with incorrect behavior, and the result is a model that learns more slowly and less reliably than it should.

The illusion is that labeling the whole output is simpler and faster. It is simpler for the labeling interface design, and it is faster per example if you measure time spent per labeled item. But it is not simpler or faster for the overall project, because it requires far more labeled examples to achieve the same model performance, and it often results in models that plateau at lower quality levels because the training signal is too coarse to drive further improvement. Atomic labeling of non-atomic outputs is a false economy. You pay for the simplicity with worse models and longer timelines.

The teams that fall into this trap often do so because they are optimizing the wrong metric. They measure labeling throughput—examples labeled per hour—and they see that holistic labeling of complex outputs is faster than segment-level labeling. They conclude that holistic labeling is more efficient. But efficiency is not throughput per hour. Efficiency is model performance per dollar spent. Segment-level labeling might label fewer examples per hour, but each example contains more training signal, so you need fewer total examples to reach your target model performance. The true efficiency calculation requires measuring how many labeled examples you need with each approach to achieve your quality target, multiplying by the cost per example, and comparing the total costs. In most cases involving complex outputs, detailed segment-level labeling is more efficient than holistic labeling, even though it is slower per example.

## Segment-Level vs Holistic Labeling: When to Use Each

The fundamental choice in labeling long-form outputs is whether to label each segment separately, label the output as a whole, or use some combination of both. **Segment-level labeling** means breaking the output into meaningful units and evaluating each unit independently. **Holistic labeling** means evaluating the entire output as a unified artifact. Both approaches have legitimate uses, and the correct choice depends on the structure of your output and the type of learning signal your model needs.

This choice has profound implications for what your model learns. Segment-level labeling teaches the model to execute sub-tasks correctly. Holistic labeling teaches the model to produce outputs that work as wholes. Neither approach is universally superior. The right choice depends on whether the value of your output is compositional or emergent, and on whether you can meaningfully decompose quality judgments into segment-level judgments.

Segment-level labeling is appropriate when the output has a clear internal structure where each segment can succeed or fail independently, and when the model needs to learn to improve specific sub-tasks. The legal discovery assistant is a textbook case: the output has three distinct steps, each step has its own success criteria, and the model needs to learn to execute each step correctly regardless of what happened in the other steps. Labeling each step separately gives the model fine-grained signal about which parts of its reasoning process need improvement. If the model consistently fails at document identification but excels at request analysis, the segment-level labels make that pattern visible, and the training process can focus on fixing the weak step.

Holistic labeling is appropriate when the output's value depends on how the parts work together as a whole, not just whether each part is individually correct. Consider a generated marketing email. The email might have a subject line, an opening paragraph, three body paragraphs, and a call to action. You could label each segment separately, and that might be useful for some purposes. But the real question is whether the email as a whole is persuasive and on-brand. A subject line that is technically accurate but tonally inconsistent with the body creates a disjointed experience. Body paragraphs that are individually well-written but repetitive in combination create a boring email. The holistic judgment—does this email work as a persuasive communication—cannot be reduced to the sum of segment-level judgments. You need to label the whole artifact.

In practice, many tasks benefit from both levels of labeling. You label segments to give the model fine-grained learning signal about sub-task performance, and you also label the whole output to capture whether the segments cohere into a successful overall result. This dual-level approach is more labeling effort, but it produces better models when the task has both segment-level and holistic quality dimensions. The discovery assistant should have been labeled this way: rate the request analysis step, rate the document identification step, rate the drafting step, and also rate the final response as a whole for completeness and persuasiveness. The segment-level labels teach the model to execute each step correctly. The holistic label teaches the model that correct steps are not enough—the steps must also combine into a coherent and useful output.

The key is to match your labeling granularity to the structure of the task and the learning signal your model needs. If the task is genuinely compositional—the whole is the sum of the parts—then segment-level labeling is sufficient. If the task is integrative—the whole is more than the sum of the parts—then you need holistic labeling as well. And if the task has both compositional and integrative aspects, you need both levels of labeling. Do not default to holistic-only labeling just because it is simpler to implement. You will pay for that simplicity with slower learning and lower model performance.

One way to test whether you need segment-level labels is to examine your model's failure modes in development. If the model is making consistent errors at a specific sub-task—it always fails at the third step of reasoning chains, or it consistently produces weak conclusions even when the body paragraphs are strong—then segment-level labels on that sub-task will help. If the model's failures are more diffuse—sometimes it fails early, sometimes late, sometimes the individual parts are fine but the whole does not work—then you might need holistic labels or a different labeling approach altogether. Let your model's actual failure patterns guide your decision about labeling granularity, rather than making the decision based on abstract principles or labeling convenience.

## Labeling Multi-Step Reasoning Chains

Multi-step reasoning is one of the most common and most challenging structures to label well. The model generates a chain of reasoning steps—premises, inferences, intermediate conclusions—and then arrives at a final answer. Each step in the chain depends on the prior steps, but each step also has its own correctness conditions. A step might be logically valid given the prior steps but factually wrong. A step might be factually correct but logically irrelevant to the question. A step might be both correct and relevant but still lead to a wrong final answer because a later step introduces an error. Labeling these chains requires you to evaluate each step on multiple dimensions and also evaluate whether the chain as a whole constitutes sound reasoning.

The first decision is how to segment the chain. Some reasoning chains have explicit step boundaries: the model uses formatting like "Step 1," "Step 2," or the model outputs a structured trace where each step is a separate object. Other chains are less structured: the model generates a continuous paragraph of reasoning, and you must infer where one step ends and the next begins. For explicitly structured chains, use the model's own segmentation as your labeling unit. For unstructured chains, you need to define segmentation rules. A common approach is to treat each sentence or each logical inference as a segment. The segmentation does not need to be perfect, but it needs to be consistent across examples so that your labels are comparable.

Once you have segments, you need to define what you are labeling about each segment. The most basic dimension is correctness: is this step factually accurate and logically valid? But correctness alone is not enough. A step can be correct but irrelevant, correct but redundant, or correct but poorly explained. You typically want to label at least three dimensions per segment: correctness, relevance to the question, and clarity of explanation. Correctness captures whether the step is true. Relevance captures whether the step advances the reasoning toward the answer. Clarity captures whether the step is explained in a way that a user can follow. All three dimensions matter for reasoning quality, and all three need to appear in your training data for the model to learn to optimize them.

You also need to label dependencies between steps. A step might be correct in isolation but incorrect given the prior context. For example, if step two makes a false assumption and step three draws a valid inference from that assumption, step three is locally valid but globally unsound. Your labeling schema needs to capture this distinction. One approach is to add a dimension called "conditional correctness": is this step correct given the prior steps in the chain, even if those prior steps were wrong? This label teaches the model to distinguish between errors in its own reasoning and errors inherited from earlier mistakes. It is a subtle distinction, but it is critical for training models that can recognize when they have been led astray by a false premise.

Finally, you need to label the chain as a whole. Does the chain arrive at the correct final answer? Is the reasoning sound from end to end, or are there gaps and leaps? Is the chain more complex than necessary, or is it appropriately detailed for the question? These holistic judgments capture whether the model has learned to reason effectively, not just whether it can generate individually correct steps. A model that produces ten correct steps but still arrives at the wrong answer has failed at reasoning, even if it has succeeded at fact recall. The holistic label makes that failure legible in the training data.

Labeling multi-step reasoning chains is labor-intensive. Each chain might have five to fifteen steps, and each step might require three to five labeled dimensions, and you also need a holistic label on the chain as a whole. A single example might require twenty to fifty judgments. But this granularity is what allows models to learn to reason well. Coarse labels on reasoning chains produce models that can mimic the surface form of reasoning—they generate text that looks like a chain of logic—but cannot actually perform sound inference. If you are building a system where reasoning quality matters, invest in segment-level reasoning chain labels. The alternative is a model that looks like it reasons but does not.

One practical consideration is how to handle chains of varying length. A simple question might require three reasoning steps, while a complex question might require fifteen. If you use a fixed labeling schema that allocates slots for a maximum number of steps, you waste labeling effort on short chains and cannot accommodate long chains. A better approach is dynamic segmentation: labelers identify and label as many steps as exist in each chain, without a predetermined limit. This requires a more flexible labeling interface, but it ensures that your labels match the actual structure of each output rather than forcing outputs into a fixed template.

Another consideration is whether to label alternative reasoning paths. In some cases, a question has multiple valid approaches, and the model might choose one approach when another would have been equally valid or better. If you only label the path the model actually took, you cannot teach the model about alternative approaches. One solution is to have expert labelers occasionally annotate not just whether the chosen path was correct, but whether a different path would have been more efficient or more robust. This counterfactual reasoning labeling is expensive, but it helps models learn to choose better strategies, not just to execute their chosen strategy correctly. Use it sparingly on high-value examples where the choice of reasoning approach significantly affects outcome quality or efficiency.

## Handling Tool Calls and Agent Actions

Agent systems introduce a new dimension of complexity: the output is not just text but a sequence of actions. The agent might call an external tool, read a file, query a database, invoke another model, or perform a search. Each action produces a result, and the agent uses that result to decide the next action. The final output is the product of this entire action sequence. Labeling these outputs requires you to evaluate not just the final answer but the entire trajectory of actions that led to it.

The first challenge is deciding what to label. You could label only the final answer, treating the action sequence as an internal implementation detail. This approach is appealing because it is simple and because it focuses on the outcome that matters to the user. But it has the same problem as atomic labeling of multi-step reasoning: it gives the model no signal about which actions were good and which were bad. If the agent takes ten actions and the final answer is wrong, the model learns that this ten-action sequence was bad, but it does not learn whether the problem was action two, action seven, or the combination of actions three and four. The training signal is too coarse to drive effective learning.

The alternative is to label each action in the sequence. For each action, you evaluate whether the action was appropriate given the context, whether the tool call was executed correctly, and whether the result was used effectively in the next step. This segment-level labeling gives the model fine-grained feedback about its action selection and execution. If the agent consistently chooses the wrong tool at step three, the labels make that pattern visible, and the model can learn to choose better. If the agent calls the right tool but uses the result incorrectly, the labels distinguish that error from tool selection errors.

Tool call labeling typically involves at least four dimensions. First, action appropriateness: was this the right action to take at this point in the sequence? Second, parameter correctness: did the agent provide the correct parameters to the tool? Third, result interpretation: did the agent correctly understand and use the tool's output? Fourth, error handling: if the tool call failed or returned an unexpected result, did the agent handle it gracefully? All four dimensions matter for agent reliability, and all four need to be labeled for the model to learn the full skill set.

You also need to label counterfactuals: what actions should the agent have taken instead? If the agent calls tool A when it should have called tool B, your label should not just mark the action as wrong but also specify that tool B was the correct choice. This counterfactual signal is critical for training. Without it, the model learns that tool A was wrong in this context, but it does not learn what would have been right. It might avoid tool A in similar contexts but still fail to choose tool B. Counterfactual labels require more annotator expertise—the labeler must know the agent's capabilities well enough to identify the correct alternative—but they produce much stronger training signal than binary right-or-wrong labels.

The final challenge is labeling action sequences that partially succeed. An agent might take three good actions, one mediocre action, and two more good actions, and arrive at a mostly correct answer. If you label only the final outcome, you might rate this as "good," and the model learns to repeat the entire sequence, including the mediocre action. If you label each action separately, you can mark actions one through three as good, action four as mediocre, and actions five and six as good, and the model learns to keep the good actions and replace the mediocre one. Segment-level labeling of agent actions allows the model to learn from partial successes, not just from complete successes and complete failures. This is essential for agent learning, because agents operate in environments where perfect execution is rare and incremental improvement is the norm.

You also need to consider labeling action efficiency. An agent might take ten actions to accomplish what could have been done in five. Each individual action might be correct, but the sequence as a whole is inefficient. Your labeling schema should capture this. One approach is to add an efficiency dimension at the sequence level: given the task, was this sequence appropriately concise, or did it include unnecessary actions? Another approach is to label redundant actions specifically: mark actions that were correct but redundant given prior actions. This teaches the model to avoid unnecessary work, which matters both for latency and for cost when tool calls involve external API charges.

Another dimension worth labeling is agent recovery from errors. When a tool call fails or returns an unexpected result, does the agent recognize the failure and try an alternative approach, or does it proceed as if the call succeeded? Recovery behavior is critical for reliability in production, where tools fail for reasons outside the agent's control. Label recovery attempts explicitly: when a tool call fails, rate the agent's next action on whether it appropriately addressed the failure. This signal helps the model learn robust error handling, not just happy-path execution.

## Labeling Multi-Turn Conversations

Multi-turn conversations present a different labeling challenge. Each turn is both a standalone response and a contribution to an ongoing dialogue. A turn might be a good response to the immediate user message but a poor contribution to the overall conversation arc. A turn might be individually mediocre but effective in the context of the prior turns. You cannot label each turn in isolation, and you cannot label the entire conversation as a single unit without losing signal about which turns succeeded and which failed. You need a labeling approach that captures both turn-level quality and conversation-level coherence.

The standard approach is to label each turn on at least two dimensions: immediate quality and contextual appropriateness. Immediate quality measures whether the turn is a good response to the user's most recent message. Did the assistant answer the question, provide relevant information, maintain appropriate tone? This dimension treats the turn as if it were a standalone response. Contextual appropriateness measures whether the turn makes sense given the full conversation history. Does the turn acknowledge information from earlier turns? Does it avoid repeating information already provided? Does it advance the conversation toward resolution, or does it stall or backtrack? This dimension treats the turn as part of a sequence.

Both dimensions matter, and they can diverge. Consider a customer support conversation where the user asks the same question twice because the assistant's first answer was unclear. The assistant's second answer might be a perfect response to the question in isolation—high immediate quality—but the need to answer twice indicates that the first answer failed at contextual appropriateness. The assistant should have provided a clear enough answer the first time. If you label only immediate quality, you miss this failure. If you label only contextual appropriateness, you might penalize the second answer for being repetitive, even though repetition was necessary given the unclear first answer. Labeling both dimensions allows you to capture the full picture.

You also need to label conversation-level outcomes. Did the conversation resolve the user's issue? Did the user's sentiment improve, stay neutral, or worsen over the course of the conversation? How many turns did it take to reach resolution, and was that efficient or excessive? These conversation-level metrics capture whether the assistant is effective at the task level, not just at the turn level. A model might generate individually good turns but still fail to resolve issues efficiently, or it might resolve issues but leave users frustrated by the tone or length of the conversation. Conversation-level labels make these patterns visible.

One common mistake is labeling only the final turn or only the final conversation outcome. This approach treats the conversation as if the only thing that matters is where it ends up, not how it gets there. But users experience the entire conversation, not just the conclusion. A conversation that resolves the issue in turn twelve after eleven frustrating turns is not the same as a conversation that resolves the issue in turn three with two helpful turns. The conversation-level outcome might be the same—issue resolved—but the user experience is vastly different. Turn-level labels capture this difference. They teach the model to be effective at every step, not just to eventually stumble toward success.

Another mistake is labeling conversations in isolation without considering the user's conversational style and needs. Some users prefer concise answers; others prefer detailed explanations. Some users are experienced with the product; others are new. A turn that is appropriately detailed for a new user might be patronizing for an experienced user. Your labeling schema needs to account for user context. One approach is to add a user profile dimension to your labels: was this turn appropriate for this type of user? This teaches the model to adapt its responses to user characteristics, not just to the literal content of the user's message.

Labeling multi-turn conversations is time-consuming because each conversation might have five to twenty turns, and each turn might require three to five labeled dimensions, and you also need conversation-level labels. But this granularity is what allows models to learn to converse effectively. Coarse labels produce models that can generate plausible responses but cannot manage coherent multi-turn interactions. If your system involves conversation, invest in turn-level and conversation-level labeling. The alternative is a model that works in demos but fails in real extended interactions.

One additional consideration for conversation labeling is capturing missed opportunities. A turn might be adequate but not optimal. The assistant answered the question, but it could have proactively offered related information that would have prevented the next three turns. Or the assistant resolved the immediate issue but failed to address the underlying problem that will cause the user to return tomorrow with the same type of question. These missed opportunities are hard to label because they require labelers to reason about what should have happened but did not. But they are valuable training signal for building assistants that are not just reactive but genuinely helpful. One approach is to have expert labelers periodically review conversations and flag turns where an opportunity was missed, then provide a brief note about what the better action would have been. Use this labeling sparingly on high-value examples to supplement your standard turn-level labels.

Another important dimension is conversation control. In multi-turn interactions, who is driving the conversation: the user or the assistant? In some contexts, the assistant should follow the user's lead and respond to whatever the user asks. In other contexts, the assistant should guide the user through a structured process, asking clarifying questions and redirecting tangents. The appropriate level of assistant control depends on the task and the user's expertise. Your labeling schema should capture whether the assistant exercised the right amount of control for the context. Too passive, and the conversation meanders. Too controlling, and the user feels railroaded. Label this dimension at the conversation level, and consider also labeling specific turns where the assistant should have taken more or less control of the direction.

## The Challenges of Labeling Long Documents

Long-form document generation—reports, articles, documentation, summaries of lengthy source material—introduces labeling challenges that go beyond multi-step reasoning or multi-turn conversation. A document might span multiple pages, cover multiple topics, and serve multiple purposes. It has macro-level structure—sections, subsections, narrative flow—and micro-level quality—sentence clarity, factual accuracy, tone consistency. Labeling a long document as a single unit is uninformative. Labeling every sentence separately is impractical. You need a labeling strategy that captures the document's hierarchical structure and evaluates quality at multiple levels of granularity.

The most effective approach is hierarchical labeling. You divide the document into sections, label each section separately, and also label the document as a whole. For each section, you evaluate section-specific quality dimensions: does this section cover the topic it is supposed to cover, is the information accurate, is the tone appropriate, is the section well-organized internally? For the document as a whole, you evaluate document-level quality dimensions: does the document have a clear overall structure, do the sections flow logically from one to the next, is the document the right length for its purpose, does the document achieve its intended goal?

The section-level labels give the model feedback about content quality and organization at the scale where most writing decisions happen. When the model writes a section, it is making decisions about what information to include, how to organize it, what tone to use. Section-level labels teach the model to make those decisions well. The document-level labels give the model feedback about higher-order decisions: how to structure the whole document, how to balance the sections, how to create a coherent narrative arc. Both levels of feedback are necessary for the model to learn to write well-structured long documents.

One challenge is defining section boundaries. Some documents have explicit section markers—headings, numbered sections, page breaks. Others are continuous prose where section boundaries are implicit. For explicitly structured documents, use the document's own structure as your labeling unit. For unstructured documents, you need to define segmentation rules. A common approach is to segment by topic shift: when the document starts discussing a new topic or sub-topic, that is a section boundary. This requires labelers to have some domain expertise—they need to recognize when a topic shift has occurred—but it produces more meaningful segments than arbitrary divisions like "label every 500 words."

Another challenge is labeling factual accuracy in long documents. A document might contain dozens or hundreds of factual claims. You cannot label every claim individually; the labeling cost would be prohibitive. But you also cannot label the entire document as "accurate" or "inaccurate" without losing information about which parts are wrong. A practical middle ground is to label factual accuracy at the section level and also flag specific high-severity errors. For each section, you rate overall factual accuracy on a scale—"no errors," "minor errors that do not affect conclusions," "major errors that undermine the section's claims." And you also mark any specific claims that are critically wrong, such as inverted statistics or false attributions. This approach balances labeling cost with signal quality.

You also need to label coherence and flow between sections. A document might have individually good sections that do not connect well to each other. The transition from section one to section two might be abrupt, or section three might repeat information from section one without acknowledging it, or section four might assume context that was not provided in the earlier sections. These coherence failures happen at the boundaries between sections, and they require labelers to read across sections, not just within them. One approach is to add transition quality labels: for each section after the first, rate how well it connects to the prior section. This teaches the model to write sections that link together, not just sections that are internally coherent.

Finally, you need to label whether the document achieves its purpose. A report might be well-written and factually accurate but still fail if it does not answer the question it was supposed to answer. A summary might be clear and concise but still fail if it omits the most important information from the source material. A piece of documentation might be technically correct but still fail if it is too complex for its intended audience. These purpose-alignment judgments are holistic and subjective, but they are critical. They teach the model that good writing is not just technically correct writing—it is writing that accomplishes a goal for a reader. Label purpose alignment at the document level, and treat it as a primary quality metric, not a secondary nice-to-have.

When labeling long documents, you also face the challenge of source fidelity when the document is generated from source material. A summary of a research paper should accurately reflect the paper's claims and conclusions. A report synthesizing multiple sources should correctly attribute information to the right sources. Labeling source fidelity requires labelers to cross-reference the generated document against the source material, which is time-consuming. But without this labeling, your model will not learn to ground its outputs in sources, and you will get documents that sound authoritative but contain subtle distortions or misattributions. One practical approach is to label source fidelity at the section level: for each section, does it accurately reflect the relevant source material, and are any claims properly attributed? Also flag specific high-severity fidelity errors, such as claims that contradict the sources or attributions to the wrong source.

You should also label information prioritization. Long documents require selecting what to include and what to omit. A good document includes the most important information and omits or summarizes the less important details. A poor document either includes too much low-importance detail, making it hard to find the key points, or omits important information, leaving gaps in the reader's understanding. Label prioritization at both the section level—does this section focus on the right level of detail for this topic—and the document level—does the document as a whole allocate space appropriately across topics? This teaches the model not just to generate text but to make editorial judgments about what matters most.

## Interannotator Agreement on Complex Outputs

Interannotator agreement becomes significantly harder when outputs are long and structured. For atomic outputs, you can measure agreement by counting how often two labelers assign the same label. For a complex output with fifteen segments and five dimensions per segment, you have seventy-five labeled attributes per example, and agreement can vary across segments and dimensions. Simply averaging agreement across all attributes is not informative. You need to measure and report agreement at multiple levels of granularity to understand where your labeling process is reliable and where it is not.

Measure agreement separately for each dimension and each level of the hierarchy. For segment-level dimensions, calculate agreement per segment type. For example, if you are labeling reasoning chains, calculate agreement separately for premise steps, inference steps, and conclusion steps. You might find that labelers agree on premise correctness but disagree on inference validity. That tells you where to invest in clearer guidelines or additional training. For holistic dimensions, calculate agreement on the overall rating and also on the specific aspects that contribute to it. If labelers agree on the overall quality score but disagree on whether the document is well-organized, you know that the disagreement is about structure, not about overall judgment.

Also measure agreement on boundary decisions. If your labeling schema requires labelers to segment the output—decide where one section ends and the next begins—do your labelers agree on where the boundaries are? Boundary disagreement is a common source of apparent label disagreement. Two labelers might agree that a particular span of text is low quality, but if they disagree about which segment that span belongs to, their segment-level labels will not align. Measuring boundary agreement separately from content agreement helps you identify whether the problem is labeler judgment or labeler segmentation.

When agreement is low on a particular dimension or segment type, do not just accept it as inherent subjectivity. Low agreement usually means that your guidelines are unclear or that the dimension is too difficult for labelers to judge reliably. Investigate the disagreements. Read examples where labelers disagreed, and try to understand why. Often you will find that the disagreement is not about fundamental judgment differences but about ambiguity in the task definition. One labeler interprets "relevance" as relevance to the immediate question; another interprets it as relevance to the overall conversation goal. Clarify the definition, update the guidelines, and re-measure agreement. In most cases, agreement improves substantially when the task is more clearly defined.

For complex outputs, you also need to decide how to resolve disagreements. For atomic outputs, you might use majority vote or escalate to a third labeler. For complex outputs with many labeled attributes, majority vote is less practical—you might need three labelers to agree on seventy-five attributes, and the cost is prohibitive. A more practical approach is to have two labelers label independently, measure agreement, and then have one of them review the disagreements and make a final decision. This gives you the quality control benefit of dual labeling without the full cost of triple labeling. It also creates learning opportunities: the reviewing labeler sees how their colleague interpreted edge cases, and the team's shared understanding of the guidelines improves over time.

Complex outputs require more sophisticated interannotator agreement measurement and disagreement resolution processes than atomic outputs. If you apply simple atomic-output methods to complex outputs, you will either spend far too much on labeling or produce low-quality labels. Design your agreement measurement and resolution processes to match the structure of the output you are labeling.

One specific challenge with complex outputs is that different labelers may bring different domain expertise, and this affects their ability to judge different dimensions reliably. A labeler with strong domain knowledge might be excellent at judging factual correctness but less reliable at judging tone or structure. A labeler with writing expertise might excel at judging clarity and flow but struggle with technical accuracy. When you measure interannotator agreement, consider measuring it separately for different labeler expertise profiles. If two domain experts agree at 85% on factual correctness but two writing experts only agree at 65%, that tells you that factual correctness requires domain expertise. You can then allocate labelers to dimensions that match their strengths, rather than expecting all labelers to be equally good at all dimensions.

Another approach is to use tiered review for complex outputs. Have less-expert labelers handle the dimensions that require less expertise—structural quality, clarity, formatting—and have expert labelers handle the dimensions that require deep knowledge—technical accuracy, reasoning soundness, domain-specific appropriateness. This division of labor makes your labeling process more efficient and more accurate. The less-expert labelers can process more examples per hour on the simpler dimensions, and the expert labelers can focus their expensive time on the dimensions where their expertise actually matters. Measure agreement separately for each tier to ensure that both groups are performing reliably on their assigned dimensions.

## When to Label Holistically Despite Complexity

There are cases where the output is complex and structured, but labeling it segment-by-segment is the wrong choice. The most common case is when the output's value is emergent—it arises from the interaction of the parts in a way that cannot be decomposed. A piece of creative writing might have well-written sentences, well-structured paragraphs, and a clear narrative arc, but still fail because it is boring or predictable. The failure is not in any individual segment. It is in the whole. Segment-level labels would all be positive, but the holistic label would be negative, and the holistic label is the one that matters.

Another case is when segment-level labeling would require labelers to have unrealistic expertise. Consider labeling a generated legal brief. You could label each argument separately, each citation separately, each section separately. But doing so accurately requires labelers to have expertise comparable to the lawyers who will use the output. If your labelers are not lawyers, their segment-level judgments will be unreliable. But they might still be able to make a holistic judgment—does this brief look professional and well-organized, or does it have obvious red flags—that is useful for training. Holistic labeling by less-expert labelers can be more reliable than segment-level labeling by labelers who lack the expertise to judge the segments accurately.

A third case is when the training objective is holistic. If you are training a model to maximize user satisfaction, and user satisfaction is determined by the overall experience rather than by the correctness of individual segments, then holistic labels aligned to user satisfaction are the right training signal. Segment-level labels might improve segment-level metrics, but if those metrics do not correlate with user satisfaction, improving them does not improve the model's performance on the objective that matters.

The decision is not "always label segment-level for complex outputs." The decision is "understand what makes the output valuable, and label the level of granularity that captures that value." If value is compositional—the sum of the parts—then segment-level labeling is appropriate. If value is emergent—arising from the whole—then holistic labeling is appropriate. And if value has both compositional and emergent aspects, you need both levels of labeling. Match your labeling strategy to the structure of the value you are trying to create.

There is also a practical consideration around labeling cost and model improvement curves. Segment-level labeling is more expensive per example because each example requires many more judgments. But it often produces faster model improvement because the training signal is richer. Holistic labeling is cheaper per example but may require more examples to achieve the same model performance. The cost trade-off depends on your label volume and your timeline. If you have a small labeling budget and need to maximize the information extracted from each labeled example, invest in detailed segment-level labeling. If you have a large labeling budget and can label tens of thousands of examples, simpler holistic labeling might give you adequate model performance at lower per-example cost. The key is to measure the cost-benefit empirically: label a pilot set with both approaches, train models on each, measure model performance per dollar of labeling cost, and choose the approach that gives you the best performance for your budget.

Another consideration is how labeling granularity affects model behavior in production. Models trained on segment-level labels often generate outputs with better internal consistency—each step or section is individually good—but may struggle with emergent properties like overall coherence or impact. Models trained on holistic labels learn to optimize for the overall impression but may not learn to fix specific types of segment-level errors. If you see this pattern in your model's production behavior, it is a signal that you need to add the missing labeling level. If your model produces well-structured documents that nonetheless fail to achieve their purpose, add holistic purpose-alignment labels. If your model produces outputs that feel right overall but contain specific types of errors in individual sections, add segment-level error-type labels. Use production performance to guide your labeling strategy evolution.

---

The legal tech team's mistake was treating a multi-step reasoning process as a single atomic output. They paid for that mistake with months of rework and millions of dollars in lost client trust. You will face the same choice every time you design a labeling workflow for a complex output: label the structure or ignore it. Labeling the structure is harder and slower per example, but it produces better models faster and with fewer total labeled examples. Ignoring the structure is easier per example, but it produces weaker models that plateau at lower performance levels. The choice seems like a trade-off between labeling cost and model quality, but it is not. It is a choice between short-term convenience and long-term success. Choose long-term success. Label the structure. The next subchapter addresses one of the most difficult labeling challenges of all: labeling safety and harm, where the judgment calls are harder and the stakes are higher than in any other labeling domain.
