# 10.3 — Labeling Safety and Harm: The Hardest Judgment Calls

In late 2024, a social media company launched a content moderation AI designed to detect harmful posts and comments at scale. The company employed 180 content moderators who labeled training data across twelve harm categories: hate speech, violence, self-harm, sexual content, bullying, misinformation, spam, and five others. The labeling guidelines defined each category with examples and edge case rules spanning 94 pages. After six months and 340,000 labeled examples, the model achieved 89% agreement with moderator consensus on a held-out test set. The company declared the system ready for production and began using it to auto-remove content flagged as harmful. The team celebrated the 89% agreement rate as evidence of both high labeler quality and high model performance.

Within three weeks, user complaints spiked. The system was removing legitimate political speech as hate speech, flagging health education content as sexual content, and blocking crisis intervention resources as self-harm promotion. At the same time, genuinely harmful content was slipping through because it used coded language or cultural references the model had not learned to recognize. When the company investigated, they discovered that the 89% agreement number masked deep problems. Agreement was high on clear-cut cases—obvious slurs, explicit violence—but on borderline cases, labeler agreement dropped to 52%. The model had learned to recognize the easy cases but not the hard ones, and the hard ones were where most of the real-world moderation decisions happened. Worse, the labelers themselves had inconsistent mental models of what constituted harm. Some labelers applied a strict literal interpretation of the guidelines. Others applied contextual judgment, considering intent and audience. The guidelines did not specify which approach to use, so different labelers used different approaches, and the training data was a mixture of inconsistent signals. The model learned the inconsistency, and it showed in production as erratic and unreliable moderation. The company spent nine months re-calibrating labelers, rewriting guidelines, re-labeling 120,000 examples, and rebuilding trust with users. The cost in reputation damage and regulatory scrutiny exceeded $4.7 million.

The root cause was a failure to understand that safety labeling is not just harder than other labeling tasks—it is a fundamentally different kind of task that requires different processes, different labeler expertise, and different quality control mechanisms. This subchapter teaches you how to design safety labeling workflows that produce reliable, consistent, and contextually appropriate judgments even in the hardest edge cases.

## Why Safety Labeling Is Different

Most labeling tasks ask labelers to recognize patterns or apply rules. Does this email belong in the spam folder or the inbox? Is this product review positive or negative? Does this image contain a dog or a cat? These tasks have clear right answers, or at least answers that are stable across contexts. A dog is a dog regardless of who is looking at it or why. But safety labeling asks labelers to make moral and social judgments that depend on context, intent, cultural norms, and harm potential. The same piece of content might be harmful in one context and benign in another. The same words might be a slur when directed at a person and a reclaimed term when used within a community. The same image might be educational in a health context and exploitative in a different context.

Pattern-based labeling tasks also have the advantage of objective verification. You can check whether an image contains a dog by examining the image. You can check whether a review is positive by reading the text. But you cannot objectively verify whether content is harmful in the same way. Harm is not an intrinsic property of content that can be measured. It is a judgment about the potential effects of content on people, and reasonable people can disagree about those effects. This makes safety labeling fundamentally more subjective and more contentious than pattern-based labeling.

This context-dependence means that safety labeling cannot be reduced to pattern matching. You cannot give labelers a list of forbidden words and tell them to flag anything that contains those words. You cannot show them examples of harmful content and tell them to flag anything that looks similar. The judgment requires understanding intent, audience, power dynamics, and social context. It requires labelers to reason about harm, not just to recognize surface features. And because the judgment is reasoning-based rather than pattern-based, it is more cognitively demanding, more subjective, and more prone to inconsistency than pattern-based labeling.

Early content moderation systems attempted to use pattern matching by maintaining lists of prohibited terms and images. These systems failed because they could not distinguish context. They removed legitimate news articles about hate groups because the articles quoted hate speech. They blocked health education because it used anatomical terms. They allowed coded harassment because it avoided the specific words on the prohibited list. Modern safety labeling has moved beyond pattern matching to contextual reasoning, but this shift makes the labeling task fundamentally harder and requires more skilled labelers.

Safety labeling is also different because the cost of errors is asymmetric and high. A false positive in safety labeling means suppressing legitimate speech or content, which erodes user trust and can have serious consequences for discourse and access to information. A false negative means allowing harmful content to reach users, which can cause real psychological harm, enable harassment, or contribute to radicalization. Both types of errors matter, but they matter in different ways, and the relative cost depends on your product and your users. A platform for children has a different risk tolerance than a platform for political debate. Your labeling process needs to account for these asymmetric costs, not treat false positives and false negatives as equivalent.

Finally, safety labeling is different because it affects labelers. Repeated exposure to harmful content causes psychological distress, burnout, and trauma. Content moderators who label violent or sexual content for hours every day experience measurable mental health effects. This is not an abstract concern. It is a well-documented occupational hazard. Your safety labeling process must include protections for labeler well-being: exposure limits, psychological support, rotation policies, and consent protocols. You cannot design a safety labeling workflow that treats labelers as machines who can process unlimited harmful content without consequence. That approach is unethical, and it also produces worse labels, because burned-out labelers make more errors and quit more often, destroying your labeling consistency.

The psychological toll is not evenly distributed across harm categories. Labeling child exploitation content or extreme violence has more severe and lasting effects than labeling spam or low-level harassment. You need different protections for different categories. The most disturbing categories require the strictest limits, the most frequent rotation, and the most intensive support. Some organizations use specialized teams for the most harmful categories, with higher pay, mandatory counseling, and strict tenure limits. Others outsource the most disturbing categories to third-party labeling services that specialize in this work and have established support infrastructure. Neither approach eliminates the harm to labelers, but both approaches acknowledge that harm exists and must be managed.

## Harm Taxonomies: More Than a List of Categories

The foundation of safety labeling is the harm taxonomy: the structured categorization of different types of harmful content. A naive approach treats the taxonomy as a simple list of categories—hate speech, violence, sexual content—and asks labelers to assign content to one or more categories. This approach fails because it does not capture the structure of harm. Harm is not a flat set of independent categories. It is a multi-dimensional space where content can be harmful in multiple ways, where categories overlap and interact, and where the severity of harm varies continuously.

A well-designed harm taxonomy is hierarchical. At the top level, you have broad harm domains: harm to individuals, harm to groups, harm to society, harm to the platform. Each domain subdivides into more specific categories. Harm to individuals includes harassment, doxxing, threats, and non-consensual intimate content. Harm to groups includes hate speech, incitement, and dehumanization. Harm to society includes misinformation, election interference, and coordination of illegal activity. Harm to the platform includes spam, manipulation, and impersonation. Each category subdivides further into specific behaviors and content types. This hierarchical structure allows labelers to reason at the appropriate level of granularity. For some decisions, it is enough to identify the top-level domain. For others, you need to drill down to the specific behavior.

The taxonomy also needs to capture cross-cutting dimensions that apply to multiple categories. One critical dimension is intent: is the content intended to cause harm, or is it describing harm for educational or documentary purposes? A news article describing a violent event is not the same as a post glorifying that violence, even though both contain violent content. Another dimension is target: is the content directed at a specific individual, a defined group, or the public generally? A threat directed at a specific person is more immediately dangerous than a general statement of hostility. Another dimension is context: is the content shared in a private space, a public feed, a moderated community, or a space specifically designated for discussing difficult topics? The same content might be appropriate in a support group and inappropriate on a public timeline.

These cross-cutting dimensions need to be labeled explicitly, not inferred from the category label. If you label content as "violent" without labeling whether it is glorifying violence or condemning it, your model will learn to flag all violent content indiscriminately. If you label content as "hate speech" without labeling whether it is directed at an individual or a group, your model will not learn the difference between a personal insult and incitement. Make the dimensions explicit in your labeling schema, and require labelers to label them separately from the category.

The taxonomy also needs to be versioned and maintained. Harm is not static. New forms of harmful behavior emerge as platforms and social norms evolve. In 2020, COVID-19 misinformation became a major harm category that did not exist in 2019. In 2021, coordinated inauthentic behavior around elections required new subcategories and definitions. In 2024, deepfake audio and video required new policies and labeling criteria. Your harm taxonomy must evolve with the threat landscape. Assign someone to monitor emerging harms, update the taxonomy quarterly, and re-train labelers on updates. A taxonomy that was accurate in 2024 is not accurate in 2026. Treat it as a living document, not a fixed specification.

When you update the taxonomy, you face a versioning problem with your existing labeled data. If you created a new harm subcategory in 2025, your 2024 labeled data does not have labels for that subcategory. You have three options. First, you can relabel historical data with the new taxonomy, which is expensive but ensures consistency. Second, you can treat the new category as having zero examples in historical data, which is cheap but means the model has no signal for the new category until you label new data. Third, you can use heuristics or model predictions to retroactively apply the new category to historical data, which is fast but potentially noisy. The right choice depends on how important the new category is and how much historical data you have. For critical new categories, invest in relabeling a sample of historical data. For minor categories, accept the gap and label new data going forward.

## Severity Scales: The Most Contentious Design Decision

Almost every safety labeling schema includes a severity scale: a way to measure how harmful a piece of content is. Severity matters because it determines the response. Highly severe content gets removed immediately. Moderately severe content might get a warning label or reduced distribution. Low-severity content might be allowed but monitored. The severity judgment is critical for model training, because the model needs to learn not just whether content is harmful but how harmful it is.

The most common approach is a three-to-five-point ordinal scale. For example: no harm, minor harm, moderate harm, severe harm, extreme harm. Labelers rate each piece of content on this scale, and the rating determines the action. This approach is simple and intuitive, but it has a fundamental problem: severity is not one-dimensional. A piece of content might be extremely harmful to the targeted individual but minimally harmful to society. A piece of misinformation might be low severity in terms of immediate harm but high severity in terms of long-term erosion of trust. A piece of hateful content might be moderate severity in terms of explicit harm but severe in terms of normalizing bias. Collapsing these different dimensions of harm into a single severity number loses critical information.

A better approach is multi-dimensional severity. You rate severity separately on multiple axes: harm to the target, harm to the audience, harm to the platform, harm to society. Each axis has its own scale, and the combination of ratings determines the response. This approach is more complex, but it produces more nuanced and defensible moderation decisions. A threat against a specific person might be high severity on harm to the target and low severity on harm to society, and it should be removed not because it is globally harmful but because it is targeted harm. A piece of misinformation might be low severity on harm to individuals and high severity on harm to society, and it should be labeled or reduced in distribution, not removed, because the response to societal harm is different from the response to individual harm.

The other critical decision is whether to anchor the severity scale to specific outcomes or to leave it as a subjective judgment. Outcome-anchored scales define severity in terms of consequences: minor harm means the content might cause temporary distress, severe harm means the content is likely to cause lasting psychological injury or physical danger. Subjective scales ask labelers to rate how harmful the content feels without specifying what the ratings mean. Outcome-anchored scales are harder to apply—labelers must reason about potential consequences, not just react to the content—but they produce more consistent and defensible labels. Subjective scales are easier to apply but more prone to labeler drift and bias.

You also need to decide whether severity is context-dependent or intrinsic. Intrinsic severity treats content as harmful or not regardless of where it appears. A death threat is severe whether it is posted on a public timeline or sent in a private message. Context-dependent severity recognizes that the same content can be more or less harmful depending on the audience, the platform norms, and the user's expectation of safety. A graphic image of war might be expected and appropriate in a news context and shocking and harmful in a parenting forum. Most modern safety systems use context-dependent severity, because it better reflects how harm actually works. But context-dependent severity requires labelers to know the context, which means your labeling data needs to include contextual metadata, not just the content itself.

Designing a severity scale is not a one-time decision. You will iterate on it as you learn how labelers use it and how the model learns from it. Start with a simple multi-dimensional scale anchored to outcomes, collect labels, measure inter-labeler agreement, and refine the definitions based on where agreement is low. Severity scale design is as much empirical work as conceptual work. Build, measure, learn, iterate.

One common failure mode is severity inflation. Over time, labelers may start rating borderline content as more severe than it actually is, either because they become more sensitive to harm after repeated exposure, or because they want to ensure that harmful content is caught and they err on the side of higher severity ratings. This inflation makes your severity scale less useful, because the middle ratings become rare and most content clusters at the extremes. Monitor for severity inflation by tracking the distribution of severity ratings over time. If you see the distribution shifting toward higher severity, recalibrate your labelers with examples that anchor the middle of the scale. Remind them that not all harmful content is severely harmful, and that accurate severity judgments require using the full range of the scale.

You also need to consider how severity interacts with uncertainty. When labelers are unsure about content, they might rate it as more severe to be safe, or they might rate it as less severe to avoid over-moderating. Neither behavior is correct. Uncertainty about content should be labeled as uncertainty, not hidden in the severity rating. Add an uncertainty or confidence dimension to your labels: how confident is the labeler in this severity judgment? This allows you to distinguish confident high-severity labels from uncertain high-severity labels, and it teaches the model to recognize when it should defer to human judgment rather than making a confident prediction.

## Context-Dependent Safety Judgments

The hardest safety labeling decisions are the ones where context determines whether content is harmful. A word that is a slur in one context is a reclaimed term in another. An image that is educational in one context is exploitative in another. A statement that is satire in one context is incitement in another. Labelers cannot make these judgments by looking at the content in isolation. They need to see the context: who posted it, where it was posted, who the audience is, what the conversation thread was, what the community norms are.

This means your labeling workflow must present context along with content. For text content, show the full conversation thread, the user's posting history, and the community or platform where it was posted. For images or videos, show the caption, the surrounding text, and the context in which the media was shared. For user reports, show what the reporter flagged and why. Labelers cannot make context-dependent judgments from context-free data. If your labeling interface shows only the isolated piece of content, you will get labels based on the content's worst possible interpretation, because labelers have no information to interpret it any other way.

Presenting context creates a tension with labeler throughput. Context takes time to read and process. A labeler who can evaluate 100 isolated pieces of content per hour might only evaluate 30 pieces per hour if each requires reading a conversation thread and considering community norms. But this is not a bug; it is a feature. Safety labeling is not a high-throughput task. It is a high-judgment task. If you optimize for throughput, you get shallow labels based on surface features. If you optimize for judgment quality, you get slower labeling but better labels and better models. Choose judgment quality. The cost of bad safety labels—erratic moderation, user trust loss, regulatory exposure—far exceeds the cost of slower labeling.

You also need to give labelers the tools to reason about context. This often means giving them access to platform-specific knowledge. If a term is a reclaimed slur within a particular community, labelers need to know that. If a type of content is normal in one community and shocking in another, labelers need to know the norms of both communities. If a piece of satire is widely recognized as satire by the platform's users, labelers need to know that context. Build a knowledge base of community norms, cultural context, and platform-specific conventions, and make it available to labelers. Update it regularly as norms evolve. Treat it as part of the labeling infrastructure, not as optional background reading.

One effective pattern is to use tiered labeling. First-tier labelers review content in isolation and flag anything that might be harmful in any context. Second-tier labelers review the flagged content with full context and make the final judgment about whether it is harmful in this context. This two-tier approach balances throughput and judgment quality. The first tier is high-throughput and catches clear-cut violations. The second tier is lower-throughput and handles the context-dependent edge cases. You can use less-expert labelers for the first tier and more-expert labelers for the second tier, which also makes the process more cost-effective.

Context-dependent labeling is hard, and it will never achieve the same inter-labeler agreement rates as context-free labeling. That is not a sign of failure. It is a sign that you are labeling the actual complexity of the task. If your agreement rates on context-dependent safety judgments are above 70%, you are doing well. If they are above 80%, you are doing very well. Do not expect 95% agreement on hard context-dependent cases. That level of agreement only happens when you have oversimplified the task to the point where it no longer captures the judgments that matter.

One way to improve context-dependent labeling is to provide labelers with decision trees or flowcharts for common edge cases. For example, when evaluating whether a word is a slur or a reclaimed term, the decision tree might ask: Is the speaker a member of the group the term refers to? Is the term being used in a conversation within that community? Is the term being directed at someone as an insult? These questions guide labelers through the contextual reasoning process and ensure that different labelers consider the same factors. Decision trees do not eliminate disagreement, but they reduce disagreement caused by labelers using different reasoning processes. Build decision trees for your most common and most contentious context-dependent judgments, and incorporate them into your labeling guidelines and training.

Another technique is to show labelers examples of similar content with different context and different correct labels. This teaches labelers that the same content can have different labels in different contexts, and it helps them develop the mental model that safety judgments are context-dependent. Without these examples, labelers may fall back on pattern matching—they see similar content and assume it should have similar labels—which defeats the purpose of context-dependent labeling. Curate a library of minimal pairs: content that differs only in context but has different correct labels. Use these pairs in training and as reference examples during labeling.

## The Challenge of Labeling Borderline Content

The cases where safety labeling is hardest are the borderline cases: content that is not clearly harmful or clearly benign but somewhere in between. A comment that is rude but not abusive. An image that is suggestive but not explicit. A claim that is misleading but not false. These cases are the majority of real-world moderation decisions, and they are where labeler disagreement is highest and where model performance is weakest.

Borderline content creates a dilemma. If you label it as harmful, you risk over-moderating and suppressing legitimate expression. If you label it as benign, you risk under-moderating and allowing content that degrades user experience. There is no universally correct answer, because the correct answer depends on your platform's values, your user base's expectations, and the specific context. But you still need to label it, because if you do not, your model will have no training data for the most common and most difficult cases it will encounter in production.

The worst approach is to let each labeler decide individually how to handle borderline cases. This produces inconsistent labels and a model that learns inconsistency. Some labelers will err toward removing borderline content; others will err toward allowing it. The model learns both patterns, and it becomes erratic. In production, it will flag some borderline content and allow similar content to pass, and users will perceive the system as arbitrary and unfair.

A better approach is to define a policy for borderline content and train labelers to apply it consistently. The policy might be "when in doubt, do not remove"—a liberal approach that prioritizes free expression. Or it might be "when in doubt, remove or warn"—a conservative approach that prioritizes user safety. Or it might be context-dependent: "in spaces designated for children, remove borderline content; in spaces designated for adults, allow it with a warning." The specific policy matters less than the consistency with which it is applied. Once you have a policy, communicate it clearly in the labeling guidelines, train labelers on how to identify borderline cases, and measure whether labelers are applying the policy consistently.

You also need to track borderline cases separately in your labeling data. Tag them as borderline during labeling, and monitor how often labelers disagree on them. If disagreement on borderline cases is much higher than disagreement on clear cases, that is expected. But if disagreement on borderline cases exceeds 50%, your policy is not clear enough or your labelers need more training. Also monitor how the model performs on borderline cases in production. If the model's precision and recall on borderline cases is significantly worse than on clear cases, you might need more borderline training data or a more explicit policy.

Another approach is to treat borderline content as a separate category. Instead of forcing labelers to decide whether borderline content is harmful or benign, allow them to label it as borderline, and then define a separate action for borderline content. For example, borderline content might receive a warning label, reduced distribution, or additional review by human moderators. This approach acknowledges that borderline cases are not clear-cut and should not be treated as if they are. It also makes the model's job easier: the model learns to recognize borderline content and flag it for special handling, rather than learning to make the same hard judgment calls that humans struggle with.

Borderline content is where safety labeling earns its reputation as the hardest labeling task. Embrace the difficulty. Do not try to eliminate it by pretending borderline cases have clear answers. Design your labeling process and your policies to handle ambiguity explicitly and consistently.

You also need to recognize that what counts as borderline changes over time. Content that was clearly harmful five years ago might be borderline today as social norms evolve. Content that was clearly acceptable five years ago might be borderline today as awareness of harm increases. Your definition of borderline is not fixed. It moves as the boundaries of acceptable content move. This means you need to periodically recalibrate what you consider borderline. Review examples that were labeled as borderline a year ago, and assess whether they would still be labeled the same way today. If the boundaries have shifted, update your guidelines and retrain your labelers. Treat borderline as a dynamic category that reflects current norms, not a static list of examples.

## Calibrating Safety Labelers

Safety labeling requires more labeler expertise, more training, and more ongoing calibration than any other labeling task. You cannot hire labelers, give them a two-hour training session, and expect them to make consistent and reliable safety judgments. Safety labeling requires labelers to internalize complex guidelines, develop a mental model of harm, and apply that model consistently across thousands of examples. Building that capability takes weeks of training and months of practice.

The calibration process starts with initial training. New safety labelers should go through at least 20 hours of structured training before labeling production data. The training should cover the harm taxonomy, the severity scales, the context-dependence of safety judgments, the platform's policies, and the cultural and social context needed to make informed judgments. It should include extensive practice on real examples with expert feedback. Labelers should not move to production labeling until they achieve at least 75% agreement with expert consensus on a calibration test set of 200 examples covering the full range of harm categories and edge cases.

After initial training, labelers need ongoing calibration. Safety norms evolve, new harm patterns emerge, and labelers drift over time as they develop their own interpretations of the guidelines. Weekly or biweekly calibration sessions are standard practice in mature safety labeling operations. In these sessions, labelers review examples where they disagreed with each other or with expert consensus, discuss why they made different judgments, and clarify the correct interpretation. The goal is to build and maintain a shared mental model of the task across the entire labeling team.

You also need to measure labeler performance individually and provide individual feedback. Calculate each labeler's agreement rate with expert consensus, broken down by harm category and severity level. If a labeler consistently under-labels hate speech or over-labels sexual content, that is a signal that they need targeted retraining on those categories. If a labeler's agreement rate is declining over time, that is a signal of drift or burnout. Individual performance tracking allows you to intervene early before a labeler's inconsistency degrades your training data quality.

Another critical component of calibration is expert review. Not all labeling decisions should be treated as equally valid. When labelers disagree, one of them might be right and the other wrong, or both might be applying valid but different interpretations. An expert reviewer—someone with deep expertise in safety policy and the specific platform context—should review high-disagreement cases and make a final judgment. That judgment becomes the ground truth for training and for calibrating the labelers. Over time, this process aligns the entire labeling team toward the expert's interpretation, which is the interpretation the model should learn.

Calibration is expensive. It requires expert time, training time, and reduced labeling throughput. But it is not optional. Uncalibrated safety labelers produce inconsistent labels, and models trained on inconsistent labels produce erratic moderation. The cost of poor calibration is higher than the cost of good calibration. Budget for it. Build it into your timeline. Treat it as a core part of the labeling process, not as overhead to minimize.

One specific calibration challenge is handling labeler bias. Labelers bring their own values, experiences, and cultural backgrounds to safety judgments, and these differences affect how they interpret harm. A labeler from one cultural background might see certain content as clearly harmful while a labeler from another background sees it as acceptable. A labeler with personal experience of harassment might be more sensitive to borderline harassment than a labeler without that experience. You cannot eliminate these differences, and in many cases you should not want to—diverse perspectives improve safety labeling by ensuring that harm is recognized across different contexts. But you do need to prevent individual labeler biases from dominating your training data.

The solution is to ensure diversity in your labeling team and to use consensus or majority-vote labeling for safety-critical content. If every piece of content is labeled by labelers from different backgrounds, individual biases average out and the resulting labels reflect a broader view of what constitutes harm. Measure whether certain labelers consistently rate content as more or less harmful than their peers, and investigate whether those differences reflect legitimate perspective differences or calibration problems. Legitimate differences should be preserved through diverse labeling. Calibration problems should be corrected through training.

## The Tension Between Safety and Over-Censorship

One of the hardest challenges in safety labeling is navigating the tension between removing harmful content and preserving legitimate expression. Over-moderate, and you suppress speech that should be allowed, eroding user trust and stifling discourse. Under-moderate, and you allow harmful content to proliferate, degrading user experience and exposing the platform to liability. There is no perfect balance, because different stakeholders have different definitions of where the balance should be. But your labeling process needs to acknowledge the tension and build in mechanisms to manage it.

The first step is to be explicit about your platform's values and where you prioritize on the safety-versus-expression spectrum. Are you a platform that prioritizes user safety and is willing to accept some over-moderation as a cost of that priority? Or are you a platform that prioritizes open discourse and is willing to accept some under-moderation as a cost of that priority? This is a values decision, not a technical decision, and it should be made by leadership, not by labelers or machine learning engineers. Once the decision is made, communicate it clearly to your labeling team and encode it in your guidelines.

The second step is to measure both false positives and false negatives and track them separately. False positives are cases where the model flagged content as harmful when it was not. False negatives are cases where the model failed to flag harmful content. Both matter, but they matter in different ways. Track the rate of each, understand the patterns—what types of content are over-moderated, what types are under-moderated—and use that information to refine your labeling guidelines and your model training. If you only track overall accuracy, you cannot see whether your system is biased toward over-moderation or under-moderation.

The third step is to build an appeals process and use appeals data to improve labeling. Users whose content is removed should be able to appeal. A human reviewer should assess the appeal and decide whether the moderation decision was correct. Track appeal overturn rates by harm category and by severity level. If a particular category has a high overturn rate, that is a signal that your labeling for that category is poorly calibrated or that your model is learning the wrong patterns. Use appeal data as a feedback loop to improve your labeling process.

You also need to recognize that some safety decisions are inherently political and will generate backlash regardless of what you decide. Content moderation around political speech, religious expression, and cultural norms is contentious. Some users will always believe you are censoring too much; others will always believe you are allowing too much harm. Your labeling process cannot resolve these conflicts, but it can ensure that your decisions are consistent, transparent, and based on clearly articulated policies. Consistency does not eliminate disagreement, but it makes the disagreement about the policy rather than about arbitrary or biased enforcement.

Finally, consider using graduated responses rather than binary remove-or-allow decisions. Content that is borderline harmful might receive a warning label, reduced distribution, or a requirement that users confirm they want to see it, rather than being removed entirely. These intermediate responses allow you to address harm without fully suppressing speech. They also reduce the stakes of individual labeling decisions: a mistake on a borderline case results in a label or a distribution reduction, not in complete censorship. Graduated responses require more complex labeling schemas—you need to label not just whether content is harmful but what level of response is appropriate—but they give you more tools to navigate the safety-versus-expression tension.

When you implement graduated responses, your labeling schema must include the recommended action, not just the harm assessment. Labelers should indicate not only that content is moderately harmful but also that the appropriate response is a warning label rather than removal. This action recommendation should be based on explicit criteria: the severity of the harm, the likelihood of the harm, the platform context, and the value of the content. Make these criteria explicit in your guidelines so that labelers make consistent action recommendations. Without explicit criteria, different labelers will recommend different actions for similar content, and your model will learn inconsistent enforcement patterns.

You also need to monitor whether graduated responses are actually less restrictive than removal. If a warning label causes most users to avoid content, it functions like removal even though it is technically less restrictive. If reduced distribution reduces visibility to near zero, it is effectively removal. Measure the real-world impact of each level of response, and adjust your policies if intermediate responses are functioning as de facto removal. The goal of graduated responses is to preserve content that has value despite being borderline harmful. If the intermediate responses eliminate that content in practice, they are not achieving their purpose.

## Protecting Labeler Well-Being

Safety labeling exposes labelers to disturbing and traumatic content repeatedly. This exposure has documented psychological effects: increased anxiety, depression, intrusive thoughts, desensitization, and burnout. Protecting labeler well-being is an ethical obligation, and it is also a practical necessity. Burned-out labelers make more errors, have lower agreement rates, and quit, destroying your team's expertise and consistency. A safety labeling operation that does not prioritize labeler well-being will produce worse labels and higher turnover.

The most basic protection is exposure limits. Labelers should not label disturbing content for more than four hours per day, and they should have mandatory breaks every 90 minutes. These limits might reduce throughput, but they are necessary to prevent acute psychological distress. Some categories of content—child exploitation, extreme violence, graphic self-harm—should have even stricter limits. No labeler should view that content for more than two hours per day, and labelers should rotate out of those categories after a maximum of three months. Long-term exposure to the most disturbing categories causes lasting harm, and no labeling dataset is worth that cost.

You also need to provide psychological support. This includes access to licensed therapists who specialize in trauma, regular check-ins with labelers to assess their well-being, and clear protocols for what to do if a labeler is experiencing distress. It also includes creating a team culture where labelers feel safe discussing the emotional impact of the work. Safety labeling is hard work with real psychological costs. Acknowledge that openly, and provide the resources labelers need to manage those costs.

Another protection is informed consent and the right to opt out. Before labelers begin safety labeling work, they should be fully informed about the types of content they will see and the potential psychological effects. They should have the right to decline to label particular categories if they have personal trauma related to that content. For example, a labeler with a history of domestic violence should not be required to label intimate partner violence content. A labeler who has experienced sexual assault should not be required to label sexual violence content. Allowing labelers to opt out of specific categories reduces the pool of labelers for those categories, but it also reduces the risk of re-traumatization and burnout.

You also need to rotate labelers across tasks. Labelers should not spend 100% of their time on safety labeling. Alternate safety labeling with other labeling tasks that do not involve disturbing content. This rotation gives labelers mental breaks and reduces cumulative exposure. It also develops broader labeling skills and makes the work less monotonous.

Finally, monitor labeler well-being as a key performance metric alongside labeling quality and throughput. Track absenteeism, turnover, error rates, and self-reported stress levels. If any of these metrics spike, investigate and intervene. High turnover in safety labeling is not just a hiring problem; it is a sign that your labeling operation is harming people. Fix the operation, not just the hiring pipeline.

One practical protection is to use AI pre-screening to reduce labeler exposure to the most disturbing content. Before content reaches human labelers, run it through a model that identifies likely violations of the most harmful categories. Route clear violations directly to removal without human review, and route borderline cases to human labelers. This reduces the volume of extreme content that labelers must review. The model will make errors, so you need human oversight of a sample of model decisions to ensure it is not systematically removing content that should be allowed. But even with this oversight, pre-screening can reduce labeler exposure to the worst content by 70% or more, which significantly reduces psychological harm.

Another protection is batch composition. Do not give labelers eight hours of continuous exposure to a single harm category. Intermix harm categories with benign content. A labeling batch might include ten pieces of potentially harmful content and twenty pieces of clearly benign content. The benign content provides psychological breaks and reminds labelers that not all content is harmful, which helps maintain perspective and reduces the cumulative emotional impact. This approach reduces throughput on harmful content, but it is necessary for sustainable operations. Labelers who burn out after three months and quit are less productive over the long term than labelers who work at a slower pace but remain healthy and effective for years.

Protecting labeler well-being is not optional, and it is not just an HR issue. It is a core operational requirement for running a sustainable and effective safety labeling operation. Build it into your process design from the beginning.

---

The social media company's mistake was treating safety labeling as a standard labeling task with standard processes. They paid for that mistake with erratic moderation, user backlash, and regulatory scrutiny. Safety labeling is the hardest labeling task you will encounter. It requires more expertise, more calibration, more contextual judgment, and more care for labeler well-being than any other task. It also requires you to navigate inherent tensions—safety versus expression, consistency versus flexibility, policy versus judgment—that have no perfect resolution. But the difficulty is not a reason to avoid doing it well. The difficulty is the reason you must invest in doing it well. Poor safety labeling produces systems that fail at the moments that matter most, with consequences that extend far beyond model performance metrics. Treat safety labeling as the specialized discipline it is. The next subchapter addresses another layer of complexity: how labeling changes when your content spans multiple languages and cultures, and why global systems require global labeling strategies.
