# 7.3 — Confidence-Based Routing: What Gets Auto-Labeled vs Human-Reviewed

In late 2025, a financial services company building a transaction monitoring system for fraud detection made a decision that seemed prudent at the time: they configured their LLM-as-judge labeling pipeline to send every machine-generated label to human review, regardless of confidence. The reasoning was sound in principle—fraud labeling has high stakes, and they wanted maximum quality assurance. What they did not anticipate was the cost and the perverse incentive structure this created. Over four months, six fraud analysts reviewed 290,000 transaction labels generated by GPT-4o. Of those 290,000 labels, the analysts changed 19,000—an override rate of 6.5%. That means they spent time and budget confirming 271,000 labels that were already correct. At $42 per hour fully loaded and an average review time of 22 seconds per item, they spent $187,000 confirming correct labels. Worse, the flood of obviously correct drafts created fatigue and complacency. By month three, analysts were approving items in under three seconds without reading them. When the engineering team audited a sample in month four, they found that analysts had rubber-stamped 340 incorrect labels they should have caught. The universal review policy intended to maximize quality had instead wasted budget on redundant work and degraded quality through fatigue. The lesson was clear: not all items need human review, and pretending they do is both expensive and counterproductive.

Confidence-based routing is the practice of using model confidence scores to automatically route items to different review workflows: auto-approve high-confidence labels, send medium-confidence labels to a single human reviewer, and send low-confidence labels to multiple reviewers or escalation queues. This approach aligns human effort with uncertainty. Where the model is certain and historically accurate, you skip human review or sample lightly for quality assurance. Where the model is uncertain, you invest human judgment. The result is lower cost, higher throughput, and better use of expert time. But confidence-based routing only works if confidence scores are calibrated, thresholds are set correctly, and you monitor for the failure modes that arise when machines make triage decisions.

## What Confidence Means and What It Does Not Mean

When an LLM returns a label with a confidence score, that score represents the model's internal probability distribution over possible labels given the input. For a classification task with three labels—urgent, routine, informational—the model computes a probability for each label and returns the highest as the predicted label and its probability as the confidence score. A confidence of 0.94 for "urgent" means the model assigns 94% probability to that label, with the remaining 6% distributed over the other labels. This is a statement about the model's certainty, not about objective truth.

Confidence is not the same as accuracy. A model can be confidently wrong. If a model assigns 0.96 confidence to "urgent" but the correct label is "routine," high confidence did not prevent the error. Conversely, a model can be correct with low confidence. A confidence of 0.52 for "routine" when "routine" is indeed correct is still a correct label, just one the model was uncertain about. Confidence and accuracy are related but not identical. The relationship between them is called calibration.

A well-calibrated model is one where confidence scores match empirical accuracy. If the model assigns 0.90 confidence to 1,000 labels, approximately 900 of those labels should be correct. If only 700 are correct, the model is overconfident. If 950 are correct, the model is underconfident. Calibration is a property of the model and prompt together, and it varies by label type, data distribution, and task complexity. GPT-4o and Claude 3.5 Sonnet are generally well-calibrated on common tasks like sentiment classification but poorly calibrated on specialized tasks where their training data is sparse. Calibration must be measured empirically on your specific task with your specific prompt before you can trust confidence scores to route items.

## Measuring Confidence Calibration

To measure calibration, you need a labeled test set—ideally the same set you used to calibrate your LLM-as-judge against human baselines. For each item, you collect the model's predicted label and confidence score. You then group items by confidence ranges: 0.50 to 0.60, 0.60 to 0.70, 0.70 to 0.80, 0.80 to 0.90, 0.90 to 1.00. For each bin, you calculate the empirical accuracy: what percentage of labels in that bin were correct. You then plot expected confidence—the midpoint of each bin—against empirical accuracy. A perfectly calibrated model produces a diagonal line where expected equals empirical. Overconfident models produce a curve below the diagonal. Underconfident models produce a curve above the diagonal.

For example, suppose you measure a model on a sentiment classification task. In the 0.90 to 1.00 confidence bin, you have 1,200 items. The model predicted sentiment correctly on 1,090 of them, an empirical accuracy of 90.8%. This is well-calibrated. In the 0.70 to 0.80 bin, you have 800 items. The model predicted correctly on 520 of them, an empirical accuracy of 65%. The expected accuracy was 75%, so the model is overconfident in this range. For items where it reports 70% to 80% confidence, it is actually only 65% accurate. This tells you that using 75% confidence as an auto-approval threshold would result in 35% of those items being mislabeled. You need to adjust.

Some models allow you to adjust calibration by applying temperature scaling or Platt scaling to the output probabilities, techniques borrowed from traditional machine learning. Temperature scaling divides the logits by a learned temperature parameter before converting them to probabilities. If the model is overconfident, you increase the temperature, which flattens the probability distribution and reduces confidence scores. If the model is underconfident, you decrease the temperature, which sharpens the distribution and increases confidence. This recalibration is done on a validation set and applied at inference time. As of early 2026, few production LLM-as-judge systems implement temperature scaling because API-based models like GPT-4o and Claude do not expose logits. You get final probabilities, not raw logits. This limits your ability to recalibrate. The alternative is to treat confidence scores as features and learn a separate calibration model—a logistic regression or isotonic regression that maps reported confidence to empirical accuracy. This adds complexity but can significantly improve routing decisions.

If you cannot or do not want to recalibrate, you simply account for the calibration curve when setting thresholds. If you know the model is overconfident in the 0.70 to 0.80 range, you do not set an auto-approval threshold at 0.75. You set it at 0.85 or 0.90 where calibration is tighter. Calibration measurement is the foundation of threshold setting.

## Setting Auto-Approval and Escalation Thresholds

Once you understand calibration, you can set thresholds that define routing tiers. A typical three-tier system works like this. Items with confidence above the auto-approval threshold are automatically labeled without human review. Items with confidence between the auto-approval threshold and the escalation threshold are sent to a single human reviewer. Items below the escalation threshold are sent to multiple reviewers or a specialist escalation queue. The thresholds depend on your tolerance for error and the cost of human review.

Auto-approval thresholds are chosen by balancing cost savings against acceptable error rate. Suppose your model is well-calibrated and has 92% empirical accuracy at 0.90 confidence. If you set the auto-approval threshold at 0.90, you will auto-approve items with 8% error rate. Whether this is acceptable depends on the downstream consequences of labeling errors. For training data generation, 8% noise is often tolerable because model training is robust to some label noise. For real-time decision-making—routing customer messages, flagging content for removal—8% errors might be too high. You might set the threshold at 0.95 where empirical accuracy is 97%, accepting a higher review cost in exchange for lower error rate.

The math is straightforward. Suppose you have 100,000 items to label. The model assigns confidence above 0.90 to 60,000 of them. If you auto-approve those 60,000, you save 60,000 human reviews but introduce approximately 4,800 labeling errors given an 8% error rate. If human review costs $0.15 per item, you save $9,000. If each labeling error costs you $2 in downstream corrections or model degradation, those 4,800 errors cost you $9,600. You have spent more fixing errors than you saved on review. This is the cost of false confidence. You must model both sides of the equation: savings from skipped reviews and costs from accepted errors.

Escalation thresholds are chosen to identify items that need extra scrutiny. Low-confidence items are often ambiguous, edge cases, or genuinely difficult. Sending them to a single reviewer who spends 20 seconds might not be enough. You want multiple reviewers, discussion, or escalation to a subject matter expert. The escalation threshold is typically set where the model's accuracy drops below 70% or where inter-annotator agreement among humans is also low. If humans disagree on these items, a machine certainly will. These items might represent underspecified schema, missing context, or irreducible subjectivity. Escalating them surfaces schema problems early and prevents bad labels from entering your dataset.

For a concrete example, a content moderation system in 2025 used the following thresholds: auto-approve at 0.93 confidence, single review between 0.70 and 0.93, escalation below 0.70. At 0.93, the model had 96% empirical accuracy, which was acceptable for a training dataset. Between 0.70 and 0.93, accuracy ranged from 78% to 92%, so single human review caught most errors. Below 0.70, accuracy was 65%, and these items were genuinely hard—borderline hate speech, sarcasm, cultural context dependence. Those went to a three-reviewer panel with majority vote. Over six months, 58% of items were auto-approved, 35% went to single review, and 7% were escalated. Labeling cost dropped by 52% compared to universal review, and label quality improved because reviewers spent their time on hard cases rather than obvious ones.

## Confidence Drift and Distribution Shift

Confidence calibration is not static. It drifts over time as your data distribution changes. A model calibrated on customer support messages from Q1 2025 might be miscalibrated on messages from Q4 2025 if customer behavior, product offerings, or query patterns shifted. Confidence drift is insidious because the model's reported confidence scores remain in the same range—0.60 to 0.95—but their empirical meaning changes. What used to be 90% accurate at 0.90 confidence might now be 82% accurate.

The root cause is distribution shift. The model's internal probabilities reflect the training distribution. When the inference distribution diverges, probabilities become less reliable. This is compounded in LLMs because they are general-purpose models not fine-tuned on your specific task. They rely on prompt-based adaptation, which is sensitive to phrasing, edge cases, and data characteristics. If you start seeing a new class of ambiguous items—say, messages in a new language, references to a new product, or slang that emerged after the model's training cutoff—the model's confidence on those items will be poorly calibrated.

To detect confidence drift, you monitor empirical accuracy within confidence bins over time. Each week or month, you sample items from each confidence range, have humans label them, and measure accuracy. If the 0.90 to 1.00 bin drops from 92% accuracy to 85%, you have drift. You investigate what changed. Did the data source shift? Did you deploy a new model version? Did your labeling guidelines change? Once you identify the cause, you recalibrate thresholds or update the prompt to handle the new distribution.

A logistics company in early 2026 caught confidence drift this way. They used an LLM to classify shipment exception messages—damaged, delayed, lost, mis-routed. In January, accuracy at 0.88 confidence was 91%. By March, it had dropped to 84%. Investigation revealed that a new warehouse had come online with different labeling conventions for package statuses. Messages from that warehouse used abbreviations the model had not seen during calibration. The fix was to add example messages from the new warehouse to the prompt and re-calibrate thresholds. Accuracy recovered to 90% within two weeks. Without drift monitoring, they would have auto-approved thousands of mislabeled items before noticing.

## Multi-Tier Routing Architectures

Basic confidence-based routing is binary: auto-approve or review. More sophisticated systems use multi-tier routing with different review intensities. A four-tier system might look like this: Tier 1, confidence above 0.95, auto-approve with 1% random sampling for QA. Tier 2, confidence 0.85 to 0.95, single reviewer approval required. Tier 3, confidence 0.65 to 0.85, single reviewer with supervisor spot-check. Tier 4, confidence below 0.65, dual reviewers with majority vote or escalation to expert panel.

Tier 1 auto-approval with sampling is critical for audit and drift detection. Even if you trust 0.95 confidence, you should randomly review 1% to 5% of auto-approved labels to verify that calibration holds. This costs almost nothing—1% of 60,000 items is 600 reviews—but catches systematic errors before they scale. If sampling reveals that auto-approved items have higher error rates than expected, you raise the threshold or investigate prompt degradation.

Tier 2 and Tier 3 distinguish between items that need careful review and items that just need a sanity check. In Tier 2, the model is probably right, so the reviewer's job is to catch the 8% to 12% of errors. In Tier 3, the model is less certain, so the reviewer's job is to make a judgment call and the supervisor's job is to verify consistency. This tiering aligns review intensity with difficulty.

Tier 4 is where you spend the most time per item. Dual reviewers or expert panels are expensive but necessary for items the model cannot handle. These items often reveal gaps in your schema, ambiguities in definitions, or edge cases you did not anticipate. Over time, you can analyze Tier 4 items, identify patterns, and either refine the schema to make them unambiguous or add them as examples in the prompt to improve model performance. Tier 4 is both a safety valve and a feedback loop.

Some systems also route based on label type rather than just confidence. Suppose you have a multi-label classification task: customer messages can be labeled with one or more of billing, technical, account, feedback. The model might be well-calibrated on billing and technical but poorly calibrated on feedback because feedback messages are more subjective. In this case, you set different thresholds per label. Billing labels with 0.88 confidence auto-approve. Feedback labels require 0.94 confidence to auto-approve. This per-label routing reflects the reality that model performance is not uniform across the schema.

## The Cost of False Confidence

The most dangerous failure mode in confidence-based routing is false confidence: the model reports high confidence on an incorrect label. If your auto-approval threshold is 0.90 and the model assigns 0.93 confidence to a wrong label, that error bypasses human review and enters your dataset as ground truth. False confidence errors are worse than low-confidence errors because they are invisible. Low-confidence errors get reviewed and corrected. High-confidence errors do not.

False confidence arises from several sources. One is adversarial or out-of-distribution inputs. If an item contains phrasing or structure the model has never seen, it might still assign high confidence by overgeneralizing from superficially similar training examples. A classic case in content moderation is coded language. A message might use euphemisms or in-group slang to express hate speech in a way that evades the model's pattern recognition. The model sees neutral words, assigns low toxicity probability, and reports high confidence in a "safe" label. The message is actually toxic, but the model is confidently wrong.

Another source is schema ambiguity. If your label definitions overlap or leave edge cases undefined, different humans might label the same item differently. The model picks one interpretation and assigns high confidence because it does not recognize the ambiguity. For example, a customer message says "I want a refund because the product did not work as expected." Is this a billing issue or a technical issue? It is both. If your schema forces single-label classification and does not specify precedence rules, the model might confidently choose "technical" while a human would choose "billing." The confidence is real but the task is underspecified.

A third source is model limitations. LLMs as of early 2026 are prone to surface-level pattern matching and struggle with deep reasoning, negation, and counterfactuals. A sentence like "I am not unhappy with the service" is a double negative expressing mild satisfaction. A model might parse "unhappy" and assign high confidence to a "negative sentiment" label. The confidence reflects the model's certainty given its understanding, but its understanding is flawed.

To mitigate false confidence, you do three things. First, you measure calibration separately on known edge cases, adversarial examples, and out-of-distribution items. If the model is well-calibrated on typical items but overconfident on edge cases, you apply different thresholds to items flagged as atypical by heuristics. Second, you implement confidence-based sampling. Even within the auto-approve tier, you randomly sample a subset and have humans verify. If you find that 5% of high-confidence labels are wrong, you either raise the threshold or add those failure cases to your prompt as examples. Third, you track downstream model performance. If you train a model on auto-approved labels and its performance degrades, you audit the training data for false-confidence errors. Model performance is a lagging indicator but a definitive one.

## Monitoring and Adapting Routing Rules

Confidence-based routing is not a set-it-and-forget-it system. Thresholds must be monitored and adjusted as data, models, and business requirements change. You track several key metrics: auto-approval rate, override rate per tier, empirical accuracy per confidence bin, and cost per label by tier.

Auto-approval rate tells you what percentage of items bypass human review. If this rate is stable at 55%, your system is consistent. If it drops to 30%, something has changed—either the data is harder or the model's confidence has shifted. You investigate. Did you switch models? Did the prompt change? Did a new data source come online?

Override rate per tier tells you how often human reviewers change labels in each routing tier. In Tier 2, you expect override rates of 8% to 15%. If it jumps to 25%, the model's performance at that confidence range has degraded. In Tier 3, you expect 20% to 35%. If it drops to 10%, either reviewers are rubber-stamping or the model has improved and you should consider lowering the escalation threshold to capture cost savings.

Empirical accuracy per confidence bin is your calibration check. You measure this monthly or quarterly on a random sample. If accuracy in the 0.90 to 1.00 bin drops from 93% to 87%, you raise the auto-approval threshold from 0.90 to 0.93 to maintain target accuracy.

Cost per label by tier tells you where your budget is going. If Tier 4 escalations are consuming 40% of your review budget but only covering 5% of items, those items are very expensive. You decide whether to invest in improving the model's handling of those cases, refining the schema to make them less ambiguous, or accepting the cost as the price of quality on hard cases.

Routing rules should also be A/B tested. Suppose you are considering raising the auto-approval threshold from 0.90 to 0.92 to reduce false confidence errors. Before applying this globally, you run an A/B test. Half of items route using the 0.90 threshold, half using 0.92. You measure cost, error rate, and throughput for both groups. If the 0.92 group has 3% lower error rate and only 8% higher cost, you adopt it. If the error reduction is negligible, you keep 0.90 to maximize throughput. Data-driven threshold tuning prevents guesswork.

## Integrating Confidence Routing with Active Learning

Confidence-based routing and active learning are natural partners. Active learning is the practice of selecting which items to label next based on their expected value for improving the model. Items the model is uncertain about—low confidence—are often the most valuable to label because they represent the decision boundary where the model needs more data. Confidence-based routing already identifies these items by sending them to human review. You can close the loop by feeding human-reviewed low-confidence items back into model training or prompt refinement.

A typical active learning loop works like this. The model labels a batch of items and assigns confidence scores. High-confidence items auto-approve. Low-confidence items go to human review. The human-reviewed items, now labeled with ground truth, are analyzed for patterns. If many low-confidence items share a characteristic—they all mention a specific product, use a particular phrase, or come from a specific user segment—you add examples of those items to your prompt or fine-tune the model on them. The model's confidence on similar future items improves, and the boundary between auto-approve and review shifts.

For instance, a fraud detection system in 2025 used confidence-based routing to identify transaction patterns the model struggled with. Transactions involving peer-to-peer payment apps consistently received low confidence scores. Human reviewers labeled them, and the team discovered that these transactions had different fraud indicators than traditional card transactions. They created a separate prompt variant for peer-to-peer transactions with tailored examples. Confidence on those transactions improved from 0.65 average to 0.84 average. The auto-approval rate for peer-to-peer transactions went from 12% to 48%, cutting review costs for that segment by 60%.

Active learning also helps you decide when to stop labeling. If you are building a training dataset and your goal is 50,000 labeled items, you do not need to label items where the model is already confident and accurate. You focus human effort on the uncertain cases. Once the model's confidence distribution stabilizes—most items are high confidence, few are low confidence, and the low-confidence items are genuinely ambiguous rather than correctable with more data—you have reached diminishing returns. Further labeling adds volume but not diversity. Confidence-based routing gives you the signal to know when to stop.

## Ethical and Operational Considerations

Confidence-based routing raises important operational and ethical questions. One is transparency. If a system auto-approves labels without human review, and those labels affect individuals—content moderation decisions, loan application processing, medical triage—the individuals affected have a right to know that a machine made the decision and a human did not verify it. In some jurisdictions, regulations require human oversight of automated decisions. Auto-approval at 0.95 confidence might technically include human oversight if you sample 2%, but whether that satisfies legal or ethical standards depends on the context and the stakes.

Another consideration is accountability. If an auto-approved label is wrong and causes harm, who is responsible? The model developer? The team that set the threshold? The organization deploying the system? Clear governance is required. Thresholds should be set by people with authority and domain knowledge, not by default configurations. Decisions to auto-approve should be documented with the rationale: what accuracy was measured, what error rate was deemed acceptable, what sampling rate was chosen for oversight.

There is also the risk of optimization pressure. If your performance metrics reward throughput and cost reduction, there is an incentive to lower review thresholds and auto-approve more items. This can lead to a race to the bottom where quality degrades in the name of efficiency. The antidote is to pair efficiency metrics with quality metrics. Track not just cost per label but also error rate, downstream model performance, and stakeholder satisfaction. If auto-approval saves money but causes user complaints or model degradation, it is not a win.

Finally, consider annotator experience. If confidence-based routing sends only the hardest, most ambiguous items to human reviewers, those reviewers face a constant stream of difficult decisions. This is cognitively taxing. Fatigue and burnout increase. Some systems intentionally route a mix of easy and hard items to reviewers to maintain morale and provide psychological breaks. This costs slightly more but improves annotator retention and long-term quality. The most efficient system on paper is not always the most sustainable system in practice.

Confidence-based routing is a powerful technique for scaling annotation while controlling cost and quality. When calibrated correctly, monitored continuously, and integrated with active learning, it transforms LLM-as-judge from a cost-saving tactic into a strategic capability. But it requires discipline, data, and ongoing investment. The next subchapter will explore active learning in depth: how to select the most valuable items to label, how to close the loop between labeling and model improvement, and how to know when you have labeled enough.
