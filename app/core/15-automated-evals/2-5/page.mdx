# 2.5 — Tool Call Argument Validation

In October 2025, a customer support agent spent three weeks in production calling a database query tool with malformed arguments. The tool expected a date range parameter as a string in ISO 8601 format. The agent sent integers representing Unix timestamps. The tool's error handling was robust—it returned a clear error message every time. The agent's error handling was not. It retried the same malformed call up to five times per conversation before giving up and apologizing to the user. The team discovered the issue when a customer complained that every support interaction took four minutes longer than it should. Investigation revealed 18,000 failed tool calls over 21 days, each one syntactically perfect but semantically wrong. The agent passed every output format check. It never passed argument validation.

**Tool call validation** ensures that every function invocation contains arguments that match the tool's schema—correct types, required parameters present, enum values within allowed sets, nested structures properly formed. This is deterministic validation. If the schema says a parameter must be a boolean, and the model sends a string, the eval fails. If the schema requires three arguments and the model provides two, the eval fails. If an enum allows five values and the model sends a sixth, the eval fails. No judgment calls, no model-based scoring, no ambiguity. The argument either conforms to the schema or it does not.

## Schema-Based Type Validation

Every tool defines a schema: parameter names, types, constraints. Argument validation starts by checking that every provided argument matches its declared type. If the schema expects a string and the model provides a number, the validation fails. If the schema expects an array and the model provides an object, the validation fails. If the schema expects a boolean and the model provides the string "true" instead of the literal true, the validation fails unless your schema explicitly allows string-to-boolean coercion.

Type mismatches happen for predictable reasons. The model confuses similar parameters across different tools—sends a user ID as a string to one tool and as an integer to another because the training data showed both patterns. The model converts types incorrectly during multi-step reasoning—starts with a string, performs a calculation, forgets to convert back. The model hallucinates parameter names and fills them with plausible-looking but wrong-typed values. All of these produce tool calls that look reasonable in prose but fail when executed.

Type validation runs before tool execution. It deserializes the model's output, parses the tool call arguments, and compares each argument's runtime type against the schema's expected type. Libraries exist for this in every language: JSON Schema validators, TypeScript type guards, Python type checkers, OpenAPI validators. The validation is deterministic and fast—microseconds to milliseconds depending on argument complexity. If validation passes, the tool call proceeds. If it fails, the system can reject the response immediately, ask the model to retry with corrected arguments, or fall back to a default behavior. What it should never do is execute the tool call with malformed arguments and hope the tool's internal validation handles it gracefully.

## Required Versus Optional Parameters

Tool schemas distinguish required parameters from optional ones. Required parameters must appear in every tool call. Optional parameters can be omitted. Argument validation enforces both rules. If a required parameter is missing, the validation fails. If an optional parameter is present, it still must pass type validation. If an optional parameter is absent, validation skips it.

Models forget required parameters for two reasons. First, the tool description does not emphasize which parameters are mandatory, and the model treats everything as optional. Second, the model generates a tool call, then edits it during self-correction, and accidentally removes a required field while cleaning up the output. Both failure modes are common enough that required parameter validation catches errors in 5 to 12 percent of tool calls during early agent development, depending on prompt quality.

Optional parameters create a different problem: the model includes them when it should not. An optional "filter" parameter exists for advanced use cases. The model includes it in every call, often with incorrect values, because the training data showed it frequently. The tool executes successfully but returns wrong results. This is not a validation failure—the argument is optional and correctly typed—but it is a logic failure. Validation cannot catch this. It requires either constraining the model's behavior through prompting or adding semantic checks that verify optional parameters are used appropriately. The former is cheaper. The latter is more reliable.

## Enum Validation and Allowed Value Sets

Many parameters accept only specific values. A status field allows "pending," "approved," "rejected." A priority field allows "low," "medium," "high," "critical." A region field allows a fixed set of geographic codes. These are enum constraints. Argument validation checks that every enum-typed parameter contains a value from the allowed set. If the model sends "canceled" to a status field that only allows "pending," "approved," or "rejected," the validation fails.

Enum violations happen when the model generalizes from examples. The training data shows "pending" and "approved" frequently. The model infers that other status-like words are valid and sends "in_progress" or "completed." The tool rejects the call. The model retries with another plausible but invalid value. This continues until the model either guesses a valid value or gives up. Every retry wastes tokens and latency. Enum validation prevents the first bad call from ever reaching the tool.

The validator maintains the canonical list of allowed values for each enum parameter. When the model sends a tool call, the validator extracts the parameter value and checks membership in the allowed set. If the value is present, validation passes. If not, it fails. The check is a simple lookup—constant time, zero cost. The failure message can include the allowed values, which helps during debugging but does nothing for the model unless you feed validation failures back into the prompt for retry logic.

Enum validation also catches case sensitivity errors. If the allowed value is "high" and the model sends "High," validation fails unless the schema explicitly allows case-insensitive matching. Models trained on diverse datasets mix casing. Human-written tool descriptions sometimes show examples in title case or uppercase. The model learns the wrong pattern. Enum validation enforces the exact casing the tool expects. If your tools are case-insensitive, configure the validator to normalize before checking. If your tools are case-sensitive, let the validator enforce it.

## Nested Argument Validation

Tool calls often require nested structures: objects containing other objects, arrays of objects, objects with array fields. Each level of nesting has its own schema. Argument validation must recurse through the entire structure, validating types and constraints at every level.

A tool that creates a calendar event might accept an argument like this: an object with a title string, a start datetime, an end datetime, and an attendees array where each attendee is an object with a name string and an email string. Validation checks the top-level object has all required fields. It checks that title is a string, start and end are valid datetime formats, and attendees is an array. It checks that every element in the attendees array is an object. It checks that each attendee object has name and email fields, both strings, and that email matches a basic email format pattern. If any level fails validation, the entire tool call fails.

Nested validation catches errors that flat validation misses. The model provides the attendees array but fills it with strings instead of objects. The model provides attendee objects but omits the email field from some of them. The model nests the structure incorrectly—puts the attendees array inside a metadata object when the schema expects it at the top level. All of these are structural errors. All of them are deterministic. All of them are catchable before tool execution.

The recursion depth matters. Deeply nested structures take longer to validate, though still only milliseconds for reasonable schemas. Extremely deep nesting—objects nested ten levels down—can indicate schema design problems or model confusion. If your tool calls regularly require validation of six-level-deep structures, the schema is probably too complex for reliable model use. Simplify the tool interface or break it into multiple tools with flatter schemas. Validation will run faster and models will make fewer mistakes.

## Tool Call Validation as a Safety Gate

For agentic systems, tool call validation is not just a quality check. It is a safety boundary. Agents operate autonomously. They make decisions. They invoke tools that interact with external systems, modify databases, send messages, charge payment methods, update records. A malformed tool call can corrupt data, double-charge customers, send emails to wrong recipients, or delete resources that should not be deleted. The tool's own validation might catch some errors. It might not catch all of them. It might execute partially and fail halfway through, leaving inconsistent state.

Argument validation runs before tool execution, before any external side effects occur. It is the last chance to stop a bad call. If validation fails, nothing happens. The agent gets feedback, retries or falls back, and the system remains in a consistent state. If validation passes but the tool call is semantically wrong—correct structure, wrong logic—the tool executes and produces incorrect results. That requires different defenses: semantic validation, dry-run modes, confirmation steps. But structural validation prevents the entire class of errors where the agent simply does not know how to format a tool call correctly.

High-stakes agent systems run tool call validation in two places: once when the model generates the call, before sending it to the tool executor, and again inside the tool executor before execution. The first check provides fast feedback to the model. The second check acts as a guardrail in case the first check was bypassed, disabled, or compromised. Redundant validation adds negligible cost and prevents entire categories of production incidents.

The next layer of rule-based validation shifts from structure to completeness—detecting when a response is technically valid but missing content the task requires.
