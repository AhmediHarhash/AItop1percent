# 8.3 — Eval Compute: Batch vs Streaming vs On-Demand

Most teams assume that evaluation happens one way: load the dataset, run every example through the model, score the outputs, aggregate results. Batch evaluation. It works for nightly regression suites, for benchmarking model variants, for compliance reports. It does not work when you need feedback in ten seconds instead of ten minutes. It does not work when your dataset has ten million examples instead of ten thousand. It does not work when you need to evaluate live production traffic as it arrives. The assumption that all evaluation is batch evaluation is wrong, and it limits what your eval pipeline can do.

There are three primary compute modes for evaluation, and each solves a different problem. **Batch evaluation** processes a fixed dataset all at once, maximizing throughput and enabling comprehensive coverage. **Streaming evaluation** processes examples as they arrive, minimizing latency and enabling real-time feedback. **On-demand evaluation** processes specific examples when requested, enabling debugging and ad hoc exploration. Most production systems need all three. The challenge is knowing when to use which mode, and how to build infrastructure that supports all of them without duplicating logic.

## Batch Evaluation: Throughput Over Latency

Batch evaluation is the default for a reason. You have a dataset of five thousand test cases. You want to measure how the current prompt performs on all of them. You queue up the entire dataset, distribute it across workers, run each example, collect the outputs, score them, and aggregate the results into a single quality metric. The job completes in twelve minutes. You get a comprehensive view of system performance across the full distribution of inputs. This is the right compute mode when coverage matters more than latency.

The infrastructure for batch evaluation is straightforward. A scheduler triggers the job. A coordinator loads the dataset and partitions it into chunks. Workers pull chunks from a queue, execute the eval for each example, and write results to a data store. The coordinator monitors progress, handles retries for failed examples, and signals completion when all chunks are processed. Results are aggregated post-execution — average scores, percentile distributions, failure breakdowns by category. This architecture maximizes parallelism, tolerates transient failures, and scales horizontally by adding more workers.

Batch evaluation works well for regression testing. Your nightly eval suite runs a fixed set of examples against the current production configuration and compares results to yesterday's baseline. You do not need results immediately — the job can take twenty minutes, an hour, even longer if the dataset is large. What matters is completeness. Every test case runs. Every failure is recorded. Every regression is detected. The latency is acceptable because the eval is not blocking development — it runs on a schedule, and the team reviews results the next morning.

Batch evaluation also works well for model comparison. You are deciding between GPT-5 and Claude Opus 4.5 for a summarization task. You run the same eval dataset through both models in parallel batch jobs. Each job takes thirty minutes. At the end, you have aggregate quality scores, per-example comparisons, and a breakdown of which categories each model handles better. The batch job gives you the data to make an informed decision. The latency is acceptable because you are not making the decision in real time — you are conducting an experiment, and experiments can take time.

The limitation of batch evaluation is that it does not adapt to what it learns during execution. If the first hundred examples all fail in the same way, the batch job still processes the remaining four thousand nine hundred examples. This is wasteful if the goal was to detect whether a regression exists — you knew the answer after example fifty, but the job ran for another eleven minutes. Batch evaluation is comprehensive but not efficient for early stopping. If you need to fail fast, you need a different compute mode.

## Streaming Evaluation: Real-Time Feedback

Streaming evaluation processes examples as they arrive, one at a time or in micro-batches. An example comes in. The eval pipeline runs it through the scoring function. The result is written immediately. If the example fails, an alert fires. If enough examples fail within a sliding window, the pipeline signals a regression and halts further execution. This is the right compute mode when latency matters more than coverage, when you need to know about failures immediately, and when processing can stop as soon as a threshold is crossed.

The infrastructure for streaming evaluation is event-driven. Examples are published to a stream — a Kafka topic, a Kinesis stream, a Redis queue. Workers consume from the stream, execute the eval for each example, and write results to a time-series database or metrics aggregator. Alerts are configured on the aggregator — if the failure rate in the past five minutes exceeds ten percent, page the on-call engineer. If the ninety-fifth percentile latency spikes above two seconds, trigger a rollback. The system reacts to quality degradation as it happens, not after the fact.

Streaming evaluation is essential for production monitoring. Your live system is handling user requests. Each request generates a response. A background worker samples a fraction of those responses — say, five percent — and runs them through an automated eval. If the eval detects a safety violation, the system logs the example and increments a counter. If the counter exceeds a threshold within a ten-minute window, the system triggers an alert. The team investigates. The regression is caught within minutes of deployment, not hours or days later when a user complains.

Streaming evaluation also enables fast feedback during development. A developer is iterating on a prompt. Instead of waiting for a full batch eval to complete, they run the prompt in streaming mode against a live feed of examples from a test dataset. As each example is processed, the eval score is displayed in their terminal. If the first twenty examples all pass, they have early confidence that the change is safe. If the fifth example fails in a new way, they stop, investigate, and adjust. The feedback loop tightens from minutes to seconds.

The trade-off is coverage. Streaming evaluation does not wait for the full dataset to be processed before reporting results. It reports results continuously, based on whatever has been seen so far. If you have processed fifty examples and forty-eight passed, you have a ninety-six percent pass rate — but that is based on a sample of fifty, not the full five thousand. The confidence interval is wide. Streaming evaluation optimizes for speed at the cost of statistical certainty. This is acceptable when the goal is early detection, not comprehensive measurement.

## On-Demand Evaluation: Debugging and Exploration

On-demand evaluation processes specific examples when requested, rather than processing a fixed dataset on a schedule. A developer is debugging a regression. They identify the exact input that triggered the failure. They run that single input through the eval pipeline with verbose logging enabled. The pipeline shows the raw model output, the intermediate scoring steps, and the final pass or fail result. The developer sees exactly what broke, adjusts the prompt, and runs the same input again. The eval completes in three seconds. This is the right compute mode when you need to inspect specific examples, not measure aggregate performance.

The infrastructure for on-demand evaluation is a synchronous API. The client sends a request specifying the input, the prompt configuration, and the scoring function. The server executes the eval, scores the output, and returns the result. The client waits for the response — this is not a background job, it is a blocking call. The latency is low because only one example is processed, and the result is needed immediately. The use case is debugging, experimentation, and manual testing — scenarios where a human is in the loop and waiting for feedback.

On-demand evaluation is essential for human review workflows. A reviewer is inspecting flagged outputs from production. They see an output that was marked as potentially unsafe by an automated classifier. They want to understand why. They trigger an on-demand eval that runs the same input through multiple scoring functions — a toxicity classifier, a policy violation checker, a semantic similarity comparison to known bad examples. The results come back in five seconds. The reviewer sees that the toxicity classifier flagged the output but the policy checker did not. They make a judgment call based on context. The on-demand eval provided the data they needed to make the decision.

On-demand evaluation also enables interactive experimentation. A product manager wants to understand how a new prompt variant handles edge cases. They open a web UI, paste in a test input, select the prompt variant and scoring function, and click "Run Eval." The result appears in the UI, along with the raw model output and the score breakdown. They try another input. Another. They build intuition about how the prompt behaves without writing code, running batch jobs, or waiting for scheduled evals. The infrastructure makes evaluation accessible to non-engineers, democratizing quality measurement across the team.

The limitation of on-demand evaluation is scale. It is not designed to process thousands of examples. It is designed to process one example at a time, with a human reviewing the result. If you need to run a hundred examples on demand, you can call the API a hundred times in a loop — but at that point, you should use batch evaluation instead. On-demand is for spot checks, debugging, and exploration. Batch is for comprehensive coverage.

## Choosing Compute Mode by Use Case

The right compute mode depends on what you are optimizing for. If you need comprehensive coverage and can tolerate latency, use batch. If you need real-time feedback and can tolerate incomplete coverage, use streaming. If you need to inspect specific examples and can tolerate low throughput, use on-demand. Most production systems need all three, and the same eval logic should run in all three modes.

For nightly regression suites, use batch. The dataset is fixed. The schedule is predictable. You want to process every example, measure aggregate quality, and report results the next morning. Latency is not a constraint. Coverage is the goal. Batch evaluation maximizes coverage by processing the entire dataset in parallel and aggregating results at the end.

For production monitoring, use streaming. Examples arrive continuously. You need to detect regressions as they happen. You cannot wait for a batch job to complete. Streaming evaluation processes each example as it arrives, updates metrics in real time, and triggers alerts when thresholds are crossed. The goal is early detection, not comprehensive measurement.

For debugging and experimentation, use on-demand. You have a specific input that failed. You want to understand why. You run it through the eval pipeline, inspect the result, adjust the prompt, and run it again. The feedback loop is seconds, not minutes. On-demand evaluation gives you the precision and interactivity needed for root cause analysis.

For commit-triggered evals, use batch with early stopping. The dataset is fixed but the goal is fast feedback. The pipeline processes examples in order of priority — golden cases first, then tier-one edge cases, then the long tail. If golden cases fail, the pipeline stops immediately and reports failure. If they pass, the pipeline continues to tier-one cases. If enough tier-one cases pass, the pipeline declares success without processing the full dataset. This is batch evaluation with streaming-like latency — you get comprehensive coverage when things are working, and fast failure detection when things break.

## Hybrid Architectures

The most robust eval pipelines combine multiple compute modes within a single workflow. A commit triggers a fast streaming eval that processes golden cases in priority order and fails fast if quality drops. If the streaming eval passes, a batch eval queues in the background to run the full regression suite. If the batch eval passes, the commit is considered safe to deploy. If either eval fails, the commit is flagged and the developer is notified. The streaming eval provides speed. The batch eval provides coverage. Together they catch regressions without blocking iteration.

Another hybrid pattern is sampling-based streaming. Your production system handles a hundred thousand requests per day. Running every request through an eval pipeline would be prohibitively expensive. Instead, you sample five percent of requests and process them in streaming mode. The sample is large enough to detect regressions with statistical confidence, but small enough to keep compute costs reasonable. If the streaming eval detects a quality drop in the sample, a batch eval is triggered on a larger dataset to confirm the regression and measure its severity. The streaming eval provides early warning. The batch eval provides confirmation.

Hybrid architectures require unified infrastructure. The same eval logic must run in batch, streaming, and on-demand modes without modification. This means separating the core evaluation function — which takes an input, runs it through the model, and scores the output — from the orchestration layer that determines when and how to execute that function. The evaluation function is pure. The orchestration layer is mode-specific. A batch orchestrator loads the dataset and distributes it to workers. A streaming orchestrator consumes from a queue and processes examples one at a time. An on-demand orchestrator exposes a synchronous API. All three call the same evaluation function. This ensures consistency — an example evaluated in batch mode produces the same score as the same example evaluated in streaming or on-demand mode.

The investment in hybrid architecture pays off as your eval pipeline scales. Early in a project, you may only need batch evaluation. As the system reaches production, you add streaming evaluation for monitoring. As the team grows, you add on-demand evaluation for debugging and experimentation. If each mode is implemented separately, you end up with three copies of the same eval logic, and they drift over time. If each mode shares the same core function, you evolve the eval logic once and all three modes stay in sync. The architecture scales with the team's needs without fragmenting the codebase.

Your eval pipeline now knows when to run and how to process examples. The next step is scaling execution so that running ten thousand examples does not take ten times as long as running one thousand — and that requires parallelization, which changes both the infrastructure design and the failure modes you must handle.

