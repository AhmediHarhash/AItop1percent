# 5.3 — Label Disagreement and Adjudication Workflows

Three annotators review the same customer service query: "Can I return this after 60 days if it was a gift?" Annotator A labels the correct response as "Yes, with gift receipt." Annotator B labels it "No, 30-day policy applies." Annotator C labels it "Escalate to supervisor." All three are experienced reviewers. All three have access to the same policy documentation. None of them are wrong — they are interpreting ambiguity differently. One assumes a gift receipt was provided. One assumes it was not. One flags the ambiguity as unresolvable without more information. This single input now has three conflicting golden answers, and your golden set requires exactly one.

## Why Annotators Disagree

Label disagreement is not a sign of poor annotator quality. It is a sign that the task contains ambiguity, subjectivity, or complexity that no amount of training can eliminate. Disagreement happens when the input is genuinely ambiguous. It happens when the correct answer depends on context the annotator does not have. It happens when policy documentation is incomplete or contradictory. It happens when the task requires judgment calls about tone, relevance, or appropriateness — dimensions where reasonable humans differ.

A content moderation team building a golden set for hate speech detection found that annotators agreed on clear-cut cases — slurs, explicit threats — 98% of the time. For borderline cases — sarcasm, in-group reclamation, context-dependent insults — agreement dropped to 52%. The disagreement was not random. It correlated with annotator demographics, cultural background, and personal experience with the content. The task was asking humans to make subjective judgments about harm, and humans disagreed.

Disagreement also happens when annotators misunderstand the task, skip parts of the input, or apply inconsistent standards. These are quality problems, not ambiguity problems, and they require different solutions. But even after fixing quality problems, some level of disagreement is irreducible. The question is how to measure it and what to do when it exceeds acceptable thresholds.

## Measuring Inter-Annotator Agreement

**Inter-annotator agreement** is the metric that quantifies how often annotators agree on the same input. The simplest version is raw agreement: if three annotators label 100 inputs, on how many inputs do all three assign the same label? If all three agree on 85 inputs, raw agreement is 85%.

Raw agreement is intuitive but misleading when some labels are much more common than others. If 90% of inputs are clearly positive and only 10% are borderline, annotators will agree 90% of the time just by chance. A better metric is **Cohen's kappa** for two annotators or **Fleiss' kappa** for multiple annotators. Kappa adjusts for chance agreement and ranges from negative one to one, where one is perfect agreement, zero is chance-level agreement, and negative values indicate systematic disagreement.

A kappa above 0.8 is considered strong agreement. Between 0.6 and 0.8 is moderate agreement. Below 0.6 suggests the task is too ambiguous, the guidelines are too vague, or annotators need better training. A legal contract analysis company measured kappa for clause classification and found 0.92 for straightforward clause types like "payment terms" but 0.51 for ambiguous types like "implied warranties." The low kappa on implied warranties signaled that the annotation guidelines were incomplete. They revised the guidelines, added examples, and retrained annotators. Kappa improved to 0.74 — still moderate but workable.

Kappa is not the only metric. For tasks with graded outputs — scoring relevance on a scale, rating tone, assigning confidence levels — you can use **intraclass correlation** or **Krippendorff's alpha**. For tasks with structured outputs, you can measure agreement field by field. The metric choice depends on your task structure, but the principle is constant: measure agreement, set thresholds, and investigate when agreement falls below acceptable levels.

## Adjudication Workflows: Deciding the Golden Answer

When annotators disagree, someone must decide which answer becomes the golden label. This process is called **adjudication**, and it requires a workflow that balances speed, cost, and quality.

The simplest adjudication workflow is **majority vote**: if two out of three annotators agree, their answer becomes the golden label. Majority vote works when disagreements are rare and when the majority is likely to be correct. It fails when all three annotators choose different labels — there is no majority — or when the majority is systematically biased. A healthcare summarization task used majority vote and discovered that two junior annotators consistently agreed with each other but were both wrong according to the senior clinical expert who reviewed their work. Majority vote had encoded junior-level mistakes as ground truth.

A more robust workflow is **expert adjudication**: when annotators disagree, escalate to a senior reviewer or domain expert who makes the final call. Expert adjudication is slower and more expensive but produces higher-quality golden labels. The trade-off is cost versus trust. A financial compliance system used expert adjudication for all disagreements. The expert reviewed both conflicting labels, re-read the policy documentation, and made a definitive ruling. The process added three days to golden set production but eliminated the risk that ambiguous or incorrect labels would corrupt the ground truth.

Some workflows use **tiered adjudication**: low-confidence disagreements go to a senior annotator, high-stakes disagreements go to a domain expert, and straightforward disagreements are resolved by majority vote. This balances cost and quality by reserving expert time for cases that need it most. A content moderation team routed disagreements through three tiers: majority vote for clear cases, senior moderator review for borderline cases, and policy committee review for cases that revealed gaps in the moderation policy itself.

## When to Average vs Break Ties

For graded outputs — relevance scores, quality ratings, confidence levels — you have a choice between averaging disagreeing scores or forcing a single tie-breaking decision. Averaging preserves information about annotator uncertainty. If three annotators rate an output as 6, 7, and 8 on a 10-point scale, the average is 7 with a standard deviation that signals moderate disagreement. You can use this distribution in your eval: outputs with low annotator variance are high-confidence ground truth, outputs with high variance are uncertain.

Averaging works when the underlying scale is meaningful and when slight differences in score do not change the interpretation. It fails when you need a binary decision — pass or fail, correct or incorrect, safe or unsafe. A medical diagnosis system evaluated outputs against golden answers graded by three physicians on a 5-point accuracy scale. Averaging the scores gave them a continuous ground truth metric. But for regulatory reporting, they needed binary correctness. They introduced a threshold: any output with an average score above 4.0 was considered correct, below 3.0 was incorrect, and between 3.0 and 4.0 was flagged for expert review. The threshold converted graded agreement into actionable decisions.

For categorical labels, you cannot average — you must break ties. Tie-breaking strategies include assigning higher weight to more experienced annotators, escalating to an expert, or using a fourth annotator to resolve the deadlock. A legal contract system used weighted voting: senior annotators' votes counted 1.5 times a junior annotator's vote. This reduced the frequency of true ties and ensured that when disagreement occurred, experience carried more weight.

## Expert Escalation: When the Answer Requires Authority

Some disagreements cannot be resolved by voting or averaging because they represent genuine ambiguity in the underlying task. The input has no single correct answer, or the correct answer depends on context the annotators do not have, or the disagreement reveals a gap in the policy or guidelines. These cases require **expert escalation** — routing the input to someone with the authority to make a definitive ruling or to clarify the task itself.

A customer service chatbot golden set encountered repeated disagreements on return policy questions where the input did not specify whether the product was defective, unwanted, or damaged in shipping. Annotators disagreed because the correct answer depended on information not present in the input. The team escalated these cases to the product manager responsible for return policy. The product manager made two decisions: first, the chatbot should ask clarifying questions when the input is ambiguous, and second, the golden answer for ambiguous inputs should be the clarifying question, not an assumed answer. This ruling resolved the disagreement and changed the task definition.

Expert escalation is expensive. It pulls senior people into annotation workflows. It slows down golden set production. But it is the only way to handle cases where the disagreement signals a deeper problem. Trying to force a decision through majority vote or arbitrary tie-breaking in these cases produces golden labels that are wrong or that encode assumptions you do not want the model to learn.

## The No Right Answer Cases

Some inputs have no right answer. The question is ambiguous beyond resolution. The task is asking for a subjective judgment where reasonable humans will always differ. The correct answer depends on information that does not exist. These cases should not be in your golden set — or if they are, they should be labeled explicitly as unresolvable.

A content recommendation system built a golden set of user queries paired with the "best" content recommendation. For straightforward queries — "how to reset password" — there was a clear best answer. For subjective queries — "what should I read next?" — annotators disagreed because "best" depended on user preferences the query did not reveal. The team added a label category: "insufficient information." Queries labeled as insufficient information were excluded from the golden set used for accuracy evaluation but included in a separate set used to evaluate the model's ability to recognize when it should ask for more context.

Admitting that some cases have no right answer is uncomfortable. It feels like giving up. But including unresolvable cases in your golden set as if they had definitive answers corrupts your ground truth. The model will be penalized for failing to match an answer that was arbitrary. Worse, the model might learn the arbitrary answer and reproduce it, encoding randomness as if it were correctness.

## Adjudication as a Feedback Loop to Guidelines

Every disagreement that requires adjudication is a signal that your annotation guidelines need improvement. If annotators consistently disagree on a specific type of input, the guidelines are not clear enough for that case. If expert adjudication repeatedly overturns majority vote, the annotators are misunderstanding the task. If certain annotators disagree with everyone else, they need retraining or removal.

A financial services company tracked adjudication decisions and analyzed patterns. They found that 40% of disagreements involved a specific clause type: force majeure. They revised the annotation guidelines to include six examples of force majeure clauses with detailed explanations of edge cases. They retrained annotators on the new guidelines. Disagreement on force majeure cases dropped from 40% to 8%. The adjudication process identified the gap, and the guidelines update closed it.

Adjudication is not just about resolving individual disagreements. It is about using disagreement as a diagnostic tool. High disagreement rates tell you where your task definition is ambiguous, where your annotators need more training, where your policies need clarification, and where your golden set should not attempt to impose a single answer on inherently subjective cases.

## Building Trust in the Golden Set Through Transparency

When your golden set is built through a rigorous adjudication process — inter-annotator agreement is measured, disagreements are escalated appropriately, and no-right-answer cases are flagged — your team trusts the ground truth. When golden labels are produced by opaque processes, inconsistent tie-breaking, or arbitrary decisions, trust erodes. Engineers stop believing the eval results. Product managers question whether regressions are real. The golden set becomes just another dataset instead of the authoritative reference it needs to be.

Transparency means documenting the adjudication workflow, publishing inter-annotator agreement metrics, and making expert rulings traceable. It means admitting when a golden label is uncertain and tagging it as such. It means revisiting adjudication decisions when new information or new policy changes the correct answer. A healthcare AI company published a changelog for their golden set: every time an adjudication ruling changed a label, they logged the change, the reason, and the expert who made the call. This transparency let engineers understand why eval results shifted and trust that the shifts reflected real ground truth updates, not annotation noise.

The next step after resolving label disagreements is managing the golden set as it evolves — versioning it, tracking how labels change over time, and ensuring that every eval run is anchored to a consistent, well-defined ground truth.

