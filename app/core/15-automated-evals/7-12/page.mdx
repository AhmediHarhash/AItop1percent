# 7.12 — Safety Eval Governance and Threshold Management

Most teams treat safety thresholds as technical parameters. They set a pass rate, maybe 95% or 99%, and use that number to gate deployments. The threshold feels objective, data-driven, defensible. It is none of those things. A safety threshold is a risk decision dressed up as a technical metric. Who decided that 95% was acceptable? What happens to the 5% of cases that fail? Who is accountable when one of those failures causes harm? These are not engineering questions. They are governance questions, and most organizations answer them poorly or not at all.

The teams that build safe AI systems understand that safety evaluation is a sociotechnical system. The technical layer, the automated test suite, the pass rates, the CI/CD integration, all of that is necessary. But it sits inside a governance layer that defines who owns safety decisions, how thresholds are set and updated, what compliance obligations apply, and how the organization responds when safety evals fail. Without that governance layer, you have a sophisticated testing system with no one clearly responsible for the outcomes it measures.

## Who Owns Safety Evals

Safety evaluation requires clear ownership at multiple levels. At the operational level, a safety engineering team builds and maintains the test suite, writes test cases, integrates red team findings, and ensures the eval pipeline runs correctly. They own the infrastructure. But they do not own the risk decisions. They should not be the people deciding what constitutes an acceptable failure rate for self-harm content or medical misinformation.

That decision sits with a safety governance body, typically a cross-functional group including representatives from Product, Engineering, Legal, Trust and Safety, and executive leadership. This group sets safety policies, defines risk tolerances, approves threshold changes, and makes deployment decisions when safety evals fail. They own the risk. The safety engineering team provides them with data. The governance body decides what to do with it.

One healthcare technology company structured this clearly. The safety engineering team ran the automated eval suite and reported results daily. When safety evals showed failure rates above threshold, they escalated to a Safety Review Board that met within twenty-four hours. The board included the VP of Engineering, the Chief Medical Officer, the General Counsel, and the head of Trust and Safety. The board reviewed the failures, assessed the risk, and decided whether to block the deployment, roll back a change, or accept the risk with mitigations. The safety engineers provided technical analysis. The board made the call.

This separation of concerns is critical. Engineers should not bear sole responsibility for risk decisions that have legal, medical, or reputational consequences. Executives should not make safety threshold decisions without technical context. The governance model brings both together, ensuring that safety decisions are informed by engineering reality and owned by people with accountability for business outcomes.

## Threshold Ownership

Every safety threshold is a policy decision. When you set a threshold that says "we deploy if harassment detection passes 97% of test cases," you are making a statement about acceptable risk. You are saying that failing to detect harassment in 3% of cases is a risk the organization is willing to accept. That decision should be explicit, documented, and owned by someone with authority.

Different risk categories require different thresholds. Self-harm content might require a 99.5% pass rate because the consequences of failure are severe and immediate. Tone policy violations might accept a 90% pass rate because the consequences are lower stakes. These thresholds are not derived from data. They are derived from risk appetite, regulatory requirements, and business context. The governance body sets them based on those factors.

Thresholds also vary by deployment context. A model serving internal employees might accept a lower safety threshold than a model serving external users. A model in a regulated industry like healthcare or finance might require higher thresholds than a model in entertainment. A model serving minors requires stricter safety standards than a model serving adults. The same model, deployed in different contexts, has different risk profiles and therefore different threshold requirements.

One financial services firm used tiered thresholds. For Tier 1 risks, anything that could cause direct financial harm or regulatory violations, the threshold was 99.9% pass rate with zero tolerance for critical failures. For Tier 2 risks, brand reputation or customer satisfaction issues, the threshold was 98%. For Tier 3 risks, minor policy violations with no external impact, the threshold was 95%. These thresholds were set by the Risk Committee, documented in the company's AI governance policy, and reviewed quarterly. Every deployment was evaluated against the appropriate threshold for its risk tier.

The thresholds are not static. As the model improves, as the test suite expands, as the organization's risk appetite changes, the thresholds update. The governance body reviews threshold performance quarterly. If the model consistently exceeds the threshold by a wide margin, they consider raising it. If the model struggles to meet the threshold, they investigate why and decide whether to lower the threshold, improve the model, or restrict the deployment scope.

## Safety Eval Versioning

Safety evaluation suites evolve continuously. New test cases are added. Old test cases are refined. Risk categories are reorganized. When the eval suite changes, the pass rates change, even if the model does not. A model that passed 98% of test cases last month might pass only 94% this month because you added five hundred new test cases targeting a previously under-covered risk area. This creates a governance problem. Are you measuring model improvement or test suite expansion?

The solution is to version the safety eval suite and track model performance against specific versions. Each version of the suite is tagged with a version number, a date, and a changelog describing what changed. When you report safety metrics, you report them with the suite version. "Model v47 passed 97.2% of safety eval suite v12." When you update the suite, you rerun the current production model against the new version to establish a baseline. "Model v47 passed 94.1% of safety eval suite v13." The drop is expected because the suite added new coverage.

This versioning discipline prevents confusion and ensures comparability. You can track model improvement over time by testing each model version against the same eval suite version. You can track eval suite expansion by testing the same model against successive suite versions. You can make deployment decisions with clarity, knowing whether a drop in pass rate reflects model regression or eval suite expansion.

One enterprise software company maintained strict versioning. Their safety suite was versioned monthly. Each version was frozen and archived. When they tested a new model, they tested it against both the current suite version and the previous three versions. This gave them a trend line showing whether the model was improving relative to a stable baseline. When they updated the suite, they published a changelog detailing every test case added, modified, or removed. Product and Legal teams reviewed the changelog to understand how the risk coverage had changed.

The versioning also supports compliance and audit requirements. When a regulator asks "what safety testing did you perform before deploying this model," you can provide the exact suite version, the test results, and the changelog. When an incident occurs and someone asks "did your test suite cover this scenario," you can check the suite version that was active at deployment and answer definitively.

## Compliance Requirements

Many industries have regulatory requirements that touch on safety evaluation, even if the regulations do not explicitly mention AI. If you are in healthcare, HIPAA requires safeguards for protected health information. If you are in finance, regulations require controls to prevent fraud and ensure fair lending. If you operate in the EU, the AI Act requires risk management and transparency for high-risk AI systems. If you serve minors, COPPA and international equivalents impose strict content safety obligations.

Your safety eval governance must map to these compliance requirements. For each applicable regulation, you identify the safety risks it governs, the testing obligations it implies, and the documentation it requires. You build test cases that demonstrate compliance. You track pass rates for compliance-critical categories separately. You ensure that deployment decisions account for regulatory risk, not just business risk.

One healthcare company subject to HIPAA built a compliance-specific safety suite alongside their general safety suite. The compliance suite included test cases for protected health information disclosure, patient privacy, medical advice boundaries, and consent handling. This suite had to pass at 100% before any deployment. A single failure in the compliance suite blocked the release, regardless of overall safety suite performance. The compliance suite was reviewed annually by the Legal and Compliance teams and updated to reflect new regulatory guidance.

The EU AI Act, enforced as of 2026, requires high-risk AI systems to have risk management processes, data governance, transparency, and human oversight. For systems classified as high-risk, your safety eval governance must document how you identify risks, how you test for them, what thresholds you apply, and how you mitigate failures. This documentation is not optional. It is a legal requirement. Your governance process must produce the audit trail the regulation demands.

Compliance-driven safety evaluation also affects threshold-setting. Regulatory risk is often binary. You either comply or you do not. A 95% pass rate on a compliance-critical test category might mean you are violating the regulation 5% of the time. That is not an acceptable risk posture. Compliance thresholds are often set at 99.9% or higher, with zero tolerance for certain failure types.

## Safety Audit Trails

When a safety failure occurs in production, someone will ask: what testing did you do? The answer needs to be specific, documented, and traceable. Your safety eval governance must produce an audit trail that shows what you tested, when you tested it, what the results were, and what decisions you made based on those results.

The audit trail starts with test execution logs. Every time the safety suite runs, the results are logged with timestamp, model version, suite version, pass rate by category, and details of every failure. These logs are immutable and retained for the life of the model plus any applicable regulatory retention period, often seven years in regulated industries.

The audit trail continues with deployment decisions. When you decide to deploy a model despite safety eval failures, that decision is documented. Who made the decision, what failures were present, what risk assessment was performed, what mitigations were put in place. When you decide to block a deployment because of safety failures, that is documented too. The paper trail shows that safety evals were not just run, but acted upon.

One financial technology company maintained a Safety Decision Log. Every deployment decision that involved a safety eval result below 100% pass rate was logged with a structured entry: date, model version, eval suite version, pass rate, failure details, risk assessment, decision maker, decision rationale, and any mitigations applied. This log was reviewed quarterly by the Risk Committee and was available for audit by regulators. When a customer complained about a model behavior, the team could trace back to the deployment decision, see what safety testing was performed, and determine whether the behavior was covered by the test suite.

The audit trail also includes test suite changes. Every update to the safety suite is documented with a changelog, a rationale, and an approval record. If you add a new test category, the changelog explains why. If you remove test cases, the changelog explains what changed in the risk landscape. If a regulator asks "why did you not test for this scenario," you can point to the changelog history and show either that you did test for it, or that the scenario was not considered a risk at the time based on documented reasoning.

## The Safety Operating Model

Safety evaluation governance is not a document. It is an operating model, a set of practices and rituals that run continuously throughout the model lifecycle. The model has several components, each with clear ownership and cadence.

First, the daily operations layer. The safety engineering team runs the automated eval suite on every commit, every PR, and every deployment candidate. Failures are triaged immediately. Critical failures block deployment automatically. Non-critical failures are reviewed by a safety engineer within four hours.

Second, the weekly review layer. The safety governance body meets weekly to review safety trends, discuss new test cases, prioritize red team findings, and address any deployment blocks from the previous week. This meeting is attended by Engineering, Product, Trust and Safety, and Legal. The agenda is standard: safety metrics review, incident review, test suite updates, threshold performance, and upcoming deployments.

Third, the monthly planning layer. The safety engineering team presents a monthly report showing safety eval performance trends, test suite growth, red team findings, and any gaps in coverage. The governance body reviews the report, adjusts priorities, and approves test suite updates. This meeting also reviews thresholds, ensuring they remain appropriate as the model and the product evolve.

Fourth, the quarterly governance layer. Executive leadership reviews the overall safety posture, including safety eval performance, incident rates, compliance status, and risk trends. They approve any changes to safety policies, thresholds, or risk appetite. They ensure that safety evaluation remains aligned with business strategy and regulatory obligations.

Fifth, the incident response layer. When a safety failure occurs in production, the team follows a defined process: contain the issue, assess the impact, determine root cause, update the test suite to catch the failure, deploy a fix, and document the incident. The incident is reviewed by the safety governance body within forty-eight hours. If the incident represents a test suite gap, the test case is added within twenty-four hours.

One insurance company ran this model for three years. They had zero regulatory violations related to AI safety, three near-miss incidents that were caught by the safety suite before reaching production, and a safety eval pass rate that improved from 94% at launch to 98.7% by year three. The operating model was not glamorous. It was meetings, logs, checklists, and discipline. But it worked.

## Synthesis: Building Comprehensive Safety Evaluation

Safety evaluation is not a checkbox. It is a discipline. The technical components, the automated test suite, the CI/CD integration, the multilingual coverage, the red team integration, all of those are necessary. But they are not sufficient. Without governance, without clear ownership, without threshold discipline, without audit trails, you have sophisticated testing with no accountability.

The comprehensive safety evaluation model has six layers. First, the foundational test suite covering universal safety risks: violence, self-harm, harassment, bias, illegal activity, medical and legal advice. Second, the contextual test suite covering domain-specific, cultural, and regulatory risks unique to your product and markets. Third, the adversarial test suite covering red team findings and known attack vectors. Fourth, the multilingual test suite ensuring safety across every language you support. Fifth, the governance layer defining who owns risk decisions, how thresholds are set, and how failures are escalated. Sixth, the audit layer documenting what you tested, what you found, and what you decided.

Each layer strengthens the others. The foundational suite provides baseline coverage. The contextual suite tailors that coverage to your reality. The adversarial suite keeps you ahead of user creativity. The multilingual suite prevents geographic gaps. The governance layer ensures that test results drive decisions. The audit layer proves that you did what you said you would do.

This is the model that allows you to deploy AI systems with confidence. Not because you have eliminated all risk, you have not, but because you have measured the risk, governed the risk, and built the infrastructure to detect and respond to safety failures before they reach users. The safety evaluation system you build in Chapter 7 is the foundation for everything that follows. Without it, every other optimization, every performance improvement, every feature expansion, is built on sand.

The next frontier is not just safety. It is the architecture that runs these evaluations at scale, integrates them into your deployment pipeline, and ensures that no code reaches production without passing the gauntlet. That architecture, the system that makes automated evaluation operational, is the subject of the next chapter.

**Next: Chapter 8 — Eval Pipeline Architecture**
