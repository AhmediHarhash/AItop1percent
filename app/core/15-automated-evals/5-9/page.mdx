# 5.9 — Citation-to-Claim Alignment Scoring

A citation is only as good as the claim it supports. A model can retrieve ten relevant documents, insert ten inline citations, and still produce a factually incorrect output if the citations do not actually support the claims they are attached to. This is the single most common failure mode in retrieval-augmented generation systems. The model generates a claim, selects a document that seems topically related, inserts a citation marker, and moves on. The user sees the citation and assumes the claim is verified. The citation gives the appearance of grounding without the reality of grounding. Verifying citation-to-claim alignment is not optional. It is the difference between a RAG system that builds trust and a RAG system that destroys it.

## The Citation Trust Assumption

Users trust cited claims more than uncited claims. This is rational — a citation signals that the claim is verifiable, that it came from a source, that it is not invented. But this trust assumption creates risk. If your model inserts citations carelessly, users will trust claims that are not actually supported. They will make decisions based on false information that appeared credible because it had a citation. In legal research, this means quoting case law that does not say what the model claims. In medical Q&A, this means citing studies that do not support the treatment recommendation. In financial analysis, this means referencing reports that contain different numbers than the ones in the output.

The risk is highest when citations are presented in a way that discourages verification. If the citation is a document ID or a paragraph number, the user has to navigate to the source, find the paragraph, and read it to verify the claim. Most users do not do this. They see the citation, assume the claim is grounded, and move on. If the citation is wrong, the user never finds out until they act on the information and it fails. By then, the damage is done.

A financial research platform deployed a GPT-5-based analyst assistant in November 2025. The assistant answered questions about company earnings by retrieving relevant sections from earnings call transcripts and SEC filings. It inserted inline citations after every claim. Within two months, three institutional clients reported that the assistant had cited Q3 revenue numbers from a Q2 filing, cited guidance that was later revised, and cited a CEO statement from the wrong quarter. The citations were real — the documents existed, the paragraph numbers were correct. But the content of the cited paragraph did not support the claim attached to it. The model had selected citations based on keyword overlap, not based on semantic entailment. The platform rebuilt their citation scoring system to verify alignment. They now reject any output where a citation does not semantically support its claim. Citation precision went from 76% to 94%. Client complaints dropped to near zero.

## Citation Extraction and Claim Mapping

**Citation-to-claim alignment scoring** starts with extracting both the claims and the citations from the model output. A claim is a factual statement. A citation is a reference to a source — a document ID, a URL, a paragraph marker, a footnote number. The first step is mapping each citation to the claim it supports. In inline citation formats, the mapping is explicit: "Revenue increased 18% year-over-year, according to document 5, paragraph 12." The claim is "revenue increased 18% year-over-year." The citation is "document 5, paragraph 12." In footnote formats, the mapping is positional: the citation marker appears at the end of a sentence or paragraph, and you infer that it supports all claims in that span.

Once you have the mappings, you retrieve the cited content. If the citation is a document ID and paragraph number, you fetch that paragraph. If the citation is a URL, you fetch the page. If the citation is a reference to a chunk in your retrieval index, you fetch that chunk. You now have a claim and the text the model cited to support it. The next step is verifying that the cited text actually supports the claim.

Support verification is a semantic entailment task. You need to determine whether the cited text logically entails the claim, contradicts it, or is neutral. If the claim is "revenue increased 18%" and the cited text says "we saw an 18% increase in revenue compared to the prior year," the cited text entails the claim. If the cited text says "revenue increased 8%," the cited text contradicts the claim. If the cited text says "we are pleased with our performance this quarter" without mentioning a specific number, the cited text is neutral — it does not support or contradict the claim.

Entailment checking can be done with LLM-based judges. You prompt GPT-5 or Claude Opus 4.5 with the claim and the cited text, then ask: "Does the cited text support this claim? Answer: supports, contradicts, or neutral." The judge reads both, reasons about the relationship, and returns a label. For numerical claims, you can also use rule-based extraction — parse the number from the cited text, compare it to the number in the claim, check for exact match or acceptable rounding. For named entities, you can use entity linking — extract entities from both the claim and the cited text, verify that they refer to the same real-world object.

## Grounding Scores and Alignment Thresholds

A **grounding score** measures what percentage of claims in an output are supported by their citations. If the output contains 10 claims with 10 citations, and 9 of the citations support their claims while 1 does not, the grounding score is 90%. You set a threshold based on your risk tolerance. For high-stakes domains — legal, medical, financial — you might require 100% grounding. Every claim must be supported. A single unsupported claim fails the output. For lower-stakes domains — product recommendations, casual Q&A — you might accept 85% grounding, allowing a small number of claims to be weakly supported or inferred.

Grounding scores can also be weighted by claim severity. A claim about a company's revenue number is more important than a claim about the CEO's speaking style. If the revenue claim is unsupported, that is a critical failure. If the speaking style claim is unsupported, that is a minor issue. You assign weights: high-severity claims require citation support with 95%+ confidence, medium-severity claims require 85%+ confidence, low-severity claims require 70%+ confidence. An output fails if any high-severity claim is unsupported, even if the overall grounding score is high.

Some teams track citation precision and citation recall separately. **Citation precision** measures how many of the citations in the output actually support their claims. **Citation recall** measures how many of the claims in the output have supporting citations. High precision means the citations that are present are accurate. High recall means most claims are cited. You want both. A model with high precision but low recall produces a few well-supported claims and many unsupported claims. A model with high recall but low precision cites every claim, but many of the citations are wrong. The healthy state is 90%+ precision and 90%+ recall: most claims are cited, and most citations are accurate.

## Detecting Citations That Do Not Support Claims

The most common alignment failure is the **topically-related but non-supporting citation**. The model generates a claim about Q3 revenue. It retrieves a document about Q3 earnings. The document mentions revenue, but in a different context — it discusses revenue guidance for Q4, or revenue from a different business segment, or revenue trends over the past year without giving a specific Q3 number. The model cites the document because it is topically related. But the specific paragraph cited does not support the specific claim made.

This failure happens because retrieval systems optimize for topical relevance, not entailment. A dense embedding model retrieves documents that are semantically close to the query. Semantic closeness does not guarantee that the document supports the claim. The document might discuss the same topic from a different angle, in a different time period, with different conclusions. The model learns that retrieving related documents and inserting citations improves user trust. It does not learn to verify that the citation actually supports the claim.

A second common failure is the **citation to an outdated or revised source**. The model cites a quarterly report from Q2 to support a claim about Q3. The Q2 report contained guidance for Q3, and the model treats the guidance as fact. But the actual Q3 results differed from the guidance. The citation is real, the document is real, but the information is not current. This failure is particularly insidious because the citation appears credible — it is from an official source, the document exists, the paragraph number is correct. Only by reading the cited content and comparing it to the claim do you discover the misalignment.

A third failure mode is **partial support**. The cited text supports part of the claim but not all of it. The claim says "revenue increased 18%, driven by strong performance in the cloud segment." The cited text says "revenue increased 18%." It does not mention the cloud segment. The citation supports the revenue number but not the attribution to cloud. If the user trusts the full claim based on the citation, they are misled about the cause of the revenue increase. Partial support is detectable with claim decomposition — break the claim into sub-claims, verify each sub-claim separately, fail the output if any sub-claim is unsupported.

## RAG-Specific Citation Evals

Retrieval-augmented generation systems have unique citation challenges because the model does not write the citations manually — it generates them programmatically based on which chunks were retrieved. A RAG pipeline retrieves five chunks, passes them to the model in the prompt, and instructs the model to cite the chunks by number when making claims. The model outputs: "Revenue increased 18%, according to chunk 3." Your citation eval must verify that chunk 3 actually contains the 18% number.

RAG citation evals run after generation. You log the retrieved chunks, the model output, and the citation markers. For each citation, you extract the claim, retrieve the cited chunk, and verify alignment. If the alignment score is below threshold, you flag the output. You can also track retrieval quality separately from citation quality. **Retrieval recall** measures whether the chunks retrieved contain the information needed to answer the query. **Citation accuracy** measures whether the citations point to the correct chunks. A RAG system can have high retrieval recall but low citation accuracy if the model retrieves the right information but cites the wrong chunk. Both must be measured.

Some RAG systems use citation confidence scoring. Instead of a binary cite-or-not decision, the model outputs a confidence score for each citation: 0.95 means the model is very confident the chunk supports the claim, 0.60 means the model is uncertain. Your eval pipeline verifies high-confidence citations more strictly than low-confidence citations. If a citation marked 0.95 confidence fails alignment, that is a critical error — the model was confident and wrong. If a citation marked 0.60 confidence fails alignment, that is expected — the model flagged its own uncertainty. You can filter outputs by minimum citation confidence: only show outputs where all citations are above 0.85 confidence.

The financial research platform that rebuilt their citation system now runs four checks on every output: retrieval recall to verify the right documents were retrieved, citation coverage to verify all claims have citations, citation alignment to verify citations support claims, and citation confidence calibration to verify that high-confidence citations are more accurate than low-confidence citations. All four checks must pass. The result is a RAG system where users can trust that every cited claim is actually supported by the cited source. Trust went up. Usage went up. Client retention went up. The investment in citation eval paid for itself in three months.

## Citation Alignment as a Quality Gate

Citation-to-claim alignment is not a nice-to-have metric. It is a quality gate. Outputs with misaligned citations should not reach users. They create false confidence, enable bad decisions, and destroy trust in your system. Your eval pipeline must verify alignment before serving the output. If alignment fails, you have three options: regenerate with a different retrieval set, fall back to a less-specific answer, or return "insufficient information to answer this query." All three are better than serving a confidently-cited but factually-wrong answer.

Some teams treat citation alignment as a deployment blocker. If alignment drops below 90% in eval, they do not ship the new model version. If alignment drops below 85% in production, they roll back to the previous version. Citation quality is as critical as uptime. A system that is up but produces wrong citations is worse than a system that is down. At least when the system is down, users know not to trust it.

In 2026, citation alignment scoring is standard practice for any RAG system, any research assistant, any legal or medical or financial Q&A tool. The tooling exists — LLM-based entailment judges, retrieval logging, claim extractors, alignment metrics. The expertise exists — teams know how to build these pipelines. The business case is clear — misaligned citations cost more in lost trust than aligned citations cost to verify. If your system produces citations, verify them. If you do not verify them, do not produce them. Unverified citations are worse than no citations.

In the next subchapter, we examine tool call verification — how to evaluate whether agents and assistants invoke the right tools with the right parameters at the right time.

