# 11.1 — Eval Governance: Who Owns What

Most teams think that building a robust eval pipeline is a technical problem. They invest months creating automated test suites, defining thresholds, instrumenting decision points, and wiring everything into CI/CD. The pipeline works beautifully for three months. Then a product manager changes a threshold without telling anyone. A domain expert deprecates an eval that another team depends on. An eval fails in production, blocks a critical deploy, and nobody knows who has authority to override it. Six months later, the pipeline is full of orphaned evals that nobody owns, conflicting thresholds that nobody can explain, and a shared distrust of the entire system. The technical architecture survived. The organizational architecture collapsed.

Eval governance is the structure that determines who can create, modify, and deprecate evals, who is accountable when an eval blocks a deploy, and who decides when thresholds change. It is not a nice-to-have policy document. It is the difference between an eval system that teams trust and one they route around. Every eval in your pipeline needs an owner. Every threshold needs a decision-maker. Every override needs an escalation path. If you cannot answer "who owns this eval" for every eval in your system, you do not have governance — you have technical debt with a countdown timer.

## The Ownership Matrix

The first governance question is not "what are our standards" — it is "who owns which evals." Different types of evals require different owners because they serve different stakeholders and fail in different ways. **Platform evals** — the ones that measure system-level properties like latency, cost per query, or catastrophic failure rates — are owned by the platform engineering team. They set thresholds based on infrastructure capacity and business constraints. Nobody else should modify them without platform approval because these evals protect shared resources that every product depends on.

**Product evals** — the ones that measure task-specific quality like summary accuracy, extraction completeness, or classification precision — are owned by the product team building that feature. Product knows what quality threshold their users will tolerate. Product knows what trade-offs matter for their roadmap. Product owns the eval, sets the threshold, and is accountable when it blocks a deploy. Platform provides the infrastructure to run the eval, but product owns the decision logic.

**Domain evals** — the ones that measure specialized correctness like medical accuracy, legal compliance, or financial regulation adherence — are owned by domain experts, often outside engineering entirely. A healthcare AI product has evals owned by clinical staff. A legal AI product has evals owned by attorneys. A financial AI product has evals owned by compliance officers. These experts define what "correct" means in their domain. Engineering implements the eval, but the domain expert owns the rubric, the threshold, and the final call on what passes.

**Safety and policy evals** — the ones that measure harmful outputs, policy violations, or adversarial robustness — are owned by a centralized Trust and Safety or AI Safety team. These evals protect the entire organization from reputational and legal risk. No single product team should have authority to lower a safety threshold because the consequences affect everyone. Safety evals are the one category where centralized ownership is non-negotiable.

The ownership matrix is not theoretical. It is a table every engineer in your organization can reference. It lists every eval type, every eval name, the owner's name, and the escalation path when the eval blocks a deploy. If your eval pipeline has eighty evals across twelve product teams, that matrix has eighty rows. It lives in version control. It is updated every time a new eval is added. Teams know where to look when they need to understand who controls a decision that just blocked their release.

## Creation Authority

The second governance question is who can add new evals to the pipeline. The wrong answer is "anyone." The worse answer is "nobody." The right answer depends on what kind of eval it is and what its failure mode looks like. For platform evals, only platform engineering should have creation authority. Adding a new latency threshold or cost gate affects every downstream product. If product teams can unilaterally add platform-level gates, you end up with redundant checks, conflicting logic, and pipelines that take twelve minutes to run because five teams each added their own version of the same test.

For product evals, the product team should have creation authority with a review gate. Product knows what quality dimensions matter for their feature. They should be able to add evals that measure those dimensions. But before the eval enters the shared pipeline, platform engineering reviews it to ensure it will not destabilize the system — no infinite loops, no calls to deprecated models, no dependency on external APIs with no timeout. The review is not about whether the eval is a good idea. It is about whether the eval is safe to automate.

For domain evals, the domain expert has creation authority but engineering implements it. The clinical lead who says "we need an eval that checks whether the model contradicts established treatment guidelines" is not expected to write the eval code. They define the rubric, provide examples, and set the threshold. Engineering translates that into an automated check. The domain expert approves the implementation before it goes live. This pattern prevents two failure modes: domain experts blocked because they cannot code, and engineers guessing at domain correctness without expert input.

For safety evals, creation authority is centralized but with input from product teams. Trust and Safety defines the eval. Product teams provide adversarial examples from their domain. A centralized team ensures consistency — every product uses the same toxicity eval, the same PII detection logic, the same jailbreak resistance tests. Product teams contribute domain-specific attack vectors — the healthcare team knows what medical misinformation looks like, the legal team knows what unauthorized practice of law looks like — and those examples feed into the shared safety eval suite.

Creation authority without a clear process creates the spread-too-thin anti-pattern: eighty evals, fifteen of which measure nearly identical things, maintained by six different people who do not talk to each other. The fix is a lightweight creation workflow. Product team submits a proposal: eval name, what it measures, why it matters, proposed threshold. Platform engineering reviews for safety and duplication. If approved, engineering implements it. The eval goes into the ownership matrix. The entire process takes a day, not a week. But it prevents the chaos of uncoordinated eval proliferation.

## Modification Authority

The third governance question is who can change an existing eval's logic, threshold, or rubric. This is where most governance systems fail. A product team launches a feature. They set a quality threshold at 92 percent because that is what their pilot users tolerate. Three months later, user expectations shift. The threshold needs to drop to 88 percent to ship a new capability on time. The product manager changes the threshold in the config file, commits it, and deploys. Nobody else notices. Six months later, another team depends on that eval and assumes it still enforces the 92 percent bar. Their product degrades because they trusted a threshold that quietly changed.

Modification authority must match ownership, but modifications must be visible. The product team that owns an eval can change its threshold — but the change must trigger a notification to every team that depends on that eval. If three teams use the same summarization quality eval, a threshold change affects all three. The owning team has authority to make the change, but the dependent teams have a right to know it happened. Visibility is not optional. It is the mechanism that prevents silent degradation.

For platform evals, modification authority is even stricter. Changing a latency threshold from 800 milliseconds to 1200 milliseconds affects every product in production. The platform team owns the threshold, but changes require a review from product leadership. The review is not bureaucratic theater. It is a forcing function: if you are changing a system-level threshold, you must explain why the old threshold no longer reflects reality and what the impact will be on user experience. If you cannot explain that in two paragraphs, the threshold should not change.

For domain evals, modification authority stays with the domain expert, but implementation changes require engineering review. A clinical lead who says "the threshold for medication contraindication detection needs to increase from 95 percent to 98 percent" has the authority to make that call. But if implementing the new threshold requires rewriting the eval logic, engineering reviews the change to ensure it does not introduce bugs or performance regressions. The domain expert approves the behavior. Engineering approves the implementation. Both signatures required.

For safety evals, modification authority is centralized and requires executive approval for threshold decreases. Raising a safety threshold — making the system more conservative — can be done by the Trust and Safety team. Lowering a safety threshold — accepting more risk — requires sign-off from a VP or Chief Product Officer. This asymmetry is intentional. Making the system safer is encouraged. Making it less safe is a business decision, not a tactical one.

## Deprecation Authority

The fourth governance question is who decides when an eval is no longer needed. Evals accumulate. A team builds an eval to measure a quality dimension for a feature that launches, succeeds for a year, then gets deprecated. The eval remains in the pipeline. Nobody removes it because nobody is sure whether another team depends on it. A year later, the pipeline has twenty evals that test features no longer in production. Every deploy runs them. Every new engineer asks what they are for. Nobody knows.

**The Orphan Eval** is the most common eval governance failure. An eval is created with an owner, but the owner leaves the company or changes teams. The eval remains in the pipeline. It fails occasionally. Someone fixes it to unblock a deploy. Six months later, it fails again. Someone else fixes it. Nobody knows what it is supposed to measure anymore. Nobody knows if the threshold is still correct. But nobody has authority to delete it because nobody is sure it is not protecting something important.

Deprecation authority must be explicit. The team that owns an eval has authority to deprecate it, but deprecation requires a public announcement and a waiting period. The announcement goes to every team that has ever depended on the eval — tracked automatically by your pipeline instrumentation. The waiting period is two weeks. If no team objects, the eval is removed from the active pipeline and archived. If a team objects, they must either adopt ownership of the eval or explain why it should remain owned by the original team. No objection and no adoption means the eval was not important enough to keep.

For platform evals, deprecation requires platform leadership approval. Removing a latency gate or a cost threshold affects every product. If platform wants to deprecate an eval, they must demonstrate that the property is now measured by a different, better eval or that the property no longer matters. Product teams have a week to object. If nobody objects, the eval is deprecated. If anyone objects, the discussion moves to architecture review.

For safety evals, deprecation requires the same executive approval as threshold changes. You do not casually remove a toxicity filter or a PII detection check. If Trust and Safety believes an eval is no longer necessary — perhaps because it is redundant with a newer, more comprehensive eval — they propose deprecation with evidence. Leadership reviews. If approved, the eval is archived, not deleted. Archived evals can be restored if needed, but they do not run in the active pipeline.

The deprecation process prevents two failure modes. It prevents orphan evals that persist forever because nobody has authority to remove them. It also prevents cavalier deletion that removes protections teams did not realize they depended on. The waiting period and the notification system ensure that deprecation is safe, visible, and reversible.

## Escalation Paths

The fifth governance question is what happens when an eval blocks a deploy and the team believes the eval is wrong. This is the most politically charged moment in eval governance. A product team is ready to ship. The eval pipeline runs. One eval fails. The team investigates. They believe the eval is misconfigured — the threshold is too strict, or the rubric does not account for a legitimate edge case, or the test data is stale. They need to ship in six hours to meet a customer deadline. Who has authority to override the eval?

The escalation path must be defined before the conflict happens. For product evals, the product team that owns the eval has final authority. If the team that built the eval and the team that is blocked are the same team, they can override their own eval — but the override must be logged, and the log must include a written justification and a plan to fix the eval. Overrides are not punished, but they are visible. Leadership sees them. Patterns of repeated overrides for the same eval indicate that the eval's threshold is wrong or that the team is shipping low-quality work. Either way, the visibility forces the conversation.

For platform evals, the escalation goes to platform engineering. If a deploy is blocked by a latency threshold and the product team believes the threshold is wrong, they escalate to the platform team that owns the threshold. Platform reviews the evidence. If the product team is right — the threshold is misconfigured or the test environment is unrepresentative — platform overrides the eval and commits to fixing it within 48 hours. If the product team is wrong — the deploy genuinely violates the latency budget — the deploy is blocked until the product team fixes the performance issue. Platform has final authority because platform is accountable for system stability.

For domain evals, escalation goes to the domain expert. If a healthcare AI deploy is blocked by a clinical accuracy eval and the product team believes the eval is wrong, they escalate to the clinical lead who owns the eval. The clinical lead reviews the failing examples. If the eval is wrong, they approve an override and update the rubric. If the eval is correct, the deploy is blocked until the model output is fixed. Domain experts have final authority on domain correctness. Engineering does not overrule a clinical lead on a medical question. Product does not overrule a compliance officer on a regulatory question.

For safety evals, escalation goes to Trust and Safety, and overrides are rare. If a deploy is blocked by a toxicity filter and the product team believes the filter is miscalibrated, they escalate to Trust and Safety. Trust and Safety reviews. If the filter is wrong — flagging benign content as toxic — they approve an override and commit to recalibrating the filter. If the filter is correct — the model is producing genuinely problematic outputs — the deploy is blocked, full stop. Trust and Safety has final authority, and their authority cannot be overridden by product pressure or deadlines. The only exception is executive override, and that requires written sign-off from a VP acknowledging the risk.

Escalation paths prevent two failure modes. They prevent teams from being permanently blocked by misconfigured evals with no recourse. They also prevent teams from unilaterally bypassing evals because they are inconvenient. The key is clarity: every team knows who to escalate to, and every decision-maker knows they are accountable for the override.

## Cross-Functional Governance

The sixth governance question is how to coordinate eval decisions when they cut across organizational boundaries. A new regulation takes effect. Legal says every customer-facing AI output must include a specific disclosure. Compliance says the disclosure must appear within the first 200 characters. Product says that requirement breaks the user experience. Engineering says the eval pipeline can enforce it, but only if product changes the prompt template. Trust and Safety says the disclosure itself might trigger toxicity filters if phrased incorrectly. Five teams, five priorities, one eval.

Cross-functional governance requires a standing committee, not ad-hoc meetings. The **Eval Governance Board** meets every two weeks. Membership is fixed: one representative from platform engineering, one from product, one from Trust and Safety, one from legal, one from compliance. The board reviews proposed changes to shared evals — the ones that affect multiple products or touch regulated domains. The board does not make technical decisions. It resolves conflicts about priorities and authority.

A typical case: product wants to lower a quality threshold to ship faster. Trust and Safety objects because lower quality increases the risk of harmful outputs. The board reviews the data. Product presents evidence that the current threshold is blocking valuable use cases. Trust and Safety presents evidence that the proposed threshold increases policy violations by 4 percent. The board makes a decision: approve the lower threshold but require product to add a secondary safety eval that mitigates the increased risk. Both teams commit. The decision is logged. The new eval goes into the pipeline.

The board does not meet for routine decisions. It only convenes when there is a conflict that cannot be resolved by the teams directly. Most governance happens at the team level. The board is the circuit breaker: when two teams with legitimate but conflicting priorities cannot agree, the board makes the call. The board's authority comes from executive sponsorship. If the board makes a decision, it is final unless overruled by a VP. That rarely happens because the board is designed to represent all stakeholder interests in one room.

Cross-functional governance also requires shared documentation. Every eval in the pipeline has a one-page spec. The spec lists the owner, the threshold, the rationale, the stakeholders who depend on it, and the date it was last reviewed. The spec is not a technical document. It is a governance document. It answers the questions that arise during conflicts: why does this eval exist, who decided on this threshold, and what happens if we change it. Teams update the spec when they modify the eval. The spec lives in the same repository as the eval code. If the spec and the code diverge, the code is wrong.

## The Eval Owner Role

The seventh governance question is what it means to own an eval. Ownership is not passive. It is not "the person whose name is in the config file." Ownership is an ongoing responsibility. The eval owner is accountable for the eval's accuracy, its maintenance, and its relevance. If the eval starts failing in production because the test data is stale, the owner is responsible for refreshing it. If the eval's threshold no longer reflects user expectations, the owner is responsible for updating it. If the eval is deprecated because it is no longer needed, the owner is responsible for removing it from the pipeline.

Eval ownership is a part-time role, not a full-time job. A product manager might own five evals. An engineering lead might own three. A domain expert might own two. Ownership does not mean writing code. It means being the decision-maker and the point of accountability. The owner decides what the eval measures, what threshold it enforces, and when it needs to change. The owner does not necessarily implement those changes — engineering often does — but the owner approves the implementation and signs off on the result.

Ownership is also time-bounded. Every eval has a review date. For product evals, the review happens every quarter. The owner answers three questions: is this eval still measuring the right thing, is the threshold still correct, and is anyone still depending on this eval. If yes, yes, yes — the review is done in five minutes. If any answer is no, the owner updates the eval or deprecates it. For safety evals, the review happens every month because the threat landscape changes faster. For platform evals, the review happens every six months because infrastructure constraints change slowly.

Ownership without accountability is theater. Accountability requires metrics. How many evals does this owner maintain? How many times did their evals fail in production due to staleness or misconfiguration? How many times did they update a threshold in response to changing requirements? How many evals did they deprecate when they were no longer needed? These metrics are not performance reviews. They are health checks. If an owner has five evals and none of them have been reviewed in eight months, that is a governance failure.

The eval owner role also prevents the orphan eval anti-pattern. Every eval has an owner. If the owner leaves the company or changes roles, the eval must be reassigned or deprecated within 30 days. No eval can persist without an owner. This rule is enforced by automation: the pipeline tracks ownership and sends alerts when an eval's owner is no longer active in the organization. The alert goes to the owner's manager. The manager assigns a new owner or deprecates the eval. No exceptions.

## Governance Documentation

The eighth governance question is what needs to be written down and what can remain verbal. The wrong answer is "nothing needs to be written down — we all know who owns what." That works for three months, until someone leaves the company or a new team joins and nobody knows the history. The worse answer is "everything needs to be documented in exhaustive detail — every decision, every threshold change, every escalation path must have a 20-page process doc." That creates governance theater: beautiful documents nobody reads and processes nobody follows.

The right answer is selective documentation: the decisions that matter six months from now need to be written. The decisions that only matter today can stay verbal. Ownership assignments must be documented. The matrix that lists every eval and its owner lives in version control and is updated every time ownership changes. Escalation paths must be documented. Every team knows where to look when they need to understand who has authority to override an eval. Threshold rationales must be documented. When someone changes a threshold, they write one paragraph explaining why. The paragraph lives in the eval spec.

Governance documentation is not a compliance exercise. It is a decision log. Six months from now, a new engineer will ask "why is this threshold set to 88 percent instead of 92 percent?" The documentation answers that question in 30 seconds. If the documentation does not exist, the engineer spends an hour asking around, gets three conflicting answers, and makes a guess. The documentation prevents the guess.

Governance documentation is also version-controlled and reviewed. Changes to ownership, thresholds, or escalation paths trigger a pull request review. The review is not about approval — the owner already has authority to make the change. The review is about visibility. Other teams see the change before it takes effect. They can ask questions. They can flag dependencies. The review is not bureaucracy. It is the mechanism that prevents silent governance decay.

Next, you need to know what happened — every automated decision your eval pipeline makes must leave a trail that can be audited, questioned, and defended when regulators, customers, or internal stakeholders demand to know why a deploy was blocked, a model was rejected, or a threshold was changed.

