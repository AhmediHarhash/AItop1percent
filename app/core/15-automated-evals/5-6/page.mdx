# 5.6 — Embedding-Based Similarity Metrics

Embeddings convert text into high-dimensional vectors that encode semantic meaning. Two sentences that express the same idea produce vectors that point in similar directions, even if they share no words in common. Measuring similarity becomes a geometry problem — calculate the angle between vectors, and you have a proxy for semantic equivalence.

This works because embedding models are trained to map semantically related text to nearby points in vector space. "The package will arrive tomorrow" and "Your delivery is scheduled for the next day" have different words but similar meanings. Their embeddings will have high cosine similarity — often above 0.90 — because the models learned that these patterns convey the same information. Embedding similarity automates what would otherwise require human judgment for every pair of outputs and references.

But embeddings are not magic. They reflect the biases, domains, and training objectives of the models that produced them. An embedding model trained on Wikipedia and web text may perform poorly on medical terminology or legal jargon. An embedding model optimized for sentence-level similarity may fail on word-level or document-level tasks. An embedding that captures semantic relatedness may give high similarity scores to statements that are related but contradictory — "The drug is safe" and "The drug is unsafe" both relate to drug safety and might score higher similarity than either does with "The weather is clear." Embeddings compress meaning, and compression loses information. The question is whether the information lost matters for your task.

## How Embedding Similarity Works

You start with two pieces of text — a model output and a reference answer. You pass both through an embedding model. The embedding model returns two vectors, each with hundreds or thousands of dimensions depending on the model architecture. You compute the cosine similarity between these vectors. Cosine similarity measures the angle between vectors, ranging from negative one to one. For text similarity tasks, scores below zero are rare because unrelated text is orthogonal, not opposite. Most comparisons produce scores between 0.3 and 1.0, where higher values indicate greater semantic similarity.

A score of 0.95 or above typically indicates near-equivalence — the texts express the same idea with minor phrasing differences. A score of 0.85 to 0.95 indicates strong similarity — the core meaning aligns but details or emphasis may differ. A score of 0.70 to 0.85 indicates moderate similarity — the texts are related and partially overlapping but not equivalent. A score below 0.70 indicates weak similarity — the texts may share a topic but diverge significantly in meaning. These thresholds are task-dependent and embedding-model-dependent, but they provide a starting framework.

The embedding model is the critical choice. Different models produce different vector spaces with different semantic properties. A model trained on conversational data will cluster casual and formal versions of the same statement. A model trained on technical documentation will distinguish fine-grained technical terms more precisely. A multilingual embedding model will map semantically equivalent statements across languages to similar vectors, enabling cross-lingual eval. The embedding model defines what "similar" means in your pipeline.

## Choosing Embedding Models

In 2026, production-grade embedding models include OpenAI's text-embedding-3-large and text-embedding-3-small, Cohere's embed-v4, Voyage AI's voyage-2, Google's Gecko embeddings in Gemini, and open-source models like the BGE series and E5-large. The choice depends on domain, latency requirements, cost, and whether you need multilingual support.

General-purpose embeddings work for most conversational and informational tasks. If your model generates customer support responses, product recommendations, or explanatory text, a general embedding model trained on diverse web data will provide reasonable similarity scores. OpenAI's text-embedding-3-large and Cohere's embed-v4 are strong defaults. They handle a wide range of topics, support long contexts, and have been tuned for semantic search and similarity tasks that align with eval use cases.

Domain-specific embeddings improve fidelity for specialized tasks. If your model generates medical diagnoses, legal contract clauses, or financial analysis, consider fine-tuning an embedding model on domain text or using an embedding model pre-trained on that domain. Medical embeddings trained on PubMed and clinical notes will distinguish between "hypertension" and "hypotension" more reliably than general embeddings. Legal embeddings will capture the semantic difference between "shall" and "may" in contract language. The investment in domain-specific embeddings pays off when general models produce too many false positives or false negatives due to domain nuance.

Multilingual embeddings are necessary if you evaluate outputs across languages. A multilingual model like Cohere's embed-multilingual or the mBERT-based embeddings maps equivalent statements in English, Spanish, French, and dozens of other languages to nearby vectors. This enables a single reference set to evaluate outputs in multiple languages without maintaining separate references per language. However, multilingual models sometimes sacrifice within-language precision to achieve cross-language alignment. If you only evaluate one language, a monolingual model will likely provide sharper similarity scores.

Model size and latency matter at scale. Larger embedding models produce richer representations but require more compute and increase eval pipeline latency. If you run evals on every commit or every pull request, embedding latency becomes a bottleneck. OpenAI's text-embedding-3-small and Cohere's embed-light offer faster inference with slightly lower fidelity. For batch evals that run overnight, latency is less critical and the largest available model is often the best choice.

## Cosine Similarity Thresholds

Embedding similarity produces a continuous score. You must decide where to draw the line between pass and fail. The threshold is not universal — it depends on the task, the embedding model, and the acceptable false positive and false negative rates.

For high-stakes tasks, set a high threshold. A medical advice system or a legal compliance checker cannot tolerate near-misses. A threshold of 0.93 or higher ensures that only outputs very close to the reference pass. This reduces false positives — outputs that score well but contain subtle errors — at the cost of increasing false negatives — outputs that are correct but phrased differently enough to fall below the threshold. For high-stakes tasks, false negatives are acceptable because you can review them manually. False positives are not, because they reach users and cause harm.

For conversational tasks, a moderate threshold balances precision and coverage. A customer support bot or a content recommendation system benefits from stylistic variation. A threshold of 0.85 to 0.90 allows valid paraphrases while still catching outputs that drift too far from the reference intent. This threshold works well for tasks where multiple phrasings are acceptable and the cost of a false negative — rejecting a valid output — exceeds the cost of a false positive — accepting an output that is close but not perfect.

For creative or generative tasks, use a low threshold or skip similarity scoring entirely. A storytelling model, a creative writing assistant, or a brainstorming tool should produce outputs that diverge from references. High similarity indicates repetition, not quality. For these tasks, embedding similarity may serve as a floor — ensuring outputs are at least on-topic — but not as a primary quality metric. Other dimensions like coherence, novelty, and user engagement matter more.

Thresholds also depend on the embedding model. Different models produce different score distributions. One model might rarely produce scores above 0.95 even for near-identical text. Another might frequently produce scores above 0.90 for loosely related text. Calibrate your thresholds by running your embedding model on a labeled set of output-reference pairs where you have human judgments of equivalence. Find the threshold that maximizes agreement between embedding similarity and human judgment. This threshold becomes your deployment gate.

## Embedding Limitations

Embeddings capture semantic relatedness, not factual correctness. Two statements can be semantically similar but factually opposite. "The treatment is effective" and "The treatment is ineffective" both discuss treatment efficacy and will produce higher similarity scores than either does with "The weather is sunny." An embedding-based eval might score a factually incorrect output highly because it is on-topic and structurally similar to the reference. This is a critical failure mode for tasks where factual accuracy is non-negotiable.

Embeddings also struggle with negation and fine-grained distinctions. "The patient should take the medication" and "The patient should not take the medication" differ by one word but have opposite meanings. Depending on the embedding model and sentence structure, they might still produce a similarity score above 0.80. Negation is one of the hardest linguistic phenomena for embeddings to encode reliably. For tasks where negation matters — medical instructions, safety warnings, legal obligations — embeddings alone are insufficient. You need additional checks that verify critical keywords or use LLM-based judges to catch negation errors.

Embeddings compress long text in ways that lose detail. A 500-word essay and a 500-word essay with one paragraph swapped might still produce a similarity score above 0.90 because most of the content is identical. But that one swapped paragraph might contain the essay's thesis, making the outputs semantically divergent despite high overall similarity. For long-form tasks, consider chunking outputs and references, computing similarity per chunk, and aggregating scores in a way that surfaces localized divergences.

Embeddings are also sensitive to phrasing artifacts. Adding filler words, changing sentence order, or using more formal or casual language can shift similarity scores even when the core meaning is unchanged. "The system will process your request in 24 hours" and "Your request will be processed within one day by the system" convey the same information but might score 0.88 instead of 0.95 due to structural differences. If your task requires robustness to stylistic variation, test your embedding model on paraphrased examples and verify that it treats valid paraphrases as highly similar.

## When Embeddings Fail

Embeddings fail when the task requires understanding that the embedding model was not trained to capture. Sarcasm, irony, humor, cultural references, and idiomatic expressions often produce misleading similarity scores. "That went well" said after a disaster and "That went well" said after a success are lexically identical and will produce a similarity score of 1.0, but they have opposite meanings. Embeddings trained on written text may not capture the pragmatic meaning that depends on context.

Embeddings also fail when the reference and output use different abstraction levels. A reference says "The company's revenue grew 12% year-over-year." The model says "The company performed well financially." These statements are related and directionally consistent, but one is specific and one is vague. An embedding model might score them at 0.80, suggesting strong similarity. But for a financial analysis task, vague outputs are failures even if they are semantically related to the reference. Similarity is not a substitute for precision.

Embeddings fail when domain-specific terminology shifts meaning. In legal text, "reasonable" has a specific technical meaning that differs from its conversational use. In medical text, "chronic" and "acute" are not just different — they are clinically opposite. General-purpose embeddings may treat these as moderately similar because they both relate to medical conditions. Domain-specific embeddings reduce but do not eliminate this risk. For high-stakes domains, combine embedding similarity with keyword checks or LLM-based verification that understands domain semantics.

Embeddings also fail when outputs are correct but novel. A model generates a valid answer that does not appear in your reference set. The embedding similarity to the closest reference is 0.75 — too low to pass. But the output is factually accurate and useful. This is a false negative caused by reference set incompleteness, not by model failure. Embedding similarity assumes your references cover the space of valid outputs. When that assumption breaks, you need additional eval layers — LLM judges, human review, or production feedback — to catch correct outputs that fall outside the reference distribution.

## Calibrating Embedding Thresholds

Threshold calibration is empirical. Start with a labeled dataset of output-reference pairs with human judgments — equivalent, similar, related, unrelated. Run your embedding model on these pairs and compute similarity scores. Plot the score distribution for each human judgment category. Find the threshold that separates "equivalent" from "similar" with the lowest error rate. This becomes your pass threshold.

Calibration also reveals when embedding similarity is the wrong metric. If your score distributions overlap heavily — the "equivalent" and "unrelated" categories both produce scores between 0.70 and 0.90 — embeddings are not providing enough signal. The task may require exact match, LLM-based judging, or a hybrid approach that combines embeddings with additional checks.

Re-calibrate when you change embedding models. Different models have different score distributions. A threshold of 0.88 calibrated for one model may be too strict or too lenient for another. When you switch models, re-run calibration on your labeled set and update your thresholds before deploying the new model in your eval pipeline.

Re-calibrate when your task evolves. If you expand your product to handle new question types, new output formats, or new domains, your embedding similarity distribution may shift. Run a calibration checkpoint on a sample of new data and verify that your thresholds still separate correct from incorrect outputs as intended. Embedding thresholds are not set-and-forget — they are task-specific parameters that require ongoing validation.

## The Embedding Model as Infrastructure

Embedding similarity is only as reliable as the embedding model you use. Treat the embedding model as critical infrastructure. Pin its version, monitor its performance, and evaluate replacements as new models emerge. Embedding models improve every quarter. A model that was state-of-the-art in early 2025 may be outperformed by a smaller, faster, cheaper model released in late 2025. Evaluate new models on your calibration set and switch when the new model provides better task alignment or cost efficiency.

Some teams fine-tune embedding models on their domain data. This requires labeled pairs of semantically similar and dissimilar text from your domain, but the result is an embedding space tuned to your specific notions of similarity. Fine-tuned embeddings reduce false positives and false negatives, particularly for domains where general-purpose embeddings struggle. The investment is justified when embedding similarity is a deployment gate and threshold calibration alone does not achieve acceptable precision.

Embedding-based similarity is a powerful tool, but it is not a universal solution. It works best for tasks where semantic relatedness aligns with correctness, where references cover the space of valid outputs, and where factual precision is verified by other means. Used correctly, embeddings automate the majority of eval cases and free human reviewers to focus on edge cases and novel outputs. Used incorrectly, they create false confidence by scoring incorrect outputs highly because they happen to be topically related to the reference. The teams that understand both the power and the limits of embeddings are the teams whose eval pipelines remain trustworthy at scale.

---

*Similarity metrics provide a foundation, but they are not the only reference-based approach. Classical metrics offer precision where embeddings are too coarse.*
