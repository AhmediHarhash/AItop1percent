# 10.5 — Alerting Thresholds and Escalation Rules

In July 2025, a healthcare documentation platform deployed a new Claude Opus 4.5 configuration to improve clinical note summarization. The eval pipeline ran hourly. Within six hours, the system detected a four-percentage-point drop in factual accuracy on a subset of cardiology notes. The metric fell from 96% to 92%. The alert fired. It went to a Slack channel that the on-call engineer had muted three weeks earlier because it was firing too often on minor fluctuations. No one saw it. By the next morning, the model had processed 8,400 patient notes, 336 of which contained factual errors that contradicted source documents. The errors were caught during manual review two days later. The root cause was a prompt change that inadvertently deprioritized verbatim medical terminology in favor of plain-language summaries. The fix took four minutes. But the detection-to-response gap was 41 hours, because the alerting system was configured incorrectly and the team had lost trust in it.

**Alerting thresholds** are the decision boundaries that determine when a metric deviation becomes a page, a ticket, or an email. Set them too tight and you drown your team in false positives. Set them too loose and you miss real incidents until they are catastrophic. The goal is not to alert on everything. The goal is to alert on the things that require human intervention, with enough context and urgency calibration that the person receiving the alert knows what to do and how fast to do it.

## Threshold Types and When to Use Each

The simplest threshold is an **absolute boundary** — a fixed value that, when crossed, triggers an alert. Pass rate drops below 90%. Latency exceeds 800ms. Cost per query goes above ten cents. Absolute thresholds work well for metrics where you have a clear, non-negotiable floor or ceiling. If your SLA guarantees 95% correctness, an alert at 94% is justified. If your budget caps cost at eight cents per query, an alert at nine cents gives you time to react before you blow the budget.

But absolute thresholds miss context. A pass rate drop from 98% to 94% is more alarming than a drop from 91% to 90%, even though the second case crosses the 90% threshold and the first does not. This is where **relative thresholds** help. A relative threshold triggers when a metric deviates by a percentage from its baseline. If the pass rate drops more than three percent below the seven-day rolling average, alert. If latency increases more than fifteen percent above the thirty-day median, alert. Relative thresholds adapt to your system's normal operating range. They catch deviations that matter in context, not just in absolute terms.

**Rate-of-change thresholds** catch trends before they breach absolute limits. If the pass rate is declining at more than 0.5 percentage points per day for three consecutive days, that is a signal even if the current value is still acceptable. Rate-of-change alerting is early warning. It gives you time to investigate before the problem becomes severe. The trade-off is complexity — calculating slopes requires enough historical data to distinguish real trends from noise, and tuning the sensitivity is harder than setting a simple boundary.

**Composite thresholds** combine multiple conditions. Alert if pass rate drops below 92% AND latency exceeds 600ms AND the error involves Tier 1 tasks. This reduces false positives by requiring multiple signals to align before escalating. A single metric fluctuation might be noise. Multiple metrics degrading simultaneously is almost always real. Composite thresholds are especially useful for distinguishing between normal variance and systemic failure.

## Severity Levels and What They Mean

Not all alerts are created equal. A **severity framework** categorizes alerts by impact and urgency, so the response is proportional to the problem. Most teams use three to five severity levels. The exact labels vary, but the logic is consistent — some alerts are informational, some require action within hours, some require immediate response.

**Critical severity** means the system is failing in a way that affects users right now or will affect them imminently. Pass rate on Tier 1 tasks drops below the SLA threshold. Latency exceeds the user-facing timeout, causing visible failures. A security eval detects prompt injection attempts succeeding at scale. Critical alerts page the on-call engineer. They interrupt focus. They demand immediate triage. The bar for critical is high — if you page someone at 2am, the issue better be worth waking them up.

**High severity** means the system is degraded but not yet failing visibly. Pass rate on Tier 2 tasks drops five percentage points. Cost per query increases twenty percent above budget. A new model deployment shows regression on a core eval suite. High-severity alerts do not page, but they do require same-day response. They go to a channel or ticket queue that the team checks multiple times per day. The expectation is investigation and resolution within business hours.

**Medium severity** means something is trending wrong but not yet impacting users or budgets. A gradual decline in pass rate over two weeks. A retrieval precision drop on a low-traffic task type. An eval suite that has not run successfully in 48 hours due to infrastructure flakiness. Medium-severity alerts are tracked but not urgent. They go into a backlog for investigation within a few days. They are often leading indicators — if ignored, they escalate to high or critical.

**Low severity** or **informational** alerts are signals that something changed, but no action is required unless the change persists. A single eval run failed due to a transient API timeout. A new task type appeared in production traffic but has too few samples to evaluate. An experimental model was deployed to a shadow traffic path and showed expected behavior. Informational alerts go to logs or low-priority channels. They exist for audit trail and future correlation, not for immediate response.

## Escalation Paths and Human Handoff

An alert that fires but reaches no one is worse than no alert at all. **Escalation paths** define who gets notified, in what order, and what happens if no one responds. The simplest path is a single point of contact — the on-call engineer gets every critical alert via PagerDuty or equivalent. If they do not acknowledge within five minutes, the alert escalates to the secondary on-call. If no acknowledgment within ten minutes, it escalates to the engineering manager.

For lower-severity alerts, the path is different. High-severity alerts go to a dedicated Slack channel monitored by the eval team and the model ops team. If no one responds within an hour, a ticket is auto-created and assigned to the team lead. Medium-severity alerts go to a ticket queue reviewed daily. Low-severity alerts go to a logging dashboard with no active notification.

The escalation path must account for **context**. An alert that fires during business hours in your primary time zone can route to a Slack channel. An alert that fires at 3am should only page if it is truly critical. An alert that fires during a known deployment window might be expected and should route to the deployment manager, not the general on-call. Context-aware routing reduces noise and ensures alerts reach the person best positioned to act.

**Handoff documentation** is part of the escalation path. When an alert fires, the notification should include not just the metric and threshold, but also a link to the relevant dashboard, a link to the runbook for that alert type, and a summary of recent system changes that might be relevant. The engineer receiving the alert should not have to hunt for context. The alert itself should provide enough information to start investigation within sixty seconds of acknowledgment.

## Alert Fatigue and How to Prevent It

**Alert fatigue** is the organizational disease where alerts fire so often, with so many false positives, that the team stops trusting them. Engineers mute channels. On-call rotations become dreaded. Real incidents get missed because they are buried in noise. Alert fatigue is not a people problem. It is a threshold-tuning problem and a severity-calibration problem.

The first symptom is **high false positive rate**. If more than twenty percent of your alerts turn out to be non-issues, your thresholds are too sensitive. False positives erode trust faster than anything else. The engineer who investigates five alerts and finds four of them are just normal variance will start ignoring alerts entirely. The solution is to tighten thresholds, add composite conditions, or move low-confidence alerts to informational severity instead of paging.

The second symptom is **alert floods** — many alerts firing at once for related reasons. If a database outage causes your eval pipeline to fail, you might get alerts for eval run failures, metric staleness, dashboard unavailability, and cost anomaly, all within two minutes. Each alert is technically accurate, but the flood obscures the root cause. **Alert deduplication** helps. If multiple alerts share the same root symptom, group them into a single incident with references to all triggered conditions. The on-call engineer sees one incident, not twelve.

The third symptom is **alert habituation** — alerts that fire so regularly that they become background noise. If your cost-per-query alert fires every Tuesday because Tuesday traffic is always higher, the alert is not useful. It is training your team to ignore cost alerts. The fix is to adjust the threshold to account for known patterns, use seasonal baselines, or suppress alerts during expected variance windows.

The fourth symptom is **lack of actionability**. If an alert fires but no one knows what to do about it, the alert is noise. Every alert should have a corresponding runbook or at least a documented first step. If the alert is "eval pass rate dropped," the runbook should say: check the dashboard for per-task breakdown, check recent deployments for prompt or model changes, check the eval result details for new failure modes, check the data pipeline for input distribution shifts. Actionable alerts get resolved. Vague alerts get ignored.

## The Alerts That Actually Matter

Not every metric needs an alert. The mistake most teams make is alerting on everything that can be measured. The result is a hundred alerts, ninety of which never fire, and ten that fire constantly. The effective alerting strategy has **fifteen to thirty distinct alert conditions**, each carefully chosen to represent a failure mode or degradation pattern that actually requires human intervention.

**Quality-based alerts** are the highest priority. Pass rate below SLA threshold for Tier 1 tasks. Factual accuracy drop on safety-critical domains. Hallucination rate increase on customer-facing outputs. Catastrophic forgetting detection after a fine-tuning deployment. These alerts map directly to user-facing risk. If they fire, something is wrong that affects production quality.

**Performance-based alerts** come next. Median latency exceeds user-facing timeout. p99 latency increases beyond acceptable UX bounds. Throughput drops below minimum traffic-handling capacity. Time-to-first-token degrades past interactive thresholds for streaming use cases. These alerts protect user experience. A model that is correct but too slow is still failing.

**Cost-based alerts** protect budget. Daily spend exceeds projected burn rate by more than twenty percent. Cost per query increases beyond economic viability for the product. A single high-cost task type starts dominating spend. Token usage spikes without corresponding traffic increase, indicating inefficiency in prompts or retrieval. These alerts prevent runaway costs that make the product unsustainable.

**Coverage-based alerts** detect distributional shifts. Percentage of queries hitting fallback paths exceeds ten percent. A new task type appears that is not covered by any eval. User queries start clustering in a language or domain your eval suite does not test. These alerts are early signals that your system is being used in ways you did not anticipate, and your eval coverage may have gaps.

**Infrastructure-based alerts** catch operational failures. Eval pipeline has not run successfully in six hours. Eval result database is unreachable. Dashboard metrics are stale by more than two hours. These are meta-alerts — they do not measure model quality, they measure whether your measurement system is working. If your eval pipeline is down, you are blind to quality problems. These alerts protect observability itself.

## The Right Alert Goes to the Right Person at the Right Time

The final piece of alerting design is **routing precision**. A critical alert about a production quality regression goes to the on-call engineer via page. A high-severity alert about cost overrun goes to both the engineering lead and the finance analyst via Slack. A medium-severity alert about eval coverage gaps goes to the evaluation team lead via ticket. An informational alert about a successful model deployment goes to a changelog channel that product managers subscribe to.

Routing precision reduces noise for each role. The on-call engineer only gets paged for things that need immediate action. The finance analyst only sees cost alerts. The product manager only sees summary notifications, not operational details. The eval team sees everything related to eval pipeline health, but does not get paged at 3am unless the entire eval system is down.

Routing also accounts for **blast radius**. An alert about a quality regression in a single low-traffic task type might go to the engineer who owns that task. An alert about a systemic failure affecting all Tier 1 tasks goes to the entire engineering team and the VP. The scope of the problem determines the scope of the notification. Small problems stay small. Big problems get the attention they deserve.

When alerting is tuned correctly, alerts are rare, trusted, and actionable. The team does not dread the on-call rotation because the pager only goes off when something real is happening. Engineers do not mute alert channels because the signal-to-noise ratio is high. Incidents get caught early because the alerts fire before users complain. That level of operational discipline does not happen by accident. It happens when you design thresholds, severity levels, and escalation paths with the same rigor you apply to model architecture. The alert is not just a notification — it is the trigger for a well-rehearsed response process, and the quality of that trigger determines whether you catch problems early or discover them too late.

Alerts tell you when to look. Dashboards tell you what to look at. But not every audience needs the same view, which is why dashboard design must account for who is looking and what decisions they need to make.

---

*Next: 10.6 — Dashboard Design for Different Audiences*
