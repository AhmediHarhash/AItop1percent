# 3.5 — Keyword and Phrase Presence Signals

**Keyword presence** is the simplest quality signal in automated evaluation. If the output contains the required term, it passes. If it does not, it fails. This is not sophisticated. It is not nuanced. It is also brutally effective for certain failure modes, runs in microseconds, and catches entire classes of errors that expensive LLM-based judges miss because they focus on semantic meaning instead of literal compliance.

## Required Phrase Detection

In domains with strict language requirements, certain phrases must appear. A financial disclaimer must include "not financial advice" or equivalent language. A medical response must include "consult a healthcare provider." A legal document summary must state "this is not legal advice." These are not stylistic preferences. They are compliance requirements, risk mitigation, or product policy.

Your heuristic evaluator scans every output for required phrases. You define the requirement set per task type or per use case. For medical Q&A, the requirement is the disclaimer phrase. For financial content, the requirement is the risk warning. For age-restricted content detection, the requirement might be the presence of a content warning label. The heuristic checks: is the required text present? Yes or no. Binary. Fast.

If a required phrase is missing, the output fails the heuristic check. It does not ship to users without review. This prevents entire categories of risk. In November 2025, a consumer finance chatbot went to production without automated disclaimer detection. Within three days, the compliance team flagged 600 outputs that provided investment guidance without the legally required disclosures. The platform was pulled offline for a week. A heuristic check for phrase presence would have blocked every single one of those outputs before they reached users.

Required phrase detection also applies to functional content. If the task is to generate a confirmation email, the email must contain the order number, the confirmation code, or both. You define the required fields, map them to expected text patterns, and check for presence. "Order number" or a digit string in a specific format or the placeholder text your template system expects. The heuristic does not validate that the order number is correct — that requires integration testing. It validates that an order number appears at all, which catches template errors, formatting bugs, and model outputs that omit required information.

## Prohibited Phrase Detection

The inverse of required phrases is **prohibited phrases** — text that must never appear. Brand competitors' names in marketing copy. Profanity in customer-facing outputs. Specific medical terms that indicate the model is providing diagnostic advice when it should only provide general information. Internal code names, project names, or employee identifiers that should never leak into user-facing text.

You maintain a prohibited phrase list. The heuristic scans every output. If any phrase on the list appears, the output is flagged or blocked. This is not content moderation at the model level — this is post-generation filtering. The model might generate prohibited content due to adversarial prompts, training data bleed-through, or edge case behavior. The heuristic catches it before the user sees it.

Prohibited phrase lists evolve. When a new competitor launches, you add their brand name to the list. When a regulatory change bans certain terminology, you add it. When an internal incident reveals that a code name leaked into outputs, you add it retroactively and scan historical logs. The list is never complete, but every addition prevents future failures. In June 2025, a B2B SaaS platform discovered that their model occasionally referenced "Project Atlas," an internal initiative name, in generated user documentation. The term had appeared in 40 outputs over two months. Adding "Project Atlas" to the prohibited phrase list and rescanning outputs identified every instance. The heuristic that should have been running from day one was implemented retroactively, but it stopped further leaks immediately.

Prohibited phrase detection also applies to competitor mentions, not just to prevent brand confusion but to detect model training artifacts. If your model occasionally suggests a competitor's product when users ask for recommendations, that indicates training data contamination or fine-tuning on comparison content that should have been filtered. Detecting competitor names in outputs is a data quality signal. It tells you the model learned patterns you did not intend to teach.

## Domain-Specific Terminology Presence

In specialized domains, **terminology presence** is a proxy for output quality. A legal contract summary should contain terms like "indemnification," "liability," "termination," or "breach." A medical literature summary should contain anatomical terms, pharmacological names, or diagnostic codes. A technical API documentation output should contain method names, parameter types, and return value descriptions. The presence of domain-appropriate vocabulary signals that the model is engaging with the subject matter at the correct depth.

You build a domain terminology list for each task type. The list contains terms that experts expect to see. The heuristic checks: how many domain terms appear in the output? If the count is below threshold, the output is flagged as potentially shallow, generic, or off-topic. This does not guarantee correctness, but it filters out responses where the model clearly failed to engage with domain content.

In a radiology report summarization task, domain terminology might include "lesion," "attenuation," "contrast enhancement," "radiodensity," "opacity," and specific anatomical region names. If a summary contains none of these terms, it is probably too high-level or incorrect. A heuristic that requires at least three domain terms per summary output catches cases where the model generated a generic health statement instead of engaging with the radiology content. This happened to a medical AI startup in early 2025. Their summarization model occasionally returned statements like "The scan shows some findings that should be discussed with a doctor." Technically not wrong, but clinically useless. Terminology presence heuristics flagged these outputs as outliers. Human review confirmed they were failures.

Domain terminology is context-dependent. Not every financial output needs to mention "amortization" or "capitalization." But if you are summarizing a loan agreement and neither term appears, something is missing. The heuristic sets a floor, not a ceiling. It catches the worst failures — outputs that are generically related to the topic but lack depth or specificity.

## Disclaimer and Warning Detection

Users often ignore disclaimers. That does not mean you can skip them. In regulated industries, disclaimers are legal requirements. In high-risk domains, disclaimers are ethical obligations. Your model must generate them consistently, and your heuristic must verify their presence.

**Disclaimer detection** checks for the existence of specific warning language in outputs that require it. Medical advice outputs must include "This is not a substitute for professional medical advice." Financial guidance must include "This is not financial advice." Legal information must include "This is not legal advice. Consult an attorney." The exact phrasing varies by jurisdiction and by company policy, but the requirement does not. The text must appear.

You encode these requirements into your evaluator. For every task type tagged as requiring a disclaimer, the heuristic checks for the presence of one of several acceptable phrasings. You allow variation — "not a substitute for," "does not constitute," "should not be considered" — but you require that one variant appears. If none do, the output fails.

In September 2025, a mental health chatbot launched without automated disclaimer verification. The system prompt included disclaimer instructions, and manual review of 100 outputs confirmed compliance. In production, 4% of outputs omitted the disclaimer due to edge case prompt structures that caused the model to truncate responses or reformat outputs in ways that dropped the final disclaimer paragraph. Users noticed. Regulatory reviewers noticed. The platform added a heuristic disclaimer check within 48 hours. Every output without the required language was blocked from delivery until a human reviewer confirmed it was safe or regenerated it. The heuristic runs in under a millisecond per output. The regulatory exposure it prevented was worth millions.

## Hedge Word Detection

**Hedge words** are qualifiers that soften claims: "might," "possibly," "perhaps," "could," "may," "sometimes," "often," "generally," "typically." In some contexts, hedging is appropriate — expressing uncertainty is more honest than false confidence. In other contexts, hedging is a red flag. If the user asked for a definitive procedure and the model responds with "you might want to try," the output is weak.

Hedge word detection counts the frequency of qualifiers in an output. If hedge word density exceeds a threshold — say, more than one hedge word per 50 tokens — the output is flagged as uncertain or evasive. This heuristic does not judge whether uncertainty is appropriate. It surfaces outputs with high uncertainty language so you can review them.

Some domains expect hedging. Medical models should hedge because definitive claims about symptoms are dangerous. Financial models should hedge because markets are unpredictable. But customer support models should not hedge when answering procedural questions. "You might be able to reset your password by clicking the link" is worse than "Reset your password by clicking the link." Hedge word detection, combined with task type tagging, lets you set different thresholds for different use cases.

In a batch of 10,000 customer support outputs, if 200 contain unusually high hedge word counts, those 200 go to human review. Reviewers often find that high-hedge outputs correlate with the model lacking confidence because the prompt was ambiguous, the knowledge base did not contain a clear answer, or the question was adversarial. The hedge words are a symptom. The heuristic detects the symptom so you can diagnose the cause.

Hedge words also correlate with soft refusals. A response that starts with "I'm not entirely sure, but you could possibly..." is functionally a deflection. Combining hedge word detection with short response length and absence of domain terminology creates a composite heuristic that flags low-quality, evasive outputs with higher precision than any single signal alone.

## Keyword Signals as Quality Proxies

Certain keywords correlate with correctness. Others correlate with failure. These correlations are domain-specific, but once you identify them, they become heuristic features. In customer support, the presence of "ticket number," "confirmation code," or "reference ID" correlates with outputs that answered the user's question. The absence of these terms in queries that ask "what is my ticket number" correlates with failure.

You identify quality-correlated keywords by analyzing eval set results. Take your labeled ground truth data. Split outputs into correct and incorrect. Extract the most frequent terms in each group. Terms that appear significantly more often in correct outputs become positive signals. Terms that appear significantly more often in incorrect outputs become negative signals. You encode these signals into your heuristic evaluator.

A legal contract QA system found that correct answers to "what is the termination clause" contained terms like "notice period," "days," "termination," and "either party." Incorrect answers contained terms like "depends," "varies," "consult," and "agreement." The heuristic awarded positive weight to outputs containing the first set and negative weight to outputs containing the second set. The weighted score was not a final judgment, but it ranked outputs by likelihood of correctness. The top 20% and bottom 20% went to human review. The middle 60% were sampled. This triaged human effort toward the cases most likely to matter.

Keyword signals are brittle. If your model updates or your data distribution shifts, the correlations break. You re-run the keyword extraction analysis every time you retrain, every time you change models, and every time you add new task types. The heuristic is cheap enough to recompute regularly. You do not hardcode keyword lists for years — you treat them as dynamic features derived from recent data.

## Negative Keyword Detection for Off-Topic Outputs

**Negative keywords** are terms that should never appear in certain contexts. A professional email generator should not produce outputs containing "lol," "brb," or emoji text representations. A medical information system should not generate outputs containing "just Google it" or "I'm not a doctor, but." A B2B sales assistant should not mention consumer products unrelated to the company's catalog.

You define negative keyword lists per use case. The heuristic scans for them. Detection triggers a flag. In most cases, a negative keyword is an automatic failure. An email to a client that contains "lol" is not subjective — it is objectively inappropriate for the context. The heuristic blocks it, logs it, and routes it to review so the team understands why the model generated informal language in a formal setting.

Negative keyword detection also catches training data leakage. If your model occasionally uses phrases from internet forums, social media, or scraped web content in contexts where those phrases do not belong, the negative keyword list surfaces that behavior. In April 2025, a contract generation tool occasionally inserted the phrase "as per usual" into legal language — a colloquialism that had no place in binding agreements. The phrase appeared in less than 0.5% of outputs, but every instance was flagged by the negative keyword heuristic. Investigation revealed it was a training data artifact from informal business communications that had been included in the fine-tuning set. The data was cleaned, the model retrained, and the phrase disappeared.

## Integration with Multi-Layer Eval

Keyword and phrase presence signals are fast, deterministic, and scalable. They run on every output in milliseconds. They do not replace semantic evaluation, but they catch failures that semantic evaluators miss because LLM-based judges reason about meaning, not literal compliance. A judge might rate an output as high quality based on tone and coherence while missing that it omitted a legally required disclaimer. The heuristic catches the omission.

You layer keyword heuristics with other eval methods. Every output passes through keyword checks first. Failures are blocked or flagged immediately. Passing outputs proceed to model-based judges, human review, or end-to-end integration tests depending on risk tier. This creates a filtering funnel: the cheapest, fastest checks eliminate obvious failures, and progressively more expensive checks evaluate the remainder.

Keyword signals also inform sampling strategy. If an output fails multiple keyword heuristics — missing required phrases, containing prohibited terms, low domain terminology count, high hedge word density — it gets prioritized for human review even if the model-based judge rated it as acceptable. Disagreement between heuristic and judge is itself a signal. Either the heuristic is miscalibrated, or the judge missed something. Human review resolves the conflict and improves both systems over time.

The next heuristic layer — readability and complexity scores — extends pattern matching into structural analysis, measuring not what the output says but how it says it, detecting complexity mismatches that degrade user experience even when content is technically correct.

