# 6.11 — Behavioral Eval Coverage for Agentic Systems

Coverage is not the percentage of your code that tests touch. Coverage is the percentage of your agent's behaviors that evaluation exercises. You can have a thousand eval cases and still have near-zero behavioral coverage if all those cases test the same narrow slice of functionality. You can have fifty eval cases and achieve meaningful coverage if those cases systematically exercise every tool, every decision path, every failure mode, and every edge case the agent might encounter in production. Most teams measure coverage wrong, achieve coverage in the wrong places, and ship agents with massive blind spots that production users discover within hours.

**Behavioral eval coverage** is the map of which agent capabilities, tool combinations, context conditions, and failure modes your evaluation suite actually tests — and which ones it does not. This is not a percentage. This is a multi-dimensional assessment across tools, conversation flows, context sizes, input variations, failure injection points, and boundary conditions. High coverage means that when your agent behaves unexpectedly in production, you have already tested a similar scenario and you have a baseline for what correct behavior looks like. Low coverage means production is your real test environment, and users are your unwitting evaluators.

The teams that achieve high behavioral coverage do not do it by writing more eval cases. They do it by building a coverage map first — a structured inventory of every behavior the agent should exhibit — and then systematically designing eval cases to exercise each area of that map. The teams that achieve low coverage write eval cases reactively, adding tests when something breaks, accumulating hundreds of overlapping test cases that all exercise the same two or three workflows while leaving entire capability areas untested.

## The Coverage Map for Agentic Systems

A coverage map is the inventory of everything your agent can do, every condition under which it operates, and every failure it might encounter. The map has six dimensions: tools and actions, conversational flows, context conditions, input variations, failure modes, and safety boundaries. Each dimension divides into specific coverage areas. Your eval suite should touch every area. If it does not, you have a coverage gap.

The first dimension is tools and actions. List every tool your agent can call, every API it can invoke, every database it can query, every external service it can access. For each tool, list the distinct ways the agent might use it — different parameter combinations, different sequencing with other tools, different conditional logic that determines whether to call it at all. A calendar agent might have a scheduling tool. The coverage map lists: schedule a single event, schedule recurring events, schedule events with multiple participants, schedule events with conflicts that require resolution, schedule events in different time zones, schedule events that span multiple days, cancel events, reschedule events, query availability before scheduling. Each of these is a coverage area. If your eval suite has no cases that test recurring event scheduling, you have a gap.

The second dimension is conversational flows. Single-turn queries. Multi-turn conversations where context builds across turns. Conversations where the user changes topic mid-flow. Conversations where the user corrects the agent's understanding. Conversations where the agent asks clarifying questions. Conversations where the agent hands off to a human. Conversations where the user provides information incrementally — asking a question, receiving an answer, then providing additional constraints that require re-planning. Each flow pattern is a coverage area. Most agent eval suites have strong coverage of single-turn queries and weak coverage of everything else. Production users have multi-turn conversations. Your eval suite should too.

The third dimension is context conditions. Small context, where the agent operates on minimal conversation history and a small set of retrieved documents. Medium context, where the conversation has gone several turns and the context window is half full. Large context, where the agent is near the context limit and must prioritize which information to retain. Empty external state, where the user has no order history, no prior tickets, no calendar events. Rich external state, where the user has extensive history that the agent must filter and prioritize. Time-sensitive context, where the user's question depends on the current date, time, or real-time event data. Each context condition changes agent behavior. If you only test with small context and empty external state, you have not tested the conditions most production users will create.

The fourth dimension is input variations. Well-formed queries that match the agent's expected input format. Ambiguous queries that require clarification. Queries with implicit context that the agent must infer from conversation history. Queries with multiple intents that the agent must decompose. Queries in different languages. Queries with typos, slang, or non-standard phrasing. Queries that reference information from previous turns. Queries that contradict prior information the user provided. Each variation tests a different aspect of the agent's input understanding. If your eval cases only use well-formed queries, you are testing a version of the agent that no production user will ever interact with.

The fifth dimension is failure modes. API timeouts. API errors. Retrieval returning no results. Retrieval returning too many results to fit in context. Tool calls with invalid parameters. Tool calls that succeed but return unexpected data formats. External services being unavailable. Rate limits being exceeded. Partial failures, where one tool call in a sequence succeeds and the next fails. Each failure mode requires the agent to recover gracefully — retry, fall back to a different tool, ask the user for more information, escalate to a human. If your eval suite assumes every tool call succeeds and every API is always available, you have not tested the conditions that cause most production incidents.

The sixth dimension is safety boundaries. Queries that attempt to trick the agent into revealing sensitive data. Queries that ask the agent to perform destructive actions without proper authorization. Queries that attempt to inject instructions into retrieved documents or tool outputs. Queries that push the agent toward policy violations. Queries designed to extract training data or internal system prompts. Each boundary test verifies that the agent refuses correctly, explains why, and does not leak information in the refusal. If your eval suite does not include adversarial cases, you have not tested whether your agent can withstand the attacks it will face in production.

## Measuring Coverage Systematically

Once you have a coverage map, measuring coverage is mechanical. For each area on the map, count how many eval cases exercise that area. If the scheduling tool has nine usage patterns and your eval suite has cases for seven of them, your tool coverage for scheduling is 78%. If conversational flows have six patterns and your eval suite covers three, your flow coverage is 50%. Aggregate across dimensions to see where coverage is strong and where it is weak.

Most teams discover that coverage is uneven. They have excellent coverage of the happy path — the most common tools, the simplest conversational flows, the best-case input formats. They have poor coverage of edge cases, failure modes, and adversarial inputs. This happens because eval cases accumulate organically. Someone builds a test for the primary workflow. Someone else adds a variation. A production bug prompts a new test case. Over time, you have fifty test cases, forty of which test minor variations of the same workflow. The coverage map makes the imbalance visible.

Systematic coverage improvement means identifying the largest gaps and writing eval cases to fill them. If you have zero cases that test API timeouts, write five cases that inject timeouts at different points in the agent's workflow and verify recovery behavior. If you have zero cases that test multi-turn conversations where the user changes topic, write ten cases with realistic topic shifts and verify the agent handles them without losing context. If you have zero cases that test time zone handling for scheduling, write cases for every time zone your users operate in. The goal is not 100% coverage — that is unachievable and unnecessary. The goal is eliminating the gaps that pose the highest production risk.

The coverage map also reveals redundancy. You might have twenty eval cases that all test the same tool in the same way with minor input variations. Those cases give you no additional coverage. They add runtime cost, maintenance burden, and noise when one fails. Consolidate them. Keep the three cases that represent meaningfully different input patterns. Delete the seventeen that are redundant. High-quality coverage is not about case count. It is about breadth across the coverage map.

## The Coverage Versus Depth Tradeoff

High coverage means you have at least one eval case for every area of the map. High depth means you have many eval cases for each area, testing subtle variations and edge cases within that area. You cannot have both everywhere — the eval suite would be too large to run, too expensive to maintain, and too slow to give feedback. The tradeoff is deliberate: prioritize coverage for high-risk areas and depth for the most critical workflows.

High-risk areas are tools with destructive potential, workflows with compliance requirements, and behaviors where failure has immediate user-visible consequences. A customer service agent that can issue refunds needs deep eval coverage of refund logic — edge cases, boundary conditions, authorization checks, audit trail verification. A scheduling agent that can cancel meetings needs deep coverage of cancellation flows — single events, recurring events, events with many participants, events that other systems depend on. The cost of a failure in these areas is high enough to justify ten or twenty eval cases per workflow.

Lower-risk areas get breadth without depth. A weather lookup tool does not need ten eval cases. It needs two: one that verifies the agent calls it correctly with valid parameters, and one that verifies the agent handles the case where the weather service is unavailable. A time zone conversion tool does not need exhaustive testing of every time zone pair. It needs a handful of cases covering common zones, edge cases like daylight saving time transitions, and the failure case where the input time zone is unrecognized. You get coverage without runaway case count.

The depth tradeoff also applies within conversational flows. Every agent should have a few end-to-end conversation scenarios with realistic depth — ten turns, topic shifts, clarifications, corrections. These scenarios test context management, state retention, and multi-step reasoning. But you do not need a hundred of them. You need five to ten high-quality scenarios that cover the most common conversation patterns and a few that cover edge cases like the user contradicting themselves mid-conversation or asking a follow-up that requires re-executing a prior tool call with updated parameters.

The stable pattern is a tiered eval suite. Tier one is breadth — one or two cases per coverage area, enough to verify the agent can handle that area at all. Tier two is depth for critical paths — ten to twenty cases for the workflows that must never fail. Tier three is adversarial and stress cases — a smaller set of cases designed to break the agent, test safety boundaries, and verify behavior under extreme conditions. Most of your eval cases are tier one. Most of your eval runtime and maintenance effort goes into tier two. Tier three is small but high-value because it is where you discover the failures that adversarial users or unexpected production conditions will expose.

## Gaps in Behavioral Testing and How They Surface

Coverage gaps do not announce themselves. They surface as production incidents that no eval case predicted. A user asks a multi-turn question that requires the agent to maintain state across five turns. Your eval suite only tests single-turn and two-turn conversations. The agent drops context in turn four, gives a wrong answer, and the user files a complaint. You had a coverage gap in multi-turn flows, and production found it.

The most common gaps are failure mode coverage, multi-turn conversation coverage, and boundary condition coverage. Teams build eval cases for success paths because success paths are easier to design and easier to verify. Failure paths require injecting errors, mocking unavailable services, and defining what graceful degradation looks like. Multi-turn conversations require realistic dialogue flows and careful management of conversation state. Boundary conditions require identifying the edges of the agent's operating envelope — the longest context it will see, the most ambiguous input it should handle, the most complex tool call sequence it might need.

Each gap has a signature. If your production incidents cluster around "agent failed to recover from API timeout," you have a gap in failure mode coverage. If incidents cluster around "agent lost context after three turns," you have a gap in multi-turn coverage. If incidents cluster around "agent mishandled query in unsupported language," you have a gap in input variation coverage. Production incidents are the signal that tells you which gaps matter most. Track them. Categorize them. When a category reaches a threshold — say, five incidents in a month — that category becomes a coverage priority. Write eval cases that would have caught those incidents, add them to your suite, and verify that future changes do not reintroduce the same failures.

Some gaps only surface under load. An agent that works perfectly for one user at a time starts failing when ten users hit it simultaneously because the retrieval system starts queueing requests, latency spikes, and the agent times out waiting for search results. Your eval suite runs serially. It has no load coverage. You need a separate load testing suite that runs a subset of your behavioral eval cases under concurrent load and verifies that performance and correctness degrade gracefully, not catastrophically.

Other gaps surface when the system scales in data size. An agent that works perfectly with a knowledge base of a thousand articles starts failing when the knowledge base grows to a hundred thousand articles because retrieval returns too many results, the ranking degrades, and the agent surfaces irrelevant documents. Your eval suite runs against a small test knowledge base. It has no scale coverage. You need eval cases that test the agent's behavior with production-scale data volumes, even if those cases run less frequently because they are slower and more expensive.

## Prioritizing Behavioral Eval Investment

You cannot achieve perfect coverage. The coverage map is large, eval cases are expensive to write and maintain, and the eval suite has a runtime budget. Prioritization is the discipline of achieving the highest-value coverage with the resources you have. The highest-value coverage is the intersection of high production usage and high failure impact.

Start with production usage data. Which tools does the agent call most frequently? Which conversational flows do users trigger most often? Which input patterns appear in the majority of queries? High-usage areas are high-priority coverage targets because failures there affect the most users. If 60% of production queries are single-turn questions that trigger one tool call, those workflows need deep eval coverage. If 5% of queries are multi-turn conversations that require three or more tool calls, those workflows need coverage, but not as much depth.

Layer on failure impact. A tool that gets called frequently but has low failure impact — like a weather lookup — does not need exhaustive coverage. A tool that gets called infrequently but has high failure impact — like issuing a refund or canceling a subscription — needs deep coverage despite low usage. The worst outcome is a rarely-used tool failing catastrophically because you never tested it. The cost of one catastrophic failure exceeds the cost of writing ten eval cases.

Consider compliance and safety requirements as forcing functions. If a regulatory requirement says the agent must never expose protected data, you need adversarial eval coverage of every prompt injection and data extraction attack you can imagine, regardless of usage frequency. If a safety requirement says the agent must never execute a destructive action without user confirmation, you need deep coverage of every destructive tool and every scenario where the agent might misinterpret confirmation. Compliance and safety coverage is not optional. It is the baseline. Everything else is prioritization above that baseline.

The final prioritization input is coverage gaps surfaced by production incidents. If production keeps breaking in an area your eval suite does not cover, that gap is now the top priority, regardless of usage frequency or impact modeling. Production has told you where the risk is. Listen.

## Synthesizing Behavioral Evaluation for Agents

Behavioral evaluation is the difference between an agent that works in demos and an agent that works in production. It tests tool usage, decision-making, conversation handling, failure recovery, and safety boundaries — the behaviors that determine whether users trust the agent or abandon it after the first bad experience. The chapters in this section have covered the techniques: tool call verification, decision path testing, conversation state management, multi-agent interaction testing, failure injection, and end-to-end scenario evaluation. The final piece is coverage — the systematic accounting of which behaviors you have tested and which you have not.

Building a coverage map is the first discipline. It forces you to enumerate everything the agent can do, every condition under which it operates, and every failure it might encounter. Without the map, you write eval cases reactively and accumulate redundancy while leaving critical gaps untested. With the map, you write eval cases strategically, filling the gaps that pose the highest production risk.

Measuring coverage is the second discipline. It makes the gaps visible. When you know that 80% of your tools have eval coverage but only 30% of your failure modes do, you know where to invest. When you know that your eval suite has fifty cases for single-turn queries and two cases for multi-turn conversations, you know you are over-investing in one area and under-investing in another.

Prioritizing coverage is the third discipline. You cannot test everything, so you test the intersection of high usage and high impact first, then layer on compliance and safety requirements, then fill gaps surfaced by production incidents. The eval suite becomes a living artifact that evolves as the agent evolves, as production usage patterns shift, and as new failure modes emerge.

The teams that treat behavioral evaluation as a checklist — "we have eval cases, we are done" — ship agents that break in production. The teams that treat it as a coverage discipline — "we have a map, we measure gaps, we prioritize investment" — ship agents that handle the complexity, ambiguity, and adversarial conditions that production throws at them. The difference is not the number of eval cases. It is the systematic approach to ensuring those cases exercise the behaviors that matter.

The next section shifts from behavior to risk. You have verified that your agent executes tools correctly, handles conversations gracefully, and recovers from failures. Now you need to verify that it does not do things it should never do — expose sensitive data, violate policies, execute destructive actions without authorization, or succumb to adversarial attacks. That is the domain of safety and risk evaluation.

