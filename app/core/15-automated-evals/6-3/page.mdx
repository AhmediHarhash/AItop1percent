# 6.3 — Tool Selection Accuracy Metrics

Tool selection evaluation measures whether the agent chose the right tool for each step of the task, used it correctly, and avoided using tools that were unnecessary or counterproductive. This is distinct from task completion — an agent can complete a task while making poor tool choices, and an agent can make perfect tool choices but fail to complete the task due to environmental factors. Tool selection accuracy is a capability metric. It tells you whether the agent understands the purpose of each tool in its toolkit, when to use them, and how to combine them effectively.

## What Tool Selection Reveals

Every tool call an agent makes is a decision. The agent observed some state, evaluated its options, and selected a tool it believed would move the task forward. That decision is either correct, suboptimal, or wrong. Correct means the tool was the right choice for the current state and it contributed to task progress. Suboptimal means a different tool would have been faster, cheaper, or more reliable, but the chosen tool still worked. Wrong means the tool did not accomplish what the agent needed or actively moved the task backward.

Tracking tool selection accuracy across thousands of task executions reveals the agent's mental model of its toolkit. If the agent consistently uses the database query tool when it should use the cache lookup tool, it does not understand the cost-performance trade-off between them. If the agent uses the web search tool to find information that exists in its knowledge base, it does not understand the scope of its own knowledge. If the agent calls three different tools in sequence when a single tool could have done the job, it does not understand tool composition. These patterns are invisible if you only measure task completion. They become obvious when you measure tool selection.

Tool selection accuracy also predicts efficiency. An agent with 95 percent tool selection accuracy completes tasks faster and with lower API costs than an agent with 70 percent accuracy, even if both eventually reach the same outcome. The high-accuracy agent makes fewer exploratory calls, fewer redundant queries, and fewer recovery attempts after using the wrong tool. Efficiency is a lagging indicator. Tool selection accuracy is a leading indicator. Improving it improves everything downstream.

## Defining Tool Selection Ground Truth

To measure tool selection accuracy, you need ground truth for which tool should have been used at each step. This is harder than it sounds. Unlike text generation, where the ground truth is the reference output, tool selection ground truth depends on task state, available context, and the full set of tools the agent could have chosen. The same tool might be correct in one state and wrong in another.

The most reliable ground truth source is expert demonstration. A human expert performs the task using the same tool set the agent has access to, and you log every tool call they make. This log becomes the reference trace. The agent's tool calls are compared to the expert's tool calls step by step. If the expert used the customer database query tool to retrieve account information and the agent used the web search tool, the agent made a wrong selection. If the expert used the sentiment analysis tool to evaluate customer tone and the agent did the same, the agent made a correct selection.

Expert demonstration works well for tasks with deterministic workflows. For tasks where multiple valid paths exist, you need multiple expert traces that cover different approaches. An agent that uses a different tool sequence than any expert trace is not necessarily wrong — it might have found a novel path. You score these cases by comparing the agent's path to the set of all valid paths, not to a single reference trace. This requires either exhaustive enumeration of valid paths, which is impractical for complex tasks, or a learned model that can judge path validity, which introduces its own evaluation challenges.

Rule-based ground truth specifies tool selection logic explicitly. For a given task state, the rules define which tools are valid, which are preferred, and which are prohibited. A customer support agent in a refund request scenario should use the order lookup tool first, the payment validation tool second, and the refund issuance tool third. Using the web search tool at any point is wrong. Using the refund tool before the payment validation tool is wrong. The rules encode the correct decision tree. The eval system compares the agent's actual tool sequence to the rule-specified sequence.

Rule-based ground truth is precise but brittle. It works when the task workflow is well-defined and stable. It fails when the workflow has conditional branches, when edge cases require creative tool use, or when the environment introduces variability that the rules did not anticipate. A hybrid approach combines rules for the common path with expert judgment for edge cases. The rules cover 80 percent of tool selection decisions. Human review covers the remaining 20 percent where rules are insufficient.

## Tool Selection Precision and Recall

Precision and recall apply to tool selection just as they apply to classification tasks. Precision is the fraction of tool calls the agent made that were necessary and correct. Recall is the fraction of necessary tool calls that the agent actually made. An agent with high precision but low recall uses tools correctly but misses opportunities. An agent with high recall but low precision uses all the necessary tools but also makes many unnecessary calls.

Low precision shows up as redundant tool use, exploratory tool calls that do not contribute to task progress, and tool calls that retrieve information the agent already has. A scheduling agent that queries the calendar three times in a row without any intervening state changes has low precision. A data retrieval agent that calls both the primary database and the backup database when only the primary was needed has low precision. Every unnecessary tool call costs money, adds latency, and increases the chance of encountering rate limits or transient errors. Precision failures are efficiency killers.

Low recall shows up as missing information, skipped validation steps, and incomplete workflows. A fraud detection agent that checks transaction amount and user location but never checks transaction velocity has low recall — it missed a necessary tool call. A contract review agent that extracts key terms but never checks for conflicting clauses has low recall. Recall failures lead to incomplete task execution and lower-quality outcomes. An agent with 60 percent recall completes tasks, but it completes them badly.

Measuring precision requires identifying which tool calls were unnecessary. This is straightforward for exact duplicates — calling the same tool with the same parameters twice in a row is always unnecessary. It is harder for near-duplicates and exploratory calls. An agent might query a database for customer orders, receive the results, then query again with slightly different filters to refine the result set. The second call might be unnecessary if the first result set already contained the needed information, or it might be necessary if the agent refined its understanding of what it needed after seeing the first results. Precision scoring must distinguish between these cases, which often requires task-level analysis rather than call-level analysis.

Measuring recall requires knowing which tools should have been called but were not. This is where ground truth traces or rule-based workflows become essential. You compare the agent's tool call set to the expected tool call set. Missing calls are recall failures. The challenge is handling conditional tool calls — tools that should be used only under certain conditions. If the agent correctly determined that a condition was not met and skipped the associated tool call, that is not a recall failure. If the agent incorrectly assessed the condition and skipped a necessary call, that is a recall failure. Distinguishing these requires evaluating the agent's condition assessment logic, not just the tool call log.

## Unnecessary Tool Calls

Unnecessary tool calls fall into four categories: duplicates, premature calls, post-completion calls, and exploratory waste. Each type has different root causes and different performance impacts.

Duplicates are tool calls that retrieve the same information the agent already has. The agent queries the customer database to get account details, receives the response, and then queries the same database with the same parameters two steps later. The second call is a duplicate. Duplicates happen when the agent does not track its own context, when it does not cache tool results, or when it regenerates its plan without considering prior actions. Duplicate call rate is your primary measure of context tracking capability.

Premature calls happen when the agent uses a tool before it has the information needed to use it correctly. A travel booking agent calls the flight search API before asking the user for travel dates, receives an error, asks for the dates, and then calls the API again. The first call was premature. The agent should have gathered all required inputs before making the API call. Premature call rate indicates poor task planning. The agent is acting reactively instead of planning ahead.

Post-completion calls happen after the agent has already achieved the task goal. The agent completes the refund, verifies the refund was processed, updates the ticket status, and then queries the customer satisfaction survey system to log feedback that was never requested. The survey call is post-completion. The task is done, but the agent kept working. Post-completion calls are a sign that the agent does not recognize terminal states correctly. It does not know when to stop.

Exploratory waste is tool calls made during trial-and-error search when the agent does not know which tool to use. The agent tries the document search tool, gets no results, tries the database query tool, gets partial results, tries the external API, gets the full result. Two of those three calls were exploratory waste. Some exploration is inevitable when the agent encounters novel situations. Excessive exploration — more than two exploratory calls per decision point — indicates that the agent does not have a good prior over which tool is likely to work in which situations.

## Missing Tool Calls

Missing tool calls are the inverse of unnecessary calls. These are tools the agent should have used but did not. Missing calls lead to incomplete information, unvalidated assumptions, skipped safety checks, and ultimately task failures or low-quality completions.

Missing validation calls are the most dangerous category. The agent takes an action without first verifying that the action is safe or appropriate. A data deletion agent deletes records without first checking whether those records are still in use. A financial transaction agent transfers funds without first verifying that the source account has sufficient balance. A notification agent sends an email without first checking whether the recipient has unsubscribed. Every one of these is a missing validation call. The agent had access to a tool that would have prevented the error, and it chose not to use it.

Missing information retrieval calls leave the agent working with incomplete context. A customer support agent resolves a ticket without first checking the customer's full interaction history, missing the fact that this is the third time the customer has reported the same issue. A content moderation agent flags a post as violating policy without retrieving the subreddit rules that explicitly allow that type of content. The agent had access to tools that would have provided critical context, and it proceeded without them. The outcome is lower quality or outright incorrect.

Missing escalation calls happen when the agent attempts to handle a situation it should have passed to a human or a more capable system. The agent encounters a task with uncertainty above its confidence threshold, recognizes the uncertainty, but tries to complete the task anyway instead of calling the escalation tool. Missing escalation calls are detectable by comparing the agent's confidence scores to the actual task difficulty. When confidence is low and the agent does not escalate, that is a missing call.

## Tool Selection as Agent Capability Signal

Tool selection accuracy is one of the clearest signals of agent intelligence and task understanding. An agent that consistently selects the right tool at the right time demonstrates that it has an accurate model of its environment, a correct understanding of each tool's purpose and preconditions, and the planning capability to sequence tool use effectively. An agent that makes frequent tool selection errors demonstrates gaps in one or more of these areas.

Tracking tool selection accuracy over time shows learning. As you add training examples, refine prompts, or improve the agent's planning module, tool selection accuracy should improve. If it does not, your improvements are not addressing the core issue. Tool selection is a more sensitive metric than task completion for detecting marginal improvements. A small prompt change might not move task completion rate from 87 percent to 89 percent, but it might move tool selection accuracy from 78 percent to 84 percent, which is a clearer signal that the change was beneficial.

Tool selection errors stratified by tool type reveal specific knowledge gaps. If the agent uses the database query tool correctly 95 percent of the time but uses the external API tool correctly only 60 percent of the time, the problem is not general planning capability — it is lack of understanding of when external APIs are appropriate. You can address this with targeted training examples, better tool descriptions, or few-shot prompts that demonstrate external API usage patterns.

Multi-step workflow execution is the next frontier — evaluating whether the agent can chain tool calls together correctly, handle dependencies, recover from intermediate failures, and navigate branching logic to reach task completion across workflows that span dozens of actions.

