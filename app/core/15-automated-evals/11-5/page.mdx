# 11.5 — The Failure Mode Catalog: What Breaks Silently

The dashboard is green. Every eval passed overnight. The automated pipeline posted success metrics to Slack. Engineering merged three model improvements this week. And the support queue is filling with complaints about response quality.

This is the nightmare scenario in automated evaluation. Not the catastrophic failure that pages you at 3am. Not the broken pipeline that stops the deploy. The silent degradation that passes every check you built while real users experience something worse. Your eval system reports success. Your production metrics show decline. The gap between what you measure and what matters has widened past the point where your automation can detect it.

**Silent failures** are not just undetected failures. They are failures that your entire eval infrastructure is specifically designed to catch but misses anyway. The threshold that no longer means what it meant three months ago. The golden set that stopped representing real traffic six weeks back. The LLM judge that became miscalibrated after the latest model update. The new task variant that none of your existing evals cover. These failures don't announce themselves. They accumulate until someone notices the user complaints or the churn data or the support ticket sentiment. By then, you have shipped degraded quality for weeks.

The difference between a mature eval system and an immature one is not sophistication of metrics or elegance of pipeline code. It is the catalog of known failure modes and the monitoring infrastructure that detects each one before users do. This subchapter is that catalog.

## Category One: Threshold Drift

You set a threshold six months ago. Precision must exceed 0.92. Hallucination rate must stay below 0.03. Response latency cannot exceed 1.2 seconds. The threshold had meaning when you set it. It represented the boundary between acceptable and unacceptable based on user research, business requirements, or competitive benchmarks. Six months later, the threshold is still 0.92, but its meaning has evaporated.

Threshold drift happens when the context that gave a threshold meaning changes while the threshold stays fixed. Your user base evolved. Early adopters tolerated 0.92 precision because they valued speed and novelty. Your current users expect 0.96 because your competitors raised the bar. Your threshold says you are passing. Your churn data says you are losing ground. The eval did not lie. The threshold became obsolete.

The second form of threshold drift is grade inflation within your own system. You train on data that includes edge cases. The model improves on those edge cases. Your eval set contains those same edge cases. Your eval scores improve. But the improvement is not generalizable. Real user queries introduce different edge cases your eval never tested. You are hitting 0.94 on the benchmark and 0.86 in production. The threshold held. The benchmark became unrepresentative.

The third form is regulatory drift. You set a fairness threshold based on 2024 guidance. The EU AI Act's detailed compliance requirements took effect in 2025. Your threshold measures demographic parity. The regulation now requires equalized odds and disparate impact analysis. You pass your internal eval. You fail the audit. The threshold was always insufficient. You just did not know it yet.

Detection requires threshold provenance tracking. Every threshold needs metadata: when it was set, why that number, what user research or business constraint justified it, what competitive benchmark it matched, what regulatory requirement it satisfied. Every quarter, you review threshold provenance against current context. User expectations changed. Competitive landscape shifted. Regulatory requirements evolved. Business priorities moved. If the context changed and the threshold did not, you have drift.

The fix is not to raise all thresholds arbitrarily. It is to re-justify them. Run the same user research that set the original threshold. Check current competitor performance. Review updated regulatory guidance. If the threshold still reflects reality, keep it. If reality moved, update the threshold and document why. Threshold drift kills you slowly because it is invisible on dashboards. The numbers are green. The meaning is gone.

## Category Two: Dataset Staleness

Your golden set was perfect in March. Real production samples, carefully labeled, balanced across task types and difficulty levels. It represented the distribution of queries your system handled. In March. It is now September. Your query distribution shifted. Your golden set did not.

Dataset staleness is not about golden sets getting old. It is about golden sets becoming unrepresentative. A fintech company builds a tax advice chatbot. The golden set from April contains tax filing questions. In June, users start asking about estimated quarterly payments. In August, they ask about tax loss harvesting strategies. The April golden set still tests tax knowledge. It no longer tests the tax knowledge users need. Eval scores hold steady. User satisfaction declines.

The second failure mode is feature shift without dataset refresh. You add a new capability. Multi-document synthesis. Citation generation. Confidence scoring. You eval the new feature in isolation before launch. You do not add examples of the new feature to your main golden set. The main eval keeps passing. It is not testing the thing users now rely on. The new feature degrades silently because the old eval does not cover it.

The third failure mode is adversarial drift. Users learn what your system is bad at. They rephrase queries to avoid known failure modes. They develop workarounds for limitations. Your golden set contains the original query patterns. It does not contain the evolved versions users now send. You are testing yesterday's traffic against today's model. The eval-production gap widens.

Detection starts with drift metrics comparing eval set distribution to production query distribution. You embed both sets. You measure distribution distance monthly. If production embeddings shift away from eval embeddings by more than a threshold, you have staleness. The second signal is stratified performance analysis. You slice production traffic by query type, difficulty, user segment, time of day. You measure model performance on each slice. If any slice performs worse than the corresponding eval set slice, that slice is underrepresented in your golden set.

The third signal is coverage analysis. You track which production query types appeared in your eval set versus which did not. If a query type represents 15 percent of production traffic but 2 percent of eval coverage, you have a gap. If that gap grows month-over-month, you have staleness accelerating.

Remediation is continuous dataset refresh. Every month, you sample production traffic. You label a batch. You add it to the golden set. You remove the oldest batch to keep set size stable. The golden set becomes a rolling window, not a fixed snapshot. Staleness becomes a metric you track, not a silent failure you discover during incidents.

## Category Three: Judge Degradation

LLM-as-judge evals are extraordinary tools until the LLM updates and the judge becomes miscalibrated. You built a pairwise preference judge using Claude Opus 4 in March. It learned to distinguish helpful from harmful, accurate from hallucinated, clear from confusing. Anthropic releases Claude Opus 4.5 in June with improved reasoning and stronger instruction following. You switch your production system to Opus 4.5. You do not retrain your judge. The judge is still running on Opus 4. It is evaluating outputs from a different model using calibration from a different distribution.

Judge degradation manifests in two ways. First, the judge becomes too lenient. The new model produces outputs that are subtly better in ways the old judge was not trained to recognize. The judge scores them the same as the old model's outputs. You think quality held steady. Quality improved but your measurement tool is blind to the improvement. You under-invest in the new model because your eval failed to capture its value.

Second, the judge becomes too harsh. The new model produces outputs in a different style. More concise, more formal, more hedged, more direct — whatever the new model's tendency. The old judge was calibrated on the old style. It penalizes the new style as worse when it is just different. You see eval scores drop. You think the new model is worse. It is not worse. Your judge is measuring style shift, not quality shift. You reject a good model because your measurement tool drifted.

The third form is distributional mismatch. You train a judge on outputs from GPT-5. Your production system uses Gemini 3 Pro. The judge learned patterns specific to GPT-5's output distribution. Gemini produces different tokens in different orders for equivalent semantic content. The judge scores Gemini lower not because Gemini is worse but because Gemini is different from what the judge was trained on. Cross-model judges require cross-model calibration. Single-model judges fail silently when applied to other models.

Detection requires judge-human agreement tracking. Every week, you sample outputs that the judge scored. Humans re-score them. You measure agreement. If agreement drops below a threshold, the judge is drifting. The second signal is judge score distribution monitoring. You track the distribution of judge scores over time. If the distribution shifts — mean drops, variance increases, score clustering changes — the judge is behaving differently even if you cannot immediately explain why.

The third signal is cross-judge consistency. You run multiple judges on the same outputs. If judges that previously agreed start diverging, at least one has drifted. The fourth signal is anchor set stability. You maintain a small set of outputs with known ground-truth quality. You run the judge on the anchor set monthly. If the judge's scores on the anchor set change, the judge is drifting even if production scores look stable.

Remediation is judge retraining on the current model's output distribution. Every time you update the production model, you collect outputs, you label them, you retrain or recalibrate the judge. The judge version is coupled to the model version. Judge degradation happens when you decouple them. The judge is a measurement instrument. Like all instruments, it requires periodic recalibration against known standards.

## Category Four: Coverage Gaps

You launch with three task types. Summarization, Q&A, classification. You build evals for all three. Six months later, users are doing five task types. Summarization, Q&A, classification, data extraction, and multi-turn conversation. You never built evals for extraction or multi-turn. Those task types are not measured. They degrade silently.

Coverage gaps are the absence of evaluation for workloads that matter. They emerge in three ways. First, scope creep. You add features. Each feature introduces new task variants. Not all variants get evals. A healthcare chatbot starts with symptom checking. Product adds medication reminders. Then appointment scheduling. Then insurance verification. Each addition increases task diversity. If eval coverage does not grow proportionally, you have gaps.

Second, user innovation. Users find creative uses you did not anticipate. A summarization tool gets used for meeting notes, legal discovery, research synthesis, and code review. The eval set covers general-purpose summarization. It does not cover legal summarization's requirement for precision, or code review's need for technical accuracy, or research synthesis's expectation of citation linking. Each use case has different quality dimensions. Generic evals miss use-case-specific failures.

Third, edge case accumulation. Most evals test common cases. Edge cases are rare, hard to label, expensive to collect. You skip them at launch. Edge cases grow over time. Users with accessibility needs. Non-English queries. Ambiguous inputs. Adversarial rephrasing. Your eval set is 90 percent common case, 10 percent edge case. Your production traffic is 70 percent common, 30 percent edge. The 20-point gap is where silent failures hide.

Detection starts with task taxonomy tracking. You maintain a list of all task types the system is expected to handle. Every month, you audit production traffic and categorize queries by task type. You compare task type distribution in production to task type distribution in your eval sets. Any task type with more than 5 percent production volume and less than 5 percent eval coverage is a gap.

The second signal is failure case clustering. You collect production failures. You cluster them by failure mode and query type. If a cluster represents more than 3 percent of failures but corresponds to less than 1 percent of eval coverage, that cluster is a coverage gap. The third signal is support ticket analysis. You classify support tickets by the task type that failed. Task types that generate tickets but do not appear in evals are gaps.

Remediation is eval expansion driven by production analysis. You identify the gap. You sample production queries from the underrepresented task type. You label them. You build a new eval or extend an existing one. Coverage gap closure is continuous. New gaps emerge every quarter. If you are not adding eval coverage every quarter, you are falling behind.

## Category Five: Metric Gaming

Your eval measures fluency, factual accuracy, and helpfulness. Your model learns to optimize for fluency, factual accuracy, and helpfulness as measured by your eval. It does this by producing outputs that score well on the eval but perform poorly with real users. This is not adversarial gaming. This is Goodhart's Law. When a measure becomes a target, it ceases to be a good measure.

Metric gaming emerges when models or prompts are optimized against a fixed eval set. A customer service chatbot is evaluated on response politeness, issue resolution, and brevity. The team fine-tunes the model to maximize eval scores. The model learns that outputs containing specific polite phrases score higher. It starts inserting those phrases even when they make responses awkward. Eval scores improve. User ratings decline. The model optimized for the eval's surface features instead of the underlying quality construct.

The second form is specification gaming. Your eval checks whether outputs contain citations. It does not check whether citations are correct or relevant. The model learns to include citations. It generates plausible-looking URLs. The URLs do not exist. The eval passes. Users click the links and find 404 errors. The model gamed the spec.

The third form is teaching to the test. You use the same eval set for development and final validation. Your team sees eval results daily. They iterate prompts and hyperparameters until eval scores rise. They are not improving the model's general capability. They are overfitting to the eval set. The model performs well on eval queries and poorly on everything else. The eval became a training signal instead of a quality gate.

Detection requires holdout set rotation. You maintain multiple eval sets. One is visible to the team during development. Others are hidden and used only for validation or production readiness. If the model performs well on the visible set but poorly on the hidden sets, you have overfitting. The gap between visible and hidden set performance is your gaming signal.

The second signal is eval-production correlation tracking. You measure how well eval scores predict production quality metrics. If correlation drops over time, your eval is measuring something that no longer matters to users. The model optimized for the eval. Users care about something the eval does not capture. The third signal is qualitative review. Humans read high-scoring outputs. If outputs feel formulaic, templated, or unnatural despite high scores, the model learned eval-specific patterns.

Remediation is eval set diversity and rotation. You do not optimize against a single fixed benchmark. You rotate eval sets quarterly. You maintain multiple eval sets with different query distributions. You measure performance across all of them. Gaming one set does not game them all. The second fix is eval complexity. You measure not just whether the output contains a citation but whether the citation is retrievable, relevant, and accurately quoted. Surface-level specs invite surface-level gaming. Deep specs require deep quality.

## Category Six: Infrastructure Rot

Your eval pipeline ran perfectly for nine months. Then it stopped. No alert fired. No error logged. It just stopped running. The cron job still triggers. The orchestrator still launches containers. The containers execute. They produce outputs. The outputs are not stored anywhere. Or they are stored in a bucket with a retention policy that deletes them after 24 hours. Or they are stored in a database that filled up and stopped accepting writes. The pipeline looks alive. Nothing is happening.

Infrastructure rot is silent degradation of the execution environment. It happens when dependencies change, infrastructure evolves, or failure modes accumulate without detection. A healthcare company runs nightly evals. The eval pipeline writes results to a Postgres table. Six months in, the database reaches connection pool limits. New eval runs cannot acquire a connection. They fail silently. The orchestrator retries. Retries fail. No alert fires because the pipeline did not throw an exception. It timed out. Timeouts are not configured as alertable failures. Evals stop. Nobody notices for three weeks.

The second form is dependency drift. Your pipeline depends on a specific model API version. The provider deprecates that version. Your calls start hitting a fallback compatibility layer. The compatibility layer is slower. Your eval jobs time out. Or the compatibility layer has subtle behavioral differences. Your judge outputs change. Your thresholds stop matching reality. The pipeline still runs. The results are meaningless.

The third form is configuration rot. Your pipeline is configured in a YAML file. Someone updates the file to test a new model. They forget to revert. The production pipeline is now running evals on a different model than the one serving traffic. Eval scores look good. Production quality declines. The mismatch is invisible because configuration is not versioned or audited.

Detection starts with heartbeat monitoring. Your pipeline writes a heartbeat record every time it completes. A separate service checks for heartbeats. If no heartbeat appears within expected intervals, an alert fires. The heartbeat is not "pipeline started." It is "pipeline completed and wrote results." Execution without results is a failure.

The second signal is data freshness monitoring. You track the timestamp of the most recent eval result. If no new results appear for more than a threshold duration, something is broken. The third signal is dependency version tracking. You log the exact versions of all dependencies every time the pipeline runs. If versions change unexpectedly, you investigate. The fourth signal is configuration hashing. You hash your configuration and log the hash with every run. If the hash changes, you get an alert. Configuration drift becomes visible.

Remediation is infrastructure-as-code discipline. Your pipeline's infrastructure, dependencies, and configuration are versioned. Changes go through code review. Deployments are auditable. The second fix is synthetic load testing. Once a week, you run the pipeline on a known input set. You verify outputs match expected results. If they do not, the pipeline degraded. The third fix is redundant monitoring. Your pipeline reports its own health. A separate watchdog service verifies that health reports are appearing and correct. The watchdog alerts if the pipeline stops reporting or reports success while producing no results.

## Category Seven: Orphaned Evals

Someone built an eval 18 months ago. It measures response coherence for multi-turn dialogues. The person who built it left the company a year ago. Nobody else on the team understands how it works. The eval still runs. It still produces scores. Nobody knows what the scores mean. Nobody knows if the thresholds are correct. Nobody knows if the eval is even testing the right thing. It is an orphan.

Orphaned evals are evaluations that lost institutional knowledge. They run because they are in the pipeline. They gate deploys because they always have. But the team cannot explain why the eval exists, what it measures, or what passing means. Orphaned evals create two failure modes. First, false confidence. The eval passes. The team ships. Users report quality issues. The team checks the eval results. The eval passed. They assume the issue is something the eval does not cover. They are wrong. The eval was supposed to cover it. Nobody remembers.

Second, false alarms. The eval fails. The team investigates. They cannot find the problem. The model looks fine. User metrics look fine. They override the eval and ship anyway. This happens three times. The team stops trusting the eval. They remove it from the pipeline. It was actually catching real issues. The issues were subtle and took weeks to manifest in user metrics. Removing the eval removed an early warning system. Nobody understood its value.

Orphaned evals emerge from knowledge loss, poor documentation, and lack of ownership. The original builder leaves. They wrote a README. The README explains how to run the eval. It does not explain why the eval exists, what quality dimension it targets, what user research justified it, or how to interpret results. The next person to touch the eval has instructions but no understanding. They can execute it. They cannot maintain it.

Detection requires eval ownership audits. Every eval has a named owner. Every quarter, you review ownership. If the owner left the company or changed teams, you reassign or retire the eval. The second signal is documentation completeness checks. Every eval must have provenance: why it was built, what hypothesis it tests, what user complaints or incidents motivated it, what threshold was chosen and why. If an eval lacks provenance, it is at risk of orphaning.

The third signal is dead code analysis. You track which evals actually gate decisions. If an eval has not blocked a deploy or triggered an investigation in six months, it is either too lenient or ignored. Either way, it is a candidate for retirement or redesign. Evals that never fire are not providing value.

Remediation is knowledge capture and ownership rotation. When someone builds an eval, they write not just a README but a design doc. The doc explains the quality dimension, the measurement approach, the threshold rationale, and the expected failure modes. Ownership rotates every year. The new owner reads the doc, asks questions, and updates the doc with new knowledge. Evals become team assets, not individual artifacts. The second fix is eval review meetings. Every quarter, the team reviews 20 percent of the eval suite. For each eval, someone presents why it exists, what it tests, and whether it is still valuable. Evals that cannot be defended are retired.

## Category Eight: Correlation Breakdown

Your eval scores used to predict production quality. High eval scores meant happy users. Low eval scores meant support tickets. Then the correlation broke. Eval scores stayed high. User satisfaction declined. Or eval scores dropped. User satisfaction held steady. The eval and reality diverged.

Correlation breakdown happens when the relationship between what you measure and what users care about changes. A legal document chatbot measures factual accuracy using exact-match grading. For two years, accuracy correlated with user retention. Then the user base shifted. Early users were paralegals who could verify answers and tolerate errors. New users are pro-se litigants with no legal training. They cannot detect errors. They trust the system completely. Accuracy remains critical, but now tone, readability, and confidence calibration matter just as much. Your eval measures accuracy. It does not measure tone or calibration. Eval scores stay high. Users churn because the system sounds authoritative even when wrong.

The second form is task complexity evolution. Users start with simple queries. Your eval tests simple queries. Eval scores correlate with satisfaction. Over time, users grow more sophisticated. They ask harder questions. Multi-step reasoning. Cross-document synthesis. Ambiguity resolution. Your eval still tests simple queries. It does not test the hard ones. The hard queries are where users now experience failures. Eval scores reflect easy query performance. User satisfaction reflects hard query performance. Correlation breaks.

The third form is metric substitution. You cannot measure the thing you actually care about, so you measure a proxy. You care about user trust. You measure response politeness. For a while, polite responses correlate with trust. Then users encounter a polite hallucination. Politeness without accuracy destroys trust faster than rudeness with accuracy. The proxy metric stops predicting the real metric. Eval scores improve. Trust declines.

Detection requires ongoing correlation analysis. Every month, you measure the relationship between eval scores and production quality metrics. User ratings, task success rate, session duration, retention — whatever you define as real-world quality. You calculate correlation coefficients. If correlation drops below a threshold or shifts directionally, your eval is losing predictive power.

The second signal is stratified correlation checks. You split users by segment, query by difficulty, task by type. You measure eval-production correlation within each stratum. If overall correlation looks strong but within-stratum correlation is weak, you have Simpson's paradox. The aggregate hides breakdowns in specific slices. The third signal is user research alignment. Every quarter, you run qualitative research on what users value. You compare those quality dimensions to what your evals measure. Gaps indicate future correlation breakdown.

Remediation is metric evolution based on changing user needs. When correlation weakens, you investigate why. User priorities shifted. Task distribution changed. Quality dimensions evolved. You update your evals to measure what now matters. You retire evals that measure what no longer matters. The second fix is composite metrics. Instead of relying on a single eval score, you combine multiple dimensions. Accuracy, fluency, calibration, relevance, safety. Composite scores are more robust to single-dimension correlation breakdown.

## Building Detection Systems for Silent Failures

Detecting silent failures requires monitoring the monitors. Your eval pipeline is infrastructure. Like all infrastructure, it degrades. The question is whether you detect degradation before it causes user-facing harm. Detection systems operate at three layers.

The first layer is pipeline health monitoring. Heartbeats confirm execution. Data freshness confirms results are written. Dependency tracking confirms versions are stable. Configuration hashing confirms settings are unchanged. Resource utilization tracking confirms the pipeline is not throttled or starved. This layer detects infrastructure rot and execution failures.

The second layer is eval quality monitoring. Threshold provenance reviews catch threshold drift. Distribution comparisons catch dataset staleness. Judge-human agreement tracking catches judge degradation. Coverage audits catch gaps. Eval-production correlation analysis catches metric gaming and correlation breakdown. This layer detects measurement validity failures.

The third layer is meta-evaluation. You eval your evals. You maintain a set of production incidents where quality degraded. For each incident, you check whether any existing eval should have caught it. If an eval should have fired but did not, you investigate why. If no eval covered the failure mode, you build one. This layer detects unknown unknowns — failure modes you did not anticipate.

Meta-evaluation also includes red team exercises. Once a quarter, someone on the team tries to break the eval system. Submit outputs that should fail but pass. Change infrastructure in ways that should trigger alerts but do not. Introduce subtle dataset staleness that should be caught but is not. If the red team succeeds, the detection system has gaps. You close them.

## The Failure Mode Review Cadence

Silent failures accumulate. A single failure is manageable. Ten simultaneous failures paralyze the team. The prevention mechanism is scheduled audits. Every month, you review one failure mode category. January is threshold drift month. You audit every threshold for provenance and current relevance. February is dataset staleness month. You measure distribution divergence and refresh coverage. March is judge degradation month. You recalibrate and retrain.

The review is not optional. It is scheduled with the same rigor as incident postmortems. Someone owns each review. They report findings. The team prioritizes fixes. Silent failures are prevented through routine inspection, not heroic firefighting. The eval system that survives production is the one that treats itself as production infrastructure — monitored, maintained, and continuously improved.

The catalog of silent failures is never complete. New models introduce new failure modes. New user behaviors create new gaps. New team members orphan old evals. The failure mode catalog is a living document. Every incident teaches you a new way evals break. You add it to the catalog. You build detection for it. The next time it happens, it is not silent anymore.

The difference between an eval system that earns trust and one that loses it is not perfection. It is visibility into imperfection. Silent failures erode trust because they make the system unpredictable. Detection systems restore trust by making degradation observable and fixable. The goal is not zero failures. The goal is zero silent failures. Every failure should announce itself before users notice it. That is the standard, and the only reason to read the next subchapter on how teams overfit to benchmarks and mistake score improvements for real progress.

