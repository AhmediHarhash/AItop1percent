# 2.1 â€” What Rule-Based Evals Can and Cannot Catch

The cheapest eval that catches a real problem is worth more than the most sophisticated eval that runs too late to matter. Rule-based evals cost nothing to run, execute in milliseconds, and catch entire categories of failure before you invoke a model for judgment or wait for human review. They are the foundation of every production eval pipeline because they filter out garbage before expensive systems waste time analyzing it.

But rule-based evals have hard limits. They can tell you if a response has the right structure. They cannot tell you if the response is correct. They can verify that a field contains a date. They cannot verify that the date is the right answer. They can confirm that a citation exists. They cannot confirm that the citation supports the claim. The gap between what rules can verify and what users actually care about is the most important boundary in eval design. Teams that understand this boundary build layered pipelines that catch 80% of failures for free and reserve expensive evals for the 20% that matter. Teams that ignore it either waste money running LLM judges on malformed outputs or ship broken responses because they assumed rules were enough.

## The Rule-Semantic Gap

**The Rule-Semantic Gap** is the difference between structural correctness and semantic correctness. A rule can tell you that a JSON response contains a field named "diagnosis." It cannot tell you if the diagnosis is medically accurate. A rule can verify that a summary is under 200 words. It cannot verify that the summary captured the key points. A rule can check that a citation follows the pattern "Author, Year" but it cannot check that the author actually wrote the paper or that the year is correct.

This gap exists because rules operate on syntax and models operate on meaning. Rules see tokens, delimiters, lengths, patterns. They have no access to the semantic content those tokens represent. A rule that checks for the presence of three supporting arguments will pass even if all three arguments contradict the conclusion. A rule that verifies a response contains no profanity will pass even if the response is subtly hostile or passive-aggressive. A rule that confirms a code snippet compiles will pass even if the code solves the wrong problem.

The gap is not a flaw in rules. It is the nature of deterministic checks. Rules are designed to be fast, cheap, and reliable. Semantic understanding requires context, world knowledge, reasoning. Rules give you none of that. They give you certainty about structure and speed about execution. That is their purpose.

## What Rules Excel At

Rules catch format violations, structural errors, and constraint breaches. If your system requires structured outputs, rules verify the structure. If your prompt specifies length limits, rules enforce the limits. If your schema defines required fields, rules confirm the fields exist. These checks run in single-digit milliseconds and catch 60 to 80% of model failures in production systems that use structured generation.

Rules excel at detecting missing fields. A healthcare system requires every patient recommendation to include a reasoning field, a confidence score, and a cited source. A rule checks for the presence of all three. If any are missing, the response is rejected instantly. No LLM judge required. No human reviewer required. The rule caught a malformed output before it entered the pipeline.

Rules excel at enforcing length constraints. A customer support system generates responses that must be under 500 characters for SMS delivery. A rule measures the character count. Responses over 500 characters are flagged immediately. The system tries a shorter generation or escalates to a human. The rule prevented a broken user experience before the message was sent.

Rules excel at pattern matching. A citation must follow a specific format. An email address must contain an at-sign and a domain. A phone number must match a regional format. A date must be ISO 8601 compliant. These are syntactic requirements. Rules verify them perfectly. A regex pattern that checks for valid email format will never hallucinate. It will never drift. It will never cost more than a few microseconds.

Rules excel at boundary enforcement. A confidence score must be between 0 and 1. A priority field must be one of four allowed values. A medical code must be exactly five digits. These are hard constraints. A rule enforces them deterministically. If the model outputs a confidence score of 1.3, the rule rejects it. If the priority field contains "super urgent" instead of "high," the rule catches it. No ambiguity. No edge cases. The rule defines correctness and checks for it.

## What Rules Cannot Catch

Rules cannot verify correctness. A customer support response passes every structural check, contains no banned language, stays under the length limit, includes the required fields, but gives the wrong answer to the customer's question. A rule has no way to detect this. It sees well-formed text. It cannot assess whether the text solves the user's problem.

Rules cannot detect hallucinations. A model generates a citation that matches the required format perfectly: "Smith, 2024, Journal of Applied Research." The citation looks valid. A rule confirms the structure is correct. But the paper does not exist. The model hallucinated a plausible-looking reference. A rule cannot catch this without external verification. It has no access to a database of real papers. It only knows what a valid citation looks like syntactically.

Rules cannot assess appropriateness. A legal brief contains all required sections, meets the length requirement, and includes citations in the correct format. But the tone is too casual for a formal court filing. Or the response cites case law from the wrong jurisdiction. Or the argument is legally sound but strategically weak. These are semantic problems. A rule cannot detect them because they require understanding of legal norms, jurisdictional rules, and argumentative strategy.

Rules cannot measure semantic coherence. A multi-paragraph response passes all structural checks. Each sentence is grammatically correct. But the paragraphs contradict each other. The introduction promises one thing, the body delivers another, the conclusion summarizes a third. A rule that checks paragraph count and sentence structure will pass the response. A human or LLM judge reading for coherence will fail it.

Rules cannot verify factual accuracy. A financial summary includes the required fields: revenue, expenses, net income. The format is correct. The numbers are plausible. But the math is wrong. Revenue minus expenses does not equal the stated net income. A rule could catch this if you write a specific arithmetic verification rule, but most rule-based systems do not. And even if you write that rule, the system still cannot verify whether the revenue number itself is accurate. It can only check internal consistency.

## The Layered Defense Model

The right approach is not to choose between rules and semantic evals. It is to layer them. Rules run first. They are fast, free, and catch the obvious failures. Semantic evals run second. They are slower, more expensive, and catch the subtle failures. Human review runs third for the cases where automation is insufficient.

A fintech company generating trade summaries runs rule-based evals on every output. The rules check for required fields: trade ID, timestamp, counterparty, asset, quantity, price. The rules verify that timestamps are valid ISO 8601 dates. The rules confirm that quantities and prices are positive numbers. The rules check that the summary is between 50 and 500 words. These checks run in under five milliseconds per response. They catch 70% of model failures: missing fields, malformed dates, negative prices, responses that are too short or too long.

The outputs that pass the rules move to the next layer: LLM-based semantic checks. The LLM judge verifies that the summary accurately reflects the trade details, that the tone is appropriately formal, that no sensitive information leaked. This layer catches hallucinations, factual errors, and tone problems that rules cannot detect. It runs on 30% of responses instead of 100% because rules already filtered the rest. The cost savings are significant.

The outputs that pass both layers enter production. Responses that fail either layer are logged, analyzed, and used to improve the system. Over time, patterns emerge. If rules consistently catch the same failure mode, the team adjusts the prompt or the model config to prevent it upstream. If the LLM judge consistently flags a specific semantic issue, the team considers whether a rule could catch it faster. The pipeline evolves. But the structure remains: rules first, semantics second.

## When to Skip Rules

There are tasks where rule-based evals add no value. If the output is purely creative, has no structural requirements, and is evaluated entirely on subjective quality, rules have nothing to verify. A storytelling application that generates fiction has no required fields, no length constraints, no format requirements. Users judge quality by engagement, emotional resonance, narrative coherence. Rules cannot measure any of that. The eval pipeline skips straight to human or model-based review.

If the output format is so rigid that the model cannot produce an invalid response, rules are redundant. A system that uses structured generation with a strict JSON schema enforced at the API level produces outputs that are guaranteed to be valid JSON. Running a post-generation JSON validation rule adds no value. The model API already enforced the schema. The rule would never fail. It wastes compute.

If the semantic quality check is so cheap that it costs the same as a rule-based check, there is no reason to separate the layers. A classifier that detects toxic language costs fractions of a cent and runs in under 50 milliseconds. Running a rule-based profanity filter first saves almost nothing. Just run the classifier. The cost and latency are negligible.

## Designing Rules That Scale

Rules that scale are simple, fast, and maintainable. A rule that requires 500 lines of regex is fragile. A rule that takes 100 milliseconds to execute is too slow for high-throughput systems. A rule that fails on edge cases you did not anticipate is a maintenance burden. Good rules are obvious, cheap, and resilient.

The best rules check for the presence or absence of required elements. Does the response contain the field? Does the response omit the banned phrase? These are binary checks. They run instantly. They rarely break.

The second-best rules enforce numeric or length constraints. Is the value between zero and one? Is the text under 500 characters? These require minimal computation. They are deterministic. They generalize well.

The worst rules attempt to verify complex patterns or domain-specific logic. A rule that checks whether a legal citation is correctly formatted for Bluebook style requires deep knowledge of citation rules, exception handling, and edge cases. It is brittle. It breaks when citation formats change. It requires constant updates. Unless citation correctness is mission-critical and you have the engineering resources to maintain the rule, skip it. Use a semantic eval instead.

## The Rule-First Principle

Every eval pipeline should start with rules unless there is a specific reason not to. Rules are cheap insurance. They catch malformed outputs before expensive systems analyze them. They reduce compute costs. They reduce latency. They reduce the surface area for semantic evals. A pipeline that skips rules wastes money and time on preventable failures.

The principle is simple: if a failure mode can be detected deterministically, detect it deterministically. Do not pay for an LLM judge to tell you that a required field is missing. Do not wait for a human reviewer to notice that a response is 5,000 words when the max is 500. Write a rule. Run it first. Filter the garbage. Then evaluate the rest.

The next layer is schema validation, the most common and most powerful rule-based eval in structured output systems.

