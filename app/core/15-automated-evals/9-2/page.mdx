# 9.2 — Calibration Loops: Comparing Automation to Humans

In November 2025, a legal research platform discovered that their automated eval system had been passing incorrect contract analyses for three months. The automation scored citation accuracy, legal reasoning quality, and completeness. Pass rates held steady at 91 percent through August, September, and October. The team trusted the automation. In November, a customer reported that several contract summaries cited cases that had been overturned. The team ran a manual audit. Human lawyers reviewed 500 examples the automation had passed. 38 percent contained errors the automation missed — outdated case law, misapplied precedents, incomplete risk analysis. The automation had drifted. It was calibrated in May when the team built it. By November, the legal knowledge base had updated, case law had evolved, and the automation's understanding of correctness no longer matched human legal judgment. The drift cost the company $280,000 in customer credits and eighteen months of customer trust.

The failure was not that the automation drifted. Drift is inevitable. The failure was that the team had no mechanism to detect drift before customers did. They built an eval system with no calibration loop — no process to continuously compare automation to human judgment and measure agreement. The automation ran in isolation, producing scores that became increasingly disconnected from reality.

## The Human Anchor for Calibration

Automated eval systems measure consistency, speed, and coverage at scale. Human reviewers measure correctness, alignment with current standards, and judgment that adapts to context. Automation without human calibration is a system measuring itself against itself. It can tell you whether today's outputs are consistent with yesterday's scores, but it cannot tell you whether those scores still represent quality.

The human anchor is the reference point that grounds automation in reality. You maintain a standing sample of production outputs that human reviewers evaluate according to current quality standards. The automation also evaluates the same sample. You measure agreement. When agreement stays high — above 90 percent, above 85 percent, whatever threshold your team has validated as operationally acceptable — the automation is still calibrated. When agreement drops below threshold, drift has occurred. The automation needs recalibration, or the humans need recalibration, or both.

The human anchor is not a one-time validation. It is a continuous comparison. You run it weekly, monthly, or quarterly depending on how fast your system changes. Each run produces an agreement score. You track agreement over time. Stable agreement means stable calibration. Declining agreement means drift. A sudden drop — agreement falls by more than five percentage points in one measurement period — signals a triggering event: model update, distribution shift, or ground truth change. A gradual decline — agreement drops one or two points per month — signals slow drift from accumulated small changes.

The anchor set must represent current production. If your production distribution is 60 percent customer support queries, 30 percent product questions, and 10 percent billing issues, your anchor set should match those proportions. If your production traffic is 70 percent English, 20 percent Spanish, and 10 percent French, your anchor set should match. If production includes edge cases — ambiguous queries, multi-turn conversations, queries that reference recent events — the anchor set must include them too. An anchor set that overrepresents easy cases or underrepresents recent distribution shifts will show falsely high agreement. The automation appears calibrated because you are only measuring it on the parts of the distribution where it still works.

## Measuring Automation-Human Agreement

Agreement is not a single number. It is a set of metrics that together tell you where calibration holds and where it breaks. The simplest agreement metric is overall pass-fail concordance. The human says pass, automation says pass: agreement. Human says fail, automation says fail: agreement. Human says pass, automation says fail: disagreement. Human says fail, automation says pass: disagreement. If 450 out of 500 examples agree, concordance is 90 percent.

Overall concordance hides directional bias. An automation that is overly strict fails examples humans would pass. An automation that is overly lenient passes examples humans would fail. Both produce the same concordance score, but they have different operational consequences. Overly strict automation creates false alerts — engineers investigate non-issues, product quality appears worse than it is, teams waste time tuning prompts to satisfy a miscalibrated metric. Overly lenient automation creates missed alerts — real quality problems go undetected, users encounter failures the eval system did not catch, silent degradation accumulates until a major incident forces discovery.

You measure directional agreement separately. Precision: of the examples automation passes, what percentage do humans also pass? Recall: of the examples humans pass, what percentage does automation also pass? A system with 95 percent precision and 70 percent recall is overly strict — it rarely passes bad examples, but it fails many good ones. A system with 70 percent precision and 95 percent recall is overly lenient — it catches most good examples, but it also passes many bad ones. Both have 82 percent overall concordance, but they need different corrections. Overly strict automation needs threshold loosening. Overly lenient automation needs threshold tightening or more sophisticated rules.

Agreement also varies by segment. You measure it for each query type, each language, each user cohort, each feature area. Automation might maintain 92 percent agreement on product questions but drift to 78 percent agreement on policy questions. It might stay calibrated for English but drift for Spanish. Overall agreement across all segments might be 88 percent — acceptable — but the drift in specific segments means the automation is unreliable where it matters most. Segmented agreement metrics surface this. You track agreement per segment over time. Segments where agreement declines faster than the overall trend are the first places to investigate and the first candidates for recalibration.

## Calibration Datasets

The anchor set used for calibration is a dataset with special requirements. It must be current — examples drawn from recent production traffic, not from six months ago. It must be diverse — covering the full range of production scenarios, not just the most common cases. It must be human-reviewed — every example has a ground truth label from a human reviewer using current quality standards. It must be stable enough to measure trends — you cannot compare agreement in March to agreement in June if the anchor set changed completely between measurements.

A typical calibration dataset is 500 to 2,000 examples, reviewed monthly or quarterly. The size depends on your production volume, your tolerance for statistical noise, and your human review budget. 500 examples gives you reasonable statistical confidence for overall agreement but limited ability to measure per-segment agreement. 2,000 examples lets you measure agreement per segment with confidence. Smaller datasets are cheaper to review but noisier. Larger datasets are more precise but more expensive.

The dataset refreshes partially each cycle. You retire the oldest 20 percent of examples and add new examples from recent production. This keeps the anchor set current without completely changing it every cycle. If you completely replace the anchor set each time, you cannot distinguish between real calibration changes and noise from dataset differences. If you never refresh the anchor set, it becomes stale and no longer represents production. Partial refresh balances continuity with currency.

The examples in the calibration dataset are selected deliberately, not randomly. Random sampling underrepresents rare but important cases — adversarial inputs, edge cases, recent product changes. You use stratified sampling. You define strata: query type, language, user segment, model version, feature area. You sample proportionally from each stratum. If 5 percent of production queries are adversarial, 5 percent of the calibration dataset is adversarial. If 8 percent involve multi-turn conversations, 8 percent of the dataset does too. Stratified sampling ensures the calibration dataset represents reality, not just the bulk of traffic.

Human reviewers label the calibration dataset using documented criteria. The criteria must match current quality standards, not the standards from when the eval system was built. If business priorities shifted from conciseness to completeness, the labeling criteria must reflect that. If ground truth changed — product features updated, policies changed, new markets launched — the criteria must reflect current ground truth. Reviewers are trained on the current criteria before each labeling cycle. You measure inter-rater agreement among reviewers. If two reviewers label the same 100 examples and agree on 88 of them, inter-rater agreement is 88 percent. If inter-rater agreement is below 85 percent, the labeling criteria are ambiguous or reviewers need more training. You cannot use human labels as a calibration anchor if the humans themselves are not consistent.

## The Feedback Loop

Measuring agreement is not the goal. The goal is using agreement measurements to maintain calibration. This requires a feedback loop: measure agreement, detect drift, decide whether to recalibrate, execute recalibration, validate that recalibration restored agreement.

The decision to recalibrate depends on the magnitude of drift and the cost of drift versus the cost of recalibration. If agreement drops from 92 percent to 90 percent, that is a two-point decline. Is that operationally significant? If your system handles 100,000 queries per day, two percentage points means 2,000 additional disagreements per day between automation and human judgment. If those disagreements lead to false alerts that waste engineering time, or missed alerts that let quality problems reach users, the cost is real. If the disagreements occur on low-stakes queries where the difference between pass and fail does not matter, the cost is negligible. The threshold for triggering recalibration is not universal. It depends on your stakes, your volume, and your tolerance for calibration error.

A typical threshold is five percentage points. Agreement drops below 85 percent: recalibrate. Agreement drops by more than five points in a single cycle: investigate immediately, recalibrate if the drop is confirmed. Agreement stays between 85 and 90 percent for three consecutive cycles: plan recalibration within the next quarter. Agreement holds above 90 percent: no action required, continue monitoring.

Recalibration has multiple forms. The lightest form is threshold adjustment. Your automation uses a threshold to convert scores into pass-fail decisions. The threshold was set to 0.82 when the system launched. Agreement measurements show the automation is now too strict — it fails examples humans pass. You lower the threshold to 0.78. Agreement improves. This takes hours, not weeks. The next form is rule updates. Your automation uses heuristics — response length, citation count, keyword presence. Those rules were defined six months ago. Ground truth has changed. You update the rules to match current standards. This takes days. The heaviest form is retraining. Your automation uses a model-graded eval where a Claude Opus 4 model scores outputs. The scoring model was fine-tuned on examples from six months ago. You retrain it on recent examples labeled with current criteria. This takes weeks and costs thousands of dollars. You choose the lightest recalibration method that restores agreement to acceptable levels.

After recalibration, you validate. You re-measure agreement on the same calibration dataset. Agreement should improve. If it does not, recalibration failed — the adjustment was insufficient, or it corrected the wrong problem. You investigate further. If agreement improves but remains below threshold, you iterate — apply a second recalibration. If agreement improves above threshold, recalibration succeeded. You deploy the recalibrated automation to production, update documentation to reflect the new thresholds or rules, and continue monitoring.

## Recalibration Workflows

Recalibration is not ad hoc. It is a defined workflow with roles, timelines, and success criteria. The workflow starts when agreement drops below threshold or when a triggering event occurs — model update, product launch, user complaint spike. An engineer is assigned as recalibration owner. The owner investigates the cause of drift, identifies which component of the automation drifted, proposes a recalibration method, executes it, and validates the result.

Investigation starts by analyzing disagreement patterns. You review the examples where automation and humans disagree. You categorize the disagreements. Is the automation too strict on response length? Too lenient on citation accuracy? Failing on a specific query type? Missing a new failure mode that did not exist six months ago? The pattern tells you what to fix. If 80 percent of disagreements involve Spanish-language queries, the automation's handling of Spanish drifted. If 70 percent involve queries about a new product feature, the automation lacks knowledge of the feature. If disagreements are evenly distributed, the drift is systemic — the entire automation has decayed, not one specific component.

The recalibration method matches the root cause. Threshold drift: adjust thresholds. Rule staleness: update rules. Model drift: retrain the scoring model. Ground truth change: update reference data. Each method has a timeline. Threshold adjustments can be tested and deployed in a day. Rule updates take three to five days — you draft new rules, validate them on a sample, measure agreement, deploy. Model retraining takes two to four weeks — you collect recent examples, label them, train a new scoring model, validate it on held-out data, measure agreement, deploy.

Validation uses the same calibration dataset. You run the recalibrated automation on the dataset. You measure agreement. You compare before-recalibration agreement to after-recalibration agreement. Success is improvement above threshold. Partial success is improvement but still below threshold — you improved calibration but did not fully restore it. Failure is no improvement or degradation — recalibration made things worse. Partial success triggers a second iteration. Failure triggers a deeper investigation — the initial diagnosis was wrong, or the recalibration method was inappropriate.

Once validated, the recalibrated automation deploys to production. You monitor production metrics for one week. You expect to see changes — pass rates might shift, alert volume might change, latency might increase if the new automation is more complex. You compare post-recalibration metrics to pre-recalibration metrics and to expectations. If metrics align with expectations, recalibration succeeded. If metrics diverge — pass rates drop more than expected, alert volume spikes unexpectedly — you investigate whether recalibration introduced a new problem.

The question is not whether to build calibration loops, but how often to run them and at what scale. That depends on how fast your system changes and how much drift you can tolerate before it becomes operationally damaging.

