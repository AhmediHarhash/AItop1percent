# 6.1 — Evaluating Behavior, Not Just Text

Text quality is not task success. Your agent can produce perfect, grammatically flawless, contextually appropriate responses and still fail completely at the job you built it to do. The shift from evaluating what an agent says to evaluating what an agent does is the fundamental divide between testing language models and testing autonomous systems. Most teams cross this divide without realizing it, applying text-based eval patterns to behavioral problems, and wondering why their agents ship with perfect scores but catastrophic production failures.

## The Illusion of Output Quality

A customer support agent receives a complaint about a delayed refund. It generates a beautifully written apology, explains the refund timeline with empathy and clarity, offers a discount code for the inconvenience, and closes with reassuring language about how much the company values the customer's patience. Your output quality metrics score it 94 out of 100. The customer never receives their refund because the agent never issued the refund command. It talked about solving the problem. It did not solve the problem.

This is the core failure mode of text-only evaluation applied to agentic systems. Traditional eval measures fluency, coherence, tone, factual accuracy, adherence to templates. These dimensions matter for language generation tasks. They do not measure whether the agent completed the task it was assigned. An agent that writes perfect emails but never sends them, that generates correct SQL queries but never executes them, that identifies the right next step but never takes it — that agent fails in production no matter what your text-based eval suite reports.

Behavioral evaluation measures what the system did, not what it said it would do. Did it call the right function? Did it retrieve the correct information? Did it update the database record? Did it escalate to a human when appropriate? Did it complete the workflow from start to finish? These are observable actions with verifiable outcomes. Text quality is a component of behavioral success, not a proxy for it. When you evaluate behavior, you measure the gap between what the agent was supposed to accomplish and what it actually accomplished. That gap is the only metric that predicts production performance for autonomous systems.

## What Behavior Means in Practice

Behavioral evaluation tracks three categories of observable evidence: actions taken, state changes produced, and workflow progression. Each category requires different instrumentation and different verification methods.

Actions taken are the function calls, API requests, tool invocations, and external system interactions the agent executes during task completion. For a scheduling agent, this includes checking calendar availability, sending meeting invites, updating event records, and notifying attendees. For a data analysis agent, this includes querying databases, running statistical computations, generating visualizations, and exporting reports. Every action is logged with timestamp, parameters, outcome, and whether it succeeded or failed. The eval system replays these action logs against the task specification to determine whether the agent did what it was supposed to do.

State changes produced are the durable modifications the agent made to the world. A refund agent that calls the refund API changes the customer's account balance and the order status in the database. A document generation agent creates a new file with specific content. A notification agent sends an email that arrives in an inbox. These state changes persist after the agent finishes. Behavioral evals verify that the correct state changes occurred, that they occurred in the right sequence, and that they match the task requirements. Text quality does not capture this. You can only verify it by inspecting the actual state of the systems the agent interacted with.

Workflow progression tracks how the agent moved through the task from start to finish. Did it gather the required information before taking action? Did it validate inputs before committing changes? Did it handle errors and retry when appropriate? Did it escalate when it encountered conditions outside its operational boundary? Workflow evaluation measures the path the agent took, not just the destination. Two agents can both complete a task successfully — one by following the correct decision tree, the other by stumbling through trial and error until something worked. Both produce the same outcome. Only one followed the intended behavior. Workflow evals distinguish between them.

## The Agent Evaluation Challenge

Agentic systems introduce evaluation complexity that pure language models never face. A language model takes an input, produces an output, and stops. Evaluation is a single comparison: does this output meet the quality criteria? An agent takes an input, decides on a sequence of actions, executes those actions in a dynamic environment, observes the results, adjusts its plan, and continues until it believes the task is complete or it encounters a terminal condition. Evaluation must cover every decision point, every action, every observation, and the final outcome. The eval surface area is exponentially larger.

The environment matters in ways it never does for text generation. A language model produces the same output regardless of whether you evaluate it on Tuesday or Thursday. An agent's behavior depends on the state of every external system it interacts with. A customer support agent that checks order status behaves differently depending on what the order management system returns. A scheduling agent behaves differently depending on calendar availability. A data retrieval agent behaves differently depending on what records exist in the database at eval time. Behavioral evals must either control the environment completely — using mocked services, synthetic data, isolated test instances — or account for environmental variability in scoring.

Partial success is the norm, not the exception. A language model either generates a response or fails to generate one. An agent can complete 80 percent of a task, fail on a single step, and leave the workflow in an ambiguous state. It retrieved the customer's order history, identified the delayed refund, drafted an apology email, but failed to issue the refund command because the payment API returned an authentication error. Did the agent succeed? By text quality metrics, yes. By task completion metrics, no. By partial credit scoring, maybe 75 percent. Behavioral evals must define how to score incomplete workflows, and that definition changes based on task criticality, failure impact, and recovery options.

The ground truth for behavior is often harder to define than the ground truth for text. For a summarization task, the ground truth is a reference summary written by a human. For an agent task, the ground truth is a specification of what actions should occur under what conditions. These specifications are frequently incomplete. The task is "resolve the customer complaint." What does resolution mean? Issuing a refund? Sending an apology? Escalating to a supervisor? Offering store credit? All of the above depending on complaint type? Behavioral ground truth requires explicit task decomposition, decision trees that cover edge cases, and criteria for success that account for multiple valid paths to the same outcome.

## Behavioral Dimensions Worth Measuring

Every behavioral eval needs to measure correctness, efficiency, and safety. Correctness is whether the agent did the right thing. Efficiency is whether it did it in a reasonable way. Safety is whether it avoided doing the wrong thing.

Correctness starts with task completion: did the agent finish the job? Beyond that, it covers action accuracy — did it call the right functions with the right parameters? Information accuracy — did it retrieve and use the correct data? Decision accuracy — did it choose the appropriate path through the workflow? A travel booking agent that books the wrong flight completed the task incorrectly. An agent that books the right flight but uses an expired credit card completed the task with an action error. An agent that books the right flight with a valid card but selects the wrong passenger name completed the task with an information error. Each failure type requires different instrumentation.

Efficiency measures wasted work. An agent that takes twelve tool calls to accomplish something that should require three is inefficient. An agent that retrieves the same data four times instead of caching it is inefficient. An agent that tries five different approaches before finding the one that works is inefficient. Inefficiency costs money in API usage, increases latency, and often signals confusion in the agent's planning logic. You measure efficiency by comparing the agent's actual action sequence to the optimal sequence for that task. The gap tells you whether the agent is operating with the right decision-making model or fumbling through trial and error.

Safety is the absence of harmful actions. Did the agent avoid unauthorized data access? Did it respect rate limits? Did it refuse to execute requests that violated policy? Did it escalate when uncertain instead of guessing? Safety behavioral evals define a negative space — the set of things the agent should never do — and verify that the agent stayed out of that space. A financial assistant that exposes account numbers in a customer-facing message failed a safety behavioral test. An agent that deletes records without confirmation failed a safety test. An agent that ignores a circuit breaker and continues calling a failing API failed a safety test. These failures do not show up in text quality metrics. They only show up in behavioral traces.

## Why Text Evals Fail for Agents

Text evaluation assumes a single terminal output. Behavioral evaluation requires observing the entire execution trace. Text evaluation can be done by comparing strings. Behavioral evaluation requires replaying actions in a controlled environment, verifying state changes across multiple systems, and reasoning about whether the observed behavior matches the intended behavior. The infrastructure requirements are different. The ground truth requirements are different. The failure modes are different.

When you apply text-based eval to an agentic system, you measure whether the agent can explain what it plans to do, not whether it can do it. You catch fluency errors, tone violations, factual mistakes in generated text. You miss action selection errors, workflow logic failures, incomplete task execution, unsafe tool usage, and the entire class of problems that matter most for autonomous systems. Your eval suite becomes a measure of the agent's ability to narrate its intentions, not its ability to accomplish its goals.

The fix is not to add behavioral eval on top of text eval. The fix is to recognize that for agentic systems, behavior is the primary eval target and text quality is a secondary concern. You still measure whether the agent's messages are clear, professional, and accurate. But those measurements are subordinate to whether the agent completed the task, used the right tools, followed the correct workflow, and produced the intended state changes. Behavioral evaluation is not a type of eval. It is the eval framework for any system that takes actions in the world.

Task completion detection is where behavioral evaluation begins — defining what it means for an agent to finish a job, and building the instrumentation to verify that finish state with the same rigor you apply to verifying text quality.

