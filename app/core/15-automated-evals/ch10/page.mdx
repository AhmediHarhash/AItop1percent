# Chapter 10 — Metrics, Dashboards, and Reporting

Eval results are only useful if the right people see them at the right time. A 12% regression in factual accuracy buried in a log file does not block a bad deploy. A dashboard that shows aggregate pass rates hides the category where failures are concentrated. Metrics and dashboards are the interface between your eval system and the humans who make decisions. This chapter covers which metrics actually matter, how to slice results by task type and cohort, how to set alerting thresholds that catch problems without crying wolf, how to design dashboards for different audiences, and how to avoid Goodhart's Law — the tendency for metrics to become targets that get gamed instead of signals that inform.

---

- 10.1 — Eval Metrics That Actually Matter
- 10.2 — Pass Rate, Failure Rate, and Regression Detection
- 10.3 — Slicing by Task Type, Model, Tenant, and Cohort
- 10.4 — Trend Analysis and Historical Comparison
- 10.5 — Alerting Thresholds and Escalation Rules
- 10.6 — Dashboard Design for Different Audiences
- 10.7 — Eval Confidence Visualization
- 10.8 — Correlation Between Eval Metrics and User Outcomes
- 10.9 — Reporting Cadence and Stakeholder Communication
- 10.10 — Avoiding Metric Gaming and Goodhart's Law
- 10.11 — Eval Metrics as Deployment Gates

---

*The metric that blocks a deploy must be trusted. The metric that is ignored might as well not exist.*
