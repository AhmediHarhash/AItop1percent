# 3.9 — Heuristic Calibration Against Human Judgments

In October 2025, a fintech company deployed a composite heuristic eval pipeline for their customer support chatbot. They set thresholds based on engineering intuition: flag outputs scoring above 0.65 on the anomaly scale. Within two weeks, the escalation queue contained four thousand flagged responses and three human reviewers. The reviewers labeled the first five hundred: eighteen percent were genuine failures. Eighty-two percent were false positives. The heuristics were catching something, but not the thing the team cared about. They had optimized for heuristic sensitivity without calibrating to human judgment. By December, after proper calibration against labeled data, the false positive rate dropped to twenty-three percent and the reviewers were finding failures the team didn't even know existed. The difference was disciplined calibration against the ground truth that mattered: what humans considered deployment-worthy.

## Calibrating Thresholds Using Labeled Holdout Sets

Heuristic calibration starts with a labeled dataset of model outputs marked by humans as "safe" or "needs escalation." You need at least five hundred labeled examples to get stable threshold estimates. A thousand is better. The labels come from your existing human review process: outputs that reviewers approved, outputs they rejected, outputs they sent back for revision. You run every labeled output through your heuristic pipeline and log the composite anomaly score. Now you have pairs: human judgment and heuristic score. You plot the distribution. Safe outputs cluster at low scores. Escalation-worthy outputs cluster at high scores. The distributions overlap in the middle.

You sweep a threshold from 0.0 to 1.0. At each threshold value, you calculate how many safe outputs would be flagged, how many escalation-worthy outputs would be flagged, how many of each would pass. This gives you precision and recall at every threshold. Precision is the fraction of flagged outputs that humans also flagged. Recall is the fraction of human-flagged outputs that your heuristic caught. At threshold 0.30, you catch ninety-five percent of bad outputs but flag sixty percent of good ones. At threshold 0.80, you catch forty percent of bad outputs and flag five percent of good ones. You choose the threshold that meets your operating constraint. If your review team can handle five hundred escalations per day and you generate two thousand outputs per day, you need a threshold that flags twenty-five percent of outputs. You find the threshold value that hits that false positive rate while maximizing recall.

## Measuring Heuristic Precision and Recall

Precision and recall for heuristics mean something slightly different than for classifiers. Precision answers: when the heuristic flags an output, what's the probability a human would also flag it? Recall answers: when a human flags an output, what's the probability the heuristic caught it? High precision means your escalation queue is mostly real problems. High recall means you're not missing many failures. You cannot have both at one hundred percent with heuristics because heuristics are approximations. The best composite heuristic pipelines achieve precision in the seventy to eighty percent range and recall in the eighty to ninety percent range. That means twenty to thirty percent of escalations are false positives, and ten to twenty percent of genuine failures slip through. You accept this because the alternative is model-based eval on every output, which costs a hundred times more.

You measure precision and recall on a monthly basis using fresh labeled samples. Take the last two thousand outputs. Sample two hundred randomly. Have humans label them. Run them through your heuristics. Calculate precision and recall at your current threshold. If precision dropped below seventy percent, your threshold is too loose. Tighten it. If recall dropped below eighty percent, your threshold is too tight. Loosen it. The calibration adjustments are small and frequent. You might shift the threshold from 0.68 to 0.71, redeploy, measure again next month. The goal is maintaining stable performance as your model, your prompts, your user base, and your output distribution evolve. Heuristics degrade silently. Scheduled recalibration prevents the degradation from compounding.

## The Calibration Feedback Loop

Calibration is not a one-time setup. It's a continuous loop. Every flagged output that enters human review generates a new labeled data point. The reviewer marks it safe or escalation-worthy. That judgment feeds back into your calibration dataset. Every month, you retrain your threshold selection using the updated dataset. This feedback loop has two benefits. First, it adapts to evolving standards. What your team considered acceptable tone in June might be unacceptable in December as your brand voice tightens. The human labels reflect the current standard. The heuristic thresholds adapt. Second, it catches heuristic drift. If a heuristic that used to correlate strongly with human judgments stops correlating, you see it in the precision-recall curves and you can investigate.

The feedback loop fails if you don't maintain labeling discipline. If different reviewers apply inconsistent standards, your labeled data becomes noisy and your calibration unstable. If you only label escalated outputs and never sample from passed outputs, you introduce selection bias. The calibration dataset should include both flagged and unflagged outputs, labeled by reviewers with consistent training and clear rubrics. Some teams run weekly calibration sessions where three reviewers independently label the same fifty outputs, compare labels, discuss disagreements, and align on standards. That alignment produces the stable ground truth your heuristic calibration depends on.

## Scheduled Recalibration After Model or Prompt Changes

Any change to your model or prompt invalidates your heuristic calibration. When you switch from GPT-5 to Claude Opus 4.5, the output distribution shifts. When you update your system prompt to enforce stricter formatting, the length distribution changes. When you add few-shot examples to improve tone, the formality distribution changes. Your heuristic thresholds, tuned to the old distribution, now misfire. You need scheduled recalibration after every significant change. The recalibration protocol is simple: deploy the change, let it run for twenty-four to forty-eight hours, sample five hundred to one thousand outputs, have humans label them, recompute precision and recall, adjust thresholds, redeploy. The entire process takes two to four hours of human time and runs automatically once you've built the tooling.

Some teams couple heuristic recalibration with A/B testing. When testing a new prompt variant, they run both old and new prompts in production, route ten percent of traffic to the new variant, collect outputs from both, label a sample from each, and compute separate heuristic thresholds for each variant. If the new prompt improves quality but shifts the heuristic distribution, they deploy both the new prompt and the new thresholds simultaneously. This prevents the common failure mode where a team launches a better prompt, the heuristics flag it as anomalous because it's different from the baseline, and the escalation queue explodes with false positives.

## Detecting Heuristic Drift Over Time

Heuristic drift is the silent killer of automated eval pipelines. Your heuristics were calibrated in September. It's now February. Your model hasn't changed. Your prompt hasn't changed. But your user base shifted. You launched in a new region. Your users started asking different kinds of questions. The output distribution drifted. Your heuristics are still measuring the same things, but the relationship between heuristic scores and human judgments has weakened. You're flagging outputs that are fine and passing outputs that are bad. You don't notice because you're not measuring precision and recall anymore. You assumed the calibration was stable.

Drift detection requires continuous monitoring. You log the distribution of heuristic scores daily. You plot the median, the 75th percentile, the 95th percentile. If the median composite anomaly score was 0.32 for three months and suddenly jumps to 0.48, something changed. Maybe your users are asking more complex questions and your model is generating longer, more nuanced responses that score higher on length anomaly and hallucination markers even though they're correct. Maybe your model started generating more refusals because a policy change tightened acceptable use. The spike in anomaly scores doesn't tell you what changed, but it tells you to investigate. You sample outputs from the spike period, label them, recalculate precision and recall, and recalibrate if needed.

The best teams automate drift detection. They run a weekly job that compares the current week's heuristic score distribution to the trailing four-week baseline. If the Kolmogorov-Smirnov test statistic exceeds a threshold, indicating the distributions are significantly different, the system alerts the eval team. The alert triggers a recalibration review. This catches drift within days instead of months. The cost of running the test is trivial. The cost of missing drift is that your eval pipeline slowly loses accuracy until it's no longer trustworthy.

## The Gap Between Heuristic Scores and Human Perception

The fundamental limitation of heuristics is that they measure proxies, not the thing you care about. You care about whether the output is helpful, accurate, and appropriate. Heuristics measure whether it's the right length, uses the right tone, avoids certain patterns. Most of the time, the proxies correlate with the target. Sometimes they don't. A response can be perfectly formatted, appropriate tone, no PII, no hallucination markers, and still be factually wrong or contextually inappropriate. A response can trigger multiple heuristic flags and still be correct. The gap between heuristic scores and human judgment is irreducible.

You close the gap by layering evaluation methods. Heuristics are the first layer. They catch obvious failures fast and cheap. Model-based evals are the second layer. They run on the subset of outputs that heuristics flagged, measuring semantic correctness and reasoning quality. Human review is the third layer. It runs on the subset that model-based evals flagged or couldn't confidently score. This three-tier architecture exploits the economic logic of evaluation: run the cheapest method on everything, run the expensive methods on the small subset that needs them. Heuristics reduce the volume that hits model-based eval by seventy to eighty-five percent. Model-based eval reduces the volume that hits human review by another eighty to ninety percent. The compound filtering means only one to three percent of total outputs require human review, but those outputs get the scrutiny they need.

The calibration work you do on heuristics—measuring precision and recall, tuning thresholds, detecting drift—ensures that the first tier of filtering is doing its job correctly. If heuristics are miscalibrated, they either flood the downstream tiers with false positives or let genuine failures slip through entirely. Calibrated heuristics don't solve the evaluation problem alone, but they make the entire pipeline economically viable by ensuring that expensive methods are reserved for the cases where they add value.

Heuristic calibration is not a one-time science project. It's an operational discipline. You build the labeled dataset, you compute precision and recall, you set thresholds, you deploy, you monitor for drift, you recalibrate monthly, and you accept that heuristics are approximations that need continuous tuning to stay aligned with the human judgments that define quality in your domain. Next, you need to understand where heuristics fail entirely, because knowing the failure modes tells you when to escalate to model-based evaluation instead.

