# 15.1.7 — The Eval Trust Hierarchy: Tier A, B, and C Signals

Not all automated evaluations deserve equal trust. Teams treat every metric on their dashboard with the same confidence, every passing test as equally valid proof that the system works. This is a dangerous fiction. A rule-based eval that checks for the presence of required fields is fundamentally different from a heuristic that estimates tone quality. A validated judge model with 98% agreement against human labels is not the same as an exploratory metric you built yesterday to surface a trend. The moment you treat all signals equally, you lose the ability to distinguish between evidence you can stake a production deploy on and signals that merely suggest a direction worth investigating.

The trust you place in an evaluation should match the confidence you have in its accuracy. Not every eval can tell you whether to ship. Not every metric can block a deploy. The teams that scale automated evaluation successfully build a hierarchy of trust and make it explicit. They know which signals are Tier A — rock solid, human-validated, deployment-blocking. They know which are Tier B — reliable enough to inform decisions but not authoritative enough to block alone. They know which are Tier C — exploratory, directional, useful for discovery but never mistaken for proof. This hierarchy is not academic classification. It is operational discipline. It determines which evals gate your releases, which ones trigger alerts, and which ones sit in a dashboard for teams to interpret.

## Tier A: High-Confidence Signals That Gate Deploys

**Tier A evaluations** are the evals you trust enough to block a production release. They have earned this privilege through rigorous validation against human judgment, deterministic behavior, or structural guarantees that make false positives and false negatives rare. These are not guesses. These are measurements you would defend in a post-incident review.

Rule-based evals belong here when they check structural correctness. Did the model return valid JSON with all required fields? Did it produce a response under 500 tokens as specified? Did it avoid a list of 47 prohibited terms? These are binary checks with no ambiguity. If the rule says the output must contain a field named customer_id, and the output does not, the eval fails. There is no room for interpretation. The eval cannot drift. It cannot degrade. It cannot misfire unless the rule itself is wrong. This determinism makes rule-based evals the gold standard for gating structural requirements.

Validated judge models also qualify as Tier A when they have been calibrated against thousands of human labels and achieve agreement rates above 95%. You built a judge to evaluate whether customer support responses are empathetic. You tested it against 5,000 human-labeled examples. It agrees with human raters 97% of the time. You periodically re-validate it with fresh samples, and the agreement holds. This judge has earned Tier A status. It can block a deploy if empathy scores drop below threshold. The key is validation at scale and ongoing monitoring. A judge model is not Tier A just because it exists. It becomes Tier A when you have statistical proof that it mirrors human judgment consistently.

Ground-truth comparison evals reach Tier A when you have verified correct answers. You are evaluating a medical coding model. Every test case has a confirmed ICD-10 code assigned by a certified coder. The eval compares the model output to the verified code. Exact match or failure. No ambiguity. The ground truth is authoritative. This is Tier A because the answer is known with certainty, and the comparison is objective. If your ground truth is uncertain or your comparison allows for subjective interpretation, the eval drops to Tier B.

Tier A evals are the ones that appear in your release gate. If any Tier A eval fails, the deploy stops. No exceptions. No "let's look at it and decide." The entire point of Tier A is that you have pre-decided these signals are trustworthy enough to automate the stop decision. This does not mean they are infallible. It means you have done the work to make their error rate low enough that blocking on them causes fewer problems than shipping without them.

## Tier B: Medium-Confidence Signals That Inform Decisions

**Tier B evaluations** are reliable enough to inform decisions but not authoritative enough to block deploys automatically. They are heuristics, calibrated judge models with slightly lower agreement rates, or metrics based on patterns that work most of the time but not always. You trust them. You act on them. But you do not give them the power to stop a release without human review.

Heuristic-based evals live here. You built an eval that estimates response relevance by checking whether the model output contains at least two keywords from the user query and does not exceed 300 tokens. This works well in practice. It correlates with human ratings 85% of the time. But it is not perfect. Sometimes a highly relevant response uses synonyms and triggers a false negative. Sometimes an irrelevant response happens to include the keywords and passes. This heuristic is useful. It helps you spot trends, compare model versions, and catch obvious regressions. But you do not block a deploy on it alone. It is Tier B.

Judge models with moderate agreement rates also belong in Tier B. Your tone quality judge agrees with human raters 88% of the time. That is strong. That is useful. But it is not 95%. Twelve percent of the time, the judge disagrees with humans. If you block deploys on this judge, you will occasionally stop a perfectly good release or let a bad one through. So you use the judge to flag candidate releases that need human spot-checking. If the judge score drops five points, you do not auto-block — you trigger a review. A human looks at a sample of outputs. If the human agrees with the judge, you block. If the human disagrees, you investigate the judge. This is Tier B in action: automation that informs human judgment, not automation that replaces it.

Pattern-based anomaly detection often operates at Tier B. You track the distribution of response lengths, the frequency of refusals, the rate at which the model cites sources. When any of these metrics shifts significantly from baseline, you flag it. This does not mean something is wrong. It means something changed, and someone should look. A sudden drop in citation rate might indicate a regression in retrieval. Or it might indicate a shift in the types of queries users are asking. The metric cannot tell you which. It can only tell you to investigate. Tier B evals are sensors, not verdicts.

You can use Tier B evals to trigger alerts, prioritize review queues, and compare candidate models before final testing. You do not use them as the sole basis for a deploy-or-block decision. The difference between Tier A and Tier B is not that one is good and the other is bad. The difference is the acceptable error rate and the consequences of being wrong. Tier A evals have low enough error rates that you automate the decision. Tier B evals have higher error rates, so you automate the detection and leave the decision to humans.

## Tier C: Exploratory Signals for Discovery and Investigation

**Tier C evaluations** are exploratory. They are the metrics you are testing, the heuristics you just built, the judge models you have not yet validated, the signals you are using to investigate a hypothesis or surface a trend. They are valuable. They are necessary. But they are not authoritative. You do not block deploys on them. You do not trigger high-severity alerts on them. You use them to learn.

Newly created metrics start in Tier C by default. You suspect that responses with more than four sentences tend to lose user engagement. You build a metric that counts sentences per response and tracks it over time. You do not know yet whether this metric correlates with actual user behavior. You do not know if it generalizes across domains. You run it in parallel with Tier A and Tier B evals, and you watch. If the pattern holds over weeks, if you validate it against user feedback data, if it proves useful in distinguishing good outputs from bad, you might promote it to Tier B. Until then, it stays in Tier C.

Experimental judge models belong here until they are validated. You fine-tuned a new judge to evaluate factual accuracy. It looks promising in initial tests. But you have not yet run it against thousands of human labels. You have not calibrated it. You have not proven it generalizes to production traffic. You deploy it in shadow mode. It scores every output, but the scores do not affect any decision. You compare its ratings to human ratings over time. If it achieves consistent agreement, you promote it. If it does not, you iterate or retire it. Tier C is the sandbox where new evals prove themselves.

Correlation-based signals often live in Tier C permanently. You notice that outputs with high perplexity scores tend to receive lower user satisfaction ratings. This is an interesting correlation. It might help you spot problem areas. But perplexity is a proxy, not a measure of quality. A low-perplexity response can still be wrong. A high-perplexity response can still be exactly what the user needed. You track perplexity. You investigate spikes. But you do not treat it as proof of anything. It is a signal that suggests where to look, not evidence of what you will find.

Tier C evals appear in dashboards, in exploratory reports, in research notebooks. They do not appear in deploy gates. They do not trigger pages. They inform curiosity. They help teams ask better questions. When a Tier C metric shows an unexpected trend, the response is not to block the deploy. The response is to investigate whether the trend represents a real problem, whether it correlates with Tier A or Tier B signals, and whether it should be promoted or retired.

## Which Tier Can Block Deploys

The rule is simple: only Tier A evals can block a deploy without human review. Tier B evals can trigger a review, flag a release for closer inspection, or require human sign-off. Tier C evals cannot block anything. They inform investigation.

This discipline is what separates teams that scale automated evaluation successfully from teams that either move too slowly or ship broken models. If you treat Tier B and Tier C evals as deploy blockers, you create false bottlenecks. You block releases because a heuristic fired or an experimental metric spiked, even though the underlying quality is fine. Your team learns to ignore the signals or to game them. Trust erodes. If you treat all evals as Tier A — if you assume every metric is equally valid — you lose the ability to prioritize. You spend equal time investigating a structural failure and a perplexity blip. Your team burns out chasing noise.

The trust hierarchy makes the trade-offs explicit. Tier A evals have earned the right to stop a deploy because their error rates are low and their validation is ongoing. Tier B evals have earned the right to demand attention but not to make the decision alone. Tier C evals have earned the right to be tracked but not to block anything. This hierarchy is not static. Evals can be promoted. A Tier C metric that proves its value and achieves validation becomes Tier B. A Tier B judge that reaches 95% agreement and maintains it under ongoing calibration becomes Tier A. An eval can also be demoted. A Tier A judge whose agreement rate degrades drops to Tier B until you fix it. The hierarchy reflects current confidence, not permanent status.

## Communicating the Hierarchy to Stakeholders

The trust hierarchy only works if everyone who consumes eval results understands it. Your dashboard shows fifty metrics. Product, Legal, and Engineering all look at the same numbers. If they do not know which ones are Tier A and which ones are Tier C, they will treat every red metric as a crisis and every green metric as proof of safety. This creates chaos.

Label every eval in your dashboard with its tier. Use visual cues. Tier A metrics appear at the top, highlighted, flagged as deploy-blocking. Tier B metrics appear next, marked as informational or review-triggering. Tier C metrics appear in a separate section, labeled experimental or exploratory. When a Tier C metric drops, the dashboard does not scream red alert. It shows a yellow note: "Investigate." When a Tier A eval fails, the dashboard does scream: "Deploy blocked."

Document the criteria for each tier. Explain what it takes for an eval to reach Tier A: validation against human labels, agreement thresholds, deterministic correctness, ongoing calibration. Explain what keeps an eval in Tier B: useful but not authoritative, correlates with quality but does not define it, requires human interpretation. Explain what defines Tier C: new, unvalidated, exploratory, tracking a hypothesis. This documentation is not buried in a wiki. It lives in the dashboard, in the deploy gate tooling, in the onboarding materials for every person who reviews eval results.

The trust hierarchy is not complexity. It is clarity. It tells your team which signals to act on immediately, which signals to investigate thoughtfully, and which signals to watch without overreacting. It prevents the two failure modes that kill automated evaluation at scale: treating everything as critical or treating nothing as authoritative. The next subchapter covers how to ensure that automation, no matter which tier it operates at, remains anchored to human truth.
