# 7.1 â€” Safety Evaluation as a First-Class Eval Family

Safety evaluation is not a secondary concern or a compliance checkbox. It is a first-class eval family that sits alongside functionality, latency, and cost as a non-negotiable component of every automated pipeline. When agents book appointments, process refunds, or answer medical questions, the downside risk of a single harmful output can eclipse months of product value. Safety evals are the system that prevents that outcome.

The distinction matters because first-class means blocking. A failed safety eval stops deployment the same way a failed functionality eval does. It triggers alerts with the same urgency. It gets the same engineering attention, the same infrastructure investment, and the same executive visibility. Teams that treat safety as an afterthought ship products that harm users, violate regulations, and destroy brand trust. Teams that treat safety as first-class build products users can rely on even when the model behaves unpredictably.

## Why Safety Evals Are Not Optional

In 2024, safety evaluation was considered best practice for consumer-facing products and nice-to-have for internal tools. In 2026, that calculus has changed. The EU AI Act classifies many agent systems as high-risk, requiring documented safety controls before deployment. The UK's AI Safety Institute has published safety benchmarks that regulators reference in enforcement actions. California's AB 2013 mandates safety disclosures for AI systems that interact with vulnerable populations. These are not theoretical compliance exercises. They are legal requirements backed by fines measured in millions of dollars and executive liability.

But the regulatory pressure is the smaller driver. The real reason safety evals became non-optional is that agents now take actions with real-world consequences. A customer service agent that processes a refund incorrectly costs money. An agent that tells a user to harm themselves costs lives. A medical triage agent that dismisses chest pain symptoms creates liability that no insurance policy covers. The model does not understand the difference between a helpful answer and a dangerous one. The safety eval system is the only layer that catches the dangerous output before it reaches a user.

The shift happened when the first wave of agent lawsuits hit in 2025. A mental health chatbot suggested self-harm methods to a teenager. A legal advice agent told a tenant to ignore an eviction notice, leading to homelessness. A financial planning agent recommended tax strategies that constituted fraud. In each case, the company argued the model output was an edge case, a rare failure, an unforeseeable error. The courts disagreed. The standard they applied was simple: if you deployed a system that interacts with users, you are responsible for what it says. Safety evals are how you demonstrate you met that responsibility.

## Safety as Blocking vs Informing

Not every safety eval blocks deployment. The distinction between blocking and informing is critical to pipeline design. A blocking safety eval prevents a release if it fails. An informing safety eval logs the failure, creates a ticket, and allows deployment to proceed with documented risk. The choice depends on the severity of the risk and the maturity of your safety controls.

Blocking safety evals apply to high-severity risks: content that could cause physical harm, content that violates legal prohibitions, content that creates uninsurable liability. A medical agent that hallucinates dosage recommendations must fail a blocking safety eval before deployment. A customer service agent that occasionally uses mildly unprofessional language can fail an informing safety eval and still ship, with the failure tracked for future improvement. The key is that the blocking decision is made once, documented in your safety policy, and enforced automatically by the pipeline. No one has discretion to override a blocking failure without a formal exception process that involves legal and senior leadership.

Informing safety evals serve a different purpose. They catch lower-severity issues, track trends over time, and create signal for safety improvement without halting velocity. If your agent occasionally produces outputs that are technically safe but culturally insensitive, you want to know about it. You want that signal in your dashboard. You want the trend visible to product managers. But you do not want to block every release until the issue is resolved, because the fix might take weeks of data collection and model retraining. Informing evals let you ship while you improve.

The architecture difference is straightforward. A blocking safety eval runs in the same pipeline stage as functionality evals. If it fails, the build fails, the deployment does not happen, and the on-call engineer gets paged. An informing safety eval runs in parallel, writes results to your observability system, and creates a task in your backlog. The build continues. The difference in how the result is handled determines whether the eval is blocking or informing, not the eval content itself. The same toxic content classifier can be blocking for a teen mental health app and informing for an internal code documentation tool.

## Safety Eval Coverage Requirements

Coverage is the percentage of possible interactions your safety evals can evaluate. A safety eval with 40 percent coverage misses 60 percent of potential harms. That is not acceptable for any system users depend on. The standard in 2026 is 95 percent coverage for all user-facing outputs, measured as the percentage of prompt types, output types, and user demographics represented in your safety eval dataset.

Achieving 95 percent coverage requires deliberate design. You cannot test only happy-path prompts and claim safety. You must test adversarial prompts, edge-case prompts, jailbreak attempts, multi-turn manipulation, and demographic variation. A safety eval that only tests English prompts has zero coverage for your Spanish-speaking users. A safety eval that only tests polite prompts has zero coverage for users who are angry, desperate, or confused. A safety eval that only tests single-turn interactions has zero coverage for agents that maintain conversation state across dozens of turns.

The coverage requirement also applies across harm types. Your safety eval suite must cover toxic content, harmful instructions, self-harm content, illegal activity, misinformation in high-stakes domains, privacy leaks, bias and discrimination, and domain-specific risks like medical or legal advice. Missing even one category creates a gap an adversarial user will find. The mental health chatbot that passes every toxic content test but never checks for self-harm content is not safe. The financial advice agent that passes every hallucination test but never checks for market manipulation is not safe. Coverage means all harm types, not just the ones you thought to test.

The measurement is simple. Divide the number of prompt-output pairs in your safety eval dataset by the number of distinct interaction patterns your product supports. If your agent handles 200 distinct task types and your safety eval dataset contains 10,000 examples, you have 50 examples per task type on average. That is barely sufficient. If your coverage is below 30 examples per task type, you have a coverage gap. If any task type has zero safety eval examples, you have a critical gap that must be closed before the next release.

## The Safety-Functionality Balance

Safety evals and functionality evals are not in opposition. They serve different purposes and must coexist in the same pipeline without creating deadlock. The failure mode is a pipeline that blocks every release because either the safety evals are too strict or the functionality evals are too loose. The solution is clarity about what each eval family measures and a documented process for resolving conflicts.

Functionality evals answer the question: does the system do what users need? Safety evals answer the question: does the system avoid doing what users must never experience? These are orthogonal. A model can pass every functionality eval and fail every safety eval if it completes tasks correctly but includes toxic language. A model can pass every safety eval and fail every functionality eval if it refuses all requests to avoid risk. The goal is a model that passes both, and the pipeline must be designed to make that outcome achievable.

The conflict arises when safety and functionality point in opposite directions. A customer service agent trained to be helpful might over-promise refunds to satisfy users, failing cost control evals. The same agent retrained to be cautious might refuse legitimate refund requests, failing functionality evals. The correct answer is not to weaken one eval to satisfy the other. The correct answer is to recognize that the model is not yet good enough and the training data or prompt architecture needs improvement. The pipeline should block deployment and surface the conflict to the team, not allow a release that fails either eval family.

The balance mechanism is threshold tuning per eval family, not per eval. If your safety evals require 98 percent pass rate and your functionality evals require 92 percent pass rate, that is a coherent policy. If your safety evals have a 60 percent pass threshold because they are too strict, your pipeline is misconfigured. The thresholds should reflect your actual risk tolerance and quality bar, and they should be enforced uniformly across all evals in that family. The balance comes from designing evals that measure the right thing, not from lowering the bar until everything passes.

## Safety Evals in the Pipeline Architecture

Safety evals integrate into your automated pipeline at three points: pre-commit, pre-deployment, and post-deployment monitoring. Each serves a different purpose and operates on different data. Pre-commit safety evals run on every code or prompt change before merge, catching regressions early. Pre-deployment safety evals run on the full model before release, validating that the integrated system is safe. Post-deployment safety evals run continuously in production, detecting drift and adversarial use.

Pre-commit safety evals are fast, focused, and regression-oriented. They run on a small dataset, usually 500 to 2,000 examples, and complete in under five minutes. The goal is not comprehensive safety validation. The goal is ensuring that the change you just made did not break something that was working. If your toxic content eval passed yesterday and fails today, the diff you are about to merge introduced a regression. The pipeline blocks the merge, alerts you, and forces a fix before the code enters main. This is the same logic as unit tests for functionality, applied to safety.

Pre-deployment safety evals are slow, comprehensive, and release-gating. They run on your full safety eval dataset, often 50,000 to 200,000 examples, and take 30 minutes to two hours depending on model size and eval complexity. They execute after all pre-commit checks pass and before the release candidate is promoted to production. If any blocking safety eval fails, the release does not happen. The results are logged, the failure is investigated, and the model is retrained or the prompt is revised. There is no option to ship a model that fails pre-deployment safety evals unless you go through a formal exception process that documents the risk and gets executive sign-off.

Post-deployment safety evals are continuous, sample-based, and drift-focused. They run on live traffic, evaluating a percentage of production outputs in real time. The percentage depends on your traffic volume and eval cost tolerance, but the standard is at least one percent of all outputs for high-risk systems and 0.1 percent for lower-risk systems. These evals detect two failure modes that pre-deployment evals miss: adversarial users actively trying to exploit your system, and gradual drift where the model behavior changes over weeks due to context or data shifts. When post-deployment safety evals detect a pattern of failures, they trigger an alert, and you investigate whether the model needs retraining, the prompt needs hardening, or a user is attempting coordinated abuse.

The architecture requirement is that all three layers share the same eval logic. A toxic content eval in pre-commit should use the same classifier, the same threshold, and the same dataset schema as the toxic content eval in pre-deployment and post-deployment. If the logic diverges, you create a gap where something passes pre-commit but fails pre-deployment, or passes pre-deployment but fails in production. Shared eval logic means shared code, shared configuration, and shared versioning. When you update the toxic content model, all three pipeline stages pick up the new version on the next run.

The final requirement is visibility. Every safety eval result must be logged to your observability system with enough metadata to reconstruct what was tested, what passed, what failed, and why. The metadata includes model version, prompt version, eval dataset version, timestamp, and the specific examples that failed. This is not optional. When a regulator asks how you validated safety before deploying the update that harmed a user, the answer is not "we ran the evals." The answer is a timestamped log showing exactly what was tested, what passed, and what thresholds were enforced. Safety evals are evidence, and evidence must be durable, queryable, and auditable.

---

*The foundation is clear: safety is first-class, blocking when necessary, comprehensive in coverage, balanced with functionality, and integrated at every pipeline stage. The next question is what specific harms to test for, starting with the most common and most damaging category: toxic, harmful, and self-harm content.*
