# 2.4 — Citation and Reference Presence Checks

The model's response contained three citations. Two of them pointed to real URLs. The third was a perfectly formatted reference to a paper that did not exist. The retrieval system had returned seven documents. The model cited three. Four went completely unused, even though they contained the answer the user needed. All three citations used the correct markdown format. The response passed format validation. It failed reality.

**Citation validation** operates at two levels: structural compliance and content presence. The first level checks whether citations exist, whether they follow the required format, and whether reference counts meet minimum thresholds. The second level validates that citations point to content the system actually retrieved, that URLs resolve to real destinations, and that reference text matches source material. Rule-based evals handle the structural layer. They catch the absence of citations, malformed reference syntax, and count violations before any model-based judge ever sees the output. They do not catch hallucinated citations that look structurally perfect. That requires retrieval-aware validation, which belongs in Tier 1. But the structural layer catches enough errors that it pays for itself in the first week.

## The Citation Presence Requirement

The most basic check: does the response contain any citations at all. For systems where citations are mandatory, this is a pass-fail gate. A response without citations fails immediately, regardless of content quality. The check scans for citation markers—numbered references in brackets, markdown links, footnote syntax, or custom citation formats your system defines. It counts occurrences. If the count is zero and the requirement is non-zero, the eval fails.

This sounds trivial until you run it against production traffic. Models drop citations for three reasons. First, the prompt does not emphasize citation requirements strongly enough, and the model treats them as optional suggestions. Second, the model generates citations in an early draft, then removes them during self-correction because it judges the tone too formal. Third, the model runs out of output tokens before writing the reference section. All three failure modes produce responses that look complete to a human skimming quickly. None of them contain citations. The presence check catches all three.

The check runs in milliseconds. It requires no API calls, no embedding models, no external dependencies. It is a regular expression or a simple string search. It runs on every response before any other eval. If it fails, the system can reject the response immediately, retry with a stronger prompt, or escalate to human review. The cost is effectively zero. The value is that downstream evals never waste computation analyzing uncited responses that were already doomed.

## Format Compliance for Citation Syntax

Citations follow formats: numbered brackets, markdown links, APA style, custom reference schemas. Format validation checks that every citation in the response matches the required pattern. If the system expects numbered references like bracket-one, bracket-two, the validator scans for that syntax. If it expects markdown links with source IDs, it checks for that structure. If it expects footnote markers followed by a reference list, it verifies both halves exist.

Format violations fall into categories. The model uses the wrong syntax entirely—parentheses instead of brackets, plain URLs instead of markdown links. The model mixes formats within a single response—some citations numbered, others not. The model starts numbering at the wrong index—bracket-zero instead of bracket-one, or bracket-three with no bracket-one or bracket-two. The model reuses the same citation marker for different sources. Each violation type has a distinct signature. A format compliance eval detects all of them with deterministic pattern matching.

The validator also checks reference list completeness. If the response contains bracket-one through bracket-five in the body text, the reference section must contain five entries. If it contains three, the eval fails. If it contains seven, the eval flags the mismatch. The rule is simple: every citation marker in the body must have a corresponding entry in the reference list, and every entry in the reference list should be cited at least once in the body. Orphaned references—entries that appear in the list but are never cited—indicate either model confusion or incomplete editing. The presence of uncited references does not always fail the eval, but it triggers a warning that downstream judges can use.

## Reference Count Thresholds

Some tasks require minimum citation counts. A research summary must cite at least three sources. A comparative analysis must reference all options being compared. A legal compliance check must cite every relevant regulation. The count threshold eval simply counts citations and compares against the minimum. If the response cites two sources and the minimum is three, it fails.

Count thresholds prevent superficial responses. Without a minimum, models default to the easiest path: cite one source, write a short answer, finish. The source might be relevant. The answer might be correct. But the response ignores the rest of the retrieved context, which often contains contradictory information, updates, or edge cases the first source missed. A minimum citation requirement forces the model to engage with multiple sources. It does not guarantee synthesis quality—that requires a model-based judge—but it guarantees the attempt was made.

The threshold is task-specific. A simple factual question might require one citation. A synthesis task might require five. A comprehensive report might require ten. The threshold should match the number of documents the retrieval system typically returns and the depth of analysis the task demands. Setting it too low allows shallow responses. Setting it too high forces the model to cite irrelevant sources just to meet the quota. The correct threshold is the number of citations a human expert would naturally include when answering the same question with the same retrieved context.

## URL Validation and Resolution

Citations that include URLs require validation beyond format checks. The URL must be syntactically valid—properly formatted with scheme, domain, and path. It must be semantically valid—it must point to a resource that exists. The first check is deterministic and instant. The second requires making HTTP HEAD requests to verify the URL resolves without a 404 or 500 error.

Syntactic validation catches malformed URLs: missing schemes, spaces in domains, invalid characters, broken encodings. A regular expression or URL parser library handles this. If the citation contains something that is not a valid URL, the eval fails immediately. If the URL is syntactically correct, the validator can optionally attempt resolution. This adds latency—HEAD requests take tens to hundreds of milliseconds depending on the target server—but it catches hallucinated citations where the model invents a plausible-looking URL that points nowhere.

Resolution validation has trade-offs. It requires network access, which complicates offline eval runs. It depends on external servers being available, which introduces flakiness. It adds latency that might exceed acceptable thresholds for real-time evals. The decision depends on risk tolerance. For high-stakes citation systems—medical information, legal research, financial analysis—resolution validation is worth the cost. For lower-risk applications, syntactic validation plus retrieval-aware checks might be sufficient. The rule-based layer handles syntax. Retrieval-aware evals, covered in Chapter 3, verify that cited URLs match retrieved documents.

## Distinguishing Structure from Truth

Rule-based citation validation detects structural failures. It does not detect hallucinations. A citation that points to a real URL, uses correct format, and meets count thresholds can still be completely wrong for the question. The document might exist but have nothing to do with the topic. The reference might be real but misquoted. The citation might point to a source the retrieval system never provided. All of these pass rule-based validation.

The boundary is clear: rule-based evals verify that citations exist and follow format requirements. They do not verify that citations are relevant, accurate, or grounded in retrieved context. That requires comparing citations against the retrieval results, which is a Tier 1 eval—cheap but not free, because it requires loading and parsing retrieval outputs. The rule-based layer runs first. It rejects responses with zero citations, malformed syntax, or missing reference lists. Everything that passes moves to retrieval-aware validation, which checks grounding. Everything that passes that moves to model-based judges, which assess synthesis quality.

This layered approach prevents waste. Retrieval-aware evals do not run on responses with no citations. Model-based judges do not run on responses with hallucinated citations. Rule-based citation checks are the first filter. They cost nothing and catch enough errors that the downstream layers process 20 to 40 percent fewer responses, depending on how well your prompts enforce citation requirements.

When citations are mandatory and rule-based validation is the first gate, you discover prompt weaknesses within hours of deployment. If 15 percent of responses fail citation presence checks, your prompt is not emphasizing citations clearly enough. If 8 percent fail format checks, your prompt is not specifying syntax precisely enough. If 12 percent fail count thresholds, your prompt is not making minimum requirements explicit. Every structural failure is a signal that the prompt needs refinement. Rule-based citation validation does not just protect production. It trains your prompts to get better.

The next layer of deterministic validation handles tool calls—structured outputs where format errors do not just degrade quality, they break functionality entirely.
