# 4.2 — Judge Model Selection: Capability vs Cost vs Latency

Most teams default to the biggest, most capable model when building LLM-as-judge evaluation. The reasoning feels sound: evaluation is critical, so use the best model available. This is wrong. The best judge is not the most capable model. The best judge is the least expensive model that reliably distinguishes good outputs from bad outputs for your specific task. Choosing GPT-5 to judge simple summarization tasks when GPT-5-nano would produce equivalent rankings wastes 95% of your evaluation budget and adds latency that blocks CI pipelines. Choosing Claude Haiku 4.5 to judge nuanced legal reasoning when only Claude Opus 4.5 has the required domain knowledge produces unreliable scores that mislead every downstream decision. Judge model selection is an optimization problem with three variables: capability, cost, and latency. Getting it wrong makes evaluation either prohibitively expensive or silently inaccurate.

## Capability Requirements for Judges

The judge model must be capable enough to execute the rubric. This is not the same as being capable enough to perform the task being evaluated. A model that cannot write a high-quality legal contract summary can still judge whether someone else's summary is accurate, complete, and clear — if the rubric defines those terms precisely and the model can follow complex instructions. The capability floor for judging is lower than the capability floor for generation, but it is not zero.

In practice, judge capability requirements break into three tiers. Tier one tasks require only instruction-following and basic reasoning: judging whether a response is polite, whether it is formatted correctly, whether it mentions required topics. Any frontier model from 2024 onward can handle these. GPT-5-nano, Claude Haiku 4.5, Gemini 3 Flash, Llama 4 Scout — all are sufficient. Tier two tasks require deeper semantic understanding: judging whether a summary captures the main points, whether an explanation is clear to a non-expert, whether a response actually answers the question asked. These require stronger reasoning models. GPT-5, Claude Sonnet 4.5, Gemini 3 Pro, Llama 4 Maverick are appropriate. Tier three tasks require domain expertise or complex multi-step reasoning: judging whether a legal argument is sound, whether a medical explanation is clinically accurate, whether a proof is logically valid. These require the most capable models available: GPT-5.2, Claude Opus 4.5, Gemini 3 Deep Think.

A retail company running product description quality evaluation in late 2025 made the mistake of using Llama 4 Scout to judge persuasiveness and brand voice alignment. The rubric defined five dimensions, each with detailed criteria. The judge produced scores that were internally consistent — the same description judged twice received the same score — but when humans audited a sample, agreement was 61%. The judge was following instructions, but it lacked the nuance to distinguish "persuasive with subtle urgency" from "persuasive with aggressive urgency." When they upgraded to GPT-5, human agreement jumped to 87%. The cost per judgment increased from 0.03 cents to 0.8 cents. Over 200,000 evaluations per week, that was $160 versus $1,600. But the $160 version was measuring noise.

The capability test is simple. Take 100 outputs that span the quality range. Have a human label them with the rubric. Have the candidate judge model score them. Measure agreement. If agreement is above 85%, the model is capable enough. If agreement is between 70% and 85%, the model might be adequate if the task is low-stakes and you accept higher error rates. Below 70%, the model is not qualified. This test must be repeated for every new task type. A model that judges summarization well may fail at judging tone, or technical accuracy, or logical coherence.

## Cost Per Judgment

Evaluation cost scales with volume and model size. A system that evaluates 10,000 outputs per day, every day, with a judge that costs one cent per call, spends $36,500 per year on evaluation. The same system using a judge that costs 0.05 cents per call spends $1,825. If both judges produce equivalent accuracy, the expensive one is unjustifiable. The challenge is that model pricing in 2026 spans three orders of magnitude. GPT-5.2 costs approximately 15 times more per token than GPT-5, which costs approximately 20 times more than GPT-5-nano. Claude Opus 4.5 costs approximately 12 times more than Claude Sonnet 4.5, which costs approximately 18 times more than Claude Haiku 4.5.

Judge cost is determined by input tokens and output tokens. The input is the judge prompt, the rubric, the task context, and the output being evaluated. The output is the score and justification. For a typical judge evaluating a 300-token model output with a 400-token rubric and 200-token prompt, total input is approximately 900 tokens. If the judge produces a 100-token justification and score, total cost is 900 input tokens plus 100 output tokens. At GPT-5 pricing in early 2026, that is roughly 0.7 cents. At GPT-5-nano pricing, it is 0.04 cents. At Claude Opus 4.5 pricing, it is 1.1 cents.

For systems evaluating millions of outputs, these differences compound into budget-defining costs. A customer support AI evaluating 5 million responses per month with a GPT-5 judge spends $35,000 on evaluation. With a GPT-5-nano judge, the cost is $2,000. The question is whether the quality difference justifies $33,000 per month. Sometimes it does. Often it does not.

The mistake is choosing the judge model once, globally, for all evaluation tasks. Teams that use GPT-5 to judge every eval because "we need high quality" are spending flagship-model budget on tasks that a small model would handle identically. The pattern that works is tiered judging: use the smallest model that meets the capability threshold for each task type. Format validation and safety checks use nano-sized models. Semantic quality and coherence use mid-tier models. Domain expertise and complex reasoning use top-tier models. An evaluation system with ten different judge types, each optimized for cost, can run at one-fifth the cost of a system using one expensive judge for everything.

## Latency Constraints

Evaluation latency determines where LLM judges can run in your pipeline. Offline batch evaluation has no latency constraints — if a judge takes five seconds per output, you run it overnight on 100,000 examples and review results in the morning. Real-time production evaluation has hard constraints — if the judge takes five seconds, it blocks the user-facing response by five seconds, which is unacceptable for most consumer applications. CI pipeline evaluation has soft constraints — if the judge takes one second per output and you are evaluating 500 outputs per commit, that is 500 seconds, which is slow but tolerable if it runs in parallel.

Judge latency is a function of model size, input length, output length, and provider infrastructure. Smaller models return faster. Shorter prompts return faster. Shorter requested outputs return faster. Providers with optimized inference infrastructure return faster. In early 2026, GPT-5-nano typically returns judgments in 400 to 800 milliseconds. GPT-5 returns in 1.2 to 2.5 seconds. Claude Opus 4.5 returns in 2 to 4 seconds. Gemini 3 Flash returns in 300 to 600 milliseconds. These are averages for typical judge prompts — actual latency varies with provider load and geographic region.

For offline batch evaluation, latency is irrelevant compared to cost and capability. Use the best model you can afford, run it asynchronously, parallelize across outputs. For CI pipelines, latency becomes a constraint when total eval time exceeds developer patience. A ten-minute eval suite is tolerable. A thirty-minute suite causes developers to skip evaluation or batch commits, which defeats the purpose of continuous validation. If your judge latency is 2 seconds per output and you are evaluating 1,000 outputs serially, that is 2,000 seconds — 33 minutes. Parallelizing across 20 workers drops it to 100 seconds, which is acceptable. Switching to a faster judge model drops it further.

For production use, LLM judges in the critical path are rare and dangerous. The use case that works is async evaluation: the production model returns a response immediately, and a background judge evaluates it after delivery. The judge score is logged for monitoring and used to trigger alerts if quality drops, but it does not block the user. The use case that fails is synchronous evaluation: the production response waits for the judge before returning to the user. This doubles user-facing latency, adds a second failure mode, and provides no value unless the judge result changes what the user sees — which implies the judge is vetoing bad outputs in real time, a pattern that works only when the production model is unreliable enough that judge override is necessary, which means the production model should not be in production.

## The Tiered Judge Pattern

The optimal judge architecture is not one model for all tasks. It is a portfolio of models, each assigned to the task tier where it is most cost-effective. Tier one tasks — format checks, keyword presence, basic safety — use the smallest models. Tier two tasks — semantic quality, coherence, relevance — use mid-tier models. Tier three tasks — domain accuracy, logical soundness, complex reasoning — use top-tier models. Within a single evaluation run, different outputs are judged by different models based on task requirements.

A legal tech company running contract analysis evaluation in late 2025 implemented this pattern with three judge tiers. Tier one used Claude Haiku 4.5 to check that extracted clauses were complete sentences, contained no obvious formatting errors, and did not include personally identifiable information from the wrong contract section. Cost: 0.02 cents per judgment. Tier two used GPT-5 to evaluate whether extracted clauses were relevant to the specified contract topic and whether summaries captured key obligations. Cost: 0.6 cents per judgment. Tier three used Claude Opus 4.5 to evaluate whether legal reasoning in generated explanations was sound and whether risk assessments aligned with case law. Cost: 1.2 cents per judgment.

All three tiers ran on every output. The total cost per evaluation was 1.82 cents. If they had used Claude Opus 4.5 for all three tiers, cost would have been 3.6 cents — nearly double. If they had used Claude Haiku 4.5 for all three tiers, cost would have been 0.06 cents, but tier three accuracy would have dropped from 89% to 68%, rendering the legal reasoning scores useless. The tiered approach balanced cost and capability by matching judge strength to task difficulty.

The implementation is straightforward. Define evaluation dimensions. Assess the capability required for each. Assign the least expensive model that meets the requirement. Run all judges in parallel. Aggregate results. The complexity is in the initial calibration — determining which models are capable enough for which dimensions — and in the ongoing monitoring, because judge model updates can shift the capability-cost frontier and require re-optimization.

## When Smaller Models Are Sufficient

The counterintuitive truth is that for many evaluation tasks, the smallest available models produce scores nearly identical to the largest models. This is especially true for tasks with objective criteria that do not require interpretation. Judging whether a response includes a disclaimer, whether it stays under a word count limit, whether it uses second person pronouns, whether it avoids a prohibited topic — these are instruction-following tasks, not reasoning tasks. GPT-5-nano can execute them as reliably as GPT-5.2.

Even for subjective dimensions, smaller models often suffice when the rubric is well-designed. A rubric that defines "clear" as "uses common words, avoids jargon, explains technical terms on first use, and structures ideas with transitions" gives a small model enough guidance to judge clarity without requiring deep linguistic intuition. A rubric that defines "clear" as "easy to understand" leaves interpretation to the model, which means only a highly capable model will judge consistently.

The test is empirical. Run the same evaluation with a small model and a large model. Measure correlation between their scores. If correlation is above 0.9, the small model is sufficient. If correlation is between 0.8 and 0.9, the small model is probably sufficient unless you need high precision. Below 0.8, the models are measuring different things, and you need the larger model's judgment.

A fintech company evaluating fraud alert clarity ran this test in early 2026. They compared GPT-5-nano and GPT-5.1 as judges on 2,000 alerts. Correlation was 0.94. The small model disagreed with the large model on 6% of outputs. Human review of the disagreements found that the large model was correct 72% of the time — but the small model's errors were not systematically biased and did not change the aggregate quality distribution. For their use case — monitoring quality trends over time — the small model was adequate, and it cost 18 times less.

## When You Need the Biggest Model

Complex reasoning, domain expertise, and ambiguity resolution require top-tier models. Legal judgment requires understanding case law. Medical judgment requires clinical knowledge. Code review requires understanding language semantics and common vulnerability patterns. Mathematical proof evaluation requires logical reasoning across multiple steps. These tasks cannot be reduced to simple rubrics. They require models trained on domain-specific data and capable of multi-step inference.

A healthcare company evaluating patient education content in mid-2025 tried using GPT-5 to judge clinical accuracy. The rubric defined accuracy as "medically correct, cites current treatment guidelines, avoids outdated recommendations." The judge scored 15% of outputs as inaccurate. Human review by clinical staff found that 31% were actually inaccurate — the judge missed half the errors. When they switched to Claude Opus 4.5, which demonstrated stronger biomedical reasoning in their pre-deployment testing, the judge identified 28% as inaccurate. Human agreement rose to 91%. The cost per judgment tripled, but the alternative was shipping incorrect medical information to patients.

The signal that you need a bigger judge model is low agreement with human evaluation, not low scores. A judge that gives low scores but agrees with humans is working. A judge that gives high scores that humans contradict is under-qualified. When human-judge agreement falls below 80%, either the rubric is poorly designed or the judge lacks capability. Fix the rubric first. If agreement remains low, upgrade the judge.

## The Model Family Bias Question

Judge model selection includes a bias variable that has no correct answer: should the judge model come from the same family as the production model? Using a GPT-5 judge to evaluate GPT-5 outputs introduces self-preference bias — the judge rates outputs from its own family higher than equivalent outputs from Claude or Gemini. Using a Claude judge to evaluate GPT outputs introduces the inverse bias. Both biases are measurable and significant, typically shifting scores by 5 to 12 percentage points in ambiguous cases.

The mitigation is to use judges from a different family than the production model — if your production model is GPT-5, use Claude or Gemini judges. This reduces self-preference bias but introduces a different risk: the judge may penalize stylistic differences that do not reflect quality differences. GPT models tend to produce more structured, formal outputs. Claude models tend to produce more conversational, narrative outputs. A Claude judge evaluating GPT outputs might score them lower for being "too formal," even when formality is appropriate for the task.

The approach that works at scale is ensemble judging with mixed families. Use three judges: one from the same family as the production model, two from different families. Average their scores. This balances self-preference bias with cross-family perspective. It triples cost and latency, so it is only appropriate for high-stakes decisions or for calibrating single-judge systems. Most teams use single judges from a different family and accept the bias as a known limitation, monitored through periodic human audits.

Judge model selection is not a one-time decision. Model capabilities evolve, pricing changes, new models launch. A judge that was optimal in January may be suboptimal by June. The pattern that works is quarterly re-evaluation: test new models, re-measure agreement with humans, re-calculate cost, adjust assignments. Treat judge model selection as an ongoing optimization, not a fixed architecture. The next question is what the judge is actually instructed to measure.

---

**Next: 4.3 — Rubric Design for LLM Judges**
