# 8.7 — Eval Configuration Management

The pipeline ran clean in staging. Every test passed. The team deployed to production with confidence. Two hours later, alerts fired — the eval pipeline was rejecting valid outputs at 40% rate. The issue was not the code. The code was identical. The issue was configuration. Staging used a toxicity threshold of 0.85. Production used 0.65. No one knew the threshold had been changed three weeks earlier by a different team. The configuration file lived in a Git repo, but the production deployment still pointed to an old S3 bucket with a manually edited copy. When the on-call engineer found the mismatch, they fixed it in five minutes. Finding it took four hours.

Configuration is where eval pipelines break silently. You version your code. You review your code. You test your code. Configuration files get edited directly in production, copied between environments without inspection, and diverge across regions until someone notices the results no longer match. The pipeline that works perfectly in one environment fails mysteriously in another, and the difference is a single boolean flag or a threshold that shifted by 0.1. Managing eval configuration with the same rigor you apply to application code is not optional. It is the difference between a pipeline you trust and a pipeline you debug every week.

## Configuration as Code

Every aspect of your eval pipeline behavior should be defined in version-controlled configuration files. Model names, API endpoints, timeout values, retry counts, quality thresholds, sampling rates, feature flags — all of it belongs in config files stored in Git, not environment variables set manually on deployment, not hardcoded constants scattered across Python files, not JSON blobs uploaded to S3 by hand. Configuration as code means the entire pipeline behavior is reproducible from a single commit hash. You can see what changed between yesterday's pipeline and today's. You can roll back a config change as easily as you roll back application code. You can diff production config against staging config in thirty seconds.

The simplest implementation is a YAML or TOML file per environment. Your staging config lives at `config/staging.yaml`. Your production config lives at `config/production.yaml`. The pipeline reads the appropriate file based on an environment variable set at deploy time. Each config file contains every parameter the pipeline needs — model identifiers, quality thresholds, timeout durations, retry limits, sampling percentages, feature flags. When someone wants to change a threshold, they open a pull request against the config file. The change goes through code review. The change deploys through your normal CI/CD pipeline. You have a Git history showing who changed what and when.

Teams that skip this step end up with configuration spread across environment variables, command-line arguments, database rows, S3 objects, and hardcoded defaults. No single source of truth exists. Reproducing a pipeline run from two weeks ago becomes archaeology. Comparing production behavior to staging behavior requires checking six different locations. Configuration drift becomes inevitable, and drift creates silent failures that surface only when results stop matching expectations.

## Environment-Specific Overrides

Some parameters must differ between environments. Production runs against GPT-5 because cost matters less than quality. Staging runs against GPT-5-mini because speed matters more than perfection. Production samples 100% of outputs. Staging samples 10% to reduce cost. Production sends alerts to PagerDuty. Staging sends alerts to a Slack channel the team checks twice a day. These differences are legitimate and necessary. The anti-pattern is managing them inconsistently — environment variables for some overrides, config files for others, command-line flags for a third category.

The correct pattern is a base configuration file that defines defaults, plus environment-specific override files that modify only what needs to change. Your `config/base.yaml` file contains everything common across environments. Your `config/production.yaml` file imports base and overrides specific keys — model name, sampling rate, alert destinations. Your `config/staging.yaml` file imports base and overrides different keys. At deploy time, the pipeline loads base first, then merges the environment-specific overrides on top. The result is a single resolved configuration object that reflects the final behavior for that environment.

This pattern makes differences explicit and auditable. You can see at a glance which parameters differ between staging and production. You can verify that every environment uses the same toxicity threshold or that production intentionally uses a stricter one. You catch errors where someone changed a value in staging but forgot to propagate it to production, because the base config did not change and the override was not added. The configuration system enforces consistency by default and requires explicit action to diverge.

## Config Validation and Schemas

A typo in a configuration file can disable an entire eval dimension. You meant to set `enabled: true`. You wrote `enabled: tru`. The YAML parser treats it as a string. The pipeline interprets any non-boolean value as false. The toxicity check stops running. No error appears. The pipeline continues. Outputs that should fail toxicity checks pass through to production. The problem surfaces three days later when a user reports offensive content. The logs show the toxicity evaluator returned no results. The reason is a one-character typo in a config file deployed on Monday.

Configuration files need validation schemas enforced at pipeline startup. Before the pipeline processes a single output, it loads the config file and validates it against a schema that defines required fields, allowed types, permitted value ranges, and dependencies between parameters. If the config specifies `model: gpt-5-mini` but `max_tokens` is set to 200000, the validation step fails — GPT-5-mini does not support that context length. If a threshold is set to 1.2, validation fails — thresholds must be between 0 and 1. If a required field is missing or misspelled, validation fails immediately with a clear error message.

The best tools for this are Pydantic for Python pipelines or JSON Schema for language-agnostic validation. You define a schema that describes every config parameter — name, type, default value, minimum, maximum, allowed enum values, whether it is required. The pipeline loads the config file, parses it, and validates it against the schema before executing any evaluation logic. If validation fails, the pipeline exits with a non-zero status code and logs the specific validation error. The deployment fails before any traffic is processed. You never deploy a broken config to production because the CI/CD pipeline catches it during the build or deploy phase.

## Config Drift Detection

Configuration drift is when the config in Git no longer matches the config running in production. Someone edited a file on the production server to fix an urgent issue and never committed the change back to Git. Someone deployed from a local branch that never merged to main. Someone manually uploaded a config file to S3 to test a hypothesis and forgot to revert it. The production system now runs on configuration that does not exist in version control. When the next deployment happens, the manual change gets overwritten, and production behavior reverts to a state the team did not intend.

Detecting drift requires periodic comparison of the running config against the source of truth in Git. The simplest implementation is a cron job that runs every hour, queries the production environment for its current configuration values, fetches the config file from the Git branch that production should be running, and diffs the two. If any difference exists, the job sends an alert to Slack or PagerDuty. The alert includes the specific keys that diverged and their current versus expected values. The on-call engineer investigates. Either the manual change was legitimate and needs to be committed to Git, or it was accidental and needs to be reverted.

More sophisticated systems enforce immutability. The config file is baked into the Docker image or Lambda deployment package at build time. The running pipeline cannot modify it. The only way to change configuration is to build a new image, which requires committing to Git and passing through CI/CD. This approach eliminates drift entirely but sacrifices the ability to make emergency config changes without a full deploy cycle. The right trade-off depends on your deployment velocity and risk tolerance. High-stakes pipelines benefit from immutability. Experimental pipelines benefit from flexibility.

## Infrastructure as Code for Eval Pipelines

Configuration management extends beyond parameter files to the infrastructure that runs the pipeline. The Kubernetes deployment manifest that specifies CPU and memory limits. The IAM roles that grant access to S3 buckets. The CloudWatch alarms that trigger on error rates. The load balancer rules that route traffic to pipeline instances. All of this is configuration, and all of it should be version-controlled, reviewed, and deployed through code.

The standard tools are Terraform, Pulumi, or AWS CloudFormation for cloud resources, and Helm or Kustomize for Kubernetes manifests. You define the entire eval pipeline infrastructure in declarative configuration files stored in Git. The CI/CD pipeline applies these files on every merge to main. Changes to infrastructure go through the same pull request and code review process as changes to application logic. You can see when someone increased the memory limit on the eval workers. You can see when someone added a new IAM permission. You can roll back infrastructure changes the same way you roll back code changes.

Teams that manage infrastructure through the AWS console, manual `kubectl apply` commands, or scripts run from laptops lose the ability to reproduce their production environment reliably. When disaster recovery is needed, no one knows which configurations were applied manually versus which were defined in code. When scaling to a new region, no one knows which resources need to be replicated. Infrastructure as code makes the eval pipeline environment reproducible, auditable, and portable. You can spin up an identical staging environment in twenty minutes because every piece of infrastructure is defined in version-controlled files.

## Config Change Review Process

Not all config changes carry the same risk. Changing a Slack webhook URL has no impact on eval results. Changing a quality threshold from 0.90 to 0.85 could let through 15% more failures. Config changes need a review process that matches their risk profile. High-risk changes — anything that affects pass/fail decisions, model selection, or sampling rates — require review from at least two people, one of whom is a domain expert who understands the eval strategy. Low-risk changes — logging verbosity, alert destinations, cosmetic parameters — can merge with a single approval.

The way to enforce this is through CODEOWNERS in GitHub or equivalent mechanisms in GitLab or Bitbucket. Your `config/production.yaml` file has a CODEOWNERS entry that requires approval from the eval platform team lead. Your `config/thresholds.yaml` file requires approval from both the platform lead and a domain expert from Trust and Safety or Product. Changes to infrastructure definitions require approval from someone on the SRE or infrastructure team. The pull request cannot merge until all required reviewers approve. This prevents junior engineers from accidentally changing a toxicity threshold and deploying it to production without oversight.

Document the rationale for every high-risk config change in the pull request description. Explain why the threshold changed. Link to the analysis that justified the new value. Describe the expected impact on pass rates and false positive rates. This documentation becomes part of the Git history. Six months later, when someone asks why production uses a threshold of 0.85 instead of 0.90, the answer is in the commit message and pull request discussion.

Managing configuration with the same discipline you apply to application code transforms the eval pipeline from a fragile system that breaks in subtle ways to a reliable system where behavior is predictable, reproducible, and auditable. The next step is building multi-stage pipelines that optimize cost by running cheap evaluations first and expensive evaluations only when necessary.

---

*Next: 8.8 — Multi-Stage Eval Pipelines: Cheap to Expensive*
