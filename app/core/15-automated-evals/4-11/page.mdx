# 4.11 — Judge Calibration Against Human Labels

In August 2025, a customer support platform relied on GPT-5 to score agent response quality across 40,000 conversations daily. The team had validated the judge against 2,000 human-labeled examples in March and achieved 84 percent agreement. By August, the agreement had dropped to 71 percent. The judge was still running. The scores were still green. But the judge had drifted — it was rewarding responses that human reviewers now considered too formal, penalizing responses that were appropriately casual, and missing tone problems that human raters flagged consistently. The pipeline produced five months of unreliable scores before anyone noticed. The fix required re-labeling 3,000 recent examples, rewriting the judge prompt, and rebuilding trust with the team that had been optimizing against bad signals.

## Why Judges Drift and Calibration Degrades

An LLM judge is a model applied to an evaluation task. The model updates. The task evolves. The judge's reliability relative to human judgment does not stay constant.

Model updates change behavior. When OpenAI releases GPT-5.1 or Anthropic releases Claude Opus 4.6, the new model checkpoint has different weights, different training data, different post-training. A judge prompt that worked perfectly on GPT-5 may produce systematically different scores on GPT-5.1. The provider does not guarantee eval stability across versions — they optimize for user-facing task performance, not for scoring consistency. If you upgrade the model powering your judge without re-validating it, you risk silent drift.

Human preferences evolve. What your support team considered a high-quality response in January may be different from what they consider high-quality in June. The company's tone guidelines change. Customer expectations shift. Regulatory requirements update. If your judge was calibrated against human labels from six months ago, it may no longer align with current human judgment, even if the model itself hasn't changed.

The task distribution shifts. If your judge was calibrated on a dataset of product support questions and you expand into technical troubleshooting questions, the judge may not generalize. It learned to score helpfulness in one domain and is now being asked to score helpfulness in a different domain where the signals are different. The calibration dataset no longer represents the production distribution.

Calibration is not a launch-time activity. It is an ongoing process. You validate the judge at launch. You re-validate when the model updates, when the task changes, when human feedback indicates drift, and on a recurring schedule even if nothing has visibly changed.

## Calibrating Judges Using Human-Labeled Data

Judge calibration starts with a labeled dataset: examples of the task the judge is evaluating, each labeled by human raters with the correct score or judgment. For a customer service quality task, this dataset might be 1,000 to 5,000 real conversations, each scored by trained human reviewers on dimensions like helpfulness, tone, accuracy, and policy compliance. For a summarization task, it might be 2,000 documents with human-written summaries and human ratings of candidate summaries.

You run the judge on the same examples the humans labeled. The judge produces scores. You compare the judge's scores to the human labels. The degree of agreement tells you how well the judge's outputs correlate with human judgment.

If the judge is being used for binary decisions — pass or fail, acceptable or unacceptable — you measure agreement as the percentage of examples where the judge's binary decision matches the human's binary decision. If humans marked 800 examples as acceptable and 200 as unacceptable, and the judge agrees on 920 of those 1,000, the agreement rate is 92 percent. This is a simple, interpretable metric, but it does not account for chance agreement.

If the task is scored on a scale — 1 to 5, 1 to 10 — you can measure agreement using correlation. Pearson correlation measures linear agreement between the judge's scores and the human scores. Spearman correlation measures rank agreement — whether the judge ranks outputs in the same order as humans do, even if the absolute scores differ. High correlation — above 0.8 — indicates strong alignment. Correlation below 0.6 indicates the judge is unreliable.

Neither simple agreement nor correlation captures the full picture. A judge might agree with humans 90 percent of the time but systematically fail on the 10 percent of hard cases that matter most. A judge might have high average correlation but poor agreement on edge cases. Calibration requires both summary metrics and error analysis on the cases where the judge disagrees with humans.

## Cohen's Kappa and Agreement Metrics

Cohen's kappa is a more rigorous measure of agreement than simple percentage agreement because it adjusts for chance. If two raters both label 90 percent of examples as "acceptable" and 10 percent as "unacceptable," they will agree by chance alone roughly 82 percent of the time. Kappa measures how much better than chance the agreement is.

Kappa ranges from -1 to 1. A kappa of 1 means perfect agreement. A kappa of 0 means agreement is no better than chance. A kappa below 0 means agreement is worse than chance, which indicates systematic disagreement. In practice, kappa above 0.8 is considered strong agreement. Kappa between 0.6 and 0.8 is moderate. Kappa below 0.6 suggests the judge is not reliably aligned with human raters.

For a customer support quality judge calibrated on 2,000 human-labeled conversations, a kappa of 0.83 indicates the judge is reliably mimicking human judgment. A kappa of 0.58 indicates the judge is only somewhat aligned and should not be trusted for high-stakes decisions without human oversight.

Kappa is most useful for categorical judgments — binary or multi-class labels. For continuous scores, weighted kappa or intraclass correlation coefficient provides similar chance-adjusted agreement measures. The specific metric matters less than the discipline of measuring agreement against a meaningful baseline and tracking that metric over time.

## Building and Maintaining Calibration Datasets

A calibration dataset is not a one-time artifact. It is a living dataset that evolves with your task and your pipeline.

Start with 1,000 to 5,000 examples representative of the task the judge will evaluate. These examples should cover the full range of quality — excellent outputs, poor outputs, edge cases, ambiguous cases. If the task is customer support quality, include short and long responses, responses in different tones, responses that are factually correct but unhelpful, responses that are helpful but violate policy. If the task is summarization, include summaries of different lengths, summaries that are accurate but verbose, summaries that are concise but miss key points.

Label each example with human raters. Use at least two raters per example, preferably three. Measure inter-rater agreement. If human raters disagree frequently, the task definition is ambiguous and the judge has no stable target to align with. Resolve disagreements through discussion or adjudication by a senior rater. The final label is the consensus or adjudicated label.

Store the dataset with metadata: when it was created, which raters labeled it, what version of the task guidelines they used, what the inter-rater agreement was. This metadata lets you trace calibration changes back to dataset changes.

Re-label a portion of the dataset periodically. If you labeled 3,000 examples in January, re-label 500 of them in June using current raters and current guidelines. If the June labels differ systematically from the January labels, your human judgment has drifted and your calibration baseline needs updating.

Add new examples when the task distribution shifts. If you launch a new product category, add 500 examples from that category to the calibration set. If customer tone expectations change, add recent examples reflecting the new expectations. The calibration dataset should always represent the current task, not the task as it existed six months ago.

## Recalibration Triggers and Continuous Monitoring

Judge calibration is not validated once and assumed stable. You recalibrate when specific triggers indicate potential drift.

Recalibrate when the underlying model updates. If you're using GPT-5 as your judge and OpenAI releases GPT-5.1, run the calibration dataset through the new model before switching in production. Measure agreement against human labels. If agreement drops by more than 3 percentage points or kappa drops by more than 0.05, investigate. The new model may require prompt updates or may not be suitable as a judge for your task.

Recalibrate when human feedback indicates drift. If human reviewers start flagging judge scores as incorrect — if they're seeing outputs the judge scored highly that they consider poor, or outputs the judge scored poorly that they consider good — that's a drift signal. Pull a sample of 200 recent flagged examples. Re-label them with current human raters. Measure agreement. If the judge's agreement with current human labels is significantly lower than its agreement with the original calibration labels, the judge has drifted.

Recalibrate on a fixed schedule. Even if no triggers fire, re-run calibration quarterly or semi-annually. Run the judge on a held-out subset of the calibration dataset that was not used during the original prompt engineering. Measure agreement. Track the metric over time. If agreement is stable, the judge is holding up. If agreement declines gradually, you're catching drift before it becomes severe.

Recalibrate when the task changes. If your eval task expands from evaluating English responses to evaluating multilingual responses, the judge needs recalibration. If the rubric changes — if "helpfulness" now includes "provides links to documentation" as a requirement when it didn't before — the judge needs recalibration. Any change in what the task is or what "good" means invalidates prior calibration.

## The Human Anchor Requirement

LLM judges are automation layers. They do not replace human judgment. They approximate it. The anchor is always human labels — the ground truth the judge is trying to replicate.

Without a human anchor, you have no way to know if the judge is reliable. You might run the judge on 100,000 examples and get beautifully consistent scores, but if those scores don't correlate with what humans would have judged, the consistency is meaningless. You're measuring something, but not the thing you care about.

The human anchor requires ongoing investment. You need a pool of trained raters who understand the task, can apply the rubric consistently, and produce labels you trust. You need the infrastructure to collect those labels, resolve disagreements, and store the labeled data. You need the process discipline to keep labeling even when the judge seems to be working fine.

Teams that skip the human anchor eventually lose confidence in their eval pipeline. The scores drift. The judge starts rewarding things it shouldn't. No one notices because there's no baseline to compare against. By the time the problem becomes obvious — when a production incident reveals that the model has been generating bad outputs for weeks while the judge scored them green — the damage is done.

Calibration is insurance. You pay the cost of human labeling up front to ensure that the automation you've built on top of those labels remains trustworthy. The cost is non-negotiable. The alternative is flying blind.

---

Even a well-calibrated single judge has blind spots and edge cases it cannot handle reliably. The next subchapter covers multi-judge ensembles and how to use disagreement between judges as a signal that human review is required.
