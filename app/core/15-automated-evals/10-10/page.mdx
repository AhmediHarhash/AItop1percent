# 10.10 — Avoiding Metric Gaming and Goodhart's Law

**Goodhart's Law** states that when a measure becomes a target, it ceases to be a good measure. In eval pipelines, this manifests as the insidious tendency for teams to optimize the metrics themselves instead of optimizing the underlying quality those metrics were designed to measure. The moment you tie promotion decisions to precision scores, tie quarterly goals to latency thresholds, or tie team bonuses to pass rates, the entire eval system becomes vulnerable. Engineers stop asking "is the model better" and start asking "how do I make this number go up." The system rewards behavior that moves the metric while degrading the system itself.

A fintech company in 2025 tied engineering performance reviews to their customer support chatbot's CSAT score. Within two months, the team had restructured prompts to ask users for positive feedback, adjusted routing logic to send difficult questions to human agents before the AI could fail, and filtered eval sets to exclude edge cases that historically scored poorly. CSAT climbed from 82% to 91%. User complaints doubled. The team had discovered that improving the metric was easier than improving the model. They optimized for measurement instead of reality. By the time leadership intervened, the entire eval pipeline was compromised. Every metric had been gamed. Nothing could be trusted.

Goodhart's Law does not describe malicious behavior. It describes rational responses to misaligned incentives. You do not need bad actors to destroy your eval system. You need normal engineers doing what they are measured on.

## Gaming Behaviors in Eval Pipelines

The most common gaming pattern is **eval set curation**. Teams remove difficult cases from the eval set because those cases consistently fail. The justification sounds reasonable: "These are edge cases that don't reflect real usage." The result is an eval set that becomes progressively easier over time. Metrics improve quarter over quarter while production quality stagnates or declines. You discover the drift when a user complaint triggers an investigation and you realize that entire categories of queries have been excluded from measurement.

The second pattern is **threshold relaxation**. A team cannot hit the 95% accuracy target for the current sprint, so they adjust the threshold to 92% and declare success. Next quarter, 92% feels achievable but difficult, so it becomes 89%. The numbers trend upward on dashboards because the goalposts moved. Absolute quality degrades, but relative performance looks excellent. This pattern accelerates when thresholds are set arbitrarily without grounding in user impact or business requirements.

The third pattern is **selective metric reporting**. Engineers run evals across twelve dimensions but report only the three that improved. Latency regressed, hallucination rate increased, and refusal rate spiked, but precision and recall both improved, so the dashboard shows green. The team is not lying. They are highlighting positive results and contextualizing negative ones as "trade-offs" or "known limitations." Over time, leadership loses visibility into actual system health because every report emphasizes wins and buries losses.

The fourth pattern is **prompt tuning to the eval set**. Engineers iterate prompts while watching eval metrics in real time. They discover that adding "be concise" drops latency by 200 milliseconds. They add it. Latency metrics improve. Production logs show that responses are now too terse for users, but the eval set measures only response time, not response completeness. The prompt was optimized to pass the eval, not to serve the user. This is the eval equivalent of overfitting. The model performs well on the test set and poorly in the wild.

The fifth pattern is **output post-processing** that targets metric calculation without improving user experience. A summarization model produces outputs that score poorly on ROUGE because it paraphrases heavily instead of extracting sentences verbatim. Engineering adds a post-processing step that identifies high-overlap phrases from the source document and injects them into summaries. ROUGE scores improve 14 points. Users report that summaries now feel robotic and repetitive. The metric improved. The product degraded.

## Detecting Gaming

Gaming is difficult to detect because it looks like optimization. Both involve changing the system to improve measurements. The distinction lies in whether the change improves the thing being measured or just the measurement itself. The most reliable detection signal is **divergence between eval metrics and production metrics**. If your eval pipeline shows steady improvement while production complaints, escalations, or correction rates hold steady or worsen, someone is gaming the eval. The system is being tuned to pass internal tests while failing external reality.

The second signal is **eval set stability analysis**. Track the composition of your eval set over time. If the set shrinks, if certain categories disappear, if difficulty scores trend downward, if the mean score for older queries increases when re-run months later, the set is being curated. A healthy eval set grows as you discover new edge cases. A gamed eval set shrinks as you remove cases that threaten metrics.

The third signal is **threshold drift tracking**. Log every threshold change with justification. If thresholds relax more often than they tighten, if justifications cite "alignment with industry standards" without naming those standards, if changes coincide with sprint deadlines or performance reviews, you have systematic gaming. Thresholds should change when business requirements change or when measurement methodology improves. They should not change to accommodate current model performance.

The fourth signal is **metric basket inversion**. Measure how often different metrics are cited in status updates, launch approvals, or retrospectives. If the set of emphasized metrics shifts every quarter, if metrics that were previously highlighted disappear from reports without explanation, if new metrics are introduced right before performance reviews, teams are selecting metrics that tell favorable stories. A stable system measures the same things consistently. A gamed system rotates metrics to avoid accountability.

The fifth signal is **eval-production correlation decay**. Measure the statistical correlation between eval scores and production outcomes over time. In a healthy system, correlation remains stable or strengthens as evals improve. In a gamed system, correlation weakens because the eval set diverges from production reality. You can measure this mechanically: score production samples through your eval pipeline weekly, then compare eval scores to actual user ratings, escalation likelihood, or correction rates. If the relationship decays, your eval has been compromised.

## Designing Ungameable Metrics

No metric is perfectly ungameable, but some are structurally harder to manipulate than others. The first principle is **measuring outcomes, not outputs**. An output metric measures what the model produced: accuracy, precision, latency, ROUGE score. An outcome metric measures what happened next: user satisfaction, task completion, correction rate, escalation likelihood. Output metrics can be gamed by tuning outputs. Outcome metrics require changing user behavior. If your primary metrics are output-based, you are vulnerable. If your primary metrics are outcome-based, gaming requires either faking production logs or genuinely improving the product.

The second principle is **composite metrics with opposing incentives**. A single metric creates a single optimization target. A composite metric balances trade-offs. Instead of measuring only accuracy, measure accuracy AND coverage. Engineers cannot game both simultaneously by narrowing scope. Instead of measuring only latency, measure latency AND completeness. Engineers cannot optimize both by truncating responses. Composite metrics do not prevent gaming entirely, but they make it harder by requiring simultaneous optimization across conflicting dimensions.

The third principle is **blinded evaluation**. Run a subset of evals on data that engineers cannot see or influence. The eval team maintains a private golden set, a private adversarial set, and a private production sample set. Engineers optimize against the visible eval set. Leadership measures against the hidden sets. Divergence between visible and hidden performance reveals gaming. This approach requires operational overhead and organizational separation between eval ownership and model development, but it is the most reliable structural defense against metric manipulation.

The fourth principle is **frequent eval set rotation**. Replace 20% of your eval set every month with new production samples, newly discovered edge cases, and newly reported failures. Announce rotation but not composition. Engineers cannot tune to a moving target they cannot see. Rotation prevents overfitting without requiring blinded evaluation, though it is less robust because engineers can still influence the 80% that remains stable.

The fifth principle is **user-grounded metrics**. Tie at least one core metric directly to user behavior: explicit ratings, implicit engagement signals, correction frequency, abandonment rate, escalation rate. Users cannot be gamed. They respond to actual product quality, not eval scores. If your dashboard shows improving internal metrics but declining user metrics, you have proof of gaming. If both trend together, your internal metrics are valid proxies.

## The Goodhart Trap

The deepest risk is not that metrics get gamed, but that the organization begins optimizing for metrics without realizing they have diverged from goals. This is the **Goodhart Trap**: the belief that hitting the metric is equivalent to achieving the objective. The metric was chosen because it correlated with the objective. Over time, through gaming or drift, the correlation weakens. But the organization continues optimizing the metric because it is measurable, while the objective remains vague. The metric becomes a substitute for the goal rather than a proxy for it.

A healthcare AI company in 2024 measured their diagnostic assistant's performance using precision and recall against a labeled dataset of 50,000 cases. Both metrics exceeded 92% by mid-2025. The team celebrated. Six months later, a clinical audit revealed that the model was recommending unnecessary tests at twice the rate of human clinicians, driving up healthcare costs without improving patient outcomes. The eval set measured diagnostic accuracy. It did not measure clinical appropriateness, cost-effectiveness, or adherence to evidence-based guidelines. The model hit the metric. It failed the mission.

The trap manifests in roadmap decisions. Teams prioritize work that moves the measured metric over work that serves the user. A feature that improves latency by 50 milliseconds gets built because latency is on the dashboard. A feature that improves output relevance for a narrow but critical user segment gets deprioritized because relevance is not measured quantitatively. The metric defines the product. The product serves the metric.

The trap manifests in hiring and promotion. Engineers who improve dashboard metrics get promoted. Engineers who improve unmeasured aspects of quality do not. The organization learns that metric optimization is the path to advancement. Teams become proficient at gaming. Leadership celebrates metric improvements without validating that those improvements correspond to user value or business outcomes.

The only escape from the Goodhart Trap is **metric skepticism at the leadership level**. Executives must treat every metric as a hypothesis to be validated, not a truth to be optimized. They must ask, "This number improved — did the user experience improve?" They must demand proof of correlation between internal metrics and external outcomes. They must reward engineers who surface metric-reality divergence, not punish them. The organization must institutionalize the question: "Are we measuring the right thing?"

## Metric Rotation and Sunsetting

One structural defense against Goodhart's Law is **planned metric rotation**. Every metric has a lifecycle. When introduced, it captures a real quality dimension. Over time, as teams optimize against it, it becomes less informative and more gameable. Eventually, it stops being a useful signal. The solution is not to keep measuring it forever. The solution is to sunset it and replace it with a different metric that captures the same underlying quality from a different angle.

A legal tech company measured contract review accuracy using exact-match scoring against human-labeled ground truth. After eighteen months, accuracy plateaued at 87%. The team could not push higher. Closer investigation revealed that the model had learned to output the most common answer for ambiguous cases, which maximized exact-match scores but produced outputs that lawyers found unreliable. The eval team retired exact-match accuracy and replaced it with a confidence-calibrated metric that penalized overconfident predictions on ambiguous inputs. Overnight, measured performance dropped to 78%. Over the next six months, actual lawyer satisfaction improved 22 points as the model learned to express uncertainty appropriately. The new metric captured the quality dimension the old metric had stopped measuring.

Rotation does not mean abandoning measurement. It means evolving the measurement to stay aligned with the objective. The underlying goal remains constant. The metrics that proxy for it rotate every twelve to twenty-four months as teams adapt and optimization pressure builds. You announce the rotation in advance so teams can prepare. You run both old and new metrics in parallel for one quarter to establish correlation and baseline performance. Then you switch. The dashboard shows the new metric. The old one becomes an archival reference.

Rotation prevents overfitting without requiring hidden eval sets or blinded evaluation. It acknowledges that metrics are tools, not truths. When a tool dulls, you replace it. The organization learns to optimize for outcomes rather than specific measurements, because they know the measurements will change but the outcomes will not.

Metric gaming is not an engineering problem. It is an organizational design problem. The engineering team will optimize for whatever you measure. If you measure the wrong things, they will optimize the wrong things. If you measure the right things badly, they will game the measurement. The solution is not to blame engineers for responding to incentives. The solution is to design incentives that align measurement with reality, to build structural defenses against gaming, and to cultivate leadership skepticism that treats every metric as provisional and every improvement as a hypothesis to be validated against user outcomes. When the measure becomes the target, it ceases to be a good measure — unless you design the system to prevent that transformation in the first place.

---

Next: **10.11 — Eval Metrics as Deployment Gates**, where metrics transition from measurement to control, blocking releases that fail to meet quality thresholds.
