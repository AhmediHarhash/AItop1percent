# 10.1 — Eval Metrics That Actually Matter

The metrics most teams track are not the metrics that drive decisions. Teams measure what's easy to measure — total test count, pipeline runtime, coverage percentage — and miss the metrics that actually determine whether a model ships or gets rolled back. A metric is only useful if it changes behavior. If leadership ignores it, if engineers don't debug when it degrades, if it never blocks a deploy, it's not a metric. It's decoration.

The gap between measurement and action destroys eval programs. Teams build dashboards with forty metrics, update them nightly, and then ship models based on gut feel because no one knows which number actually matters. The eval system produces data. The organization ignores it. Six weeks later, production degrades, and the post-mortem reveals that three metrics had been declining for weeks — but no one was watching them, no one had thresholds, and no one knew they were decision-relevant. The metrics existed. They just didn't matter.

## The Decision Test

A metric matters if it answers a question someone is actively asking. Product asks: "Is this model good enough to show users?" Engineering asks: "Will this change break existing capabilities?" Leadership asks: "Can we turn off the expensive model and route more traffic to the cheaper one?" If your metric doesn't answer one of these questions, it's not driving decisions. It's consuming dashboard space.

Apply the decision test to every metric in your system. For each one, write down the decision it informs. If you can't name a decision, delete the metric or move it to a debug-only view. The primary dashboard — the one that determines whether a model ships — should have five to twelve metrics, not forty. Each metric should map to a specific question that a specific role needs answered before taking a specific action. Coverage percentage doesn't pass this test. "Does the model maintain baseline performance on safety-critical tasks?" passes.

The metrics that matter are the ones that can say no. A metric that never blocks a deploy is a vanity metric. It might correlate with quality. It might trend upward over time. But if a model can ship even when that metric is red, the metric has no authority. The organization has already decided it doesn't matter. The useful metrics are the ones with thresholds that stop work. When those metrics fail, the deploy doesn't happen, the experiment gets paused, the engineer investigates. Everything else is commentary.

## Vanity Metrics Versus Actionable Metrics

**Vanity metrics** make the team feel productive without enabling action. Test count is a vanity metric. You ran 12,000 tests this week. Great. Did quality improve? Did you catch a regression? Did anyone make a decision based on that number? Test count measures activity, not outcome. It goes up when the team writes more tests, but writing more tests doesn't guarantee better models. A team with 12,000 low-signal tests is worse off than a team with 200 high-signal tests, but test count makes the first team look more rigorous.

Pipeline runtime is a vanity metric. Coverage percentage is a vanity metric. "Evals run nightly" is a vanity metric. These numbers describe the eval system, not the model. They tell you the system is running. They don't tell you whether the model is good. A team can have 98% test coverage and ship a model that hallucinates in production because the tests measured the wrong things. High coverage of low-value assertions is still low value.

**Actionable metrics** drive specific decisions. Pass rate on the golden set is actionable. If it drops below 94%, the model doesn't ship. That threshold creates a forcing function. When pass rate hits 93.8%, someone investigates. They find the failure mode, trace it to a prompt change, revert the change, re-eval, confirm recovery, and then the model ships. The metric caused the investigation. The investigation found the problem. The problem got fixed. That's an actionable metric.

Regression rate is actionable. If more than 2% of previously passing cases now fail, the deploy is blocked. Domain-specific accuracy is actionable. If the model's performance on medical terminology drops below the baseline, you don't ship it to healthcare customers. Latency at p99 is actionable. If inference takes longer than 800 milliseconds, you don't deploy to the real-time voice product. Each of these metrics has a threshold, a decision, and a consequence. They matter because they can stop work.

The shift from vanity to actionable happens when you add three elements: a threshold, a decision rule, and an owner. A metric without a threshold is just a number. A metric with a threshold but no decision rule is just an alarm no one responds to. A metric with a threshold and a decision rule but no owner is an alarm that everyone assumes someone else is handling. Actionable metrics have all three. The threshold is documented. The decision rule is automated or enforced by process. The owner is named. When the metric goes red, a specific person or team knows they need to act.

## Metrics That Block Deploys

Not every metric needs to block deploys, but at least three should. These are your **deployment gates** — the metrics that define "good enough to ship." They represent the minimum quality bar. If the model clears these gates, it can enter production. If it fails any of them, it stays in staging until someone fixes it. The gates are not negotiable. They're not "we should probably look into this" metrics. They're "the model does not ship" metrics.

The first gate is usually **golden set pass rate** — performance on the curated set of cases that define baseline quality. This is your anchor metric. It represents the organization's shared understanding of what the model must handle correctly. If golden set pass rate is below threshold, every other metric is suspect. You can't trust domain-specific performance if you can't even clear the baseline. Golden set pass rate is a forcing function that ensures the model meets the minimum bar before anyone evaluates its suitability for specific use cases.

The second gate is often **regression rate** — the percentage of previously passing cases that now fail. This metric protects against quality degradation. A new model might have a higher overall pass rate than the previous model while also breaking cases that users depend on. If you deploy it, those users file bugs, support volume spikes, and the team spends two weeks firefighting regressions. Regression rate catches this before deploy. It says: you can improve overall quality, but you cannot break things that already work — or if you do, the breaks must be intentional, documented, and approved.

The third gate varies by use case. For safety-critical applications, it's often **harm detection rate** — the percentage of inputs that would cause reputational, legal, or safety harm that the model correctly refuses or escalates. For latency-sensitive applications, it's **p99 latency** — the slowest 1% of requests must still complete within the acceptable window. For multi-tenant systems, it's often **per-tenant minimum pass rate** — no tenant's performance can drop below a floor, even if aggregate performance improves. The third gate is the metric that represents your specific risk profile.

These three gates are automated. The CI/CD pipeline runs the eval suite, calculates the metrics, compares them to thresholds, and either allows the deploy or blocks it. No human discretion. No "let's ship it anyway and monitor closely." The pipeline enforces the rule. This automation is what makes the metrics matter. When a human has to manually decide whether to block a deploy based on a metric, they rationalize. They find reasons why this deploy is special, why the metric doesn't apply, why they can ship now and fix later. Automation removes that discretion. The metric decides.

## Leading Versus Lagging Indicators

**Lagging indicators** measure outcomes after they happen. Production accuracy is a lagging indicator. It tells you the model is performing poorly after users have already experienced the failures. Support ticket volume is a lagging indicator. Latency degradation detected in production is a lagging indicator. These metrics are essential for understanding real-world impact, but they can't prevent problems. By the time they signal, the problem has already occurred.

**Leading indicators** predict outcomes before they happen. Eval suite pass rate is a leading indicator. It tells you whether the model is likely to perform well in production before you deploy it. Regression rate in staging is a leading indicator. Eval coverage of recent user queries is a leading indicator. These metrics give you a chance to intervene. If eval pass rate drops in staging, you fix it before it reaches production. If regression rate spikes during testing, you investigate before users see the failures.

The eval system's value is in its leading indicators. That's the entire point of pre-production testing. You measure quality in a controlled environment so you can predict quality in the wild. If your eval metrics don't correlate with production outcomes, they're not leading indicators — they're noise. A leading indicator has predictive power. When it goes red in staging, production quality degrades if you ship anyway. When it stays green in staging, production quality holds or improves.

Validate the correlation. Track your leading indicators over twenty or thirty deploys, then compare them to the lagging indicators. Does a drop in eval pass rate predict a rise in production failures? Does an increase in regression rate predict a spike in support tickets? If the correlation is strong, the leading indicator is doing its job. If there's no correlation, the leading indicator is measuring something orthogonal to user experience. You need a different metric.

The best eval programs track both. Leading indicators determine whether the model ships. Lagging indicators validate whether the leading indicators were accurate. If the leading indicators say the model is ready, you deploy it. Then you watch the lagging indicators. If they stay green, the leading indicators were correct. If they go red, you've discovered a gap in your eval coverage. The leading indicators missed something production exposed. You trace the failure back to the eval suite, add coverage for the failure mode, and now the leading indicator will catch it next time. This closed loop is how eval systems improve.

## Metric Prioritization and the Primary Dashboard

Most teams have too many metrics. They track everything measurable, display it all on one dashboard, and overwhelm anyone who tries to use it. The dashboard has six tabs, forty metrics, twelve charts, and no clear hierarchy. An engineer opens it, scans for two minutes, closes it, and makes a decision based on intuition. The information is there. It's just unusable.

The fix is prioritization. Identify the five to twelve metrics that matter most. These are the metrics with thresholds, decision rules, and owners. These are the metrics that block deploys, trigger investigations, or inform resource allocation. Everything else goes into a secondary dashboard or a debug view. The primary dashboard is the one the team checks daily. It should load in under three seconds, fit on one screen without scrolling, and make the current state obvious at a glance. Red means action required. Yellow means watch closely. Green means proceeding as expected.

Organize the primary dashboard by decision context. Group the metrics that Product cares about. Group the metrics that Engineering cares about. Group the metrics that inform cost decisions. Each group gets a section. Within each section, order metrics by priority. The most important metric — the one that most directly answers the group's primary question — goes first. The supporting metrics follow. This hierarchy tells the viewer where to look first. If the top metric is green, they can trust the rest. If the top metric is red, they know the group has a problem even if they don't understand every supporting metric.

For a content moderation system, Product's primary metric might be "harmful content detection rate" — the percentage of policy-violating content the model correctly flags. Engineering's primary metric might be "p95 latency" — the time it takes to evaluate 95% of content. Cost's primary metric might be "cost per million evaluations." Each group sees their metric first, supported by secondary metrics that provide context. Product sees detection rate, then false positive rate, then edge case coverage. Engineering sees latency, then throughput, then error rate. Cost sees unit cost, then total monthly spend, then cost trend over time.

The secondary dashboard holds everything else. Metrics that are useful for debugging but not for daily decisions. Metrics that track system health but don't inform model quality. Metrics that matter to one specialist but not to the broader team. The secondary dashboard is where you put test count, pipeline runtime, coverage percentage, intermediate layer performance, ablation study results, and experimental metrics you're validating. These metrics exist. They're tracked. They're just not on the primary dashboard because they don't drive the core decisions.

This separation prevents metric fatigue. If every metric is treated as equally important, no metric is important. The team learns to ignore the dashboard because it demands too much attention. By prioritizing, you create focus. The primary dashboard shows what matters today. The secondary dashboard preserves detail for when you need it. Engineers check the primary dashboard before every deploy. They check the secondary dashboard when a primary metric goes red and they need to understand why. Each dashboard serves its purpose.

The metrics that matter are the ones that change what you build, when you ship, and how you allocate resources. Everything else is interesting data. Interesting data has value — but not on the primary dashboard, not blocking deploys, and not consuming executive attention. The eval system's job is to surface the metrics that matter and make them impossible to ignore. Pass rate, regression rate, and slicing breakdowns give you that visibility and drive the decisions that keep quality high.

---

*Next: 10.2 — Pass Rate, Failure Rate, and Regression Detection*
