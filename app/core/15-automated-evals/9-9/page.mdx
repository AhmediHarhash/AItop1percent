# 9.9 — The Silent Drift Problem: Catching Invisible Degradation

**The Silent Drift Problem** is what happens when your eval metrics stay green, your pass rates hold steady, and your production quality quietly degrades. The eval is not broken. The eval is doing exactly what it was calibrated to do. But the world has changed. User expectations shifted. The input distribution evolved. A new edge case became common. The model started producing outputs that technically pass your criteria but fail in ways your criteria never anticipated. Your dashboard shows stability. Your users experience decline. By the time you notice, the damage is done. Silent drift is invisible degradation — and it is the hardest failure mode to catch because every standard monitoring practice tells you nothing is wrong.

## Why Drift Goes Unnoticed

Traditional monitoring catches sudden breaks. If your pass rate drops from eighty percent to sixty percent overnight, alerts fire. If your latency spikes from two hundred milliseconds to two seconds, you get paged. If a new model version produces malformed outputs, your schema validation fails. These are loud failures. Silent drift is different. Pass rate stays at eighty percent. Latency stays at two hundred milliseconds. Schema validation passes. But the outputs that pass your eval are subtly worse than they used to be. A customer support chatbot starts giving technically accurate but less helpful responses. A legal assistant starts citing correct cases but missing the most relevant ones. A medical summarizer starts producing summaries that pass coherence checks but omit critical context that a physician would need.

The problem is that your eval measures what you told it to measure. It does not measure what you forgot to specify. A fintech company ran a tone evaluator on investment advice outputs. The eval checked for professional language, absence of hype words, and appropriate hedging. In November 2025, the underlying model was updated from GPT-5 to GPT-5.1. Pass rate on the tone eval stayed at seventy-six percent. But user complaints about advice being "too cautious" increased by nineteen percent over six weeks. The team investigated and found that GPT-5.1 was producing more hedged language than GPT-5. The tone eval checked that hedging existed — it passed outputs with phrases like "may potentially" and "could possibly consider." It did not check that hedging was proportional to uncertainty. GPT-5.1 hedged on low-uncertainty statements where GPT-5 would have been direct. The eval passed both. Users noticed the difference. The dashboard did not.

Silent drift also happens when input distribution changes. Your eval was calibrated on one kind of traffic. Production starts seeing a different kind. A legal AI system had an eval tuned on contract review queries from enterprise customers. In early 2026, the product launched a self-serve tier for small businesses. Small business queries used simpler language, referenced fewer legal precedents, and asked more basic questions. The eval had been calibrated to expect complex, multi-clause queries with specific legal citations. It passed outputs that matched that complexity. For simple queries, it passed outputs that over-explained and included unnecessary jargon. The eval criteria had not changed. The traffic had changed. Pass rate stayed stable. User satisfaction for the self-serve tier was fourteen points lower than for enterprise. The team discovered this only when they segmented satisfaction scores by customer tier.

## Detection Strategies for Silent Drift

Catching silent drift requires monitoring what your eval does not measure. This means tracking proxy metrics that correlate with quality but are not part of your eval criteria. For a customer support system, track escalation rate — how often users ask for a human after the AI responds. If escalation rate increases while your eval pass rate stays flat, your eval is missing something. For a summarization system, track user engagement — how often users expand the summary to read the full document. If engagement drops, your summaries may be losing relevance even if they pass coherence and conciseness checks. For a code generation system, track acceptance rate — how often developers use the generated code without modification. If acceptance rate declines while your syntax and style checks pass, the code is technically correct but practically less useful.

A healthcare chatbot tracked a metric called **time to resolution** — how many conversational turns it took before a user's question was fully answered. The eval checked accuracy, empathy, and safety. Time to resolution was not part of the eval. In December 2025, time to resolution started creeping upward. It went from an average of 2.3 turns to 2.7 turns over eight weeks. The eval pass rate stayed at eighty-one percent. The team investigated and found that a recent prompt update had made responses more cautious. Cautious responses were accurate and safe — they passed the eval — but they were less direct. Users needed more follow-up questions to get the information they wanted. The team adjusted the prompt to balance caution with directness. Time to resolution dropped back to 2.4 turns. Without tracking the proxy metric, the drift would have continued indefinitely.

Another detection strategy is periodic deep dives. Every month, take a random sample of outputs that passed your eval and review them manually. Ask domain experts: would you have passed these outputs? Are there patterns you notice that the eval is missing? A legal AI team did this in January 2026 and discovered that their citation evaluator was passing citations that were technically correct but outdated. The eval checked whether the cited case law existed and matched the topic. It did not check whether the case was still good law or had been overruled by later decisions. The manual review found this in eleven percent of sampled outputs. The team updated the eval to include recency checks. Pass rate dropped by nine percent — which was the correct outcome. The eval had been too lenient. The monthly deep dive caught what the dashboard missed.

## Proactive Monitoring for the Degradation Dashboards Miss

The outputs your users complain about are often not the outputs your eval flags. A customer support platform tracked complaint keywords — words like "unhelpful," "confusing," "wrong," "not what I asked" — in user feedback. When complaint volume increased by twelve percent in late 2025, the team cross-referenced complaints with eval scores. Seventy-three percent of outputs that users complained about had passed the eval. The eval was checking accuracy and tone. Users were complaining about relevance and completeness. The eval did not measure those dimensions. The team added relevance and completeness criteria, recalibrated on a sample that included complaint cases, and redeployed. Complaint volume dropped by eighteen percent over the next month.

Proactive monitoring also means watching for correlation breaks. If pass rate and user satisfaction usually move together, and suddenly they diverge — pass rate stable, satisfaction declining — you have silent drift. If false positive rate and escalation rate usually correlate, and suddenly escalations increase while false positives stay flat, your eval is missing a new failure mode. Correlation breaks are early warning signals. A fintech company tracked correlation between eval pass rate and customer retention. For two years, the correlation was strong: higher pass rate meant higher retention. In Q1 2026, retention started declining while pass rate held steady. The team dug in and found that a competitor had launched a product with faster response times. Users were leaving not because quality declined but because speed expectations shifted. The eval measured quality. It did not measure speed. The correlation break revealed that quality alone was no longer sufficient.

Another proactive approach is canary evaluations — evals that run on production traffic but do not affect production decisions. A canary eval is an experiment. You suspect your current eval is missing something. You build a new eval that checks for it. You run the new eval in shadow mode, scoring every output but not gating any. You track how often the new eval flags outputs that the production eval passes. If the new eval flags a significant percentage, you review those cases. If the flags are legitimate, you promote the new eval. If the flags are false positives, you tune the new eval and try again. Canary evaluations let you test hypotheses about what your current eval is missing without risking production stability.

## The Degradation That Dashboards Miss

Dashboards show you what you measure. If you measure pass rate, false positive rate, and latency, that is what your dashboard shows. Silent drift happens in the dimensions you are not measuring. The fix is not to measure everything — that creates noise and alert fatigue. The fix is to measure the right proxy metrics and to refresh your eval regularly based on production feedback. A legal AI company ran quarterly eval audits. Every quarter, they sampled two hundred outputs that passed production evals and sent them to domain experts for blind review. Experts rated outputs on a five-point scale. If average expert rating dropped by more than 0.2 points between quarters, the team triggered a calibration review. In Q4 2025, expert ratings dropped from 4.1 to 3.8. The review found that the eval was passing outputs with correct legal reasoning but poor structure. Users found them hard to follow. The team added a structure criterion, recalibrated, and expert ratings returned to 4.0 in Q1 2026.

Silent drift also happens when your model improves in one dimension and regresses in another. A summarization system upgraded to GPT-5.2 in early 2026. The new model produced more concise summaries. The eval checked conciseness — pass rate improved from seventy-nine percent to eighty-four percent. But domain experts reviewing a sample noticed that the summaries were missing key details that GPT-5 had included. The eval checked for conciseness and coherence. It did not check for completeness relative to source material. The model got better at what the eval measured and worse at what the eval missed. The dashboard showed improvement. Production quality declined in a dimension no one was tracking.

The hardest part of silent drift is that it feels like stability. Your metrics are steady. Your alerts are quiet. Your pipeline is running smoothly. The only signal is qualitative: user feedback, domain expert intuition, support ticket trends. Teams that ignore qualitative signals in favor of quantitative dashboards miss silent drift until it becomes a crisis. Teams that treat qualitative signals as hypotheses to test with data catch silent drift early. When a domain expert says "the outputs feel less helpful lately," that is not noise. That is a hypothesis. Test it. Pull a sample. Measure the dimension the expert is describing. If the data supports the hypothesis, update your eval. If the data contradicts it, investigate why the expert perceives a problem that the data does not show.

## Canary Evaluations as Early Warning Systems

A **canary evaluation** is a secondary eval that monitors for degradation your primary eval might miss. It runs continuously on production traffic but does not block outputs. It logs scores and flags patterns. If canary scores diverge from primary eval scores, you investigate. If canary flags cluster around a specific input type or failure mode, you consider promoting the canary criterion to the primary eval. Canary evals are inexpensive to run because they do not affect production decisions. They are high-value because they detect drift before it reaches users.

A healthcare AI system ran a canary eval that checked for medical term accuracy. The primary eval checked for hallucination, safety, and empathy. The canary eval used a specialized medical terminology validator that was too slow and too expensive to run on every output. Instead, it ran on a ten percent sample. In February 2026, the canary eval started flagging an increasing percentage of outputs for incorrect drug name spelling. The primary eval did not check spelling — it checked whether the drug recommendation was contextually appropriate. The canary caught the drift. The team investigated and found that a recent model update had introduced a subtle tokenization issue that affected medical terminology. They rolled back the model update and fixed the tokenization. Without the canary eval, misspelled drug names would have shipped to production until a user reported it.

Canary evals work best when they target known unknowns — dimensions you suspect might matter but have not validated yet. You think citation recency might be important. You build a canary eval that checks it. You run it for a month. If it never flags anything, you discard it. If it flags ten percent of outputs and manual review shows those flags are legitimate, you add recency to your primary eval. Canary evals are cheap experiments. They let you test eval hypotheses without committing to full deployment.

## Refreshing Evals to Keep Pace with Reality

Your eval should evolve as fast as your product. When user expectations shift, update your eval. When input distribution changes, recalibrate. When a new failure mode emerges, add a criterion. When a criterion stops correlating with quality, remove it. Static evals drift out of sync with production reality. Dynamic evals stay aligned. A customer support platform refreshed their eval every six months. Each refresh included: a sample review by domain experts, a calibration run on recent production data, a survey of support agents about emerging quality issues, and a comparison of eval scores to user satisfaction. If gaps appeared, the team updated criteria or thresholds. The six-month cadence kept the eval aligned with evolving user needs.

Silent drift is not a failure of monitoring. It is a failure of measurement design. You cannot catch what you do not measure. The fix is to measure proxy metrics, run periodic deep dives, track correlation breaks, deploy canary evals, and treat qualitative signals as hypotheses worth testing. The eval that worked six months ago may not work today. The question is whether you notice — and how fast you adapt. The next question is when to rebuild your eval entirely versus when to recalibrate what you already have.

