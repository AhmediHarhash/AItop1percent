# 8.10 — Eval Result Storage and Querying

Eval results are data, and like all data in production systems, they need a schema, a retention policy, and a query interface. The difference is that eval results are not just operational telemetry—they are the evidence trail that justifies every model deployment, every prompt change, every architectural decision. When a stakeholder asks why you rolled back a release, when Legal asks how you validated a compliance requirement, when Engineering asks whether the regression happened gradually or suddenly, the answer comes from your eval result store. If that store is an append-only blob dump with no query layer, you have data but no answers. If the schema is inconsistent across pipeline versions, you cannot compare results over time. If retention deletes the baseline run that established your quality floor, you have lost your reference point. Eval result storage is not infrastructure you build once and forget. It is the foundation of every investigation, every decision, every retrospective.

## The Eval Result Data Model

The schema for eval results must support three use cases simultaneously: operational monitoring, historical analysis, and compliance auditing. Operational monitoring needs recent results grouped by pipeline run, with fast access to pass rates and failure details. Historical analysis needs results spanning months or years, joinable by model version, prompt version, dataset version, and infrastructure config. Compliance auditing needs immutable records with timestamps, user attribution, and the ability to reconstruct exactly what was tested and why. A schema optimized for one use case at the expense of the others creates friction. You end up with a real-time dashboard that cannot answer historical questions, or an audit log that cannot power alerts.

The core entity is the eval run. Each run has a unique identifier, a timestamp, a triggering event—commit hash, pull request number, schedule trigger, manual invocation—and a reference to the system under test. The system under test is not just a model ID. It includes the model version, the prompt template version, the retrieval config version, the tool definitions, the routing logic, and any other component that affects behavior. If you store only the model ID, you cannot distinguish between two runs that used the same model but different prompts. If you store a JSON blob of config without extracting key fields, you cannot query by prompt version or group by retrieval strategy.

Each run produces results for a dataset. The dataset reference includes the dataset name, version, and hash. The hash is non-negotiable. Datasets change over time—entries are added, corrected, deprecated. If you store only the dataset name, you cannot determine whether a score drop was caused by model degradation or dataset evolution. The hash ties the result to a specific snapshot. If the dataset changed between two runs, the hashes differ, and you know the comparison is not apples-to-apples.

Within a run, each test case produces a result. The result includes the input, the output, the expected output or evaluation criteria, the pass/fail status, the scores from each metric, and any metadata the evaluator recorded—latency, token count, tool calls made, retrieval documents returned. The result also includes a reference to the evaluator version. If you update the evaluator logic—tighten a threshold, fix a bug, add a new dimension—the evaluator version changes, and results become incomparable to prior runs unless you track which version produced which result. Teams that store results without evaluator versioning cannot distinguish true model improvements from evaluator drift.

The schema also captures failure modes explicitly. A test case can fail for multiple reasons: the output was incorrect, the output was correct but violated a constraint, the system timed out, the system raised an exception, the retrieval stage returned no documents, the tool call was malformed. Storing only a boolean pass/fail loses signal. You need a failure type field and a failure detail field. When a release causes 200 test cases to fail, you need to group them by failure type to understand the root cause. If 180 failures are "incorrect output" and 20 are "timeout," the root cause is likely a behavior regression. If 180 are "timeout" and 20 are "incorrect output," the root cause is infrastructure or latency.

## Storage Backend Selection

The choice of storage backend is a function of query patterns, scale, and retention requirements. If you run 500 eval cases per pipeline execution and execute 50 pipelines per day, you produce 25,000 result records daily. Over a year, that is nine million records. If each record is 2KB, that is 18GB of data. At that scale, a relational database works. PostgreSQL handles nine million rows with proper indexing. You can query by run ID, group by model version, filter by failure type, and join with metadata tables.

If you run 50,000 eval cases per execution and execute 200 pipelines per day, you produce ten million records daily. Over a year, that is 3.6 billion records. At that scale, a relational database still works if you partition by date and archive old partitions to cold storage, but query performance becomes a concern unless indexes and partitions are carefully managed. Teams at this scale often use a columnar store or a time-series database. Columnar stores like ClickHouse or BigQuery optimize for analytical queries—grouping, aggregation, percentile calculations—at the expense of row-level updates. Time-series databases like TimescaleDB optimize for append-heavy workloads with time-based queries. Both are good fits for eval results, which are written once and queried many times.

If you need to support full-text search over outputs—finding all cases where the model mentioned a specific entity, or all cases where the failure detail contained a specific error message—you need a search index. Elasticsearch or OpenSearch provide full-text search with filtering and aggregation. Teams that use a relational database for structured queries and a search index for unstructured queries run both in parallel, writing results to both systems or streaming from the primary store to the search index.

Blob storage is not a storage backend for queryable results. Blob storage is where you archive the raw data—inputs, outputs, full traces—that is too large or too unstructured to fit in the primary store. You write a summary record to the database and link to the full data in S3 or GCS. When someone needs the full output, they fetch the blob. This keeps the database fast and prevents row sizes from balloating into multi-kilobyte records that slow down scans.

## Indexing for Query Performance

The most common query is "show me the results for this run." This requires an index on run ID. The second most common query is "show me all runs for this model version in the last 30 days." This requires a composite index on model version and timestamp. The third most common query is "show me all failures of type X across all runs this week." This requires a composite index on timestamp and failure type. If your query patterns are different, your indexes should be different. The mistake is building indexes based on what feels intuitive rather than profiling actual queries.

A query that filters by multiple dimensions—model version, dataset version, failure type, date range—needs either a composite index covering those fields or a query planner smart enough to merge multiple indexes. If your database does not support index merging efficiently, the query scans the full table and filters in memory. At a million rows, this is slow. At a hundred million rows, this times out. The solution is either to add the composite index or to denormalize the schema so the query can be answered from a pre-aggregated table.

Pre-aggregation is common for dashboards. A dashboard shows pass rate by model version over the last 90 days. Computing this from raw results requires scanning every result row, grouping by model version and day, and calculating pass rate. If you have a billion result rows, this is expensive. The alternative is a rollup table that stores daily aggregates—model version, date, total cases, passed cases, failed cases, failure type distribution. The rollup table is updated at the end of each pipeline run. The dashboard queries the rollup table, which has 90 rows per model version instead of millions. Rollup tables trade write complexity for read performance. They are worth it when dashboards are queried frequently and results are written in batches.

## Retention and Archival

Eval results do not need to live in hot storage forever. Results from a model version that was deprecated six months ago are rarely queried, but they cannot be deleted because they establish the historical baseline. The solution is tiered retention. Recent results—last 30 or 90 days—live in the primary database with full indexing. Older results are archived to cold storage—S3, GCS, or a columnar warehouse like BigQuery or Snowflake. The archive is queryable but slower and more expensive per query. Compliance-critical results are retained indefinitely. Non-critical results are deleted after a defined period, typically one to three years.

The retention policy must align with your compliance obligations. If a regulation requires you to demonstrate that every model deployment was validated, you need to retain the eval results that justified each deployment for as long as the regulation specifies. If the regulation is silent, the safe default is to retain results for the lifetime of the model plus two years. This ensures that if a model causes harm and an investigation happens years later, you have the evidence to reconstruct what was tested and when.

Archival happens automatically on a schedule. A cron job or scheduled query identifies runs older than the retention threshold, exports the results to the archive, verifies the export, and deletes the rows from the primary store. The verification step is non-negotiable. Teams that skip verification discover data loss months later when they need archived data and find the export was truncated or corrupted. The verification step reads a sample of archived rows and compares checksums or row counts to the original export manifest.

## Querying Historical Results

The most valuable queries are not "show me the latest run" but "show me the trend over time." Did the pass rate improve after the prompt change? Did latency increase after the infrastructure migration? Did a specific failure type spike after the dataset refresh? These questions require joining results across runs and plotting metrics over time. If your schema does not support efficient time-range queries, these analyses are slow or impossible.

A common pattern is the comparison query: "compare the results of this run to the baseline run from last week." The query filters results by run ID, joins them on test case ID, and computes the delta for each metric. If pass rate was 94% last week and 89% today, that is a 5-point drop. If the query also groups by failure type, you can see which failure types increased. This tells you whether the drop was caused by one systematic issue or many independent regressions.

Another pattern is the regression search: "find the run where metric X first dropped below threshold Y." This requires scanning results in reverse chronological order until the condition is met. If results are not indexed by metric value, the scan is slow. Some teams pre-compute metric ranges—minimum, maximum, percentiles—per run and store them in a summary table. The regression search queries the summary table first to narrow the window, then scans detailed results only within that window.

Queries that span dataset versions require special handling. If the dataset changed between two runs, comparing raw pass rates is misleading because the test cases are different. The correct approach is to filter to the intersection—test cases that exist in both datasets—and compare pass rate on that subset. This requires joining on test case ID and filtering to cases where the ID exists in both runs. If test case IDs are not stable across dataset versions, the intersection is empty and comparison is impossible. Stable test case IDs are not a schema nicety—they are a requirement for historical analysis.

## Enabling Analytics and Reporting

Eval results are a rich data source for understanding model behavior, team velocity, and quality trends. Analytics on eval data answer questions that operational dashboards cannot. Which types of inputs cause the most failures? Which evaluators have the highest variance? Which dataset subsets have the lowest pass rates? Which model versions performed best on which task types? These questions require grouping, filtering, and aggregation across dimensions that are not pre-indexed.

The analytics use case often drives the decision to export results to a data warehouse. A warehouse like BigQuery, Snowflake, or Redshift is optimized for ad-hoc analytical queries. You export eval results daily or hourly, and analysts query the warehouse using SQL or BI tools. The export process flattens nested structures, extracts dimensions into separate columns, and pre-joins with metadata tables. This makes the data easier to query but means the warehouse schema may differ from the operational schema.

Some teams build a unified analytics schema that combines eval results with other operational data—production logs, user feedback, incident reports. This enables cross-domain analysis. For example, "were there more user complaints during the week when eval pass rates dropped?" requires joining eval results with support ticket data. If both datasets are in the same warehouse, the join is straightforward. If they are in separate systems, the analysis requires exporting, aligning, and merging.

Reporting on eval results is not just internal. Stakeholders outside the ML team need summaries—Product needs to know whether the new feature passed quality gates, Legal needs evidence that compliance criteria were tested, Executives need to know whether the AI system meets business KPIs. These reports are not generated by querying the raw result table. They are generated from rollup tables or pre-built reports that aggregate results by time period, deployment, and success criteria. The report generation process is part of the pipeline. At the end of each run, the pipeline writes results to the database, updates rollup tables, and triggers a report generation job that produces a summary artifact—a PDF, a dashboard snapshot, a Slack message. The artifact is stored with the run metadata so it can be retrieved later.

The eval result store is not passive infrastructure. It is the memory of your AI system. Every decision you make—to deploy, to roll back, to investigate—relies on being able to ask a question and get an answer. If the schema is clean, the indexes are right, the retention is sound, and the query layer is fast, the eval store becomes the single source of truth for quality. The next question is whether the eval pipeline itself is healthy, and how you know when it is not.

---

**Next: 8.11 — Pipeline Monitoring and Alerting**
