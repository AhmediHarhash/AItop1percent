# 4.12 — Multi-Judge Ensembles and Disagreement Signals

The dashboard showed three scores for the same output. GPT-5 scored it 9 out of 10. Claude Opus 4.5 scored it 4 out of 10. Gemini 3 Pro scored it 8 out of 10. The ensemble average was 7 — a passing score under the pipeline's threshold. But the disagreement was a five-point spread, the widest the team had seen in weeks of production eval. They routed the output to human review. The human rater scored it 3 out of 10. The output contained a subtle but critical factual error that GPT-5 and Gemini 3 Pro missed entirely, while Claude correctly identified it and penalized heavily. The disagreement signal caught a failure that the ensemble average would have let through.

## Ensemble Voting Strategies in Production

A multi-judge ensemble runs multiple LLM judges on the same output and combines their scores into a single decision or score. The combination strategy determines how the ensemble behaves — whether it is conservative or permissive, whether it emphasizes consensus or outlier detection, whether it treats all judges equally or weights them by reliability.

Mean voting takes the arithmetic average of all judge scores. If three judges score an output at 8, 7, and 9, the ensemble score is 8. This strategy is simple, symmetric, and treats all judges as equally trustworthy. It works well when the judges have similar calibration quality and you want a central tendency estimate. It fails when one judge has a systematic bias — if one judge consistently scores outputs two points higher than the others, it pulls the ensemble average up, even if that judge is less reliable.

Median voting takes the middle score when judges are ranked. If three judges score at 8, 7, and 9, the median is 8. If they score at 9, 7, and 3, the median is 7. Median voting is more robust to outliers than mean voting. A single judge producing an extreme score — either very high or very low — does not shift the ensemble result as much as it would under mean voting. Use median voting when you suspect one judge may occasionally produce erratic scores but the other judges are stable.

Weighted voting assigns different weights to each judge based on measured reliability. If GPT-5 has 82 percent agreement with human labels, Claude Opus 4.5 has 88 percent, and Gemini 3 Pro has 79 percent, you might weight their scores as 0.3, 0.5, and 0.2 respectively. The ensemble score is the weighted average: 0.3 times GPT-5 score plus 0.5 times Claude score plus 0.2 times Gemini score. This strategy improves overall accuracy when calibration data clearly shows one judge is more reliable than the others. It requires maintaining calibration datasets and recalculating weights when models update.

Threshold voting requires a minimum number of judges to score above a threshold for the output to pass. If the threshold is 7 and you require at least two of three judges to score 7 or higher, an output with scores of 9, 8, and 4 passes because two judges exceed the threshold. An output with scores of 9, 6, and 5 fails because only one judge exceeds the threshold. Threshold voting is more conservative than averaging. It protects against false positives — outputs that one or two judges rate highly but that have significant flaws detected by other judges.

Veto voting gives any single judge the power to fail an output if it scores below a critical threshold. If the veto threshold is 3, and any judge scores the output at 3 or below, the output fails regardless of the other judges' scores. Veto voting is used in safety-critical tasks where a single severe flaw is unacceptable. It is highly conservative and increases false negatives — good outputs that one judge incorrectly penalizes will fail even if the other judges are correct.

The voting strategy is a policy decision that depends on the task's risk profile. If false positives are costly — if shipping a bad output causes harm — use threshold or veto voting. If false negatives are costly — if rejecting a good output wastes expensive human review time — use mean or median voting. If you have strong calibration data, use weighted voting. If you lack that data, use mean or median.

## Disagreement as an Escalation Signal

The most valuable signal from a multi-judge ensemble is not the final score. It is the disagreement between judges. When judges agree, the ensemble's confidence is high. When they disagree, the output is ambiguous, edge-case, or adversarial — exactly the cases where automated eval is least reliable and human judgment is most needed.

Disagreement is measured as the range or standard deviation of scores. If three judges score an output at 8, 8, and 7, the range is 1 and the standard deviation is 0.47. If they score at 9, 7, and 3, the range is 6 and the standard deviation is 2.49. High disagreement indicates the judges interpreted the output differently or weighted different quality dimensions differently.

A production pipeline routes high-disagreement cases to human review. Set a disagreement threshold — for example, any output where the range of judge scores exceeds 3 points, or where the standard deviation exceeds 1.5. When an output crosses that threshold, flag it for human escalation instead of accepting the ensemble's average score. The human reviewer sees the output, the three judge scores, and the reasoning each judge provided. They make the final call.

This routing strategy allows the ensemble to act as a filter. The 80 to 90 percent of outputs where judges agree are handled automatically. The 10 to 20 percent where judges disagree are escalated. This reduces human review load while ensuring that ambiguous cases receive human attention. The alternative — routing a random sample to human review — wastes human time on obvious cases and misses subtle cases where judges quietly agree on the wrong answer.

Disagreement is particularly valuable when one judge detects a flaw the others miss. In the opening story, Claude Opus 4.5 correctly identified a factual error and scored the output at 4, while GPT-5 and Gemini 3 Pro missed it and scored at 9 and 8. If the pipeline had used mean voting without disagreement routing, the output would have scored 7 and passed. Because the pipeline flagged high disagreement for human review, the error was caught. This pattern — one judge catching what others miss — is the core value proposition of multi-judge ensembles.

## Weighted Ensembles and Calibration-Driven Weights

Weighted ensembles require calibration data to determine the weights. You measure each judge's agreement with human labels on a held-out dataset. The judge with the highest agreement gets the highest weight. The judge with the lowest agreement gets the lowest weight. The weights are normalized to sum to 1.

If you have 3,000 human-labeled examples, you might allocate 2,000 for prompt engineering and 1,000 for weight calibration. You run all three judges on the 1,000 calibration examples. Measure agreement — Cohen's kappa, correlation, or binary accuracy depending on the task. GPT-5 achieves kappa of 0.81. Claude Opus 4.5 achieves 0.87. Gemini 3 Pro achieves 0.78. You convert these to weights proportional to their agreement: GPT-5 gets 0.33, Claude gets 0.36, Gemini gets 0.31.

Weighted voting improves ensemble accuracy by an average of 3 to 7 percentage points compared to unweighted mean voting, based on production data from teams using multi-judge pipelines in 2025-2026. The improvement is largest when one judge is significantly better than the others. If all judges have similar calibration quality — kappas within 0.03 of each other — weighted voting provides minimal benefit and the added complexity may not be justified.

Weights are not static. When a model updates, recalibrate. When the task changes, recalibrate. When you add a new judge to the ensemble, recalibrate all weights. Stale weights degrade performance — you may be over-weighting a judge that has drifted or under-weighting a judge that improved after a model update.

Some teams use dynamic weights that adapt based on recent performance. If the ensemble has access to a continuous stream of human-reviewed examples — outputs that were scored by judges and then reviewed by humans — it can track each judge's agreement over the most recent 500 or 1,000 examples and update weights weekly or monthly. This approach keeps weights aligned with current performance but requires infrastructure to collect ongoing human feedback and recompute weights automatically.

## When Judges Disagree: What It Means

Judge disagreement is not random. It correlates with specific output characteristics. Outputs where judges disagree are systematically different from outputs where they agree.

Disagreement is higher on edge cases. If the output is almost but not quite correct — if it answers the question but misses a nuance, if it's mostly helpful but includes a minor error, if it's mostly appropriate in tone but has one awkward phrase — judges will weigh the flaw differently. One judge penalizes it heavily. Another judges it leniently. The output lands in the boundary zone where reasonable evaluators disagree.

Disagreement is higher on subjective dimensions. If the task is evaluating tone, style, or creativity, judges trained on different datasets with different aesthetic preferences will score differently. If the task is evaluating factual accuracy or schema compliance, disagreement is lower because the criteria are more objective. When designing a multi-judge pipeline, expect higher disagreement on subjective tasks and set disagreement thresholds accordingly.

Disagreement is higher when the output exploits a judge-specific bias. If an output is verbose and includes many caveats, GPT-5 may score it highly because it was trained to reward comprehensive responses, while Claude Opus 4.5 scores it lower because it values conciseness. If an output uses casual, conversational language, Claude may score it highly for tone appropriateness, while Gemini 3 Pro scores it lower because its training data emphasized formal informational content. These bias-driven disagreements reveal that the output is optimized for one judge's preferences rather than for genuine quality.

Disagreement is higher on adversarial or jailbreak-adjacent inputs. If the output is attempting to subtly violate policy — if it provides an answer that is technically compliant but contextually inappropriate — different judges' safety filters will trigger at different sensitivities. One judge flags it. Two judges pass it. The disagreement is a safety signal that warrants human review.

When disagreement is consistently high across many outputs, the problem is not the outputs — it is the task definition or the judge prompts. If 30 percent of outputs trigger disagreement routing, the judges are not aligned on what quality means. The solution is not tuning the disagreement threshold. The solution is revisiting the rubric, improving the judge prompts, or collecting more calibration data to clarify what the judges should be optimizing for.

## Using Disagreement for Human Routing

The routing decision is binary: either the ensemble's score is accepted automatically, or the output is escalated to human review. Disagreement is the primary trigger for escalation.

Set a disagreement threshold based on the distribution of disagreement in your eval data. Run the ensemble on 5,000 to 10,000 representative outputs. Measure the range or standard deviation of scores for each output. Plot the distribution. If 85 percent of outputs have a range below 2, 10 percent have a range between 2 and 4, and 5 percent have a range above 4, set the threshold at 4. This routes the top 5 percent of disagreement cases to humans.

The threshold is a dial you tune based on human review capacity and risk tolerance. If you have abundant human review capacity, set the threshold lower — route the top 10 or 15 percent. If human review is expensive or slow, set the threshold higher — route only the top 3 to 5 percent. If the task is high-stakes, err on the side of more routing. If the task is low-stakes, err on the side of less routing.

Track what happens to routed cases. If 80 percent of high-disagreement cases are confirmed as genuinely ambiguous or flawed by human reviewers, the disagreement signal is accurate. If only 30 percent are, the threshold is too low and you're wasting human time on cases the ensemble could have handled. If 95 percent are severe problems, the threshold is too high and you're missing cases that should have been routed.

Disagreement routing is not the only escalation trigger. You can also route based on low-confidence scores — if all judges score between 5 and 6, the output is neither clearly good nor clearly bad, and human review may be warranted. You can route based on specific judge reasoning — if any judge flags a potential safety issue in its reasoning, escalate even if the scores are not highly divergent. You can route based on downstream signals — if users report an output as incorrect, escalate it for re-evaluation regardless of what the judges initially scored.

The routing logic becomes part of the eval pipeline's operating model. The ensemble is the first-pass filter. Human review is the second-pass filter. The disagreement threshold controls the flow rate between them. The goal is not zero human review — it is targeted human review on the cases that need it most.

## Ensemble Cost Management

A three-judge ensemble costs three times the per-judgment API cost of a single judge. At scale, this is significant.

For a pipeline processing 200,000 eval judgments per day, using GPT-5, Claude Opus 4.5, and Gemini 3 Pro costs approximately 4,000 dollars per day, or 120,000 dollars per month. If only 10 percent of cases are genuinely ambiguous and benefit from multi-judge evaluation, spending three times the cost on all 200,000 is wasteful.

The cost-optimized strategy is tiered judging. Use a single fast, cheap judge — GPT-5 or Gemini 3 Pro — as the first pass. If the single judge's score is unambiguous — clearly above the pass threshold or clearly below the fail threshold — accept it. If the score is near the boundary, invoke the full three-judge ensemble. This reduces ensemble invocations to 20 to 30 percent of cases while still catching ambiguous outputs.

Tiered judging requires defining boundary zones. If the pass threshold is 7, you might define the boundary zone as scores between 6 and 8. Any output the first-pass judge scores at 6, 7, or 8 triggers the ensemble. Outputs scored at 9 or 10 pass immediately. Outputs scored at 5 or below fail immediately. This reduces ensemble costs by 70 to 80 percent while maintaining most of the ensemble's accuracy improvement.

Another cost management strategy is ensemble sampling. Instead of running the ensemble on every eval case, run it on a stratified sample — 10 percent of outputs uniformly at random, or 20 percent of boundary-zone outputs. Use the ensemble scores to monitor whether the single judge is drifting. If the ensemble and the single judge agree 90 percent of the time on the sample, trust the single judge on the non-sampled cases. If agreement drops below 80 percent, expand the ensemble to more cases or switch to full ensemble evaluation until the judges realign.

Cost and accuracy trade off. Full ensemble evaluation is the most accurate and most expensive. Single-judge evaluation is the cheapest and least accurate. Tiered judging and ensemble sampling are intermediate strategies that balance cost and quality. The choice depends on your budget, your risk tolerance, and the stakes of the task.

---

Judges drift over time as models update and tasks evolve. The next subchapter covers how to detect judge drift, measure it, and trigger recalibration before drift degrades production eval quality.
