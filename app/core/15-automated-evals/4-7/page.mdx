# 4.7 — Position Bias and Verbosity Bias in LLM Judges

In October 2025, a healthcare AI team ran what should have been a straightforward A/B test. They were comparing two versions of their clinical summary generator, using Claude Opus 4 as the judge. After two weeks and 3,200 evaluations, Model A won decisively: 68% preferred over Model B. They deployed it to production. Three days later, their quality monitoring caught something disturbing. Model A was actually producing less accurate summaries than Model B — the judge had been wrong about two-thirds of the time. The team pulled the deployment and spent the weekend investigating. What they found was simple and devastating: they had always shown Model A's output first in the comparison prompt. The judge wasn't evaluating quality. It was favoring the first option it saw.

## Position Bias: The First-Answer Advantage

**Position bias** is the phenomenon where an LLM judge systematically prefers outputs shown earlier in the prompt, independent of their actual quality. It shows up strongest in pairwise comparisons — choosing between Option A and Option B — but appears in any evaluation where the judge sees multiple candidates. The bias exists across all major models in 2026, though the magnitude varies. GPT-5 models show position bias in 12 to 18% of pairwise judgments. Claude Opus 4.5 shows it in 8 to 14%. Gemini 3 Pro shows it in 15 to 22%. These are not small effects. In a close quality matchup, position bias decides the winner.

The mechanism is rooted in how LLMs process sequential information. Early tokens in the context window receive more attention weight during generation. The judge sees the first option, forms an initial impression, and that impression anchors the evaluation. When the second option appears, the model evaluates it relative to the anchor, not on absolute quality. If the second option is dramatically better, the judge notices. If it is slightly better or roughly equivalent, the first option wins. The judge is not deliberately unfair — it is exhibiting a fundamental characteristic of transformer architectures. You cannot prompt your way out of this. Instructing the judge to "ignore order" or "evaluate fairly regardless of position" reduces bias by perhaps 2 to 4 percentage points. It does not eliminate it.

Position bias compounds across your entire eval pipeline. If you always show your production model's output first and the candidate model's output second, you systematically underestimate the candidate's quality. If you always show the candidate first, you overestimate it. If your pipeline compares multiple models in a fixed order — always GPT-5, then Claude, then Llama 4 — you create a ranking that reflects presentation order as much as actual performance. Teams ship the wrong model. Teams kill good experiments. Teams optimize toward the biased signal and degrade real quality without realizing it.

## Verbosity Bias: Longer Equals Better

**Verbosity bias** is the phenomenon where LLM judges systematically prefer longer outputs, independent of whether the additional length adds value. A 400-word answer beats a 200-word answer that says the same thing. A response with five examples beats a response with two, even when the two examples are better. The bias appears across all evaluation tasks — summarization, question answering, content generation, code explanation. The judge equates thoroughness with quality, even when thoroughness is actually redundancy.

The pattern is strongest when the judge lacks a precise rubric. If your evaluation criteria say "assess helpfulness," the judge interprets length as a proxy for helpfulness. More detail feels more helpful, even when the user did not ask for detail. If your criteria say "evaluate completeness," the judge interprets length as coverage, even when the extra content is tangential. If your criteria say "rate clarity," verbosity bias is weaker but still present — the judge sometimes mistakes elaboration for explanation. The only reliable way to reduce verbosity bias is to explicitly penalize unnecessary length in your rubric. "Deduct points for repetition. Deduct points for examples that do not add new information. Prefer concise answers when they fully address the question." Even with explicit anti-verbosity criteria, the bias does not disappear. It weakens from 18-25% of judgments to 8-12%.

Verbosity bias punishes efficient models. If you fine-tune a model to be more concise — a common goal in production systems where latency and cost matter — your LLM judge rates the concise version lower, even though conciseness was the explicit objective. If you prompt-engineer for brevity, your evals tell you the shorter outputs are worse. The feedback loop drives you away from the behavior you want. Teams see this most painfully in summarization tasks. A dense, information-rich 150-word summary loses to a 300-word summary that includes filler sentences and redundant phrasing. The judge prefers the longer one because it feels more complete, even though the shorter one delivers more value per word.

## Detecting Bias in Your Evaluation Data

You detect position bias by running the same evaluation twice with the order reversed. Take 500 pairwise comparisons where Model A is shown first. Reverse the order and re-run the same comparisons with Model B shown first. If the judge is unbiased, the preference rate should flip symmetrically. If Model A won 60% of comparisons in the first run, Model B should win roughly 60% in the reversed run. If that does not happen — if Model A still wins 55% even when shown second — you have position bias. The magnitude of the bias is the difference between the expected symmetric flip and the actual flip. A 10-point gap indicates moderate bias. A 20-point gap indicates severe bias.

You detect verbosity bias by correlating judgment scores with output length. Pull 1,000 evaluation examples. Plot the judge's quality score against the word count of the output. If the correlation is positive and statistically significant — Pearson r above 0.3 — you have verbosity bias. The higher the correlation, the stronger the bias. Then segment by actual quality. Take examples where you have human labels or other ground truth. Compare short correct answers to long incorrect answers. If the judge rates the long incorrect answers higher, verbosity bias is dominating your signal. In a well-calibrated judge, length should have near-zero correlation with score after controlling for actual quality. In most production LLM judges in 2026, the correlation is 0.25 to 0.45 before mitigation.

Both biases interact with each other and with other biases. A verbose answer shown first gets a compounding advantage. A concise answer shown second gets a compounding penalty. If you do not measure both biases and correct for both, your eval system drifts further from reality over time. The drift is silent. Your dashboards show improving scores. Your actual quality is flat or declining.

## Randomizing Position and Controlling for Length

The only reliable mitigation for position bias is randomization. For every pairwise comparison, randomly decide which model's output appears first. Half the time Model A is first, half the time Model B is first. Aggregate the results across both orders. If Model A wins 65% of comparisons when shown first and 55% when shown second, the bias-corrected estimate is 60%. This approach doubles your evaluation cost — you run each comparison twice — but it produces the correct answer. Some teams try to avoid the cost by randomizing once without re-running. This reduces bias but does not eliminate it. You still get a biased sample; you just do not know which direction the bias leans.

Randomization must happen at the prompt construction layer, not at the data selection layer. If your pipeline always constructs prompts with Model A first, then shuffles the dataset, you have not randomized position. The judge still sees Model A first every single time. The randomization must inject the model outputs into the prompt template in random order per evaluation. That means your prompt template has placeholders for "first output" and "second output," not placeholders for "Model A output" and "Model B output." Your pipeline randomly assigns which model fills which placeholder.

Controlling for verbosity bias requires rubric engineering. Add explicit length-awareness criteria. "If both answers are correct, prefer the shorter one. Award no points for repetition. Award no points for examples that restate prior examples without adding information. If the answer exceeds the necessary length by more than 30%, deduct points for verbosity." These criteria do not eliminate the bias, but they create a counterweight. Your judge now has conflicting signals — one bias toward length, one criterion against length — and the result is closer to neutral. The alternative is to normalize by length during score aggregation. Calculate quality per word, not absolute quality. A 200-word answer with a score of 8 out of 10 gets a per-word score of 0.04. A 400-word answer with a score of 9 out of 10 gets a per-word score of 0.0225. Rank by per-word score. This approach works when conciseness is genuinely valuable. It fails when the task legitimately requires longer outputs — detailed explanations, comprehensive summaries, multi-step reasoning.

## The Insidious Nature of These Biases

Position bias and verbosity bias are insidious because they look like valid signal. A team sees their judge consistently prefer Model A, and they trust that preference because the judge is an advanced LLM with strong reasoning capabilities. The scores are consistent. The win rates are clear. The ranking feels definitive. The team does not think to check whether the ranking would reverse if they randomized the order or penalized verbosity. They assume the judge is doing what it was instructed to do: evaluate quality. By the time they discover the bias — usually when production metrics diverge from eval metrics — they have made weeks or months of decisions based on corrupted data.

The biases also reinforce existing team assumptions. If your team believes Model A is better, and the judge ranks Model A higher, you do not question the result. The judge confirmed what you already thought. If the judge ranked Model A higher because it always appeared first, you never find out. Confirmation bias in humans meets position bias in LLMs, and the combination is invisible until something breaks. The healthcare team from the opening case spent two weeks running their A/B test, fully confident in the results, because the results matched their prior belief that the new model was an improvement. The bias hid inside the confidence.

Both biases are stable over time. They do not fluctuate randomly. They do not disappear when you add more data. If your pipeline has position bias today, it will have position bias next month unless you redesign the pipeline. If your judge has verbosity bias today, it will have verbosity bias next month unless you re-engineer the rubric. The stability makes the biases harder to detect — there is no sudden anomaly, no spike in the metrics that triggers an alert. The bias is the baseline. You are blind to it until you deliberately test for it.

Every team running LLM-as-judge evaluation in 2026 has position bias, verbosity bias, or both. The teams that ship reliable products are the ones who measure the biases, correct for them, and validate that the correction worked. The teams that fail are the ones who assume their judge is neutral because the model is powerful. Power does not equal fairness. Capability does not equal calibration. The most capable judge in the world still prefers the first answer it sees 15% more often than the second.

---

Position bias and verbosity bias corrupt ranking, but they are at least measurable and correctable. The next failure mode — style bias — is harder to detect because it masquerades as quality judgment, and harder to fix because style and substance are entangled in ways that no rubric fully disentangles.

