# 10.9 — Reporting Cadence and Stakeholder Communication

The Tuesday morning eval review is fifteen minutes. Engineering leads from three teams sit in a conference room staring at a dashboard projected on the wall. Accuracy is 88.2%. Latency p95 is 980 milliseconds. Hallucination rate is 3.1%. Refusal rate is 6.4%. All metrics are green. Someone asks if there are any concerns. No one speaks. The meeting ends. Everyone goes back to work.

Three weeks later, user satisfaction drops by 12%. A deep-dive investigation reveals that hallucination rate ticked up from 3.1% to 4.9% over two weeks, then jumped to 7.2% in week three. The Tuesday meetings showed green because the threshold was 8%. By the time the metric crossed the threshold, production was already degraded. The postmortem asks why no one noticed the trend. The answer: the report showed a snapshot, not a story. The cadence was regular, but the communication was ineffective. The team was looking at numbers, not understanding them.

**Reporting cadence** is the rhythm of how often you generate, review, and act on eval results. **Stakeholder communication** is how you translate eval data into narratives that drive decisions. The two are inseparable. A daily report that no one reads is noise. A monthly report that arrives too late to prevent a production issue is negligence. The right cadence matches the pace of change in your system. The right communication matches the needs of each audience. When you get both right, your eval system becomes a driver of continuous improvement. When you get either wrong, it becomes a compliance ritual that no one trusts.

## Reporting Frequency and the Pace of Change

Your reporting cadence should match the pace at which your system changes and the speed at which quality can degrade. If you deploy model updates daily, you need daily eval reports. If you deploy weekly, weekly reports might suffice. If your model is stable and your data distribution drifts slowly, monthly reports might be enough. The rule is simple: you need to see problems before they reach users. If your cadence is too slow, you will always be reacting, never preventing.

A consumer AI company deploys prompt changes several times per day and model updates twice per week. They run automated evals on every commit. Those evals generate a report within 20 minutes. Engineering reviews the report before merging. If metrics drop, the commit does not land. This near-real-time cadence catches regressions before they reach production. They also generate a daily rollup report that aggregates all commits from the prior 24 hours and highlights any trends. Engineering leads review this report every morning. Finally, they generate a weekly executive summary that goes to product and leadership. Three cadences, three audiences, three levels of detail.

The daily rollup is narrative-driven. It does not just show numbers. It says "Accuracy held steady at 88%. Latency improved by 40 milliseconds due to prompt optimization in commit 7a3f. Hallucination rate increased slightly from 3.1% to 3.4%, within normal variance. Refusal rate dropped from 6.4% to 5.8% after updating the content policy classifier. No concerns." The narrative gives context. It explains why metrics moved. It identifies commits that drove changes. It calls out concerns or confirms stability. An engineering lead can read this in 90 seconds and know the system state.

The weekly executive summary is even more compressed. It highlights only the most significant movements, the biggest risks, and the key decisions needed. "Overall quality stable. Latency improved 8% week-over-week due to infrastructure changes. One new failure mode detected in financial advice queries—team is investigating and will report back by Thursday. No launch blockers." Executives do not need to know that accuracy was 88.2% versus 88.4%. They need to know whether the system is improving, stable, or degrading, and whether any decisions are required. The summary respects their time while keeping them informed.

## Engineering Deep-Dives and the Diagnostic Report

Engineers need depth. When they review eval results, they want to see not just the top-line metrics but the breakdowns, the trends, the edge cases, and the failure examples. A shallow report tells them accuracy dropped. A deep report tells them accuracy dropped specifically for multi-turn conversations involving financial calculations, that the issue started after commit 9f2e, and that 14 of the 18 failures involve currency conversion edge cases. The deep report is actionable. The shallow report is frustrating.

Engineering deep-dive reports include metric breakdowns by category, intent, cohort, and time window. They show trends over the last 30 days, not just the last snapshot. They highlight metrics that crossed thresholds or moved more than expected. They include sample failures with full context: the input, the output, the expected output, the eval score, and the reason for failure. They link to the commit or model version that introduced the change. They surface correlations: "Latency and hallucination rate both increased after switching to GPT-5-mini on Friday."

The format is usually a combination of charts, tables, and annotated samples. A time-series chart showing accuracy over 30 days with confidence intervals. A table showing accuracy by category, with deltas from the prior week. A section titled "Top 10 Failures This Week" with the actual examples. Another section titled "Metrics That Moved" with explanations for why. The report is long—sometimes five or six pages—but engineers read it because it contains the information they need to debug, prioritize, and improve.

Some teams generate these deep-dive reports automatically after every eval run and post them to a Slack channel or a wiki page. Other teams generate them on demand when a metric degrades or when preparing for a launch. Either way, the report is designed for engineers by engineers. It assumes technical fluency. It does not explain what accuracy means. It assumes you know. It focuses on what changed, why it changed, and what to do about it.

## Executive Summaries and the Narrative of Progress

Executives do not read six-page reports. They need a summary that communicates system health, movement, risk, and decisions in two or three paragraphs. The summary is not a data dump. It is a narrative. It tells a story about whether the system is getting better, staying stable, or showing warning signs. It highlights progress on strategic initiatives. It flags risks that require executive attention. It makes asks when decisions are needed.

A strong executive summary follows a structure. Start with overall system health: "The model performed well this week. All tier-one metrics remain above quality thresholds. User satisfaction is stable at 4.3 out of 5." Then highlight key movements: "We improved response specificity by 6 percentage points by refining our prompt architecture. This change is correlated with a 4% increase in task completion rate." Then flag concerns: "We observed a slight increase in hallucination rate for medical queries. Engineering is investigating and will provide an update by end of week." Finally, make asks if needed: "We recommend approving the prompt update for production deployment. No additional stakeholder approval required."

The summary uses plain language. It avoids jargon. It does not say "p95 latency regressed by 120ms due to inefficient token sampling in the decoding layer." It says "response time slowed slightly this week due to a model configuration issue, which the team has already fixed." It translates technical reality into business impact. It respects the executive's time while keeping them informed enough to make decisions or escalate concerns.

Some teams include a single annotated chart in the executive summary: a trend line showing the most important metric over the last eight weeks, with a confidence band and key events annotated. "Accuracy improved from 84% to 88% over six weeks. Dip in week 4 was due to infrastructure outage, recovered immediately. Trend is positive." The chart makes the narrative visual. Executives can see progress at a glance and read the summary for context.

## Stakeholder-Specific Reporting and the Multi-Audience Problem

Different stakeholders care about different things. Engineering cares about accuracy, latency, failure modes, and technical debt. Product cares about user satisfaction, feature readiness, and launch timelines. Legal cares about compliance risks, refusal rates, and content policy adherence. Finance cares about cost per query, infrastructure spend, and unit economics. Trust and Safety cares about harmful outputs, false positives on safety filters, and adversarial robustness. You cannot send the same report to all of them.

Stakeholder-specific reporting means you generate different views from the same underlying data. The engineering report shows technical metrics and diagnostic details. The product report shows user-facing quality metrics and correlations with satisfaction. The legal report shows compliance metrics, policy adherence rates, and examples of borderline cases. The finance report shows cost metrics, token usage trends, and cost-per-successful-query. The Trust and Safety report shows harmful content detection rates, false positive rates, and adversarial test results.

These reports share the same eval data, but they emphasize different dimensions. A single eval run might produce a 0.3% harmful content rate, which appears as a footnote in the engineering report, a top-line number in the Trust and Safety report, and does not appear at all in the finance report. The reporting system knows which metrics each audience cares about and surfaces those metrics prominently.

Some teams use templated reports with conditional sections. The template includes blocks for every possible metric. When generating a report for Trust and Safety, the system includes the harmful content section and excludes the latency section. When generating for engineering, it includes everything. The template approach scales. You maintain one reporting pipeline and configure it per audience. The alternative—manually creating separate reports—does not scale and leads to inconsistency.

## The Report That Drives Action

A report that does not drive action is waste. The goal is not to inform—it is to enable better decisions. Every report should make it clear what the reader should do next. If metrics are green, the action is "approve deployment" or "continue current work." If a metric is degraded, the action is "investigate root cause by Thursday" or "pause deployment pending fix." If a metric improved, the action is "consider expanding this approach to other use cases." The report should never leave the reader wondering what to do.

This requires explicit recommendations. At the end of the engineering deep-dive: "Recommendation: Fix the currency conversion edge cases before launching the financial advice feature. Estimated effort: 3 engineer-days." At the end of the executive summary: "Recommendation: Approve deployment to production. All quality gates passed. Monitor user satisfaction for two weeks post-launch." At the end of the Trust and Safety report: "Recommendation: Update content policy classifier to reduce false positives on medical advice. Current false positive rate of 4.2% is above our 3% threshold."

The recommendations are data-driven but not passive. They do not say "metrics suggest consideration of potential improvements." They say "fix this now" or "ship it" or "investigate further before deciding." The clarity reduces decision paralysis. Stakeholders trust the report because it does not just present data—it interprets data and proposes next steps. Over time, this trust makes the report essential. People start structuring their decision-making around it.

## Communicating Uncertainty and Building Trust

Reports should communicate uncertainty as clearly as they communicate results. If a metric improved but the confidence interval is wide, the report says so: "Accuracy appears to have improved from 87% to 89%, but the sample size was only 200 examples, so the result is not statistically significant. We are running a larger eval to confirm." If a metric is based on a new eval set that has not been validated, the report flags it: "First week using the updated eval set. Trends may not be comparable to prior weeks." If production correlation for a metric is weak, the report mentions it: "Refusal rate improved, but historical correlation with user satisfaction is low, so impact on users may be minimal."

This level of honesty builds trust. Stakeholders know that the report is not spinning the data. It is telling the truth, including the parts that are uncertain. When you later report a confident result, they believe it. Teams that hide uncertainty or oversell weak results lose credibility fast. One overstated improvement followed by a production regression, and stakeholders stop trusting the eval system entirely. Transparency is not optional—it is the foundation of trust.

Some reports include a confidence section at the end: "Data quality notes for this week: All metrics based on 1,500+ examples. Confidence intervals are tight. One new eval category added, which represents 8% of total examples—treat trends in this category as preliminary." This section takes 30 seconds to read and prevents misinterpretation. It also signals rigor. Stakeholders appreciate that the team is tracking data quality and communicating it proactively.

## Automated Reporting Pipelines and the Human Review Gate

Most of the reports described here are generated automatically. After every eval run, the system produces the engineering deep-dive report, posts it to Slack, and files it in a wiki. At the end of each day, it produces the daily rollup and emails it to engineering leads. At the end of each week, it produces the executive summary and stakeholder-specific reports and sends them to the appropriate distribution lists. Automation ensures consistency, speed, and coverage. No one forgets to send the report. No one has to spend two hours formatting charts. The pipeline does it.

But automation is not enough. Every critical report should pass through a human review gate before it reaches executives or external stakeholders. An engineer or eval lead reads the automated summary, checks the numbers, and adds context if needed. "The automated summary says accuracy dropped 2 points, but that drop was due to an intentional expansion of our eval set to include harder examples, not a regression. Adjusting the summary to clarify." The human review catches misinterpretations, adds context, and ensures the narrative matches reality.

For internal engineering reports, the human review gate is lighter—maybe just a quick scan to confirm nothing looks broken. For external stakeholder reports or board-level summaries, the review is thorough. Someone senior reads the full report, verifies the claims, and approves it before it goes out. This review prevents embarrassment. It also prevents incorrect decisions based on data the report misrepresented.

The teams that combine automated generation with human review gates get the best of both worlds: the speed and consistency of automation, and the judgment and context of human expertise. The reports go out on time, every time, and they are accurate, nuanced, and actionable. That combination turns reporting from a compliance burden into a competitive advantage.

When your reports consistently surface the right information at the right time to the right people, you create the conditions for faster iteration and smarter decisions—but only if your eval system is resilient to the most predictable threat: people gaming the metrics to make the numbers look good.

