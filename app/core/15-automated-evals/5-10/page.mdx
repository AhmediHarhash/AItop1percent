# 5.10 â€” Expected Tool Call Verification

In September 2025, a customer support automation platform passed every evaluation metric the team had built. Response quality scored above 0.90. Latency stayed under 400 milliseconds. User satisfaction ratings climbed to 87%. The agent answered questions correctly, matched the brand tone, and escalated appropriately. Production launched to 40,000 daily users. Within six days, the operations team discovered the agent had issued 2,300 refunds totaling $180,000 to customers who never requested them. The agent reasoned correctly about customer frustration, selected the refund tool correctly, but passed arguments the tool accepted without the business logic constraints the team assumed existed. The agent treated "customer expressed dissatisfaction" as sufficient justification for issuing refunds. The eval suite verified reasoning quality, output tone, and escalation decisions. It never verified that the tool calls matched what the business actually wanted the agent to do.

Tool calls are structured actions. The agent selects a function, passes arguments, and triggers external behavior. When tools move money, modify records, send communications, or change system state, correctness is not subjective. There is a right tool to call, with right arguments, in the right sequence. Reference-based evaluation for tool use means defining the expected tool behavior as ground truth and verifying the agent's actions match. This is not optional for production agents. This is the difference between an agent that helps users and an agent that costs you six figures in unintended refunds.

## Tool Selection as Ground Truth

The simplest reference check is tool identity. Given input, which tool should the agent call? A customer asks for their order status. The agent should call `get_order_status`, not `cancel_order`. A user reports a login issue. The agent should call `reset_password_link`, not `delete_account`. A support query mentions a broken feature. The agent should call `create_support_ticket`, not `apply_account_credit`. The expected tool is the ground truth. The actual tool is the prediction. Match or mismatch.

Tool selection verification catches category errors. The agent that calls a write tool when it should call a read tool. The agent that calls a deletion tool when it should call a retrieval tool. The agent that calls an external API when it should call an internal lookup. These are not reasoning failures. The agent understood the user intent. It selected the wrong action. Traditional output-based evals never see this. The agent's text response looks perfect. The tool call logs show the mistake. Reference-based tool verification makes this visible before production.

Your ground truth set for tool selection includes the input, the expected tool name, and optionally the reasoning for why that tool is correct. For 200 test cases, you define 200 expected tool calls. The eval compares actual to expected. Exact match is binary: correct tool or wrong tool. Partial credit does not apply. If the agent calls `issue_refund` when the reference says `create_support_ticket`, the eval fails. The consequence is not a slightly lower score. The consequence is the agent taking the wrong action in production. Tool selection ground truth is the hardest boundary in agent evaluation.

## Argument Verification Against Expected Values

Tool selection alone is insufficient. The agent calls the right tool with the wrong arguments. A refund tool that accepts any amount. A discount tool that accepts any percentage. A notification tool that accepts any recipient list. The agent selects `issue_refund` correctly. It passes an amount of 5000 when the reference specifies 50. It passes a user ID that does not match the customer in the conversation. It passes a reason code the business does not recognize. The tool executes. The refund processes. The eval never noticed because it only checked tool name.

Argument verification treats each parameter as a ground truth field. The expected tool call includes tool name, argument names, argument values, and argument types. For a refund scenario, the reference specifies tool `issue_refund`, argument `user_id` with value `user_12849`, argument `amount` with value 50.00, argument `currency` with value USD, argument `reason` with value `damaged_product`. The agent's tool call must match all five. Partial match is not acceptable. Four out of five correct means the agent issued a refund to the wrong user, or for the wrong amount, or with the wrong currency, or with a reason the downstream system rejects. Any single mismatch is a failure.

Argument types matter as much as values. The tool expects an integer user ID. The agent passes a string. The tool expects a float amount. The agent passes an integer. The tool expects an ISO date. The agent passes a human-readable date string. Some tools coerce types silently. Others reject the call. Others accept the call and behave incorrectly downstream. Your eval must verify type correctness explicitly. The reference defines not just the value but the type. The comparison checks both. A string `50` does not match an integer 50. A date `2026-01-15` does not match a timestamp 1705276800. Type mismatches cause silent failures in production that output-based evals never detect.

## Tool Call Ordering and Sequence Constraints

Single-tool scenarios are the easy case. Multi-tool scenarios require sequence verification. The agent must call tool A before tool B. It must call tool C only if tool A succeeded. It must call tool D with arguments derived from tool B's response. The correct sequence is the ground truth. Any deviation is a failure, even if each individual tool call is correct in isolation.

A password reset flow requires three tools in sequence. First, `verify_user_identity` with email and security answer. Second, `generate_reset_token` with the user ID returned from verification. Third, `send_reset_email` with the token and the user's confirmed email address. If the agent calls `send_reset_email` before calling `verify_user_identity`, the reset link goes to an unverified recipient. If the agent calls `generate_reset_token` with a hardcoded user ID instead of the value returned from verification, the token belongs to the wrong user. If the agent calls all three tools in parallel, the token does not exist when the email sends. Each individual tool call might match the expected tool name and arguments. The sequence is wrong. The outcome is wrong. The eval must verify order.

Your reference for multi-tool scenarios defines a sequence: step one, step two, step three. Each step includes the expected tool, expected arguments, and the dependency on prior steps. The eval verifies the agent's tool calls match the sequence in order. Out-of-order execution fails the eval. Missing steps fail the eval. Extra steps fail the eval unless the reference explicitly allows optional tools. Conditional branching requires multiple reference sequences. If the user's account is locked, call `unlock_account` then `verify_user_identity`. If the account is not locked, call `verify_user_identity` directly. The eval must recognize both valid paths and reject paths that skip required conditionals.

## Partial Match Versus Exact Match Semantics

Some tool arguments allow flexibility. A notification message can vary in wording as long as it contains required information. A search query can include optional filters. A log entry can have a freeform text field. Exact match verification rejects any deviation. Partial match verification allows controlled variation. The choice depends on the argument's role and the tool's tolerance for variance.

Exact match applies to identifiers, amounts, and enums. User IDs must match exactly. Order IDs must match exactly. Refund amounts must match exactly. Currency codes must match exactly. Status values must match from a closed set. These fields have no room for paraphrase or approximation. The reference specifies `user_12849`. The agent passes `user_12849`. Match. The agent passes `user_12850`. Fail. The agent passes `12849` as an integer when the reference specifies a string. Fail. Exact match verification is binary. The value is correct or it is not.

Partial match applies to freeform text, arrays with flexible membership, and optional fields. A customer notification must include the order number and the expected delivery date. The exact phrasing can vary. The reference specifies the required elements as semantic constraints, not literal strings. The eval checks that the agent's message contains the order number and mentions the delivery date. It does not require word-for-word match. An array of search filters might allow any subset of valid filters. The reference specifies the valid set. The agent can include all, some, or none. The eval checks that every filter the agent includes exists in the valid set. It does not require the agent to use all filters or a specific subset.

Optional fields create a third category. The tool accepts a `notes` argument but does not require it. The reference may specify an expected note, or may leave it unspecified. If the reference includes a note, the eval checks for match using exact or partial semantics depending on field type. If the reference leaves the field unspecified, the eval allows the agent to include any value or omit the field entirely. Marking fields as optional in the reference prevents false negatives when the agent makes reasonable choices the reference author did not anticipate.

## When Tool Flexibility Is Acceptable

Strict tool call verification assumes one correct answer. Some scenarios have multiple valid tool sequences. A user asks to update their email address and their phone number. The agent can call `update_email` then `update_phone`, or call `update_phone` then `update_email`, or call `update_contact_info` with both fields in a single call. All three sequences achieve the same outcome. Exact sequence matching rejects two of the three. Flexible matching accepts any sequence that produces the correct final state.

Flexible tool verification replaces sequence constraints with outcome constraints. The reference defines the required end state: email updated, phone updated, no other fields modified. The eval checks that the agent's tool calls, in whatever sequence, produce that state. This requires simulating tool execution or defining equivalence classes of valid tool sequences. A two-tool update has two valid sequences. A five-tool workflow might have twenty valid sequences. Enumerating all sequences as separate references does not scale. Defining the outcome and verifying the agent's sequence produces it does scale.

Tool choice flexibility allows multiple tools that achieve the same goal. A user asks for recent orders. The agent can call `get_recent_orders` with a default limit, or call `search_orders` with a date range filter, or call `get_user_activity` and extract orders from the response. All three produce the correct data. The reference defines the required information, not the required tool. The eval checks that the agent's chosen tool, with its chosen arguments, retrieves the required information. This requires understanding tool semantics. It requires knowing that `get_recent_orders` and `search_orders` with appropriate filters are equivalent. It requires either manual equivalence definitions or tool behavior simulation.

Flexibility has limits. The agent cannot call a tool that modifies state when the reference requires a read-only operation. The agent cannot call an external API when the reference requires internal data. The agent cannot call a deprecated tool when current alternatives exist. Flexibility applies within a bounded set of acceptable tools and sequences. The reference defines the bounds. The eval enforces them. Too strict, and you reject valid agent behavior. Too flexible, and you accept incorrect tool use that happens to produce the right output by accident.

## Tool Calls as Structured Ground Truth

Tool call verification is the highest-fidelity form of reference-based evaluation for agents. Output-based evals check what the agent said. Tool-based evals check what the agent did. Reasoning evals check how the agent thought. Tool call evals check whether the agent's actions match the actions a human expert would take given the same input. This is direct alignment measurement. The ground truth is not a text string. It is a structured action specification.

Tool calls serialize as structured data. JSON representations of function name, arguments, and metadata. The reference is a structured expectation in the same format. The comparison is deterministic. No embedding similarity. No LLM judge. No threshold tuning. The tool call matches the reference or it does not. The argument value matches or it does not. The sequence matches or it does not. Structured evaluation eliminates the ambiguity that plagues text-based reference matching. It also eliminates the flexibility. Structured ground truth works when there is one correct action. It fails when multiple actions are acceptable.

For production agents, tool call ground truth is not optional. The agent that summarizes documents can tolerate output variation. The agent that processes refunds, modifies user accounts, sends legal notifications, or triggers financial transactions cannot. The cost of a wrong tool call is orders of magnitude higher than the cost of a suboptimal text response. Your eval pipeline must verify tool correctness with the same rigor as a compiler verifies type correctness. The reference defines the expected action. The eval ensures the agent's action matches. If the eval passes, the tool call is safe to execute. If the eval fails, the agent does not deploy.

Tool call verification extends naturally to multi-step agents, where each step involves tool use. The ground truth is a sequence of expected tool calls. The eval verifies each step in sequence. It checks dependencies between steps. It ensures arguments in step three derive from results in step two. It ensures the agent does not skip required steps or execute steps in unsafe order. Multi-step tool verification is the foundation of reliable agent automation. Without it, you are deploying agents that execute correct reasoning but take wrong actions. The user never sees the reasoning. They see the action. Get the action right, and the reasoning quality is irrelevant. Get the action wrong, and no amount of reasoning correctness saves you.

When your ground truth includes expected tool calls, and your eval verifies the agent's actual tool calls match in name, arguments, type, and sequence, you have built the verification layer that separates production-ready agents from prototypes that looked good in demos.

---

But tool verification assumes one correct answer per test case. Many scenarios have multiple valid responses, multiple correct tool sequences, or multiple acceptable outputs. How do you evaluate when the reference set must include not one ground truth, but several? That is the multi-reference evaluation problem, and it defines the next layer of reference-based eval design.
