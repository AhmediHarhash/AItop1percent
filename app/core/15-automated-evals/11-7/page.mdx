# 11.7 — Prompt Leakage into Evals

In October 2025, a healthcare documentation startup celebrated a major milestone. Their clinical summarization model scored 94% on their automated eval suite across 1,200 test cases. The team pushed to production with confidence. Within two weeks, customer complaints flooded in. The summaries were adequate but inconsistent — nothing like the quality the eval scores suggested. When the team analyzed production transcripts against their eval set, they found the problem. Every eval case followed the exact structure of the few-shot examples in their system prompt. Real clinical conversations did not. The eval was testing whether the model could match the prompt format, not whether it could summarize real medical conversations. They had built an eval set contaminated by their own prompt engineering. Eval scores dropped to 71% when measured against actual user inputs.

**Prompt leakage** is what happens when the artifacts of prompt design — few-shot examples, format specifications, style guidelines, edge case handling — bleed into the eval process. The eval cases become proxies for the prompt itself rather than representatives of production inputs. The model performs well because the eval is testing prompt adherence, not task competence. Scores look strong in CI/CD. Quality collapses in production. Prompt leakage is one of the most common causes of eval-production divergence, and it is invisible until you compare eval inputs to real user traffic.

## The Three Forms of Contamination

Prompt leakage manifests in three distinct patterns. **Source contamination** occurs when teams build eval sets from the same examples they use for few-shot demonstrations. A customer support team building a ticket classification model uses twenty hand-picked support tickets as few-shot examples in their prompt. Two months later, they expand the eval set by pulling fifty more tickets from the same queue, during the same time period, from the same support categories. The eval set now contains tickets structurally identical to the few-shot set. The model scores 96% because it is matching learned patterns from the prompt, not generalizing to new inputs.

**Format contamination** happens when eval cases follow the exact input format specified in the prompt, while production inputs do not. A legal contract analysis system includes a prompt instruction: "Contracts will be provided in markdown format with section headers clearly delineated." The eval team builds 800 test cases by converting PDF contracts to markdown with clean headers. Production users upload scanned PDFs with inconsistent formatting, handwritten annotations, and merged clauses. The eval passes because the inputs match the prompt specification. Production fails because real contracts do not.

**Style contamination** emerges when eval inputs mirror the polished, structured style of prompt examples while production inputs are messy, incomplete, or ambiguous. A financial advisory chatbot's prompt includes three carefully crafted example questions: "What is the optimal asset allocation for a moderate-risk portfolio with a 10-year horizon?" The eval set contains 600 similarly polished questions. Real users ask: "im 34 should i buy more stocks or bonds idk." The eval measures performance on well-formed inputs. Production delivers performance on real human speech. The gap can be thirty percentage points or more.

## How Teams Create the Leak

Prompt leakage is not intentional. It is a byproduct of workflow proximity. The team responsible for prompt engineering is often the same team responsible for eval set development. They work from the same source data, the same user research, the same edge case catalog. When they build the prompt, they select the clearest, most representative examples. When they build the eval set, they select from the same pool. The eval set becomes a test of whether the model can replicate the prompt's examples, not whether it can handle production diversity.

The pattern accelerates when teams iterate on prompts using eval performance as the signal. A prompt engineer adds a new few-shot example to handle a specific failure mode. Eval scores improve. The team assumes the model generalized. In reality, the eval set contained cases similar to the new example, and the model learned to match that pattern. The improvement is real but narrow. When production traffic includes variations the eval set did not capture, the model regresses.

The healthcare documentation team that opened this subchapter traced their contamination back to a single shared resource: a library of 400 anonymized clinical transcripts used for both prompt development and eval case generation. The few-shot examples in the prompt were drawn from the twenty highest-quality transcripts in that library. The eval set was sampled from the remaining 380. Both sets shared the same vocabulary distribution, the same conversation structures, the same absence of real-world noise. The model learned to summarize conversations that looked like the library. It struggled with conversations that did not.

## The Symptom: Eval-Production Divergence

The signature of prompt leakage is a large gap between eval scores and production quality metrics. Eval scores are stable and high. Production scores are lower and more volatile. When teams investigate, they find that the eval set has lower input variance than production traffic. Every eval case is well-formed, complete, and unambiguous. Production cases include typos, incomplete sentences, mixed languages, formatting errors, and ambiguous intent.

A fintech startup building a document extraction model scored 92% accuracy on their eval set of 1,500 bank statements. Production accuracy was 68%. The eval set contained only PDFs generated directly from banking software — clean fonts, consistent layouts, machine-readable text. Production users uploaded scanned statements, photographed statements, and statements exported from legacy systems with rendering errors. The eval set tested a narrow slice of the input distribution. Production covered the full distribution. The model was optimized for the narrow slice.

The divergence is often invisible in aggregate metrics. Teams look at overall pass rates and see strong performance. When they segment by input characteristics — format type, input length, vocabulary diversity, structural complexity — they see that eval cases cluster in a narrow band while production cases span a wide range. The model performs well in the band. It performs poorly outside it. Prompt leakage creates a model that is overfit to the eval set's distribution, which is itself overfit to the prompt's distribution.

## Detection Through Distribution Comparison

The most reliable detection method is comparing the statistical distribution of eval inputs to the distribution of production inputs. For text-based tasks, this means measuring vocabulary overlap, sentence length distribution, punctuation density, capitalization patterns, and structural features like paragraph count or header presence. For structured data tasks, this means measuring field completeness, value range distribution, and schema adherence rates. If the eval set has consistently higher quality metrics than production traffic, prompt leakage is likely.

A customer support chatbot team built a distribution comparison dashboard that ran weekly. It measured the percentage of eval questions that included proper punctuation versus production questions. It measured average sentence length. It measured the percentage of questions that followed the subject-verb-object structure versus fragmented speech. In the first analysis, they found that 94% of eval questions used proper punctuation compared to 41% of production questions. Average eval sentence length was 18 words. Average production sentence length was 11 words. The eval set was testing formal written English. Production was conversational speech. They rebuilt the eval set to match production distributions and scores dropped from 89% to 74%. The new score reflected actual user experience.

Distribution comparison does not require sophisticated tooling. A simple script that samples 200 eval inputs and 200 production inputs, then calculates summary statistics, is often sufficient. The goal is to spot systematic differences. If eval inputs are consistently longer, cleaner, more structured, or more topically focused than production inputs, the eval set is not representative. Prompt leakage is the most common cause of non-representative eval sets, because prompts are designed to demonstrate ideal inputs.

## Prevention: The Prompt-Eval Firewall

The organizational fix is separation. The team that develops prompts should not be the team that develops eval sets. The team that selects few-shot examples should not have access to the eval case library. The data sources used for prompt engineering should be distinct from the data sources used for eval case generation. This separation is a **prompt-eval firewall** — a structural barrier that prevents contamination by making it impossible for the same inputs to appear in both contexts.

A large enterprise AI team enforced this firewall through role-based access control. Prompt engineers had read access to a curated library of 500 high-quality examples. Eval engineers had read access to a separate repository of 10,000 production samples. Neither team could access the other's data. When prompt engineers needed new examples, they requested them from a data operations team that sampled from a third, isolated source. When eval engineers needed production data, they pulled directly from anonymized production logs. The two workflows never intersected. Prompt leakage incidents dropped from four per quarter to zero.

The firewall also applies to iterative workflows. When teams run experiments to improve prompts, they should evaluate those experiments on a holdout set that is completely separate from the eval set used in CI/CD. If prompt optimization uses the same eval set as CI/CD monitoring, the prompt will overfit to that set. The eval will pass. Production will not improve. The solution is a three-tier data split: a prompt development set used only during prompt iteration, an eval set used in CI/CD, and a final holdout set used only for pre-deployment validation.

## Remediation When Contamination Is Detected

When prompt leakage is discovered, the only fix is rebuilding the eval set. Patching is not sufficient. Adding a few diverse cases to a contaminated set does not remove the contamination — it just dilutes it. The model will still perform artificially well on the majority of cases that match the prompt's structure. The eval set must be rebuilt from scratch using production data or synthetic data that mirrors production distributions.

The healthcare documentation team rebuilt their eval set by sampling 1,500 clinical conversations directly from production logs, stratified by conversation length, specialty, and patient demographic. They excluded any conversation that had been used in prompt development. They measured the new set's distribution against six months of production traffic and confirmed alignment. Eval scores on the new set dropped to 71%, matching production quality. The team used the new baseline to identify actual model weaknesses. Over the next quarter, they improved the model through targeted fine-tuning and better retrieval. Eval scores rose to 84%. Production scores rose to 83%. The eval was now predictive.

Rebuilding is disruptive. It invalidates historical trend data. It resets the baseline. It often reveals that the model is weaker than the team believed. But the alternative is worse. Operating with a contaminated eval set means making decisions based on false signals. Teams optimize for metrics that do not reflect user experience. They ship changes that improve eval scores but degrade production quality. They lose trust in their eval system, which leads to manual testing, which does not scale. Rebuilding the eval set is the only path back to a trusted, automated eval pipeline.

## The Ongoing Vigilance Requirement

Prompt leakage is not a one-time risk. Every time the prompt changes, the risk resurfaces. A team adds a new few-shot example to handle a production edge case. If that edge case also appears in the eval set, the eval becomes easier. The model's score improves, but its actual capabilities do not. Vigilance requires periodic distribution checks, regular eval set refreshes, and organizational discipline to maintain the prompt-eval firewall.

A financial services AI team instituted a quarterly eval audit. Every three months, they sampled 300 production inputs and compared their distributions to the current eval set. If vocabulary overlap exceeded 70%, or if structural similarity exceeded 80%, they flagged the eval set for refresh. Over two years, they refreshed the eval set four times. Each refresh revealed drift — either the prompt had evolved to cover new patterns, or production traffic had shifted to include new formats. The audits kept the eval set aligned with reality. Eval scores remained within five percentage points of production scores, quarter over quarter.

Prompt leakage is the silent killer of eval pipeline trust. It is not dramatic. It does not crash systems. It just makes every score unreliable. The fix is organizational: separate the teams, separate the data, and measure the distributions. When the eval set reflects production reality, the scores mean something. When it reflects the prompt's examples, the scores mean nothing. The difference is the foundation of a functioning automated eval pipeline.

Your eval system's next single point of failure is architectural: relying on one judge model to evaluate every output, regardless of task complexity or domain requirements.

