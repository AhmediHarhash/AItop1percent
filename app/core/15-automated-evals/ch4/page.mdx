# Chapter 4 — LLM-as-Judge Evaluation

Using one model to evaluate another is the most powerful and most dangerous technique in automated evaluation. Powerful because LLM judges can assess semantic quality, factual accuracy, and rubric adherence in ways rules and heuristics cannot. Dangerous because LLM judges share the same failure modes as the systems they evaluate — they hallucinate, they have biases, and they can agree with themselves in ways that create false confidence. This chapter covers the full landscape of LLM-as-judge evaluation: how to select judge models, how to design rubrics that produce consistent judgments, how to detect the self-agreement trap and position bias, how to use cross-model judges to reduce systematic errors, and how to catch rubric hacking before your production model learns to game your eval system.

---

- 4.1 — Model-Based Evaluation: Principles and Pitfalls
- 4.2 — Judge Model Selection: Capability vs Cost vs Latency
- 4.3 — Rubric Design for LLM Judges
- 4.4 — Pointwise vs Pairwise vs Reference-Based Judging
- 4.5 — Prompt Engineering for Consistent Judgments
- 4.6 — The Self-Agreement Trap: When Judges Agree with Themselves
- 4.7 — Position Bias and Verbosity Bias in LLM Judges
- 4.8 — Style Bias vs Substance: When Judges Reward Form Over Content
- 4.9 — Reference Leakage: When Judges Over-Credit Answer Structure
- 4.10 — Cross-Model Judges: Using Different Families to Reduce Bias
- 4.11 — Judge Calibration Against Human Labels
- 4.12 — Multi-Judge Ensembles and Disagreement Signals
- 4.13 — Judge Drift: When Model Updates Break Your Evals
- 4.14 — Rubric Hacking: When Models Learn to Game the Judge

---

*An LLM judge that agrees with your production model 98% of the time might be validating quality — or it might be sharing the same blind spots. The difference matters.*
