# 9.6 — Ground Truth Evolution and Eval Staleness

Most teams assume ground truth is static. They write reference answers in January, build an eval suite around them, and run that suite for eighteen months. The ground truth sits in version control, unchanged, while the world moves on. Policies update. Product features launch. Legal guidance shifts. Competitive standards rise. Customer expectations evolve. By month six, the ground truth represents what was correct in the past, not what is correct today. By month twelve, the eval suite actively penalizes the model for following current policy and rewards it for citing deprecated documentation. The team wonders why production quality degrades even as eval pass rates climb. The eval became a test of historical correctness, not current correctness. Ground truth fossilized while reality kept moving.

Ground truth is not a fixed set of facts. It is a snapshot of correctness at a moment in time, and that moment expires. **Ground truth evolution** is the process by which reference answers, policy documents, ideal outputs, and correctness definitions change in response to external and internal forces — regulation updates, product roadmap shifts, competitive pressure, user feedback, domain knowledge advancement, and organizational priority changes. Every ground truth artifact has a shelf life. Some expire in weeks. Some last for years. The challenge is detecting when expiration happens and refreshing before staleness causes eval drift, misaligned incentives, or active harm.

## How Ground Truth Becomes Stale

The most common cause of staleness is **policy and documentation updates**. A healthcare AI references treatment guidelines from 2024. In early 2025, new clinical evidence emerges and guidelines update. The model learns the new guidelines through fine-tuning or RAG updates. The eval suite still compares outputs to the 2024 version. The model correctly cites 2025 guidelines. The eval marks it wrong. The team sees eval pass rate drop from 91% to 78% and assumes the model regressed. In reality, the model improved and the eval ossified. Policy staleness is particularly dangerous in regulated industries where outdated ground truth can lead to shipping a model that passes eval but violates current regulatory requirements.

**Product feature launches** invalidate ground truth by changing what the system is capable of. The ground truth was written when the product could only summarize documents. Three months later, the product adds citation extraction. High-quality outputs now include citations. The ground truth does not. The eval penalizes models that include citations because the reference answers do not have them. The team either rewrites the model to match outdated ground truth or ignores eval failures, eroding trust in the eval suite. Feature launches should trigger ground truth refresh as a standard part of the release process, but most teams do not have this discipline.

**Competitive standard shifts** change what "good" means. In 2024, a customer service AI response time of 1.8 seconds was excellent. In 2026, competitors deliver responses in 0.6 seconds. User tolerance for 1.8 seconds evaporates. The ground truth still defines quality as "accurate and professional responses within two seconds," which is technically still met, but users are unsatisfied. The eval suite passes outputs that production users rate poorly. Competitive staleness is hardest to detect because it is not a discrete event. It is a continuous erosion of what users consider acceptable. Teams detect it through divergence between eval scores and user satisfaction metrics.

**Organizational priority evolution** redefines success criteria. The company initially prioritized accuracy above all else. After a competitor wins market share with faster, slightly less accurate responses, the company shifts priorities to speed. The ground truth still emphasizes exhaustive accuracy. The eval rewards slow, thorough responses. The team tunes the model for speed. Eval pass rates drop. Leadership sees the drop and questions the decision to prioritize speed. Misaligned ground truth creates organizational conflict by suggesting that the right strategic decision is the wrong tactical one. Ground truth must stay synchronized with business priorities or it becomes a tool for defending legacy strategy against necessary evolution.

## Detecting Stale Ground Truth

Staleness is invisible in eval metrics. Pass rates can rise, fall, or stay flat while ground truth quietly rots. Detection requires looking outside the eval suite. **Correlation breakdown** is the strongest signal. When eval pass rates decouple from user satisfaction, escalation rates, task success rates, or revenue outcomes, ground truth has likely drifted from what users value. Track the correlation monthly. When it drops from negative 0.74 to negative 0.52 over three months, investigate every ground truth assumption. Review reference answers against current policy. Check whether success criteria still match user priorities. Correlation breakdown does not tell you which specific ground truth is stale, but it tells you that staleness exists.

**Manual audits triggered by version milestones** force periodic ground truth review. Every time the model version increments — v1.4 to v1.5, a major fine-tuning iteration, a significant prompt rework — trigger a ground truth audit. A designated engineer reviews 10% of the ground truth, checking each reference answer against current documentation, current product behavior, and recent user feedback. For any reference answer that feels outdated, the engineer files a ticket to refresh it. Manual audits do not scale to thousands of examples, but they catch the highest-impact staleness before it metastasizes. The 10% sample should rotate, covering the full ground truth set over ten audits.

**User feedback loops that contradict eval results** expose staleness directly. A user escalates an output, claiming it violates policy. The eval suite scored that output 9 out of 10. Investigation reveals the eval compared the output to a policy document from 14 months ago. The user is right. The eval is wrong. Every user escalation or content report should be cross-checked against eval results. When users report failures the eval scored as successes, log the discrepancy and investigate the ground truth. User feedback is the ultimate source of truth. When user truth contradicts eval truth, eval truth is stale.

**Regulatory or legal review flags** trigger immediate ground truth refresh. Legal publishes updated terms of service. Compliance updates the data retention policy. Trust and Safety revises content moderation guidelines. Each of these changes invalidates ground truth that references the old versions. Regulatory flags are high-priority because stale ground truth in these domains can expose the company to legal liability. The eval suite must reflect current legal and compliance requirements, not historical ones. Legal and compliance teams rarely notify engineering when policies update. Engineering must monitor policy change logs and proactively refresh ground truth.

## Ground Truth Refresh Process

Refreshing ground truth is not rewriting from scratch. It is targeted updates to the subset that has expired. **Differential refresh** compares current ground truth to current source material and updates only the deltas. Take the ground truth for a compliance eval. Compare each reference answer to the current policy document. For answers that match, no action needed. For answers that reference deprecated clauses, update them to cite current clauses. For answers that miss new requirements introduced in the latest policy, add the new requirements. Differential refresh keeps 80% of the ground truth stable while fixing the 20% that drifted. Stability matters because frequent wholesale rewrites break historical comparisons and make drift detection impossible.

**Stakeholder re-validation** ensures refreshed ground truth aligns with current organizational consensus. The original ground truth was validated by Product, Legal, and Domain Experts in March 2025. It is now November 2025. Before deploying refreshed ground truth, reconvene stakeholders. Present the changes. Confirm alignment. Stakeholders may have evolved their positions since March. What Legal considered edge-case acceptable in March may be unacceptable in November after a competitor faced regulatory action for similar behavior. Re-validation takes two meetings and prevents the scenario where Engineering refreshes ground truth to match old stakeholder intent that stakeholders themselves have abandoned.

**Versioned ground truth with change logs** tracks what changed and when. Ground truth is infrastructure. Treat it like code. Every refresh creates a new version: ground-truth-v1.0, ground-truth-v1.1, ground-truth-v2.0. Each version has a change log: "Updated 34 reference answers to align with Q3 2025 policy revisions. Added 12 new examples covering new product features. Removed 8 deprecated examples for sunset workflows." Versioning enables reproducibility. An engineer investigating why eval pass rates dropped in August can diff ground-truth-v1.5 and ground-truth-v1.6 to see exactly what changed. Change logs provide context for future audits and help new team members understand how ground truth evolved.

**Parallel eval runs during transition** prevent abrupt metric discontinuities. Run the eval suite with both old and new ground truth for two weeks. Compare results. If pass rates differ by less than 3%, the refresh is low-impact and can be deployed immediately. If pass rates differ by 12%, the refresh represents a significant shift in correctness definitions. Communicate to stakeholders that eval metrics will shift, explain why, and set new baseline expectations. Parallel runs also catch refresh errors. If the new ground truth causes pass rates to drop to 40%, something went wrong in the refresh. Investigate before deploying. Parallel validation costs two weeks of compute but prevents deploying bad ground truth that undermines trust in the eval suite.

## When Ground Truth Becomes Wrong

The hardest staleness scenario is when ground truth transitions from correct to incorrect. **Factual ground truth in dynamic domains** becomes factually wrong over time. A finance AI's eval suite includes ground truth about tax law from 2024. In 2025, tax law changes. The ground truth is now objectively incorrect. Evaluating against it actively harms the model by penalizing correct citations of 2025 law. This is not a judgment call. The ground truth is wrong. Delete it or update it immediately. Factual staleness in regulated domains creates compliance risk. A model trained to maximize eval pass rate will learn to cite outdated law, expose the company to legal liability, and harm users.

**Ethical ground truth that reflects outdated norms** becomes morally wrong as social context evolves. A content moderation eval suite from 2023 includes ground truth marking certain phrases as acceptable. By 2026, those phrases are widely recognized as harmful. The ground truth encourages the model to allow content that modern standards consider unacceptable. The eval suite becomes a vehicle for perpetuating outdated norms. Ethical staleness requires not just updating reference answers but revisiting the assumptions embedded in success criteria. What counted as "neutral tone" in 2023 may be recognized as subtly biased in 2026. Ground truth must evolve with ethical understanding.

**Business ground truth that reflects abandoned strategy** becomes strategically wrong. The company built an eval suite optimizing for accuracy at the expense of speed, reflecting a 2024 strategy to compete on quality. In 2025, the company pivots to compete on cost and speed. The ground truth still rewards expensive, slow, highly accurate responses. The eval suite fights the pivot by suggesting that faster models are lower quality. The ground truth is technically correct by 2024 standards but strategically misaligned with 2025 priorities. This is not staleness. This is active opposition to current direction. Ground truth must be rewritten to reflect the pivot or the eval suite will anchor the organization to legacy strategy.

**Deprecated feature ground truth** becomes meaningless after feature sunset. The product offered a translation feature. The eval suite included 200 translation test cases with ground truth. The company sunsets translation to focus on summarization. The translation ground truth is now irrelevant. Keeping it in the eval suite dilutes focus and wastes compute evaluating capabilities the product no longer offers. Delete deprecated ground truth aggressively. Every deprecated example that remains in the eval dataset is a tax on clarity, velocity, and cost. Sunsetting features should trigger automatic ground truth pruning as part of the deprecation checklist.

## The Staleness Audit as Ongoing Practice

Ground truth refresh cannot be a once-per-year event. It must be continuous. **Quarterly staleness reviews** are the baseline discipline. Every quarter, the eval owner reviews 25% of the ground truth dataset. Over four quarters, the full dataset is audited. For each example, the owner asks: Does this still reflect current policy? Does this align with current product capabilities? Does this match current user expectations? Does this represent current business priorities? Any example that fails any question is flagged for refresh. Quarterly reviews catch staleness before it accumulates into a multi-month recalibration project.

**Event-triggered audits** respond to specific changes that invalidate ground truth. Policy update: audit all ground truth referencing that policy. Feature launch: audit all ground truth for that workflow. Competitive shift: audit all ground truth related to the affected quality dimension. Regulatory change: audit all ground truth in the regulated domain. Event-triggered audits are reactive but fast. They focus effort on the subset of ground truth most likely to be stale, allowing rapid refresh within days of the triggering event. Event triggers should be automated. A policy document update in the company knowledge base triggers a notification to the eval owner listing all ground truth examples citing that document.

**Metrics that force regular confrontation with ground truth health** keep staleness top of mind. Track and dashboard: ground truth last-refresh date per category, number of days since last audit, percentage of ground truth older than six months, correlation between eval pass rate and user satisfaction. Expose these metrics in sprint reviews and quarterly business reviews. When leadership sees that 40% of ground truth has not been audited in eight months, they allocate time for refresh. Metrics create accountability. Without metrics, ground truth maintenance is perpetually deferred in favor of shipping features. With metrics, staleness becomes a visible risk that demands mitigation.

Ground truth is living documentation of what correct means. Like all documentation, it decays the moment it is written. The teams that build enduring eval systems treat ground truth as infrastructure requiring active maintenance, not as an artifact created once and frozen forever. They version it, audit it, refresh it, and align it continuously to the moving target of correctness. The next subchapter shifts from calibration challenges to evaluation methodology, exploring how to use A/B testing to compare model versions, judge configurations, and prompt variants with statistical rigor rather than subjective assessment.
