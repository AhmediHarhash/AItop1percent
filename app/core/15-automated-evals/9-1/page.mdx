# 9.1 — Why Evals Drift Over Time

Every eval system you build will drift. The automation you trust today will become unreliable tomorrow. The ground truth you validated last quarter will misrepresent reality this quarter. The thresholds that accurately separated good from bad outputs six months ago will flag false positives and miss real failures now. Drift is not a possibility you plan for. It is a certainty you design around.

The teams that treat eval systems as static infrastructure discover drift when users complain, when silent quality degradation shows up in engagement metrics, or when a major incident forces an audit. The teams that treat eval systems as living infrastructure detect drift before it matters, recalibrate continuously, and maintain trust in their automation across model generations, product changes, and distribution shifts.

## The Five Sources of Eval Drift

Eval drift has five root causes, each with different timescales and different detection requirements. First, model updates change the output distribution. When you upgrade from GPT-5 to GPT-5.1, from Claude Opus 4 to Opus 4.5, or when you fine-tune a model, the character of outputs shifts. Sentence structure changes. Verbosity changes. The model's handling of edge cases changes. Your eval automation was calibrated against the old distribution. It now measures something slightly different. The automation still runs, still produces scores, still passes or fails examples — but the scores mean something different than they did before the model change.

Second, user behavior shifts the input distribution. In January, 60 percent of queries were product questions and 40 percent were policy questions. By April, the ratio reversed. In Q1, most queries were in English. In Q3, Spanish queries doubled. The eval suite you built in January sampled the January distribution. Your production traffic in Q3 looks different. The eval scores are still accurate for January-shaped traffic — but production is no longer January-shaped. Your automation is measuring the wrong slice of reality.

Third, ground truth ages. The customer support knowledge base you used to define correct answers was last updated eight months ago. Product features have changed. Pricing has changed. The company launched in three new markets. The answers that were correct when you built your eval suite are partially wrong now. Your automation still compares outputs to those outdated answers. It flags new, correct answers as failures because they contradict obsolete ground truth. It passes old, incorrect answers because they match the stale reference data.

Fourth, business priorities change the definition of quality. In Q1, the priority was reducing support volume, so verbosity was penalized and conciseness was rewarded. In Q2, the priority shifted to customer satisfaction, so thoroughness became more important than brevity. Your eval automation still uses Q1's quality definition. It marks verbose-but-helpful answers as failures because they violate the old priority. It passes terse-but-incomplete answers because they matched the old standard. The automation's output is technically correct by the old definition and completely misleading by the new one.

Fifth, adversarial behavior evolves. Users discover prompt injection patterns. Competitors probe for weaknesses. The model learns to produce outputs that score well on your automated metrics while degrading on dimensions the automation does not measure. A six-month-old eval suite does not account for adversarial techniques that did not exist six months ago. Your automation confidently passes examples that human reviewers would immediately flag as manipulated or unsafe.

## Why Drift Is Invisible

Drift does not announce itself. Your eval pipeline continues to run. Pass rates remain stable or drift slowly. Latency stays within bounds. Cost per eval does not spike. Every dashboard looks normal. The automation produces numbers every day. Those numbers change gradually, not abruptly. A pass rate that was 94 percent in January, 92 percent in March, 89 percent in May, and 86 percent in July looks like a smooth trend, not a calibration failure. Your team interprets it as model degradation or harder traffic. The real cause — that your automation is measuring the wrong thing — remains hidden until someone runs a manual audit and discovers that examples the automation passes are failures by current standards and examples the automation fails are actually correct.

The invisibility is structural. Automated eval systems measure themselves against themselves. They do not have an external reference point unless you deliberately build one. A model-graded eval compares model outputs to model-generated scores. If the scoring model drifts, the scores drift, but the system has no way to detect it. A rule-based eval applies static rules to a changing distribution. If the distribution shifts, the rules produce different results, but the system has no way to know the shift happened. A human-in-the-loop eval uses human feedback — but if you are only collecting feedback on edge cases or failures, you are not sampling the full distribution, and you cannot detect drift in the parts of the distribution you are not reviewing.

Drift also hides in aggregate metrics. Your overall pass rate might stay stable while specific slices drift dramatically. Pass rate for English queries stays at 93 percent. Pass rate for Spanish queries drops from 91 percent to 78 percent. The aggregate metric — weighted by volume — moves from 92.4 percent to 90.7 percent. That looks like normal variance. You do not notice that one language has degraded catastrophically. Your automation does not segment by language, so the drift in Spanish is invisible in the top-level dashboard. The same pattern repeats for every dimension you do not explicitly track: query type, user segment, feature area, time of day, session length.

## The Calibration Decay Problem

Calibration is not binary. An eval system is not calibrated or uncalibrated. Calibration exists on a spectrum, and it decays over time. When you first build an eval system, you validate it against human judgment. You measure agreement. You tune thresholds. You achieve 94 percent automation-to-human agreement. That is your calibration baseline. One month later, you upgrade the model. Agreement drops to 91 percent. You do not re-measure. Three months later, the input distribution shifts. Agreement drops to 87 percent. You still do not re-measure. Six months later, ground truth has aged. Agreement is now 81 percent. You have not re-measured once. Your team still trusts the automation as though it were at 94 percent agreement, but it is actually at 81 percent. Thirteen percent of the time, your automation produces a score that contradicts what a human reviewer would conclude.

Decay accelerates when multiple sources compound. A model update drops agreement by three percentage points. A distribution shift drops it by another four. Stale ground truth drops it by another five. Within six months, your automation is wrong 20 percent of the time. Within a year, it is wrong 30 percent of the time. The decay is gradual enough that no single week looks alarming, but the cumulative effect is that your eval system has become unreliable.

The operational danger is that teams continue to make decisions based on decayed automation. A product manager looks at eval pass rates and decides the new model is worse than the old one. The conclusion is based on automation that is no longer calibrated. The new model might actually be better, but the eval system is measuring the wrong thing. An engineer investigates a drop in eval scores and tunes the prompt to optimize for the outdated metric. The prompt now performs worse on the actual quality dimensions users care about, but better on the stale automation. A executive reviews quarterly metrics and concludes quality has improved because eval pass rates increased. The increase is an artifact of drift — the automation has become more lenient as ground truth aged, not because outputs actually improved.

## Drift as Silent Quality Degradation

The most insidious form of drift is when automation degrades but production quality stays constant or improves. This is the inverse of the usual failure mode. Normally, you worry that production quality degrades while automation scores stay stable. That is a missed-alert problem. Drift can also manifest as automation scores degrading while production quality stays stable. That is a false-alert problem, and it is just as damaging.

A customer support system upgrades from GPT-5 to GPT-5.1. Human reviewers and user satisfaction scores both show that answer quality improved. Eval pass rates drop by six percentage points. The team investigates, assumes the new model introduced a regression, and considers rolling back. The real issue is that GPT-5.1 produces longer, more thorough answers. The old eval automation was tuned for GPT-5's concise style. It interprets the new verbosity as a quality failure. The automation is measuring the wrong thing. The team nearly makes a bad decision — rolling back a quality improvement — because they trusted decayed automation.

Another pattern: a model fine-tuned on recent support tickets starts handling new product features correctly. Ground truth in the eval suite still references the old product behavior. The model's answers are correct by current standards but fail automated evals because they contradict stale ground truth. The team sees eval scores drop, assumes fine-tuning degraded the model, and stops the fine-tuning program. The automation blocked a real quality improvement.

Drift creates a trust problem. If your team cannot distinguish between real quality changes and calibration decay, they stop trusting eval scores. Once trust erodes, the entire eval infrastructure becomes ceremonial. Engineers run evals because the process requires it, not because the results inform decisions. Product managers ignore eval metrics in favor of anecdotal user feedback. Leadership makes quality decisions based on intuition instead of data. The eval system still runs, still costs money, still produces dashboards — but it no longer influences outcomes.

## Designing for Continuous Recalibration

The solution is not to build evals that never drift. That is impossible. The solution is to build evals that detect drift, measure drift, and correct drift continuously. Drift is not a failure mode to eliminate. It is an operational reality to manage.

This starts with treating calibration as an ongoing process, not a one-time event. When you build an eval system, you also build a recalibration system. The recalibration system has three components. First, a human reference set that represents current ground truth, current quality standards, and current input distribution. Second, a measurement process that periodically compares automation to human judgment on that reference set. Third, a feedback loop that uses the comparison to detect calibration decay, quantify it, and decide whether to recalibrate.

The recalibration system runs on a cadence — weekly, monthly, or quarterly depending on how fast your system changes — and on triggers. Model update: trigger recalibration. Major product launch: trigger recalibration. Eval pass rate drops by more than five percentage points in a week: trigger investigation, possibly recalibration. User complaints spike: trigger manual audit and recalibration review. The cadence ensures you catch slow drift. The triggers ensure you catch fast drift.

Recalibration is not free. It requires human time to review examples, update ground truth, and validate thresholds. It requires engineering time to retrain scoring models, update rules, and redeploy automation. The cost is real, and it scales with the size of your eval suite. A team running 10,000 automated evals per day might recalibrate monthly. A team running 100,000 per day might recalibrate weekly. The cost of recalibration increases with frequency, but the cost of not recalibrating — silent quality degradation, false alerts, eroded trust — increases even faster. The economic question is not whether to recalibrate, but how often and at what scope.

The next question is how to detect drift early enough that recalibration prevents damage rather than responding to it. That requires building calibration loops that continuously compare automation to human judgment and flag divergence before it becomes operationally significant.

