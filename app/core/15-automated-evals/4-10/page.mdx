# 4.10 — Cross-Model Judges: Using Different Families to Reduce Bias

A single model family as your judge creates a monoculture that amplifies its own biases. The judge prefers outputs that match its training distribution, rewards phrasing patterns it recognizes, and penalizes reasonable variations it was never taught to value. Cross-model judges — using different families from different vendors with different training data — reduce this bias through diversity. When GPT-5, Claude Opus 4.5, and Gemini 3 Pro all agree on a score, you have convergent evidence. When they disagree, you've found a signal worth investigating.

## Why Different Model Families Reduce Bias

Model families are trained on different data, with different architectures, by different teams, optimizing for different goals. GPT-5 was trained with OpenAI's reinforcement learning from human feedback process and their proprietary instruction datasets. Claude Opus 4.5 was trained with Anthropic's constitutional AI framework and their harmlessness-helpfulness balance. Gemini 3 Pro was trained with Google's multimodal pipeline and their search-grounded objectives. These differences create different preferences.

When a single judge evaluates outputs, its preferences become your evaluation's preferences. If your production model happens to generate text that matches the judge's style — even if that style isn't objectively better — it scores higher. If it generates perfectly good text that deviates from the judge's expectations, it scores lower. You're not measuring quality against human judgment. You're measuring alignment with one model's learned preferences.

Cross-model judges break this correlation. If three judges from different families all score an output highly, the output is likely meeting a shared, more universal standard of quality. If GPT-5 scores it at 9, Claude Opus 4.5 at 8, and Gemini 3 Pro at 9, you have evidence that crosses training boundaries. If GPT-5 scores it at 9 and both Claude and Gemini score it at 4, you've found either an output that exploits GPT-5's specific biases or a disagreement worth human review.

The reduction in bias is statistical, not absolute. All frontier models share some training data — they were all exposed to similar public corpuses, similar instruction-tuning patterns, similar feedback from overlapping populations. But the differences are large enough that using multiple families produces measurably better agreement with human judgment than using a single family alone.

## OpenAI, Anthropic, and Google Judges in Practice

The three primary model families used as judges in production pipelines in 2026 are OpenAI's GPT series, Anthropic's Claude series, and Google's Gemini series. Each has documented strengths and documented blind spots.

GPT-5 and its variants are fast, cost-effective, and tend to reward structured, comprehensive responses. They perform well on factual accuracy tasks, code evaluation, and tasks where verbosity correlates with quality. They have a documented bias toward longer outputs and can over-reward outputs that include disclaimers or hedging language even when direct answers are more appropriate. When evaluating creative tasks, they favor coherence and narrative structure but can penalize stylistic experimentation.

Claude Opus 4.5 tends to reward nuance, safety-aware outputs, and responses that acknowledge complexity. It performs well on tasks requiring ethical reasoning, harmlessness evaluation, and tone assessment. It has a documented preference for balanced, measured responses and can under-reward outputs that are direct but lack qualifiers. When evaluating factual recall, it is more conservative than GPT-5 — it will mark an output as partially correct when GPT-5 would mark it fully correct, if the output includes minor imprecision.

Gemini 3 Pro is grounded in Google's search and knowledge graph infrastructure and tends to reward factual precision, citation-like specificity, and outputs that align with structured information retrieval patterns. It performs well on knowledge-heavy tasks, summarization, and tasks where grounding in external facts is central. It has a documented preference for outputs that match the style of informational content and can under-reward creative or informal outputs even when they meet user needs.

Using all three as an ensemble creates a judge system that balances these preferences. The ensemble scores an output highly only if it satisfies GPT-5's coherence standards, Claude's nuance requirements, and Gemini's factual grounding expectations. This triangulation produces scores that correlate more strongly with human judgment than any single model alone.

## When to Use Cross-Family Judges

Cross-model judging has a cost and latency penalty. You're making three API calls instead of one. You're paying for three model invocations. You're waiting for the slowest of the three to return. The decision to use cross-family judges depends on whether the bias reduction is worth the expense.

Use cross-family judges when the task being evaluated has high ambiguity or high subjectivity. If you're scoring creative writing, customer service tone, product recommendation quality, or open-ended question answering, different judges will interpret quality differently. The ensemble reduces the risk that your pipeline's definition of quality is just one model's idiosyncratic preference.

Use cross-family judges when the production model being evaluated is from the same family as a potential single judge. If you're fine-tuning GPT-5.2 and using GPT-5 as the judge, you risk inflating scores because the judge recognizes its own output patterns. Switching to a cross-model ensemble — GPT-5, Claude Opus 4.5, Gemini 3 Pro — eliminates that same-family correlation.

Use cross-family judges when the cost of a missed failure is high. If you're evaluating legal advice generation, medical triage responses, or financial guidance, you want convergent evidence that an output is correct. A single judge saying "score 9" is weaker evidence than three independent judges all saying "score 8 or higher." The ensemble acts as a redundancy layer.

Do not use cross-family judges when the task is narrow, objective, and fast-moving. If you're evaluating whether a JSON extraction is correctly formatted, whether a classification label matches a schema, or whether a SQL query includes a specific clause, a single fast judge is sufficient. The cost and latency of three judges buys you nothing when the task has one unambiguous correct answer.

Do not use cross-family judges when cost or latency is a binding constraint. If you're running 500,000 eval judgments per day and your budget is fixed, using three models triples your spend. If you're in a latency-sensitive pipeline where eval scores feed back into a real-time routing decision, waiting for three sequential or parallel API calls may exceed your latency budget. In these cases, use a single well-calibrated judge and accept the bias risk.

## Cost and Latency of Multi-Model Judging

The financial cost of cross-model judging is straightforward: you pay the per-token input and output cost for each model. In early 2026, GPT-5 costs approximately 3 dollars per million input tokens and 15 dollars per million output tokens. Claude Opus 4.5 costs approximately 15 dollars per million input tokens and 75 dollars per million output tokens. Gemini 3 Pro costs approximately 4 dollars per million input tokens and 12 dollars per million output tokens. A three-model ensemble for a judgment task that consumes 800 input tokens and produces 200 output tokens per eval costs roughly 0.02 dollars per judgment — ten times the cost of using GPT-5 alone.

At 10,000 judgments per day, that's 200 dollars daily, or 6,000 dollars monthly. At 100,000 judgments per day, it's 60,000 dollars monthly. The cost is acceptable for high-stakes pipelines where quality matters more than expense. It is prohibitive for high-volume, lower-stakes tasks where a single cheaper judge suffices.

The latency cost depends on whether you call the models sequentially or in parallel. Sequential calls add linearly: if GPT-5 responds in 1.2 seconds, Claude Opus 4.5 in 1.8 seconds, and Gemini 3 Pro in 1.5 seconds, your total latency is 4.5 seconds. Parallel calls reduce latency to the slowest model's response time: 1.8 seconds in this example. Most production pipelines call judges in parallel unless they need to use one judge's output as input to another.

Parallel calls require managing multiple simultaneous API connections, handling partial failures, and implementing retry logic per model. If Claude's API is experiencing elevated latency, you need to decide whether to wait, timeout and proceed with two judges, or fail the entire judgment. The engineering overhead of multi-model ensembles is higher than single-judge pipelines, and that overhead must be justified by the quality improvement.

## Ensemble Voting Strategies

The simplest ensemble strategy is average voting. Each judge produces a score from 1 to 10. You take the mean of the three scores. If GPT-5 scores an output at 8, Claude Opus 4.5 at 7, and Gemini 3 Pro at 9, the ensemble score is 8. This strategy treats all judges as equally reliable and weights them identically.

A more sophisticated strategy is weighted voting. You assign weights to each judge based on their measured agreement with human labels on a calibration dataset. If GPT-5 achieves 82 percent agreement with human raters, Claude Opus 4.5 achieves 88 percent, and Gemini 3 Pro achieves 79 percent, you weight Claude's scores more heavily. The ensemble score becomes a weighted average: score equals 0.3 times GPT-5 score plus 0.5 times Claude score plus 0.2 times Gemini score. This strategy improves correlation with human judgment when one judge is measurably better than the others.

A third strategy is threshold voting for binary pass-fail decisions. You set a threshold — say, 7 out of 10 — and require at least two of three judges to score the output above that threshold for it to pass. This strategy is more conservative than averaging. An output that receives scores of 9, 8, and 4 would average to 7 and pass under mean voting, but would fail under threshold voting because only two judges exceeded the threshold. Use threshold voting when false positives are more costly than false negatives.

A fourth strategy is veto voting, where a single judge scoring an output below a critical threshold automatically fails it, regardless of the other judges' scores. If any judge scores the output at 3 or below, the output fails. This strategy is used in safety-critical tasks where a single severe flaw — recognized by even one judge — disqualifies the output. Use veto voting sparingly. It gives disproportionate power to the most conservative judge and can produce false negatives when one model has an idiosyncratic blind spot.

The choice of voting strategy depends on the task's risk profile and the calibration data available. If you have strong human-labeled data showing which judge is most reliable, use weighted voting. If you lack that data, use mean voting. If safety is paramount, use threshold or veto voting.

## The Independence Assumption and Its Limits

Cross-model ensembles assume that judges are independent — that their errors are uncorrelated, so combining them reduces overall error. This assumption is only partially true. All frontier models share some training data, some feedback loops, and some learned representations. They are not perfectly independent.

If all three judges fail to detect a subtle factual error because none of them were exposed to the specific domain knowledge required, the ensemble fails. If all three judges reward verbosity because all three were trained on datasets where longer answers were rated higher by human annotators, the ensemble inherits that bias. The independence assumption holds best when the errors you want to catch are random or model-specific, not when they are systematic across the entire frontier model class.

The value of cross-model judges is not that they are perfectly independent. The value is that they are more independent than three instances of the same model, or three checkpoints from the same training run, or three prompt variations applied to the same base model. The diversity is partial, but partial diversity is better than no diversity.

In practice, cross-model ensembles reduce same-family bias by 40 to 60 percent compared to single-model judges, based on production measurement against human-labeled datasets. That reduction is meaningful. It is not a silver bullet. If you need higher confidence than a three-model ensemble provides, the next step is not adding a fourth model — it is routing disagreements to human review, which is the subject of the next chapter's final subchapters.

---

Judge calibration is not a one-time setup. Models update. Human preferences drift. The next subchapter covers how to continuously calibrate LLM judges against human labels to ensure they remain reliable over time.
