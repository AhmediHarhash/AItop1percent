# 8.9 — Early Stopping and Fast-Fail Logic

In August 2025, a healthcare company ran a nightly batch eval pipeline that processed 200,000 clinical note summaries. The pipeline had six evaluation stages — format validation, PII detection, factual accuracy against source notes, readability scoring, tone assessment, and compliance checks. Each stage ran in sequence. The compliance stage was last. One night, an upstream data pipeline bug corrupted the input dataset. Every single clinical note was malformed JSON. The format validation stage should have caught this immediately. It did catch it. But the pipeline was not configured to stop on validation failure. It logged 200,000 format errors and proceeded to the next stage.

The PII detection stage attempted to parse the malformed JSON, threw exceptions on every input, logged 200,000 errors, and continued. The factual accuracy stage sent all 200,000 malformed inputs to GPT-5 with a complex prompt and retrieved context. GPT-5 returned error messages saying it could not parse the input. That stage cost $40,000 in wasted inference. The pipeline continued through readability, tone, and compliance, each stage failing on every input, each stage consuming compute and API quota. Total time wasted: eleven hours. Total cost wasted: $73,000. Total useful work accomplished: zero. The pipeline should have stopped after the first stage detected the first validation failure.

## The Fail-Fast Principle

When a pipeline detects an error that makes further processing pointless, it should stop immediately. Not log the error and continue. Not complete the current batch and stop. Stop now. Return a failure status. Alert the on-call engineer. Save the compute, save the cost, save the time. The fail-fast principle is simple: detect failure as early as possible and abort execution as soon as failure is detected. Every stage in an eval pipeline should have abort conditions — specific failure modes that trigger immediate pipeline termination.

For format validation, the abort condition is any input that fails to parse. If the input is supposed to be JSON and it does not parse as valid JSON, do not send it to the next stage. Fail the entire pipeline run and alert that the upstream data source is broken. For PII detection, the abort condition might be detection rate above a threshold. If PII detection suddenly flags 80% of outputs when the baseline is 2%, the input data is likely corrupted or the detection model is broken. Stop the pipeline and investigate. For factual accuracy, the abort condition might be inability to retrieve context. If the vector database is unreachable or returns empty results for every query, the accuracy stage cannot function. Stop the pipeline rather than hallucinating accuracy scores.

The anti-pattern is treating every failure as an individual output failure rather than recognizing systemic failures that indicate pipeline or data integrity issues. A single malformed JSON input is an output failure — log it, skip it, continue processing the rest. A thousand consecutive malformed JSON inputs is a systemic failure — the data source is broken, and continuing wastes resources on a doomed run.

## Early Stopping Criteria

Each stage in your pipeline needs two types of failure logic: per-output failure and systemic failure. Per-output failure means this specific output failed this specific check — log it, mark it as failed, continue to the next output. Systemic failure means the stage itself is not functioning correctly or the input data is so broken that continued processing is pointless — abort the entire pipeline run.

Systemic failure criteria include error rate thresholds, consecutive failure counts, and critical dependency unavailability. If the error rate in any stage exceeds 50% within the first hundred outputs, stop the pipeline. If any stage experiences ten consecutive failures with the same error message, stop the pipeline. If a required service — the vector database, the LLM API, the annotation storage backend — returns 5xx errors for three consecutive calls, stop the pipeline. These criteria detect problems early in the run, often within the first minute, before significant cost is wasted.

For batch pipelines, you can implement a preview mode where the pipeline processes the first fifty to one hundred outputs, computes error rates and failure patterns, and decides whether to proceed with the full batch. If the preview run shows error rates within historical norms, the pipeline continues to the full batch. If the preview run shows anomalous error rates or systemic failures, the pipeline stops, alerts the team, and waits for manual investigation. This prevents wasting eleven hours and $73,000 on a batch that was doomed from the start.

## Abort Conditions for Each Stage Type

Different evaluation stages have different abort conditions based on their purpose and failure modes. For rule-based stages — regex, format validation, length checks — the abort condition is typically a sudden spike in failure rate. If your profanity filter normally flags 0.3% of outputs and suddenly flags 30%, something is wrong. Either the filter configuration changed accidentally, or the input data shifted in a way that suggests upstream corruption, or the filter itself is broken. Stop the pipeline and investigate.

For LLM-based stages, the abort condition is typically API failure rate or response validity. If the LLM API returns 5xx errors on more than 5% of calls, the service is degraded and results will be unreliable. Stop the pipeline. If the LLM returns malformed responses — responses that do not match the expected JSON schema or that omit required fields — on more than 10% of calls, the prompt or model behavior has changed unexpectedly. Stop the pipeline. If the LLM response latency suddenly doubles compared to historical baseline, the service may be overloaded or routing to a degraded instance. This might not justify stopping immediately, but it should trigger an alert.

For retrieval-based stages — RAG evals, factual accuracy checks — the abort condition is typically retrieval failure rate or empty result rate. If the vector database is unreachable for more than three consecutive queries, stop the pipeline. If retrieval returns zero results for more than 20% of queries when the baseline is 2%, something is broken — either the query embedding service, the vector index, or the upstream data ingestion pipeline. Stop and investigate.

For human review stages, the abort condition is typically reviewer unavailability or review queue overflow. If no human reviewers are active and the queue contains more than five hundred pending items, stop sending new items to the queue. Alert the ops team that the review queue is backing up. If the average time-in-queue exceeds four hours when the SLA is thirty minutes, stop adding to the queue until the backlog clears.

## Cascading Failures and Dependency Checks

Eval pipelines depend on external services — LLM APIs, vector databases, annotation storage, metadata stores, logging systems. When one dependency fails, it often cascades. The LLM API is unreachable, so the accuracy stage fails. The accuracy stage fails, so it cannot write results to the annotation store. The annotation store never receives data, so downstream monitoring that watches for eval result writes assumes the pipeline is not running and triggers alerts. Now you have three alerts — LLM API down, annotation writes failing, pipeline not running — when the root cause is a single upstream service outage.

Prevent cascades by checking critical dependencies at pipeline startup. Before processing any outputs, the pipeline sends health check requests to every required service. LLM API: send a minimal test prompt and verify response. Vector database: send a test query and verify results. Annotation storage: write a test record and read it back. Logging system: write a test log entry. If any health check fails, the pipeline exits immediately with a clear error message naming the unavailable dependency. You get one alert — "eval pipeline cannot start, LLM API health check failed" — instead of three cascading alerts from different subsystems.

During execution, implement circuit breakers. If the LLM API fails three consecutive requests, the circuit breaker opens. The pipeline stops sending requests to that API for sixty seconds. After sixty seconds, it sends a single test request. If the test succeeds, the circuit closes and normal traffic resumes. If the test fails, the circuit stays open for another sixty seconds. This prevents the pipeline from wasting time and cost retrying a service that is clearly down, and it allows automatic recovery when the service comes back online without requiring manual intervention.

## Avoiding Wasted Compute on Failed Outputs

In a multi-stage pipeline, an output that fails stage one does not need stage two. An output that fails stage two does not need stage three. This is the early-exit pattern from the previous subchapter. But even within a single stage, you can apply early stopping. If a stage runs five different checks — toxicity, PII, tone, factual accuracy, compliance — and the output fails the toxicity check, do you need to run the other four? The answer depends on whether you need a complete diagnostic or just a pass/fail decision.

For pipelines that gate production deployment, you only need to know that the output failed. As soon as any check fails, mark the output as failed, log which check triggered, and stop processing that output. Do not run the remaining checks. This saves compute when multiple failures are present. For pipelines that support debugging and model improvement, you want a complete diagnostic. Run all checks even after the first failure, because knowing that an output failed both toxicity and factual accuracy tells you more about the model's failure mode than knowing it failed toxicity alone. The right choice depends on your use case.

The implementation is a configuration flag: `stop_on_first_failure`. When true, the stage executor runs checks in sequence and exits as soon as any check fails. When false, the executor runs all checks and aggregates results. For production gating pipelines, set it to true. For model development and analysis pipelines, set it to false. This single flag can reduce stage execution time by 30-50% in production pipelines where outputs frequently fail early checks.

## Logging and Debugging Early Stops

When a pipeline stops early due to a systemic failure, the logs need to make it immediately clear why the pipeline stopped, which stage triggered the stop, and what the failure condition was. "Pipeline failed" is not sufficient. "Pipeline aborted during PII detection stage after error rate exceeded 50% threshold within first 100 outputs; 73 of 100 outputs returned INVALID_INPUT error" is sufficient. The log message should include the stage name, the abort condition that triggered, the specific metric values that exceeded thresholds, and the error messages or patterns that indicate root cause.

Structured logging makes this easier. Each stage logs events as JSON objects with consistent fields: stage name, output ID, check name, result, latency, error code, error message. When the pipeline aborts, the abort log entry includes aggregated statistics from the failed stage: total outputs processed, error count by error type, error rate, consecutive failure count, time elapsed. The on-call engineer reading the logs can reconstruct exactly what happened without running the pipeline in debug mode.

For pipelines that process millions of outputs, sampling is necessary to keep log volume manageable. Log every failure during the first hundred outputs. After that, log failures with decreasing probability — 10% of failures between output 100 and 1,000, 1% of failures between 1,000 and 10,000, 0.1% of failures beyond 10,000. Always log systemic abort events at full detail regardless of sampling. This keeps log storage costs reasonable while ensuring you have enough diagnostic data to debug problems.

Early stopping and fail-fast logic transform a pipeline that wastes $73,000 on a doomed batch run into a pipeline that detects the problem in the first sixty seconds, stops execution, alerts the team, and costs effectively zero. The next architectural concern is where and how to store the massive volumes of eval results the pipeline generates.

---

*Next: 8.10 — Eval Result Storage and Retrieval*
