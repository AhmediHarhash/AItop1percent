# 5.8 — Factual Consistency Checking

In August 2025, a legal tech company deployed a contract summarization tool that passed all their evals. ROUGE scores averaged 0.71. Semantic similarity between summaries and source documents measured 0.88. User acceptance testing showed positive feedback. The tool went live for 4,000 in-house attorneys. Within three weeks, attorneys reported that 11% of summaries contained factual errors — wrong dates, wrong monetary amounts, wrong party names, wrong obligations. The errors were subtle. Instead of "payment due within 30 days," a summary said "payment due within 60 days." Instead of "liability capped at $500,000," it said "liability capped at $5,000,000." The model had paraphrased fluently, matched the semantic topic of each clause, and produced high similarity scores. But it had also hallucinated facts. The company pulled the tool, spent $340,000 rebuilding their eval pipeline with factual consistency checks, and relaunched five months later. The lesson: similarity metrics measure whether the output is about the right topic. They do not measure whether the output is factually correct.

## Why Similarity Scores Miss Factual Errors

Similarity metrics — cosine similarity of embeddings, ROUGE overlap, BERTScore alignment — measure topical relevance and surface-level coherence. If the reference text discusses contract payment terms and the model output also discusses contract payment terms, similarity is high. The metric does not verify that the specific payment terms match. A reference that says "net 30 payment terms" and an output that says "net 60 payment terms" are semantically similar — both are about payment timelines — but factually inconsistent. The embedding vectors for "30 days" and "60 days" are close in semantic space because they are both temporal expressions. The similarity score does not drop enough to signal an error.

This failure mode is particularly severe for models trained to paraphrase and summarize. The model learns that copying exact wording is unnecessary — it can rephrase, reorder, and compress. That learned behavior is correct for fluency and naturalness. But when the model rephrases a number, a date, or a name, it sometimes gets the rephasing wrong. It says "the company was founded in 1998" when the source says 1989. It says "the defendant owes $12,000" when the source says $21,000. It says "Dr. Sarah Mitchell" when the source says "Dr. Sara Mitchells." These are not semantic shifts — they are factual errors. But to a similarity metric, they look like minor surface variation.

The legal tech company's post-mortem revealed that 87% of the factual errors occurred in numerical values, dates, and proper names. The model had learned strong patterns for paraphrasing prose — "the agreement stipulates" became "the contract requires," "pursuant to section 3.2" became "under clause 3.2." But it had weaker patterns for preserving exact numbers and names. When the training data contained minor numerical inconsistencies, the model learned that approximate numbers were acceptable. When the eval pipeline measured only similarity, it never signaled that this behavior was wrong.

## Claim Extraction and Verification

**Factual consistency checking** starts with decomposing the model output into verifiable claims. A claim is a discrete factual statement that can be true or false. "The contract was signed on March 15, 2025" is a claim. "The liability cap is $500,000" is a claim. "The payment terms are net 30" is a claim. The first step in factual consistency eval is extracting every claim from the model output, then verifying each claim against the source material.

Claim extraction can be done with rule-based parsers for structured domains — parsing dates, currency amounts, percentages, named entities. It can also be done with an LLM prompted to list every factual assertion in the text. A GPT-5 prompt that says "extract every factual claim from the following summary as a numbered list" produces a structured set of claims. Once you have the claims, you need to verify them. Verification means checking whether each claim is supported by the source document. If the claim says "the contract was signed on March 15, 2025" and the source document says "this agreement is executed on March 15, 2025," the claim is verified. If the source says "executed on March 12, 2025," the claim is false.

Verification can be done with exact string matching for simple cases — looking for the date or number in the source text. But exact matching fails when the source uses different phrasing. The source might say "fifteen thousand dollars" and the claim says "$15,000." The source might say "mid-March 2025" and the claim says "March 15, 2025." For these cases, you need semantic matching. You embed the claim and search the source document for sentences with high semantic similarity, then use an LLM to judge whether the retrieved sentence supports, contradicts, or is neutral toward the claim.

A factual consistency pipeline in 2026 looks like this: model generates output, claim extractor produces a list of claims, each claim is verified against the source using retrieval plus LLM judgment, unverified claims are flagged, the output is scored based on what percentage of claims are verified. If 95% of claims are verified, the output is factually consistent. If 70% are verified, the output has introduced unsupported information. You set a threshold — 90% verified, 95% verified — and fail outputs below it.

## The Hallucination Detection Problem

Factual consistency is a subset of the broader **hallucination detection** problem. Hallucination occurs when a model generates content that is not grounded in the input. In summarization, hallucination means the summary contains claims not present in the source document. In question answering, hallucination means the answer contains information not present in the retrieved context. In dialogue, hallucination means the model invents facts, events, or attributes that were never mentioned.

Detecting hallucination is harder than detecting factual inconsistency because hallucination can be subtle and context-dependent. A customer support model says "your order will arrive tomorrow." If the user never mentioned an order, this is a hallucination. If the user mentioned an order ten turns ago and the model is referring back to it, this is correct context usage. The difference is not in the sentence itself — it is in the conversational history. A claim extractor that looks only at the current model output cannot tell the difference.

For summarization and QA tasks, factual consistency is well-defined: every claim in the output must be supported by the source. For dialogue and generative tasks, the boundary is fuzzier. The model might make reasonable inferences that are not explicitly stated in the source. A user says "I live in Seattle." The model says "you will need rain gear this week." The model did not hallucinate — it inferred weather patterns from location. But it also made a claim not grounded in the user's utterance. Whether this is acceptable depends on your task definition and risk tolerance.

The practical approach is to define grounding scope. For high-risk domains — medical, legal, financial — every claim must be directly supported by source material. Inferences are not allowed. For low-risk domains — entertainment recommendations, casual conversation — reasonable inferences are acceptable as long as they are not presented as definitive facts. Your factual consistency eval enforces the grounding scope you define. A strict eval fails any claim not directly extractable from the source. A lenient eval allows claims that are plausible given the source.

## Claim-Level Attribution and Scoring

Factual consistency evals produce **claim-level scores**, not just document-level scores. Instead of a single number representing overall consistency, you get a breakdown: 12 claims extracted, 11 verified, 1 unverified. This granularity is critical for debugging. If your summary fails factual consistency, you need to know which specific claim was wrong. Was it a date error? A name error? A monetary amount? Claim-level attribution tells you.

You can also weight claims by importance. In a legal contract summary, a wrong liability cap is more severe than a wrong signing date. In a medical summary, a wrong dosage is catastrophic; a wrong appointment time is not. You assign severity weights to claim types — high severity for numbers and entities related to obligations, medium severity for dates and timelines, low severity for descriptive details. Your factual consistency score is a weighted sum. If a high-severity claim fails, the output fails regardless of how many low-severity claims pass.

Some teams track claim types over time. They log every factual error and categorize it: date error, name error, numerical error, inference error, entity confusion. Over six months, they see that 60% of errors are numerical, 25% are names, 15% are dates. This tells them where to focus their model improvements. They add more examples of correct numerical handling to the training data. They fine-tune with a loss function that penalizes numerical errors more heavily. They rerun the eval and see numerical error rate drop from 60% to 20%. Claim-level tracking turned a vague quality problem into a specific, solvable engineering problem.

## Factual Consistency as a Distinct Dimension

Factual consistency is not the same as relevance, not the same as fluency, not the same as completeness. A summary can be highly relevant, perfectly fluent, and factually inconsistent. It can cover all the key topics, read naturally, and introduce three false claims. This is why factual consistency must be measured separately from other quality dimensions. You cannot assume that a high similarity score implies factual correctness. You cannot assume that a high LLM judge score implies no hallucination — the judge might not catch subtle numerical errors.

The legal tech company that spent $340,000 rebuilding their eval pipeline now runs four separate checks on every summary: semantic similarity to measure topical coverage, an LLM judge to measure fluency and coherence, a factual consistency check to verify claims, and a completeness check to ensure no key information was omitted. All four checks must pass. A summary that passes three but fails factual consistency is rejected. A summary that passes factual consistency but fails completeness is rejected. The four dimensions are independent. Measuring all four catches failures that any single metric would miss.

In 2026, factual consistency is a non-negotiable eval dimension for any task involving grounding — summarization, question answering, retrieval-augmented generation, report generation, contract analysis, medical documentation. If your model is expected to produce outputs grounded in source material, you must verify that grounding. Similarity alone is not sufficient. Fluency alone is not sufficient. You need explicit claim extraction, explicit verification, explicit tracking of unsupported statements. The teams that skip this step ship models that hallucinate. The teams that include it ship models that stay grounded.

In the next subchapter, we examine citation-to-claim alignment scoring — how to verify not just that outputs are factually consistent, but that every citation actually supports the claim it is attached to.

