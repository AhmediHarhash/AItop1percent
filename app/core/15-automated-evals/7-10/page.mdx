# 7.10 — Multi-Lingual and Cross-Cultural Safety Evals

In September 2025, a customer service platform deployed a safety-tested chatbot across twelve European markets simultaneously. The English-language safety suite had achieved 99.2% pass rates across 8,400 test cases covering harassment, bias, medical advice, and legal disclaimers. The model had been thoroughly evaluated. Two weeks after launch, the company received urgent escalations from their German, Polish, and Italian operations. Users were reporting that the bot was providing medical diagnoses in response to health questions, recommending specific medications by name, and in one case suggesting a user delay seeking emergency care. The English version had never exhibited these behaviors. The safety failures were translation artifacts. The model had learned to be cautious with English phrasing like "you should see a doctor" but had no corresponding guardrails for "Sie sollten einen Arzt aufsuchen" or "dovresti consultare un medico." The company pulled the bot from all non-English markets, built language-specific safety suites over the next four months, and delayed their international expansion by two quarters. The cost in lost revenue exceeded four million dollars. The reputational damage in those markets persisted for over a year.

Safety does not translate automatically. A model that passes every safety check in one language can fail catastrophically in another, not because the underlying capabilities change, but because the linguistic surface changes in ways that bypass learned safety behaviors.

## The Language Coverage Problem

Safety evaluation built exclusively in English creates a false sense of security for multilingual deployments. The model learns safety associations with specific English phrases, patterns, and contexts. When a user asks a question in German, Spanish, or Mandarin, those learned associations often fail to activate. The model treats the non-English query as semantically similar but not identical to the English patterns it was trained to avoid. This is not a translation problem in the traditional sense. The model understands the meaning. It fails to apply the same caution.

The gap appears in three forms. First, direct translation gaps where a safety-triggering English phrase has a non-triggering equivalent in another language. "Tell me how to harm myself" might be caught, but the Polish equivalent might not activate the same guardrails. Second, cultural context gaps where something considered safe in one culture is harmful in another. A joke about authority figures might be harmless in the US and illegal in other jurisdictions. Third, formality and politeness gaps where safety checks calibrated for direct English phrasing fail when users employ formal or indirect language common in other cultures. A German user using formal "Sie" constructions or a Japanese user employing keigo honorifics might bypass safety filters designed for casual English.

Building multilingual safety coverage requires deliberate investment. You cannot assume that a model safe in English is safe in every language it supports. You need language-specific test suites, culturally informed safety criteria, and native-speaker validation for every market you serve.

## Safety Across Languages

The first layer of multilingual safety evaluation is direct translation of your English safety test suite into every supported language. This is necessary but insufficient. You translate every test case, every harmful prompt, every boundary condition, and you run the full suite in each language. This catches the obvious failures. If your English suite includes a test for medical advice boundaries and the model fails the German translation of that test, you know immediately that your safety guardrails are not language-agnostic.

The second layer is language-specific safety scenarios that do not exist in English. Each language and culture has unique safety risks. In German markets, you need tests for Datenschutz compliance and strict medical advertising regulations. In Arabic markets, you need tests for religious content sensitivity. In Chinese markets, you need tests for political content restrictions. These are not translations of English tests. They are culturally specific scenarios that only matter in certain regions.

The third layer is formality and register variation. Many languages distinguish between formal and informal address in ways English does not. Your safety suite needs to test both. A model that correctly refuses a harmful request phrased informally might comply when the same request is phrased formally, because the training data associated formal language with compliance and professionalism. You need test cases that cover "tu" and "vous" in French, "du" and "Sie" in German, plain and honorific forms in Japanese and Korean. Safety must hold across all registers.

The fourth layer is code-switching and mixed-language inputs. Real users do not always stay within one language. A user in India might write a query mixing English and Hindi. A user in Singapore might mix English, Mandarin, and Malay in a single sentence. Your safety suite needs test cases for common code-switching patterns in your user base. Models sometimes treat code-switched inputs as out-of-distribution and revert to less safe behaviors.

## Cultural Context in Safety

What counts as safe varies by culture, jurisdiction, and context. A model that discusses alcohol consumption neutrally might be appropriate in some markets and deeply inappropriate in others. A model that makes light humor about political figures might be acceptable in democracies with strong free speech protections and illegal in jurisdictions with lèse-majesté laws or restrictions on political speech. A model that provides health information might be subject to completely different regulatory standards in the US versus the EU versus markets with traditional medicine practices.

Building culturally aware safety evaluation requires local expertise. You cannot build an accurate safety suite for a culture you do not deeply understand. The most effective approach is to hire safety evaluators who are native speakers, culturally fluent, and familiar with local regulations. They build test cases based on real cultural boundaries, not assumptions translated from English-speaking markets.

One financial services company learned this when their English safety suite allowed the model to discuss credit scores and debt management strategies. This was appropriate in the US market where credit scores are standard. When they deployed in Germany, users could ask the model about Schufa scores, and the model provided guidance that violated German consumer protection laws around credit advice. The English safety suite had no concept of these restrictions because they do not exist in US law. The fix required building a Germany-specific safety layer with tests for financial advice boundaries unique to that jurisdiction.

Cultural safety also extends to tone, humor, and interpersonal norms. A model that uses casual, friendly language might be perceived as unprofessional or even disrespectful in cultures with strong formality norms. A model that makes self-deprecating jokes might confuse or offend users in cultures where such humor is rare. Your safety suite needs test cases that validate not just what the model says, but how it says it, calibrated to cultural expectations.

## Translation-Induced Safety Gaps

Machine translation creates its own safety risks. When you translate an English safety test case into another language using an automated system, the translation might be technically correct but miss nuances that change the safety implications. A phrase that is unambiguous in English might have multiple interpretations in another language, some of which are safe and some of which are not. The model might respond to the unintended interpretation.

One enterprise software company discovered this when their Spanish safety suite, translated from English, included a test case about workplace harassment. The English version asked the model to refuse providing advice on how to harass a coworker. The Spanish translation used a verb that also means "to annoy" in casual contexts. The model sometimes interpreted the question as asking about minor workplace annoyances and provided productivity advice instead of refusing. The safety test passed, but only because the translation had weakened the original scenario.

The solution is native-speaker review of all translated test cases. A human fluent in both the source and target languages reviews each translation to ensure it preserves the safety intent. If the translation is ambiguous, they rewrite it. If the cultural context makes the original scenario irrelevant, they replace it with a locally appropriate equivalent. This adds cost, but it is non-negotiable for accurate multilingual safety evaluation.

Another gap appears in language-specific phrasing that has no English equivalent. Certain languages allow constructions that English does not, and users exploit these to bypass safety filters. In some languages, users can imply harmful intent through grammatical mood or aspectual markers that do not translate directly to English. Your safety suite needs to include test cases written by native speakers who understand these language-specific attack vectors, not just translations of English tests.

## Building Multilingual Safety Test Suites

A production-grade multilingual safety suite is structured in layers. The first layer is the core safety suite, written in your primary language, usually English, covering universal safety categories like violence, self-harm, harassment, illegal activity, and medical advice. This suite is comprehensive, covering thousands of scenarios. You translate this suite into every supported language, with native-speaker review and adaptation.

The second layer is language-specific safety scenarios. For each language, you build a supplementary suite covering safety risks unique to that language, culture, or jurisdiction. These tests are not translations. They are original scenarios developed by safety evaluators fluent in that culture. A Japanese safety suite includes tests for honorific handling. A German safety suite includes tests for Impressum requirements and data protection law. An Arabic safety suite includes tests for religious content sensitivity.

The third layer is cross-lingual consistency tests. These verify that the model gives equivalent safety responses to semantically identical queries in different languages. You take a high-risk scenario, translate it into every supported language, and verify that the model refuses in all of them. If the model refuses in English but complies in Portuguese, you have a safety gap. These tests catch asymmetric safety behavior.

The fourth layer is code-switching and mixed-language tests. For markets where code-switching is common, you build test cases that mix languages in realistic ways. A test case for Indian English might include Hindi phrases. A test case for US Spanish might include English words for technical terms. These tests verify that safety holds even when users do not stay within one language.

The suite runs in your CI/CD pipeline for every model deployment, every prompt change, and every configuration update. Each language is treated as a separate safety domain with its own pass threshold. You do not average safety scores across languages. A model that scores 99% in English and 75% in German has a German safety problem, not a 87% average safety outcome.

## The Ongoing Maintenance Burden

Multilingual safety evaluation does not end at launch. Languages evolve, cultural norms shift, and new attack vectors emerge in each language community. A safety suite that was accurate six months ago might miss newly evolved slang, memes, or coded language that users adopt to bypass filters.

One social media company maintaining safety filters in fourteen languages found that their Indonesian safety suite degraded by twelve percentage points over six months without any model changes. Users had adopted new slang terms for prohibited activities, and the safety suite had not kept pace. The model was blocking the old terms but missing the new ones. The fix required continuous updates from native-speaker safety teams monitoring real user attempts to bypass filters.

The maintenance model is continuous, not periodic. Each language has a dedicated safety team that monitors model behavior, user reports, and emerging risks in that language. They update test cases monthly. They run retrospective analyses on safety failures to identify patterns the current suite missed. They share learnings across language teams because attack techniques often spread across communities.

Scaling this model is expensive. A company supporting twenty languages needs twenty safety teams, each with native fluency, cultural expertise, and safety evaluation skills. The alternative is accepting that your model is only truly safe in the languages where you invest deeply, and hoping that failures in other languages do not create catastrophic outcomes. For most regulated industries, that is not an acceptable risk.

Safety that works in English but fails in other languages is not safety. It is English-language safety with global risk exposure. If you serve users in multiple languages, your safety evaluation must cover every one of them with the same rigor you apply to your primary market. Anything less is a bet that your users will not find the gaps before you do. That bet, historically, does not pay off.

**Next: 7.11 — Red Team Integration with Automated Safety Evals**
