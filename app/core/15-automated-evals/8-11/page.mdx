# 8.11 — Pipeline Monitoring and Alerting

In July 2025, a conversational AI company discovered that their eval pipeline had been failing silently for three weeks. The pipeline ran nightly, executed 8,000 test cases, and reported results to a dashboard. The dashboard showed green for 21 consecutive days. Engineering trusted the dashboard and deployed four prompt changes during that period. On day 22, a manual investigation revealed that the pipeline had not actually executed evaluators since day 1—a dependency version conflict caused evaluators to return null scores, which the aggregation logic interpreted as passing. The dashboard showed 100% pass rate because every test case "passed" by default. The four prompt changes had gone to production without real evaluation. Two of them caused user-facing quality regressions that took a week to identify and another week to roll back. The cost was not just engineering time. It was user trust. The root cause was not the dependency conflict—those happen. The root cause was that no one was monitoring whether the eval pipeline itself was healthy.

## The Eval Pipeline as a System Under Test

Your eval pipeline is infrastructure. Like all infrastructure, it can fail in ways that are silent, gradual, and undetectable without instrumentation. The failure modes are different from application failures. An application that crashes returns a 500 error and triggers an alert. An eval pipeline that fails silently returns a 200 status code and writes results that look correct but are not. The failure is in the semantics, not the syntax. The pipeline executed, the database wrote rows, the dashboard rendered charts, but the results are wrong.

The first failure mode is evaluator crashes. An evaluator raises an exception, the exception is caught and logged, and the test case is marked as passed because no failure was explicitly recorded. This happens when exception handling is too broad—catch all exceptions, log them, and continue. The intent is to prevent one bad test case from stopping the entire run. The consequence is that a systemic evaluator failure looks like a 100% pass rate. The solution is to distinguish between test case failures and evaluator failures. If an evaluator crashes, the test case should be marked as "error" not "passed," and the run should be marked as incomplete.

The second failure mode is data pipeline failures. The eval dataset fails to load, or loads partially, or loads an old version. The pipeline runs, evaluates the cases it found, and reports a pass rate. If the dataset is smaller than expected, the pass rate may be artificially high because the hardest cases are missing. If the dataset is an old version, the pass rate may be stable even though the model regressed on new cases. The solution is to assert dataset properties before execution. Check the row count, check the schema, check the hash. If the dataset does not match expectations, fail the pipeline before running evaluators.

The third failure mode is infrastructure degradation. The pipeline runs on a cluster that is oversubscribed, and evaluators time out at a higher rate than normal. The timeout cases are marked as failed, but the failure is not a model regression—it is an infrastructure problem. If you do not monitor timeout rates separately from correctness failures, you interpret the quality drop as a model issue and waste time investigating the wrong root cause. The solution is to track timeout rate, out-of-memory errors, and other infrastructure signals as distinct metrics. If timeout rate spikes, alert on infrastructure health, not model quality.

The fourth failure mode is configuration drift. The pipeline references a config file that is updated independently of the pipeline code. The config changes a threshold, disables a metric, or repoints to a different dataset. The pipeline runs with the new config, produces different results, and no one notices because the config change was not tied to a pipeline version. The solution is to version configs alongside code and fail the pipeline if the config version does not match the expected version. Some teams hash the config file and include the hash in the run metadata. If the hash changes between runs, the results are flagged as potentially incomparable.

## Instrumentation for Pipeline Health

Monitoring an eval pipeline requires instrumenting every stage: dataset load, evaluator execution, result aggregation, result storage, and reporting. Each stage emits metrics and logs. The metrics track throughput, latency, error rate, and resource utilization. The logs capture failure details, warnings, and context that metrics cannot encode.

Dataset load metrics include row count, schema validation result, hash verification result, and load duration. If any of these metrics deviates from the expected range, the pipeline should alert and fail before running evaluators. A row count that is 10% lower than the baseline is a signal that the dataset is incomplete. A schema validation failure is a signal that the dataset format changed. A hash mismatch is a signal that the dataset content changed. All three are reasons to stop.

Evaluator execution metrics include evaluator invocation count, success count, failure count, error count, timeout count, and latency distribution. These metrics are tracked per evaluator and per test case type. If an evaluator that normally succeeds 99% of the time starts succeeding 80% of the time, that is a signal. If latency for a specific evaluator increases from 200ms to 2 seconds, that is a signal. If timeout rate increases from 0.1% to 5%, that is a signal. All three suggest that the evaluator or its dependencies have degraded.

Result aggregation metrics include the number of results processed, the number of results with missing scores, the number of results with null outputs, and the distribution of pass/fail/error statuses. If 10% of results have missing scores, that is an evaluator failure. If 20% of results have null outputs, that is a system-under-test failure. Both should trigger alerts and mark the run as incomplete.

Result storage metrics include write throughput, write latency, write error rate, and storage utilization. If write latency spikes, the database may be overloaded or the schema may need optimization. If write error rate increases, the database may be rejecting records due to schema violations or constraint failures. Both indicate that results are not being persisted correctly.

Reporting metrics include the number of reports generated, report generation latency, and report delivery success rate. If a report fails to generate, stakeholders do not see the results, and the pipeline appears to have succeeded even though its output is incomplete. Report generation and delivery are part of the pipeline's contract, not optional post-processing.

## Alerting on Pipeline Failures

An alert fires when a metric crosses a threshold or a condition is violated. The threshold is either static—timeout rate exceeds 5%—or dynamic—timeout rate is 3 standard deviations above the 30-day mean. Static thresholds are easier to set but less adaptive. Dynamic thresholds are harder to tune but catch anomalies that static thresholds miss.

Alerts are categorized by severity: critical, warning, info. A critical alert means the pipeline failed and results are invalid. A warning alert means the pipeline succeeded but something unusual happened. An info alert means the pipeline succeeded and a metric of interest crossed a boundary. Critical alerts page on-call engineers. Warning alerts post to a Slack channel. Info alerts write to logs. If every alert is critical, alert fatigue sets in and engineers stop responding. If no alert is critical, incidents are discovered late.

A critical alert triggers when the dataset fails to load, when more than 5% of evaluators crash, when more than 10% of results are missing scores, when result writes fail, or when the pipeline exits with a non-zero status code. These are all conditions that invalidate the run. The pipeline should not report results to the dashboard when a critical alert is active. It should mark the run as failed and surface the failure prominently. If the dashboard shows green when the pipeline is red, the monitoring system has failed its purpose.

A warning alert triggers when timeout rate exceeds baseline, when a specific evaluator's error rate increases, when dataset row count is outside the expected range but still above the minimum threshold, or when report generation is delayed. These conditions do not invalidate the run but warrant investigation. The engineering team reviews the alert, determines whether the cause is transient or structural, and decides whether to re-run the pipeline or accept the results.

An info alert triggers when a new model version is evaluated for the first time, when the dataset version changes, when evaluator versions are updated, or when a scheduled run completes successfully after a prior failure. These events are informational—they provide context for interpreting results but do not indicate a problem.

## The Pipeline Health Dashboard

The pipeline health dashboard is separate from the model quality dashboard. The model quality dashboard shows eval results—pass rates, failure distributions, metric trends. The pipeline health dashboard shows pipeline execution—run status, duration, resource utilization, error rates, alert history. The two dashboards serve different audiences. The model quality dashboard is for ML engineers, product managers, and stakeholders. The pipeline health dashboard is for platform engineers and SREs.

The pipeline health dashboard shows the status of the last N runs in a timeline view. Each run is color-coded: green for success, yellow for success with warnings, red for failure. Clicking a run expands details: which stages completed, which stages failed, which alerts fired, which metrics were out of range. The timeline view makes patterns visible. If every run for the past week is red, the pipeline is broken. If every third run is yellow, there is an intermittent issue. If runs are green except for one red spike, that was a transient failure.

The dashboard also shows per-stage metrics over time. A chart for dataset load shows row count per run. A chart for evaluator execution shows success rate per evaluator per run. A chart for result storage shows write latency per run. These charts answer questions like "did evaluator X start failing gradually or suddenly?" and "has write latency been increasing over the past month?" Patterns that are invisible in a single run become obvious when plotted over dozens of runs.

The dashboard includes SLOs for the pipeline. An SLO might be "95% of runs complete within 30 minutes" or "99% of runs have fewer than 1% evaluator errors." If the SLO is violated, the dashboard surfaces it. SLO violations are a signal that the pipeline is degrading, even if individual runs are still passing. Addressing SLO violations before they cascade into critical failures is the difference between proactive and reactive infrastructure management.

## When the Eval System Itself Fails

The eval pipeline is a dependency for every other system. If the eval pipeline is down, you cannot validate changes, you cannot deploy with confidence, and you cannot investigate regressions. The eval pipeline has a higher reliability requirement than most application infrastructure because its failure blocks the entire development and deployment process.

The failure modes that are hardest to detect are semantic failures—results that are wrong but look right. The dependency conflict that returned null scores is one example. Another is an evaluator that silently changes behavior due to a library upgrade. The evaluator still runs, still returns scores, but the scores mean something different than they did before. If no one notices, the scores are compared to historical baselines and interpreted as model performance changes when they are actually evaluator changes.

The defense against semantic failures is meta-evaluation—evaluating the evaluators. Meta-evaluation runs a golden set of inputs through each evaluator and asserts that the outputs match expected results. The golden set includes edge cases designed to catch common bugs: null inputs, empty inputs, malformed inputs, inputs that previously caused crashes. If an evaluator's output on the golden set changes, the evaluator has drifted and should not be used until the change is investigated. Meta-evaluation runs before every pipeline execution. It is a health check for the evaluation logic itself.

Another defense is redundancy. Critical evaluators are implemented twice, using different libraries or different approaches. If the two implementations agree, the result is trusted. If they disagree, the test case is flagged for manual review. This catches bugs that would be invisible in a single-evaluator setup. Redundancy doubles the compute cost, so it is reserved for high-stakes evaluations—compliance checks, safety filters, user-facing quality gates.

## Building Reliability Into the Pipeline From Day One

Pipeline monitoring is not infrastructure you add after the pipeline is in production. It is part of the pipeline's design. The pipeline should emit metrics from the first version. The dashboard should exist before the first production run. The alerting rules should be defined when the pipeline is defined. If you wait until the pipeline is critical infrastructure to add monitoring, you will operate blind during the period when you are learning the failure modes.

The reliability requirements for the eval pipeline scale with the organization's dependence on it. A startup with three engineers can tolerate a pipeline that fails once a week and requires manual restarts. An enterprise with 200 engineers and 50 model deployments per week cannot. The pipeline must run on schedule, handle transient infrastructure failures, retry failed stages, and alert on persistent failures without human intervention. This level of reliability does not happen by accident. It requires investment in observability, alerting, self-healing logic, and runbooks for failure scenarios.

The eval pipeline is the infrastructure that makes AI engineering systematic, repeatable, and trustworthy. It is the system that tells you whether your changes are safe to deploy. It is the system that tells you whether your model is regressing. It is the system that provides the evidence trail for every decision. If the pipeline is unreliable, everything built on top of it is unreliable. Monitoring the pipeline is not a nice-to-have. It is the foundation of confidence. The architecture is complete when the pipeline can fail loudly, recover gracefully, and report its own health with the same rigor it applies to the models it evaluates. With that foundation in place, the next challenge is detecting when the world has changed—when the eval results are stable but the real-world distribution is not.

---

**Next: Chapter 9 — Calibration and Drift Detection**
