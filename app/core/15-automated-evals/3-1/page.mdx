# 3.1 â€” Heuristics as Early Warning Systems

Heuristics detect anomalies, not quality. They run in milliseconds, cost nothing, and catch the most common failure modes before you spend dollars on model-based evaluation. A heuristic does not tell you if an output is good. It tells you if an output is suspicious enough to warrant deeper inspection. The teams that master automated evaluation understand this distinction. The teams that struggle treat heuristics as definitive judges and model-based evals as optional luxuries.

## What Heuristics Are and Why They Matter

A **heuristic** is a simple, deterministic rule that approximates a quality check without semantic understanding. It operates on surface features: length, repetition, formatting, presence or absence of patterns. Heuristics do not parse meaning. They do not understand context. They count words, detect duplicated phrases, check for expected structure. In exchange for this shallowness, they execute in single-digit milliseconds and scale to millions of outputs without marginal cost.

Heuristics matter because they provide the first line of defense in a tiered evaluation system. When a model starts generating outputs that are 10x longer than expected, a word count heuristic catches it immediately. When a fine-tuned model begins repeating the same phrase three times in every response, an n-gram detector flags it before a single user complains. When an agent enters an infinite loop, a repetition heuristic stops the process before it consumes thousands of dollars in API calls. Heuristics are cheap anomaly detectors. They do not replace deeper evaluation. They triage it.

The mistake teams make is treating heuristics as binary pass-fail gates. A heuristic violation is not a failure verdict. It is a flag that says: this output is different from the expected baseline in a way that correlates with problems. Investigate further. The difference between a flag and a verdict is the difference between a system that catches real issues and a system that blocks half of production traffic with false positives.

## Heuristics as Approximate but Fast Signals

The value of a heuristic is its speed relative to its predictive power. A word count threshold takes 0.3 milliseconds to compute. A GPT-5-mini evaluation call takes 180 milliseconds and costs two cents. If word count correlates with quality issues 60% of the time, you run word count on every output and escalate the 15% that violate thresholds to model-based evaluation. You just saved 85% of your eval budget while catching most of the problems.

The tradeoff is precision. Heuristics produce false positives and false negatives. An output can be verbose and correct. An output can be terse and wrong. A heuristic that flags all responses over 300 words will catch the model that started generating essays when it should write two-sentence summaries. It will also flag legitimate long-form explanations where 300 words is appropriate. The heuristic does not know the difference. It only knows the pattern.

The way to manage this is calibration. You build heuristics by analyzing historical data. You measure how often a heuristic violation correlates with a real quality issue. If 80% of outputs flagged for excessive length fail subsequent human review, the heuristic is valuable. If 10% do, the threshold is miscalibrated or the heuristic does not apply to your task. Heuristics are not guesses. They are measured signals tuned to your specific system.

## The Role of Heuristics in Tiered Evaluation

Tiered evaluation is the pattern where cheap checks run first and expensive checks run only on flagged outputs. Heuristics occupy Tier 1. They run on every output, always, in real time. Rule-based checks occupy the same tier: formatting validation, schema compliance, required field presence. Both are deterministic, fast, and approximate.

Tier 2 is model-based evaluation with smaller, cheaper models. GPT-5-mini scoring outputs for coherence, Claude Haiku 4.5 checking for policy violations. These run on outputs flagged by Tier 1 or sampled at a fixed rate. Tier 3 is expensive evaluation: GPT-5 with long prompts, multi-turn verification, human review. These run only on outputs that failed Tier 2 or represent high-stakes decisions.

Heuristics enable this pyramid. Without them, you either evaluate everything with expensive models and burn budget, or you sample randomly and miss systematic failures. With heuristics, you catch the obvious anomalies immediately and route only the suspicious cases to deeper inspection. The cost profile changes from linear to logarithmic. At 10,000 outputs per day, heuristics cost nothing. Model-based evals cost dollars per output. Tiered evaluation with heuristics as the first filter brings the total cost down to 5-15% of what full model-based evaluation would require.

The failure mode is skipping Tier 1 entirely. Teams that go straight to model-based evaluation because heuristics feel crude discover that their eval pipeline costs more than their inference pipeline. They throttle evaluation to stay within budget. Silent degradation follows. The crude heuristic that catches 60% of problems for zero marginal cost is better than the perfect eval you run on 2% of traffic because you cannot afford more.

## Heuristics as Anomaly Detectors Not Quality Judges

A heuristic does not tell you if an output is good. It tells you if an output deviates from baseline in a measurable way. The deviation might indicate a problem. It might indicate a valid edge case. The heuristic does not know. It only signals: this is different, pay attention.

The analogy is a smoke detector. Smoke does not always mean fire. It might mean burnt toast. But smoke correlates with fire often enough that detecting smoke is worth the false alarms. A heuristic that flags outputs with more than 40% repeated bigrams does not prove the output is bad. It proves the output is statistically unusual in a way that correlates with degenerate loops and memorization failures. You investigate. Sometimes you find a real issue. Sometimes you find a valid use case where repetition is appropriate. Either way, you are now aware.

The mental shift required is from gating to monitoring. A heuristic should not block production traffic unless the anomaly is so severe that shipping the output is unacceptable risk. Most heuristics should log, flag, and escalate. The output goes to the user. The flag goes to the evaluation queue. If the heuristic consistently flags outputs that later fail human review, you tighten the threshold or add a blocking rule. If the heuristic flags outputs that pass review, you loosen the threshold or remove the heuristic entirely.

This requires telemetry. You need to track heuristic violations alongside downstream outcomes. Did the flagged output get escalated to model-based eval? Did it fail? Did a human reviewer mark it as correct or incorrect? Without this feedback loop, heuristics drift. A threshold calibrated on 2024 data becomes useless after six months of model updates and task evolution. The heuristic that once caught 70% of verbosity issues now catches 30% because the model's baseline verbosity shifted. You recalibrate or retire the heuristic. You do not keep running a check that produces noise.

## When to Trust Heuristics vs Escalate

Heuristics are trustable when the violation is extreme and the correlation with failure is high. If an output contains the same 5-word phrase repeated 18 times in a row, you do not need a model-based eval to confirm the problem. The heuristic is definitive. If an output is 4,000 words when the task expects 200, you do not need semantic analysis to know something broke. The heuristic alone justifies action.

Heuristics require escalation when the violation is moderate and context-dependent. An output that is 350 words when the task expects 200 might be a problem. It might be a complex query that legitimately required more explanation. A heuristic cannot distinguish these cases. It flags the output. A Tier 2 eval reads the output and decides. The heuristic did its job by surfacing the anomaly. The model-based eval did its job by interpreting it.

The decision framework is: How confident are you that this violation always indicates a problem, regardless of context? If confidence is above 95%, block or auto-fail based on the heuristic alone. If confidence is 60-95%, flag and escalate to Tier 2. If confidence is below 60%, the heuristic is not useful. Remove it. The threshold is not arbitrary. It comes from measuring historical data. You annotate 1,000 outputs. You apply the heuristic. You calculate: of the outputs flagged, what percentage were actually bad? That percentage is your confidence. If it is too low, the heuristic wastes eval budget on false positives. If it is high enough, the heuristic earns its place in Tier 1.

The second variable is cost of failure. In a low-stakes task, you can tolerate false negatives. A heuristic that catches 50% of problems might be good enough if the uncaught problems cause minor UX friction. In a high-stakes task, you cannot tolerate misses. A heuristic that catches 90% of problems is insufficient if the 10% it misses involve patient safety or financial errors. You add more heuristics, tighten thresholds, and escalate more aggressively to Tier 2 and Tier 3. The cost of running extra evaluation is cheaper than the cost of shipping a dangerous output.

## The Heuristic Development Cycle

You do not invent heuristics from intuition. You discover them from failure analysis. When a batch of outputs fails human review, you ask: what surface feature do these failures share? They are all over 500 words. They all contain the phrase "I apologize" three or more times. They all lack a structured list when the task requires one. That shared feature becomes a candidate heuristic. You measure its precision and recall on historical data. If the numbers justify it, you deploy the heuristic to Tier 1.

Heuristics evolve as the system evolves. A heuristic that worked well with GPT-4 might be irrelevant with Claude Opus 4.5 because the new model has different baseline characteristics. A heuristic that caught refusals in zero-shot prompts might stop firing after you add few-shot examples that reduce refusal rates. You review heuristic performance monthly. You retire the ones that no longer correlate with failures. You add new ones as new failure modes emerge.

The pattern is: observe failures, extract features, measure correlation, deploy if justified, monitor performance, retire if ineffective. Heuristics are not permanent fixtures. They are adaptive tools that reflect the current state of your system. The team that treats them as static rules ends up with a Tier 1 eval layer full of obsolete checks that flag nothing useful and ignore the failure modes that matter in 2026.

Length and verbosity are the most common heuristic categories because they are universal to almost every task and easy to measure without semantic understanding.

