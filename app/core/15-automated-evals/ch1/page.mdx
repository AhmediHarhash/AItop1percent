# Chapter 1 — Foundations of Automated Evaluation

Automated evaluation is not a replacement for human judgment. It is an extension of human judgment — a way to apply your quality standards to every output your system produces, not just the ones humans have time to review. The foundations you learn in this chapter determine whether your automated evals become trusted signals that drive deployment decisions or noise that teams learn to ignore. You will learn why automated evals exist, the core categories that structure the field, the cheap-to-expensive tiering pattern that makes scale economical, how to design eval coverage that matches your actual risks, and how to think about eval systems as infrastructure with SLOs just like any other production service.

---

- 1.1 — Why Automated Evals Exist: Extending Human Judgment at Scale
- 1.2 — The Core Categories of Automated Evaluation
- 1.3 — The Cheap-to-Expensive Tiering Pattern
- 1.4 — Eval Coverage Strategy: Tasks, Risks, and User Journeys
- 1.5 — Sampling Strategy: Head, Tail, and Adversarial Cases
- 1.6 — SLOs for Quality: Defining Eval Targets Like Uptime
- 1.7 — The Eval Trust Hierarchy: Tier A, B, and C Signals
- 1.8 — Automation Must Be Anchored to Human Truth
- 1.9 — The Cost of Wrong Automation: False Confidence at Scale
- 1.10 — When Evals Lie: The Measurement-Reality Gap
- 1.11 — Eval System Design Principles for 2026
- 1.12 — The Eval Maturity Model: From Ad-Hoc to Continuous

---

*The team that treats automated evaluation as infrastructure — with the same rigor they apply to databases and APIs — is the team that ships confidently while others guess.*
