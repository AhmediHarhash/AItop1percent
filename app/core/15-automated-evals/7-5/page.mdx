# 7.5 â€” Helpfulness Without Hallucination Tradeoffs

Why do models trained for safety become less useful? The answer is obvious once you measure both dimensions: safety training and helpfulness training pull in opposite directions, and most teams optimize one at the expense of the other without realizing they have created a tradeoff curve they never intended. The model that refuses to answer "how do I reset my password" because it flagged the word "password" as sensitive is safe in the narrowest technical sense and completely useless. The model that cheerfully explains how to bypass security controls because it was optimized for user satisfaction is helpful in the narrowest sense and catastrophically unsafe. The product goal is not maximum safety or maximum helpfulness. It is the optimal point on the curve where both are high enough to ship.

The tradeoff exists because safety interventions reduce the model's willingness to answer, and helpfulness optimization increases it. Fine-tuning for refusal behavior teaches the model to say no. Fine-tuning for user satisfaction teaches the model to say yes. RLHF trained on safety feedback makes the model conservative. RLHF trained on helpfulness feedback makes the model permissive. If you apply both, the model learns conflicting objectives. If you apply only one, the model is unbalanced. The eval pipeline's job is to measure both dimensions, identify the curve, and validate that your model sits at the intended point.

## Over-Refusal as Safety Failure

Over-refusal is the failure mode where the model refuses requests it should handle. It is not a safety success. It is a safety failure. A medical chatbot that refuses to answer "what are the symptoms of dehydration" because it detected medical terminology is broken. A legal assistant that refuses to explain basic contract terms because it flagged legal advice policies is broken. A customer service bot that refuses to discuss account cancellations because it flagged sensitive account operations is broken. Users encountering these refusals do not conclude the system is safe. They conclude the system does not work.

Over-refusal happens when safety classifiers have high false-positive rates and the system defaults to refusal on any positive signal. The safety filter sees "symptoms," flags it as medical advice, and blocks the response. The safety filter sees "contract," flags it as legal advice, and blocks the response. The individual classifier decisions are defensible. The aggregate behavior is a product failure. Users cannot use the system for its intended purpose.

The organizational dynamic that produces over-refusal is risk aversion in safety teams. Trust and Safety sees every missed harmful output as an incident. Product sees every false refusal as user friction. When these teams do not share a measurement framework, Trust and Safety tunes classifiers to minimize false negatives, and false positives rise until the product is unusable. The resolution is not to override Trust and Safety. The resolution is to measure both false negatives and false positives as failures, set acceptable thresholds for both, and tune the system to meet both constraints.

Over-refusal is measured by helpfulness eval suites. You build a dataset of legitimate requests that the model should answer. You measure the refusal rate. If the model refuses 15 percent of legitimate requests in a customer service context, the system is over-refusing. If it refuses 40 percent, the system is unusable. The acceptable threshold is context-dependent. A creative writing assistant can tolerate higher refusal rates than a customer service bot. An internal compliance tool can tolerate higher refusal rates than a public-facing chatbot. But every system has a threshold. The eval pipeline enforces it.

## Under-Refusal as Safety Failure

Under-refusal is the failure mode where the model answers requests it should refuse. A financial chatbot that provides specific stock recommendations when policy prohibits personalized investment advice is under-refusing. A healthcare bot that diagnoses conditions when policy limits it to general information is under-refusing. A customer service bot that discloses confidential account details without verifying identity is under-refusing. These are not edge cases. They are core safety violations.

Under-refusal happens when helpfulness optimization dominates. The model is trained to satisfy user requests. The user asks for a diagnosis. The model provides one. The user asks for investment advice. The model complies. The model has learned that saying yes increases user satisfaction scores, and user satisfaction was the training signal. The safety constraints were either not included in training or were weighted too weakly to override the helpfulness objective.

The organizational dynamic that produces under-refusal is product pressure to reduce friction. Users complain about refusals. Product asks Engineering to make the model more helpful. Engineering loosens safety thresholds. The refusal rate drops. User satisfaction scores improve. Then the first policy violation reaches production, and Legal or Compliance intervenes. The system is rolled back or shut down. The team learns that under-refusal is not a product optimization. It is a regulatory and reputational risk.

Under-refusal is measured by safety eval suites. You build a dataset of requests the model must refuse. You measure the compliance rate. If the model refuses 98 percent of prohibited requests, it is under-refusing on 2 percent. If that 2 percent includes critical violations like medical diagnoses or financial advice, the system is unsafe. If it includes only moderate-risk cases that would be handled with disclaimers, the under-refusal rate might be acceptable. The severity of the missed refusals matters more than the raw percentage.

## Calibrating the Balance

The balance between helpfulness and safety is not found by intuition. It is found by measurement. You define acceptable thresholds for both over-refusal and under-refusal. You measure the model against both thresholds. You adjust the model or the safety filters until both thresholds are met. This is a constraint satisfaction problem, not a single-objective optimization.

A customer service chatbot sets thresholds: over-refusal must be below 5 percent on legitimate requests, and under-refusal must be below 1 percent on prohibited requests. The initial model refuses 12 percent of legitimate requests and allows 3 percent of prohibited requests. Both thresholds are violated. The team tunes the safety classifier to reduce false positives. Over-refusal drops to 4 percent. Under-refusal rises to 5 percent. The safety threshold is now violated. The team adjusts the classifier's confidence thresholds and adds contextual rules. Over-refusal stays at 4 percent. Under-refusal drops to 0.8 percent. Both thresholds are met. The model ships.

The acceptable thresholds are policy decisions. They are set by Product, Legal, and Trust and Safety together. They are not the same across deployments. A public-facing chatbot serving millions of users has tighter under-refusal thresholds than an internal tool used by trained employees. A chatbot in a regulated industry has tighter thresholds than one in an unregulated domain. A chatbot serving minors has tighter thresholds than one serving adults. The thresholds are inputs to the eval process, not outputs.

Some teams use a weighted cost model. Over-refusal has a cost in user friction and potential churn. Under-refusal has a cost in regulatory risk, reputation damage, and potential legal liability. The costs are not symmetric. A single under-refusal that violates HIPAA or securities regulations can cost millions. A single over-refusal costs one user interaction. The acceptable thresholds reflect this asymmetry. The model is tuned to minimize total expected cost, not to balance error rates at 50-50.

## Measuring Both Dimensions in the Same Eval

The helpfulness-safety tradeoff is only visible if you measure both in the same eval run. Separate evals produce separate metrics that Product and Trust and Safety optimize independently. Joint evals produce a shared view of the tradeoff curve. You see how changes in one dimension affect the other. You make informed decisions about where to sit on the curve.

A joint eval suite includes both legitimate requests and prohibited requests. The legitimate requests test helpfulness. The prohibited requests test safety. Each request is labeled with ground truth: should the model answer or refuse. The model's responses are scored on both dimensions. You calculate four rates: true positive helpfulness means the model answered a legitimate request, false positive helpfulness means the model answered a prohibited request, true positive safety means the model refused a prohibited request, false positive safety means the model refused a legitimate request.

False positive helpfulness is under-refusal. False positive safety is over-refusal. The eval pipeline tracks both. You plot them over time. You see the tradeoff. If a model update reduces over-refusal from 8 percent to 3 percent but increases under-refusal from 0.5 percent to 2 percent, you have moved along the curve. Whether that move is acceptable depends on the thresholds. If the under-refusal threshold is 1 percent, the update is rejected. If it is 3 percent, the update is accepted.

The joint eval also reveals cases where the model is neither helpful nor safe. A response that misinterprets a legitimate request as prohibited and refuses is a false positive safety failure. A response that misinterprets a prohibited request as legitimate and answers is a false positive helpfulness failure. A response that answers a legitimate request incorrectly is a true positive helpfulness attempt but a quality failure. The eval suite distinguishes all of these. You do not optimize for a single number. You optimize for a profile.

## The Pareto Frontier and Optimal Tradeoffs

The Pareto frontier is the set of models where improving one dimension requires degrading the other. If a model is below the frontier, you can improve both helpfulness and safety simultaneously. If a model is on the frontier, improving one means accepting a regression in the other. The goal is to reach the frontier and then choose the point that matches your risk tolerance.

Most models start below the frontier. The initial version over-refuses because safety filters are tuned conservatively, and it under-refuses because helpfulness optimization was done without safety constraints. The model refuses some legitimate requests and allows some prohibited requests. Both error rates can be reduced. You refine the safety classifier to reduce false positives. You add policy-aware training data to reduce false negatives. Over-refusal and under-refusal both drop. The model moves toward the frontier.

Eventually you hit the frontier. Reducing over-refusal further requires loosening safety filters, which increases under-refusal. Reducing under-refusal further requires tightening filters, which increases over-refusal. You are at the curve. The next decision is where to sit. If you are in a high-risk domain, you accept higher over-refusal to minimize under-refusal. If you are in a low-risk domain with high user friction sensitivity, you accept higher under-refusal to minimize over-refusal. The position on the frontier is a business decision informed by the eval metrics.

The eval pipeline measures proximity to the frontier. If the model's error rates are higher than the best observed rates from prior experiments, the model is below the frontier. If they match the best observed rates, the model is on the frontier. If a new technique or dataset allows you to reduce both error types simultaneously, you have pushed the frontier outward. The frontier is not fixed. It moves as models, training methods, and safety techniques improve. The eval system tracks it.

Teams that do not measure the frontier make random tradeoffs. They tune one dimension until someone complains, then tune the other until someone else complains. They never converge. Teams that measure the frontier make deliberate tradeoffs. They know the cost of each choice. They choose the point that aligns with organizational risk tolerance. The eval pipeline provides the data that makes the choice legible.

The next layer of safety evaluation is robustness against adversarial attacks: measuring how well the model resists jailbreaks and prompt injections, which are explicit attempts to bypass the safety and policy constraints the helpfulness-safety balance is designed to enforce.

