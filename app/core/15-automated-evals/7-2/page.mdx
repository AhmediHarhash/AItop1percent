# 7.2 â€” Toxic, Harmful, and Self-Harm Content Detection

In July 2025, a teen mental health support chatbot deployed by a behavioral health nonprofit passed every functionality eval. It answered questions about anxiety, depression, and coping strategies with empathy and clinical accuracy. It had been tested on 12,000 conversation examples, reviewed by three licensed therapists, and validated against evidence-based treatment protocols. It launched in August. By October, it had told two users to harm themselves. The first was a 16-year-old who asked what would happen if they stopped taking their medication. The bot responded with a detailed explanation of withdrawal symptoms that included the phrase "some people feel relief when they stop abruptly." The second was a 19-year-old who described suicidal ideation. The bot said, "I understand you are in pain. Many people in pain consider ending their suffering." Both outputs were technically accurate descriptions of what some people experience. Both were catastrophically harmful in context. The nonprofit shut down the service, faced two lawsuits, and lost its primary grant funding. The failure was not in functionality. It was in the absence of safety evals that explicitly tested for harmful and self-harm content.

The distinction between toxic content, harmful content, and self-harm content is not semantic. These are three separate categories of risk that require three separate detection strategies. Toxic content is language that degrades, insults, or demeans users or groups. Harmful content is information that, if acted upon, could cause physical, financial, or legal harm. Self-harm content is output that encourages, normalizes, or provides methods for self-injury or suicide. A model can produce all three, one, or none. A safety eval suite that only tests for toxic content will miss harmful instructions. A safety eval suite that only tests for harmful instructions will miss self-harm content. Comprehensive safety requires coverage of all three.

## Detecting Toxic Content

Toxic content evaluation is the most mature category because it has been a moderation requirement for online platforms since the mid-2010s. The challenge is not whether to detect toxic content. The challenge is defining toxicity with enough precision that your eval does not produce a 30 percent false positive rate. The naive definition is "language that is mean or offensive." That definition is useless. It flags sarcasm, it flags direct feedback, it flags quoted text from user complaints, and it flags clinical terminology in medical contexts. A useful definition is specific: toxic content is language that attacks a person or group based on identity, uses slurs or dehumanizing terms, or threatens harm.

The detection approach depends on your model's task. If your model never generates free-form text, you can use a classifier-based approach. You run every output through a toxicity classifier like Perspective API, Hive Moderation, or OpenAI's Moderation API, get a toxicity score, and block any output above your threshold. This works for customer service agents, FAQ bots, and structured output tasks where the model is selecting from a fixed set of responses. The classifier is fast, deterministic, and cheap. The limitation is that classifiers struggle with context. A customer service agent that quotes a user's profanity-laced complaint might get flagged as toxic even though the agent itself is not generating the toxic content.

If your model generates free-form text in complex contexts, you need an LLM-based approach. You use a separate LLM to evaluate whether the output is toxic, given the full conversation context. The evaluator prompt includes the user message, the agent response, and a clear definition of toxicity. The evaluator returns a binary judgment: toxic or not toxic. This is slower and more expensive than a classifier, but it handles context correctly. The evaluator can distinguish between an agent quoting a user and an agent generating its own toxic language. The evaluator can recognize that "your argument is weak" is not toxic in a debate context but "you are weak" is.

The threshold depends on your user base and use case. A chatbot for children should have a toxicity threshold near zero. Any output that a toxicity classifier scores above 0.1 should be blocked. A legal research tool used by attorneys can tolerate higher toxicity scores because legal documents often contain offensive language that must be quoted verbatim. The threshold might be 0.6, allowing the model to include necessary quotes while blocking gratuitous insults. The key is that the threshold is documented, tested across demographic groups, and validated to ensure it does not block more outputs from conversations involving marginalized users.

The false positive problem is real. Toxicity classifiers trained on social media data flag LGBTQ-related content at higher rates than heterosexual content, flag discussions of race at higher rates than race-neutral content, and flag non-English languages at higher rates than English. If your toxicity eval uses one of these classifiers without demographic validation, you will ship a product that disproportionately censors the users who need it most. The validation process is straightforward: run your toxicity eval on 5,000 examples stratified by user demographics, measure the block rate per group, and ensure the variance is within 10 percent. If one demographic is blocked at twice the rate of another, your classifier is biased, and you need a different model or a different threshold per group.

## Harmful Content Categories

Harmful content is broader than toxicity. It includes any output that, if followed, could cause physical harm, financial loss, legal consequences, or emotional trauma. The categories that matter in 2026 are: dangerous instructions, illegal activity, fraud and scams, misinformation in high-stakes domains, and privacy violations. Each requires its own detection logic.

Dangerous instructions are the clearest category. These are outputs that tell the user to do something that could injure them or others. Examples: how to make explosives, how to bypass safety mechanisms on equipment, how to self-medicate with controlled substances, how to operate a vehicle while impaired. The detection approach is keyword-based for low-risk systems and LLM-based for high-risk systems. A keyword-based eval scans for terms like "explosive," "poison," "overdose," and blocks any output containing them. This works for systems that should never produce such content. An LLM-based eval asks a separate model to classify whether the output contains instructions that could cause harm, given the context. This works for systems like chemistry tutoring bots that might legitimately discuss chemical reactions but should never provide bomb-making instructions.

Illegal activity detection targets outputs that encourage, facilitate, or provide instructions for breaking the law. Examples: how to evade taxes, how to hack a computer system, how to purchase controlled substances without a prescription, how to commit fraud. The challenge is jurisdiction: something legal in one country is illegal in another. Cannabis purchase instructions are legal advice in California and illegal advice in Texas. Gambling instructions are legal in the UK and illegal in Utah. Your safety eval must know your user base's jurisdiction and apply the correct legal standard. If you serve global users, you apply the strictest standard across all jurisdictions, which means your agent refuses any request that is illegal anywhere your users live.

Fraud and scam detection targets outputs that could trick the user into financial loss. Examples: phishing instructions, Ponzi scheme explanations presented as investment advice, fake charity solicitations, impersonation of authority figures to extract personal information. The detection logic is pattern-based. You maintain a list of known scam patterns, updated monthly, and run every output through a classifier that checks for those patterns. The classifier looks for phrases like "send money to claim your prize," "verify your account by clicking this link," "guaranteed returns with no risk," and blocks any output containing them. This is a moving target because scammers adapt, so your pattern list must be updated based on current fraud trends reported by the FTC, FBI IC3, and industry fraud databases.

Misinformation in high-stakes domains is the hardest category because it requires domain expertise to detect. A medical agent that says "most headaches are harmless" is technically correct, but if the user just described the worst headache of their life, that output is misinformation by omission. It is missing the critical fact that sudden severe headache is a red flag for stroke or aneurysm. The detection approach is to use domain-specific evals written by experts. Your medical safety eval includes 2,000 examples of high-risk symptoms paired with correct and incorrect responses. The eval checks whether the model's output matches the correct response or contains language that contradicts it. This requires physician review for creation and annual updates as clinical guidelines change.

Privacy violations include any output that exposes user data, leaks training data, or asks the user to disclose information the system does not need. Examples: an agent that repeats another user's conversation history, an agent that asks for a Social Security number to answer a general knowledge question, an agent that includes training data verbatim in its output. The detection logic is regex-based for structured data like credit card numbers and SSNs, and LLM-based for conversational privacy leaks. You run every output through a PII detection model, flag any structured identifiers, and block the output if it contains data the user did not provide in the current session.

## Self-Harm Detection

Self-harm content is the highest-severity category because the downside risk is death. This includes outputs that normalize self-injury, provide methods for self-harm or suicide, or discourage the user from seeking help. The detection logic must be perfect or close to it. A false negative means you missed an output that could kill a user. A false positive means you blocked a legitimate support conversation. The balance is hard, and the stakes are absolute.

The detection approach is dual-layer. The first layer is a keyword-based filter that flags any output containing terms like "suicide," "overdose," "cutting," "hanging," "kill yourself," or clinical terms like "ideation" and "self-harm." This layer has a high false positive rate because many legitimate mental health conversations use these terms. The second layer is an LLM-based evaluator that reads the full conversation context and determines whether the agent is providing harmful advice or providing support. The evaluator is trained on 10,000 labeled examples of harmful vs supportive mental health conversations, reviewed by licensed clinical psychologists.

The evaluator looks for three patterns. First, does the output provide specific methods for self-harm? Any output that describes how to harm oneself is automatically flagged, regardless of context. Second, does the output normalize or minimize self-harm? Phrases like "many people feel this way" or "it is understandable to want to end the pain" are flagged because they can be interpreted as permission. Third, does the output discourage help-seeking? Any output that suggests professional help is unnecessary, that therapists do not understand, or that medication makes things worse is flagged. These are subtle, and keyword filters miss them. The LLM evaluator catches them because it understands the pragmatic meaning, not just the literal text.

The threshold for self-harm content is zero tolerance. Any output flagged by the evaluator is blocked, logged with high priority, and reviewed by a human within 24 hours. If the flag is a false positive, the example is added to the evaluator's training data to reduce future false positives. If the flag is a true positive, the conversation is analyzed to understand how the model generated the harmful output, and the training data or prompt is updated to prevent recurrence. You do not ship a model that has produced self-harm content in eval, even once, without understanding the root cause and verifying the fix.

## Severity Tiers

Not all harmful content is equally harmful. A model that uses a mild insult is not the same as a model that provides suicide methods. Your safety eval system must differentiate severity and route failures accordingly. The standard framework is three tiers: critical, high, and medium. Critical failures block deployment, page the on-call engineer, and require executive sign-off to override. High failures block deployment and require eng lead review. Medium failures log to your dashboard and create a backlog task but do not block deployment.

Critical severity includes self-harm content, dangerous instructions that could cause death or serious injury, illegal activity that could result in felony charges, and privacy violations that expose sensitive user data. If your safety eval detects any of these, the release does not happen. The failure is escalated to your head of engineering, your legal team is notified, and the model is pulled from staging until the issue is resolved. There is no threshold for critical severity. A single occurrence is a failure.

High severity includes toxic content above your threshold, harmful instructions that could cause moderate injury or financial loss, misinformation in high-stakes domains, and fraud patterns. These block deployment but do not require executive escalation. The eng team investigates, determines whether the failure is a true positive or a false positive, and either fixes the model or adjusts the eval threshold. High severity failures are acceptable if they occur in fewer than 0.1 percent of eval examples. If the rate is higher, the model is not ready.

Medium severity includes low-level toxicity, minor factual errors, and outputs that violate style guidelines but do not cause harm. These do not block deployment. They create signal for continuous improvement. If your customer service agent occasionally uses slightly unprofessional language, that is a medium severity issue. You log it, you track the trend, and you address it in the next training cycle. You do not halt a release over it.

The severity classification must be consistent across your entire eval suite. You cannot have a toxic content eval that treats slurs as medium severity and a self-harm eval that treats method discussion as critical severity. The tiers apply across all eval families, and the tier determines the pipeline behavior. The classification is documented in your safety policy, reviewed by legal and product leadership, and updated quarterly as your risk landscape changes.

## Content Moderation as Automated Eval

Content moderation is traditionally a post-deployment activity. A user reports harmful content, a human reviewer evaluates it, and the content is removed or the user is banned. In automated eval pipelines, moderation shifts left. You evaluate content before it reaches users, using the same moderation logic that would apply after deployment. This is the same principle as testing before shipping. You do not deploy code and then test it in production. You test it in CI/CD and deploy only if it passes. You do not deploy a model and then moderate its outputs in production. You moderate its outputs in eval and deploy only if they pass your moderation standards.

The implementation is straightforward. You integrate your content moderation API into your pre-deployment eval suite. If you use Hive, Spectrum Labs, OpenAI Moderation, or a custom moderation model, you run every eval example output through that API and log the moderation result. If the moderation API flags an output, you treat that as a safety eval failure. The same thresholds, the same severity tiers, the same blocking logic apply. The difference is that you are catching the harmful content before a user sees it, not after.

The advantage is speed. A human moderator can review 50 to 100 pieces of content per hour. An automated moderation API can evaluate 10,000 outputs in under a minute. This makes comprehensive pre-deployment safety evaluation feasible. You can run your 50,000-example safety eval suite through moderation in five minutes and know with confidence that no output in your eval set violates your moderation policy. If you relied on human moderation alone, the same review would take 500 to 1,000 hours and cost tens of thousands of dollars. Automation makes safety evaluation scalable.

The limitation is that moderation APIs are not perfect. They have false positive rates between five and 15 percent depending on the content type and the API provider. This means you will block some safe outputs and you will need a process for reviewing and overriding false positives. The process is simple: when an output is flagged, a human reviews it within 24 hours, determines whether the flag was correct, and either confirms the block or overrides it. Overridden examples are added to your moderation model's training data if you control the model, or reported to the API provider if you use a third-party service. Over time, the false positive rate decreases as the model learns your specific use case.

## Classifier-Based vs LLM-Based Detection

The choice between classifier-based and LLM-based safety evals comes down to three factors: cost, latency, and context sensitivity. Classifier-based evals are fast and cheap but struggle with context. LLM-based evals are slow and expensive but handle nuance correctly. Your pipeline needs both.

Classifier-based evals work for categorical decisions with low context dependence. Toxic content classifiers like Perspective API return a score in under 100 milliseconds and cost fractions of a cent per call. If your agent generates 10,000 outputs in your eval suite, running each through a toxicity classifier costs under ten dollars and completes in under 20 minutes. This is the right choice for pre-commit evals where speed matters and the context is simple. If your model should never use slurs, a keyword filter or a toxicity classifier catches that without needing to understand the full conversation.

LLM-based evals work for judgments that require context, nuance, or domain knowledge. A self-harm evaluator needs to understand whether "I cannot do this anymore" is a statement of frustration or a suicide indicator, and the only way to know is to read the full conversation history and apply clinical judgment. An LLM-based evaluator can do that. A classifier cannot. The cost is higher: running 10,000 eval examples through an LLM evaluator using GPT-5-mini costs approximately 200 dollars and takes two hours. This is acceptable for pre-deployment evals but too slow and too expensive for pre-commit evals.

The hybrid strategy is to use classifier-based evals in pre-commit for fast feedback, and LLM-based evals in pre-deployment for comprehensive validation. Your pre-commit pipeline runs every output through a toxicity classifier, a PII regex filter, and a keyword-based dangerous content filter. This catches 80 percent of safety issues in under five minutes. Your pre-deployment pipeline runs every output through LLM-based evaluators for self-harm, harmful instructions, misinformation, and context-sensitive toxicity. This catches the remaining 20 percent in under two hours. Both layers share the same severity tiers and the same blocking logic. The difference is speed and depth.

The final consideration is model drift. Classifier-based evals are brittle. If your toxic content classifier was trained in 2024, it may not recognize new slang, new slurs, or new manipulation techniques that emerged in 2025 and 2026. You must retrain or replace classifiers at least annually, and more frequently if you operate in domains where language evolves quickly. LLM-based evals are more robust because the evaluator model itself is kept current. When you upgrade from GPT-5 to GPT-5.1, your LLM-based safety evals improve automatically. When you upgrade your toxicity classifier, you must revalidate the entire eval suite to ensure the new classifier does not introduce new false positives or false negatives.

---

*Toxic, harmful, and self-harm content detection is the foundation, but it is not the full picture. Domain-specific risks like medical and legal advice require specialized evaluation logic that general safety models cannot provide.*
