# 7.4 â€” Policy Compliance Scoring vs Binary Refusal

Policy compliance is not a binary decision. A response can violate policy in a dozen different ways with a dozen different levels of severity, and treating every violation the same turns your model into a refusal machine that destroys user trust as fast as it prevents harm. The medical chatbot that refuses to answer "how do I treat a cut" because it detected the word "treat" is as broken as the one that explains how to synthesize controlled substances. The enterprise assistant that refuses to discuss employee performance reviews because it flagged the word "termination" is as useless as the one that leaks confidential data. Binary refusal is the first instinct of every safety team. It is almost never the right approach.

**Policy compliance scoring** is the practice of evaluating responses on a spectrum of severity, assigning numeric or categorical scores that distinguish between critical violations, moderate risks, minor policy breaches, and acceptable responses. It replaces the yes-no refusal gate with a graduated response system that matches the intervention to the actual risk. A critical violation triggers an immediate refusal. A moderate risk might allow the response with a disclaimer. A minor breach might log a warning without blocking. The model becomes a nuanced policy enforcer instead of a blunt instrument.

## Why Binary Refusal Fails at Scale

Binary refusal creates two failure modes that compound each other. The first is over-refusal. When every detected policy trigger results in blocking, the model refuses legitimate requests at rates that make it unusable. A financial advisory chatbot blocks questions about tax optimization because it flags any mention of reducing payments to government entities. A customer service bot refuses to discuss account closures because the training data flagged account termination as sensitive. Users learn that certain topics are off-limits for reasons they do not understand, and they stop trusting the system to handle anything nuanced.

The second failure mode is under-refusal. Because binary systems are intolerable when tuned too strictly, teams tune them loosely to reduce false positives. The refusal threshold gets raised. The keyword lists get narrowed. The model starts allowing responses that actually do violate policy because the team could not afford the user friction of blocking borderline cases. You end up with a system that refuses harmless questions and allows harmful ones, optimized for neither safety nor usefulness.

The root problem is that binary refusal cannot represent the structure of real policies. Most policies are not absolute. They have gradients. "Do not provide medical diagnoses" is different from "do not prescribe controlled substances" is different from "do not advise users to ignore symptoms of medical emergencies." The first might allow general health information with a disclaimer. The second is a hard refusal. the third is a hard refusal with an escalation to crisis resources. A binary gate cannot encode this. A scoring system can.

## Severity Tiers and Graduated Responses

Policy compliance scoring starts with severity tiers. Most organizations need three to five levels. A common structure uses four: **critical violation**, **high risk**, **moderate concern**, and **acceptable with caveats**. Each tier maps to a different system response. Critical violations trigger immediate refusal and logging. High risks allow the response with a prominent warning or disclaimer. Moderate concerns log the interaction for review but do not block or warn. Acceptable-with-caveats responses might append policy reminders or direct the user to official resources.

A healthcare chatbot scores responses on medical advice severity. Critical violations include diagnosing conditions, prescribing medications, or advising users to delay emergency care. High risks include providing treatment suggestions for symptoms without appropriate disclaimers. Moderate concerns include general health information that could be misinterpreted without additional context. Acceptable-with-caveats includes wellness advice, first aid basics, or directing users to professional resources. Each tier has a different response template. Critical violations return "I cannot provide medical diagnoses. If you are experiencing a medical emergency, contact emergency services immediately." High risks return the requested information followed by "This is general information only and not a substitute for professional medical advice. Consult a healthcare provider for personal health concerns." Moderate concerns return the information with no in-line warning but log the interaction for Trust and Safety review.

The tiers are not arbitrary. They map directly to the organization's risk tolerance and regulatory obligations. A financial services chatbot operating under securities regulations might have stricter tiers for investment advice than a general productivity assistant. A customer service bot in a regulated industry might have tighter controls on data disclosure than an internal tool. The tier definitions come from Legal, Compliance, and Trust and Safety. Engineering implements them. Product validates that the graduated responses do not destroy the user experience.

## Compliance Rubrics and Automated Scoring

Scoring policy compliance at scale requires rubrics that evaluators can apply consistently. A rubric defines what constitutes each severity level for each policy dimension. If the policy prohibits harmful instructions, the rubric specifies what counts as "harmful," what level of harm maps to each tier, and what context modifiers increase or decrease severity. The rubric is the shared truth between human reviewers, LLM-based auto-graders, and production safety classifiers.

A content moderation policy for a community platform prohibits hate speech, harassment, illegal content, and graphic violence. The rubric breaks each category into tiers. Hate speech tiers might include: advocating violence against protected groups is critical; derogatory slurs in context of hostility is high; stereotyping without hostility is moderate; discussing historical discrimination in educational context is acceptable. Harassment tiers might include: specific credible threats is critical; targeted insults and doxxing attempts is high; general hostile language is moderate; disagreement without personal attacks is acceptable. Each tier has example scenarios. The rubric is not exhaustive. It provides enough structure that evaluators make consistent calls.

Automated scoring uses the rubric as training data. If you have a labeled dataset of responses scored by human reviewers using the rubric, you can train a classifier to predict severity tiers. If you do not have labeled data, you can use LLM-based grading with the rubric embedded in the prompt. The LLM sees the response, the rubric, and explicit instructions to classify the severity level. You validate the LLM grader against human judgments on a holdout set. If the LLM agrees with humans on critical violations 95 percent of the time and agrees on high-risk cases 85 percent of the time, the system is usable. If not, you refine the rubric until the LLM can apply it reliably.

The key constraint is that critical violations cannot have false negatives. If the rubric says a response is a critical violation, the automated system must catch it. High false-negative rates on critical violations mean unsafe content reaches users. Moderate false positives on lower tiers are tolerable. Over-flagging a moderate concern as high risk is a warning UX problem, not a safety failure. Missing a critical violation is a safety failure. The rubric and the grading system are tuned to this asymmetry.

## When to Refuse, Warn, or Allow

The intervention decision is driven by the score, but it is also contextual. A high-risk response in a customer service context might be acceptable in an internal compliance training tool. A moderate concern in a public-facing chatbot might be acceptable in a domain-expert assistant used by professionals who understand the limitations. The tier score is the input. The deployment context, user type, and policy enforcement mode determine the output.

Most systems implement a decision matrix. The matrix maps tier scores and context variables to interventions. For a public-facing chatbot, critical violations always refuse. High risks allow with disclaimer. Moderate concerns allow with logging. For an internal tool used by trained professionals, critical violations still refuse, but high risks might allow without disclaimer if the user has acknowledged the policy. Moderate concerns allow with no intervention. The matrix is policy. It is versioned, reviewed by Legal, and enforced consistently across all production instances.

Some policies require refusal at lower thresholds. A chatbot for minors might refuse any response that scores moderate or higher on adult content policies, even if the same content would be acceptable-with-warning for adult users. A financial advisory bot operating in a highly regulated jurisdiction might refuse high-risk responses that a less-regulated deployment would allow with disclaimers. The tier score is universal. The refusal threshold is deployment-specific.

You also handle edge cases where the model is uncertain. If the safety classifier assigns a response 60 percent probability of high risk and 40 percent moderate concern, you treat it as high risk. If it assigns 50 percent moderate and 50 percent acceptable, you escalate to human review or default to the more conservative tier. Uncertainty thresholds are policy decisions. Some teams set a confidence floor: if the classifier is less than 70 percent confident in any tier assignment, the response is blocked or sent to human review. This prevents the model from guessing on ambiguous cases.

## Policy Gradients and the Refusal Spectrum

Policy gradients are the realization that most policies exist on a spectrum, not as binary boundaries. "Do not provide financial advice" is not one rule. It is a gradient from "explain what a stock is" at the safe end to "tell me which penny stocks to buy" at the prohibited end. "Do not discuss politics" might prohibit endorsing candidates but allow explaining how legislative processes work. "Do not generate harmful content" prohibits instructions for making weapons but allows discussing historical military technology in educational contexts. The gradient is the policy's actual shape. Binary enforcement is a poor approximation.

Automated eval pipelines measure where responses fall on the gradient. Instead of asking "does this violate policy," you ask "where on the severity spectrum does this fall, and is that position acceptable given the deployment context." The eval suite includes examples spanning the full gradient for each policy dimension. You score the model on each example and validate that the scores align with the intended enforcement posture. If the model's outputs cluster near the "acceptable with disclaimer" tier when you expected them in the "moderate concern" tier, you have a calibration problem. If outputs that should be critical violations score as moderate concerns, you have a safety gap.

The gradient also helps you detect drift. If the model's average policy compliance score shifts over time, you see it before users report problems. A fine-tuned model that shifts high-risk responses into the critical tier is over-refusing. A model that shifts critical violations into high-risk is under-refusing. Both are regressions. The gradient gives you a continuous signal instead of waiting for binary violations to cross a threshold.

## Measuring Both Compliance and Utility

The hardest problem in policy compliance scoring is measuring it alongside utility. A model that refuses every request has perfect compliance and zero utility. A model that allows every request has perfect utility and zero compliance. The product goal is the Pareto frontier: maximum utility for a given compliance threshold. The eval pipeline must measure both.

Utility measurement depends on the task. For a customer service bot, utility might be resolution rate: the percentage of user requests that receive a helpful, actionable response. For a creative writing assistant, utility might be user satisfaction with the output quality. For a coding assistant, utility might be the percentage of generated code that compiles and passes tests. The utility metric is domain-specific. The compliance metric is universal: the percentage of responses that meet policy requirements at each tier.

You track both in the same eval run. Each test case has a utility label and a policy compliance label. The model's response is scored on both dimensions. You calculate utility pass rate and compliance pass rate independently. You also calculate the joint distribution: what percentage of responses are both high-utility and policy-compliant. This is the target metric. If 90 percent of responses are compliant but only 40 percent are useful, the model is over-refusing. If 95 percent are useful but only 60 percent are compliant, the model is under-refusing. If 85 percent are both useful and compliant, you are near the frontier.

Teams that measure compliance without utility build refusal machines. Teams that measure utility without compliance build unsafe systems. The eval pipeline that measures both builds production-ready models.

The next challenge in safety evaluation is the helpfulness-safety tradeoff: preventing the model from refusing so aggressively that it becomes useless, and from being so permissive that it violates policy, measured as a joint optimization problem across both dimensions simultaneously.

