# 10.2 — Pass Rate, Failure Rate, and Regression Detection

**Pass rate** is the percentage of test cases the model handles correctly. Out of 1,000 test cases, the model passes 927. Pass rate is 92.7%. This is the most fundamental metric in automated evaluation. It tells you, in one number, how much of your test suite the model satisfies. Every eval system tracks pass rate. Most track it as the primary quality indicator. It's simple, interpretable, and comparable across models, across time, and across teams.

Pass rate has limits. It treats all test cases as equally important, which is almost never true. A model that passes 95% of trivial cases and fails 5% of safety-critical cases is worse than a model that passes 90% overall but nails every safety-critical case. Pass rate doesn't capture severity. It doesn't capture user impact. It doesn't tell you which failures matter. But it's still the anchor metric because it's the easiest to understand and the hardest to game. You either pass the test or you don't. The percentage is objective.

**Failure rate** is the inverse: the percentage of test cases the model fails. Out of 1,000 test cases, 73 fail. Failure rate is 7.3%. Some teams track failure rate instead of pass rate. The numbers are mathematically equivalent, but the framing shifts attention. Pass rate emphasizes what's working. Failure rate emphasizes what's broken. For systems where failures are rare and high-stakes — medical diagnosis, legal document review, autonomous vehicle decision-making — tracking failure rate keeps the team focused on what still needs fixing.

**Regression detection** is the process of identifying cases that used to pass and now fail. A model that was deployed two months ago passed 94.2% of the golden set. The new model candidate passes 94.8% of the golden set. Pass rate improved. But when you compare case-by-case, 83 cases that the old model passed now fail, and 109 new cases now pass. The new model introduced 83 regressions. Whether you ship it depends on whether those 83 regressions are acceptable trade-offs or unacceptable breaks.

## Pass Rate Calculation and Weighted Pass Rate

Basic pass rate is a count. You have N test cases. The model passes P of them. Pass rate is P divided by N. If you have 1,000 cases and pass 927, your pass rate is 0.927, displayed as 92.7%. This number is easy to calculate, easy to track over time, and easy to compare across models. It's the first metric every eval dashboard shows.

But not all test cases are equal. A case that tests whether the model can summarize a news article is not as critical as a case that tests whether the model refuses to generate a phishing email. If both cases are weighted equally, the pass rate doesn't reflect actual risk. A model that passes 98% of low-stakes cases and 60% of high-stakes cases looks better than a model that passes 90% of low-stakes cases and 95% of high-stakes cases, even though the second model is safer.

**Weighted pass rate** solves this by assigning importance scores to test cases. Safety-critical cases get a weight of 10. High-value business cases get a weight of 5. Standard cases get a weight of 1. Edge cases that rarely occur get a weight of 0.5. Now instead of counting passes, you sum the weights of passed cases and divide by the total weight. A model that passes 927 out of 1,000 cases might have a raw pass rate of 92.7% but a weighted pass rate of 89.3% if most of the failures are high-weight cases.

Weighted pass rate requires you to make explicit decisions about what matters. You can't just count tests. You have to evaluate each test's importance, assign a weight, document the rationale, and maintain those weights as the test suite evolves. This is more work. It's also more honest. The raw pass rate pretends every test is equal. The weighted pass rate admits that some failures are worse than others and adjusts the metric to reflect that reality.

The challenge with weighted pass rate is consistency. If different teams assign weights differently, you can't compare their metrics. If weights change frequently, trends become meaningless. The fix is a standardized weighting schema. Define five tiers — critical, high, medium, low, edge — and assign each tier a multiplier. Critical cases are 10x. High cases are 5x. Medium cases are 1x. Low cases are 0.5x. Edge cases are 0.1x. Every test case gets tagged with a tier during test creation. The tier determines the weight. This schema is applied uniformly across all eval suites, making weighted pass rate comparable.

## Failure Rate Tracking and Failure Case Management

While pass rate is the most common primary metric, **failure rate** is often more actionable. When failure rate is low — say, 3% — you can afford to review every failure. You export the list of 30 failed cases out of 1,000, assign them to engineers, and investigate. Each failure gets a root cause. Each root cause gets a fix or a documented decision to accept the limitation. This is the ideal workflow for high-quality systems. You treat every failure as a signal.

When failure rate is high — say, 25% — reviewing every failure is impractical. You have 250 failed cases. You can't assign 250 investigations. Instead, you cluster the failures. You group them by error type, by task category, by input pattern. You identify the five or six failure modes that account for 80% of the failures. You fix those modes. You re-eval. Failure rate drops to 12%. Now you repeat. Cluster, prioritize, fix, re-eval. This iterative process is how you drive failure rate down when starting from a low-quality baseline.

**Failure case management** is the operational discipline of tracking failures from detection to resolution. Every failed test case gets logged with metadata: test ID, input, expected output, actual output, failure reason, timestamp, model version. The log becomes a database. You query it. Which failure modes are most common? Which ones are increasing over time? Which ones only appear with specific model versions? Which ones correlate with production incidents? The failure database is one of the most valuable artifacts in the eval system. It's a catalog of everything the model doesn't do well.

Some failures are permanent limitations. The model can't do math reliably. The model hallucinates on rare entity names. The model struggles with ambiguous instructions. These failures won't be fixed by prompt tuning or additional examples. They're inherent to the model's architecture or training data. When you identify a permanent limitation, you document it, add it to the known issues list, and decide whether it's acceptable. If it's not acceptable, you switch models or add a specialized component. If it is acceptable, you add guardrails, set user expectations, or constrain the input space to avoid triggering the limitation.

Other failures are fixable. The model misunderstands a specific instruction format. The model produces the right content but the wrong structure. The model fails on a category of inputs that are rare in training data but common in your domain. These failures are improvement opportunities. You add targeted examples to your fine-tuning dataset. You adjust the prompt. You add a post-processing step. You re-eval. The failure rate for that category drops. You move on to the next fixable failure. This workflow is how eval systems drive continuous improvement.

## Regression Detection Algorithms

A **regression** is a case that the previous model passed and the new model fails. Regressions are worse than new failures. A new failure means the model never handled that case. A regression means users were relying on that capability, and now it's gone. If you deploy the regression, users notice. They file bugs. They complain. They lose trust. Regression detection is the process of finding these cases before deploy.

The simplest regression detection algorithm is a case-by-case comparison. You run the same test suite on the old model and the new model. For each test case, you record pass or fail. Then you compare. If the old model passed and the new model failed, that's a regression. You count the regressions. If regression count is above threshold — say, more than 2% of the test suite — you block the deploy. If it's below threshold, you review the regressions manually to confirm they're acceptable, then proceed.

This approach requires stable test IDs. Each test case needs a unique identifier that persists across eval runs. If test IDs change, you can't compare results. If test cases are regenerated every run, you can't detect regressions because you're not testing the same inputs. Stable test IDs mean you maintain a golden set — a fixed collection of test cases with consistent IDs — and you eval every model against that set. The golden set becomes the regression baseline. Any model that fails a case the baseline passed is flagged.

A more sophisticated approach is **semantic regression detection**. Instead of requiring exact test ID matches, you cluster test cases by semantic similarity. If the new model fails a case that's semantically similar to cases the old model passed, that's a potential regression even if the exact test case is new. This approach is useful when your test suite evolves over time. You add new cases, remove outdated ones, rephrase instructions. The exact test IDs don't match, but the underlying capabilities being tested are the same. Semantic clustering lets you detect regressions across evolving test suites.

Semantic regression detection requires embeddings. You embed every test case input using a model like GPT-5 or Gemini 3. You cluster the embeddings. You label each cluster with the capability it tests — say, "summarization of news articles" or "refusal of harmful requests." When the new model fails cases in a cluster that the old model mostly passed, you flag it as a cluster-level regression. This approach is noisier than exact-match regression detection, but it's more robust to test suite changes.

## Baseline Comparisons and Delta Metrics

Regression detection is one form of baseline comparison. You're comparing the new model to the old model. But you can compare against other baselines too. You can compare against the **initial baseline** — the very first model you deployed, before any fine-tuning or iteration. This tells you how much you've improved since day one. You can compare against the **best historical performance** — the highest pass rate you've ever achieved on this test suite. This tells you whether you're still improving or whether quality has plateaued.

You can compare against **competitor models**. If you're evaluating Claude Opus 4.5, you also run the same eval suite on GPT-5.2 and Gemini 3 Pro. You compare pass rates. Claude passes 91.2%, GPT-5.2 passes 88.7%, Gemini 3 Pro passes 89.4%. Now you know your model is leading on this particular test suite. If you're choosing between models, this comparison informs the decision. If you're justifying the cost of a more expensive model, this comparison provides evidence.

**Delta metrics** express change relative to baseline. Instead of reporting "new model pass rate is 92.7%," you report "new model pass rate is +1.4 percentage points versus baseline." Instead of "regression count is 27," you report "regression count is +12 versus last release." Delta metrics make trends visible. A pass rate of 92% sounds good. A pass rate delta of -3 percentage points sounds alarming. The delta reframes the metric in terms of change, which is often more decision-relevant than the absolute value.

Delta metrics require consistent baselines. If your baseline changes every week, deltas are meaningless. The baseline should be stable — updated only when you intentionally shift it, such as after a major model upgrade or after a significant test suite revision. Between baseline updates, every eval run compares against the same reference point. This consistency makes deltas interpretable. A +2 percentage point delta means the same thing in January and June. If the baseline keeps shifting, you can't tell whether quality is improving or whether you're just measuring against an easier test.

## Statistical Significance and Confidence Intervals

When pass rate changes by 0.3 percentage points, is that real improvement or random noise? When regression count increases by 4 cases, is that a signal or statistical fluctuation? Small deltas are hard to interpret without understanding variance. **Statistical significance** tells you whether a change is large enough to be meaningful given the size and variability of your test suite.

For a test suite with 1,000 cases, a 1 percentage point change is 10 cases. If your test suite has high variability — meaning re-running the same model on the same tests sometimes produces different results due to model non-determinism — a 10-case delta might be noise. If your test suite has low variability — meaning results are highly consistent across runs — a 10-case delta is a real signal. Statistical significance quantifies this. It answers: given the observed variability, what's the probability that this delta is due to chance?

The standard approach is to run multiple eval trials and compute a **confidence interval**. You run the same model on the same test suite five times. Pass rates are 92.4%, 92.7%, 92.1%, 92.6%, 92.3%. The mean is 92.4%. The standard deviation is 0.24 percentage points. A 95% confidence interval is approximately mean plus-or-minus two standard deviations, so 91.9% to 92.9%. Now if a new model has a pass rate of 93.2%, you can say with confidence that it's outside the baseline's confidence interval — the improvement is statistically significant.

For deterministic test suites — where the model always produces the same output for the same input — variance is zero, and every delta is significant. For non-deterministic models — where sampling, temperature, or non-deterministic infrastructure introduces variability — you need multiple trials to separate signal from noise. Most teams run three to five trials for critical evals and use the median or mean as the reported metric. The confidence interval tells you how much to trust small deltas.

Statistical significance prevents overreaction. Without it, teams chase noise. They see a 0.5 percentage point drop in pass rate, investigate for three days, find nothing wrong, and realize the drop was within normal variance. With significance testing, they see the 0.5 percentage point drop, check the confidence interval, confirm it's within expected variance, and move on. This discipline saves time and keeps attention on real quality changes.

## The Regression Alert and Automated Blocking

Regression detection only matters if it stops bad deploys. The workflow is: run eval on new model, compare to baseline, detect regressions, count them, compare count to threshold, block deploy if over threshold. This workflow must be automated. If a human has to manually review regression reports and decide whether to block, they'll rationalize. They'll find reasons why this regression is acceptable, why the new model's improvements outweigh the regressions, why they can ship now and fix later. Automation removes that discretion.

The **regression alert** is the mechanism. The CI/CD pipeline runs the eval suite on every model candidate. It calculates regression count. If regression count exceeds threshold — say, 2% of test cases, or 20 regressions in a 1,000-case suite — the pipeline fails. The deploy is blocked. The alert fires. It includes the list of regressed cases, the delta in pass rate, and a link to the detailed failure report. The engineer gets paged or receives a Slack message. They investigate. They don't have the option to override without fixing.

Some teams implement a tiered alert system. If regression count is between 1% and 2%, the alert is a warning. The deploy proceeds, but the team is notified. If regression count is above 2%, the alert is a blocker. The deploy stops. This tiered system balances strictness with pragmatism. Low regression counts might be acceptable trade-offs if the new model has significant improvements elsewhere. High regression counts are never acceptable.

The regression alert should also categorize regressions by severity. If all 20 regressions are low-priority edge cases, the impact is minimal. If even 2 of the 20 regressions are safety-critical cases, the impact is severe. The alert should flag high-severity regressions separately. A deploy that introduces 15 low-severity regressions and 0 high-severity regressions might proceed. A deploy that introduces 3 high-severity regressions is blocked regardless of how many low-severity regressions exist. This severity-based blocking is what weighted pass rate enables.

Regression detection is the last line of defense before production. It's the check that says: you can improve the model, but you can't break what already works. Without regression detection, every deploy is a gamble. With it, you have evidence that the new model is at least as good as the old one on the cases you care about — and hopefully better overall. Pass rate, failure rate, and regression detection are the three core metrics that define model readiness, and slicing those metrics by task type, model, tenant, and cohort is what reveals the hidden failures that aggregate numbers miss.

---

*Next: 10.3 — Slicing by Task Type, Model, Tenant, and Cohort*
