# Chapter 7 — Safety and Risk Evals

Safety is not a feature you add after the product works. It is a first-class evaluation family that runs alongside every other eval in your pipeline. In 2026, with agents that take real-world actions and models that handle medical, legal, and financial queries, safety evaluation is as critical as functionality evaluation. This chapter covers how to detect harmful content, evaluate medical and legal risk, distinguish policy compliance scoring from binary refusal, test jailbreak and prompt injection robustness, catch tool abuse before it causes damage, and integrate red team findings into automated safety regression tests. Safety evals must be fast enough to run on every output and sensitive enough to catch the cases that matter.

---

- 7.1 — Safety Evaluation as a First-Class Eval Family
- 7.2 — Toxic, Harmful, and Self-Harm Content Detection
- 7.3 — Medical and Legal Risk Evaluation
- 7.4 — Policy Compliance Scoring vs Binary Refusal
- 7.5 — Helpfulness Without Hallucination Tradeoffs
- 7.6 — Jailbreak and Prompt Injection Robustness Evals
- 7.7 — Tool Abuse and Unintended Action Detection
- 7.8 — Adversarial Eval Suite: Prompt Injection and Misuse
- 7.9 — Safety Regression Detection Across Model Updates
- 7.10 — Multi-Lingual and Cross-Cultural Safety Evals
- 7.11 — Red Team Integration with Automated Safety Evals
- 7.12 — Safety Eval Governance and Threshold Management

---

*The safety eval that runs after deployment catches incidents. The safety eval that runs before deployment prevents them.*
