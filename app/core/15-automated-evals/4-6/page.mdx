# 4.6 — The Self-Agreement Trap: When Judges Agree with Themselves

**The Self-Agreement Trap** is the most dangerous bias in LLM-as-judge evaluation: a model systematically overrating outputs from itself or from models in the same family. The trap is invisible if you only look at eval scores. Your dashboard shows your system performing well. Your A/B tests show the new prompt winning. Your regression suite stays green. Then you ship to users and discover the quality is not what your evals promised. The judge was not measuring quality — it was measuring similarity to its own reasoning style.

## Why Models Agree with Themselves

LLMs are trained to predict what text comes next based on patterns in their training data. When a model judges an output, it is implicitly asking: does this output look like something I would generate? If the answer is yes — because the output was generated by the same model or a similar one — the judge scores it higher. The model is not deliberately biased. It is doing what it was trained to do: recognize patterns that match its learned distribution.

The effect is strongest when the judge and the subject are the same model. GPT-5.2 judging GPT-5.2 outputs will score them higher than a neutral evaluator would. The judge recognizes its own phrasing, sentence structure, hedging patterns, and formatting preferences. It treats these familiar patterns as signals of quality, even when they are not. A response that sounds like GPT-5.2 gets a boost. A response that sounds different — even if objectively better — gets penalized.

Same-family models show the same bias at slightly lower intensity. Claude Opus 4.5 judging Claude Sonnet 4.5 outputs will favor them over GPT-5.2 or Gemini 3 Pro outputs, because the Claude family shares training data, architecture, and fine-tuning strategies. The models have correlated preferences. They like the same kinds of answers. When you use a Claude model to judge another Claude model, you are not measuring quality — you are measuring family resemblance.

## Detecting Self-Agreement in Your Eval Data

The symptom is consistent overrating of one model relative to others when judged by a model from the same family. You run a pairwise comparison: GPT-5.2 versus Claude Opus 4.5, judged by GPT-5.2. The judge picks GPT-5.2 seventy percent of the time. You reverse the judge: now Claude Opus 4.5 is judging. It picks Claude seventy percent of the time. Both judges are confident. Both claim their preferred model is better. The truth is that both are biased.

A more subtle version appears in pointwise scoring. You evaluate the same outputs with three different judges: GPT-5.2, Claude Opus 4.5, Gemini 3 Pro. Each judge scores its own family's outputs half a point to a full point higher on a five-point scale than the other judges do. The differences are not huge, but they are systematic. When you average across all judges, the bias washes out — but if you only use one judge, you get a distorted view.

The test is cross-model correlation. Take fifty outputs from each of your production models. Evaluate all of them with all of your candidate judges. Calculate the correlation between each judge and each model. If GPT-5.2 as judge correlates most strongly with GPT-5.2 as subject, and Claude Opus 4.5 as judge correlates most strongly with Claude as subject, you have self-agreement. The judges are not measuring quality — they are measuring similarity.

## Cross-Model Evaluation as Mitigation

The fix is to never use a single judge. Use three judges from different families and aggregate their scores. GPT-5.2, Claude Opus 4.5, and Gemini 3 Pro as judges. Each one has its own bias, but the biases point in different directions. Averaging the three scores reduces self-agreement bias significantly.

The aggregation can be a simple mean: add the three scores and divide by three. Or it can be weighted if you trust one judge more than the others — but be careful with weighting because your intuition about which judge is best may itself be biased toward whichever model family you use most often. Equal weighting is safer unless you have strong evidence that one judge is systematically more accurate.

Cross-model judging triples your eval cost because every output gets three judgments instead of one. For high-stakes decisions — model selection, major prompt changes, production regression gates — this cost is justified. For high-volume monitoring, you might use a single judge most of the time and run cross-model validation weekly or monthly to check for drift.

Another mitigation is to use a judge from a different family than any of your production models. If your system uses GPT-5.2 and Claude Opus 4.5, judge with Gemini 3 Pro or Llama 4 Maverick. The judge has no family loyalty. It will not systematically favor one of your models over the other. This works well for pairwise A/B tests where you are comparing two systems and just need an unbiased tie-breaker.

## The Correlation Between Judge and Subject

Self-agreement is not binary. It exists on a spectrum. The strongest bias is same model judging same model. Slightly weaker is same family judging same family. Weaker still is models trained on overlapping data or using similar architectures. Even models from completely different families show some correlation because they all learned from internet text and absorbed similar notions of what good writing looks like.

You cannot eliminate this correlation entirely. But you can measure it and account for it. Run a one-time calibration study: take a few hundred diverse outputs, evaluate them with every judge you are considering, and also get human ratings for a subset. Calculate the correlation between each judge and the human ratings. The judge with the highest human correlation is your best choice — not the judge that gives the highest scores, but the judge whose scores best predict human judgment.

This calibration also reveals whether your judges agree with each other. If GPT-5.2, Claude Opus 4.5, and Gemini 3 Pro all correlate strongly with human ratings and with each other, you have convergent validity. The judges might have individual biases, but they agree on what quality looks like. If they do not correlate with each other, you have a deeper problem: your task might not have a well-defined notion of quality, or your rubric is too vague for any judge to apply consistently.

## When Self-Agreement is Actually Fine

Self-agreement is only a problem when you are comparing outputs from different models. If your eval suite measures one model against itself over time — baseline versus new prompt, version one versus version two, this week versus last week — using that same model as judge is fine. The bias is constant. It affects both sides of the comparison equally. The judge might be overrating everything, but it is overrating both versions by the same amount, so the relative comparison is still valid.

The same applies when you are judging a single model on an absolute scale for monitoring. If GPT-5.2 is your production model and you use GPT-5.2 as judge to track whether quality is degrading, self-agreement does not matter. You are not comparing GPT-5.2 to another model. You are comparing GPT-5.2 today to GPT-5.2 yesterday. The judge's bias is consistent across both time periods. A drop in score still indicates real degradation, even if the absolute scores are inflated.

Where self-agreement kills you is in model selection and cross-model A/B testing. You cannot use GPT-5.2 to decide whether GPT-5.2 is better than Claude Opus 4.5. You cannot use Claude to judge whether a Claude-based prompt beats a GPT-based prompt. The judge will favor its own family, and your decision will be wrong. For these scenarios, cross-model judging is not optional — it is the only way to get a trustworthy result.

## Practical Patterns for Avoiding the Trap

The simplest safe pattern is the three-judge ensemble: one judge from each major family, scores averaged. GPT-5.2, Claude Opus 4.5, Gemini 3 Pro. Run all three on every eval, aggregate the scores, track the aggregate over time. This costs three times as much as single-judge eval, but it eliminates the majority of self-agreement bias.

A cheaper pattern is single-judge for monitoring, multi-judge for decisions. Use GPT-5.2 as your daily judge for production monitoring. The bias is constant, so trends are still valid. When you need to make a major decision — switch models, change prompts, ship a new feature — run a multi-judge eval on a curated test set. You spend most of your eval budget on cheap single-judge monitoring and reserve the expensive multi-judge validation for when it actually matters.

The most sophisticated pattern is adversarial cross-validation. Evaluate each model with the judge least likely to favor it. Judge GPT-5.2 outputs with Claude Opus 4.5. Judge Claude outputs with GPT-5.2. If a model still scores well when judged by a competitor, it is genuinely good. If it only scores well when judged by its own family, the quality is suspect. This flips the bias: instead of inflating scores, you are deliberately deflating them. The model that survives adversarial judging is the one you can trust.

## The Meta-Problem: Judging the Judges

Who judges the judges? This is the infinite regress problem in LLM-as-judge systems. You need to know if your judge is reliable, so you compare it to human ratings. But human ratings are expensive, so you only do this on a small sample. You assume the judge behaves the same way on the rest of your data. If that assumption is wrong — if the judge is well-calibrated on your sample but biased on production data — you will not catch it until users complain.

The mitigation is ongoing spot-checking. Every week, pull a random sample of fifty to a hundred eval cases. Send them to human raters. Compare human scores to judge scores. If the correlation drops below ninety percent, investigate. Maybe your data distribution shifted. Maybe the judge model was updated. Maybe your rubric no longer matches what users actually care about. Spot-checking does not prevent self-agreement, but it detects when your mitigation strategies stop working.

Self-agreement is not a bug you fix once. It is a bias you manage continuously. You choose judges that minimize it. You validate that your mitigation is working. You spot-check against human ratings to catch drift. And you never, ever use a single judge from the same family as your production model to make a high-stakes decision — because that is not evaluation, it is confirmation bias with extra steps.

Position bias is the other major pitfall in LLM judging: models favor whichever option appears first in the prompt, independent of quality, and most teams do not even realize it is happening.

