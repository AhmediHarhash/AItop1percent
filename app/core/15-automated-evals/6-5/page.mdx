# 6.5 — Error Recovery and Graceful Degradation Checks

In November 2025, a customer service agent handled thirty-two thousand support conversations in its first week of production. On day eight, a user submitted a request that triggered a parsing error in the intent classification step. The agent had no error handling. It logged the exception and terminated. The user saw a blank screen. The agent logged the conversation as "resolved." The same parsing error occurred in four hundred additional conversations over the next seventy-two hours. Every one terminated silently. The company discovered the failures only when a customer complained publicly on social media that the support chat had gone dead mid-conversation. The agent's eval suite tested the happy path — when everything worked. It never tested what happened when something broke.

Error recovery is not an edge case. In production, errors are routine. External APIs return unexpected responses. Data sources become temporarily unavailable. Users submit malformed inputs. Network requests time out. Token limits are exceeded. Every failure is an opportunity for the agent to recover gracefully or to crash spectacularly. Graceful recovery means detecting the error, logging it, attempting a fallback action, and either continuing with reduced functionality or failing transparently with a clear explanation. Spectacular failure means silent termination, corrupted state, or cascading errors that take down the entire workflow. Your eval pipeline must verify that every recoverable error is actually recovered.

## Testing Error Recovery Paths

An error recovery path is the sequence of actions the agent takes when a step fails. It might retry the failed operation with adjusted parameters. It might fall back to an alternative data source or method. It might skip the failed step and continue with partial data. It might escalate to a human. It might terminate gracefully with an explanation. The correctness of the error recovery path depends on the type of error and the context in which it occurs.

**Testing error recovery paths** means injecting failures into your eval runs and verifying that the agent responds correctly. You simulate network timeouts, API errors, missing data, malformed inputs, rate limit violations, and authorization failures. You verify that the agent detects each failure, logs it with sufficient detail for debugging, and takes the appropriate recovery action. You verify that the recovery action succeeds or, if it fails, that the agent escalates or terminates cleanly.

A financial data aggregator built an agent that pulled transaction data from multiple bank APIs. The workflow called each API, normalized the response, merged the data, and returned a unified transaction history. The eval tested the happy path — all APIs available, all responses valid. In production, APIs failed constantly. One bank returned HTTP 503 during maintenance windows. Another returned incomplete data when the user's account was locked. A third occasionally returned transactions in the wrong currency format. The agent had no error handling. When any API call failed, the agent crashed, and the user received no transaction history at all — even from the banks whose APIs succeeded.

The team added error recovery. If an API call failed with a retryable error — timeout, 503, rate limit — the agent retried with exponential backoff. If an API call failed with a permanent error — 401, 403, invalid credentials — the agent logged the failure and continued processing the other banks. If an API returned incomplete data, the agent marked the data as partial and included it in the merged output with a flag indicating missing fields. The agent returned a unified transaction history even when some APIs failed, and it explicitly told the user which banks were unavailable and why.

The eval verified this behavior by mocking the bank APIs and injecting failures. Test case one: API returns 503. Expected behavior: agent retries three times, then logs failure and proceeds with other banks. Test case two: API returns 401. Expected behavior: agent logs authentication failure, does not retry, proceeds with other banks. Test case three: API returns valid data missing the amount field. Expected behavior: agent logs data quality issue, includes transaction with amount marked as unavailable, proceeds. The eval ran these cases on every deploy and failed if the agent crashed, retried a permanent error, or returned incomplete data without flagging it.

## Graceful Degradation Verification

**Graceful degradation** means reducing functionality in response to failures while still providing some value to the user. The agent cannot complete the full workflow because a required step failed, but instead of crashing or returning nothing, it completes a partial workflow and clearly communicates what succeeded and what did not. The user gets less than they requested but more than zero.

A travel booking agent had a five-step workflow: search flights, search hotels, search rental cars, check loyalty program benefits, generate itinerary. If the rental car API was unavailable, the agent could still search flights and hotels and generate a partial itinerary without a car. If the loyalty program API failed, the agent could still book everything but could not apply points or discounts. Graceful degradation meant continuing the workflow with the steps that succeeded and marking the steps that failed.

The eval tested this by disabling each external dependency and verifying that the agent completed the workflow with reduced functionality. Test case: rental car API returns 500. Expected behavior: agent completes flight and hotel search, generates itinerary, includes a note that rental car search failed and the user should check availability separately. Actual behavior in the first implementation: agent crashed because it tried to merge the car search results before checking whether the search succeeded. The eval caught this before production. The team added conditional logic — if car search fails, skip the car merge step and add a fallback message to the itinerary.

Graceful degradation is not the same as ignoring errors. Ignoring errors means pretending the step succeeded when it failed, which leads to silent corruption. Graceful degradation means acknowledging the failure explicitly, adjusting the output to reflect the reduced scope, and ensuring the user understands what is missing. The eval verifies that the agent never silently drops a failed step and never includes placeholder or null data without explanation.

## Fallback Behavior Testing

A **fallback** is an alternative action the agent takes when the primary action fails. If the primary data source is unavailable, query a secondary source. If the primary model is overloaded, route to a smaller backup model. If the preferred output format is unsupported, return a simpler format. Fallbacks ensure continuity when the optimal path is blocked.

The correctness of a fallback depends on whether it provides equivalent value to the primary path. A fallback that returns stale data when fresh data is unavailable might be acceptable for a dashboard refresh but unacceptable for a fraud detection check. A fallback that uses a smaller model might be acceptable for a low-stakes classification task but unacceptable for a medical diagnosis. The eval must verify not only that the fallback executes when the primary fails but also that the fallback is appropriate for the use case.

A legal research agent queried a primary case law database and, if that failed, queried a secondary archive. The secondary archive was slower and less comprehensive but still useful for most queries. The eval tested the fallback by simulating primary database unavailability and verifying that the agent queried the secondary database and returned results. This worked well for routine queries. It failed catastrophically for time-sensitive queries. A lawyer preparing for a hearing needed the most recent case filings. The primary database was updated daily. The secondary archive was updated weekly. When the primary database was unavailable, the agent fell back to the secondary archive and returned week-old data without indicating that the results were stale. The lawyer relied on outdated case law and lost a motion.

The eval was updated to verify that when the agent used a fallback, it checked whether the fallback met the quality requirements for the query context. For time-sensitive queries, the secondary archive was not an acceptable fallback. The agent should either wait for the primary database to recover or tell the user that current data is unavailable. For routine research queries, the secondary archive was acceptable as long as the results included a timestamp indicating when the data was last updated. The eval now tests not just that the fallback executes but that the fallback is valid for the query type.

## Retry Logic Evaluation

Retry logic is the most common error recovery mechanism and the most commonly implemented incorrectly. The agent encounters a transient failure — a network timeout, a temporary API outage, a rate limit — and retries the operation. If implemented well, retries resolve transient failures without user impact. If implemented poorly, retries amplify failures, exhaust rate limits, create duplicate actions, or delay inevitable permanent failures.

**Retry logic evaluation** means verifying that the agent retries the right failures, skips retries for permanent errors, uses appropriate backoff strategies, respects retry limits, and does not create side effects through repeated execution. This requires testing both successful retries and retry exhaustion.

An e-commerce agent submitted orders to an order management API. The API occasionally returned 503 during high traffic. The agent retried failed submissions with a one-second delay between attempts, up to five retries. The eval tested this by mocking the API to return 503 for the first two attempts and 200 on the third. The agent successfully submitted the order on the third try. The test passed.

In production, the agent created duplicate orders. When the API was overloaded, it accepted the request but took eight seconds to process it and return a response. The agent's request timed out after five seconds. The agent interpreted the timeout as a failure and retried. The retry created a second order. The original request completed in the background, creating the first order. The user was charged twice. The root cause: the agent retried non-idempotent operations without verifying that the original request had not succeeded.

The fix: before retrying an order submission, the agent queried the order status API to check whether the order already existed. If it did, the agent skipped the retry and returned the existing order ID. If it did not, the agent retried safely. The eval was updated to test retry logic with delayed responses, not just immediate failures. Test case: API returns 503 immediately on first attempt, returns 200 after six seconds on second attempt. Expected behavior: agent times out on first attempt, checks order status, finds no existing order, retries, succeeds. Test case: API accepts request but times out before responding, then completes in background. Expected behavior: agent times out, checks order status, finds existing order, does not retry.

Retry logic must also verify backoff strategies. Retrying with a fixed delay can amplify load on an already overloaded service. Retrying with exponential backoff spreads the load and gives the service time to recover. The eval should verify that retry delays increase with each attempt and that the total retry duration does not exceed a reasonable timeout. Test case: API returns 503 on first five attempts. Expected behavior: agent retries with delays of one second, two seconds, four seconds, eight seconds, sixteen seconds, then gives up after thirty-one seconds total. Actual behavior in a poorly implemented agent: agent retries every one second for thirty attempts, hammering the API for thirty seconds straight and getting rate-limited.

## How Agents Should Behave When Things Go Wrong

When an error occurs, the agent has four possible responses: recover and continue, degrade and continue, escalate to a human, or terminate with explanation. The correct response depends on the error type, the workflow context, and the stakes of the task. Your eval must define the expected behavior for every error category and verify that the agent responds correctly.

A healthcare scheduling agent handled appointment bookings. The workflow queried the provider's calendar, checked insurance eligibility, verified referral requirements, and confirmed the appointment. The eval defined error behaviors for each step. If the calendar API fails with a retryable error, retry up to three times. If it fails with a permanent error, escalate to a human scheduler. If insurance eligibility cannot be verified, book the appointment but flag it for manual review. If referral verification fails, do not book the appointment and explain to the patient what document is needed. If confirmation fails after the appointment is booked, escalate immediately because the provider's schedule is now out of sync with the patient's expectations.

The eval tested each scenario. Test case: calendar API returns 503. Expected behavior: retry, succeed on second attempt, book appointment. Test case: calendar API returns 403. Expected behavior: log auth failure, escalate to human scheduler, do not book. Test case: insurance API times out. Expected behavior: book appointment, flag for manual review, notify patient that insurance verification is pending. Test case: confirmation fails after booking. Expected behavior: escalate to human, do not tell patient the booking succeeded until confirmation is verified.

This level of error behavior specification requires close collaboration between Engineering, Product, and domain experts. You cannot define correct error recovery without understanding the domain consequences of each failure mode. A missed appointment is worse than a delayed booking. A double-booked provider is worse than an unconfirmed appointment. The eval encodes these priorities and ensures the agent behaves correctly under pressure.

Error recovery is the difference between an agent that works when everything goes right and an agent that works when things go wrong. The next failure mode is subtler and more expensive: the agent that does not crash but never stops running because it entered a loop it cannot escape.

