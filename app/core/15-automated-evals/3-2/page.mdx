# 3.2 â€” Length and Verbosity Detection

In November 2025, a legal tech company fine-tuned Llama 4 Maverick on contract summaries. The eval suite focused on accuracy: did the model extract the correct clauses, dates, and parties? It did. Accuracy held at 94% through testing. They deployed. Within three days, users complained. The summaries were correct but unreadable. What had been concise 150-word overviews in the base model became 600-word essays in the fine-tuned version. The model had learned to be thorough. It had also learned to be exhausting. Token costs tripled. User satisfaction dropped 40%. The problem was not accuracy. It was verbosity. They had measured the wrong dimension.

## Word and Token Count Heuristics

The simplest verbosity heuristic is raw length. You count words or tokens in the output. You compare against a threshold. If the output exceeds the threshold, you flag it. This requires knowing what normal looks like. You cannot set a threshold without baseline data. You analyze 500-1,000 outputs from your current production system. You measure the distribution of lengths. You set thresholds at the 90th and 95th percentiles. Outputs longer than the 90th percentile get logged and monitored. Outputs longer than the 95th percentile get flagged for deeper evaluation.

The difference between word count and token count matters for cost analysis but not for verbosity detection. Token count correlates almost perfectly with word count for English text. One token is roughly 0.75 words. If you are measuring verbosity to detect when outputs become unusually long, word count is sufficient and easier to compute. If you are measuring verbosity to estimate API costs, token count is necessary. Most teams track both. Word count for heuristic flagging. Token count for budget monitoring.

The failure mode is setting thresholds based on intuition rather than data. A product manager declares that summaries should never exceed 200 words. The heuristic flags 60% of production traffic. Half the flags are false positives: complex contracts that legitimately require 250 words to summarize accurately. The heuristic becomes noise. The team stops trusting it. They disable it. Six months later, a model update causes verbosity to spike and no one notices because the heuristic is gone. Data-driven thresholds prevent this. You set thresholds based on what the system currently produces, not what someone wishes it would produce.

## Detecting Over-Verbose Responses

Over-verbosity is when the output contains more words than necessary to complete the task. It is not the same as exceeding a length threshold. An output can be long and appropriately detailed. An output can be verbose and short. Verbosity is about density of information per word. The challenge is that heuristics cannot measure information density directly. They approximate it using length relative to task complexity.

The pattern that works is length-to-input ratio. You measure the length of the user query or input document. You measure the length of the model output. You calculate the ratio. For summarization tasks, a 10-to-1 compression ratio might be normal. A 3-to-1 ratio suggests the summary is not compressing enough. For question-answering, a 2-to-1 expansion ratio might be normal: the user asks a 15-word question, the model provides a 30-word answer. A 10-to-1 expansion ratio suggests the model is over-explaining.

These ratios are task-specific. You cannot use the same thresholds for summarization and question-answering. You calibrate per task type. You analyze historical data for each task. You measure typical ratios. You flag outputs that deviate significantly. The threshold is not a hard rule. It is a signal that says: this output is unusually verbose relative to the input, investigate why.

The second verbosity indicator is filler phrase density. Certain phrases add words without adding information. "It is important to note that," "as previously mentioned," "in order to," "due to the fact that." You build a list of these phrases. You count how many times they appear in an output. You flag outputs where filler phrases make up more than 8-10% of the total word count. This heuristic catches a specific failure mode: the model that learned to sound formal and thorough by padding every sentence with unnecessary clauses.

The legal tech company that shipped the verbose contract summarizer discovered this pattern when they analyzed flagged outputs. The fine-tuned model used "it is important to note that" an average of 4.2 times per summary. The base model used it 0.3 times. The fine-tuning dataset had been sourced from formal legal writing where that phrase was common. The model memorized the style. The heuristic caught it. They added a post-processing rule: if filler phrase density exceeded 10%, re-generate with a prompt that explicitly discouraged filler. Verbosity dropped 35% without accuracy loss.

## Detecting Under-Length Responses

Under-length is less common but more dangerous. An over-verbose output wastes tokens and user patience. An under-length output might omit critical information. A summary that compresses a 10-page contract into two sentences is not concise. It is incomplete. A customer support response that answers a three-part question with a single sentence probably missed two parts.

The heuristic for under-length is the same as over-length: measure length relative to baseline, flag deviations. The difference is directionality. You set a lower threshold at the 5th or 10th percentile of the baseline distribution. Outputs shorter than this threshold get flagged. The challenge is distinguishing between appropriately brief responses and dangerously terse ones.

One signal is input-output length ratio again, but inverted. If the user input is 200 words and the model output is 8 words, something is likely wrong unless the task is specifically constrained to brief answers. You measure this ratio across historical data. You flag outputs where the ratio is unusually low. The key is calibration. A customer support system might expect detailed responses where output length is roughly equal to input length. A command-parsing system might expect outputs 50x shorter than inputs. The threshold depends on task.

The second signal is required component presence. Many tasks have structural requirements: a summary should include key entities, dates, and actions. A product recommendation should include pros, cons, and a verdict. You build a lightweight heuristic that checks for the presence of these components without evaluating their quality. The heuristic does not parse semantics. It looks for keyword patterns. If the task requires a pros-and-cons structure and the output contains neither the word "pros" nor "advantages" nor "benefits," flag it. If the task requires a date and the output contains no date-formatted strings, flag it.

This heuristic is crude but effective. It catches the model that omits entire sections of the expected output. It does not catch the model that includes the section but fills it with wrong information. That is fine. The heuristic is Tier 1. It flags the obvious structural failures. Tier 2 model-based evaluation catches the subtle content errors.

## Length-to-Complexity Ratio

Not all inputs are equally complex. A user asking "What is the capital of France?" requires a 3-word answer. A user asking "Explain the geopolitical implications of France's position in the EU after Brexit" requires 200 words minimum. A length threshold that works for the first query will incorrectly flag the second. The solution is measuring input complexity and adjusting length expectations accordingly.

Input complexity heuristics include: word count of the input, number of sentences, number of questions asked, presence of multi-part structure, presence of domain-specific terminology. You assign a complexity score. Simple inputs get low scores. Complex inputs get high scores. You set length thresholds that scale with complexity. A simple input expects 20-50 word outputs. A complex input expects 150-300 word outputs. Outputs that deviate from the expected range for their complexity tier get flagged.

The scoring model does not need to be sophisticated. A basic rubric works: single-sentence input with no domain terms equals complexity score 1. Multi-paragraph input with technical terminology equals complexity score 5. You analyze historical data to see what output lengths correspond to each complexity tier. You set thresholds based on observed patterns. The goal is not perfect classification. The goal is reducing false positives where a heuristic flags a legitimately long response to a complex query.

A financial services company built a complexity-adjusted length heuristic for their advisory chatbot. Simple questions like "What is my account balance?" had expected output length of 10-20 words. Complex questions like "Should I refinance my mortgage given current rates and my financial situation?" had expected output length of 250-400 words. They assigned complexity scores based on input length and keyword presence. They flagged outputs that were more than 2x or less than 0.5x the expected length for their complexity tier. The heuristic reduced false positive flags by 60% compared to a static length threshold while catching the same number of true verbosity issues.

## Context-Appropriate Length

Some tasks have explicit length constraints. "Summarize this article in three sentences." "Provide a one-paragraph overview." "Answer in 50 words or less." When the user specifies a length constraint, the heuristic enforces it strictly. You parse the input for length-related instructions. You extract the constraint. You measure whether the output respects it. If the user asked for three sentences and the model produced seven, flag it. This is not approximate. This is deterministic verification of an explicit requirement.

The challenge is when length constraints are implicit. A tweet generation task implies 280 characters maximum even if not stated. A meta description task implies 150-160 characters. An email subject line implies 60 characters. You encode these task-specific constraints as heuristics. The constraint is not in the user input. It is in the task definition. You know the task type from routing logic or metadata. You apply the appropriate length constraint automatically.

This requires task taxonomy. Your system must know which task is being executed so it can apply the right heuristics. A task labeled "summarization:tweet" gets a 280-character limit. A task labeled "summarization:executive" gets a 200-word target. The heuristic layer receives the task type as input and selects thresholds accordingly. Without task taxonomy, you end up with one-size-fits-all thresholds that fit nothing well.

## The Verbosity Creep Problem

Verbosity creep is the pattern where model outputs gradually become longer over time without anyone noticing until costs spike or users complain. It happens for several reasons. Fine-tuning on human-written text that tends toward formality. Prompt changes that add instructions like "be thorough" without quantifying thoroughness. Model updates where the new version defaults to longer outputs. Feedback loops where users occasionally ask for more detail and the system learns to always provide more detail.

The defense against verbosity creep is continuous monitoring of length distributions. You track median output length, 90th percentile length, and mean length per task type. You plot these metrics over time. You set alerts for when median length increases by more than 15% week-over-week or when 90th percentile length exceeds historical norms by more than 25%. When the alert fires, you investigate. You sample recent outputs. You identify what changed. You adjust prompts, retrain models, or update heuristics to bring length back to target.

A healthcare company ran into verbosity creep after switching from GPT-5-mini to GPT-5 for patient education content. GPT-5 produced higher quality explanations but also 40% longer outputs on average. Token costs increased. They had not anticipated this because they tested accuracy, not length. They added length monitoring. They discovered the issue two weeks into rollout instead of two months. They adjusted the prompt to request concise explanations and set a token budget. Length returned to baseline. Cost impact was contained to $18,000 instead of the $120,000 it would have reached if uncaught.

Heuristics catch verbosity creep because they run on every output and trend over time. Model-based evaluation runs on samples. If you sample 2% of outputs, you might miss a gradual length increase until it is severe. Heuristics on 100% of outputs detect the trend immediately. This is the monitoring use case for heuristics, distinct from the anomaly detection use case. You use length heuristics both to flag individual outlier outputs and to track baseline shifts in population-level behavior.

Repetition is a different failure mode but shares the characteristic of being cheap to detect and highly correlated with model malfunction.

