# 5.11 â€” Multi-Reference Evaluation Strategies

A single reference assumes a single correct answer. This assumption breaks for most real-world AI tasks. A customer asks how to return a damaged product. Five different responses are acceptable: direct them to the return portal, provide the return policy link, offer to initiate the return via agent, escalate to a human specialist, or provide instructions for in-store return. All five are correct. They vary in efficiency, user experience, and operational cost, but none are wrong. If your ground truth includes only the return portal response, your eval rejects four valid answers. If your ground truth includes all five, you need a scoring strategy that handles multiple acceptable references. Multi-reference evaluation is not an edge case. It is the default for any task with legitimate response diversity.

The core question is simple: when multiple references exist, how do you score the model output? Do you require match with all references, or any reference, or the best-matching reference? Do you calculate similarity to each reference independently and aggregate? Do you treat all references as equally valid, or weight them by quality? The strategy you choose changes which outputs pass and which fail. It changes how your eval scores correlate with production quality. It changes whether your eval penalizes reasonable model behavior or rewards it. Multi-reference evaluation is not just "add more examples to the reference set." It is a distinct scoring problem with distinct trade-offs.

## When Multiple Answers Are Correct

Multiple valid answers arise from task ambiguity, user preference variation, stylistic flexibility, and equivalent solution paths. A summarization task might accept a five-sentence summary, a three-sentence summary, or a single-sentence headline, depending on use case. A translation task might accept formal or informal register. A customer support response might offer a refund, a replacement, or a discount, depending on policy interpretation. A code generation task might produce iterative loops or list comprehensions. All are correct. None are more correct than the others in isolation.

Multi-reference scenarios also arise from annotation disagreement. Three human experts write reference answers for the same input. Expert one prioritizes brevity. Expert two prioritizes completeness. Expert three prioritizes empathy. All three references reflect valid expert judgment. Choosing one as the single ground truth discards the others' expertise. Averaging them produces a reference that no expert wrote and no user would prefer. The correct approach is to keep all three as valid references and score the model output against the set. If the model matches any of the three, it matched expert-level quality. If it matches none, it deviated from all expert opinions.

Stylistic variation is the most common source of multi-reference needs. The same factual answer phrased ten different ways. The same instruction written in active voice, passive voice, imperative mood, or question form. The same JSON response with fields in different order. The same refund policy stated in two sentences or four. If your eval requires exact lexical match, it rejects nine of the ten valid phrasings. If your eval uses semantic similarity, it still picks one reference as the comparison target. Multi-reference evaluation explicitly acknowledges that variation is acceptable and builds scoring logic that accommodates it.

## Best-Match Scoring Across References

Best-match scoring evaluates the model output against each reference independently and assigns the highest score. If you have three references, you compute similarity to reference one, similarity to reference two, similarity to reference three. The model's score is the maximum of the three. This strategy rewards the model for matching any valid answer. It does not penalize the model for failing to match all answers.

Best-match scoring works when references represent equivalent alternatives. A user asks for the capital of France. Reference one says "Paris." Reference two says "The capital is Paris." Reference three says "Paris is the capital of France." All three are correct. The model outputs "Paris." Exact match with reference one: score 1.0. Partial match with references two and three: lower scores. Best-match scoring assigns 1.0. The model matched the most concise valid reference. It does not need to match the verbose ones.

Best-match scoring prevents over-penalization when one reference is much easier to match than others. A code generation task has three reference implementations. Reference one is 8 lines. Reference two is 15 lines with error handling. Reference three is 25 lines with logging and type hints. The model generates an 8-line solution identical to reference one. Similarity to reference two is 0.60. Similarity to reference three is 0.45. Average similarity is 0.68. Best-match similarity is 1.0. The model produced a correct solution. It matched one valid reference exactly. Average scoring penalizes it for not matching the more complex references. Best-match scoring rewards it for correctness.

The risk with best-match scoring is reference set gaming. If you include one trivial reference, every model output gets a high score. A summarization task includes references of 200 words, 100 words, and 5 words. The 5-word reference is technically correct but not useful. The model outputs 5 words. Best-match score is 1.0. The model matched a valid reference, but it did not produce the quality you wanted. The solution is reference set curation. Every reference must represent acceptable production quality. If a reference is too terse, too verbose, or too low-effort, remove it. Best-match scoring assumes all references are equally acceptable. If they are not, the scores lie.

## Any-Match Scoring for Threshold Evals

Any-match scoring is a binary variant of best-match. Instead of returning the maximum similarity score, it returns 1.0 if similarity to any reference exceeds a threshold, and 0.0 otherwise. This collapses continuous similarity into pass/fail. It works when you care about "did the model produce an acceptable answer" rather than "how close was the model to the best possible answer."

Any-match scoring simplifies reporting. Each test case is a binary outcome. The model passed or failed. You do not aggregate similarity scores. You count pass rate. If 85 out of 100 test cases pass, your model has 85% accuracy. This aligns with classification eval conventions. It also hides information. A test case that scores 0.79 similarity to the best reference fails if your threshold is 0.80. A test case that scores 0.81 passes. The eval treats these as equally different from the perfect case, even though one is borderline and one is a clear match.

The threshold determines eval sensitivity. A threshold of 0.95 requires near-exact match with at least one reference. A threshold of 0.70 allows significant deviation as long as semantic similarity is high. Setting the threshold too high produces false negatives. The model output is acceptable, but it does not match any reference closely enough. Setting it too low produces false positives. The model output is problematic, but it matches one reference at a surface level. The correct threshold depends on your task, your similarity metric, and the diversity of your reference set. A highly diverse reference set with ten references per input can use a lower threshold. A narrow reference set with one or two references per input needs a higher threshold.

Any-match scoring works best when references are discrete alternatives, not continuous variations. A multiple-choice question has four valid phrasings of the same answer. Any-match scoring checks whether the model's answer aligns with at least one phrasing above a threshold. It does not matter which reference matched. All are equally correct. The model either understood the answer or it did not. This is different from a summarization task where one reference is concise, one is detailed, and one is formal. Matching the concise reference is not equivalent to matching the detailed reference. Any-match scoring treats them as equivalent. If your references represent meaningfully different quality levels, any-match is the wrong strategy.

## Coverage Scoring for Completeness

Coverage scoring applies when the ideal output should include elements from multiple references. A product description task has three references. Reference one highlights technical specs. Reference two highlights user benefits. Reference three highlights pricing. The best output includes all three. Coverage scoring measures what percentage of reference content the model output includes.

Coverage scoring computes similarity to each reference independently, then sums or averages the scores. If the model output matches reference one at 0.90, reference two at 0.85, and reference three at 0.70, the coverage score is the sum: 2.45, or the average: 0.82. Summing rewards models that cover more references. Averaging normalizes for reference set size. Choosing sum versus average depends on whether you want absolute coverage or proportional coverage.

Coverage scoring changes optimization incentives. Best-match scoring encourages the model to match one reference well. Coverage scoring encourages the model to match all references partially. A model that outputs a hybrid response covering elements from all three references scores higher than a model that outputs a perfect match to one reference. This aligns with tasks where completeness matters. It misaligns with tasks where focus matters. A customer support response that tries to address three different interpretations of the user's question is worse than a response that confidently addresses the most likely interpretation. Coverage scoring rewards the hedging response. Best-match scoring rewards the focused response.

Coverage scoring also inflates scores when references overlap. If all three references say the same thing in different words, the model output that matches the common content scores high on all three. The coverage score suggests the model covered diverse information. In reality, it covered one idea phrased three ways. The solution is reference deduplication. Before computing coverage, cluster references by semantic similarity. If two references are more than 0.90 similar, treat them as one. Compute coverage over the deduplicated set. This prevents inflated scores from redundant references.

## Handling Equivalent Paraphrases

Paraphrase equivalence is the hardest problem in multi-reference evaluation. Two references express the same information with different words, structure, and length. The model output matches the meaning but uses a third phrasing. How do you score it? Exact match fails. Lexical overlap is low. Embedding similarity works if all three phrasings map to nearby points in semantic space. LLM-based judges work if the judge can recognize paraphrase equivalence. But all these approaches introduce variance. The model output is equivalent to the references, but the eval score fluctuates depending on how the similarity metric handles paraphrase.

The most reliable approach is reference normalization. Convert all references and the model output into a canonical form before comparison. For structured outputs, canonical form is straightforward: normalize JSON key order, strip whitespace, lowercase strings, sort arrays. For natural language, canonical form is harder. You can extract entities and relations, compare the extracted sets, and ignore surface phrasing. You can convert sentences to semantic triples and compare triples. You can generate multiple paraphrases of each reference using an LLM and expand the reference set to include all paraphrases. Each approach trades off simplicity, accuracy, and computational cost.

Paraphrase detection also benefits from reference augmentation. Start with one human-written reference. Generate five paraphrases using Claude Opus 4.5 or GPT-5.2. Add all five to the reference set. Now the model output is compared against six references instead of one. If the model output is a valid paraphrase, it likely matches at least one of the generated references. This increases recall. It also increases the risk of adding invalid paraphrases to the reference set. The LLM that generates paraphrases might introduce subtle meaning shifts, factual errors, or tone mismatches. Every generated reference must be reviewed before inclusion. If you generate paraphrases at scale, you need an automated quality check on the generated references themselves. Otherwise, your reference set includes incorrect ground truth, and your eval scores reward model outputs that match the incorrect references.

## The Reference Explosion Problem

Multi-reference evaluation scales poorly. A dataset with 1,000 test cases and one reference per case requires 1,000 references. A dataset with 1,000 test cases and five references per case requires 5,000 references. A dataset with 1,000 test cases and twenty paraphrase-augmented references per case requires 20,000 references. Every reference must be written, reviewed, and maintained. Every reference increases eval runtime. Every reference increases storage and versioning cost. The reference set grows faster than the test set. At some point, the cost of maintaining the reference set exceeds the value of the eval.

Reference explosion is most severe for generative tasks with high output diversity. Summarization, creative writing, code generation, open-ended Q&A. A single input can have hundreds of valid outputs. You cannot enumerate them all. You must either sample a small set of valid references and accept that the eval will reject some valid outputs, or you must switch to reference-free evaluation. Multi-reference evaluation is the middle ground. It acknowledges that one reference is insufficient. It avoids the cost of exhaustive enumeration. It balances coverage and practicality.

The practical limit for manual reference creation is around five to ten references per test case. Beyond that, annotation cost becomes prohibitive. If your task requires more references, you need automated reference generation. LLM-generated paraphrases, rule-based transformations, retrieval of similar historical outputs. Automated generation introduces quality risk. Every generated reference is a potential source of eval drift. If 10% of your generated references are subtly wrong, and your model learns to match them, your eval rewards incorrect outputs. The solution is layered validation. Generate references automatically. Sample 10% for human review. Measure agreement between human reviewers and generated references. If agreement is above 0.90, trust the generated set. If agreement is below 0.90, increase the review sample or switch back to manual reference creation.

## Weighting References by Quality or Preference

Not all references are equally good. Some represent best practices. Some represent acceptable minimums. Some represent legacy behavior you are trying to move away from. If you treat all references as equivalent, your eval rewards the model for matching the worst reference as much as the best. Weighted multi-reference evaluation assigns each reference a quality score. The model's score is a weighted combination of its similarity to each reference.

Quality weighting requires reference rating. A human expert reviews each reference and assigns a score from 0.0 to 1.0. A reference rated 1.0 is exemplary. A reference rated 0.70 is acceptable but not ideal. A reference rated 0.50 is borderline. The model's final score is the weighted sum of similarity to each reference, where the weight is the reference quality score. If the model matches the exemplary reference at 0.90 similarity, and matches the acceptable reference at 0.95 similarity, the weighted score favors the exemplary match: 0.90 times 1.0 plus 0.95 times 0.70, divided by 1.7, equals approximately 0.92. The model is rewarded more for matching the high-quality reference than the acceptable reference.

Quality weighting aligns eval scores with production preferences. You prefer outputs that match best practices over outputs that match legacy behavior. You prefer outputs that match formal tone over outputs that match casual tone if formal is your brand standard. You prefer outputs that match concise summaries over verbose summaries if conciseness is a quality dimension. Unweighted multi-reference evaluation treats all references as equally desirable. Weighted multi-reference evaluation encodes your preferences directly into the scoring function.

The risk is subjectivity. Quality ratings are human judgments. Two annotators might disagree on whether a reference is exemplary or merely acceptable. If quality weights vary across annotators, eval scores vary. The solution is calibration. Before rating references at scale, have multiple annotators rate a sample set. Measure inter-annotator agreement. If agreement is low, refine the rating rubric. Define what makes a reference exemplary versus acceptable. Provide examples. Retrain annotators. Measure again. Only deploy quality weighting when inter-annotator agreement exceeds 0.85. Otherwise, the noise in quality ratings creates noise in eval scores, and your eval loses reliability.

## When to Use Single-Reference Versus Multi-Reference

Single-reference evaluation is simpler, cheaper, and easier to interpret. Use it when there is genuinely one correct answer, or when output diversity is low enough that one representative reference suffices. Factual Q&A, classification, structured extraction, tool call verification. Tasks where correctness is objective and variance is minimal.

Multi-reference evaluation is necessary when legitimate answer diversity exists. Open-ended generation, summarization, style-sensitive tasks, scenarios with multiple valid solution paths. Tasks where excluding valid answers from the reference set creates false negatives. Tasks where human annotators produce diverse but equally correct references.

The cost of multi-reference evaluation is annotation burden, storage overhead, and scoring complexity. The cost of single-reference evaluation is false negatives when the model produces valid outputs that do not match the one reference. The trade-off depends on your task, your team's capacity, and your tolerance for eval errors. If false negatives block valid model improvements, multi-reference is worth the cost. If false negatives are rare, single-reference is sufficient. Most production evals use a hybrid: single-reference for objective tasks, multi-reference for subjective tasks, and reference-free for tasks where reference enumeration is impossible.

Multi-reference evaluation assumes your references stay correct over time. But ground truth is not static. Facts change. Policies update. APIs evolve. When your references fall out of sync with reality, your eval scores become meaningless.

---

The next failure mode is stale ground truth: references that were correct when written but are now outdated, and how to detect and fix them before they break your eval pipeline.
