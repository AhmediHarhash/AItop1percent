# 11.11 — Retraining Triggers and Regression Gates

Most teams believe that retraining makes models better. The logic feels sound: eval failures show where the model struggles, so you collect those failures, retrain on them, and ship the improved version. Except this creates a different problem. The enterprise that retrained every two weeks saw their customer satisfaction scores drop from 4.2 to 3.6 over three months. Every new model version fixed the latest batch of failures but broke something else. Support tickets shifted from "the bot doesn't understand refund requests" to "the bot used to handle password resets perfectly, now it fails half the time." Their retraining cadence was destroying the consistency their users depended on. They were optimizing for recency at the cost of reliability.

The insight they missed: not every eval failure means the model needs retraining. Most failures indicate prompt issues, missing context, or stale knowledge that retrieval should handle. The model itself is often fine. Retraining is the right response only when failures accumulate in patterns that indicate the model's learned capabilities have genuinely degraded or when new capabilities are needed that prompting alone cannot provide. And when retraining is warranted, regression gates are non-negotiable. You cannot ship a new model version without proving it preserves every capability the current version has. Retraining without regression gates is Russian roulette with your production system.

## The Retraining Decision Framework

Before any retraining trigger fires, you need a decision framework that distinguishes model-level issues from everything else. The production healthcare chatbot was failing on 8% of appointment scheduling requests. The team's first instinct was retraining. The actual root cause: their prompt didn't include the clinic's updated booking policy from January 2026. The model was performing exactly as trained. The failure was a knowledge gap, not a capability gap. A RAG update fixed it in twenty minutes. Retraining would have taken three weeks and likely introduced new failures elsewhere.

The framework works in layers. First layer: is this a knowledge issue? If the model needs information it doesn't have—updated policies, recent events, new product features—retrieval or prompt updates solve it faster and safer than retraining. Second layer: is this a prompt issue? If tweaking instructions, examples, or output format fixes the failure, the model's capabilities are sufficient. You're just using them wrong. Third layer: is this a context issue? If the failure disappears when you provide different or additional context, the model can handle the task—it just needs better inputs. Fourth layer: is this a model capability issue? Only when failures persist across knowledge updates, prompt variations, and context improvements do you have evidence that the model itself needs to change.

Most failures resolve at layers one through three. The teams that retrain too often skip this diagnostic step. They see failures and assume the model is the problem. The teams that retrain effectively treat it as a last resort after systematically eliminating cheaper, safer fixes.

## Accumulation Triggers and Pattern Detection

When failures do indicate model issues, you need triggers that initiate retraining automatically but conservatively. **Accumulation triggers** fire when a specific failure category crosses a threshold over a defined period. The financial services platform set their trigger at fifty failures in the tax calculation category over a rolling thirty-day window. Below fifty, failures were treated as edge cases—logged, analyzed, but not sufficient to justify retraining. At fifty, an automated workflow kicked in: collect all failing cases, package them for review, and prepare the retraining dataset if human reviewers confirmed the pattern.

The threshold isn't arbitrary. It balances two risks. Set it too low and you retrain constantly, chasing noise and destabilizing the system. Set it too high and you tolerate degraded performance for too long, eroding user trust. The right threshold depends on your failure rate, your traffic volume, and your deployment cadence. High-traffic systems can afford lower thresholds because they hit them faster with more statistical confidence. Low-traffic systems need higher thresholds to avoid retraining on flukes.

Pattern triggers are more sophisticated. They fire not when failures hit a count but when failures exhibit specific characteristics that indicate systemic model degradation. The e-commerce support bot had a pattern trigger for sentiment deterioration: if customer satisfaction scores on bot interactions dropped below 3.8 for five consecutive days AND the failure mode analysis showed increasing frustration language in transcripts, the trigger fired. This caught drift that raw failure counts missed—the model was technically completing tasks but doing so in ways that annoyed users.

Another pattern trigger: consistency degradation. If the model gives different answers to semantically identical questions more than 12% of the time over a week, that suggests the decision boundary is becoming unstable. Retraining on carefully curated consistency examples often tightens this up. Pattern triggers require more instrumentation than simple counts, but they catch problems earlier and with higher precision.

## The Retraining Queue and Data Collection

When a trigger fires, failing cases enter the **retraining queue**, a structured dataset of inputs, outputs, and metadata that forms the foundation for the next training run. The queue is not just a pile of failures. It's a curated dataset with diversity controls, deduplication, and labeling workflows. The team that dumped every failure into their retraining data created a model that memorized their most common edge case—and started hallucinating it everywhere. Their queue had 2,400 examples of a specific error pattern and thirty examples each of twelve other patterns. The retrained model fixed the dominant pattern and ignored the rest.

Your retraining queue needs compositional balance. Track failure categories and ensure no single category dominates beyond its actual prevalence in production. If appointment scheduling failures are 40% of your total failures but only 15% of actual traffic, cap them at 20% of your retraining queue. Overrepresentation teaches the model that rare cases are common, skewing behavior.

Deduplication is equally critical. Users often retry the same failing request multiple times, creating duplicate entries. If one user tries the same query fifteen times and it fails fifteen times, that's one failure, not fifteen. Deduplicate by semantic similarity, not exact string matching—users rephrase, so "cancel my order" and "I want to cancel my recent order" are the same intent. The embedding-based deduplication that clustered queries and sampled one per cluster reduced queue size by 63% while preserving diversity.

Metadata matters. Every queue entry needs the failure mode, the user context, the timestamp, and the human-reviewed label if available. This metadata enables filtering when you build the actual training set. You might pull high-confidence labels for the core training set and medium-confidence labels for validation, keeping low-confidence labels out entirely until a human reviews them.

## Automated Retraining Initiation

Once the queue reaches sufficient size and quality, automated workflows can initiate retraining without human intervention—but only if you've built the right safeguards. The logistics company's automation kicked off a fine-tuning run every time their queue hit 800 labeled examples. The workflow pulled the latest base model, merged queue data with their golden training set, started the training job on reserved GPU quota, and posted a Slack notification to the ML team. Humans didn't need to click anything. The system recognized the trigger condition and executed the pipeline.

But automation doesn't mean blind execution. Their workflow included pre-flight checks. Before training started, an automated script validated that the queue had no catastrophic data quality issues: no examples with empty labels, no examples with the same input mapped to contradictory outputs, no category with fewer than ten representatives. If any check failed, the workflow halted and notified the team. Automation handles the routine path. Humans handle the exceptions.

The training job itself ran on a fixed schedule—weekends only, when traffic was low and rollback risk was minimal. Even though the trigger might fire on a Tuesday, training didn't start until Saturday morning. This prevented mid-week deployments during high-traffic periods and gave the team time to review the queue composition before the job ran. Fully automated retraining is appropriate only when your data quality processes, your eval coverage, and your rollback procedures are mature enough that you trust the system to make decisions without real-time oversight.

Some teams never fully automate initiation. They use triggers to prepare the queue and notify the team, but a human always makes the final decision to start training. This is the safer default. Automation is a maturity unlock, not a day-one requirement.

## Regression Gates and Capability Preservation

No retrained model deploys without passing regression gates—a suite of eval tests that prove the new version preserves every capability the current version has. The SaaS company that skipped regression testing deployed a retrained model that improved accuracy on their target failure category by 9 points but broke their multi-turn conversation handling entirely. Users who previously had smooth three-turn exchanges suddenly got confused, context-free responses on turn two. The model had forgotten how to track conversation state. Regression gates would have caught this before it reached production.

Your regression suite is distinct from your improvement suite. The improvement suite tests whether the new model fixes the failures that triggered retraining. The regression suite tests whether the new model still handles everything the old model handled. Both must pass. A model that fixes new problems but breaks old capabilities is not an improvement—it's a lateral move with different failure modes.

The regression suite pulls from your golden set, your historical pass cases, and your capability inventory. If your current model handles twenty distinct task types, your regression suite must include examples of all twenty. If your current model supports four languages, regression tests all four. If your current model respects six different output format constraints, regression validates all six. The healthcare system's regression suite had 1,800 cases covering appointment scheduling, symptom checking, prescription refills, insurance verification, lab result explanations, and general health questions. Every retrained model ran against the full suite before deployment consideration.

Thresholds matter. The regression gate isn't "the new model must be perfect on all 1,800 cases." It's "the new model must not degrade performance on any category by more than X points compared to the current model." Their threshold was 2 percentage points per category. If the new model scored 94% on appointment scheduling and the current model scored 96%, that's a 2-point drop—right at the threshold. Reviewers investigated. If the new model scored 91%, that's a 5-point drop and an automatic failure. The model doesn't deploy until that regression is fixed.

Regression gates also test for behavioral consistency. If your current model refuses to answer questions about self-harm with a specific safety message, the new model must maintain that behavior. If your current model formats addresses in a particular structure, the new model must preserve that format. Capability preservation includes both what the model does and how it does it. Inconsistent behavior breaks user trust even if the underlying capability remains.

## The Retraining Loop Architecture

The full loop connects eval failures to model updates to deployment in a continuous cycle. Failures flow into the queue. The queue triggers retraining when thresholds are met. Training produces a candidate model. The candidate runs through regression gates and improvement evals. If both pass, the candidate enters staged rollout. If either fails, the candidate is rejected and the failures inform the next queue iteration. Deployed models generate new eval data, which feeds back into the queue. The loop never stops.

Architecture this loop as a state machine with explicit transitions. State one: collecting failures. State two: queue ready, awaiting approval or auto-trigger. State three: training in progress. State four: candidate eval. State five: staged rollout. State six: full deployment. Each state has entry conditions, exit conditions, and rollback paths. The financial platform's state machine prevented simultaneous retraining jobs—if a training job was already in state three, new triggers were queued but not executed. This avoided resource contention and made debugging easier.

Your loop needs versioning discipline. Every model version gets a unique identifier tied to the exact training data, the exact base model, and the exact hyperparameters used. When a regression appears in production three weeks after deployment, you need to trace it back to the specific training run and the specific queue composition that produced it. The team that lost this traceability spent six weeks trying to reproduce a failure because they couldn't identify which version introduced it or what data was involved.

Monitoring the loop itself is as important as monitoring the models it produces. Track queue growth rate, queue composition over time, trigger frequency, training success rate, regression gate pass rate, and deployment cadence. If your queue is growing faster than your retraining cadence, you're accumulating technical debt. If your regression gates fail more than 20% of candidates, your training process isn't stable. If your deployment cadence exceeds one new model per week, you're likely retraining too aggressively and destabilizing production.

## Guardrails and When Not to Retrain

Some failures should never trigger automated retraining. Safety failures, legal violations, and bias incidents require human investigation before any model changes. The insurance chatbot that started denying valid claims due to a data labeling error would have made the problem worse if it retrained automatically on those denials. The failures indicated a systematic issue in ground truth, not a model capability gap. Retraining would have baked the error into the model's behavior.

Your trigger logic needs exclusion rules. If a failure is tagged as a safety incident, it bypasses the retraining queue entirely and routes to the incident response workflow. If a failure involves regulated content—medical advice, financial recommendations, legal guidance—it requires manual review before it can inform training. If a failure occurs in a demographic subgroup and suggests disparate impact, it triggers a fairness audit, not retraining.

Temporal guardrails matter too. Don't retrain in the 48 hours after a major product change, a policy update, or a significant traffic spike. Let the system stabilize first. The retail platform that retrained the day after launching a new product category saw their failure rate triple because the queue was filled with examples from a single chaotic day. Wait a week, let traffic normalize, then evaluate whether retraining is warranted.

And some failures should trigger rollback, not retraining. If your current model's performance suddenly degrades across the board—failure rate jumps from 3% to 15% in six hours—that's not a retraining signal. That's an incident. Roll back to the previous version, investigate the root cause, and fix the production issue before considering model changes. Retraining is for gradual drift and capability expansion. It's not a fix for acute production failures.

The disciplines that make automated retraining safe—regression gates, capability preservation, guardrails—are the same disciplines that make eval systems improve themselves over time, which is where the next subchapter turns.

