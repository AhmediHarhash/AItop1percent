# 4.9 — Reference Leakage: When Judges Over-Credit Answer Structure

Providing a reference answer to an LLM judge does not just tell the judge what the correct answer looks like. It biases the judge toward any answer that structurally resembles the reference, even when other structures are equally valid or better. The judge learns a template. It rewards outputs that match the template. It penalizes outputs that deviate, regardless of whether the deviation improves quality. This is **reference leakage** — the phenomenon where the reference answer contaminates the evaluation criteria, replacing "is this answer correct" with "does this answer match the reference format."

## The Double-Edged Sword of Golden Answers

A **reference answer** — sometimes called a golden answer or ideal answer — is the example you provide to the judge to anchor its evaluation. In a customer support task, the reference might be the response your best human agent would write. In a summarization task, the reference might be the summary your editorial team produced. In a code generation task, the reference might be the implementation your senior engineer wrote. The intent is to give the judge a concrete standard. Instead of asking "is this good," you ask "how close is this to the ideal."

The benefit is consistency. Judges with references produce more stable scores across evaluations. The same output gets roughly the same score when judged multiple times. Judges without references drift more — their notion of quality shifts based on recent examples, time of day effects in the model, or random variation in reasoning paths. The reference acts as a calibration point. It prevents the judge from inventing its own quality standards.

The cost is that the reference becomes the standard, not a standard. The judge does not ask "is this answer as good as the reference." It asks "is this answer similar to the reference." Similarity is easier to evaluate than quality. The judge can compare structures, count matching elements, measure lexical overlap. It cannot reliably assess whether a different structure achieves the same goal better. So it defaults to template matching. An answer that uses the reference's structure but makes factual errors scores higher than an answer that is completely correct but organized differently. You asked the judge to evaluate quality. It is evaluating conformity.

## How Reference Leakage Shows Up in Practice

Reference leakage appears as unexpected score penalties for valid variations. A question-answering task asks "What are the main causes of the 2025 supply chain disruption?" Your reference answer lists three causes in a specific order: shipping delays, semiconductor shortages, labor strikes. Model A produces an answer with the same three causes in a different order. Model B produces an answer with four causes — the same three plus a fourth valid cause: port congestion. Model C produces an answer with two causes — shipping delays and labor strikes — combined into a single integrated explanation because they are causally linked. All three models are factually correct. Model A scores 92. Model B scores 78. Model C scores 68. The judge penalized Model B for adding information and Model C for reorganizing the structure, even though both changes are improvements.

The penalty is not random. It is systematic and predictable. Judges with references consistently score answers lower when they add, remove, or reorder elements relative to the reference, even when the changes are appropriate. The magnitude of the penalty correlates with the magnitude of the structural difference. Change the order of two items: small penalty. Add a new section: moderate penalty. Reorganize the entire answer: large penalty. The judge is not evaluating whether the answer is correct. It is evaluating edit distance from the reference.

Reference leakage also biases the judge toward the reference's style. If your reference answer is formal, the judge penalizes casual answers. If your reference is verbose, the judge penalizes concise answers. If your reference uses bullet points, the judge penalizes prose paragraphs. You intended the reference to define content standards. It defines presentation standards too. The judge cannot separate them. It learns both, and it enforces both.

## The Insidiousness of Template Matching

Template matching is insidious because it looks like valid evaluation. The judge's scores are consistent. Answers that closely match the reference get high scores. Answers that deviate get lower scores. The ranking feels rational. The problem is that the ranking reflects adherence to a template, not actual quality. In domains with one correct answer — arithmetic, closed-form questions, deterministic tasks — template matching approximates correctness. In domains with multiple valid answers — open-ended generation, creative tasks, strategic decisions — template matching suppresses diversity and punishes innovation.

A team building a summarization system sees this when they update their reference summaries. They improve the references based on user feedback — making them more concise, more actionable, better aligned with what users actually need. They re-run their eval with the new references. Scores drop across the board. Not because quality degraded, but because the model outputs still match the old reference structure. The judge now penalizes them for not matching the new template. The team has two choices: retrain the model to match the new references, or accept that their eval scores no longer reflect production quality. Either way, they are optimizing for template conformity, not for user value.

Template matching also makes it impossible to discover better answers than the reference. If your reference represents your current best understanding of what a good answer looks like, any model that finds a better approach will be penalized for deviating. Your eval becomes a ceiling, not a floor. You can iterate toward the reference, but you cannot iterate past it. This is acceptable when the reference is objectively optimal — solving a math problem, translating a sentence with one correct translation. It is catastrophic when the reference is subjective, incomplete, or domain-limited.

## Detecting Reference Leakage in Your Data

You detect reference leakage by measuring score correlation with structural similarity. Take 500 evaluation examples. For each, calculate structural similarity between the model output and the reference answer. Use metrics like edit distance, token overlap, section alignment, or semantic similarity focused on structural features rather than meaning. Then measure the correlation between structural similarity and the judge's quality score. If the correlation is above 0.4, you have reference leakage. If it is above 0.6, your judge is primarily a template matcher.

You also detect leakage by testing valid variations. Take a reference answer and create three variations: same content with reordered sections, same content with one additional valid point, same content rewritten in a different structure. All three should score equivalently to the reference because they are equally correct. Run them through your judge. If any variation scores more than 10 points lower on a 100-point scale, the judge is penalizing structure, not evaluating correctness. The gap between the reference score and the variation score is the magnitude of the leakage.

A third test: remove the reference and re-run the same evaluations with a reference-free rubric. If scores change dramatically — correlation below 0.6 between reference-based scores and reference-free scores — the reference was dominating the judgment. If scores stay stable — correlation above 0.8 — the reference was providing useful calibration without creating leakage. Most teams see correlations between 0.5 and 0.7, indicating moderate leakage.

## Reference-Free Evaluation Alternatives

The most direct mitigation is to remove the reference entirely. Design your rubric to evaluate absolute quality, not similarity to an ideal. Instead of "compare this answer to the reference," use criteria like "does the answer contain factually accurate information," "does the answer fully address the question," "is the reasoning logically sound." The judge evaluates the output on its own terms. It does not have a template to match against.

Reference-free evaluation works well when the task has objective correctness criteria. Factual question answering, code correctness, compliance checks, constraint satisfaction — these are domains where you can specify what makes an answer right without showing an example. The judge applies the criteria to each output independently. Variation in structure does not affect the score as long as the criteria are met.

Reference-free evaluation struggles when the task is subjective or complex. "Write an engaging marketing email" is hard to evaluate without an example of what engaging means. "Summarize this document" has hundreds of valid summaries with different trade-offs; the judge needs some anchor to decide which trade-offs are acceptable. In these cases, you need references, but you need multiple references, not one. Provide three to five reference answers that represent different valid approaches. Instruct the judge: "A high-quality answer should resemble one of these references, but it does not need to match any specific one." The judge now has flexibility. It can reward answers that align with any of the reference styles, not just a single template.

## When to Use References and When to Avoid Them

Use references when the task has a well-defined correct answer and you want to prevent the judge from inventing alternative definitions of correctness. Use references when you need consistency across thousands of evaluations and cannot afford drift. Use references when you are comparing models and want to ensure they are all evaluated against the same standard. In these scenarios, the benefits of references — stability, clarity, comparability — outweigh the costs of leakage.

Avoid references when the task has multiple valid solutions and you want to encourage diversity. Avoid references when you are exploring new approaches and do not want to anchor the judge to existing patterns. Avoid references when your current best answer is likely suboptimal and you want the eval to reward improvement, not conformity. Avoid references when your models are likely to produce outputs in different formats or styles, and you want to evaluate substance independent of structure.

When you do use references, rotate them. Do not use the same reference answer for months of development. Update references every few weeks based on the best outputs your models produce. If a model generates an answer that is clearly better than the current reference, promote it to the new reference. This prevents your eval from becoming a static ceiling. It also reduces leakage because the models are chasing a moving target — they cannot overfit to a single template.

## The Three-Reference Heuristic

A practical middle ground: for every evaluation task, create three reference answers that represent different valid approaches. A concise reference, a detailed reference, and a balanced reference. A formal reference, a conversational reference, and a neutral reference. A structured reference, a narrative reference, and a hybrid reference. Provide all three to the judge. Instruct it to score based on the best match among the three. An answer that aligns with any of the three references can score highly. An answer that aligns with none of them is either wrong or represents a fourth valid approach that you have not seen yet.

The three-reference heuristic reduces leakage by reducing the constraint. Instead of one template to match, the model has three acceptable zones. The judge still uses the references as calibration points, but it has room to reward valid variation. The cost is three times the reference creation effort. The benefit is an eval system that does not collapse your output space into a single mode.

When you catch a model producing a fourth valid approach — something clearly correct but structurally distinct from all three references — add it as a fourth reference. Your reference set grows to represent the diversity of valid solutions your models discover. Over time, you build a library of valid answer archetypes. New models are evaluated against the library, not against a single golden standard. The eval becomes a map of the solution space, not a gate that only opens for one path.

---

Position bias corrupts the ranking. Verbosity bias rewards length over value. Style bias rewards form over content. Reference leakage rewards conformity over correctness. These four biases together make LLM judges unreliable unless you measure and mitigate them. The next layer of sophistication — cross-model judges and adversarial probing — builds on the assumption that you have already handled the basics. If you have not, the advanced techniques just layer complexity on top of broken foundations.

