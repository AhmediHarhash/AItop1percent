# 3.6 — Readability and Complexity Scores

In August 2025, a healthcare company launched a patient education chatbot powered by GPT-5.2. The model answered medical questions accurately. The content was evidence-based. The disclaimers were present. Within three weeks, patient satisfaction scores dropped by 30%. The problem was not correctness. The problem was readability. The model wrote at a graduate reading level. The average patient read at an eighth-grade level. Every response was technically correct and completely inaccessible.

## The Readability Mismatch Problem

Users do not complain that your AI is "too hard to read." They stop using it. They click away. They ask someone else. Readability failures are silent until they compound into engagement drops, and by then you have already lost trust. **Readability scoring** makes this failure mode measurable before production, during eval, on every output.

Readability scores quantify how difficult a text is to read. The most common metric is the **Flesch-Kincaid Grade Level**, which estimates the U.S. school grade required to understand the passage. A score of 8.0 means an eighth-grader can read it. A score of 14.0 means it requires a college sophomore. A score of 18.0 means it is graduate-level academic prose. The formula uses sentence length and syllable count. Longer sentences and longer words increase the score.

You run readability scoring on every output. You set a maximum acceptable grade level based on your audience. If you are building a consumer health app, your ceiling might be grade 9. If you are building legal contract summaries for attorneys, your ceiling might be grade 16. If you are building patient discharge instructions, your ceiling might be grade 6 because health literacy studies show that even educated patients prefer simpler language when they are stressed or unwell.

When an output exceeds the readability threshold, the heuristic flags it. The output does not necessarily fail — readability violations are often softer quality issues than factual errors — but it routes to review or triggers a rewrite prompt. Some teams configure automatic simplification: if readability score exceeds threshold, the system sends the output back to the model with an instruction to "rewrite this at a sixth-grade reading level" and re-evaluates. This closes the loop in seconds without human intervention.

## Sentence Complexity Metrics

Readability scoring relies on sentence structure. Long sentences with many clauses are harder to parse than short declarative sentences. Your heuristic evaluator calculates **average sentence length** in tokens or words. If the average exceeds a threshold — say, 25 words per sentence — the output is flagged as potentially hard to read.

Sentence length variability also matters. A passage where every sentence is exactly 18 words long feels robotic. A passage where sentences range from 5 to 30 words feels natural. You measure standard deviation of sentence length. Low variance suggests monotonous structure. High variance mixed with very long maximums suggests the model generated run-on sentences. Both patterns are detectable and both correlate with readability problems.

Some models default to complex sentence structures because they were trained on formal documents, academic papers, or legal text. GPT-5-mini tends toward shorter sentences than GPT-5 base because it was optimized for chat. Claude Opus 4.5 tends toward longer, more formal sentences. Gemini 3 Flash is somewhere in between. If you fine-tune on domain-specific data, sentence structure shifts to match the training distribution. Monitoring sentence complexity across model versions and deployments detects these shifts before users experience them.

In February 2026, a customer support platform switched from GPT-5-mini to GPT-5 base to improve technical accuracy. Average sentence length increased from 14 words to 22 words. Readability grade level climbed from 8.2 to 12.4. User comprehension complaints doubled within a week. The team had evaluated accuracy, latency, and cost. They had not evaluated readability. Adding a readability heuristic to the eval pipeline would have surfaced the problem during A/B testing, before full rollout. They reverted to GPT-5-mini and added a post-processing simplification step for complex answers.

## Vocabulary Complexity

Readability is not just sentence length. It is also word choice. **Vocabulary complexity** measures how often a text uses uncommon, multisyllabic, or domain-specific jargon. The Flesch-Kincaid formula incorporates syllable count as a proxy for word difficulty, but you can go deeper. You can measure the percentage of words that appear in a basic vocabulary list — like the 3,000 most common English words — versus the percentage that fall outside it.

If 95% of words in an output are in the top 3,000 most common words, the text is accessible. If only 70% are, the text uses specialized or uncommon vocabulary that many readers will not recognize. You set a vocabulary accessibility threshold. Outputs below the threshold get flagged. This catches outputs where the model used "utilize" instead of "use," "commence" instead of "start," "endeavor" instead of "try," or "ascertain" instead of "find out." All of these word pairs have the same meaning. One is harder to read.

Vocabulary complexity is domain-dependent. A medical output aimed at doctors should use medical terminology. A medical output aimed at patients should not. You define separate thresholds for patient-facing content and clinician-facing content. The same applies to legal, financial, and technical domains. The heuristic checks: is this output appropriate for its intended audience based on vocabulary distribution?

In 2025, a fintech app generated investment portfolio summaries. Summaries for financial advisors used terms like "alpha," "beta," "Sharpe ratio," and "volatility clustering." Summaries for retail users should have used "performance," "risk," "return," and "fluctuation." The model occasionally mixed the two because it did not reliably distinguish user personas. Vocabulary complexity scoring flagged outputs where technical jargon appeared in retail-facing summaries. The team refined prompts to enforce persona-specific language and re-evaluated. Vocabulary heuristics caught the bleed-through before it reached production.

## Audience-Appropriate Readability

The concept of **audience-appropriate readability** means different users need different complexity levels, and your system must adapt. A legal chatbot answering a law student should write at a higher grade level than the same chatbot answering a tenant asking about lease terms. A medical information tool answering a physician should use clinical terminology. The same tool answering a patient should use plain language.

You tag every task with an intended audience. The audience determines the readability threshold. Eval runs check whether outputs meet their audience-specific targets. If you generate 1,000 patient-facing outputs and 200 exceed grade 9 readability, those 200 fail the heuristic. If you generate 500 physician-facing outputs and 100 fall below grade 12, those might be too simplified, depending on the use case.

Audience tagging also prevents one-size-fits-all models from degrading quality for edge cases. If you fine-tune a model on a mix of patient and physician content without clear segmentation, the model learns an intermediate style that satisfies neither audience well. Readability heuristics, split by audience tag, surface this failure. Outputs for patients creep toward complexity. Outputs for physicians drift toward oversimplification. Both trends are measurable, and measurement drives action — either persona-based prompting, separate models per audience, or more careful data segmentation during fine-tuning.

## Technical versus Consumer Content

**Technical content** often requires complexity. An API reference must use precise terminology. A software deployment guide cannot avoid terms like "ephemeral storage," "stateful set," or "horizontal pod autoscaler." Readability scoring must account for this. If you apply a blanket grade-level ceiling to developer documentation, you will flag every technically accurate output as a failure.

You set different readability rules for technical versus consumer content. Technical docs allow higher grade levels, lower basic vocabulary percentages, and longer average sentence length. Consumer-facing content enforces stricter readability limits. You tag every task as technical or consumer. The heuristic applies the appropriate rubric.

The boundary is not always clear. A technical concept explained to a non-technical user requires balancing accuracy and accessibility. The model must use some technical terms but define them inline. The readability score might be slightly higher than pure consumer content but much lower than pure technical content. You define a middle tier — "technical explanation for general audience" — with its own thresholds.

In June 2025, a SaaS company built a chatbot to explain API error messages to end users who were not developers. The error messages were technical. The explanations needed to be accessible. The model initially parroted the error message language, producing outputs at grade 14 readability. Users did not understand them. The team added a heuristic: explanations of errors must stay below grade 10, must define any term above basic vocabulary on first use, and must include at least one concrete example. Readability scoring enforced the grade level cap. Vocabulary analysis enforced the definition requirement by flagging outputs where technical terms appeared without nearby plain-language equivalents.

## Readability Thresholds by Use Case

You do not set one readability threshold for your entire system. You set thresholds **per use case** based on domain research, user testing, and regulatory requirements. Health literacy research shows that patient-facing medical information should target grade 6 to 8. Legal plain-language initiatives recommend grade 10 for consumer contracts. Government digital service standards often mandate grade 9 for public-facing content.

Your eval pipeline encodes these standards. Each task type has a readability ceiling. Outputs that exceed it fail the heuristic check. This is not subjective. The threshold is defined in advance, justified by research or policy, and applied deterministically. You are not guessing whether an output is too complex. You are measuring whether it violates a documented standard.

When you launch a new task type, you research the audience. What is their average education level? What is the reading level of existing content they consume successfully? What do accessibility guidelines recommend? You set the threshold based on evidence, run a calibration eval on sample outputs, and adjust if the threshold proves too strict or too loose. Once calibrated, the heuristic runs automatically.

Some teams run tiered readability checks. If an output exceeds grade 12, it is automatically flagged. If it exceeds grade 9, it is sampled for review. If it stays below grade 9, it passes. This creates a risk-based funnel. Severely complex outputs always get scrutiny. Moderately complex outputs get spot-checked. Simple outputs proceed without friction.

## Readability Scoring Does Not Measure Clarity

**Readability scores measure complexity, not clarity.** A passage can score at grade 6 and still be confusing if it uses ambiguous pronouns, unclear references, or logical gaps. A passage can score at grade 14 and be perfectly clear if the reader is an expert. Readability is one dimension of quality. It is not the only dimension.

You combine readability heuristics with other signals. An output that scores well on readability but fails coherence checks is still a failure. An output that scores poorly on readability but perfectly on factual accuracy might be acceptable if the audience is technical. Readability scoring does not override other quality dimensions. It adds a measurable constraint that other evaluation methods often ignore.

The most common mistake is assuming that simplifying language automatically improves quality. It does not. Oversimplification loses precision. A medical disclaimer that says "talk to your doctor before doing anything" is more readable than "this information is not a substitute for professional medical advice, diagnosis, or treatment," but it is also less legally defensible and less informative. Readability optimization is a bounded problem. You want the simplest language that preserves meaning, not the simplest language possible.

## Readability Drift and Model Updates

Readability is not stable across model versions. When you update from Claude Opus 4 to Claude Opus 4.5, sentence structure might shift. When you fine-tune on new data, vocabulary distribution changes. When you adjust system prompts to improve factual accuracy, complexity often increases as a side effect. **Readability drift** is the phenomenon where model updates unintentionally change how complex outputs are.

You monitor readability across deployments. Baseline readability is established during initial eval. Every subsequent eval measures deviation from baseline. If average readability grade level increases by more than one full grade, you investigate. If it decreases significantly, you also investigate — oversimplification might indicate the model is avoiding depth.

In December 2025, a legal tech company fine-tuned Llama 4 Maverick on case law summaries. Post-tuning readability jumped from grade 11 to grade 15. The model had learned the formal, complex sentence structures of judicial opinions. Those structures were appropriate in legal briefs but inappropriate in client-facing summaries. The readability heuristic caught the drift during eval. The team adjusted the fine-tuning data to include more plain-language legal writing and re-tuned. Readability returned to baseline.

Readability drift also happens in production due to input distribution shifts. If users start asking more complex questions, the model's answers become more complex in response. If adversarial users intentionally prompt the model with academic jargon, outputs mirror that style. Monitoring production readability alongside eval set readability detects whether the shift is a model issue or a user behavior issue.

## Automating Readability-Based Rewrites

When an output fails readability checks, some systems trigger **automatic simplification**. The output is sent back to the model with an instruction: "Rewrite this response at a seventh-grade reading level while preserving all key information." The rewritten output is rescored. If it now passes, it proceeds. If it still fails, it routes to human review.

This closed-loop approach works when readability is a hard requirement and when the model reliably follows simplification instructions. GPT-5 and Claude Opus 4.5 both handle simplification prompts well in 2026. Earlier models sometimes oversimplified to the point of losing critical detail. You test simplification reliability during eval before enabling it in production.

Automatic simplification adds latency — a second model call per failed output. If 10% of outputs fail readability checks, you add a second inference call to 10% of requests. This is acceptable in asynchronous use cases like email generation or document summarization. It is often unacceptable in real-time chat. You decide where the trade-off is worth it.

Some teams pre-generate multiple outputs at different complexity levels during eval. The model generates one version targeting grade 8, one targeting grade 12, and one targeting grade 16. Readability scoring validates each version. In production, the system selects the version appropriate to the detected user persona. This front-loads the cost to eval time and eliminates latency in production, but it triples inference cost during generation. The trade-off depends on volume, cost sensitivity, and the importance of persona-specific readability.

## Readability as a Quality Gate

Readability scoring is fast enough to be a **pre-deployment quality gate**. Before a new model version ships, you run it against a test set, calculate readability statistics, and compare them to baseline. If readability degrades beyond threshold, the deployment is blocked until the team investigates. This is identical to blocking a deployment for accuracy regression or latency increase.

Readability gates are particularly important for consumer-facing applications in regulated industries. If you are generating patient discharge instructions, financial disclosures, or public health messaging, readability is not optional. It is a compliance and safety requirement. Automated scoring enforces the requirement at scale without requiring human reviewers to subjectively judge "is this too hard to read?"

In January 2026, a government digital services team implemented readability gates for all public-facing AI-generated content. No output could exceed grade 9. Any model update that increased average readability above grade 8.5 required executive approval and a documented justification. This policy was enforced via heuristic eval integrated into their CI/CD pipeline. It eliminated the risk of complexity creep and ensured accessibility standards were non-negotiable.

Readability heuristics are the final structural layer of cheap, fast evaluation. They measure how text is constructed — sentence length, word choice, vocabulary distribution — independently of what the text means. The next heuristic layer — sentiment and tone detection — moves from structure into affect, measuring the emotional and stylistic qualities that shape how users perceive your AI, even when the content is factually correct.

