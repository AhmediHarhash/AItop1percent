# 4.13 â€” Judge Drift: When Model Updates Break Your Evals

In March 2025, a legal tech company running 400,000 contract evaluations per month discovered that their precision scores had jumped from 0.81 to 0.94 overnight. The team celebrated for three hours before someone checked production. User complaints had tripled. The model hadn't improved. The judge had changed. Their eval provider had silently updated the underlying model from GPT-5 to GPT-5.1, and the new judge scored contract clauses with different criteria. Every baseline they had built over six months was now worthless. The team spent four weeks re-establishing ground truth, re-running historical evals, and explaining to executives why their "improvement" was actually measurement collapse. The incident cost $180,000 in engineering time and destroyed trust in their eval system for a quarter.

When the model you use to evaluate output changes, your entire measurement apparatus shifts beneath you. The scores you collected yesterday do not mean the same thing as the scores you collect today. This is **judge drift**, and it is one of the most insidious failure modes in LLM-as-judge evaluation. Unlike other forms of drift, judge drift does not reflect real changes in your system. It reflects changes in your measurement instrument. You cannot fix it by improving your prompts or retraining your models. You can only detect it, contain it, and architect your eval system to survive it.

## Why Judge Drift Happens

Judge drift happens because the models you use as judges are not static artifacts. They are services operated by third parties who update them on their own schedules for their own reasons. When Anthropic releases Claude Opus 4.1 to replace Opus 4, they do so to improve general capabilities. They are not optimizing for the stability of your eval scores. When OpenAI updates GPT-5 with a new training run, they may shift the model's calibration, its verbosity, its interpretation of ambiguous criteria, or its sensitivity to edge cases. None of these changes are bugs. They are the normal operation of a continuously improving service. But every change introduces drift into your measurement.

The drift manifests in multiple ways. The most obvious is score shift: the new judge assigns higher or lower scores to the same outputs the old judge evaluated. A contract summary that scored 0.78 on relevance under GPT-5 now scores 0.84 under GPT-5.1, not because the summary changed, but because the judge's internal weighting of what relevance means has shifted. The less obvious form is criteria reinterpretation. The new judge reads your rubric differently. Where the old judge penalized verbosity heavily, the new judge is more lenient. Where the old judge required three supporting facts to award full marks on accuracy, the new judge accepts two. The rubric text is identical. The judgment behavior is not.

The third form is variance change. The new judge may be more or less confident, more or less consistent, more or less sensitive to edge cases. If your old judge had a standard deviation of 0.09 across repeated evaluations of the same output, and the new judge has a standard deviation of 0.15, your entire confidence interval calculation is now wrong. You built thresholds assuming one level of noise. You are now operating with a different level of noise. Your false positive and false negative rates have shifted, and you will not know by how much until you re-run calibration.

## Detecting Drift After Model Updates

The first defense against judge drift is knowing when it happens. Most eval providers do not announce model updates with the granularity you need. They may say "we upgraded GPT-5 to GPT-5.1," but they will not tell you whether that affects your specific use case, your specific rubrics, or your specific score distributions. You cannot rely on provider announcements. You must instrument your system to detect drift autonomously.

The most reliable detection mechanism is the **golden set re-evaluation cadence**. You maintain a fixed set of 200 to 500 outputs that span your quality distribution, covering edge cases, known failure modes, and representative examples. Every week, you re-run your LLM judge on this golden set and compare the new scores to the historical baseline. If the mean score shifts by more than 0.05, or if the variance shifts by more than 20 percent, you trigger a drift investigation. If the shift crosses your statistical threshold for significance, you lock the eval pipeline and notify the team before any new scores are used for decision-making.

The golden set must be large enough to capture real shifts and stable enough to avoid noise. Two hundred examples is the minimum for most tasks. Five hundred is safer. The set must include examples at every quality tier: outputs that should score below 0.3, outputs that should score between 0.4 and 0.6, outputs that should score above 0.8. If your golden set is all high-quality examples, you will not detect drift that affects low-quality outputs. If it is all edge cases, you will not detect drift that affects the typical case.

You also need version metadata. Every eval run must log the judge model name, the provider-reported version, and a hash of the judge prompt. When drift is detected, you need to know which model version produced which scores so you can segment your historical data correctly. Without version metadata, you cannot tell whether a score shift happened because your product changed or because your judge changed. You lose the ability to reason about causality.

## Version Pinning Strategies

The most straightforward way to prevent judge drift is to pin your judge to a specific model version and never update it. If you use GPT-5-2025-03-01 as your judge, you continue using GPT-5-2025-03-01 indefinitely, regardless of what newer versions become available. This gives you perfect measurement stability. The scores you collect in March 2026 mean exactly the same thing as the scores you collected in March 2025. You can compare them directly, trend them over time, and build cumulative baselines without re-calibration.

The cost is that your judge does not improve. If GPT-5.2 is meaningfully better at nuanced reasoning, you do not get that improvement. If Claude Opus 4.5 fixes a bug in multi-hop evaluation, you do not get that fix. Your judge becomes a frozen artifact, and as the rest of the AI landscape advances, it becomes relatively weaker. For many use cases, this trade-off is worth it. Measurement stability is more valuable than incremental judge improvements. A slightly worse judge that produces consistent scores is more useful than a slightly better judge that produces incomparable scores.

The problem is that not all providers offer indefinite version pinning. OpenAI's API allows you to specify model versions like gpt-5-2025-03-01, but they do not guarantee those versions will remain available forever. Anthropic's versioning scheme is less granular. Google's Gemini API has historically deprecated old versions on short notice. If your pinned version is deprecated, you are forced to migrate, and migration means drift. You need a plan for when pinning fails.

The alternative is **controlled judge migration**. You do not pin indefinitely. You update your judge on your own schedule, not the provider's schedule. When a new model version is released, you treat it as a migration event. You run a parallel evaluation: the old judge and the new judge both score the same golden set. You measure the score delta, the variance delta, and the criteria reinterpretation. If the drift is acceptable, you migrate. If the drift is too large, you delay migration and escalate the decision to the eval team lead. You never allow the provider to force an unplanned migration onto your production eval pipeline.

Controlled migration requires infrastructure. You need the ability to run two judges in parallel. You need the tooling to compare their outputs statistically. You need the process to decide what "acceptable drift" means for your use case. For a high-stakes medical eval, acceptable drift might be a mean score shift of less than 0.02 and no variance increase. For a lower-stakes content moderation eval, acceptable drift might be a mean shift of less than 0.10 and a variance increase of up to 30 percent. The threshold depends on how you use the scores.

## The Drift Detection Cadence

Drift detection cannot be a one-time check. It must be continuous. You need to re-run your golden set evaluation weekly, not just when you suspect a problem. The reason is that provider updates are often silent. You do not get a notification email. You do not see a changelog. The API endpoint you call today may return a different model than the endpoint you called last week, and the only way to know is to measure.

Weekly re-evaluation is the industry standard for production eval systems. It is frequent enough to catch updates within a reasonable window and infrequent enough to keep costs manageable. If your golden set is 500 examples and each evaluation costs two cents, weekly re-evaluation costs $10 per week or $520 per year. That cost is trivial compared to the cost of undetected drift. A single quarter of invalid scores can invalidate months of product decisions and require expensive re-work.

Some teams run daily re-evaluation for critical evals. If you are evaluating medical advice, financial recommendations, or legal contract analysis, the stakes are high enough that daily monitoring is justified. If you are evaluating marketing copy or internal search results, weekly is sufficient. The cadence is a risk decision. The more you depend on score stability, the more frequently you check.

When drift is detected, the response protocol matters. You do not simply log the drift and continue using the scores. You halt the pipeline, notify stakeholders, and investigate. The investigation has three questions. First, did the provider update the model? Check version metadata, API changelogs, and provider status pages. Second, is the drift consistent across the full golden set, or is it localized to specific output types? Run a segmented analysis. Third, is the drift directional or random? If all scores shifted up by 0.06, that is a systematic bias. If scores shifted randomly with increased variance, that is a consistency problem.

Once you understand the drift, you make a decision. If the drift is small and directional, you may choose to recalibrate your thresholds and continue. If the drift is large or inconsistent, you roll back to the previous judge version if possible, or you pause eval until you can migrate to a new judge. You do not simply adjust scores post-hoc. Post-hoc adjustment is statistical malpractice. It assumes you understand the drift well enough to correct it, and you almost never do.

## Impact Assessment for Model Changes

When a judge model changes, the impact radiates through your entire eval system. It is not just that new scores are different from old scores. It is that every decision you made based on old scores is now uncertain. Every threshold you calibrated is suspect. Every trend you observed may be an artifact. Every model you selected based on comparative scores may have been selected for the wrong reasons.

The first impact is on baselines. If you established that your production system scores 0.83 on contract relevance using GPT-5 as the judge, and you migrate to GPT-5.1, you cannot assume the new baseline is still 0.83. You must re-run baseline evaluation with the new judge. You cannot mix old and new scores. You cannot trend across the migration boundary. The migration is a discontinuity. You either re-establish baselines or you accept that you have lost historical context.

The second impact is on thresholds. If you set a quality gate that requires a score above 0.75 to approve outputs for production, that threshold was calibrated against the old judge's scoring behavior. The new judge may score more leniently, in which case your threshold is too loose. Or it may score more harshly, in which case your threshold is too tight. You must re-calibrate thresholds using the new judge's scores on a labeled test set. Threshold re-calibration is not optional. It is the minimum viable response to judge drift.

The third impact is on model comparison. If you ran an experiment in January comparing Model A and Model B, and Model A scored 0.81 while Model B scored 0.79, you concluded that Model A was better. If the judge drifted in February, that conclusion may no longer hold. The new judge might reverse the ranking. You cannot trust historical model comparisons after a judge migration unless you re-run them with the new judge. For active experiments, this means re-running the entire experiment. For historical decisions, it means accepting that you made the best decision you could with the measurement tools you had at the time.

## The Reproducibility Problem with API Judges

One of the hardest problems with LLM-as-judge evaluation is that API-based judges are inherently non-reproducible. If you call the OpenAI API with the same prompt and the same output today, you may get a different score tomorrow even if the model version has not changed. The model is not deterministic. The API applies sampling, temperature, and top-p parameters that introduce variance. Even with temperature set to zero, floating-point rounding and distributed infrastructure can produce slightly different outputs.

This lack of reproducibility makes drift detection harder. If you re-run your golden set and the scores shift by 0.02, is that drift or is it noise? You need statistical tests to distinguish the two. The standard approach is to run each evaluation multiple times and measure the variance. If your judge has a test-retest variance of 0.03 when the model has not changed, and you observe a shift of 0.06 after a suspected model update, the shift is statistically significant. If the shift is only 0.03, it is within the noise threshold and you cannot conclude that drift occurred.

The gold standard for reproducibility is self-hosted judges. If you fine-tune your own judge model and host it on your own infrastructure, you control the model weights, the inference parameters, and the sampling behavior. You can make the judge fully deterministic. Every re-run produces identical scores. Drift becomes impossible unless you explicitly update the model. The cost is that you now own the judge's accuracy, its calibration, and its maintenance. You cannot rely on provider improvements. You cannot blame provider drift for bad scores. The judge is your responsibility.

Most teams do not self-host judges. The engineering cost is too high, and the quality trade-off is not worth it for most use cases. API-based judges are good enough if you accept their limitations and build your eval system to handle non-determinism and drift as normal operating conditions. That means statistical thresholds for drift detection, version pinning where possible, controlled migration when pinning fails, and continuous monitoring of judge behavior.

The reader who builds their eval system around a fragile assumption that the judge will never change will discover, usually after months of false confidence, that the assumption was wrong. The reader who builds their system to survive judge drift, detect it quickly, and respond systematically will have an eval pipeline that remains trustworthy even as the models beneath it evolve. The difference between these two approaches is the difference between a measurement system that works and one that fails silently.

In the next subchapter, we will examine the most insidious failure mode of all: when the models you evaluate learn to satisfy your judge without actually improving quality, a phenomenon called rubric hacking.
