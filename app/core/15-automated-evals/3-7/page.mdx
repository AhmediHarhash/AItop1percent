# 3.7 â€” Sentiment and Tone Heuristics

Can you detect professionalism without a language model? The answer is yes, and the detection is nearly instant. A customer support response that contains the word "whatever" or "obviously" flags differently than one using "certainly" or "I understand." The character-level signals exist independent of semantic meaning. Sentiment and tone heuristics operate on these surface patterns, catching tonal mismatches fast enough to block a response before it reaches a customer. They miss nuance, but they catch the worst failures at a fraction of the cost and latency of model-based evaluation.

## The Three Sentiment Registers

Sentiment classification in production systems operates on a simpler scale than academic research suggests. You need three states: positive, negative, and neutral. Positive responses contain affirmative language, expressions of agreement, and forward-looking phrasing. "Happy to help," "that's correct," "you're all set" all signal positive sentiment. Negative responses contain refusal patterns, corrections, or expressions of limitation. "Unfortunately," "unable to," "that's not possible" flag negative sentiment. Neutral responses state facts without emotional coloring. "Your account balance is 340 dollars," "the meeting is scheduled for Tuesday" carry neutral sentiment. Most heuristic classifiers use lexicon-based scoring: maintain a weighted dictionary of positive and negative terms, sum the scores, threshold into the three categories.

The misclassification rate for lexicon-based sentiment sits around fifteen to twenty-five percent compared to human judgment. The failures cluster in sarcasm, negation handling, and domain-specific language. "Not bad" scores negative on word-level analysis despite expressing mild approval. "I'm thrilled to inform you that your request was denied" scores positive on "thrilled" while delivering bad news. These edge cases matter less than you'd expect. In production, sentiment heuristics serve as anomaly detectors, not ground truth. A response that scores strongly negative when the conversation context is a simple information request triggers escalation. A response that scores positive when rejecting a refund request triggers escalation. The heuristic doesn't need perfect accuracy; it needs to surface the cases where sentiment and context are misaligned.

## Formality Detection Through Linguistic Markers

Tone formality exists on a spectrum from casual conversation to legal documentation. The heuristic markers are surprisingly consistent across domains. Formal tone uses complete sentences, avoids contractions, employs passive voice, and uses longer words. "I am unable to process this request at the current time" is formal. Casual tone uses contractions, active voice, shorter words, and sentence fragments. "Can't do that right now" is casual. You can score formality through several features: contraction count, average word length, passive construction frequency, sentence completeness, use of personal pronouns. A weighted combination of these features produces a formality score from zero to one hundred.

The business value of formality detection appears in consistency enforcement. A financial services chatbot maintains formal tone across all interactions. A teen fashion brand chatbot maintains casual, conversational tone. The model sometimes drifts. GPT-5-mini in particular exhibits tone inconsistency when context windows get long or when system prompts conflict with few-shot examples. The formality heuristic catches drift in real time. If your target tone is formal and a response scores below forty on the formality scale, you flag it. If your target tone is casual and a response scores above seventy, you flag it. The thresholds tune to your brand voice, your domain, your risk tolerance. A law firm sets tight bounds. A gaming company allows wider variance.

## Emotional Register and Urgency Signals

Emotional register measures how much affective language a response contains independent of positive or negative valence. High emotional register uses intensifiers, superlatives, emphatic punctuation in human writing, and emotionally loaded vocabulary. "This is absolutely critical," "you must act immediately," "we're incredibly sorry" all carry high emotional register. Low emotional register states facts without amplification. "The deadline is Friday," "your request was processed," "the balance is due" carry low register. The heuristic counts intensifier words like "very," "extremely," "absolutely," checks for superlatives like "best," "worst," "only," and measures exclamation-equivalent patterns in model outputs.

Urgency detection overlaps with emotional register but focuses on time-pressure language and action-forcing phrases. "Immediately," "urgent," "as soon as possible," "deadline," "expires," "last chance" all signal urgency. The use case for urgency heuristics splits into two categories. First, detecting inappropriate urgency insertion. If your model starts telling users their account "urgently needs attention" when no actual urgency exists, you've introduced dark pattern language. The heuristic flags urgency language in non-urgent contexts for human review. Second, detecting missing urgency. If a user reports a critical production outage and your model responds with low-urgency language, you've mismatched severity. The heuristic can flag the absence of urgency markers when the inbound message contains urgency signals.

## Tone Consistency Across Multi-Turn Responses

Single-turn tone checking misses a critical failure mode: tone drift across a conversation. The model starts formal, shifts casual, then swings back to formal. The user experiences whiplash. Tone consistency heuristics compare formality scores, emotional register, and sentiment across turns within a session. You maintain a rolling window of the last three to five assistant responses, score each on formality and emotional register, then calculate variance. High variance flags inconsistency. A conversation where formality scores are ninety-two, forty-five, eighty-eight, thirty-seven across four turns indicates broken tone control.

The root cause of tone drift is usually context window truncation or conflicting few-shot examples. When the conversation gets long and early turns drop out of context, the model loses the tonal grounding established in the opening. When few-shot examples show different tones, the model samples from the distribution inconsistently. The heuristic doesn't diagnose the cause, but it catches the symptom fast enough to intervene. Some teams route flagged conversations to a tone-normalized prompt. Others escalate to human handoff. The key is catching drift before the customer notices. A user who receives three formal responses followed by "hey no worries, we'll sort that out for ya" experiences a jarring shift that damages trust even if the information is correct.

## Tone-Context Mismatch as Quality Signal

The most valuable application of sentiment and tone heuristics is detecting misalignment between conversational context and response tone. A user asks "What's your return policy?" and receives a response with negative sentiment and urgent emotional register. That mismatch signals a probable error. A user reports a complaint and receives a response with positive sentiment and casual tone. That mismatch signals probable mishandling. The heuristic works by scoring both the inbound user message and the outbound model response, then checking for incompatible combinations.

The incompatibility matrix is domain-specific but follows predictable patterns. Neutral or positive user inputs paired with negative model outputs suggest the model is injecting problems that don't exist. Negative or urgent user inputs paired with casual or positive model outputs suggest the model is minimizing legitimate concerns. High-formality user inputs paired with low-formality outputs suggest the model isn't matching the user's communication style. You encode these rules as a lookup table: if user sentiment is X and model sentiment is Y, flag if the pair falls into forbidden combinations. The false positive rate sits around ten to fifteen percent, mostly from edge cases where the context justifies the apparent mismatch. A user asks "Can I return this after 90 days?" and the factually correct answer is a polite refusal, which scores negative. The heuristic flags it, human review confirms it's correct, you add the pattern to your exception list.

## Sentiment as First-Pass Filter for Customer-Facing Outputs

In high-volume customer support, legal review, or regulated communications, you cannot afford to model-evaluate every response. Sentiment and tone heuristics serve as the first-pass filter. You run every response through fast heuristic checks: sentiment classification, formality scoring, emotional register measurement, urgency detection. Responses that pass all heuristics proceed to the user. Responses that fail any heuristic enter the escalation queue for model-based eval or human review. The throughput advantage is enormous. Heuristic evaluation runs in single-digit milliseconds. Model-based evaluation runs in hundreds of milliseconds to seconds. For a system serving ten thousand requests per hour, heuristic pre-filtering reduces expensive eval volume by seventy to eighty-five percent.

The trade-off is that you're optimizing for catching bad outputs, not certifying good ones. A response that passes heuristics is probably fine, not guaranteed fine. A response that fails heuristics is possibly bad, not definitely bad. The economic logic works because the cost of letting a bad response through is high, but the cost of over-escalating to secondary eval is manageable. You tune thresholds to your risk tolerance. A healthcare chatbot tunes tight, flagging anything with even mild negative sentiment or formality drift. A hobbyist recipe chatbot tunes loose, only flagging extreme outliers. The beauty of heuristics is that threshold tuning is instant and interpretable. You don't retrain a model. You adjust a number in a config file and redeploy in seconds.

Sentiment and tone heuristics are the fastest, cheapest signal you have that something went wrong. They don't tell you what the model should have said. They don't measure factual correctness. They don't evaluate reasoning quality. They tell you that the surface-level linguistic properties of a response deviate from expected patterns. That signal is weak, but it's available in milliseconds for nearly zero cost, and when combined with other heuristics in the next layer of your evaluation pipeline, it becomes part of a robust anomaly detection system that catches most failures before they reach production.

Next, you need to turn multiple weak signals into one strong signal by combining heuristics into composite anomaly scores that isolate the worst outputs with precision.

