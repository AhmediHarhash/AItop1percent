# 2.8 — Threshold-Based Pass/Fail Logic

A threshold is the line where evaluation becomes a decision. Above it, the output passes. Below it, it fails. The threshold is arbitrary until you set it, and once you set it, every output lives or dies by that number. Get it right and you filter failures without blocking good outputs. Get it wrong and you either ship broken responses to users or block 40% of valid responses in pre-production.

Threshold logic converts continuous evaluation into binary outcomes. It is the simplest decision system in automated evaluation and the one most teams configure incorrectly on the first attempt.

## Binary Thresholds for Presence and Absence

The simplest threshold is existence. Either the output contains the required element or it does not. A legal document generator required every contract to include a jurisdiction clause. The eval checked for the presence of the string "governing law" or "jurisdiction" anywhere in the document. If present, the output passed. If absent, it failed. No scoring, no weighting, no ambiguity. The threshold was binary: one or zero. The system blocked 9% of generated contracts in the first month. Every blocked contract was missing the jurisdiction clause. The product team initially thought 9% was a false positive rate. It was not. The model had genuinely failed to include required language. The threshold was correct. The model needed fixing.

**Absence-based thresholds** work the same way in reverse. A customer support chatbot was prohibited from including internal system identifiers in user-facing responses. The eval searched for regex patterns matching database IDs, internal ticket numbers, and employee usernames. If any pattern matched, the output failed. If none matched, it passed. The threshold was zero tolerance — a single match was a failure. This caught 3% of responses in pre-production. All 3% were legitimate failures. The model occasionally included phrases like "I have escalated this to ticket 47382 for review" when it should have said "I have escalated this to our team." Zero tolerance was appropriate because even one exposure of an internal ID was a policy violation.

**Count-based binary thresholds** extend this logic. A chatbot generating product comparisons was required to mention at least three features for each product. The eval counted feature mentions using keyword matching. If the count was three or higher, the output passed. If lower, it failed. The threshold was presence of a minimum quantity. The first calibration run showed 22% of comparisons mentioned only two features. The team considered this a model failure and retrained with examples emphasizing breadth. After retraining, the failure rate dropped to 4%. The threshold stayed at three. The model improved to meet it.

## Numeric Thresholds for Continuous Metrics

Continuous metrics require a cutoff. Response length, toxicity scores, confidence levels, similarity scores — all produce numbers, and numbers need thresholds. A content moderation system used an LLM-based toxicity classifier that returned a score from 0.0 to 1.0. The team needed to decide: what score is too toxic to publish? They started with 0.5 — anything above midpoint fails. The false positive rate was 18%. Responses like "this policy is harmful to small businesses" scored 0.53 because the word "harmful" triggered the classifier. The team raised the threshold to 0.7. False positives dropped to 6%. They raised it again to 0.8. False positives dropped to 2%, but now obvious toxic content was slipping through at 0.76. They settled on 0.75 as the best trade-off: 4% false positives, near-zero false negatives. The threshold was tuned by measuring both error modes on labeled data.

**Length thresholds** are common and deceptively tricky. A summarization system required outputs between 50 and 150 words. Below 50, the summary was too terse to be useful. Above 150, it was not a summary. The eval counted words and applied both bounds. In the first production month, 11% of summaries exceeded 150 words. The team initially assumed this was a model failure. It was not. Many input documents were complex enough that a useful summary required 160 or 180 words. The product team revised the requirement: summaries should be under 200 words, and under 150 is preferred but not required. The threshold changed because the requirement was wrong, not because the model was broken. Thresholds expose mismatches between policy and reality.

**Confidence thresholds** determine when a model is too uncertain to respond. A question-answering system returned a confidence score with every answer. The team set a threshold of 0.6 — any answer with confidence below that triggered a fallback response like "I am not certain, but here is what I found." The threshold was based on calibration: below 0.6, the accuracy of answers dropped below 70%, which the product team deemed unacceptable for user trust. The system deferred to fallback on 14% of questions. User feedback showed this was correct — users preferred an honest uncertainty admission over a confidently wrong answer. The threshold encoded a product decision about acceptable error rates.

## Threshold Tuning with Calibration Data

You cannot set a threshold by intuition. You set it by measuring outcomes on labeled data. A contract analysis system used a classifier to detect risky clauses. The classifier returned a risk score from 0 to 100. The team needed a threshold to decide which contracts to flag for legal review. They labeled 800 contracts as high-risk or low-risk based on lawyer review. Then they tested thresholds from 30 to 80 in increments of 5. At threshold 40, they caught 96% of high-risk contracts but flagged 51% of low-risk contracts as false positives. At threshold 60, they caught 88% of high-risk contracts and flagged 19% false positives. At threshold 70, they caught 71% of high-risk contracts and flagged 8% false positives. The product team decided the cost of missing a risky contract was higher than the cost of over-reviewing safe ones. They set the threshold at 55, which caught 91% of risky contracts with 27% false positives. The threshold was a business decision informed by data.

**Threshold tuning is re-calibration.** Models drift, data distributions shift, user expectations change. A spam classifier had a threshold of 0.8 for two years. Then spammers adapted their language to sound more legitimate. The classifier's scores for spam emails dropped from an average of 0.91 to 0.76. Spam started slipping through at 0.79. The ops team noticed the drift in monitoring dashboards and re-labeled 500 recent emails. The new calibration showed the threshold should drop to 0.7 to maintain the same false negative rate. They updated the threshold. Spam detection recovered. The threshold is not a constant. It is a parameter that tracks the model and the world.

**Multi-threshold systems** use different cutoffs for different actions. A content moderation pipeline had three thresholds for toxicity scores: below 0.5 auto-approve, 0.5 to 0.75 flag for human review, above 0.75 auto-reject. This created three outcome zones instead of two. The system auto-approved 78% of content, flagged 18% for review, and auto-rejected 4%. Human reviewers focused their time on the uncertain middle zone. The thresholds were tuned so that auto-approve had less than 1% error rate and auto-reject had less than 0.5% error rate. The middle zone had a 12% error rate, but that was acceptable because humans reviewed it. The thresholds created a triage system, not a binary gate.

## Hard Thresholds Versus Soft Thresholds

A **hard threshold** is binary enforcement. If the score is below the threshold, the output is blocked. No exceptions, no overrides, no context. A PII detection system had a hard threshold of zero — any detection of a Social Security number, credit card number, or other PII pattern triggered an automatic rejection. The response never reached the user. Hard thresholds are appropriate when the failure mode is catastrophic. Leaking PII is not a "sometimes acceptable" risk. The threshold is zero tolerance because the policy is zero tolerance.

A **soft threshold** is a warning, not a block. A tone classifier scored customer support responses for empathy. Responses below a score of 0.6 were flagged with a warning to the support agent: "This response may come across as cold. Consider revising." The agent could override the warning and send the response anyway. Soft thresholds guide behavior without enforcing it. They work when the human has context the system lacks. An agent replying to an angry customer about a billing error might write a terse, factual response — "Your account has been credited 42 dollars" — which scores low on empathy but is exactly what the customer wants. The soft threshold flags it, the agent ignores the flag, the customer is satisfied. Hard thresholds would have blocked a correct response.

**Threshold documentation** is not optional. A pricing optimization system used seventeen different thresholds across margin checks, competitive positioning, discount limits, and regional pricing rules. When a pricing analyst joined the team, she asked why the discount threshold was 22%. Nobody knew. The threshold had been set two years earlier by someone who had left the company. The analyst ran a calibration experiment and found the optimal threshold was 18%. The team updated it. Revenue per transaction increased by 3% because discounts were better controlled. The original threshold had been arbitrary, and nobody had written down the reasoning. Every threshold should have a comment in the code or a line in the documentation explaining why it exists, who set it, when it was last calibrated, and what trade-off it encodes.

## The Threshold Cliff Problem

A threshold creates a discontinuity. A response that scores 0.749 fails. A response that scores 0.751 passes. The difference is 0.002, but the outcome is binary. This is the threshold cliff — tiny changes in score produce massive changes in outcome. A loan approval system used a creditworthiness threshold of 650. An applicant with a score of 649 was denied. An applicant with a score of 651 was approved. The one-point difference determined access to capital. The system was deterministic and fair in application — everyone with 650 or above was approved — but the cliff at 649 felt arbitrary to applicants who were barely below the line.

**Threshold cliffs amplify noise.** If your scoring system has measurement error of plus-or-minus 5 points, and your threshold sits in the middle of that error range, you are making random decisions. A resume screening system scored candidates from 0 to 100 and set a threshold at 70 for interview invitations. The model's score variance on identical resumes was plus-or-minus 3 points due to randomness in embedding generation. Candidates who scored 68 on one run might score 71 on another. The threshold turned noise into outcome variance. The team addressed this by adding a confidence interval: candidates who scored 67 to 73 were marked as "borderline" and reviewed by a human. The threshold became a zone, not a line.

**Threshold stacking** multiplies cliffs. A content generation pipeline had six thresholds: minimum length 40 words, maximum length 200 words, toxicity below 0.3, reading level between grade 8 and grade 12, keyword presence required, and formatting compliance required. An output had to pass all six thresholds to go live. Each threshold had a 2-4% failure rate in isolation. Combined, the overall pass rate was 81% — nearly one in five outputs failed at least one check. Some failures were legitimate. Others were borderline cases caught by threshold cliffs. The team reviewed 200 failed outputs and found 23% were false positives — good outputs that barely missed one threshold. They softened two of the thresholds and added human review for outputs that failed only one check by a narrow margin. The false positive rate dropped to 9%, and the system stopped blocking outputs that were borderline acceptable.

Once you have individual thresholds for individual rules, the next challenge is combining them. Composite checks use AND, OR, and weighted logic to turn multiple rule outcomes into a single decision, and that is where rule-based evaluation either scales or collapses under its own complexity.

