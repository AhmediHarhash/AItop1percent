# 6.7 — Stopping Conditions and Appropriate Termination

Most teams think task completion is binary: the agent either finished or it didn't. They're wrong. The hardest behavioral failure mode is not an agent that never stops — those are obvious and get caught in development. The hardest failure mode is an agent that stops at the wrong time. It terminates after three attempts when six were needed. It declares success when the underlying task hasn't been solved. It keeps retrying a fundamentally impossible request until it hits a timeout, wasting compute and user time. The decision to stop is itself a behavior, and it needs evaluation just like tool selection or reasoning quality.

**Appropriate termination** is the ability to recognize when a task is complete, when it's impossible, and when continued effort would be productive versus wasteful. Agents that can't make this judgment produce two kinds of damage. Premature termination looks like success — the agent reports completion, the workflow continues — but the underlying goal wasn't achieved. Over-persistence looks like diligence — the agent keeps trying, exploring alternatives — but the task was never solvable in the first place, and every retry burns tokens and time. Both patterns degrade trust. Users learn that "completed" doesn't mean done, and that timeouts are the only reliable stopping mechanism. Your eval pipeline must detect both.

## The Premature Termination Pattern

An agent stops too early when it mistakes partial progress for completion. It books one leg of a two-leg flight and reports success. It retrieves a document but doesn't extract the requested information. It sends an email draft to a review queue instead of to the recipient. The task workflow continues as if the goal was achieved, but downstream steps fail because the prerequisite wasn't actually met. These failures often appear in production logs as user complaints, not as system errors, because the agent never crashed — it just stopped before finishing.

Detecting premature termination requires encoding what "done" actually means for each task type. For a multi-step task, completion means all steps succeeded, not just the first one. For a retrieval task, completion means the requested information was found and extracted, not just that a search returned results. For a modification task, completion means the change was applied and verified, not just attempted. Your eval must compare the agent's self-reported completion status against the actual state of the world after it stopped. If the agent says it's done but the goal state hasn't been reached, that's a premature termination failure.

You encode this as a post-termination verification check. After the agent reports completion, your eval runs a separate verification function that inspects the environment, the tool call history, and the final outputs. Did the booking system show both flights? Did the document contain the extracted answer? Did the email actually send? The verification check doesn't trust the agent's self-assessment — it measures the outcome independently. If the verification fails, the eval logs the case as premature termination, captures the tool trace, and flags the stopping logic for review. The pattern you're looking for is an agent that confidently reports success while leaving the task half-finished.

## The Over-Persistence Pattern

An agent persists too long when it continues attempting a task that can't be solved, either because the goal is impossible with available tools or because the required information doesn't exist. It retries a database query with different parameters when the record was deleted. It attempts to schedule a meeting when all participants declined. It searches for a document that was never created. The agent interprets failure as a signal to try harder, when failure is actually the correct answer. The task should terminate with "impossible" or "not found," but instead it loops through alternatives until a retry limit or timeout forces it to stop.

Detecting over-persistence requires recognizing when repeated attempts aren't making progress. If the agent tries the same action three times with identical results, that's a signal. If it cycles through tool variations without ever reaching a success condition, that's a signal. If it's been executing for ten times the median task duration with no completion in sight, that's a signal. Your eval pipeline must track attempt count, outcome diversity, and elapsed time relative to task type norms. When an agent crosses thresholds that indicate stuck behavior, the eval flags it as over-persistence even if the agent hasn't technically failed yet.

The hardest over-persistence cases are when the agent is making small amounts of progress on an unsolvable task. It finds partial matches when an exact match was required. It retrieves related documents when the specific document doesn't exist. It keeps refining a query that will never return the needed data. The agent interprets each small improvement as validation that persistence will eventually succeed, but the goal state is unreachable. Your eval must distinguish between productive iteration — where each attempt gets closer to the goal — and unproductive iteration, where attempts explore variations without converging. You measure this by tracking goal-distance metrics over time. If the distance isn't decreasing across attempts, the agent is stuck.

## Stopping Condition Specification

Agents need explicit criteria for when to stop, and those criteria need to cover three outcomes: success, failure, and impossibility. Success conditions define what the completed goal state looks like. Failure conditions define when the current approach isn't working and a different strategy is needed. Impossibility conditions define when the goal can't be achieved with available information or tools, and the correct response is to terminate with an explanation rather than keep trying. Most agents only have success conditions. They don't know how to give up appropriately.

You specify stopping conditions as part of task definitions, not as agent-wide defaults. A scheduling task stops successfully when a meeting is booked and confirmed. It stops with failure when participants decline but reschedules are possible. It stops with impossibility when no valid time exists within constraints. A retrieval task stops successfully when the requested data is found and extracted. It stops with failure when initial queries return nothing but alternative search strategies exist. It stops with impossibility when the data source confirms the information doesn't exist. Each task type gets a condition set that defines all three outcomes.

Your eval pipeline tests whether agents honor these conditions. You create test cases where the goal is achievable, where it requires strategy changes, and where it's impossible. The agent should stop at different points in each scenario — quickly for impossible tasks, after exploring alternatives for difficult tasks, after verification for successful tasks. If the agent treats all three scenarios the same way, either by always stopping early or always persisting to timeout, the stopping logic is broken. The eval measures how accurately the agent classifies task outcomes and whether its stopping behavior matches the classification.

The most sophisticated stopping conditions are dynamic, adjusting based on context. A high-priority task might persist longer before declaring impossibility. A low-risk task might stop after fewer retries. A task with tight latency requirements might terminate faster than one where thoroughness matters more than speed. Your stopping conditions should encode these priorities, and your eval should verify that agents adjust their persistence appropriately based on task metadata.

## Appropriate Termination as a Quality Signal

When an agent stops appropriately, it's a signal that the reasoning layer understands task state. The agent knows what success looks like, recognizes when it's been achieved, and can distinguish productive effort from wasted effort. Inappropriate termination — either too early or too late — indicates a reasoning failure, even if individual tool calls were correct. The agent lost track of the goal, misinterpreted feedback, or couldn't assess progress. Stopping behavior is a window into the agent's self-awareness.

You track termination appropriateness as a first-class behavioral metric. For each completed task, your eval logs whether the agent stopped at the right time, too early, or too late. Over time, you measure termination accuracy across task types, agent configurations, and model versions. A model update that improves reasoning on benchmarks but degrades termination accuracy is a regression. An agent that handles 95% of tasks correctly but stops prematurely on 30% of multi-step tasks has a stopping logic bug. The termination metric surfaces issues that task success rates alone don't reveal.

You also use termination behavior to detect edge cases in task design. If agents consistently over-persist on a specific task type, the impossibility conditions might be underspecified. If they prematurely terminate on another task type, the success criteria might be ambiguous. Termination patterns reveal where your task definitions are unclear, where tool feedback doesn't provide enough signal, and where agents need better guidance about what "done" means. The eval doesn't just measure agent behavior — it exposes gaps in how you've defined the tasks themselves.

## Stopping Conditions in Multi-Agent Systems

When multiple agents collaborate on a task, stopping conditions get more complex. One agent might complete its portion while another is still working. One agent might determine a subtask is impossible while the parent task is still solvable through a different approach. The orchestrator needs stopping logic that aggregates signals from multiple agents and decides whether the overall task should continue, shift strategies, or terminate. Premature termination happens when one agent's failure cascades into full task termination. Over-persistence happens when agents keep retrying subtasks independently without recognizing that the parent task is stuck.

Your eval for multi-agent stopping must trace decisions across the agent hierarchy. When a task terminates, which agent made the stopping decision? Was it based on local subtask state or global task state? Did the orchestrator override a subtask completion signal because the overall goal wasn't met? Did a subtask impossibility signal propagate correctly to the parent, or did the parent keep assigning work to other agents? The eval captures the decision tree that led to termination and identifies cases where the wrong agent or the wrong signal triggered the stop.

You test this with scenarios where subtask outcomes don't align. One subtask succeeds, one fails but is retryable, one is impossible. The correct stopping behavior depends on task structure. For a sequential task, impossibility in any step should terminate the whole task. For a parallel task with redundancy, some subtask failures are acceptable as long as enough succeed. For a best-effort task, impossibility in optional subtasks shouldn't block completion. Your eval creates test cases across these structures and measures whether the multi-agent stopping logic matches the intended behavior.

## Timeout vs Appropriate Termination

A timeout is a safety mechanism — the eval's last line of defense against runaway agents. Appropriate termination is a behavioral skill — the agent's ability to stop itself when continuing is unproductive. Teams often rely on timeouts as the primary stopping mechanism, but timeout-driven termination is a failure mode, not a success. It means the agent couldn't recognize when to stop on its own. Your eval must distinguish between tasks that stop because the agent made a stopping decision and tasks that stop because they hit a time or retry limit.

You track timeout rate as a separate metric from task success rate. If 40% of tasks terminate via timeout, your agents don't have functional stopping logic — they're just running until the eval kills them. Even if those tasks eventually succeed, the latency is unacceptable and the compute cost is wasted. The goal is to drive timeout rate toward zero by improving the agent's ability to recognize completion, failure, and impossibility before hitting external limits. A well-tuned agent should almost never time out.

Your eval also tests what happens when timeout thresholds change. If you cut the timeout in half, do task success rates collapse, or do agents adapt by making stopping decisions faster? If you double the timeout, do over-persistence cases double, or does behavior stay consistent? Timeout sensitivity reveals whether agents are using time budgets as a decision input or just executing until something external stops them. You want agents that adjust their persistence based on task progress and goal distance, not based on how much time is left on the clock.

Stopping conditions and appropriate termination are the behavioral layer that determines whether agents use resources efficiently and produce reliable outcomes. Your eval pipeline must test not just what agents accomplish, but how they decide when to stop trying. The next behavioral dimension is latency — how long the agent takes to reach that stopping point, and whether performance stays within acceptable bounds for the task at hand.

