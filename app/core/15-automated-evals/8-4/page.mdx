# 8.4 — Parallelization and Throughput Optimization

In November 2025, a financial services company rebuilt their eval pipeline after their pre-deployment suite started taking six hours to complete. The team had grown their test set from 200 examples to 4,800 examples over eight months, adding coverage for new product features, edge cases, and regulatory scenarios. Each example required three LLM judge calls—one for factual accuracy, one for compliance tone, and one for risk disclosure completeness. The math was brutal: 4,800 examples times three judge calls times an average of 3.2 seconds per call equaled over twelve hours of sequential execution. The pipeline ran overnight, but failures meant rerunning the entire suite the next night. Release velocity collapsed. A fix that took two hours to write spent three days waiting for eval confirmation. The team spent $18,000 on parallel execution infrastructure over the next month. The same suite now completes in eleven minutes. The lesson was not that parallelization is nice to have. It is that sequential eval execution at production scale makes continuous deployment impossible.

## Parallelization as Pipeline Requirement

When your eval suite takes longer to run than your team's patience for waiting, you have already lost. The correct threshold is not hours. It is minutes. A pipeline that runs in under ten minutes enables real iteration. A pipeline that runs in forty minutes gets run once, maybe twice if the engineer is motivated, then skipped when deadlines press. A pipeline that runs in three hours gets run overnight, which means feedback latency destroys the development loop. **Parallelization** is not an optimization you add later. It is the architectural foundation of a usable eval system.

The core constraint is that most modern evals involve LLM calls—either to the model under test or to LLM judges evaluating outputs. LLM inference is network-bound and latency-sensitive. A single call to GPT-5 or Claude Opus 4.5 might take 800 milliseconds to 4 seconds depending on output length and API load. If your eval suite has 2,000 examples and you run them sequentially, you are looking at minimum 1,600 seconds of wall-clock time just for inference, assuming perfect conditions. Add judge calls, retries, and logging overhead and you are well past an hour. Parallelization is the only way to collapse this timeline into something compatible with continuous deployment.

The design principle is simple: treat each eval example as an independent unit of work that can execute in any order. Most evals are stateless by design—each example is a self-contained input-output pair with no dependency on prior examples. This makes them trivially parallelizable. The challenge is not the algorithm. The challenge is managing the infrastructure that executes those parallel tasks without overwhelming rate limits, burning budget, or losing observability.

## Worker Pool Architecture

The standard implementation is a worker pool model. You define a fixed number of worker threads or processes, each capable of executing one eval example at a time. You feed the worker pool a queue of examples. Each worker pulls an example from the queue, runs the model and any associated judges, records the result, and moves to the next example. The pool size determines your parallelism ceiling.

Choosing the pool size is a trade-off between throughput and cost. A pool of ten workers runs ten examples simultaneously. If each example takes three seconds on average, you complete 200 examples per minute in theory. A pool of 100 workers would complete 2,000 examples per minute—but you hit other constraints first. The most immediate constraint is API rate limits. Most LLM providers impose per-minute request limits and token-per-minute limits. OpenAI's GPT-5 API might allow 10,000 requests per minute for enterprise customers, but typical accounts see limits between 500 and 3,000 requests per minute. If each eval example generates two API calls—one for the model under test, one for the judge—your effective parallelism ceiling is half your rate limit.

The second constraint is cost. LLM judge calls are not free. A typical judge call using GPT-5-mini costs between $0.0002 and $0.0008 per call depending on prompt length and output verbosity. Running 5,000 examples with two judge calls each costs $2 to $8 per run. If you run that suite twenty times a day during active development, you are spending $40 to $160 daily just on judge inference. Higher parallelism does not change the total inference cost, but it does concentrate spend into shorter windows, which can trigger budget alerts or provoke questions from finance teams who see a $600 line item for "API usage" in a single afternoon.

The third constraint is observability. A pool of 100 workers executing simultaneously produces logs, metrics, and error traces in a chaotic interleaved stream. Debugging a failure in example 1,847 when the logs for that example are scattered across twelve seconds of output from ninety-nine other workers requires structured logging with correlation IDs, trace context, and the ability to filter and reconstruct execution flow. Most teams underestimate the observability tax of high parallelism. You need logging infrastructure that can handle burst write load and query infrastructure that lets you isolate individual example executions from the noise.

## Rate Limiting and Backoff

The naive parallel implementation sends all requests as fast as the worker pool allows and crashes into rate limits within seconds. The production implementation meters requests to stay just below rate limits and implements exponential backoff when limits are hit anyway. **Rate metering** is the practice of tracking request velocity in real time and throttling workers when you approach known limits.

The simplest metering strategy is a shared token bucket. The bucket starts with a fixed number of tokens representing available API capacity. Each worker consumes a token before making an API call. Tokens regenerate at a fixed rate matching your known rate limit—if your limit is 1,000 requests per minute, you regenerate 16.67 tokens per second. When the bucket is empty, workers wait until tokens regenerate. This prevents the worker pool from exceeding rate limits even under full load.

The challenge is that rate limits are often multi-dimensional. You have a requests-per-minute limit, a tokens-per-minute limit, and sometimes a concurrent requests limit. A single token bucket does not capture all three constraints. The production solution is multiple buckets or a composite metering layer that checks all constraints before allowing a request. If your eval examples vary widely in length—some generate 50-token outputs, some generate 2,000-token outputs—your token-per-minute limit becomes the binding constraint before your request-per-minute limit. You need to estimate token consumption per example and meter accordingly.

Even with perfect metering, you will hit rate limits occasionally due to API provider throttling, transient capacity issues, or concurrent usage from other systems sharing the same API key. The correct response is exponential backoff with jitter. When a request returns a 429 rate limit error, the worker waits for an exponentially increasing delay before retrying—first one second, then two seconds, then four seconds, up to a maximum like thirty seconds. Jitter adds random noise to the delay to prevent all workers from retrying simultaneously and causing a thundering herd. The backoff policy should be configurable per provider because different APIs have different recovery characteristics.

## Throughput Bottlenecks Beyond Inference

Parallelizing inference is the first bottleneck you hit. The second bottleneck is dataset loading and preprocessing. If your eval suite loads a 4,800-example dataset from a remote database at the start of each run, and that load takes twenty seconds, you have added twenty seconds of sequential overhead before any parallelism begins. The fix is to preload and cache datasets in memory or store them in fast local storage like SSDs. Some teams store eval datasets as pre-serialized files—Parquet, JSONL, or custom binary formats—that load in under a second.

The third bottleneck is result aggregation and storage. Each worker produces eval results—scores, labels, latencies, metadata—that must be collected, aggregated, and written to storage. If every worker writes results to a shared database with row-level locking, you create write contention that throttles throughput. The better pattern is buffered writes. Each worker accumulates results in memory and flushes to storage in batches—every 100 examples or every ten seconds, whichever comes first. After the full run completes, a single aggregation pass computes summary metrics. This trades real-time visibility for throughput, which is the correct trade for batch eval pipelines.

The fourth bottleneck is logging. High-parallelism pipelines generate enormous log volume—thousands of lines per second when dozens of workers are executing simultaneously. If your logging backend cannot keep up with write volume, you get dropped logs, delayed writes, or out-of-memory crashes when log buffers overflow. The production solution is structured logging with log levels, sampling, and async writes. Debug-level logs that trace every API request are sampled at 1% during normal runs and enabled at 100% only when debugging specific failures. Info-level logs that record per-example results are written asynchronously to avoid blocking worker threads. Error-level logs are written synchronously and trigger alerts.

## Cost-Performance Tradeoffs

Faster eval pipelines cost more money. The cost comes from three sources: compute for worker infrastructure, inference for LLM calls, and storage for results and logs. The trade-off is not linear. Doubling parallelism does not double cost—it doubles compute cost but leaves inference cost unchanged, and storage cost grows only slightly. The break-even calculation depends on how much you value time.

A team running evals twenty times a day values ten-minute runtime over forty-minute runtime at roughly $200 per day—the productivity cost of engineers waiting. If parallelizing from ten workers to forty workers costs an extra $30 per day in compute, the ROI is obvious. If parallelizing to 100 workers costs an extra $180 per day but only shaves another two minutes off runtime, the ROI disappears. The correct optimization is to profile your pipeline, identify the binding constraint, and scale just enough to break it.

Some teams over-parallelize out of habit. Running 1,000 workers for a 2,000-example eval suite that completes in eight minutes is waste if the same suite completes in twelve minutes with 200 workers. The marginal four-minute savings does not justify the 5x compute cost. The correct heuristic is to target a runtime threshold that matches your development cadence—ten minutes for teams deploying multiple times per day, thirty minutes for teams deploying daily, sixty minutes for teams deploying weekly. Beyond that threshold, additional parallelism is vanity.

## Scaling Infrastructure for Eval

The infrastructure that runs your eval pipeline must scale with parallelism. At low scale—ten workers, 500 examples—you can run evals on a single VM or in a single container. At medium scale—fifty workers, 5,000 examples—you need horizontal scaling across multiple containers or VMs. At high scale—200 workers, 50,000 examples—you need orchestration platforms like Kubernetes, batch job systems like AWS Batch, or serverless compute like Lambda.

The trade-off is operational complexity versus efficiency. A single VM running a multi-threaded eval script is simple to deploy and debug. A Kubernetes job that spawns 200 pods in parallel is operationally complex—you need to configure resource requests, handle pod failures, manage log aggregation, and monitor cluster capacity—but it scales elastically and shares infrastructure cost across teams. The inflection point is around twenty to thirty workers. Below that threshold, vertical scaling on a single instance is simpler. Above it, horizontal scaling with orchestration is more cost-effective.

Serverless compute like AWS Lambda is attractive for bursty eval workloads because you pay only for execution time. A Lambda-based eval pipeline spins up 200 concurrent invocations, runs for eight minutes, and costs only for those eight minutes across 200 instances. The challenge is cold start latency—Lambda functions take 500ms to 3 seconds to initialize, which adds overhead when launching hundreds of instances—and API rate limits. If every Lambda invocation hits the same OpenAI API key, you hit rate limits immediately unless you implement distributed rate limiting with a shared state store like Redis.

The infrastructure choice depends on team maturity and existing platform capabilities. Teams already running Kubernetes use Kubernetes jobs. Teams on AWS with existing Lambda expertise use Lambda. Teams with neither default to managed batch services like AWS Batch or Google Cloud Run Jobs, which abstract infrastructure complexity without requiring deep platform knowledge. The worst choice is building custom orchestration from scratch. The second-worst choice is running evals on a developer laptop past the prototype stage.

## Monitoring Parallelized Pipelines

High-parallelism pipelines are harder to monitor than sequential pipelines. In a sequential pipeline, execution is deterministic—example one completes before example two begins. In a parallel pipeline, execution is non-deterministic—any example might complete first, failures appear out of order, and logs interleave unpredictably. Monitoring must account for this chaos.

The baseline is per-example tracing. Each eval example gets a unique trace ID that flows through all associated API calls, judge invocations, and result writes. Logs and metrics are tagged with trace IDs. When an example fails, you query logs by trace ID and reconstruct the full execution path. Without trace IDs, debugging parallel failures is guesswork—you see errors in logs but cannot correlate them to specific examples.

The second requirement is real-time throughput dashboards. You need visibility into active worker count, examples completed per minute, average latency per example, API rate limit headroom, and failure rate. These metrics tell you whether the pipeline is healthy or stalled. A sudden drop in throughput signals rate limiting, infrastructure issues, or a systemic model failure that causes retries. A spike in failure rate signals a code regression, API outage, or dataset issue. Without real-time visibility, you discover these problems only after the pipeline completes or times out.

The third requirement is failure isolation. When five examples out of 5,000 fail, you need to know which five, why they failed, and whether the failures are correlated. Did they all fail due to the same judge timeout? Do they share a common input pattern? Are they clustered in a specific dataset segment? Structured error logging with failure categorization—API timeout, rate limit, assertion failure, invalid input—enables post-run analysis that distinguishes systemic issues from random noise.

The next subchapter covers caching strategies that reduce the cost and runtime of repeated eval runs by reusing prior results for unchanged examples.

