# Chapter 11 — Governance, Compliance, and Closed-Loop Improvement

Automated eval systems make decisions that affect users, products, and compliance posture. Who owns an eval? Who can change its thresholds? What audit trail proves that the eval was running when a decision was made? How do you ensure reproducibility when a regulator asks you to demonstrate how a model was validated before deployment? And how do you close the loop — turning eval failures into fixes, fixes into retraining triggers, and retraining into verified improvements? This chapter covers eval governance, audit trails, versioning, the failure modes that break silently, and the closed-loop remediation systems that turn your eval infrastructure into a self-improving quality engine.

---

- 11.1 — Eval Governance: Who Owns What
- 11.2 — Audit Trails for Automated Decisions
- 11.3 — Reproducibility Requirements for Regulated Industries
- 11.4 — Eval Versioning and Change Control
- 11.5 — The Failure Mode Catalog: What Breaks Silently
- 11.6 — Overfitting to Benchmarks
- 11.7 — Prompt Leakage into Evals
- 11.8 — Single Point of Failure: One Judge for Everything
- 11.9 — Debugging Eval Failures in Production
- 11.10 — Closed-Loop Remediation: From Eval Failure to Fix
- 11.11 — Retraining Triggers and Regression Gates
- 11.12 — Building Eval Systems That Improve Themselves

---

*The eval system that catches a failure is good. The eval system that automatically triggers the fix is better. The eval system that prevents the failure from recurring is best.*
