# 8.2 — Trigger Points: When Evals Run

The pipeline kicks off at 2:47 AM. A developer merged a pull request that changed three lines in a system prompt. The commit triggered a webhook. The webhook queued a job. The job spun up a container, pulled the latest prompt configuration, loaded the tier-one eval dataset, ran four hundred test cases through the new prompt, scored the outputs, and wrote results to the metrics database. By 3:02 AM, the pipeline had completed. The aggregate quality score dropped from ninety-one percent to eighty-six percent. The Slack integration posted a warning to the team channel. When the developer woke up six hours later, they saw the failure, reviewed the outputs, realized the prompt change broke handling of ambiguous requests, reverted the merge, and opened a new pull request with a fix. The regression never reached production. The user never saw degraded outputs. The eval pipeline did its job because it ran at the right moment — immediately after the change, before deployment, with enough time to fix the problem.

Choosing when evaluations run is as important as choosing what to evaluate. Run too early and you catch nothing — the system has not changed yet, there is no regression to detect. Run too late and you catch everything after it reaches users — the damage is done, the trust is lost, the support tickets are filed. Run too often and you waste compute on redundant checks. Run too infrequently and regressions slip through the gaps. The right trigger strategy depends on what you are evaluating, how fast your system changes, and what failure modes you are trying to prevent.

## Commit-Triggered Evals

Every commit to the main branch is a potential source of regression. A prompt change. A retrieval query adjustment. A new tool added to an agent's schema. A parameter tweak in a fine-tuned model. Some changes are intentional improvements. Some are mistakes. Some are improvements in one context and regressions in another. The only way to know is to run evaluations immediately after the commit and compare results to the baseline.

Commit-triggered evals run automatically when code is pushed to the repository. The CI system detects the commit, identifies which components changed, and triggers the relevant eval pipelines. If the commit modified a summarization prompt, the pipeline runs the summarization eval suite. If the commit changed retrieval logic, the pipeline runs the retrieval eval suite. If the commit touched shared infrastructure, the pipeline runs the full regression suite. The scope of evaluation matches the scope of change.

The challenge is speed. Developers expect commit-time feedback in minutes, not hours. If your full eval suite takes three hours to run, developers will not wait for results before moving to the next task. The solution is tiering. Your commit-triggered pipeline runs a fast smoke test suite that covers the most common failure modes — correctness on golden examples, safety violations, formatting errors, critical edge cases. This suite completes in ten to twenty minutes. If it passes, the commit is considered safe for merge. A slower, more comprehensive eval suite runs on a schedule overnight and catches deeper regressions that the smoke test missed. The smoke test optimizes for speed. The nightly suite optimizes for coverage. Both are necessary.

The decision rule is simple: if the smoke test fails, the commit is reverted or fixed immediately. If the nightly suite fails, the team investigates the next morning. This creates fast feedback loops without blocking development on exhaustive evaluation. The trade-off is that some regressions will make it through the smoke test and only be caught by the nightly run. That is acceptable. The goal is not to catch every regression within ten minutes. The goal is to catch the regressions that matter most while keeping iteration speed high.

## Pull Request-Triggered Evals

Pull request evals run before code is merged, not after. A developer opens a PR that changes a prompt, adjusts a retrieval parameter, or modifies a tool definition. The PR triggers an eval pipeline. The pipeline runs the relevant test suite and posts results as a comment on the PR. The reviewer sees the eval results alongside the code diff. If quality dropped, the reviewer asks why. If quality improved, the PR is approved and merged. The eval becomes part of the code review process.

This shifts quality checks earlier in the development cycle. Instead of discovering regressions after merge, you discover them during review. Instead of reverting commits that degrade quality, you reject PRs that degrade quality. The feedback loop tightens. The cost of fixing a problem is lower — the developer is still in the context of the change, the code has not shipped, the revert does not disrupt other work in progress.

The challenge is noise. Not every PR needs full eval coverage. If a developer is fixing a typo in a comment, running a three-hundred-case eval suite is overkill. If a developer is refactoring internal logic without touching prompts or model calls, eval results are not relevant. Your pipeline must be smart enough to scope evaluation to the change. One approach is path-based triggers — if the PR modifies files in the prompts directory, run the prompt eval suite. If the PR modifies files in the retrieval directory, run the retrieval eval suite. If the PR touches neither, skip evals. Another approach is explicit annotation — the developer adds a label to the PR indicating which evals to run, or a comment instructing the bot to run specific test suites.

The key is making eval results actionable. A PR comment that says "quality score: 87%" is not actionable. A PR comment that says "quality score dropped from 91% to 87%, driven by failures in ambiguous request handling — see cases 14, 27, 38 for examples" is actionable. The reviewer can inspect those cases, decide whether the regression is acceptable, and make an informed merge decision. The eval becomes a tool for better code review, not a gate that blindly blocks merges.

## Scheduled Evals

Some evaluations should run whether or not code changes. A nightly eval suite that runs the full regression test set against the current production configuration. A weekly eval that measures quality against a static benchmark dataset to detect drift over time. A monthly eval that re-runs historical test cases to ensure that improvements from six months ago have not regressed. These are scheduled evals — they run on a timer, not in response to a trigger event.

Scheduled evals catch the failures that commit-triggered evals miss. Model behavior drifts due to upstream changes in a hosted API. A third-party data source changes format, breaking retrieval. A fine-tuned model degrades over time as the training data becomes stale. None of these are triggered by a code commit, but all of them cause production regressions. Scheduled evals detect these silent failures by continuously measuring quality against a stable baseline.

The frequency of scheduled evals depends on how fast your system changes and how much drift you can tolerate. If you are using a hosted model like GPT-5 that receives frequent updates from the provider, you might run full eval suites daily to catch behavior changes early. If you are using a static fine-tuned model deployed on your own infrastructure, weekly evals may be sufficient. If you are measuring long-term quality trends, monthly evals provide enough signal without generating noise.

Scheduled evals also serve as historical record. Every night, the pipeline runs the same test suite against the same dataset. Results are written to a time-series database. You can query this database to see how quality has changed over the past three months, six months, a year. You can correlate quality drops with specific deploys, model updates, or dataset changes. You can track whether the improvements you shipped in Q3 are still holding in Q1 of the following year. This longitudinal view is impossible with commit-triggered evals alone — those only tell you whether the latest change caused a regression, not whether the system as a whole is improving or degrading over time.

## Production-Triggered Evals

Some evaluations should run in response to production events. A user reports that the chatbot gave an incorrect answer. The support team flags the conversation ID. An automated job pulls the conversation from logs, reruns the same input through the current prompt configuration, scores the output, and checks whether the issue reproduces. If it does, the case is added to the eval dataset and a regression test is created. The next time someone changes the prompt, the pipeline will check that the same failure does not recur. This is production-triggered evaluation — turning live failures into automated regression tests.

Another production trigger is anomaly detection. Your monitoring system detects that response latency spiked by forty percent in the past hour. The spike could be caused by increased load, infrastructure issues, or a recent deployment. A production-triggered eval runs a subset of test cases through the live system to check whether quality degraded alongside latency. If quality is stable, the issue is infrastructure. If quality dropped, the issue is in the model or prompt logic. The eval disambiguates between infrastructure problems and AI problems, allowing the on-call engineer to route the incident correctly.

Production-triggered evals are reactive, not preventive. They do not catch regressions before deployment — they diagnose regressions after deployment. But they are still valuable because they close the loop between production failures and test coverage. Every production failure that triggers an eval becomes a test case. Every test case that gets added to the eval dataset reduces the chance of the same failure recurring. Over time, your eval dataset becomes a living record of everything that has ever gone wrong in production, and your pipeline ensures that none of those failures happen again.

## Manual Triggers

Not every eval run is automated. Sometimes a researcher wants to test a new prompt variant against the full eval suite before opening a PR. Sometimes a product manager wants to compare quality across three different model configurations to inform a trade-off decision. Sometimes an engineer is debugging a regression and wants to run a single test case with verbose output to understand what the model is doing. These are manual eval triggers — initiated by a human, executed by the pipeline.

Manual triggers use the same infrastructure as automated triggers. The researcher runs a CLI command, specifies the prompt variant and eval suite, and the pipeline executes the same way it would for a commit. The difference is that the results are returned to the researcher's terminal instead of posted to a PR or written to a dashboard. The researcher can iterate quickly — run an eval, review failures, adjust the prompt, run the eval again. The pipeline provides the compute and consistency. The researcher provides the judgment and creativity.

Manual triggers also enable ad hoc experimentation. A team wants to evaluate a new model that just launched — Gemini 3 Flash, Llama 4 Scout, a new version of Claude. They configure the pipeline to swap in the new model, run the full eval suite, and compare results to the baseline. The pipeline abstracts away the mechanics of running hundreds of test cases, scoring outputs, and aggregating results. The team focuses on interpreting the data and making the decision — does the new model improve quality enough to justify the switch, or does it introduce regressions that outweigh the gains?

The infrastructure that supports manual triggers is the same infrastructure that supports automated triggers. The same datasets, the same scoring functions, the same result storage. The only difference is the entry point. This is critical — if manual and automated evals use different code paths, they will drift. A researcher will iterate on a prompt variant using a manual eval, get good results, open a PR, and then watch the automated eval fail because it was running a different version of the scoring function. Unifying the infrastructure ensures that what you test manually is what gets tested automatically, and that improvements validated in experimentation hold up in production.

## Event-Driven Eval Architecture

The most flexible trigger strategy is event-driven. Your system emits events — code committed, PR opened, deploy completed, user complaint filed, latency anomaly detected. Each event type maps to one or more eval pipelines. The event is published to a queue. A consumer service reads events from the queue, determines which pipelines to trigger, and executes them. Results are written back to a central store where they can be queried by dashboards, alerting systems, or incident response tools.

This architecture decouples event sources from eval execution. A new event type can be added without modifying the eval pipeline code. A new eval pipeline can be added without modifying the event publishing logic. The system scales horizontally — multiple workers consume events from the queue in parallel, each executing a different eval job. If the queue backs up, you add more workers. If a worker fails, the event is retried. The infrastructure behaves like any other distributed system, with the same fault tolerance, observability, and scaling patterns.

Event-driven architecture also enables complex workflows. A code commit triggers a smoke test eval. If the smoke test passes, a deployment event is published. The deployment event triggers a production-triggered eval that runs a subset of test cases against the live system. If the production eval passes, a success event is published. If it fails, an alert event is published and the on-call engineer is paged. Each event triggers the next step in the workflow, and the eval pipeline becomes a sequence of quality gates that code must pass through on the way to production.

The investment required for event-driven architecture is higher than for simple scheduled or commit-triggered evals. You need an event queue, worker infrastructure, retry logic, observability, and coordination between services. But the return is flexibility. You can compose eval pipelines into workflows. You can add new trigger points without rewriting code. You can scale eval execution independently from event publishing. And you can treat evaluation as a first-class part of your development and deployment process, not as a separate manual step that happens when someone remembers. The next question is how those evaluations execute — whether they process examples in batch, stream results in real time, or run on demand when triggered.

