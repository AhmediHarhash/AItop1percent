# 11.2 â€” Audit Trails for Automated Decisions

In October 2025, a European fintech company received a formal inquiry from their national data protection authority. The regulator wanted to understand the safeguards in place for their customer service AI, which had been flagged after a user complaint escalated to a political office. The company's head of AI explained that every model deployment passed through a rigorous automated eval pipeline with safety checks, performance thresholds, and regression gates. The regulator asked to see the records. The team had eval dashboards showing current performance. They had code in GitHub showing the pipeline configuration. What they did not have was a timestamped, immutable record of what happened on the date the disputed model was deployed. They could not prove which eval suite ran, what the scores were, or whether the thresholds in place that day were the same as today's thresholds. The investigation expanded. Legal fees reached six figures. The company could not demonstrate compliance because they had built an eval system without an audit trail.

An automated eval pipeline makes decisions with real consequences. It blocks a deployment because safety scores fell below threshold. It approves a release because regression tests passed. It triggers an alert because prompt injection detection crossed a boundary. Every one of these decisions must be provable after the fact. When a regulator asks what happened, when legal needs evidence for a lawsuit, when an incident post-mortem requires reconstructing exactly what the system did on a specific date, the audit trail is your only defense. Without it, you cannot prove what your system decided or why it made that decision.

## What Must Be Logged

Every automated eval decision requires a complete record. The minimum set includes the model version tested, the eval suite version that ran, every input and output pair evaluated, every metric computed, every threshold applied, and the final decision made. You also need the timestamp of when the eval started and when it completed, the identity of who or what triggered the evaluation, and the configuration state of the pipeline at that moment.

Model version means more than a version number. It includes the base model identifier, any fine-tuning run ID, the training data hash if applicable, and the deployment artifact checksum. If your system evaluates ten different model candidates in a day, the audit log must distinguish all ten. If someone asks which specific model was deployed to production on November 3rd at 14:22 UTC, you must be able to answer definitively.

Eval suite version means the exact set of tests that ran. If your eval suite contains 4,000 examples split across safety, performance, and regression categories, and someone updates one of those examples, that is a new version. The audit log records which version ran. You cannot compare November's results to October's results unless you know whether the test set changed between those dates.

Every metric computed must be logged with full precision. Not just pass or fail. Not just a summary score. The raw metric value, the threshold it was compared against, and the comparison result. If your pipeline calculates accuracy, precision, recall, F1, toxicity score, prompt injection rate, and latency at the 95th percentile, all seven metrics go into the log. If the threshold for toxicity was 0.03 and the observed score was 0.041, both numbers appear in the record. A year from now, when someone asks why a deployment was blocked, the answer is in the log.

The decision itself must be explicit. Passed. Failed. Blocked. Approved with warning. Escalated for manual review. The log does not just show data and let someone infer the outcome. It states the outcome as a structured field that can be queried.

## Immutability Requirements

An audit trail that can be modified after the fact is not an audit trail. It is a liability. Logs must be immutable from the moment they are written. If a deployment decision was recorded at 10:15 AM, no one should be able to edit that record at 10:20 AM. If a regulator or auditor sees a log entry timestamped for March, they must trust that the entry reflects what actually happened in March, not what someone wishes had happened.

Immutability starts with append-only storage. Once a log entry is written, it is never updated or deleted. If an eval runs a second time for the same model, that creates a second log entry. If a threshold changes, the new threshold applies to future evals, but past eval records remain unaltered. This sounds simple but requires enforcement. Your logging system must reject any write operation that attempts to modify an existing record.

Cryptographic signing provides proof that logs have not been tampered with. Each log entry can include a hash of its contents, and each batch of entries can be signed with a private key controlled by the eval infrastructure. If someone later questions the authenticity of a log, you can verify the signature. This is especially important in regulated industries where audit logs may be submitted as evidence in legal or regulatory proceedings.

Write-once storage backends make immutability easier. Cloud object storage with versioning and write-protection enabled, blockchain-based logging systems, or dedicated audit log services all provide technical guarantees that data cannot be altered. The choice depends on your compliance requirements, your data residency constraints, and your budget. The principle is the same: once written, the log is locked.

## Retention Policies

Audit logs must be retained long enough to satisfy regulatory requirements, support incident investigations, and allow for longitudinal analysis of eval system performance. The minimum retention period is determined by the strictest regulation your system is subject to. GDPR allows for logs to be retained as long as necessary for compliance and legal defense, but requires deletion once that purpose no longer applies. HIPAA requires audit logs for at least six years. The EU AI Act's Article 12 requires high-risk AI systems to maintain logs that enable post-market monitoring and investigation of incidents, with retention periods tied to the system's operational lifespan.

In practice, most teams retain eval audit logs for at least two years, often longer. Two years covers most regulatory lookback windows, allows you to analyze year-over-year trends, and provides enough history to reconstruct incidents that were not immediately identified. If you deploy a model in January 2025 and discover a problem in September 2026, you need the eval logs from January 2025 to understand what your system approved and why.

Retention policies must balance storage cost with evidentiary value. Eval logs are not usually large compared to training data or production inference logs, but they accumulate. A company running 50 model evaluations per week with 10 megabytes of log data per eval will generate 26 gigabytes per year. Over five years, that is 130 gigabytes. Compressed and stored in cold object storage, this is inexpensive. The cost of not having the data when you need it is vastly higher.

Deletion policies require care. When a retention period expires, logs should be deleted automatically to comply with data minimization principles under GDPR and similar regulations. But the deletion process itself must be logged. If a regulator asks whether you retained data longer than permitted, you need to prove that you did not. The audit log of the audit log deletion is part of the compliance story.

## Queryability

An audit log that exists but cannot be searched is useless. When legal asks for all eval decisions related to a specific model version, you must be able to retrieve those records in minutes, not days. When a regulator requests proof that a particular eval suite was applied to all deployments in a six-month window, you must be able to generate that report without writing custom scripts.

Queryability requires structured logging. Every log entry follows a schema. Model version is a field. Eval suite version is a field. Decision is a field. Timestamp is a field. You can filter by any field or combination of fields. If your logging system dumps JSON blobs into files without indexing, you will spend hours with grep and jq trying to answer simple questions. If your logging system writes to a database with indexes on model version, eval type, decision, and timestamp, you answer those questions in seconds.

Common queries must be fast. Show me all failed evals in the last 30 days. Show me all evals for model version 2024-11-03.v7. Show me all deployments that passed safety checks but had latency warnings. Show me every eval decision made by user Alice versus the automated scheduler. These queries happen during incident response, during audits, and during routine monitoring. If they take ten minutes to run, they will not be run often enough.

Retention and queryability interact. If you keep two years of audit logs in a hot database, query performance may degrade as the table grows. If you archive logs older than six months to cold storage, queries spanning a year require searching both hot and cold data. A common pattern is to keep 90 days in a fast queryable system, keep two years in a data warehouse with slower but still reasonable query performance, and keep anything older in archival storage that can be restored if needed but is not queried regularly.

## Chain of Custody

Audit trails must record not just what the eval system did, but also who changed the eval system. If a threshold is adjusted, that adjustment is a logged event. If an eval suite is updated, that update is a logged event. If a new rule is added to the automated decision logic, that change is a logged event. The chain of custody shows who made each change, when, and ideally why.

This prevents silent erosion of safeguards. If a safety threshold starts at 0.02 and someone raises it to 0.05 to reduce false positives, that change might be reasonable. If the threshold drifts upward to 0.10 over six months through a series of small adjustments by different people, the cumulative effect is that you are now tolerating five times the original risk. The audit log makes this visible. You can query threshold changes over time and see the trend.

Access control and audit trails reinforce each other. Only specific roles can modify eval configurations. Every modification is logged with the identity of the person who made it. If an incident occurs, you can trace back through the chain of changes to understand what was different. If a deployment failed because a threshold was temporarily disabled, the audit log shows who disabled it, when, and whether it was re-enabled.

The most mature teams treat eval configuration changes like production code changes. A threshold adjustment goes through a pull request. The pull request links to a ticket explaining why the change is needed. The merge is logged. The deployment of the new configuration is logged. The first eval run under the new configuration is logged. Six months later, when someone asks why the threshold is 0.04 instead of 0.02, the answer is in the log, in the pull request, and in the ticket.

## Compliance Requirements

GDPR Article 30 requires organizations to maintain records of processing activities, including automated decision-making. If your eval pipeline decides whether a model is safe to deploy, that is a processing activity. The audit log is part of your Article 30 record. GDPR also requires the ability to demonstrate compliance. When a data protection authority audits you, they will ask how you ensure AI systems meet your stated policies. The audit log is your proof.

HIPAA requires covered entities to log all access to protected health information and to retain those logs for six years. If your eval suite includes test cases with synthetic patient data that resembles real patient data closely enough to be considered PHI, those eval runs must be logged. If your model processes real PHI during evaluation, every interaction is a logged event. Audit logs must include who accessed the data, when, and for what purpose.

The EU AI Act Article 12 imposes specific logging requirements on high-risk AI systems. Systems must automatically log events throughout their operation, including the period of use, the database against which input data was checked, any relevant information about decisions made, and any human oversight actions. For an automated eval pipeline, this means logging not just pass or fail, but the reference data used for comparison, the human review events if a deployment was escalated, and the final deployment decision even if it happened outside the eval system.

Article 12 also requires that logs enable the traceability needed for post-market monitoring and investigation of serious incidents. If a model causes harm in production, regulators will want to know what eval processes it passed, what scores it achieved, and whether any warning signals were missed. The audit log must contain enough detail to reconstruct the eval history of that specific model version.

Compliance requirements vary by industry, by geography, and by the risk tier of your AI system. A Tier 1 customer service chatbot may have minimal audit requirements. A Tier 3 medical diagnosis assistant in the EU must meet Article 12 in full. Your logging architecture must be flexible enough to meet the strictest requirements you are subject to, because retrofitting audit logging is far harder than building it from the start.

## The Audit Trail for Threshold Changes

Thresholds define what passes and what fails. If your safety threshold is 0.03 and a model scores 0.029, it passes. If the threshold were 0.02, it would fail. Thresholds are policy decisions encoded as numbers. Every threshold change must be logged, justified, and traceable.

The log entry for a threshold change includes the old value, the new value, the timestamp of the change, the identity of the person who made the change, and a reference to the justification. The justification might be a ticket ID, a pull request link, or a reference to a policy document. If the threshold change was part of an incident response, the log entry links to the incident. If the change was part of routine calibration based on new data, the log entry links to the calibration analysis.

Threshold changes ripple through future eval decisions. If you lower a safety threshold, more models will be blocked. If you raise a performance threshold, fewer models will pass. The audit log allows you to analyze the impact. You can query all evals that ran before the threshold change and compare them to all evals that ran after. You can identify models that would have passed under the old threshold but fail under the new one. You can assess whether the change achieved its intended effect.

In regulated environments, threshold changes may require approval before they take effect. The approval workflow itself is logged. A proposed threshold change is submitted. Reviewers approve or reject it. If approved, the change is deployed and takes effect on a specific date. The audit log captures the entire workflow, not just the final state.

## Forensic Reconstruction

When something goes wrong, the audit trail allows you to reconstruct exactly what happened. A model was deployed on Thursday. By Monday, users are reporting unexpected behavior. The incident response team needs to know: what eval suite ran before that deployment, what were the scores, what thresholds were in place, and what decision logic was applied.

Forensic reconstruction requires logs that are complete, accurate, and timestamped with precision. If the deployment happened at 09:47 UTC, you need the eval log from the hours before 09:47 UTC. If multiple evals ran that morning, you need to identify which one was associated with the deployment. If the deployment process pulled the eval result from a cache, you need to know when that cache entry was written and what data it contains.

The most thorough teams log not just eval results but also the entire eval execution trace. Which eval tasks ran in which order. How long each task took. Whether any tasks were retried. Whether any tasks failed but were marked as non-blocking. This level of detail is not necessary for every eval decision, but for high-risk systems or for systems subject to strict regulatory oversight, it provides the evidence needed to close an investigation.

Reconstruction also requires the ability to correlate eval logs with other system logs. The eval pipeline logged a decision at 09:42 UTC. The deployment system logged a release at 09:47 UTC. The production monitoring system logged the first user queries at 10:03 UTC. You need to tie these three events together to understand the causal chain. Shared request IDs, shared model version identifiers, or a centralized event log make correlation possible.

## Storage and Security

Audit logs are sensitive data. They reveal what models were tested, what data was used for testing, what thresholds were applied, and what decisions were made. If an attacker can modify audit logs, they can hide evidence of a compromised eval system. If an attacker can delete audit logs, they can make incidents untraceable. If an attacker can read audit logs, they may gain insight into your eval criteria and design attacks that exploit gaps.

Storage must enforce immutability, as discussed earlier. It must also enforce access control. Only specific systems and specific roles can write to the audit log. Only specific roles can read the audit log. General developers do not need access to audit logs for day-to-day work. Security teams, compliance teams, and incident responders need read access. The eval pipeline itself needs write access, ideally through a service account with minimal permissions.

Encryption protects audit logs at rest and in transit. Logs should be encrypted in storage, whether that storage is a database, an object store, or a dedicated logging service. Logs should be transmitted over encrypted channels. If logs are exported for analysis or submitted to a regulator, they should be encrypted during transfer and decrypted only by authorized recipients.

Tamper detection mechanisms provide an additional layer of assurance. Cryptographic hashing of log batches, blockchain-based append-only logs, or third-party audit log services that provide independent verification all make it harder to alter logs without detection. These mechanisms are especially valuable in adversarial contexts, where you must assume that an attacker with elevated privileges might attempt to cover their tracks.

Backup and disaster recovery apply to audit logs just as they apply to production data. If your primary logging system fails, you must not lose the audit trail. If a datacenter outage wipes out your logs, you must be able to restore them. The same rigor you apply to protecting customer data applies to protecting audit data. In some regulatory contexts, the audit log is more critical than the data it describes, because the audit log is the evidence of compliance.

The audit trail is not optional. It is the proof that your eval system did what you claim it did. It is the evidence you present to regulators, to auditors, and to internal stakeholders. It is the foundation of your ability to investigate incidents, demonstrate compliance, and continuously improve your eval processes. Every automated decision your eval pipeline makes must leave a permanent, immutable, queryable record. Next, we turn to the reproducibility requirements that make audit trails useful in regulated industries, where being able to prove what happened also means being able to recreate the exact conditions under which it happened.
