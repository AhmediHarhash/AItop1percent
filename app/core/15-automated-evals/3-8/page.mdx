# 3.8 — Combining Heuristics into Anomaly Scores

A single heuristic is a weak signal. Length flagging catches some errors. Sentiment checking catches others. Format validation catches a third class. None of them alone is reliable enough to block a production output with confidence. But when you combine five heuristics and four of them flag the same response, you have a strong signal. The output is an anomaly. The redundancy principle in evaluation systems states that multiple independent weak signals, when they agree, produce a detection confidence that exceeds any single strong signal. This is why production eval pipelines don't rely on one heuristic or even one evaluation method. They layer heuristics into composite anomaly scores that isolate genuinely bad outputs while letting normal variance pass through.

## Weighted Scoring Across Heuristic Dimensions

The first step in combining heuristics is defining a scoring function that aggregates signals without losing information. Each heuristic produces a score: length deviation from mean, PII pattern match count, sentiment classification, formality score, refusal pattern presence, hallucination marker count. Some heuristics output binary flags. Others output continuous scores. You need to normalize them into a common scale. The standard approach is converting each heuristic output to a zero-to-one anomaly probability. A response that's within normal length range scores 0.0 on length anomaly. A response that's three standard deviations outside normal length scores 1.0. A response with no PII matches scores 0.0. A response with four PII matches scores 1.0.

Once normalized, you weight each heuristic by its precision and impact. Not all heuristics are equally trustworthy. PII detection with high-confidence regex patterns gets a higher weight than sentiment classification, which has a twenty-percent error rate. Not all failures are equally severe. A tone mismatch is less critical than a PII leak. Your weight vector reflects both reliability and business impact. A typical weight distribution for a customer support system might look like: PII detection at 0.25, length anomaly at 0.10, refusal pattern at 0.15, sentiment mismatch at 0.10, formality deviation at 0.05, hallucination markers at 0.20, repetition score at 0.10, special character ratio at 0.05. The weights sum to one. You compute the composite anomaly score as the weighted sum of normalized heuristic outputs. A response scores 0.72 on the composite scale. That number alone doesn't mean much until you threshold it.

## Anomaly Detection Through Composite Thresholds

Thresholding a composite anomaly score requires calibration data. You need a labeled set of outputs marked "safe to deploy" or "needs escalation." You sweep the threshold from 0.0 to 1.0, calculate precision and recall at each point, and choose the threshold that meets your operating requirements. If you're optimizing for precision, you set a high threshold. Only outputs scoring above 0.80 get flagged. This minimizes false positives but lets some bad outputs through. If you're optimizing for recall, you set a low threshold. Outputs scoring above 0.40 get flagged. This catches most bad outputs but floods your review queue with false positives.

Most production systems operate in the middle. A threshold of 0.60 to 0.70 typically balances precision and recall for general-purpose applications. But the optimal threshold varies by use case, model, and risk tolerance. A legal document generator sets the threshold at 0.45, flagging anything with even moderate anomaly signals. A creative writing assistant sets the threshold at 0.85, only flagging extreme outliers. The threshold is not static. As your model changes, as your prompt evolves, as your user base shifts, the distribution of anomaly scores changes. You recalibrate thresholds monthly or after every major model update. The calibration process is simple: take the last thousand flagged outputs, have humans label them, measure precision and recall at your current threshold, adjust if performance degraded.

## The Redundancy Principle: Multiple Weak Signals Equal Strong Signal

The mathematical foundation of composite scoring is ensemble reliability. A heuristic with seventy-percent accuracy is too noisy to trust alone. But five independent heuristics, each with seventy-percent accuracy, agreeing on the same output, produce a combined confidence above ninety-five percent. The independence requirement is critical. If your heuristics all measure variants of the same underlying property, they're not independent. Length anomaly and repetition score both correlate with generation errors, so they're partially redundant. Sentiment score and formality score measure different properties, so they're more independent. You want your heuristic set to cover orthogonal dimensions of output quality.

The redundancy principle explains why heuristic pipelines scale better than single-method eval. Adding a sixth heuristic to your composite score costs almost nothing in latency because heuristics are parallel and fast. Adding a second model-based evaluator doubles your latency and cost. The incremental value of each additional heuristic follows diminishing returns, but the first five to seven heuristics provide substantial lift. A team running only length and PII checks catches forty percent of bad outputs. Adding sentiment, refusal patterns, and hallucination markers brings coverage to seventy-five percent. Adding formality, repetition, and special character checks brings coverage to eighty-five percent. The last fifteen percent of bad outputs require model-based or human eval to catch, but you've triaged the bulk of failures with heuristics alone.

## The Anomaly Envelope: Setting Multi-Dimensional Boundaries

Named pattern: the Anomaly Envelope. Instead of a single composite score, you define acceptable ranges for each heuristic independently, then flag outputs that fall outside the envelope on multiple dimensions simultaneously. A response can score poorly on sentiment but still pass if it's within bounds on all other heuristics. A response that scores poorly on sentiment and length and refusal patterns is outside the envelope. The envelope approach preserves interpretability. When you flag an output with a composite score of 0.74, the engineer reviewing it doesn't know which heuristics fired. When you flag an output for "sentiment anomaly, length anomaly, hallucination markers present," the engineer knows exactly what to look for.

The envelope is defined by per-heuristic thresholds rather than a global threshold. For each heuristic, you set a safe range. Length: 50 to 800 characters. Sentiment: formality score between 40 and 90. PII matches: zero. Refusal pattern: absent unless user query was a policy violation. Hallucination markers: fewer than two. Repetition score: below 0.30. Any output within bounds on all heuristics passes. Any output violating two or more bounds enters escalation. The two-violation rule prevents single-heuristic false positives from flooding the queue while still catching genuine multi-dimensional anomalies. A response that's slightly too long but perfect on all other dimensions passes. A response that's too long and has high repetition and shows hallucination markers fails.

## Heuristic Disagreement as Escalation Trigger

Occasionally, heuristics contradict each other in ways that signal deeper problems. A response scores high on formality but also high on emotional register. Formal language is typically low-emotion, so this combination is unusual. A response has zero PII matches but also contains placeholder-like text patterns that suggest the model tried to insert PII and failed, leaving artifacts. A response passes sentiment and tone checks but has extreme length deviation. These disagreement patterns often indicate edge cases that heuristics weren't designed to handle. The model did something unexpected.

You encode disagreement rules as meta-heuristics. If formality score is above 80 and emotional register is above 70, flag for human review. If PII score is zero but placeholder pattern score is above 0.50, flag. If sentiment is neutral and length is greater than two standard deviations, flag. Disagreement flags enter a separate queue from standard anomaly flags. They're rarer, typically two to five percent of total output volume, but they have higher diagnostic value. When you review disagreement cases, you often discover prompt bugs, model behavioral quirks, or edge cases in your task definition. A team reviewing disagreement flags in their legal contract generator found that the model would sometimes produce formally correct but semantically meaningless outputs—high formality, low coherence, a combination the heuristics weren't designed to catch. They added a coherence heuristic to the composite score and closed the gap.

## Tuning Composite Scores for Evolving Models

The composite anomaly score that worked perfectly for Claude Opus 4 in November 2025 degrades when you switch to GPT-5.1 in January 2026. Different models have different failure modes, different baseline distributions, different stylistic tendencies. GPT-5.1 generates longer responses on average than Claude Opus 4. Your length anomaly heuristic, tuned to Claude's distribution, flags half of GPT-5.1's outputs as anomalies even though they're fine. The formality distribution shifts. The refusal pattern language changes. Your heuristic weights and thresholds need retuning.

The tuning process is semi-automated. When you swap models, you run the first thousand outputs through your heuristic pipeline without blocking any, just logging scores. You compare the score distribution to your historical baseline. If the median composite score shifted from 0.30 to 0.55, your heuristics are miscalibrated. You re-run threshold calibration on a labeled sample from the new model. You adjust heuristic weights if certain dimensions became more or less predictive. You update the envelope bounds to match the new model's normal operating range. The recalibration takes hours, not weeks, because you're adjusting parameters, not retraining models. This is why heuristics remain valuable even as models evolve rapidly. They degrade gracefully and recalibrate cheaply.

Combining heuristics into composite anomaly scores turns a collection of noisy, unreliable signals into a detection system that catches seventy to eighty-five percent of production failures at near-zero cost and single-digit millisecond latency. But composite scores are only as good as their calibration, and calibration is only as good as the human judgments you use to set thresholds. Next, you need to understand how to calibrate heuristics against human judgments and maintain that calibration as your system evolves.

