# 5.12 â€” When References Are Wrong: Stale Ground Truth

The Stale Ground Truth anti-pattern is silent, pervasive, and corrosive. Your eval suite passes at 0.92. Your model deploys to production. User complaints spike within three days. The model outputs accurate information, but it contradicts the current documentation, the updated policy, and the newly launched feature set. Your ground truth references were written eight months ago. The product changed. The business logic changed. The external data changed. The references did not. Your eval measured how well the model matched outdated expectations. It rewarded the model for being wrong in exactly the way your old references were wrong. Stale ground truth does not cause eval failure. It causes eval success on models that fail in production.

Ground truth decays. Every reference that depends on external facts, business rules, product behavior, or real-world state has a shelf life. Some references expire in days. Some expire in months. A few remain valid for years. The eval pipeline that assumes references are permanent is the pipeline that silently drifts out of alignment with production. Stale ground truth is not a data quality problem you fix once. It is an operational problem you manage continuously. If your eval system does not include reference staleness detection, reference refresh cadence, and invalidation workflows, your eval scores will eventually measure the wrong thing. You will optimize models against outdated expectations. You will deploy systems that pass every eval and fail every user.

## How Ground Truth Becomes Stale

References become stale when the world they describe changes. Factual references expire when facts update. A reference answer says the current president is X. The presidency changes. The reference is now wrong. A reference says product Y costs 49 dollars. Pricing changes to 59 dollars. The reference is outdated. A reference describes feature Z as unavailable. The feature launches. The reference is false. Every reference that encodes real-world state carries an expiration risk.

Policy-dependent references expire when policies change. A customer support reference says refunds are available within 30 days. The policy updates to 60 days. The reference under-promises. A compliance reference says data must be deleted within 90 days. The regulation changes to 30 days. The reference violates the new rule. A moderation reference flags content as acceptable under old community guidelines. The guidelines tighten. The reference is now misaligned. Policy changes propagate slowly through organizations. The legal team updates the policy. The product team updates the UI. The eval team's references remain unchanged. The model trains and evaluates against the old policy. It deploys with outdated behavior.

Code-dependent references expire when APIs, tools, or system behavior changes. A reference expects a tool to return a JSON response with five fields. The tool updates to six fields. The reference is incomplete. A reference expects an API call to succeed with certain parameters. The API deprecates those parameters. The reference describes invalid usage. A reference expects a database query to return results in a specific order. The database schema changes, and the default sort order changes. The reference expects the wrong order. Every reference that depends on system internals is vulnerable to code changes. If your eval references are tightly coupled to implementation details, every deployment can invalidate them.

Domain knowledge references expire when scientific, medical, legal, or technical knowledge updates. A medical reference says treatment A is first-line for condition B. New research establishes treatment C as superior. The reference is obsolete. A legal reference cites a regulation that was amended. The reference is out of date. A technical reference describes best practices that have been superseded. The reference teaches deprecated patterns. Domain knowledge evolves. If your ground truth is domain-specific, it requires domain expert review on a regular cadence. Experts know when knowledge changes. Annotators without domain expertise do not.

Temporal references expire when time passes. A reference says "the most recent model is GPT-5, released in May 2025." By January 2026, GPT-5.2 has been released. The reference is stale. A reference says "the current quarter ends in March 2025." That quarter ended. The reference is nonsensical. A reference includes a date-stamped URL that returns a 404 after the content expires. The reference is broken. Any reference that includes time-specific information has a built-in expiration date. The eval system must either refresh these references automatically or flag them for review when the timestamp becomes outdated.

## Detecting Outdated References

Stale references do not announce themselves. They look identical to fresh references. The file format is the same. The schema is the same. The only difference is the content no longer matches reality. Detection requires external validation. You compare the reference to a source of truth outside the eval system. If they diverge, the reference is stale.

Automated staleness detection works for references derived from structured sources. If your references include product pricing, and pricing data lives in a product database, compare the reference to the database. If they match, the reference is current. If they diverge, the reference is stale. If your references describe API response schemas, compare the reference to the actual API spec. If the spec version has incremented, the reference may be outdated. If your references cite documentation, compare the reference to the current version of the docs. If the docs have been updated since the reference was created, flag the reference for review.

Automated detection requires traceability. Each reference must include metadata linking it to its source of truth. A reference derived from a pricing database includes the database name, table name, and the date the reference was generated. A reference derived from API documentation includes the API endpoint, the spec version, and the retrieval timestamp. A reference derived from a policy document includes the document name, version number, and the section or paragraph. With this metadata, the staleness check is straightforward: retrieve the current version of the source, compare to the reference, flag differences.

Manual staleness detection works for references that depend on human judgment or domain expertise. A medical reference says a drug is contraindicated for a condition. An expert reviews the reference quarterly. If new research has emerged, the expert flags the reference as stale. A legal reference cites a regulation. A compliance specialist reviews the reference twice per year. If the regulation has been amended, the specialist flags the reference. A content moderation reference reflects community norms. A trust and safety lead reviews the reference after major platform policy updates. Manual detection does not scale to tens of thousands of references. It scales to the high-value, high-risk references where staleness has the most severe consequences.

Implicit staleness detection monitors eval score drift. If a reference is stale, and the model is trained on current data, the model's output will diverge from the reference. Eval scores drop. If scores drop suddenly across a subset of test cases, investigate the references for those cases. The model might have regressed. Or the references might be stale, and the model is correctly reflecting updated reality. Compare the model output to external sources of truth. If the model is correct and the references are wrong, the references are stale. This approach detects staleness only after it has affected eval scores. It is reactive, not proactive. Use it as a backup when automated and manual detection miss something.

## The Staleness Audit

A staleness audit is a systematic review of every reference in the eval set. It asks: when was this reference created? What is its source of truth? Has the source changed since creation? Is the reference still valid? The audit produces a staleness report: how many references are current, how many are stale, how many are uncertain. The report drives refresh priorities.

Audits start with metadata review. For every reference, check the creation date. References older than six months are audit candidates. References older than one year are high-priority candidates. If a reference has no creation date, assume it is stale until proven otherwise. Undated references are technical debt. They accumulate when teams create references manually without enforcing metadata standards. The first pass of a staleness audit is often just adding creation dates to references that lack them.

The second pass checks source alignment. For references with documented sources, retrieve the current source and compare. For references without documented sources, attempt to infer the source. A reference about pricing likely came from a pricing page or database. A reference about a product feature likely came from product documentation or release notes. A reference about a regulation likely came from a legal database. Trace the reference backward to its likely origin. Compare the current state of that origin to the reference. If they match, mark the reference as current. If they diverge, mark it as stale. If the source cannot be determined, mark the reference as uncertain and escalate to a human reviewer.

The third pass evaluates references that depend on implicit knowledge. A customer support reference says "our standard response is X." There is no external source of truth. The response is based on team norms, historical practice, or annotator judgment. These references are the hardest to audit. The only validation is asking a domain expert: is this still correct? If the expert says yes, mark it current. If the expert says no, mark it stale. If the expert is unsure, mark it uncertain and flag for discussion.

Staleness audits are expensive. For a dataset with 10,000 references, a full audit might take 40 hours of human review. You cannot audit every reference every week. You can audit every reference once per year. You can audit high-risk references quarterly. You can audit low-risk references on demand when scores drift. The audit cadence depends on your domain's rate of change. A healthcare eval dataset in a rapidly evolving treatment area needs quarterly audits. A legal eval dataset tied to stable regulations needs annual audits. A product eval dataset for a fast-moving consumer app needs monthly audits.

## Reference Refresh Cadence

Refresh cadence defines how often you update references to match current reality. Cadence depends on the reference's source, its risk profile, and the cost of staleness. High-risk references refresh frequently. Low-risk references refresh on long intervals. No-risk references never refresh because they describe static facts.

Static references never expire. A reference that says two plus two equals four is valid forever. A reference that describes a historical event is valid as long as the event description is accurate. A reference that defines a mathematical concept is valid unless the definition itself is redefined. Static references are rare in AI evals. Most references depend on something that changes. But when they exist, they should be flagged as static to avoid wasting audit effort.

Slow-changing references refresh annually. A reference that describes fundamental product functionality. A reference that reflects long-standing company policy. A reference that cites stable regulations or standards. These references change, but slowly. Annual review is sufficient. If the reference is still correct, update the metadata with the new review date. If the reference is stale, update the content. If the reference is obsolete because the underlying concept no longer exists, delete it.

Moderate-changing references refresh quarterly. A reference tied to product features that evolve every release cycle. A reference tied to competitive positioning that shifts as competitors launch. A reference tied to market data that updates seasonally. Quarterly review catches most staleness before it causes eval drift. The review can be lightweight: a domain expert scans the reference, confirms it is still accurate, and updates the review timestamp. If the expert identifies staleness, they flag it for a detailed rewrite.

Fast-changing references refresh monthly or on-demand. A reference tied to real-time data, such as stock prices, weather, or news. A reference tied to a product in beta with weekly updates. A reference tied to a regulation under active amendment. Fast-changing references are high-maintenance. If you have hundreds of them, consider replacing them with dynamic references that query a live source of truth at eval time instead of storing a static snapshot. Alternatively, automate the refresh. A script pulls the latest data from the source, compares it to the reference, and updates the reference if it has changed.

Event-triggered refresh updates references when a known change occurs. A new product launches. A policy updates. An API version deprecates. A regulation takes effect. An event trigger signals the eval team to review all references related to the change. Event-triggered refresh is more efficient than time-based refresh because it targets only the affected references. It requires integration between the eval system and the change management process. Product teams, legal teams, and engineering teams must notify the eval team when changes occur. This integration often does not exist. Event-triggered refresh is the goal. Time-based refresh is the fallback.

## When to Invalidate Versus Update

A stale reference can be fixed or removed. Fixing means updating the reference content to match current reality. Removing means deleting the reference and its associated test case. The decision depends on whether the underlying test case is still relevant.

Update the reference when the test case is still valid but the expected output has changed. A product's price changed from 49 to 59 dollars. The test case "user asks for product price" is still valid. The reference answer updates from "49 dollars" to "59 dollars." The test case remains in the eval set. Only the reference changes. This is the common case. Most staleness is content drift, not concept obsolescence.

Invalidate the reference when the test case is no longer relevant. A test case asks the model to describe a feature that no longer exists. The feature was deprecated. There is no correct answer because there is no feature. The test case should be removed from the eval set. Keeping it would confuse the model and distort eval scores. Similarly, a test case asks about a policy that was fully replaced by a new policy. The old policy is not just outdated; it is irrelevant. The test case should be removed or replaced with a new test case about the new policy.

Invalidate the reference when the cost of updating exceeds the value. A test case has five references, all of which are stale. Updating all five requires expert review, rewriting, and validation. The test case covers an edge case that occurs in 0.1% of production traffic. The cost of refresh is high. The value is low. Remove the test case. Spend the refresh effort on high-value test cases. This is a harsh trade-off. It means your eval coverage shrinks. But it is better than maintaining references that consume resources without driving quality.

Replace the reference when the concept is still relevant but the specifics have changed significantly. A test case asks the model to recommend a product. The old product was discontinued. A new product serves the same use case. The reference should not be updated to name the new product. It should be replaced with a reference that reflects the current product portfolio. The test case might also need rephrasing to remove references to the old product name. This is more than a content update. It is a test case refresh.

## The Cost of Wrong Ground Truth

Stale ground truth does not break evals in obvious ways. Tests still run. Scores still compute. Reports still generate. The system looks healthy. The failure is silent. Models are penalized for correct outputs that diverge from outdated references. Models are rewarded for incorrect outputs that match outdated references. Optimization loops converge on the wrong target. Teams make decisions based on scores that measure alignment with the past, not alignment with the present.

The cost is wasted model improvement effort. A team fine-tunes a model to increase eval scores. The scores improve. The team deploys. Production quality does not improve, because the eval was measuring the wrong thing. The fine-tuning optimized the model to match stale references. The team spent compute, time, and money to make the model worse at the actual task. This is not hypothetical. It is the single most common cause of eval-production misalignment in organizations with mature eval systems but immature reference maintenance.

The cost is false confidence. Leadership sees eval scores at 0.90 and approves deployment. The model fails in production. The eval did not predict the failure because the references did not reflect production reality. The eval was a lagging indicator. It measured how well the model matched the old world. Production operates in the new world. The gap between eval and production is the staleness gap. Every month of staleness widens it.

The cost is annotator confusion. Annotators label new data using current knowledge. The eval references reflect outdated knowledge. The annotators' labels disagree with the references. The disagreement is not annotator error. It is reference staleness. But without visibility into reference age and provenance, the team attributes the disagreement to annotator quality. They retrain annotators. They replace annotators. They add review layers. None of this fixes the problem because the problem is not the annotators. It is the references. Misdiagnosing staleness as annotator error wastes training effort and damages team morale.

## Building Staleness Resistance Into the Eval Pipeline

Staleness resistance starts with metadata. Every reference includes creation date, source of truth, source version, author, and last review date. This metadata is not optional. It is structural. References without metadata are technical debt. They will become stale, and you will not know when or why.

Staleness resistance requires automated monitoring. A scheduled job runs weekly. It checks every reference with a documented source of truth. It compares the reference to the current source. If they diverge, it flags the reference for review. It does not auto-update, because the divergence might be intentional or the source might be wrong. It flags for human decision. Automated monitoring does not eliminate manual review. It reduces the manual review surface by identifying which references need attention.

Staleness resistance requires refresh workflows. When a reference is flagged as stale, it enters a review queue. A domain expert reviews the flagged reference, determines whether it is truly stale or whether the flag is a false positive, updates the reference if needed, or invalidates it if the test case is obsolete. The workflow tracks review status. Stale references do not sit in limbo. They are either refreshed or removed within a defined SLA. For high-risk references, the SLA is one week. For low-risk references, the SLA is one month.

Staleness resistance requires organizational integration. The eval team is not an island. When product changes, the product team notifies the eval team. When policy changes, the legal team notifies the eval team. When APIs change, the platform team notifies the eval team. These notifications trigger reference review. The eval system includes hooks for external change events. A product launch triggers an automated scan for references related to that product. A policy update triggers a scan for references citing that policy. Integration turns staleness from a reactive problem into a proactive one.

Staleness resistance requires accepting that some references will always be stale. You cannot achieve 100% freshness. The best you can do is minimize staleness, detect it quickly, and fix it before it distorts decisions. A realistic goal is 95% of references current within the last six months, 99% current within the last year. The remaining 1% are edge cases, obscure test scenarios, or references tied to deprecated features you have not yet removed. Track the staleness rate as a health metric. If it rises above 5%, you have a process problem. If it stays below 5%, you have a sustainable system.

When your ground truth is fresh, your eval measures what you want it to measure. When your ground truth is stale, your eval measures what you used to want. The difference is the gap between building the right system and building the wrong system well.

---

But even fresh, accurate references have a fundamental limitation: they assume you can define the correct answer in advance. Many tasks do not allow this. The next chapter explores reference-free evaluation, where correctness is judged without predefined ground truth.
