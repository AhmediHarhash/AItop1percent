# 9.8 — Calibration Documentation and Audit Trails

The auditor sits across from your team with a single question: why does your system reject this output but pass that one? You explain the eval criteria. The auditor asks when those criteria were defined. You say last March. The auditor asks who approved them. You say the product team. The auditor asks for the record. You do not have one. The criteria live in a config file. The commit message says "update thresholds." No design doc. No review notes. No explanation of what changed or why. The auditor marks your system as non-compliant with internal governance standards. Your deployment freezes until you can produce documentation. This scenario played out at a healthcare company in late 2025. The fix took six weeks and required reconstructing decision history from Slack threads and calendar invites. The lesson is simple: if you cannot explain why your eval works the way it does, you do not control your eval. And if you do not control your eval, you do not control your AI system.

## Why Calibration Needs Documentation

Every calibration decision is a policy decision. When you set a threshold at 0.75 instead of 0.80, you are deciding which outputs are good enough for production. When you reweight accuracy over fluency, you are deciding what your system optimizes for. When you update a judge model, you are changing the definition of quality that every downstream process depends on. These decisions have consequences. Users see different outputs. Regulators audit different behavior. Teams debug different failure modes. If the decisions are not documented, they become invisible. No one remembers why the threshold is 0.75. No one knows who approved the reweighting. No one can reconstruct what changed between the version that worked and the version that broke.

Documentation serves three audiences. First, your future team. Six months from now, someone will need to understand why the eval behaves the way it does. If the original decision-makers have left or moved to other projects, the documentation is the only record. Second, your stakeholders. Legal, compliance, product leadership, and domain experts all need to understand how quality decisions are made. Documentation gives them visibility and a mechanism to challenge decisions they disagree with. Third, auditors and regulators. If your AI system is subject to the EU AI Act, SOX compliance, HIPAA, or internal governance policies, you must be able to show how quality standards are set and enforced. Documentation is not optional — it is the artifact that proves you are in control.

A financial services company built a credit risk AI in early 2026. The eval included a fairness criterion that checked whether approval rates were consistent across demographic groups. The threshold for acceptable variance was set at five percent. When auditors reviewed the system, they asked why five percent. The team had documentation. The threshold was based on a joint review by Legal, Risk, and Engineering in November 2025. The review included analysis of historical approval data, comparison to industry benchmarks, and signoff from the Chief Risk Officer. The documentation included the meeting notes, the data analysis, the decision rationale, and the approval signature. The auditors accepted the threshold without further challenge. Without that documentation, the team would have spent weeks justifying a decision that had already been made responsibly.

## What to Document

Every calibration decision requires five pieces of documentation. First, what changed. Describe the modification in plain language: "Raised hallucination detection threshold from 0.70 to 0.78." "Switched tone judge from Claude Opus 4 to GPT-5.1." "Added new criterion for citation accuracy." Be specific enough that someone reading the record six months later can understand exactly what you did. Second, why you changed it. State the problem you were solving or the goal you were pursuing: "Hallucination threshold at 0.70 was producing twelve percent false positives based on domain expert review." "GPT-5.1 shows eight percent higher agreement with expert judgment on tone than Claude Opus 4." Third, who approved it. Name the stakeholders who reviewed and signed off on the change: product owner, domain expert lead, engineering lead, compliance if required.

Fourth, what evidence supported the decision. Include data: benchmark scores, expert review results, production metrics, shadow mode analysis. If you tested three threshold values and chose 0.78, show the comparison. If you evaluated four judge models and picked GPT-5.1, document the evaluation. The evidence does not need to be exhaustive — a summary with key numbers is enough. But it must exist. Fifth, what you expected to happen. State the predicted impact: "Expect false positive rate to drop from twelve percent to under five percent. Expect pass rate to decrease by three to four percent. Expect no change to false negative rate." Documenting expectations gives you a baseline for measuring whether the change worked.

A legal AI team documented a judge model swap in December 2025 using this structure. What changed: swapped citation evaluator from Gemini 3 Pro to Claude Opus 4.5. Why: Gemini 3 Pro showed fifteen percent false negative rate on legal citation validation, Claude Opus 4.5 showed six percent. Who approved: Head of Legal Product, Senior Legal Domain Expert, VP Engineering. Evidence: shadow mode comparison over two weeks, expert review of one hundred disagreement cases, benchmark scores on internal test set. Expected impact: false negatives drop to under seven percent, pass rate decreases by two to three percent, no degradation in false positive rate. Actual impact after two weeks: false negatives at five percent, pass rate down 2.1 percent, false positives at 1.8 percent. The documentation took twenty minutes to write. It became the permanent record of why the system behaves the way it does.

## Audit Trails for Every Change

An audit trail is a timestamped, immutable record of every change to your eval system. Every threshold adjustment, every criterion update, every judge model swap, every calibration run gets logged with metadata: what changed, when it changed, who made the change, and what commit or deployment triggered it. The audit trail is not the same as documentation. Documentation explains why. The audit trail records what and when. Together, they create a complete history of your eval system's evolution.

The simplest audit trail is a dedicated log table in your data warehouse. Every eval change writes a record: timestamp, change type, changed parameters, old values, new values, user ID, commit hash, deployment ID. This table becomes the single source of truth for eval history. You can query it to see all changes in the last six months. You can filter by change type to see every threshold adjustment. You can trace a specific output back to the eval configuration that scored it. A healthcare AI company built this log table in mid-2025. When a production issue surfaced in February 2026, the team queried the audit trail and discovered that a threshold change two weeks earlier had shifted pass rates by six percent. Without the audit trail, they would have spent days searching through git history and deployment logs. The audit trail gave them the answer in thirty seconds.

The audit trail must be immutable. No one should be able to delete or modify records. This requires write-only access for the logging service and read-only access for everyone else. Immutability ensures that the audit trail is trustworthy. Regulators and auditors need to know that the record has not been tampered with. Internal investigations need to know that the history is complete. A fintech company learned this in early 2026 when an engineer accidentally deleted three months of eval change logs while cleaning up old data. The company had to reconstruct the history from git commits and deployment artifacts. The reconstruction took two weeks and was incomplete. After that incident, the company moved audit logs to a separate, write-only database with retention policies enforced at the infrastructure level.

## Compliance Requirements for Eval Documentation

If your AI system operates under regulatory oversight, eval documentation is not optional — it is a compliance artifact. The EU AI Act requires high-risk AI systems to maintain technical documentation that includes quality management processes, evaluation methodologies, and performance metrics. HIPAA requires covered entities to document how patient data is used and how decisions are made. SOX requires publicly traded companies to demonstrate internal controls for systems that affect financial reporting. In every case, the eval system is part of the compliance scope. If you cannot document how quality decisions are made, you cannot demonstrate compliance.

Compliance documentation has different requirements than internal documentation. It must be version-controlled, timestamped, and retained for the full regulatory period — often five to seven years. It must be accessible to auditors without requiring access to production systems. It must be written in language that non-technical stakeholders can understand. A healthcare platform built a compliance documentation system in late 2025 that exported eval change records and decision rationale to a secured document repository every quarter. Each export included the audit trail, the design decisions, the approval records, and the performance metrics. The repository was accessible to Legal and Compliance without granting access to the production data warehouse. When HIPAA auditors requested eval documentation in early 2026, the team provided a complete record within two hours.

Compliance also requires demonstrating that your eval system is under active governance. This means showing that changes go through a review process, that stakeholders have visibility, and that decisions are made deliberately rather than casually. A financial services company built a lightweight approval workflow for eval changes in mid-2025. Threshold changes smaller than 0.05 required engineering lead approval. Threshold changes larger than 0.05 or any criterion redefinition required product and domain expert approval. Judge model changes required VP-level signoff. The workflow was enforced in code — changes could not deploy without the required approvals logged in the audit trail. When auditors reviewed the system, they saw a clear governance structure with evidence that it was actually followed. The company passed the audit without additional documentation requests.

## The Audit-Ready Eval System

An audit-ready eval system is one where every calibration decision is documented, every change is logged, and every artifact is accessible. Building this system requires three components. First, documentation templates that enforce the five-part structure: what changed, why, who approved, what evidence, what impact. Teams fill out the template before deploying any eval change. The template lives in the same repository as the eval code, versioned alongside the change. Second, an automated audit trail that logs every change without requiring manual input. The logging happens at the infrastructure layer — when a new eval config deploys, the deployment system writes the audit record. Third, a retrieval interface that lets stakeholders query the audit trail and access documentation without needing database access or engineering support.

A legal AI company built this system in Q4 2025. The documentation template was a markdown file with required sections. Engineers filled it out as part of the pull request for any eval change. Code review included checking that the template was complete. The audit trail was a Postgres table that received writes from the deployment pipeline. Every time a new eval config deployed, the pipeline logged the change with timestamp, commit hash, and configuration diff. The retrieval interface was a web app where product, legal, and compliance teams could search the audit trail, view documentation, and export records for external audits. Building the system took two weeks. The payoff came in January 2026 when the company's largest enterprise customer requested documentation of all eval changes in the previous twelve months. The team exported the full record in under an hour. Without the system, the request would have required days of manual work.

Documentation and audit trails are not overhead. They are the artifacts that prove you know what your system is doing and why. They prevent the scenario where an auditor asks a question and you have no answer. They prevent the scenario where a production issue surfaces and you cannot trace the root cause. They prevent the scenario where six months from now, no one on the team remembers why the eval works the way it does. The next challenge is detecting the kind of degradation that never shows up in dashboards — the slow, invisible drift where your eval stays stable but the world around it changes, which is the silent drift problem.

