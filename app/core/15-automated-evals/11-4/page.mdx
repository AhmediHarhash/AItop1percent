# 11.4 — Eval Versioning and Change Control

Why do teams version their code with obsessive discipline — Git commits, semantic versioning, pull request reviews for every change — but let their eval configurations drift without tracking? A threshold gets adjusted. A rubric gets reworded. A dataset gets three new examples. None of it is recorded. Six months later, nobody can explain why the model's quality score dropped from 0.88 to 0.84, whether it reflects real degradation or just eval drift. The code has a history. The eval does not. One is reversible, debuggable, auditable. The other is archaeology.

Eval versioning is not optional hygiene. It is the foundation of interpretable monitoring. Without it, every chart in your dashboard becomes untrustworthy. You lose the ability to compare results across time, to understand why metrics shifted, to roll back a bad change. You cannot debug what you did not track. You cannot trust a trend line when the measurement itself keeps changing. The eval is infrastructure. Version it like infrastructure.

## The Version Control Gap

Your codebase lives in Git. Every change has an author, a timestamp, a commit message, a diff. You can checkout any commit from six months ago and reproduce the exact behavior. You can bisect to find the commit that introduced a bug. You can review changes before they merge. You can revert a bad deploy in minutes.

Your eval system lives in a different world. The threshold for acceptable latency sits in a YAML file that three people have edit access to. Someone changes it from 1200ms to 1000ms because a stakeholder complained. No pull request. No review. No commit message. The change goes live immediately. Two weeks later, the failure rate doubles. Nobody remembers the threshold change. The team spends a day debugging the model before someone notices the eval config is different. The threshold gets changed back. The failure rate drops. The incident report does not mention eval drift because nobody realizes that is what happened.

This is not rare. This is the default state for most teams until they have lived through enough false alarms to realize the eval itself is a moving target. The gap exists because evals feel like configuration, not code. Configuration files get edited in place. Code gets versioned. But eval configuration determines what your monitoring trusts, what your regression suite blocks, what your dashboards display. Treating it as ephemeral configuration instead of critical infrastructure is the root cause of most eval debugging nightmares.

## Semantic Versioning for Evals

Every eval suite needs a version number. Not a timestamp. Not a Git SHA. A semantic version that communicates the nature of the change: major, minor, patch. The convention is the same one used in software: major.minor.patch. A breaking change bumps the major version. A backward-compatible improvement bumps the minor version. A bug fix or clarification bumps the patch version.

**A major version bump** means the results are not comparable to the previous version. You cannot plot v2.0 results on the same chart as v1.x results. Examples: you replace the entire dataset. You switch from human grading to LLM-as-judge. You redefine what "correct" means for a task. You change the scoring function from binary pass-fail to a five-point scale. You add a new quality dimension that was not measured before. Any change that fundamentally alters what the eval measures or how it scores bumps the major version.

**A minor version bump** means you added something or refined something, but the core measurement stayed the same. Results are still comparable, but the eval got more rigorous or more comprehensive. Examples: you add 50 new test cases to an existing 200-case dataset. You add a new sub-metric that complements existing metrics without replacing them. You refine a rubric to handle an edge case that was previously ungraded. You improve prompt clarity without changing the intent. The eval measures the same thing, just more thoroughly.

**A patch version bump** means you fixed a bug or clarified an ambiguity without changing what the eval measures. Results remain fully comparable. Examples: you fix a typo in a test case that caused misgrading. You correct a misconfigured threshold that was triggering false positives. You update a judge prompt to match the documented rubric after discovering a discrepancy. You fix a data loading bug that was silently skipping five test cases. Patch changes should not alter results unless the previous version had an actual error.

The discipline is not in the numbering system. The discipline is in asking the question: can I compare this version's results to the last version's results without misleading myself? If the answer is no, it is a major bump. If the answer is yes but the eval got stricter or broader, it is a minor bump. If the answer is yes and nothing substantive changed, it is a patch bump. The version number encodes interpretability.

## Version Everything

The eval suite is not a single artifact. It is a bundle of artifacts that must be versioned together. The version number applies to the entire configuration state: thresholds, rubrics, prompts, datasets, judge configurations, scoring functions, quality dimensions, and the metadata schema that defines how results are recorded.

If you version the dataset but not the rubric, you cannot reproduce the results. If you version the rubric but not the judge prompt, you cannot reproduce the scoring. If you version the judge prompt but not the model that runs it, you lose fidelity when the model provider updates their deployment. If you version all of that but not the thresholds, your regression suite's pass-fail logic is non-deterministic across time. Every piece that influences the outcome must be captured.

The gold standard is a versioned configuration file that references everything by hash or identifier. The dataset is stored in a versioned blob store and referenced by SHA-256 hash. The rubric is stored in the same repository as the eval code and referenced by Git commit. The judge model is pinned to a specific version or snapshot ID, not a moving tag like "latest." The thresholds are declared in the config file, not in environment variables or a database someone can edit. The entire configuration can be checked out from version control and will produce identical results given identical inputs.

This is harder than it sounds. Datasets grow incrementally. Rubrics get refined in response to edge cases discovered in production. Judge models improve and teams want to use the better version. The temptation is to update in place and keep the same version number. Resist it. Every substantive change gets a version bump and a changelog entry. If the change is non-breaking, bump minor or patch. If it is breaking, bump major and accept that the historical comparison stops at that point. The discomfort of losing historical continuity is the signal that you are making a significant change and should document it accordingly.

## The Version Comparison Problem

You deploy model v3.4 and run it through eval suite v1.8. Quality score: 0.89. Three months later, you deploy model v3.5 and run it through eval suite v2.1. Quality score: 0.86. Did the model get worse? Or did the eval get stricter? You cannot know without understanding the diff between eval v1.8 and v2.1.

If the version bump from v1.8 to v2.1 was minor, the result is interpretable: the eval got slightly more rigorous, and the model's score drop likely reflects real degradation. If the bump was major, the results are not comparable. You are comparing apples to oranges. The only way to know if the model regressed is to rerun model v3.4 through eval v2.1 and compare apples to apples. That requires keeping old model checkpoints around and rerunning them through new evals whenever you need a fair comparison.

Most teams do not do this. They compare the latest model to historical results from whatever eval version was current at the time. The chart shows a declining trend. Engineering panics. After two days of investigation, someone realizes the eval threshold was tightened six weeks ago. The model did not regress. The goalposts moved. The version number could have told them that immediately, but nobody checked it.

The safest rule is: never compare results across major version boundaries without rerunning both models through the same eval version. For minor version changes, you can compare trends but should annotate the chart with the version boundary. For patch changes, results are directly comparable. This rule only works if your versioning discipline is strict enough that minor and patch bumps actually mean what they claim to mean.

## Breaking Changes and Non-Breaking Changes

A **breaking change** is any modification that makes historical results non-comparable. This includes: replacing the dataset, redefining correctness, changing the scoring scale, switching judge models in a way that alters grading behavior, adding a new quality dimension that was not previously measured, or fundamentally restructuring the eval's output format. Breaking changes require a major version bump and clear documentation of what changed and why.

When you make a breaking change, you also need a migration plan. If your regression suite depends on eval v1.x passing a threshold of 0.85, and you bump to v2.0, what happens? The first run of v2.0 might score 0.78 because the eval got stricter. That does not mean the model regressed. It means the measurement changed. You need to establish a new baseline by running your current production model through v2.0, observing its score, and setting a new threshold based on that baseline. The old threshold is not portable.

A **non-breaking change** preserves comparability. Adding test cases to an existing dataset is non-breaking as long as the new cases measure the same quality dimensions as the old ones. Refining a rubric to handle an edge case is non-breaking if it does not change how the original cases are graded. Fixing a bug in the scoring logic is non-breaking if the bug was rare enough that most results were unaffected. Clarifying a judge prompt to reduce grader variance is non-breaking if the intent stayed the same. Non-breaking changes bump minor or patch, not major.

The judgment call is: if I rerun yesterday's model through today's eval, will the score change for reasons unrelated to the model? If yes, the change is breaking. If no, it is non-breaking. This is why patch-level changes should be true fixes, not improvements. Improving the eval is good, but improvements change scores. Fixes restore the intended behavior without changing scores for cases that were already graded correctly.

## Change Request Workflows

Eval changes should go through the same review process as code changes. Someone proposes a modification: tightening a threshold, adding test cases, refining a rubric. The proposal includes the version bump, the rationale, and the expected impact. A reviewer checks whether the version bump matches the scope of the change. If the proposer claims it is a patch but the change rewrites half the rubric, the reviewer pushes back. If the proposer claims it is minor but the change replaces the judge model, the reviewer escalates.

The review should also ask: is this change necessary? Eval drift is not always bad, but unnecessary drift is noise. If the rubric works and the proposal is aesthetic rewording with no functional improvement, reject it. If the threshold works and the proposal is vibes-based tightening with no data backing, reject it. If the dataset is sufficient and the proposal adds redundant test cases that measure the same failure modes already covered, reject it. Every change has a cost: it creates a version boundary, it requires documentation, it adds complexity to historical interpretation. The change must justify that cost.

For breaking changes, the review should be stricter. A major version bump means you are giving up direct comparability with historical data. That is sometimes necessary — the eval was wrong, or the product evolved, or the original design was insufficient. But it is never casual. The reviewer should ask: can this be done as a non-breaking change instead? Can you add the new test cases without removing the old ones? Can you run both eval versions in parallel for a transition period? Can you migrate gradually instead of all at once? If the answer is no and the breaking change is justified, approve it. But require a migration plan and a changelog that future engineers will read when they wonder why the chart has a discontinuity.

## Rollback Procedures

A new eval version goes live. Within 24 hours, the false positive rate triples. The regression suite blocks three legitimate deploys. The team investigates and discovers the new rubric is misinterpreting a common edge case. The fix will take two days to write and review. In the meantime, the eval is unusable.

If the eval is versioned properly, the rollback is trivial. You update the configuration to reference the previous version. The eval suite switches back to the last known good state. The regression suite resumes blocking only real failures. The team has two days to fix the new version without production impact. When the fix is ready, you bump to the next version and deploy again.

If the eval is not versioned properly, the rollback is archaeology. You need to figure out what changed. Was it the rubric? The threshold? The dataset? The judge prompt? Someone might remember, or you might find it in a Slack thread, or you might have to diff config files by hand. You revert the changes you think caused the problem. You deploy. The false positives continue. You realize you missed something. You revert more. Eventually, it works. You have lost a day. The team has lost trust in the eval system. The incident was not the bad version. The incident was the inability to undo it quickly.

Rollback procedures only work if you version everything and if the version history is complete. A missing version, a gap in the changelog, or a config change that bypassed the versioning system all break rollback. The discipline of versioning is not about the version numbers. It is about ensuring that every state of the eval is reproducible and that moving between states is a controlled operation, not a manual reconstruction.

## Version Documentation

Every version bump requires a changelog entry. The entry includes: the version number, the date, the author, the type of change (breaking, non-breaking), and a description of what changed and why. For major version bumps, the changelog should also include migration guidance: what the new baseline is, what thresholds need updating, what historical comparisons are no longer valid.

The changelog is not a Git commit message. Git commit messages describe code changes. The changelog describes semantic changes to the measurement. A Git commit might say "update rubric prompt." The changelog says "v1.9.0: refined hallucination rubric to handle cases where the model fabricates citations. Non-breaking change. Minor version bump because the rubric now catches a failure mode that was previously ungraded. Expect hallucination scores to drop by 2-4 points on average."

This level of documentation feels like overkill until you are debugging a metric anomaly six months later and the changelog tells you exactly why the score shifted in July. The documentation is cheap. The investigation you avoid is expensive. Write the changelog as if the person reading it has no context, because that person is you in six months, or your teammate who joined after the change, or the auditor trying to understand why your model's safety score dropped 5 points in a month.

For breaking changes, the documentation should also include a snapshot of the old version's configuration and results. If you cannot compare results directly, you should at least be able to inspect both versions side by side and understand what each one measured. Store this in the same repository as the eval code. Version the documentation with the eval. When you bump to v2.0, the v1.x documentation does not disappear. It gets tagged as historical and remains accessible.

## The Stealth Edit Anti-Pattern

**The Stealth Edit** is when someone changes the eval without bumping the version or documenting the change. The threshold gets tightened. The rubric gets reworded. The dataset gets five new examples. None of it is tracked. The version number stays the same. The results change. Nobody knows why.

This happens for three reasons. First, the person making the change does not realize it is significant. They think they are clarifying, not changing. Second, the version bump process feels like bureaucracy. It is faster to just edit the file. Third, the eval lives in a database or a config management system that does not have built-in versioning, so the path of least resistance is in-place edits.

The Stealth Edit is invisible until it causes a problem. A trend line shifts. A regression suite starts failing. A stakeholder questions why the metrics changed. The team investigates the model, the data pipeline, the infrastructure. They do not investigate the eval because the version number is the same, so they assume the eval is the same. Days of debugging later, someone diffs the current config against an old backup and discovers the change. The problem was not the model. The problem was the unmeasured measurement.

The fix is cultural and technical. Culturally, treat eval changes with the same seriousness as code changes. A threshold adjustment is a behavior change. A rubric refinement is a logic change. An added test case is new coverage. These are not tweaks. These are modifications to the system that determines what production looks like. Technically, make it harder to do Stealth Edits than to version properly. Store eval configs in Git. Require pull requests. Run a diff check in CI that fails if the config changes without a version bump. Make the versioned path the path of least resistance, and the stealth path require extra effort.

## Versioning as Interpretability

Eval versioning is not process for the sake of process. It is interpretability infrastructure. Every chart in your monitoring dashboard is a time series of eval results. If the eval changes over time, the chart is uninterpretable. A rising line could mean improving quality or a loosening eval. A falling line could mean degrading quality or a tightening eval. Without version annotations, you are guessing.

With proper versioning, every chart can be annotated with version boundaries. The tooltip shows not just the score but the eval version that produced it. When you see a discontinuity, you check the changelog. The changelog tells you whether the eval changed and whether the comparison is valid. The version number encodes whether the trend is real or an artifact of measurement drift. This turns your dashboard from a mystery into a diagnostic tool.

The discipline pays off most when you are debugging a crisis. The production model's quality score dropped 12 points overnight. Is the model broken? Is the data pipeline broken? Is the eval broken? With versioning, you check the eval version. It is the same as yesterday. The eval is not the variable. You focus on the model and the data. Without versioning, you do not know. You have to consider all three possibilities. The investigation takes longer. The resolution takes longer. The confidence in your diagnosis is lower. Versioning eliminates ambiguity. Ambiguity eliminates velocity.

But knowing what changes in your eval system only protects you from one class of silent failure — eval drift. The other class is functional failure: the eval runs, the version is stable, and the results are wrong because some component broke silently. The failure mode catalog for automated evals is long, specific, and impossible to catch with version control alone.

