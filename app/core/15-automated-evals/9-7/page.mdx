# 9.7 — A/B Testing Eval Changes

Changing an evaluation is a deployment. The moment you modify a threshold, reweight a criterion, or swap in a new judge model, you change what passes and what fails. You change which outputs reach production and which get rejected. You change the quality bar for every user request that flows through your system. Teams that treat eval changes as configuration tweaks discover the impact weeks later when production quality shifts in ways no one can explain. The fix is to test eval changes exactly like you test model changes — with shadow mode, gradual rollout, and measurement of impact before full adoption.

## Why Eval Changes Break Production

An eval change is invisible to end users but visible in every decision your pipeline makes. A healthcare company raised their hallucination detection threshold from 0.7 to 0.8 in January 2025, believing it would reduce false positives. It did — but it also let through eleven percent more borderline hallucinations that the old threshold would have caught. The team discovered this in March when domain experts reviewing flagged cases noticed the quality drop. By then, six thousand patient-facing outputs had shipped under the new, looser standard.

The problem was not the threshold change itself. The problem was changing it globally, instantly, without measuring what happened. The team assumed the new threshold was strictly better. They did not test whether the tradeoff — fewer false positives, more false negatives — matched their actual production distribution. They deployed the eval change the way you would deploy a bug fix. But an eval change is not a bug fix. It is a shift in the definition of quality. Every shift changes the outcomes. If you do not measure the outcomes before full rollout, you are flying blind.

Eval changes also interact with upstream model changes in ways that are hard to predict. A fintech company updated their tone classifier to use a newer judge model in late 2025. The new judge was more accurate on their benchmark set. But when combined with a GPT-5.1 base model they had deployed two weeks earlier, the tone classifier rejected fourteen percent more responses than expected. The new base model produced outputs with slightly different phrasing patterns. The new judge model penalized those patterns more harshly than the old judge did. Neither change was wrong in isolation. Together, they created a quality gate that was too strict for production traffic. The team caught this only because they ran the new judge in shadow mode for a week before switching it on.

## Shadow Mode for Eval Changes

**Shadow mode** means running the new eval alongside the old eval without affecting production decisions. Every output gets scored by both the current eval and the candidate eval. Production continues using the old scores. You log the new scores and compare them. After enough data accumulates, you analyze the differences. If the new eval produces better decisions, you promote it. If it produces worse decisions or unexpected behavior, you revise it before rollout.

Shadow mode answers three questions. First, does the new eval agree with the old eval on most cases? If agreement is above ninety percent, the change is low risk. If agreement is below eighty percent, something fundamental shifted — investigate before proceeding. Second, where do the two evals disagree? Pull a sample of disagreement cases. Review them with domain experts. Determine whether the new eval is making better calls or worse calls. Third, does the new eval change the pass rate? If your current eval passes seventy-eight percent of outputs and the new eval passes sixty-two percent, you need to understand why. The new eval might be correctly catching problems the old eval missed. Or it might be rejecting valid outputs due to overfitting or miscalibration.

A legal AI platform ran shadow mode for two weeks in December 2025 when testing a new citation accuracy evaluator. The old evaluator checked whether citations existed. The new evaluator checked whether citations matched the content they were supposed to support. In shadow mode, the new evaluator flagged twenty-three percent of outputs that the old evaluator had passed. The team reviewed a hundred disagreement cases. Eighty-one were legitimate problems — the old evaluator had been passing outputs with irrelevant or mismatched citations. Nineteen were false positives — the new evaluator penalized citations that were conceptually correct but phrased differently than expected. The team adjusted the new evaluator to handle phrasing variation, re-ran shadow mode for another week, and saw false positives drop to six percent. Only then did they promote the new evaluator to production.

Shadow mode has a cost. You are running two evals on every output, which doubles evaluation latency and compute. For high-traffic systems, this is expensive. The solution is to run shadow mode on a sample — ten percent of traffic, or one thousand outputs per day, whichever gives you enough data to measure differences. You do not need shadow mode to run forever. Two weeks is usually enough to cover typical traffic patterns and edge cases. For lower-risk changes like threshold adjustments, one week may suffice. For higher-risk changes like swapping judge models or redefining criteria, run shadow mode until disagreement patterns stabilize.

## Gradual Rollout of Eval Changes

Once shadow mode validates the new eval, roll it out gradually. Start with five percent of production traffic. Monitor pass rates, rejection reasons, and downstream quality metrics. If everything looks stable, increase to twenty percent. Then fifty percent. Then one hundred percent. Gradual rollout gives you three safety mechanisms. First, you limit blast radius. If the new eval breaks something, only a fraction of users are affected. Second, you get real production feedback before full adoption. Edge cases that did not appear in shadow mode will appear at scale. Third, you can roll back instantly. If pass rates drop unexpectedly or user complaints spike, you revert to the old eval without needing a code deploy.

A customer support platform rolled out a new response quality evaluator in October 2025 using a five-twenty-fifty-hundred schedule. At five percent, everything looked normal. At twenty percent, they noticed a two percent drop in pass rate for outputs handling billing disputes. The new eval was stricter about procedural accuracy. The team reviewed fifty billing dispute cases and determined the new eval was correct — the old eval had been too lenient. They accepted the lower pass rate and continued rollout. At fifty percent, they saw no new issues. At one hundred percent, the new eval became the production standard. Total rollout time was nine days. If they had deployed the new eval globally on day one, the two percent drop would have affected all users immediately, triggering escalations and confusion.

Gradual rollout also lets you measure business impact. Track user satisfaction scores, escalation rates, and task completion metrics during each rollout phase. If the new eval improves output quality, you should see positive movement in these metrics. If the metrics stay flat or degrade, the new eval might be optimizing for the wrong thing. A healthcare chatbot updated their empathy evaluator in early 2026. Shadow mode showed strong agreement with domain experts. Gradual rollout showed no change in user satisfaction scores. The team dug deeper and discovered that while the new eval correctly identified empathetic language, it did not correlate with what patients actually valued — which was clarity and actionability. The team kept the new eval for internal monitoring but did not use it as a production gate. Gradual rollout revealed what shadow mode could not: the eval was technically accurate but strategically misaligned.

## Measuring Eval Impact

An eval change succeeds if it improves production quality without breaking throughput or user experience. To measure this, you need three baselines captured before the change. First, the current pass rate — what percentage of outputs pass the old eval. Second, the current quality level as measured by downstream metrics — user ratings, escalation rates, domain expert reviews. Third, the current throughput — how many outputs per hour your pipeline processes. After deploying the new eval, track all three. If pass rate drops by more than five percent, investigate. If downstream quality improves, the stricter eval is working. If downstream quality stays flat, the stricter eval is rejecting outputs unnecessarily.

A fintech company deployed a new PII detection eval in November 2025. Pass rate dropped from eighty-two percent to seventy-six percent. Escalation rate — outputs flagged for human review — increased by four percent. But user complaints about leaked PII dropped to zero, down from three incidents per month. The eval impact was clear: the new eval was catching real PII that the old eval missed, slightly increasing manual review load but eliminating production leaks. The team accepted the tradeoff. Six weeks later, they tuned the eval to reduce false positives, bringing pass rate back to seventy-nine percent while maintaining zero leaks.

Measuring eval impact also means tracking false positive and false negative rates over time. Collect ongoing human feedback on eval decisions. When an output is rejected, log whether the rejection was correct. When an output passes, sample a fraction and have domain experts review them. If false positives increase after an eval change, your new eval is too strict. If false negatives increase, your new eval is too lenient. Both problems are fixable — but only if you measure them. Teams that deploy eval changes without ongoing measurement drift into miscalibration without realizing it. The eval continues making decisions, but the decisions stop matching production reality.

## Eval Change as Deployment Discipline

Treat every eval change with the same rigor you treat model deployment. Write a deployment plan. Run shadow mode. Execute gradual rollout. Measure impact. Document the change and the results. Roll back if metrics degrade. This discipline prevents the slow, invisible quality drift that kills AI products. Teams that change evals casually — adjusting thresholds in a config file, swapping judge models without testing, redefining criteria based on one stakeholder conversation — accumulate technical debt in their quality system. Every untested change compounds. Within six months, no one understands why the eval behaves the way it does. Debugging becomes archaeology.

A legal AI company built an eval change policy in mid-2025 after three incidents where threshold tweaks caused unexpected production issues. The policy required shadow mode for any judge model change, any criterion redefinition, or any threshold adjustment larger than 0.05. It required gradual rollout for all changes. It required a post-deployment review one week after full rollout, comparing pre-change and post-change metrics. The policy slowed down eval iteration slightly — changes that used to take one day now took one week. But production incidents related to eval changes dropped to zero. The team shipped fewer eval changes, but every change they shipped worked.

Eval changes are deployment. Shadow mode is staging. Gradual rollout is canary deployment. Impact measurement is monitoring. The practices that keep production stable apply equally to the evals that define production quality. The next question is how you document every calibration decision, every threshold change, and every judge model swap so that when an auditor asks why your system behaves the way it does, you can show them the record — which is the topic of calibration documentation and audit trails.

