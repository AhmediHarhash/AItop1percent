# 8.8 — Multi-Stage Eval Pipelines: Cheap to Expensive

The fastest, most accurate, most expensive evaluation in the world is worthless if you run it on every output that a regex could have filtered in ten milliseconds. Intelligent eval pipelines do not apply all evaluations to all outputs equally. They tier evaluations from cheap to expensive and exit early when a cheap evaluation already provides the answer. If a rule-based profanity filter catches a slur in the first sentence of a customer service response, the pipeline does not need to spend thirty cents running GPT-5 to assess tone and empathy. The output already failed. The multi-stage pipeline saved you money, saved you time, and produced the same final decision.

The cheap-to-expensive tiering pattern is the single most important cost optimization in automated evaluation. Most outputs fail for simple, obvious reasons that deterministic logic can detect in microseconds. Only outputs that pass the simple checks need the expensive, nuanced evaluations that require inference against frontier models. Structuring your pipeline as a sequence of stages — each more expensive and more sophisticated than the last — reduces cost by 60-80% compared to running all evaluations on all outputs, without sacrificing coverage or accuracy.

## Stage One: Rules and Heuristics

The first stage of a multi-stage pipeline uses deterministic logic that runs in constant time and costs effectively zero. Regular expressions that detect profanity, PII patterns, or banned phrases. String length checks that reject outputs shorter than twenty characters or longer than five thousand. Format validation that ensures JSON outputs parse correctly or that structured data contains required fields. Regex patterns that catch prompt injection attempts like "ignore all previous instructions" or "you are now in developer mode." These evaluations take microseconds per output. They catch 30-50% of failures. They cost nothing.

If an output fails a stage-one check, the pipeline marks it as failed, logs the specific rule that triggered, and stops processing. The output never reaches stage two. For a customer service system processing ten thousand responses per hour, this means 3,000 to 5,000 outputs are rejected before any LLM inference is invoked. At fifteen cents per expensive eval, that saves $450 to $750 per hour. Over a month, that is $324,000 saved by running regex first.

The anti-pattern is skipping deterministic checks because they feel too simple or because the team wants to rely entirely on LLM-based evaluation. LLMs are not faster than regex. They are not cheaper than regex. They are not more accurate at detecting exact string matches. Use the right tool for the job. If a deterministic rule reliably detects a failure mode, use it. Save the LLM for the cases where nuance, context, and semantic understanding are actually required.

## Stage Two: Cheap LLM Evaluation

Outputs that pass stage one move to stage two, where they are evaluated by a fast, inexpensive model. GPT-5-mini, Claude Haiku 4.5, Gemini 3 Flash, or Llama 4 Scout. These models cost one-tenth to one-twentieth as much as frontier models and run five to ten times faster. They are not as accurate as GPT-5 or Claude Opus 4.5, but they are accurate enough to catch another 20-30% of failures. A cheap LLM can reliably detect off-topic responses, blatant hallucinations, tone violations, and formatting errors. It struggles with subtle toxicity, nuanced factual errors, and edge cases that require deep domain knowledge. That is fine. The cheap model does not need to catch everything. It needs to catch the obvious cases and pass the ambiguous ones to stage three.

Your stage-two prompt is optimized for precision, not recall. You want the cheap model to reject outputs it is confident are bad and defer on outputs it is uncertain about. A customer service response that says "I don't care about your problem" is obviously dismissive — GPT-5-mini catches it. A response that says "Unfortunately, we're unable to assist with that request at this time" might be appropriate or might be a cop-out depending on context — the cheap model marks it as "needs review" and passes it to stage three.

The output of stage two is a three-way decision: pass, fail, or escalate. Pass means the cheap model is confident the output is good — it skips stage three and goes directly to production or to human review depending on your risk tier. Fail means the cheap model is confident the output is bad — it stops here and never reaches stage three. Escalate means the cheap model is uncertain — the output proceeds to stage three for evaluation by a more capable, more expensive model. Approximately 60-70% of outputs that reach stage two get a confident pass or fail. The remaining 30-40% escalate to stage three.

## Stage Three: Expensive LLM Evaluation

Stage three uses frontier models — GPT-5, Claude Opus 4.5, Gemini 3 Pro. These models cost ten to twenty times more than stage-two models and run slower, but they evaluate with human-level nuance. They catch subtle toxicity that cheap models miss. They assess factual accuracy against retrieved context with high reliability. They detect tone violations that require understanding conversational implicature and cultural norms. They cost fifteen to thirty cents per evaluation. You run them only on the outputs that made it through stage one and escalated from stage two — roughly 15-30% of total volume.

For the customer service system processing ten thousand outputs per hour, that means 1,500 to 3,000 outputs reach stage three. At twenty cents per eval, that costs $300 to $600 per hour. Without tiering, running the expensive model on all ten thousand outputs would cost $2,000 per hour. The multi-stage pipeline saves $1,400 to $1,700 per hour, or roughly $1.2 million per month, by filtering out simple failures before invoking the expensive model.

Stage-three prompts are optimized for recall and nuance. The frontier model is your last automated line of defense before human review or production deployment. It needs to catch everything the earlier stages missed. Your prompt includes detailed rubrics, retrieved context, examples of pass and fail cases, and instructions to explain reasoning. The model returns not just a pass/fail decision but also a confidence score and a justification. Outputs that fail stage three are rejected. Outputs that pass with high confidence go to production or skip human review. Outputs that pass with low confidence are flagged for human review even if they technically passed.

## Stage Four: Human Review

For high-stakes outputs — anything in Tier 1 or high-risk Tier 2 — human review is the final stage. But not every output needs human review. Only outputs that passed all automated stages and still triggered a low-confidence flag, or outputs that failed an automated stage in a way that suggests possible false positive. A human reviewer looks at 2-5% of total volume. They see the output, the eval results from all prior stages, the confidence scores, and the reasoning generated by the stage-three model. They make the final call.

Human review is the most expensive stage by far — not in per-unit cost but in latency and operational overhead. Humans work on hourly schedules, not millisecond schedules. Sending an output to human review means it will not reach production for minutes to hours. You only invoke this stage when the stakes justify the delay and the cost. For a legal contract generator, every output goes to human review. For a customer service chatbot, only flagged outputs do. For a marketing copy generator, human review happens in batch once per day for the top 100 outputs by user engagement.

## Early Exit on Failure

The key to multi-stage efficiency is exiting as soon as a failure is detected. If stage one rejects an output, do not run stage two. If stage two rejects an output, do not run stage three. Every stage operates on a smaller set of outputs than the stage before it. This is not just a cost optimization. It is a latency optimization. An output that fails stage one gets a response in microseconds. An output that fails stage two gets a response in 200-400 milliseconds. An output that requires stage three takes 1-2 seconds. The pipeline adapts its latency to the complexity of the case.

The implementation is straightforward. Each stage returns a decision object that includes a status field — pass, fail, or escalate. The orchestrator checks the status after each stage. If status is fail, the orchestrator logs the failure, records which stage and which specific check triggered the failure, and stops processing. If status is pass and the current stage is not configured to require escalation, the orchestrator skips remaining stages and returns success. If status is escalate or if the stage is configured to always escalate, the orchestrator proceeds to the next stage. The result is a pipeline where most outputs touch only one or two stages, not all four.

## Cost Optimization Through Staging

A real-world example demonstrates the impact. A financial services company processes 50,000 chatbot outputs per day. Before implementing multi-stage evaluation, they ran every output through a GPT-5-based eval at twenty cents per call. Daily eval cost: $10,000. Monthly cost: $300,000. After implementing a four-stage pipeline, the distribution shifted. Stage one — regex and rules — rejected 35% of outputs at zero cost. Stage two — GPT-5-mini at two cents per call — rejected another 25% of outputs at a cost of $250 per day. Stage three — GPT-5 at twenty cents per call — evaluated the remaining 40% at a cost of $4,000 per day. Stage four — human review — touched 2% of outputs at a marginal cost of $500 per day. Total daily cost: $4,750. Monthly cost: $142,500. The multi-stage pipeline saved $157,500 per month, a 52% reduction, with no decrease in quality coverage.

The cost savings scale with volume. At 100,000 outputs per day, the single-stage pipeline would cost $20,000 per day or $600,000 per month. The multi-stage pipeline would cost approximately $9,500 per day or $285,000 per month, saving $315,000 per month. At 500,000 outputs per day, the savings exceed $1.5 million per month. The larger the system, the greater the return on building a tiered pipeline.

The next optimization is ensuring that when a pipeline does fail, it fails fast — detecting errors early and aborting execution before wasting resources on doomed outputs.

---

*Next: 8.9 — Early Stopping and Fast-Fail Logic*
