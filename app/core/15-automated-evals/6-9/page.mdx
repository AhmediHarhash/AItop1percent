# 6.9 — Side Effect Detection: What the System Changed

In October 2025, a financial services company deployed an agent designed to help customer service representatives resolve account disputes. The agent had read access to transaction history and write access to case management systems — it could update case notes, reassign cases to specialists, and mark issues as resolved. During the first production week, dispute resolution time dropped by 30%. Representatives loved it. Then the compliance team ran a routine audit and discovered that the agent had been marking cases as resolved without actually processing refunds. It saw that representatives typically marked cases resolved after initiating a refund, so it learned the pattern: update case status, add a closure note. The agent never called the refund API, because the training data didn't show that step — representatives processed refunds through a different system that wasn't in the logs. The company had to manually review 1,847 cases, reopen 623 of them, and process $240,000 in delayed refunds. The agent's task success rate was 94%. Its side effect detection rate was zero.

## Detecting What Agents Changed

An agent's primary output is its response — the text, the structured data, the completion signal. But in any system where the agent has write access to tools, databases, or APIs, the primary output is only part of the story. The agent also produces **side effects**: changes to external state that persist after the task completes. It creates records, updates fields, deletes files, sends messages, triggers workflows. Some of these side effects are intended — they're the whole point of the task. Some are unintended — they're artifacts of how the agent explored options, retried actions, or misunderstood tool semantics. Your eval must detect both.

You start by instrumenting every write operation the agent performs. When the agent calls a tool that modifies state, your eval logs the operation type, the target resource, the before state, and the after state. A database update gets logged with the record ID, the old field values, and the new field values. A file deletion gets logged with the file path and a hash of the deleted content. An API call that triggers a workflow gets logged with the workflow ID and the initial status. This instrumentation isn't optional — without it, you have no visibility into what the agent actually changed.

Your eval then categorizes side effects based on task intent. An intended side effect is one that aligns with the task goal. If the task is "update customer address," and the agent updates the address field in the customer record, that's an intended side effect. An unintended side effect is one that doesn't align with the task goal. If the agent also updates the last_modified timestamp, resets a preference flag, or triggers a notification email, those might be unintended — they happened because of how the tool works, not because the task required them. The eval compares the set of changes the agent made against the set of changes the task specification defined as necessary.

## Side Effect Classification: Intended vs Unintended

The line between intended and unintended side effects isn't always obvious. Some side effects are implicit requirements that weren't spelled out in the task definition. Updating a timestamp when modifying a record is usually intended, even if the task didn't explicitly mention it. Sending a confirmation email after completing a transaction is usually intended, even if the task only said "process payment." Other side effects are genuine mistakes. Deleting a temporary file that's still referenced by another process is unintended. Updating a shared configuration that affects other users is unintended. Your eval needs rules to distinguish the two.

You encode intent as part of your task definitions. Each task specifies not just the primary action, but the expected side effects. A payment task expects a transaction record, a balance update, and a receipt generation. It doesn't expect changes to user preferences or modifications to unrelated accounts. When the agent completes the task, your eval compares the actual side effects against the expected set. Side effects that match the expected set are classified as intended. Side effects outside the expected set are classified as unintended and flagged for review.

The hardest cases are when the agent produces the intended side effects plus additional ones. It updates the customer address as requested, and also updates the billing address, the shipping address, and the contact preferences. The task said "update address" without specifying which one. The agent interpreted that broadly and changed everything address-related. From a task success perspective, it succeeded — the customer's address is updated. From a side effect perspective, it over-acted — it changed more than the user intended. Your eval must detect this pattern, because over-acting creates downstream problems even when the primary task succeeds.

## Side Effect Verification

Producing the right side effects isn't enough — the side effects must be correct. An agent might update the intended field in the intended record, but set it to the wrong value. It might send the intended notification, but to the wrong recipient. It might create the intended workflow, but with incorrect parameters. Your eval must verify not just that side effects occurred, but that they contain the right data.

You verify side effects the same way you verify task outputs: by comparing the post-task state against a known correct state. For a database update, you query the modified record and check that field values match expectations. For a file modification, you compare the file content against a reference version. For an API call, you inspect the triggered workflow's initial parameters. Verification requires read access to every resource the agent can write to. If the agent can modify a database, your eval must be able to query it. If the agent can create cloud resources, your eval must be able to inspect them.

Side effect verification catches errors that output verification misses. The agent might return a success message saying "address updated," and that message is technically correct — an address was updated. But the eval verifies the database and discovers that the agent updated the billing address when the task specified shipping address. The agent's self-reported output was accurate. Its side effects were wrong. Without side effect verification, this failure mode is invisible until a user reports an issue.

## Dangerous Side Effect Detection

Some side effects are not just unintended — they're dangerous. Deleting production data, modifying access controls, triggering irreversible workflows, sending messages to external systems, or changing billing configurations all carry risk. Even if the agent believes these actions are necessary to complete a task, they need review before execution. Your eval must detect when an agent performs a dangerous side effect and classify it as a high-severity issue regardless of whether the task succeeded.

You maintain a list of dangerous operations as part of your eval configuration. Any operation that modifies user permissions is dangerous. Any operation that deletes data without a retention policy is dangerous. Any operation that sends communication outside your organization is dangerous. Any operation that changes financial or billing state is dangerous. When your eval detects that an agent called one of these operations, it flags the case, captures the full tool trace, and logs the before and after state. These cases get manual review, even in automated pipelines.

The goal is not to prevent dangerous operations — some tasks legitimately require them. The goal is to ensure that dangerous operations only happen when they're actually necessary for task completion, and that they're executed correctly. If an agent deletes a file to complete a cleanup task, that's expected. If an agent deletes a file as part of a retrieval task, that's an error. Your eval uses task context to determine whether a dangerous side effect was appropriate. The same operation can be correct in one task and a critical failure in another.

## The Read-Only vs Write Audit

One of the most effective safety mechanisms for side effect detection is the read-only audit. Before deploying an agent with write access, you run it in a mode where it has read access to all tools but cannot execute write operations. The agent goes through its full decision process — it plans, it reasons, it decides which tools to call — but when it attempts a write operation, the eval logs the attempt without executing it. You build a profile of what the agent would have changed if it had write access.

You then review the read-only audit log to identify unintended or dangerous side effects before they happen. If the agent attempted to delete files, update unrelated records, or modify shared configuration, you see those attempts in the log. You can then refine the task definition, adjust the agent's tool permissions, or add guardrails to prevent those actions before giving the agent real write access. The read-only audit is a preview of side effects, and it catches errors in controlled conditions rather than in production.

Your eval pipeline can automate part of this review. It compares the attempted side effects against the expected side effect set for each task. Any attempted write that falls outside the expected set gets flagged as a potential issue. A human reviews the flagged cases and decides whether they're legitimate or whether they indicate a reasoning error. Over time, you build a library of safe and unsafe side effect patterns, and the eval uses those patterns to score new audit logs. High scores mean the agent's side effects align with intent. Low scores mean the agent is doing things it shouldn't.

## Side Effects as First-Class Eval Targets

Most eval pipelines treat side effects as a secondary concern. The primary metric is task success rate. Side effects get checked occasionally, usually only after a production incident. This is backward. For any agent with write access, side effects are just as important as outputs. An agent that produces correct outputs but incorrect side effects is not succeeding — it's failing in a way that traditional evals don't measure.

You make side effects a first-class eval target by tracking them with the same rigor as task accuracy. Every eval run logs the full set of side effects produced by the agent. Every task gets a side effect success rate: the percentage of tasks where the actual side effects matched the expected side effects. You track side effect accuracy alongside task accuracy, and you require both to meet thresholds before promoting an agent to production. An agent with 95% task accuracy and 70% side effect accuracy is not production-ready, even if the outputs look perfect.

Your eval also tracks side effect stability across agent versions. A model update might maintain task accuracy while changing side effect patterns. The new model might call tools in a different order, retry actions that the previous model didn't retry, or skip verification steps that used to happen. Each of these changes produces different side effects. If the new side effects are less aligned with intent than the old ones, that's a regression. Side effect accuracy must be stable or improving across updates, just like task accuracy.

## Side Effect Replay and Impact Analysis

When your eval detects an unintended or dangerous side effect, you need to understand its impact. What would have happened if this agent ran in production? Which users would be affected? Which systems would be modified? Which workflows would trigger? You answer these questions with side effect replay: taking the logged side effects from an eval run and simulating their impact in a production-equivalent environment.

Side effect replay uses a staging environment that mirrors production data and system state. Your eval applies the side effects the agent produced — the database updates, the file modifications, the API calls — and then measures the downstream consequences. Did the update break a dependent workflow? Did the deletion orphan related records? Did the API call trigger notifications to real users? The replay reveals impact that isn't obvious from looking at individual operations in isolation.

You use impact analysis to prioritize side effect issues. An unintended side effect that changes a non-critical field in an isolated record is low severity. An unintended side effect that modifies shared state used by fifty workflows is high severity. An unintended side effect that sends an email to a customer is critical. Your eval scores side effects based on replay impact, and high-impact issues block deployment regardless of task success rates.

## Side Effects in Multi-Agent Workflows

When multiple agents collaborate on a task, side effects compound. One agent creates a record. Another agent updates it. A third agent deletes it. The final state might be correct, but the intermediate states could violate constraints or trigger unintended workflows. Your eval must track side effects across the full multi-agent execution and verify that the cumulative impact aligns with task intent.

You log side effects with agent attribution. Each write operation is tagged with the agent that performed it, the step in the workflow where it occurred, and the task context. When the workflow completes, your eval builds a timeline of side effects showing which agent changed what and when. If the timeline reveals conflicting changes — one agent updates a field that another agent later resets — that's a coordination issue. If it reveals redundant changes — multiple agents updating the same field to the same value — that's an efficiency issue.

Your eval also tests whether agents handle side effects from other agents correctly. If Agent A creates a record and Agent B expects that record to exist, Agent B's behavior depends on Agent A's side effects. If Agent A fails to create the record, or creates it with incorrect data, Agent B's task will fail. Your eval creates scenarios where side effects are missing, malformed, or delayed, and measures whether downstream agents detect these issues or propagate them into their own outputs.

## The Write Audit as a Safety Layer

For high-risk agents, you implement a write audit layer that reviews side effects before committing them. The agent executes its task in a sandbox where write operations are staged but not applied. When the task completes, the eval examines the staged side effects, verifies them against task intent, and checks for dangerous operations. Only if all checks pass does the eval commit the side effects to the real system. If any check fails, the staged changes are discarded and the task is flagged for review.

The write audit adds latency — side effects don't happen until the eval approves them. For interactive tasks, this delay is unacceptable. For batch tasks, background workflows, or high-stakes operations like financial transactions or data deletions, the delay is worth the safety. You apply write audits selectively, based on task risk level and agent trust level. A mature agent with a long history of correct side effects might skip the audit. A new agent or a newly modified agent goes through full audit until it earns trust.

Your eval tracks audit rejection rates. If 5% of tasks get rejected during write audit, the agent is making too many side effect errors to deploy safely. If 0.1% get rejected, the agent is reliable but the audit is still catching edge cases. Audit rejection rate is a leading indicator of side effect quality. It tells you how often the agent attempts something wrong before the system stops it. High rejection rates mean the agent needs better guardrails or clearer task definitions.

Side effect detection transforms your eval from a measure of what the agent said to a measure of what the agent did. In production, side effects are often more consequential than outputs. A wrong answer frustrates a user. A wrong database update breaks workflows for hundreds of users. Your eval must measure both, with equal rigor and equal enforcement. The next layer is end-to-end evaluation: testing how all these behavioral dimensions combine when agents operate in realistic scenarios with full system integration.

