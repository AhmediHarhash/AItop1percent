# 11.3 — Reproducibility Requirements for Regulated Industries

**Reproducibility** means that given the same inputs, you get identical eval results. Not similar results. Not close enough results. Identical results. In healthcare, finance, and other regulated industries, this is not a nice-to-have engineering practice. It is a compliance requirement. When a regulator or auditor asks to see the evaluation that validated your AI system before deployment, you must be able to run that exact evaluation again and produce the exact same pass-or-fail decision. If you cannot, your validation evidence is worthless.

The challenge is that AI evaluation systems are composed of moving parts. Your production model has a version. Your eval dataset has a version. Your judge model has a version, and if that judge is GPT-5 or Claude Opus 4.5, the API provider might update it without telling you. Your rubrics and thresholds evolve as your understanding improves. Your infrastructure dependencies update. Random seeds affect sampling. Every one of these variables can change the eval outcome. Reproducibility means locking down all of them, versioning all of them, and being able to reconstruct the exact state of the system at any point in history.

## What Regulators Expect

The FDA's guidance on software validation, the EU AI Act's technical documentation requirements, and financial regulators' model risk management frameworks all demand the same thing: you must be able to demonstrate that your AI system was tested, that the test was rigorous, and that the test can be independently verified. Independent verification requires reproducibility. If an auditor cannot re-run your evaluation and get the same result, they cannot trust that the evaluation happened the way you claim it did.

This expectation shapes every architectural decision in your eval pipeline. You cannot use a model API that does not guarantee version stability unless you build a reproducibility layer on top of it. You cannot modify your golden dataset in place without maintaining versioned snapshots. You cannot upgrade your judge model without documenting the change and re-baselining all historical comparisons. Reproducibility forces discipline that most non-regulated teams never adopt, but that discipline is what separates a compliant system from one that fails audit.

The standard is higher than software engineering's typical "works on my machine" reproducibility. It is forensic reproducibility. Someone who was not on your team, six months from now, with access to your versioned artifacts, must be able to reproduce your results exactly. If they cannot, your documentation is incomplete.

## Versioning Everything That Affects Eval Outcomes

Reproducibility starts with versioning. Every component that influences an eval result must have an immutable version identifier. Your production model has a version — a git SHA, a timestamp, a semantic version number. Your eval dataset has a version. Your judge model has a version. Your scoring rubric has a version. Your threshold configuration has a version. Your pipeline code has a version. Your container image has a version. If any of these changes between two eval runs, the results are not directly comparable, and reproducibility is broken.

The mistake most teams make is versioning the obvious things — the production model, the pipeline code — but leaving the eval dataset and judge model as floating dependencies. They point to the latest version of the golden set. They call the OpenAI API without specifying a model snapshot. They update their rubric file in place without tracking the change. Three months later, when they need to reproduce a pre-deployment eval, they discover that the golden set has 200 new examples, the judge model has been updated twice, and the rubric now scores edge cases differently. The eval runs, but the results are not the same. The reproducibility requirement is violated.

The fix is to treat every eval input as a versioned artifact. Your golden dataset is stored with a content hash and a timestamp. When your pipeline runs, it specifies the exact dataset version. Your judge model is called with a version pin — if using OpenAI's API, you specify the dated model snapshot like gpt-5-2026-01-15, not the floating alias gpt-5. Your rubric is checked into version control, and each pipeline run references a specific commit SHA. Your threshold config is versioned alongside the rubric. Your container image is tagged with a semantic version and pushed to a registry. Every artifact has a name that uniquely identifies its contents.

## Snapshot Requirements at Decision Points

Reproducibility does not mean versioning everything all the time. It means capturing complete snapshots at every decision point. A decision point is any moment where an eval result influenced a real-world action. You ran an eval before deploying to production. You ran an eval to validate a prompt change. You ran an eval to approve a fine-tuning dataset. Each of those evals is a decision point, and each one requires a full snapshot.

A snapshot includes the model being evaluated, the eval dataset version, the judge model version, the rubric version, the threshold configuration, the pipeline code version, the container image version, and the complete set of eval outputs — every prediction, every judgment, every score. You store all of this together, indexed by a snapshot ID. Six months later, when someone asks to reproduce the eval, you load the snapshot, point the pipeline at the versioned artifacts, and run it again. If your versioning was complete, the results match exactly.

The storage cost is real. A single snapshot might include a multi-gigabyte model checkpoint, a multi-megabyte eval dataset, and tens of thousands of judgment records. A team running evals daily generates terabytes of snapshot data per year. But the alternative — being unable to reproduce a regulatory validation — is far more expensive. Teams in regulated industries build retention policies that archive snapshots for seven years, the typical regulatory lookback period. The storage cost is treated as cost of compliance, not optional infrastructure spend.

## Judge Model Reproducibility and API Instability

The hardest reproducibility problem is judge models accessed through APIs. If your judge is GPT-5 via the OpenAI API, and you call it without specifying a dated model snapshot, OpenAI can update GPT-5 at any time. The update might improve general performance, but it changes how the model scores your specific task. An eval that passed yesterday might fail today, not because your production model changed, but because the judge changed. This breaks reproducibility completely.

The fix is to pin judge models to dated snapshots whenever the provider offers them. OpenAI and Anthropic both provide dated model versions — gpt-5-2026-01-15, claude-opus-4-5-20251101. These versions are immutable for a guaranteed period, usually six months to a year. Your eval pipeline specifies the dated version, not the floating alias. When the dated version nears deprecation, you explicitly migrate to a new dated version, re-baseline your thresholds, and document the change as a breaking update to the eval system. This migration is intentional and tracked, not invisible and silent.

If your judge model provider does not offer dated snapshots, you lose reproducibility unless you self-host. A self-hosted judge model — running Llama 4 Maverick on your own infrastructure — gives you full control over versioning. You pin the model weights to a specific checkpoint, version the checkpoint in your artifact registry, and reference that version in every eval run. The model never changes unless you explicitly update it. The trade-off is operational complexity and inference cost, but for regulated industries, reproducibility often justifies the cost.

Some teams use a hybrid approach: they run evals with both an API-based judge and a self-hosted judge. The API judge provides fast iteration during development. The self-hosted judge provides reproducibility for decision-point snapshots. This doubles the eval cost but eliminates the risk of unreproducible validation.

## Dataset Immutability and Version Control

Your eval dataset must be immutable. If you add examples to the golden set, that is a new version, not an update to the existing version. If you correct a mislabeled example, that is a new version. If you remove an example because it became outdated, that is a new version. Every change creates a new versioned artifact. The old version remains intact and accessible.

The mistake teams make is treating the golden set like a living document. They add new examples as they discover edge cases. They fix labeling errors when annotators catch mistakes. They remove examples that no longer represent production traffic. All of this happens in place, overwriting the previous dataset. Three months later, when they need to reproduce an eval, the dataset they reference has 50 fewer examples and 30 different labels. The eval results differ, and they cannot explain why.

The fix is copy-on-write versioning. Your golden set lives in a versioned artifact store — S3 with versioning enabled, a Git LFS repository, a dataset registry like Hugging Face Datasets or your own internal system. When you make any change, you create a new version with a new identifier. The pipeline references dataset versions explicitly: golden-set-v47, not golden-set-latest. Historical eval runs remain tied to the dataset version they used. When you reproduce an eval, you load the exact dataset version from the snapshot metadata.

You document every dataset version change in a changelog. Version 47 added 15 examples covering a new edge case. Version 48 corrected a labeling error in 3 examples. Version 49 removed 10 examples that became non-representative after a product change. The changelog is human-readable and included in audit documentation. Regulators understand that datasets evolve — what they require is that the evolution is tracked, intentional, and does not silently invalidate prior validation work.

## Environment Reproducibility and Dependency Pinning

Reproducibility extends beyond data and models to the execution environment. Your eval pipeline runs in a container. That container has a base image, a Python version, a set of installed packages. If any of these change, the pipeline might produce different results. A Hugging Face Transformers update might change tokenization behavior. A NumPy version bump might change random number generation. A CUDA library update might change numerical precision in model inference. These are rare, but they happen, and when they do, reproducibility breaks.

The fix is to pin every dependency and version the entire environment. Your pipeline runs in a Docker container built from a versioned Dockerfile. The Dockerfile specifies exact package versions: transformers==4.38.1, not transformers. The container image is tagged with a semantic version and pushed to a container registry. Your eval pipeline references the container version explicitly. When you snapshot an eval, you include the container image SHA in the snapshot metadata. To reproduce the eval, you pull the exact container image and run it with the exact inputs.

Random seeds are part of environment reproducibility. If your pipeline samples a subset of the eval dataset, the random seed determines which examples are sampled. If your judge model uses temperature greater than zero, the random seed affects generation. If you do not set seeds explicitly and consistently, you get different results on every run, even with identical inputs. The fix is to set all random seeds at pipeline startup, include the seed value in snapshot metadata, and restore the seed when reproducing the eval. This determinism is essential for regulatory compliance.

## The Reproducibility Test

The test for reproducibility is simple: can you run yesterday's eval with yesterday's configuration and get the same result? You pick a snapshot from last week. You load the model version, the dataset version, the judge model version, the rubric version, the container version. You set the random seeds to the snapshot values. You run the pipeline. You compare the new results to the stored results from the snapshot. Every prediction matches. Every judgment matches. Every aggregate metric matches to the decimal. If this works, you have reproducibility. If it does not, you have a gap to fix.

Teams in regulated industries run reproducibility tests on a schedule. Once a month, they select a random historical snapshot and attempt to reproduce it. If reproduction fails, they investigate. Was a dependency not pinned? Did a dataset version get overwritten? Did a model API change behavior unexpectedly? The investigation surfaces gaps in versioning discipline, and the team fixes them before the next audit.

The reproducibility test also runs during audits. An auditor picks an eval from your submission — the one that validated your production deployment six months ago. They ask you to reproduce it. You load the snapshot, run the pipeline, and show them the results. If the results match, the auditor gains confidence that your validation process is sound. If the results do not match, the auditor flags your submission as unreliable, and you face regulatory delay or rejection. The cost of failing a reproducibility test is measured in months of rework and lost market opportunity.

## Regulatory Standards Across Industries

Different regulators have different ways of articulating the reproducibility requirement, but the underlying expectation is the same. The FDA's software validation guidance requires that testing be repeatable and verifiable. The EU AI Act's Article 11 requires technical documentation that includes validation datasets and metrics, with the implicit expectation that validation can be re-run. Financial regulators' model risk management standards require that model testing be documented sufficiently for independent replication.

In practice, this means your eval pipeline must produce documentation that an external party can follow to reproduce your results. The documentation includes the versions of all artifacts, the configuration of all components, the procedures for running the pipeline, and the expected outputs. It is not enough to say "we tested the model and it passed." You must say "we tested model version 3.2.1 against dataset version 47 using judge model claude-opus-4-5-20251101 with rubric version 2.8 and thresholds defined in config version 12, and here are the complete results, and here is how to reproduce them."

The bar is higher than most engineering teams are used to. It requires treating your eval system as a production system with the same rigor you apply to the AI product itself. It requires versioning discipline, snapshot discipline, and documentation discipline. It requires infrastructure investment in artifact storage, container registries, and reproducibility testing. It requires a cultural shift from "good enough to ship" to "good enough to defend in court."

## The Cost of Reproducibility

Reproducibility is expensive. You store every model checkpoint used in a decision-point eval, which can be tens of gigabytes per checkpoint. You store every eval dataset version, which can be hundreds of megabytes. You store every container image, which can be several gigabytes. You store every set of eval outputs, which can be megabytes of JSON per run. A mature eval system in a regulated industry might generate a terabyte of snapshot data per quarter. The storage cost, at cloud object storage rates, runs to thousands of dollars per year, and that is before accounting for the compute cost of running reproducibility tests.

The operational cost is also significant. You cannot use the latest and greatest judge model via a floating API alias. You must pin to dated versions, which means you lag behind improvements and you must periodically migrate and re-baseline. You cannot casually update your golden set. Every change requires versioning, changelog updates, and threshold re-evaluation. You cannot upgrade dependencies without testing that the upgrade does not break reproducibility. The pace of iteration slows because every change has compliance overhead.

But the cost of non-reproducibility is higher. A failed audit delays product launch by months. A regulatory rejection blocks market entry. A lawsuit over an AI system's behavior can demand proof that the system was validated, and if you cannot reproduce the validation, you lose the case. The teams that pay for reproducibility upfront avoid these catastrophic costs later. The teams that skip it pay in ways that are harder to predict but far more painful.

Reproducibility also has a hidden benefit: it forces clarity. If you cannot reproduce an eval, it means you do not fully understand what the eval is doing. You do not know all the dependencies. You do not know all the variables. You do not know all the ways the system can change. The discipline of building reproducibility surfaces these gaps and makes the eval system more reliable, even for teams not subject to regulation. Reproducibility is a proxy for rigor, and rigor is what separates professional AI engineering from guesswork.

---

Reproducibility ensures your eval results can be trusted, but it creates a second challenge: how do you manage version changes across models, datasets, judges, and thresholds without breaking historical comparisons? That is the domain of eval versioning and change control.
