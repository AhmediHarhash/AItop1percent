# 15.10 — When Evals Lie: The Measurement-Reality Gap

The dashboard was perfect. Accuracy at 94%, latency under 200 milliseconds, pass rate climbing steadily from 78% to 91% over six weeks. The product team at a healthcare platform launched their medical triage assistant to general availability in December 2025, confident their eval system had validated the model through rigorous testing. Three weeks into production, emergency room physicians began reporting something the metrics had missed entirely: the assistant was routing patients with chest pain to general practitioners instead of emergency cardiology. The severity classification eval measured whether the model assigned a severity score from one to five. It passed every time. What it didn't measure was whether a score of three for chest pain was clinically appropriate. The eval was accurate. The model was dangerous. The measurement told the truth about the wrong thing.

This is the **measurement-reality gap** — the space between what your eval measures and what actually matters in production. Every eval system lives in this gap. The question is not whether the gap exists. The question is whether you know where it is, how wide it is, and what falls through it.

## Goodhart's Law in Evaluation Systems

Goodhart's Law states that when a measure becomes a target, it ceases to be a good measure. In eval pipelines, this manifests as optimization toward the metric at the expense of the underlying goal. You build an eval that measures response relevance by checking whether the output contains keywords from the query. The model learns to stuff outputs with query terms regardless of whether the answer is actually useful. Your relevance score climbs to 96%. User satisfaction drops to 63%. The eval improved. The product degraded.

This happens because evals measure proxies, not outcomes. **Proxy metrics** are the observable signals you can capture automatically — keyword overlap, sentence similarity scores, format compliance, API response codes. **Outcome metrics** are what users actually experience — whether they got the information they needed, whether they trusted the response enough to act on it, whether the system saved them time or cost them money. Proxy metrics are cheap and fast to collect. Outcome metrics require user feedback, production telemetry, and sometimes weeks of observation. So teams optimize for proxies and assume outcomes will follow. Sometimes they do. Often they don't. The measurement-reality gap is the distance between those two assumptions.

The healthcare triage system had a severity classification eval that measured format compliance, score range validity, and internal consistency. All proxies. None of them asked whether the severity score matched clinical guidelines for the presenting symptoms. That would have required medical expertise encoded into the eval logic, reference ranges for dozens of symptom combinations, and a grounding dataset built by actual physicians. The team assumed that if the model produced valid severity scores, those scores would be clinically appropriate. The proxy metric said yes. Reality said no.

## When Evals Measure the Wrong Dimension Entirely

The gap widens when your eval measures a dimension orthogonal to the failure mode. A financial document summarization system at a European investment bank had a comprehensive eval suite covering summary length, entity extraction accuracy, and factual consistency against source documents. In March 2025, a compliance review discovered that 11% of summaries omitted risk disclosures that appeared in the original filings — not because the model couldn't extract them, but because it deemed them less important than revenue figures and growth projections. The eval measured whether facts in the summary were correct. It never measured whether critical facts were missing.

This is the **omission problem** in eval design. Your eval checks what the model said. It rarely checks what the model didn't say. Measuring recall requires knowing the full set of correct elements that should appear. That requires reference answers or structured ground truth. Most teams only have ground truth for happy-path cases. They don't have comprehensive coverage of everything that could go wrong. So they measure precision — are the things the model said correct — and ignore recall — did the model say all the things it should have said. Precision evals catch fabrication. They miss silent omissions. The measurement looks good. The gaps in output go undetected.

The investment bank added a mandatory disclosure checklist to their eval pipeline. Every summary was checked against a regulatory keyword list: risk factors, legal proceedings, going concern warnings, material changes. If any of those terms appeared in the source document but not in the summary, the eval failed. This didn't solve the problem completely — some omissions were conceptual, not keyword-based — but it closed the most dangerous gap. The metric no longer lied about the dimension that mattered most.

## Silent Failures and the False Negative Trap

The healthcare triage assistant had another problem the dashboard missed: it was failing silently on 6% of queries. When the model encountered ambiguous symptom descriptions — "I feel off," "something's not right," "just tired all the time" — it returned a generic response suggesting the user contact their primary care provider. The response was grammatically correct, tone-appropriate, and passed every format check. The eval marked it as successful. The user received no triage, no severity assessment, and no routing. The system failed to perform its core function, but the eval reported a pass.

This is the **false negative trap** — cases where the model produces output that looks superficially acceptable but fails to deliver value. The eval can't detect the failure because it doesn't know what value looks like in context. It checks syntax, format, and surface-level correctness. It doesn't check whether the output actually solved the problem the user asked about. That would require understanding user intent, domain semantics, and the task's real purpose. Most evals don't have that level of reasoning. They check patterns. Patterns can be satisfied by useless output.

The opposite problem — false positives, where the eval flags good output as bad — is easier to detect because it creates user friction. When your eval rejects a correct response, someone complains. When your eval accepts a useless response, the failure is silent. Users disengage quietly. Metrics look stable. The measurement-reality gap grows invisibly. By the time you notice through production telemetry or user feedback, months of bad outputs have already shipped.

## The Metrics That Don't Degrade When Quality Does

A customer support automation platform at a SaaS company had eval metrics that stayed flat even as user satisfaction declined. The model's accuracy on their intent classification benchmark held steady at 89%. Response time remained under 300 milliseconds. Format compliance stayed above 98%. But user escalation rates — the percentage of conversations transferred to human agents — climbed from 14% to 31% over eight weeks in late 2025. The eval metrics didn't degrade because the model's technical capabilities hadn't changed. What changed was the distribution of user queries. The benchmark dataset, built in early 2024, reflected the product's feature set from that time. By late 2025, the product had added new workflows, new integrations, and new terminology. Users asked about features the model had never been trained on. It produced responses that were confidently wrong or uselessly generic. The eval still measured performance on the old distribution. It reported stability. The product experienced failure.

This is **distribution drift** in the eval layer. Your eval dataset becomes a snapshot of the past. If it's not updated to reflect current production patterns, it measures performance on problems you no longer face while ignoring problems you do face. The measurement becomes decoupled from reality. The numbers stay green while the user experience degrades. You need a mechanism to detect when your eval distribution has diverged from your production distribution. That requires ongoing sampling of production data, comparison of feature distributions, and regular eval dataset refresh cycles. Most teams refresh their training data. Fewer refresh their eval data. The eval becomes a historical artifact pretending to be a current measurement.

## Closing the Gap with Hybrid Measurement

The measurement-reality gap can't be eliminated, but it can be narrowed. The teams that do this well use **hybrid measurement**: automated evals for speed and coverage, human evals for grounding and alignment, production telemetry for reality checks. The automated evals catch regressions in technical dimensions — format, speed, consistency. Human evals validate that outputs are actually useful, appropriate, and aligned with user intent. Production telemetry measures outcomes — user satisfaction, task completion, escalation rates, time spent per interaction.

Each layer measures a different part of the stack. Automated evals measure what the model produced. Human evals measure whether a human would accept it. Production telemetry measures whether users actually benefited. When all three align, you have confidence. When they diverge, you have a signal that one of your measurement layers is lying. The healthcare triage team added clinical expert review to 5% of outputs, sampled randomly, scored on a binary scale: would you trust this routing decision for a family member. That single question surfaced the chest pain misclassification problem within the first week of review. The automated evals never caught it because they were measuring the wrong thing. The human eval closed the gap.

The hardest part of building eval systems is not automating the checks. It's knowing what to check, how to check it, and when the check itself becomes the problem. Metrics are tools, not truth. They tell you what you asked them to measure. If you asked the wrong question, the answer will be technically correct and practically useless. The next subchapter covers the design principles that keep eval systems honest, reproducible, and resistant to the drift that creates measurement-reality gaps in the first place.
