# 15.5 — Sampling Strategy: Head, Tail, and Adversarial Cases

Tail cases matter more than head cases. The common paths through your system are where volume lives, but the rare paths are where trust breaks. A model that handles 98% of queries correctly but fails on the 2% of edge cases loses user confidence faster than a model that handles 92% correctly but degrades predictably. Users forgive mediocrity across the board. They do not forgive excellence that randomly collapses. Your sampling strategy determines whether your eval pipeline measures the easy wins or the hard truths.

Most teams sample eval cases the way they sample production data: proportional to frequency. If 60% of production queries are simple lookups, 60% of eval cases are simple lookups. If 3% of queries involve multi-step reasoning, 3% of eval cases test multi-step reasoning. This produces an eval suite that mirrors production distribution. It also produces an eval suite that spends most of its effort confirming what you already know and almost no effort testing what might break. Proportional sampling optimizes for confidence in the average case. It does not optimize for discovering failure before users do.

## Head Cases: The Common Paths That Define Baseline Quality

Head cases are the most frequent user interactions. If your system is a document Q&A tool, head cases are the straightforward questions users ask most often: What is the refund policy? When does the warranty expire? How do I reset my password? These are the queries that appear in the top 20% of your production logs and account for 80% of volume. Head cases define your baseline quality. If the model fails on common queries, the system is not production-ready.

Your eval pipeline needs head case coverage, but not proportional coverage. If 60% of production traffic is head cases, your eval suite does not need 60% head case test cases. Head cases are stable. They do not change much sprint to sprint. Once you have confidence the model handles them, you need maintenance coverage to detect regression, not exhaustive coverage to discover new failure modes. Allocate 30-40% of your eval budget to head cases. Enough to catch regression. Not so much that you ignore everything else.

Sample head cases to cover query diversity, not query frequency. If the top head case appears 10,000 times per day and the twentieth head case appears 500 times per day, you do not need 20x more test cases for the top query. You need a few cases for each distinct head pattern. Diversity beats volume. Ten test cases covering ten different head patterns teach you more than 100 test cases testing the same pattern with minor variations.

Refresh head cases quarterly. User behavior shifts. The queries that were common six months ago might be rare now. New features introduce new common paths. If your eval suite still tests last year's head cases, you are measuring a system that no longer exists. Head case sampling is not a one-time exercise. It is a maintenance discipline.

## Tail Cases: The Rare Paths Where Models Break Silently

Tail cases are the infrequent interactions that stress the model in ways head cases do not. Ambiguous questions. Questions that require combining information from multiple sources. Questions phrased in unexpected ways. Questions that assume context the model does not have. Questions in mixed languages. Questions with typos, slang, or domain-specific jargon. Tail cases appear in the bottom 80% of your frequency distribution and account for 20% of traffic. They also account for 80% of user frustration.

Tail cases reveal model brittleness. A model trained and evaluated on head cases learns to handle common patterns. It does not learn to generalize. When a tail case appears, the model guesses. Sometimes the guess is right. Sometimes it hallucinates. Sometimes it refuses when it should answer or answers when it should refuse. Tail case performance is the delta between a system users trust and a system users tolerate until a better option appears.

Allocate 30-40% of your eval budget to tail cases. Sample tail cases to maximize coverage of edge conditions, not to mirror production frequency. If a specific query pattern appears once per thousand interactions, you still need test cases for it. One test case is enough to detect the failure mode. Zero test cases means you ship blind.

Identify tail cases through production log analysis, user research, and adversarial thinking. Production logs show you the rare queries users already tried. User research shows you the queries users wanted to try but abandoned because the system failed. Adversarial thinking shows you the queries users have not tried yet but will try eventually: the malicious prompt, the out-of-distribution input, the request that combines two capabilities in a way you never tested. All three sources feed your tail case sample.

Group tail cases by failure mode, not by query type. A tail case where the model hallucinates, a tail case where the model refuses incorrectly, and a tail case where the model leaks PII are three different risks. Even if all three cases are rare, they need separate coverage because they break in different ways and require different mitigations. Tail case sampling is risk-based sampling. You are not trying to mirror production. You are trying to find the breaks before users do.

## Adversarial Cases: Inputs Designed to Trigger Failure

Adversarial cases are test inputs crafted specifically to exploit model weaknesses. They do not appear in production logs because users have not tried them yet, or because the system blocked them, or because the users who would try them have not found your product yet. Adversarial cases assume an intelligent attacker, a confused user, or an unlucky edge case. They test whether your defenses work when someone tries to break them.

Adversarial sampling covers three categories: safety, robustness, and capability boundaries. Safety adversarials test jailbreaks, prompt injections, PII extraction, policy violations, harmful content generation. Robustness adversarials test malformed inputs, encoding tricks, language mixing, extremely long inputs, inputs designed to maximize latency or cost. Capability boundary adversarials test queries the model should refuse because they are out of scope, queries that require knowledge the model does not have, queries that require reasoning the model cannot perform.

Allocate 20-30% of your eval budget to adversarial cases. This is not paranoia. This is the only way to know your system fails gracefully under adversarial pressure. A model that passes all head cases and most tail cases can still be trivially jailbroken if you never tested jailbreak resistance. Adversarial coverage is insurance. You pay the cost upfront to avoid the catastrophic loss later.

Build adversarial cases using red-teaming, public attack libraries, and domain-specific threat modeling. Red-teaming means asking someone to break your system on purpose. Give them access to the model, give them time, give them permission to try every attack they can imagine. Public attack libraries like Garak, HarmBench, and red-teaming prompt collections provide hundreds of known jailbreak and extraction techniques. Domain-specific threat modeling asks: what are the worst-case inputs for this specific use case? A medical chatbot's worst case is not the same as a code assistant's worst case. Tailor adversarial cases to the risks your system actually faces.

Refresh adversarial cases continuously. New attacks emerge every month. What worked as a defense in October fails in December. If your adversarial eval suite is six months old, you are testing last year's attacks. Adversarial sampling is an arms race. You do not win by testing once. You win by testing continuously and updating defenses as attacks evolve.

## Sampling Ratios: Balancing Coverage Across Case Types

A starting ratio for most systems: 35% head cases, 35% tail cases, 30% adversarial cases. This is not a rule. It is a baseline. Adjust based on risk profile, maturity, and production data. A mature system with stable head case performance can reduce head case sampling to 20% and increase tail and adversarial coverage. A new system with unstable baselines might start at 50% head cases until baseline quality stabilizes.

Tier 1 systems need higher adversarial ratios. If your model handles regulated data, financial transactions, or safety-critical decisions, adversarial cases should be 40-50% of your eval budget. The cost of adversarial failure is catastrophic. Spend eval resources proportional to the risk. Tier 3 systems can reduce adversarial coverage to 10-15% if failure impact is low and mitigations are strong.

Track sampling ratios as a metric. Your eval dashboard should show: percentage of test cases in each category, pass rates by category, how the ratio has shifted over time. If your adversarial pass rate is 60% and your head case pass rate is 95%, you have a problem. The system works when users are cooperative and breaks when they are not. That is not production-ready. Sampling ratios make that gap visible.

Rebalance sampling every quarter based on production failure data. If production incidents cluster in tail cases, increase tail case sampling. If adversarial attacks are succeeding in the wild, increase adversarial sampling. Let production teach you where your eval coverage is weak and adjust. Sampling strategy is not static. It evolves as the system, the users, and the threat landscape evolve.

## Distribution Balance: Matching Eval Cases to Real-World Variability

Sampling strategy is not just about case types. It is also about input variability within each type. A head case eval suite that only tests well-formed English queries with perfect grammar does not match production, where head cases arrive with typos, abbreviations, autocorrect errors, and mixed languages. Your eval cases need to reflect the variability users actually produce.

Sample inputs across language, tone, length, format, and user intent. If your system supports multiple languages, every case type needs multilingual coverage. If users interact via voice, text, and API, every case type needs coverage for all three modalities. If user queries range from five words to five paragraphs, your eval suite needs both extremes and everything in between. Distribution balance ensures you are not just testing ideal inputs. You are testing the messy, inconsistent, unpredictable inputs production delivers.

Use synthetic generation to fill distribution gaps. If your production logs have ten examples of a rare tail case but you need fifty to feel confident, generate synthetic variations. Vary phrasing, swap entities, introduce typos, translate to other languages, rephrase with different tone. Synthetic cases are not as valuable as real user inputs, but they are infinitely more valuable than no coverage. Generation lets you achieve distribution balance without waiting years for production logs to fill in the gaps.

Validate synthetic cases with domain experts. Synthetic cases can drift into nonsense if generation is unconstrained. Have a human review a sample of generated cases to confirm they are realistic, representative, and testing what you think they test. Synthetic quality matters. A hundred synthetic cases that test nothing teach you nothing.

Your sampling strategy determines what your eval pipeline measures. Measure the easy cases and you will ship a system that works until it does not. Measure the hard cases and you will ship a system users trust. The next step is defining what "passing" means — because a test suite without SLOs is just a dashboard with no decision criteria.

