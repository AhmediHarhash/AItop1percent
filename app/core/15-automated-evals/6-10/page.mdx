# 6.10 — End-to-End Scenario Evaluation

The test suite runs every night at 2am. Twenty-seven end-to-end scenarios execute in sequence, each simulating a complete user journey through the customer support agent. Reservation changes. Refund requests. Multi-turn troubleshooting conversations. The agent calls four different APIs, searches a knowledge base, and hands off to humans when needed. Every scenario passes. The dashboard shows green. Three days later, production breaks when a user asks to change a flight and add baggage in the same conversation — a workflow that hits three APIs in a specific order that no test scenario ever covered. The system had hundreds of unit tests and dozens of integration tests. It had zero end-to-end tests that actually mirrored what real users do.

**End-to-end scenario evaluation** is the discipline of testing AI systems through complete, realistic user journeys that exercise every component in the production configuration. Not isolated model calls. Not mocked dependencies. The full system, from user input to final output, through every retrieval step, every API call, every decision point, exactly as it runs in production. This is where you discover the failures that only emerge when the pieces interact — when retrieval returns partial context and the model makes an API call based on incomplete information, when the first tool call succeeds but the second one times out and the agent enters a recovery loop it never trained for, when the conversation context grows large enough that the routing logic starts dropping critical state.

End-to-end evaluation is not the same as integration testing, though teams confuse the two constantly. Integration tests verify that component A can talk to component B. End-to-end tests verify that a realistic user workflow produces the correct outcome when all components work together. The distinction matters because most production failures in agentic systems happen not at component boundaries but in the interaction of three or more components under realistic load, context size, and timing conditions.

## The Coverage Gap Between Integration and End-to-End

Integration tests give you confidence that your retrieval system can return documents and your model can consume them. They give you confidence that your agent can call your APIs and your APIs can return results. They do not give you confidence that a user asking a complex multi-step question will get the right answer. That confidence comes only from end-to-end scenarios that replicate the full complexity of real usage.

The gap shows up in three places. First, integration tests typically use simplified inputs designed to test the interface contract. End-to-end scenarios use realistic inputs with all the ambiguity, context dependencies, and edge cases that real users introduce. A user asking to reschedule a meeting does not say "reschedule meeting ID 47392 to 3pm next Tuesday." They say "can we move the thing with Sarah to sometime next week, maybe afternoon?" Your integration test verifies that the rescheduling API works when given a meeting ID and a timestamp. Your end-to-end test verifies that the agent can extract the meeting ID from conversation history, infer the participant from the name Sarah, resolve "next week maybe afternoon" to candidate timeslots, check availability, and execute the change — then confirm the outcome in natural language that the user understands.

Second, integration tests run components in isolation or in pairs. End-to-end scenarios exercise the full dependency chain. A customer asking for a refund might trigger retrieval to check the refund policy, an API call to verify purchase details, a second API call to check if the return window is still open, a third API call to issue the refund, and a knowledge base lookup to generate return shipping instructions. Each integration might pass in isolation. The end-to-end scenario reveals that when the second API call times out, the agent retries it three times, burns through the latency budget, and returns a partial answer that tells the user the refund is approved but omits the shipping instructions because the final retrieval step got skipped when the timeout shortened the remaining execution window.

Third, integration tests do not accumulate context across multiple turns. End-to-end scenarios do. Many agent failures happen in turn three or turn five of a conversation, after the context window has filled with prior exchanges, retrieved documents, and tool call results. The model starts dropping information. The routing logic degrades. The agent loses track of which APIs it already called. You do not see this in a two-turn integration test. You see it in a ten-turn end-to-end scenario where the user asks a follow-up question that depends on information from turn two, and the agent has already evicted that context to make room for the search results from turn four.

## Designing Realistic End-to-End Scenarios

An end-to-end scenario is a complete user journey with a defined starting state, a sequence of realistic inputs, and a set of observable outcomes that indicate success. The scenario should mirror production usage in complexity, ambiguity, and dependency patterns. This means you need real conversational flows, not artificial test scripts. You need realistic starting conditions, not clean-slate environments. You need success criteria that measure the full outcome, not just the final model response.

Start with production logs. The best end-to-end scenarios come directly from real user sessions. Find conversations that represent common workflows. Find conversations that exposed edge cases. Find conversations where users asked follow-up questions that required the agent to maintain context across multiple turns. Strip out any sensitive data, preserve the conversational structure, and turn each session into a test scenario. You now have inputs that match the linguistic complexity, the ambiguity, and the context dependencies that your system actually faces in production.

Each scenario needs a starting state that mirrors realistic conditions. If your agent operates on a knowledge base that changes daily, your scenario needs to specify the knowledge base snapshot it expects. If your agent calls APIs that return user-specific data, your scenario needs a test user account with the relevant order history, subscription status, or profile settings already in place. If your agent maintains conversation history, some scenarios should start mid-conversation with prior turns already in context. Clean-slate starting conditions are easier to set up, but they miss entire classes of failures that only emerge when the system is already carrying state.

Define observable outcomes at multiple levels. The simplest outcome is the final response — does the agent give the correct answer, execute the correct action, or route to the correct destination? But end-to-end scenarios should also verify intermediate steps. Did the agent retrieve the right documents? Did it call the right APIs in the right order? Did it handle ambiguity by asking a clarification question before taking action? Did it stay within latency bounds? Did it avoid calling expensive APIs when cheaper retrieval would suffice? Each observable outcome is a point of verification. The final response might be correct, but if the agent called six APIs when two would have been enough, the scenario reveals a cost and latency problem that your output-only eval would miss.

## Mocking Versus Live Dependencies in End-to-End Tests

End-to-end evaluation requires dependencies — retrieval systems, APIs, databases, third-party services. You have two choices: run against live production dependencies or mock them. Each approach has failure modes. Live dependencies give you the most realistic test environment, but they introduce flakiness, data drift, and cost. Mocked dependencies give you control and repeatability, but they can diverge from production behavior in ways that make your tests pass while production fails.

Most teams start with live dependencies and then selectively introduce mocking as they hit pain points. The pain points are predictable. Third-party APIs rate-limit your test runs. Database queries return different results as production data changes, breaking tests that expect specific outcomes. Nightly test runs fail intermittently because an external service had a five-minute outage at 2am. The cost of running full end-to-end tests against production APIs becomes prohibitive when each scenario makes twenty API calls and you have a hundred scenarios.

Mocking makes tests stable and fast, but only if the mocks stay synchronized with production behavior. The worst outcome is a test suite that passes confidently while mocking an API response format that changed three weeks ago. Your end-to-end tests show green. Your production agent breaks because it expected a field that the API no longer returns. The mocks gave you false confidence.

The stable pattern is a hybrid approach. Use live dependencies for the critical path and mock the periphery. If your agent's core function is searching a knowledge base and generating answers, run end-to-end tests against a real knowledge base snapshot in a staging environment. If the agent also calls a currency conversion API and a time zone lookup service, mock those — they are less likely to cause subtle behavioral issues, and their response formats are stable. If an API is known to be flaky or slow, mock it, but run a separate integration test daily that calls the real API with a simple request and verifies the response format has not changed. That test acts as a smoke detector. When it fails, you know the mock needs updating.

For APIs you control, use contract testing to keep mocks aligned. The API publishes a contract — a schema defining request and response formats. Your mock implements that contract. Your API's own test suite verifies it still honors the contract. When the contract changes, both the API test suite and the mock need updates, and that dependency is explicit. You still get the speed and stability of mocking without the silent divergence risk.

## Handling State and Data Setup in End-to-End Scenarios

End-to-end scenarios need realistic starting data. If your agent answers questions about a user's order history, the test scenario needs a test user account with realistic orders. If your agent schedules meetings, the scenario needs a calendar with existing events that might create conflicts. If your agent processes support tickets, the scenario needs a test account with an open ticket, a conversation history, and associated metadata. Setting up this state is the operational cost of end-to-end testing, and teams that skip it end up with scenarios that only test the happy path with empty starting conditions.

The simplest approach is a static test fixture — a database snapshot or a set of seed data that gets loaded before each test run. Every scenario runs against the same starting state. This works when your scenarios are independent and when the starting state does not need to be large or complex. It stops working when scenarios modify state — if one scenario closes a ticket and the next scenario expects that ticket to be open, they interfere with each other. It also stops working when realistic starting state is too large to manage as a static fixture. A customer support agent might need access to a knowledge base with ten thousand articles. Recreating that from scratch before every test run is not viable.

The more scalable approach is isolated test environments per scenario, created on-demand. Each scenario gets its own database instance, its own user account, its own knowledge base snapshot. The scenario runs, produces a result, and the environment is torn down. This requires infrastructure — containerized databases, ephemeral knowledge base replicas, API endpoints that can be spun up per-test. The operational cost is higher, but the payoff is that scenarios can run in parallel without interfering with each other, and you can test realistic data volumes without maintaining massive static fixtures.

Some systems cannot be fully isolated. If your agent calls third-party APIs, you cannot spin up a separate instance of that service per test. If your retrieval system depends on a vector database that takes hours to index, you cannot rebuild it for every scenario. In those cases, use shared staging infrastructure but design scenarios to operate on non-overlapping data slices. One scenario works with customer ID 10001, another with customer ID 10002. One scenario works with orders from January, another with orders from February. Overlap is the source of flakiness. If two scenarios try to modify the same record, whichever runs second sees unexpected starting state and fails intermittently.

## Defining Success Criteria for End-to-End Scenarios

An end-to-end scenario is not just input and output. It is a set of observable behaviors that define success. The final response is one observable. The path the agent took to get there — which APIs it called, which documents it retrieved, how many turns the conversation required — is another. Latency is another. Cost is another. A scenario might produce the correct answer but fail because it took twelve seconds when the requirement is under five, or because it burned three dollars in API costs when the expected cost is under fifty cents.

Success criteria should cover the outcome, the process, and the constraints. Outcome criteria verify correctness — did the agent give the right answer, take the right action, escalate to a human when appropriate? Process criteria verify the path — did it retrieve the right documents, call the right APIs, avoid redundant calls, ask clarifying questions before making assumptions? Constraint criteria verify latency, cost, and safety — did it stay under the latency budget, stay under the cost budget, avoid making destructive API calls without confirmation?

For many scenarios, correctness is not binary. A user asks for a restaurant recommendation. There is no single correct answer. The scenario passes if the agent returns restaurants that match the user's stated preferences — cuisine type, location, price range. It fails if it recommends a closed restaurant, a restaurant in the wrong city, or a restaurant that does not serve the requested cuisine. The success criteria need to encode this flexibility. Instead of asserting that the response exactly matches a reference answer, you verify that the response contains a restaurant name, that the restaurant is in the correct city, that it matches the cuisine filter, and that it is currently open. Each of these is a separate assertion, and the scenario passes if all of them hold.

Some scenarios test negative cases — the agent should refuse to answer, should escalate, should ask for more information. A user asks the agent to delete their account. The agent should not execute that action without confirmation. The success criteria verify that the agent did not call the account deletion API, that it asked the user to confirm, and that it explained the consequences of deletion. A user asks a medical question that the agent is not authorized to answer. The success criteria verify that the agent declined, that it explained why, and that it suggested an appropriate alternative — like directing the user to consult a healthcare professional.

Latency criteria are common but often implemented wrong. Teams set a flat timeout — if the scenario takes longer than ten seconds, it fails. This works for simple scenarios but breaks down for complex ones. A ten-turn conversation scenario might legitimately take thirty seconds if each turn involves retrieval and an API call. The better approach is per-turn latency budgets. Each turn should complete in under three seconds. If any single turn exceeds that, the scenario fails, even if the total runtime is under thirty seconds. This catches the specific turn where the agent made an expensive API call or retrieved too many documents, rather than just signaling that something, somewhere, was slow.

Cost criteria are rare but increasingly necessary. If your agent calls paid APIs or uses expensive models, scenarios should track cumulative cost and fail if it exceeds a threshold. A scenario that costs five dollars to run is a signal that something is wrong — the agent called the same API ten times instead of once, or it routed every query to GPT-5 when GPT-5-mini would have been sufficient. Cost thresholds surface inefficiency that you would not notice from output correctness alone.

## The Integration Test Versus End-to-End Distinction in Practice

Integration tests and end-to-end tests both run multiple components together, which is why teams conflate them. The distinction is scope and realism. Integration tests verify that two or three components can communicate correctly under controlled conditions. End-to-end tests verify that a realistic user workflow succeeds when all components interact under production-like conditions.

An integration test verifies that your agent can call the order lookup API and parse the response. It sends a test request with a known order ID, receives a response, and asserts that the agent extracted the order total correctly. The test uses a mocked API response or a test API endpoint that always returns the same data. It runs in under a second. It tells you that the interface contract is honored.

An end-to-end scenario verifies that a user asking "what did I order last week?" gets the correct answer when the agent has to interpret the question, map "last week" to a date range, call the order lookup API with that date range, handle pagination if the user has many orders, rank the results by date, and present the most recent order in natural language. The scenario runs against a test user account with realistic order history. It exercises conversation context, date parsing, API pagination, and response generation. It takes five seconds. It tells you that the entire workflow produces the correct outcome for a realistic input.

You need both. Integration tests catch interface breaks early and run fast enough to execute on every commit. End-to-end tests catch workflow failures that only emerge when the full system runs together, but they are slower and more expensive, so they run less frequently — nightly, or on every release candidate. The failure is running only integration tests and assuming that if the interfaces work, the workflows will too. They will not. The failure is also running only end-to-end tests and then spending hours debugging which component caused a failure because you have no integration tests to narrow it down. You need the fast feedback loop of integration tests and the realism of end-to-end tests.

The boundary between the two shifts as your system matures. Early in development, you might have only integration tests because you are still validating that components can talk to each other at all. As the system stabilizes, you add end-to-end scenarios for the most common workflows. As the system reaches production, you expand end-to-end coverage to include edge cases, multi-turn conversations, and failure recovery paths. By the time the system is mature, your end-to-end suite is the primary signal of production readiness, and your integration suite is the early warning system that catches breaks before they reach end-to-end tests.

## Running End-to-End Scenarios at Scale

A handful of end-to-end scenarios run fine on a developer's laptop. A hundred scenarios running against live dependencies in a staging environment require infrastructure. You need orchestration to run scenarios in parallel. You need environment management to spin up and tear down test instances. You need result aggregation to surface which scenarios failed and why. You need cost controls to prevent runaway API bills when a bug causes a scenario to loop indefinitely.

Parallelization is the first scaling requirement. If each scenario takes ten seconds and you have a hundred scenarios, serial execution takes sixteen minutes. Parallel execution across ten workers brings it down to under two minutes. But parallelization introduces coordination problems. If scenarios share infrastructure — a staging database, a shared knowledge base — running them in parallel causes collisions. One scenario modifies a record that another scenario is reading. Tests become flaky. The solution is either full isolation — each scenario gets its own environment — or careful partitioning so scenarios operate on non-overlapping data.

Timeouts and cost limits are the second scaling requirement. A bug in your agent or in a scenario can cause infinite loops. The agent gets stuck in a retry cycle. A tool call fails, the agent retries, the retry fails, the agent retries again, forever. Without a timeout, the scenario runs until it exhausts your API quota or your test infrastructure crashes. Every scenario needs a maximum runtime — if it has not completed in five minutes, kill it and mark it failed. Every scenario also needs a cost cap — if it has consumed more than ten dollars in API calls, kill it. These caps prevent one broken scenario from taking down your entire test suite or your entire AWS bill.

Result storage and analysis matter as scenarios proliferate. When you have ten scenarios, you can read the output logs manually and see what failed. When you have a hundred, you need structured result storage. Each scenario run produces a result record — pass or fail, runtime, cost, which assertions passed, which failed, any error logs. These records go into a database. You query them to see trends. Is scenario 47 flaky — does it fail intermittently? Is scenario 82 getting slower over time? Did the last code change cause five scenarios that were passing to start failing? The test suite becomes a data source, and you need the tooling to analyze it like one.

## When End-to-End Scenarios Fail in Production Anyway

End-to-end scenarios reduce production failures. They do not eliminate them. You will still see failures in production that your scenarios did not catch. This happens for three reasons. First, production has usage patterns your scenarios did not cover. A user asks a question in a way no one anticipated. A user combines two features in a sequence your scenarios never tested. A user operates in a locale, language, or time zone that your test data did not include. The coverage is never complete.

Second, production has scale and concurrency that staging does not. An agent might work perfectly for one user and break when a hundred users hit it simultaneously because the retrieval system starts throttling, the API rate limits kick in, or the database connection pool exhausts. Your end-to-end scenarios run serially or with low parallelism. They do not replicate the load conditions that cause these failures.

Third, production has data drift. The knowledge base changes. The APIs change. The user behavior changes. Your end-to-end scenarios run against a fixed snapshot of test data in a staging environment. They pass. Three months later, production breaks because the knowledge base article your agent relied on was updated, and the new version uses different terminology that the retrieval system does not match as effectively. Your scenarios did not catch it because they still run against the old snapshot.

The response to these gaps is not more end-to-end scenarios. At some point, adding more scenarios has diminishing returns. The response is production monitoring that detects the failures your scenarios missed, and a feedback loop that turns production failures into new scenarios. Every time production breaks in a way your tests did not predict, you write a new end-to-end scenario that replicates that failure. Over time, your scenario suite becomes a catalog of every failure mode you have ever seen. It does not prevent new failure modes, but it prevents old ones from recurring.

End-to-end scenario evaluation is the final verification layer before production. It tests the system the way users will experience it — full workflows, realistic inputs, real dependencies, observable outcomes. It is slower, more expensive, and more complex than unit tests or integration tests, but it is the only evaluation method that gives you confidence that the complete system works. The teams that skip it ship faster in the short term and spend months debugging production failures that realistic end-to-end testing would have surfaced in staging.

The next question is coverage — how much of your agent's behavior have you actually tested, and how do you know when you have tested enough? That is what behavioral eval coverage addresses.

