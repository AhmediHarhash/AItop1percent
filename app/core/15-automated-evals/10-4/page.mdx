# 10.4 — Trend Analysis and Historical Comparison

The team is staring at a dashboard showing current eval metrics. Everything looks fine. Pass rate at 94%, latency median at 340ms, cost per query at 4.2 cents. All within acceptable ranges. But someone asks to see the thirty-day trend. The graph loads and the room goes quiet. The pass rate has dropped six percentage points over three weeks. Latency has crept up 80 milliseconds. Cost has increased eighteen percent. None of the individual daily changes was large enough to trigger an alert. But the cumulative drift is unmistakable. The model is degrading, and without trend analysis, no one would have noticed until users started complaining.

**Trend analysis** is the practice of tracking eval metrics over time to detect patterns that single-point measurements miss. A metric reading taken today tells you the current state. A metric tracked across weeks or months tells you the direction. In automated eval pipelines, trend analysis is what separates reactive firefighting from proactive system management. You catch degradation while it is still gradual. You spot improvements from recent changes. You identify seasonality, cyclical patterns, and long-term drift that would be invisible in daily snapshots.

## The Metrics That Need Trending

Not every metric requires historical tracking with the same intensity. Some metrics are inherently trend-sensitive. Others are more useful as point-in-time indicators. **Quality metrics** — pass rate, task-specific scores, correctness percentages — are critical to trend. A model that passes 97% of cases today but was passing 99% two months ago is degrading, even if 97% still meets your threshold. The trend reveals the trajectory. Without it, you only see the moment.

**Performance metrics** — latency at p50, p95, p99, throughput, time-to-first-token — also demand trend analysis. Latency degradation is rarely sudden. It creeps. A median response time that grows from 280ms to 340ms over six weeks is a signal that something in your infrastructure, model routing, or data pipeline is changing. If you only check current latency, you miss the drift until it crosses a hard threshold. By then, you are already late.

**Cost metrics** trend differently. Cost per query, total daily spend, token usage per task — these metrics often show step changes when you switch models or change prompts, but they also show gradual drift as user behavior evolves or as your retrieval layer returns more context over time. Trending cost reveals whether your system is becoming more expensive to operate at the same quality level, which is a sustainability signal that finance and engineering both need.

**Coverage metrics** — the percentage of queries that hit each routing path, the distribution of tasks across tiers, the frequency of fallback invocations — benefit from trend analysis because they reveal how user behavior is shifting. If the percentage of high-complexity queries increases from 12% to 19% over two months, your cost and latency profiles will shift even if per-query performance stays constant. Trending coverage shows you whether your system's usage patterns are stable or evolving.

## Baseline Establishment and Rolling Windows

Trend analysis requires a **baseline** — a reference period that represents normal, acceptable performance. Without a baseline, you have no way to determine whether a trend is meaningful or just noise. The baseline can be the first week after deployment, the last stable release period, or a manually defined "golden week" where you know the system performed as intended. The baseline anchors the trend. Every future measurement is compared against it.

But baselines age. A baseline from six months ago may no longer reflect your current data distribution, user base, or model configuration. This is where **rolling windows** come in. A rolling thirty-day window recalculates the baseline every day using the most recent thirty days of data. A rolling seven-day window is more sensitive to recent changes. A rolling ninety-day window is more stable and less reactive to short-term fluctuations. The choice depends on how quickly your system evolves and how sensitive you want your trend detection to be.

Rolling windows prevent baseline drift from hiding real trends. If your system gradually improves over six months, a fixed baseline from launch will make every week look like an improvement, even if the last month has been flat or declining. A rolling window adjusts the reference point continuously, so you are always comparing against recent normal performance, not ancient history. The trade-off is that rolling baselines can obscure long-term trends if the window is too short. A seven-day rolling window will not show you a slow three-month degradation because the baseline itself is drifting downward with the metric.

## Detecting Gradual Degradation

The hardest trend to catch is gradual degradation — the slow, consistent decline in quality or performance that happens so incrementally that no single day looks wrong. A pass rate dropping from 96% to 95.8% to 95.5% to 95.1% over four weeks will not trigger a threshold alert if your alert is set at 93%. But the direction is clear. The model is getting worse, and if the trend continues, you will hit the threshold in another month.

**Slope-based detection** is one way to catch this. Instead of alerting when a metric crosses a threshold, you alert when the trend line over the last N days has a slope steeper than a defined limit. If the pass rate is declining at more than 0.3 percentage points per week, that is a signal worth investigating even if the current value is still acceptable. Slope-based alerts catch degradation early, before it becomes a crisis.

Another approach is **percentage deviation from baseline**. If the current seven-day average is more than three percent below the thirty-day rolling baseline, you trigger an investigation. This catches both sudden drops and gradual slides, as long as the baseline window is long enough to smooth out daily noise but short enough to stay relevant. The key is tuning the deviation threshold. Too sensitive and you get alerts on normal variance. Too loose and you miss real degradation until it is severe.

Gradual degradation often has an inflection point — a moment where the decline accelerates. Trending helps you spot that inflection. A metric that declined slowly for two months and then started declining faster in week nine is telling you something changed in week nine. It might be a data shift, a model update, a configuration change, or a dependency degrading. Without trend analysis, you would only notice when the decline became catastrophic. With it, you notice when the rate of decline changes, which is often more actionable than noticing the decline itself.

## Seasonality and Cyclical Patterns

Not all trends are linear. Some metrics show **seasonality** — predictable patterns that repeat on a weekly, monthly, or annual cycle. A customer support eval pipeline might see higher query volumes on Mondays and lower volumes on weekends. A financial document analysis system might see different task distributions at month-end versus mid-month. An e-commerce recommendation system might show quality shifts during holiday shopping seasons versus off-peak periods.

If you do not account for seasonality, you will confuse normal cyclical variation with real trends. A pass rate that drops every Monday and recovers by Wednesday is not degradation — it is a pattern. But if you compare Monday to Friday without accounting for the cycle, it looks like the system is failing. Seasonal decomposition techniques separate the trend component from the seasonal component, so you can see whether the underlying trend is stable even as the metric oscillates week to week.

The simplest approach is to compare like-to-like. Compare this Monday to last Monday, this month-end to last month-end. If the system is stable, those comparisons should show minimal drift. If this Monday is consistently worse than the previous four Mondays, you have a real trend, not just a weekly cycle. More sophisticated pipelines use time-series models that explicitly fit seasonal patterns and alert only when the residual — the part that is not explained by the cycle — deviates from expected.

Cyclical patterns also emerge at the hour level in systems with global users. A multilingual eval pipeline might see different language distributions during European business hours versus Asia-Pacific business hours. If your alerting does not account for this, you might trigger false positives when the hourly mix shifts in predictable ways. Trending at the hourly level with historical comparison lets you see whether 3am performance this Tuesday matches 3am performance from the last four Tuesdays, accounting for the natural variation in traffic and task mix.

## Historical Comparison for Regression Detection

Every time you deploy a new model, update a prompt, or change a retrieval configuration, you create a **comparison point**. Historical comparison means running the same eval suite before and after the change and trending the difference. If your new GPT-5.2 deployment shows a two-percentage-point drop in pass rate compared to the previous GPT-5.1 deployment on the same eval set, that is a regression. The trend line shows a step change at the deployment timestamp.

Historical comparison is most powerful when you maintain a **golden eval set** that never changes. This set becomes your benchmark. You run it against every model version, every prompt revision, every infrastructure change. The trend line across those runs shows you whether your system is improving, stable, or regressing over time. The golden set isolates the impact of your changes from the impact of data drift, because the eval inputs are constant.

But golden sets have a weakness — they do not capture distributional shift. If user queries evolve but your golden set stays frozen, the golden set trend might show stability while production performance degrades. This is why you need both a static golden set for regression detection and a dynamic eval set that samples recent production traffic. The golden set tells you whether your changes are improving the system on known cases. The dynamic set tells you whether the system is keeping up with real-world usage.

Historical comparison also reveals **improvement stagnation**. If your pass rate has been flat at 94% for three months despite five model updates and ten prompt revisions, the trend is telling you that incremental changes are no longer moving the needle. You are stuck. The flat trend is a signal to revisit your strategy — maybe you need better data, a different model, a rearchitected pipeline, or a harder look at whether 94% is actually achievable given your task definition.

## The Trend Dashboard That Actually Gets Used

A trend dashboard is only useful if people look at it. The mistake most teams make is building a dashboard with every possible trend line, every metric, every time window, all on one page. The result is visual noise. No one knows where to look. The important trends are buried next to irrelevant ones. The dashboard exists but it does not drive decisions.

The effective trend dashboard has **hierarchy**. The top level shows the three to five metrics that matter most for system health — overall pass rate, median latency, cost per query, and maybe coverage distribution. Each metric is shown as a line chart with a thirty-day rolling window and a highlighted seven-day trend. If something looks wrong at the top level, you drill down. The second level shows per-task-type breakdowns, per-model breakdowns, per-tier breakdowns. The third level shows granular metrics — individual eval criteria, specific failure modes, latency percentiles.

The dashboard highlights **deviations** automatically. If a metric is trending outside expected bounds, it gets flagged. The flag is visual — a color change, a marker on the trend line, a percentage deviation number. The user does not have to interpret the trend themselves. The dashboard interprets it for them and surfaces the anomalies. This reduces cognitive load. The person looking at the dashboard is not hunting for problems. The dashboard is showing them where the problems are.

Dashboards also need **annotations**. Every deployment, every configuration change, every data pipeline update should show up as a vertical line on the trend chart with a label. This lets you correlate changes with trend shifts. If pass rate drops the day after a prompt update, the annotation makes that correlation obvious. Without annotations, you are left guessing why the trend changed. With them, you have a hypothesis within seconds.

When a trend dashboard integrates with your alerting system, it becomes the investigation hub. An alert fires. The alert links to the relevant trend chart. The engineer opens the chart, sees the historical context, sees the annotation showing what changed, and immediately has a starting point for debugging. The dashboard does not just show the trend. It accelerates root cause analysis by connecting the trend to the timeline of system changes. That integration is what turns trend analysis from a passive monitoring tool into an active operational asset that shapes how your team responds to quality shifts in production.

The ability to detect gradual degradation, spot seasonal patterns, and compare performance across deployments transforms how you run an eval pipeline. Without it, you are flying blind between threshold breaches. With it, you see the trajectory before the crisis, which is the only way to maintain quality at scale. But seeing the trend is not enough — you need to know when the trend demands action, which is where alerting thresholds and escalation rules come in next.

---

*Next: 10.5 — Alerting Thresholds and Escalation Rules*
