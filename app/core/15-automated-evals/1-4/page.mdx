# 15.4 — Eval Coverage Strategy: Tasks, Risks, and User Journeys

In late 2025, a logistics platform spent four months building an automated eval pipeline for their AI routing assistant. They tested route accuracy, distance calculations, and delivery time predictions. Their eval suite ran 2,400 test cases every deploy. Quality metrics held steady at 94% across all checks. Two weeks after launch, support tickets spiked to 340 per day. Users were furious. The assistant handled routes perfectly but failed catastrophically at edge cases the team never measured: what happens when a driver calls in sick mid-route, when a customer changes delivery instructions after dispatch, when weather closes a highway. The eval pipeline measured tasks. It never measured risks. It never mapped user journeys. The company pulled the feature, rebuilt their eval strategy from scratch, and lost six months of roadmap progress.

You can have comprehensive task coverage and still ship something broken. Task coverage answers "does the model do what we asked?" Risk coverage answers "what breaks when the model fails?" Journey coverage answers "does this work the way users actually use it?" Most teams build eval pipelines that only address the first question. They test individual capabilities in isolation and declare victory when each one passes. Then production reveals the gaps: the model handles each step correctly but fails when steps combine, or it works for common cases but breaks on the rare ones that define user trust, or it passes technical checks but violates user expectations about how the interaction should flow.

## Task Coverage: Measuring Every Capability You Built

Task coverage is the foundation. You map every distinct capability the model performs and ensure your eval suite tests it. If your model generates summaries, answers questions, extracts entities, classifies intent, and suggests actions, you need test cases for all five. If one capability goes untested, you have no evidence it works. Task coverage is not optional. It is the floor.

Start with your task taxonomy from problem framing. Every task type you defined needs eval coverage. If you identified twelve types of user queries your chatbot must handle, your eval suite needs cases for all twelve. If some task types appear rarely in production, they still need coverage. Low-frequency tasks often carry high stakes. A support chatbot that handles password resets once per thousand conversations still needs eval cases for password resets, because when that conversation happens and fails, the user escalates to a human agent and the automation provides zero value.

Break compound tasks into atomic subtasks and measure both. If your system retrieves documents, synthesizes information, and formats a response, test retrieval accuracy, synthesis correctness, and format compliance separately. Then test the full pipeline. Subtask coverage tells you where the failure happened. End-to-end coverage tells you whether the user gets what they need. You need both. A model can pass every subtask check and still produce a final output that violates user expectations because the subtasks combine poorly.

Track task coverage as a percentage: tested tasks divided by total tasks. Aim for 100%. If you launch with 83% task coverage, you are explicitly shipping untested capabilities. That is not a risk-aware decision. That is negligence. If certain tasks are too hard to eval automatically, eval them manually or delay launch until you can test them. Untested tasks fail in production. This is not a maybe. This is how systems work.

## Risk Coverage: Mapping Failure Modes to Eval Cases

Risk coverage starts with the question: what happens when this fails? For every task, identify the failure modes that matter. A summary task can fail by omitting key information, by adding false information, by using inappropriate tone, by leaking sensitive data, by producing output too long for the UI, by generating content that violates policy. Each failure mode needs eval cases designed to detect it.

Map failure modes to user impact and business impact. Omitting key information frustrates users. Adding false information destroys trust. Leaking sensitive data creates legal exposure. Violating policy risks regulatory action. Not all failures carry the same weight. Your eval coverage should reflect that. If your model handles medical information, hallucination risk gets 10x the eval investment of tone risk. If your model interacts with children, safety risk gets 10x the coverage of latency risk. Risk-based coverage means spending eval resources proportional to the damage each failure mode causes.

Build adversarial test cases for high-risk failure modes. Adversarial cases are inputs designed to trigger the failure: edge cases, ambiguous inputs, malicious inputs, inputs that exploit known model weaknesses. If your model must refuse harmful requests, your eval suite needs hundreds of adversarial prompts testing refusal behavior. If your model must protect PII, your eval suite needs cases where PII appears in unexpected formats, in mixed languages, embedded in code, obfuscated with typos. Adversarial coverage is not paranoia. It is the only way to know your defenses work before an attacker or an unlucky user finds the gap.

Document every failure mode you are NOT testing and why. Maybe the failure is low-probability and low-impact. Maybe you have a separate mitigation like a human review step. Maybe you plan to add coverage in the next sprint. The documentation forces you to make the trade-off explicit. Undocumented blind spots are forgotten blind spots. Forgotten blind spots become production incidents.

## User Journey Coverage: Testing How People Actually Use the System

User journey coverage maps eval cases to real usage patterns. A user does not invoke one task in isolation. They have a conversation. They iterate. They switch contexts. They interrupt and resume. Your eval pipeline needs to test the flows users actually experience, not just the atomic tasks your architecture exposes.

Identify the top user journeys from product analytics, user research, or early beta data. A customer support journey might look like: user asks question, model requests clarification, user provides detail, model searches knowledge base, model provides answer, user asks follow-up. That is six turns with dependencies between them. Your eval needs multi-turn test cases that walk the full journey. A model can handle each turn perfectly in isolation but fail when context from turn two affects the response at turn five.

Journey coverage reveals failures that task coverage misses. A model that passes all single-turn question-answering evals can still fail when users ask clarifying questions, when they reference something mentioned three turns ago, when they change their mind mid-conversation, when they express frustration. Conversations are stateful. Static task evals are not. If your production system is stateful, your eval pipeline must be stateful too.

Map journeys to user segments. Power users follow different paths than new users. Users in crisis follow different paths than users browsing. If your system serves multiple personas, your journey coverage needs cases for each. A healthcare triage bot used by a panicked parent at 2am needs different journey coverage than the same bot used by a nurse doing routine follow-up. The model might handle the same medical question differently depending on the urgency signals in the conversation. If you only test the happy-path journey, you never know.

## The Coverage Map: Visualizing What You Measure and What You Miss

Build a coverage map that shows task coverage, risk coverage, and journey coverage in one view. The map is a structured document, not a diagram. For each capability: list the task, list the associated risks, list the user journeys that invoke it, list the eval cases that test it, note the coverage gaps. The map makes blind spots visible. It forces you to answer: are we testing what matters, or just what is easy to test?

Update the coverage map every sprint. As you add features, add tasks, risks, and journeys to the map. As you discover new failure modes in production, add them to the risk list and create eval cases. The map is a living document. A static map from three months ago is fiction. The system evolved. The risks evolved. The coverage map must evolve with it.

Use the coverage map in design reviews and launch reviews. When Product proposes a new feature, the first question Engineering asks is: how does this change our coverage map? What new tasks does this introduce? What new risks? What new journeys? If the team cannot answer, the feature is not ready for sprint planning. Coverage planning happens during design, not after the code ships.

Treat coverage gaps as technical debt. Every untested task, every unmeasured risk, every missing journey is debt that accumulates interest. The longer a gap exists, the more production risk it represents, the harder it becomes to close. If you ship with 80% journey coverage, you are not deferring work. You are accepting that 20% of user flows have no quality evidence. That is a choice. Make it consciously or do not make it at all.

## Coverage as a Quality Gate

Define minimum coverage thresholds for launch. A reasonable starting point: 100% task coverage, 90% risk coverage for high-severity failure modes, 80% journey coverage for top-10 user flows. These are floors, not targets. High-risk systems need higher bars. A Tier 1 medical system might require 100% coverage across all three dimensions and adversarial test suites for every risk. A Tier 3 content generation tool might accept 85% journey coverage if the missing journeys are low-traffic and low-stakes.

Enforce coverage thresholds in CI. If a pull request adds a new task without adding eval cases, the build fails. If a feature introduces a new high-severity risk without adversarial coverage, the build fails. Automation prevents coverage gaps from merging. Manual review catches the gaps automation misses. You need both.

Review coverage quarterly with cross-functional stakeholders. Product, Engineering, Trust and Safety, Legal, and Support all see different risks. A coverage map that looks complete to Engineering might reveal obvious gaps to Support, who fields the user complaints Engineering never sees. Quarterly reviews surface those gaps before they become incidents.

The goal is not perfect coverage. Perfect coverage is infinite test cases. The goal is informed coverage: you know what you are testing, you know what you are not testing, and you have reasons for both. Coverage strategy turns eval from a checkbox into a systematic risk-management discipline. The next question is how you sample the cases that populate your coverage map — because testing everything is impossible, and testing the wrong things is worse than testing nothing.

