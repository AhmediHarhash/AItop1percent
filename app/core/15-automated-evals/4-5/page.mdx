# 4.5 — Prompt Engineering for Consistent Judgments

The team has been staring at the eval dashboard for twenty minutes. The judge scores are incoherent. The same output gets a four on Monday and a two on Wednesday. Two identical inputs produce judgments that are three points apart. The logs show the judge giving contradictory reasoning — first it says the response lacks detail, then it rates an even shorter response higher. They assumed LLM-as-judge meant plugging in a model and getting reliable scores. They learned the hard way that without careful prompt engineering, judges produce noise.

## Prompt Structure Determines Stability

A judge prompt is not a casual question. It is an instruction set that must produce consistent behavior across thousands of evaluations. The structure matters as much as the content. A vague prompt — "rate this response for quality" — gives the judge too much freedom. It will invent its own criteria, change them across runs, and produce scores that do not correlate with anything you care about.

A well-structured judge prompt has five components. First, the task definition: what you are evaluating and why. Second, the rubric: the specific criteria the judge should apply. Third, the scale: what each score means. Fourth, the output format: how the judge should structure its response. Fifth, the reasoning instruction: whether and how the judge should explain its score. All five must be explicit. Leaving any of them implicit invites the judge to guess, and guessing produces inconsistency.

The task definition sets context. "You are evaluating customer support responses for helpfulness" is not enough. Add what helpfulness means in this context: "Helpfulness means the response directly answers the customer's question, provides actionable next steps, and uses a supportive tone." Now the judge knows what dimension of quality to focus on. It will not drift into evaluating grammar or brevity unless you tell it to.

## System vs User Message Placement

Most teams put the entire judge prompt in the user message. This works but wastes flexibility. The better pattern is to split the prompt: put the rubric and instructions in the system message, put the input and output being judged in the user message. The system message acts as persistent context. The user message is the variable part.

This separation makes the judge more stable because the rubric stays consistent across all evaluations. You are not re-explaining the criteria every time. The model internalizes the system message as its role and applies it uniformly. It also makes iteration easier. When you refine the rubric, you change the system message once. Every evaluation after that uses the updated criteria. If everything is in the user message, you have to regenerate all your prompts.

The system message might contain: "You are an expert evaluator of customer support responses. You will be given a customer question and a response. Rate the response on helpfulness using the following rubric: 1 = does not answer the question, 2 = partially answers but lacks detail, 3 = answers the question but misses actionable steps, 4 = fully answers with clear next steps, 5 = exceeds expectations with empathy and thoroughness." The user message contains only the customer question and the response being judged. The judge applies the rubric from the system message to the data in the user message.

## Output Format Specification

Judges need explicit output format instructions. If you want a score and reasoning, tell the judge exactly how to format them. "Provide a score from 1 to 5 and explain your reasoning" produces inconsistent structure. Sometimes the judge writes the score first, sometimes last. Sometimes it uses "Score: 4" and sometimes "Rating: 4" and sometimes just "4." Parsing these variations is fragile.

The fix is to specify the format precisely. "Output your evaluation in two lines. First line: Score: followed by the number. Second line: Reasoning: followed by your explanation." Now every output has the same structure. You can parse with a simple regex or string split. The judge has one less degree of freedom, which means one less source of variance.

For structured tasks, you can push further and ask for JSON output — but only in prose description, never with actual curly braces. Tell the judge: "Output an object with two fields: score as a number from 1 to 5, and reasoning as a text string explaining your decision." Many 2026 models can produce structured outputs reliably when you describe the schema in words. This makes downstream parsing trivial and eliminates format variation entirely.

## Chain-of-Thought for Judges

Asking a judge to explain its reasoning before giving a score improves consistency. This is the chain-of-thought pattern applied to evaluation. The judge that must articulate why it is assigning a score produces more stable scores than the judge that outputs a number with no explanation.

The mechanism is straightforward: reasoning first forces the judge to apply the rubric explicitly. It cannot shortcut to a score based on vague intuition. It has to identify which criteria the output meets or fails. This makes the judgment more grounded in the actual rubric and less influenced by surface features like length or politeness.

The instruction looks like: "First, analyze the response against each criterion in the rubric. Then, provide your score based on that analysis." You can make it more explicit by listing the criteria and asking the judge to address each one. "Evaluate whether the response: 1. Directly answers the question, 2. Provides actionable next steps, 3. Uses a supportive tone. Then assign a score from 1 to 5 based on how many criteria are met." The judge that writes out its evaluation of all three criteria will score more consistently than the judge that jumps straight to a number.

The cost is token usage. Chain-of-thought adds fifty to two hundred tokens per judgment depending on the task. For ten thousand evaluations, that is an extra half million to two million tokens. But the accuracy gain is usually worth it. A team evaluating RAG responses found that adding chain-of-thought reduced score variance by forty percent — the same output evaluated twice produced scores within one point instead of two or three points apart.

## Few-Shot Examples in Judge Prompts

Including a few example judgments in the prompt calibrates the judge. This is the same pattern as anchoring in pointwise evaluation, extended to any judging mode. You show the judge two or three inputs, outputs, and the correct scores with reasoning. The judge learns what your rubric looks like applied to real data.

Few-shot examples are especially important when your rubric has subjective dimensions. "Supportive tone" means different things to different people. Showing the judge one response that scores a two for tone, one that scores a four, and one that scores a five gives it concrete reference points. It no longer has to guess what you mean by supportive — it has seen examples.

The examples should span the score range. If you only show high-scoring outputs, the judge learns to be lenient. If you only show low-scoring outputs, it learns to be harsh. Cover the full spectrum so the judge understands the boundaries of each score level. A three-example set might include a clear failure rated one, a borderline case rated three, and an excellent response rated five.

Few-shot adds context length — another hundred to three hundred tokens per example, so three examples costs three hundred to nine hundred tokens. For high-stakes evaluations where consistency matters more than cost, this is a small price. For high-volume monitoring, you might skip few-shot and rely on a very detailed rubric instead. The trade-off is between token cost and calibration stability.

## Temperature and Sampling Settings for Consistency

LLM judges should run at low temperature — typically zero or close to it. Temperature controls randomness in the model's output. High temperature produces creative variation, which is the opposite of what you want in a judge. You want the same input and output to produce the same score every time. Low temperature maximizes consistency.

Temperature zero is deterministic for most models in 2026, though some models still show minor variance due to floating-point arithmetic or batching differences. Running the same judgment twice at temperature zero should produce identical scores ninety-nine percent of the time. If you see more variation than that, the problem is not temperature — it is prompt ambiguity or model instability.

Some teams experiment with slightly higher temperature — zero point one or zero point two — to reduce position bias in pairwise judging. The theory is that a tiny amount of randomness breaks the model's tendency to favor the first option. In practice, this helps marginally but introduces score variance. The better fix for position bias is to run comparisons in both orders and aggregate, which we cover in the next subchapter.

Sampling parameters beyond temperature — top-p, top-k, frequency penalty — should generally stay at defaults. These settings exist to control repetition and diversity in generation tasks. They do not improve judgment quality. Changing them usually just adds variance. Set temperature to zero, leave everything else default, and focus your tuning effort on the prompt structure and rubric clarity.

## Monitoring Judge Consistency Over Time

Once your judge is deployed, you need to track whether it stays consistent. The easiest check is to include a small set of canary examples in every eval run — inputs and outputs where you know the correct score. Evaluate those canaries alongside your real data. If the judge starts scoring the canaries differently than it did last week, something has changed. Maybe the model provider updated the model. Maybe your prompt changed. Maybe token limits caused truncation. The canaries give you early warning.

Another consistency check is score distribution. If your judge historically assigns thirty percent fives, forty percent fours, twenty percent threes, and ten percent ones and twos, and suddenly this week everything is a three or four, the judge has drifted. Real system quality does not shift that abruptly. The judge's internal threshold moved. You need to recalibrate — often by revisiting the rubric, adding examples, or switching to a different model version.

The most sophisticated teams track inter-run agreement. They evaluate a random sample of outputs twice — same judge, same prompt, same settings — and measure how often the scores match. Ninety-five percent agreement or higher is good. Eighty percent means the judge is too noisy to trust. Below seventy percent, you are measuring randomness, not quality. If agreement is low, the fix is almost always in the prompt: vague rubric, missing examples, unclear output format, or no chain-of-thought instruction.

Consistency is not the only thing that matters in LLM judges — but without consistency, nothing else matters, because you cannot distinguish real quality changes from measurement noise.

The most insidious failure mode in LLM-as-judge systems is self-agreement: models that rate their own outputs higher than they deserve, creating an eval system that systematically lies to you.

