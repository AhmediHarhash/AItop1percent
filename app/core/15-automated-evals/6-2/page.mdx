# 6.2 — Task Completion Detection for Agents

In July 2025, a legal tech company deployed a contract analysis agent that achieved 96 percent accuracy on clause extraction evals and passed every quality gate in their test suite. Two weeks into production, paralegals reported that approximately 30 percent of contract reviews were incomplete. The agent would analyze the first eight sections of a contract, produce detailed summaries, flag key risks, and then stop — leaving the remaining sections unprocessed. The eval suite never caught this because every test case used contracts with eight or fewer sections. The agent learned that after processing eight sections, it had done enough to satisfy the evaluators. It never learned what "complete" meant for the actual task.

This is the task completion problem. An agent can perform every individual step correctly, produce high-quality outputs at each stage, and still fail to finish the job. Traditional evals measure output quality at specific checkpoints. They do not measure whether the agent reached the end state the task required. Without explicit completion detection, you ship agents that stop too early, skip required steps, declare success prematurely, or continue working long after they should have finished. Task completion is not a byproduct of quality. It is a distinct dimension that requires its own instrumentation, its own ground truth, and its own scoring logic.

## Defining Task Completion

Task completion is the state where the agent has satisfied all requirements specified in the task definition and there is no remaining work that advances toward the goal. This definition has three components: requirements satisfaction, goal achievement, and work exhaustion. An agent that meets two out of three has not completed the task.

Requirements satisfaction means the agent executed every action, gathered every piece of information, and produced every output specified in the task breakdown. For a document generation task, this includes retrieving source data, applying the template, generating all sections, validating formatting, and saving the final file. Missing any step leaves the task incomplete. For a customer onboarding task, this includes collecting required information, verifying identity, creating account records, sending welcome communications, and logging completion status. Skipping the welcome email or failing to log completion means the task is unfinished, even if the account was created successfully.

Goal achievement means the world is now in the state it needed to be in for the task to be considered done. A support agent's goal is to resolve the customer issue. Resolution might require issuing a refund, updating account settings, escalating to a specialist, or simply providing information. The specific actions vary, but the end state is consistent: the customer's issue is resolved. If the agent issued a refund but did not confirm the refund was processed, the goal is not achieved. If the agent escalated the issue but did not verify the escalation was received, the goal is not achieved. Goal achievement is about outcome, not effort.

Work exhaustion means the agent has nothing productive left to do. It has attempted all viable paths to completion, handled all errors, retried all recoverable failures, and reached a terminal state — either success or unrecoverable failure. An agent that stops because it encountered an error has not exhausted work if there was a retry strategy available. An agent that stops because it ran out of ideas has not exhausted work if there were unexplored options in the decision tree. Work exhaustion is the hardest component to verify because it requires reasoning about what the agent could have done but did not.

## Success Criteria Specification

You cannot detect task completion without a machine-readable specification of what completion looks like. This specification is the completion contract — a formal definition of the end state, the required actions, and the verification conditions. Most teams write task descriptions in natural language and expect the agent to infer what done means. This works until it does not. Completion detection requires explicit criteria that an automated eval can check without human interpretation.

The completion contract defines three types of criteria: mandatory actions, required outcomes, and terminal conditions. Mandatory actions are the steps the agent must execute regardless of circumstance. For a data export task, mandatory actions include authenticating with the source system, querying the specified data, transforming it to the target format, writing it to the destination, and verifying the write succeeded. Every one of these actions must appear in the execution trace. If any are missing, the task is incomplete.

Required outcomes are the observable state changes that must exist after the task finishes. For a password reset task, the required outcome is that the user's password hash in the database is updated, a reset confirmation email was sent, and the password reset token is invalidated. The agent might achieve these outcomes through different action sequences depending on the user's account state, but the outcomes are non-negotiable. Completion evals verify these outcomes by inspecting system state, not by inspecting the agent's action log.

Terminal conditions define the states where the agent should stop. Success terminals are states where all requirements are met and all outcomes are achieved. Failure terminals are states where the agent has exhausted all recovery options and cannot proceed further. Timeout terminals are states where the agent has exceeded the allowed time budget. Each terminal condition needs explicit detection logic. The eval system checks whether the agent stopped at one of these terminals and whether it correctly identified which terminal it reached. An agent that stops at a failure terminal but reports success has a completion detection failure.

## Partial Completion Scoring

Not all task failures are total. An agent that completes 80 percent of a multi-step workflow and fails on the final step has provided more value than an agent that fails on the first step. Partial completion scoring assigns credit for work done while penalizing incomplete execution. This matters for two reasons: it provides finer-grained signal for model improvement, and it reflects the real-world value of partial work in degraded scenarios.

The challenge is defining what partial credit means for a given task. Some tasks are atomic — either fully complete or fully failed with no meaningful intermediate states. A database transaction either commits or rolls back. An authentication check either succeeds or fails. There is no partial credit for "almost authenticated." Other tasks are compositional — they consist of multiple independent subtasks where each subtask has standalone value. A report generation task might involve collecting data, running analysis, generating charts, and writing summaries. If the agent completes data collection and analysis but fails on chart generation, the partial results still have value.

Your completion scoring model must distinguish between these cases. For atomic tasks, score binary: one point for complete, zero points for incomplete. For compositional tasks, score proportionally: assign point values to each subtask based on its contribution to the overall goal, sum the points for completed subtasks, divide by total possible points. The weighting matters. If data collection is worth 40 percent of the task value, analysis is worth 30 percent, charts are worth 20 percent, and summaries are worth 10 percent, then an agent that completes everything except summaries scores 90 percent, not 75 percent. The weighting should reflect the actual value delivered, not just the number of steps completed.

Partial credit should penalize incomplete work more heavily when the incomplete portion blocks the value of completed work. An agent that retrieves customer data, analyzes it, generates insights, but fails to write those insights to the destination system has completed 75 percent of the steps but delivered zero percent of the value — because the insights are inaccessible to downstream consumers. In this case, partial credit should be zero or near-zero. The scoring function must model dependencies. If subtask B depends on subtask A, completing B without completing A is often worthless. If subtask C depends on both A and B, completing C alone is always worthless.

## Detecting Premature Stopping

The most common completion failure is the agent stopping before it should. Premature stopping happens when the agent believes it has finished but has not satisfied all completion criteria. This occurs for three reasons: the agent misidentified the terminal condition, the agent did not understand the full task scope, or the agent encountered an error it did not recognize as recoverable.

Misidentified terminals happen when the agent treats an intermediate success as a final success. A document generation agent completes the first draft, runs a quality check, sees that the draft passes basic formatting rules, and declares the task complete — without applying the required review process, incorporating stakeholder feedback, or finalizing the version for publication. The agent saw "quality check passed" and inferred "task done" because it did not have a complete model of the workflow. Detection requires comparing the agent's stopping point to the full workflow specification. If the agent stopped at step four of a seven-step process, it stopped prematurely regardless of what it reported.

Incomplete task understanding happens when the task specification is ambiguous or incomplete and the agent fills in the gaps incorrectly. The task is "generate a quarterly financial report." The agent generates revenue and expense summaries, which technically are financial reports. It does not generate cash flow analysis, balance sheet summaries, or variance explanations because those were not explicitly mentioned in the task description. The agent completed what it thought the task was. It did not complete what the task actually required. Detection requires explicit enumeration of all required components in the completion contract. Ambiguity in the task spec shows up as premature stopping in production.

Unrecognized recoverable errors happen when the agent encounters a temporary failure — API timeout, rate limit, transient database lock — and treats it as a terminal failure instead of retrying. The agent stops and reports failure even though a retry would have succeeded. Detection requires inspecting the error log to identify recoverable error types and verifying whether the agent attempted the expected retry logic. If the agent stopped after a single 503 error from a service that was available again two seconds later, it stopped prematurely.

## Verifying End State

Task completion is not what the agent reports. It is what actually happened. An agent can report success while leaving the task half-finished. It can report failure while having actually completed the work correctly. Completion verification checks the actual state of the world, not the agent's self-assessment.

End state verification requires access to all systems the agent interacted with. For a data pipeline agent, this means checking the destination database, the processing logs, the error queues, and the notification system. For a customer support agent, this means checking the ticket status, the refund records, the email logs, and the escalation queue. You compare the observed end state to the required end state defined in the completion contract. Every required outcome should be present. Every prohibited outcome should be absent. Discrepancies are completion failures.

Verification logic must account for eventual consistency. An agent might issue a refund command that succeeds immediately in the payment API but takes 30 seconds to reflect in the customer account view. If you verify end state one second after the agent reports completion, you see an incomplete state even though completion was correct. Your verification checks need delays, retries, or eventual consistency polling to avoid false negatives. The delay duration depends on the systems involved. Synchronous APIs need no delay. Asynchronous message queues might need several seconds. Batch processing systems might need minutes.

Negative verification is as important as positive verification. The agent should not have done certain things. A data deletion agent should not have deleted records outside the specified scope. A notification agent should not have sent emails to unsubscribed users. A financial transaction agent should not have bypassed approval workflows. End state verification checks for the presence of required changes and the absence of prohibited changes. Both are task completion criteria.

## Task Completion as the Core Agent Metric

For agentic systems, task completion rate is the single most important eval metric. It is the percentage of assigned tasks the agent finishes successfully without human intervention. Everything else — output quality, efficiency, safety — is secondary. An agent that produces perfect outputs but completes only 60 percent of its tasks is not production-ready. An agent that produces adequate outputs and completes 95 percent of its tasks is.

Task completion rate stratified by task type reveals capability gaps. An agent might complete 98 percent of data retrieval tasks, 85 percent of data transformation tasks, and 62 percent of multi-step workflow tasks. The stratification tells you where to focus improvement effort. It also tells you which tasks are safe to delegate to the agent and which require human involvement. You do not need the agent to have uniform completion rates across all task types. You need to know the completion rate for each type so you can route work appropriately.

Task completion rate over time reveals degradation or improvement. A regression in completion rate signals that something changed — the task distribution shifted, the external systems changed behavior, the model degraded, or the task specifications became misaligned with production reality. An improvement in completion rate signals that training data additions, prompt refinements, or workflow logic updates are working. Completion rate is your primary production health metric for agents. When it drops, you investigate. When it improves, you study what changed and do more of it.

Tool selection accuracy is the next behavioral dimension to measure — whether the agent chose the right tools at the right time, and whether those choices contributed to task completion or created inefficiency and failure.

