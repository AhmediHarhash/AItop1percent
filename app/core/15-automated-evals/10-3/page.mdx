# 10.3 — Slicing by Task Type, Model, Tenant, and Cohort

In March 2025, a SaaS company deployed a new model with an aggregate pass rate of 94.1%, up from 93.3% on the previous model. The eval dashboard showed green across the board. The deploy proceeded. Within 48 hours, enterprise customers started filing support tickets. The model was failing on contract summarization — a feature critical to their legal workflows. The aggregate pass rate had masked a segment-specific collapse. Contract summarization accuracy dropped from 89% to 71%. But contract summarization represented only 6% of the test suite. The other 94% improved, lifting the aggregate. No one noticed the segment failure until production users experienced it.

The post-mortem identified the root cause: the new model was fine-tuned on a dataset weighted toward customer support and FAQ tasks. It improved on those tasks. But the fine-tuning induced catastrophic forgetting on tasks underrepresented in the dataset, including contract summarization. The eval suite had the right test cases. The team just didn't slice the metrics. They looked at the aggregate pass rate, saw improvement, and shipped. If they had sliced by task type, they would have seen contract summarization's 18-percentage-point drop, investigated before deploy, and either fixed the fine-tuning or held the release.

## The Failure of Aggregate Metrics

**Aggregate metrics** average performance across all test cases. They're useful for high-level trends, but they hide segment-specific failures. A model can have a 95% pass rate overall while failing catastrophically on 10% of task types. A model can perform better on average than the baseline while performing worse for your highest-value customers. Aggregates smooth over variation. That smoothing is valuable when you want a single number for executive reporting. It's dangerous when you're making deploy decisions.

The danger is that aggregate metrics create a false sense of safety. The dashboard is green. Pass rate is up. Regression count is low. Leadership sees the metrics and approves the deploy. But the aggregate hides a collapse in a specific segment — a segment that might represent a minority of test cases but a majority of revenue, or a minority of users but the ones most sensitive to quality, or a minority of tasks but the ones most likely to cause legal or reputational harm if wrong.

Slicing breaks the aggregate into segments. Instead of one pass rate, you have ten pass rates — one per task type. Instead of one regression count, you have regression counts per model, per tenant, per input cohort. Slicing reveals the heterogeneity that aggregates hide. It shows you where the model is strong and where it's weak. It surfaces the segments that are improving and the segments that are degrading. It turns a single green light into a nuanced picture of quality across the problem space.

Slicing is not optional for production systems. If your eval dashboard only shows aggregate metrics, you're flying blind. You have no visibility into whether quality is evenly distributed or concentrated. You don't know if your model is excellent for 80% of users and broken for 20%, or mediocre for everyone. You don't know if the new model's improvement comes from getting better at what it's already good at or from fixing weaknesses. Slicing gives you that visibility.

## Slicing by Task Type

**Task type** is the most common and most valuable slice. Most AI systems handle multiple task types: summarization, classification, question answering, generation, translation, extraction, refusal, escalation. Each task type has different characteristics, different difficulty, different business impact. A model that's great at summarization might be terrible at classification. A model that excels at question answering might fail at generation. You need to measure each task type separately.

Slicing by task type requires task-type labels on every test case. When you create a test case, you tag it with the task type it evaluates. The tag becomes metadata. When you run evals, you calculate pass rate per task type. Instead of "overall pass rate: 94%," you report "summarization: 96%, classification: 92%, question answering: 91%, generation: 89%, refusal: 97%." Now you can see which tasks the model handles well and which need improvement.

This granularity informs prioritization. If generation pass rate is 89% and refusal pass rate is 97%, you know where to focus fine-tuning efforts. You add more generation examples to the training data. You adjust the generation prompt. You re-eval. Generation pass rate climbs to 93%. You've improved the model's weakest area without degrading its strengths. Without slicing, you wouldn't know which area was weakest. You'd be tuning blindly.

Slicing by task type also reveals catastrophic forgetting during fine-tuning. If you fine-tune a model to improve summarization and then re-eval, you expect summarization pass rate to increase. But you also check the other task types. If classification pass rate drops from 92% to 78%, you've induced forgetting. The model got better at the task you optimized for and worse at tasks you didn't. This is a common fine-tuning failure mode. Slicing catches it before deploy. Without slicing, you'd see aggregate pass rate increase — because summarization improved more than classification degraded — and ship a model that's broken for classification users.

Task-type slicing is especially critical for multi-use-case products. If your AI system serves ten different use cases, each with different task types, you can't rely on aggregate metrics. A model might be ready for nine use cases and not ready for the tenth. If you deploy it universally, the tenth use case breaks. If you slice by task type, you can deploy selectively. You route the new model to the nine use cases where it's ready. You keep the old model on the tenth use case until the new model's performance improves. This selective deployment is only possible if you have per-task-type metrics.

## Slicing by Model

In a multi-model system, you route different requests to different models based on latency requirements, cost constraints, or capability needs. You might route simple queries to GPT-5-mini, complex queries to GPT-5.2, and domain-specific queries to a fine-tuned Claude Opus 4.5. Each model has different strengths. Each model needs separate eval metrics.

**Slicing by model** means running your eval suite on every model in your routing logic and comparing performance. GPT-5-mini passes 87% of the test suite. GPT-5.2 passes 94%. Claude Opus 4.5 passes 96%. Now you know which model is strongest overall. But you also slice by task type within each model. GPT-5-mini might be 91% on summarization but only 78% on generation. GPT-5.2 might be 93% on generation but 90% on summarization. Claude Opus 4.5 might be 97% on both. This cross-cutting analysis tells you which model to use for which task.

Model slicing also reveals routing inefficiencies. If your routing logic sends 40% of requests to GPT-5-mini to save cost, but GPT-5-mini's pass rate on those requests is only 82%, you're trading cost for quality. You calculate the cost-quality trade-off. Is the 12-percentage-point quality drop worth the cost savings? If yes, you keep the routing logic. If no, you shift more traffic to the higher-quality model. Without model-specific metrics, you can't make this trade-off explicit. You're just guessing.

Model slicing is essential when you're evaluating model upgrades. Claude releases Opus 4.6. You run your eval suite on Opus 4.6 and compare it to Opus 4.5. Aggregate pass rate increases by 1.2 percentage points. But when you slice by task type, you see that Opus 4.6 improved on generation tasks by 4 percentage points and regressed on classification tasks by 2 percentage points. Now you decide: is the generation improvement worth the classification regression? If classification is higher-value for your use case, you stick with Opus 4.5. If generation is higher-value, you upgrade. The decision is informed by sliced metrics, not aggregates.

Model slicing also applies to A/B tests. You route 50% of traffic to the new model and 50% to the old model. You track pass rate for each. If the new model's pass rate is higher, you ramp it to 100%. If the old model's pass rate is higher, you roll back. But if you slice by task type, you might discover that the new model is better on 80% of tasks and worse on 20%. You can route the 80% to the new model and the 20% to the old model. This hybrid routing maximizes quality across the task distribution. It's only possible if you have per-model, per-task metrics.

## Slicing by Tenant and Customer Cohort

In multi-tenant systems, different customers have different use cases, different data distributions, different quality requirements. A model that works well for 90% of tenants might fail for 10%. If those 10% represent your highest-revenue customers or your most regulated verticals, the failure is unacceptable. **Slicing by tenant** surfaces these customer-specific quality issues before they reach production.

Tenant slicing requires tenant-labeled test cases. Each test case is tagged with the tenant ID or tenant segment it represents. When you run evals, you calculate pass rate per tenant. Tenant A: 96%. Tenant B: 94%. Tenant C: 87%. Tenant C's pass rate is below threshold. You investigate. You discover that Tenant C's use case involves medical terminology, and the model underperforms on medical language. You add medical examples to the fine-tuning dataset, re-eval, and Tenant C's pass rate climbs to 93%. You've fixed the tenant-specific failure before it impacted production.

Tenant slicing is especially valuable in regulated industries. If you serve healthcare customers, financial services customers, and e-commerce customers, their compliance requirements differ. A model that meets e-commerce quality standards might not meet healthcare standards. If you slice by tenant segment, you can enforce different quality thresholds for different segments. Healthcare tenants require 97% pass rate. E-commerce tenants require 92%. You don't deploy the model to healthcare tenants until it clears the higher bar. You deploy it to e-commerce tenants as soon as it clears the lower bar.

**Customer cohort slicing** is a generalization of tenant slicing. Instead of per-tenant metrics, you group tenants into cohorts — enterprise versus SMB, US versus EU, healthcare versus retail — and measure per-cohort. This is useful when you have too many tenants to track individually. You identify the 5-10 cohorts that matter strategically, label test cases by cohort, and track cohort-level metrics. If a cohort underperforms, you drill into that cohort's test cases, identify the failure modes, and fix them.

Cohort slicing also enables cohort-specific fine-tuning. If your enterprise cohort underperforms, you fine-tune on enterprise-specific data. If your EU cohort underperforms due to language or regional differences, you fine-tune on EU-specific data. Without cohort slicing, you wouldn't know which cohort needs attention. You'd fine-tune on a global dataset and hope it lifts all cohorts evenly. Slicing makes fine-tuning targeted and efficient.

## Slicing by Input Cohort and Edge Case Type

Not all inputs are created equal. Some inputs are common, well-formed, and easy for the model to handle. Others are rare, ambiguous, adversarial, or edge cases. **Slicing by input cohort** separates mainstream cases from edge cases and measures each independently. This prevents edge case failures from being hidden by strong performance on mainstream cases.

Input cohort slicing requires clustering test cases by input characteristics. You might have cohorts for: short inputs versus long inputs, structured inputs versus unstructured, common queries versus rare queries, well-formed instructions versus ambiguous instructions, standard language versus slang or jargon, safe content versus borderline content. Each cohort gets its own pass rate. Mainstream cases: 97%. Edge cases: 82%. Now you know the model struggles with edge cases. You decide whether 82% is acceptable for edge cases or whether you need to improve.

Edge case slicing is critical for adversarial robustness. If your eval suite includes adversarial test cases — inputs designed to trick the model — you slice them separately. Adversarial pass rate: 68%. Standard pass rate: 95%. The aggregate might be 93%, which looks fine, but the adversarial slice reveals fragility. You know the model is vulnerable to certain input patterns. You add adversarial training data, re-eval, and track whether adversarial pass rate improves. Without slicing, adversarial failures are invisible.

Input cohort slicing also informs product decisions. If long-input pass rate is 78% and short-input pass rate is 96%, you might decide to limit input length in production. You set a character limit. Inputs longer than 2,000 characters get truncated or rejected. This constraint keeps users in the high-quality input cohort and prevents them from hitting the low-quality cohort. It's a trade-off — you're reducing functionality to protect quality — but it's a trade-off you can only make if you have cohort-level visibility.

## The Dashboard View: Sliced Metrics at a Glance

Slicing creates more metrics. More metrics create dashboard complexity. The challenge is surfacing sliced metrics without overwhelming the viewer. The solution is a **hierarchical dashboard**. The top level shows aggregate metrics. The second level shows slices. The third level shows drill-downs.

At the top level, you see overall pass rate: 94%. That's the headline number. One row below, you see task-type slices: summarization 96%, classification 92%, generation 89%. One more row below, you see model slices: GPT-5.2 94%, Claude Opus 4.5 96%. One more row below, you see tenant cohort slices: enterprise 95%, SMB 93%, healthcare 91%. Each slice is color-coded. Green means above threshold. Yellow means near threshold. Red means below threshold.

The viewer scans the top-level aggregate. If it's green, they glance at the slices to confirm there are no red segments. If the aggregate is red or if any slice is red, they drill down. They click on the red slice. The dashboard expands to show the specific test cases that failed in that slice, the failure modes, the trends over time, and the comparison to baseline. This hierarchical structure keeps the dashboard scannable while preserving access to detail.

Sliced metrics should also support filtering and sorting. You filter to show only task types with pass rate below 90%. You sort by delta versus baseline. Now the dashboard shows the segments that regressed most. You focus your debugging effort there. You fix the biggest regressions first. This workflow is only possible if the dashboard supports slicing and dynamic filtering. A static, aggregate-only dashboard can't enable it.

## Discovering Hidden Failures Through Slicing

Slicing is a diagnostic tool. It reveals problems the aggregate hides. A model with 94% aggregate pass rate seems healthy. But when you slice by task type, you find that one task type is at 68%. When you slice by tenant, you find that one high-value customer has a pass rate of 74%. When you slice by input cohort, you find that adversarial cases are at 59%. Each of these is a hidden failure. The aggregate masked it. Slicing exposed it.

The workflow is: run evals, calculate aggregate metrics, celebrate if the aggregate looks good, then slice immediately. Never ship based on the aggregate alone. Always check the slices. If any slice is below threshold, investigate. Trace the failures back to root causes. Fix the root causes. Re-eval. Confirm the slice recovered. Only then do you proceed with deploy.

Slicing also surfaces opportunities. You might discover that a low-cost model performs just as well as a high-cost model on 70% of task types. You shift those task types to the low-cost model. You save money without sacrificing quality. You wouldn't have discovered this opportunity without task-type slicing. The aggregate pass rate for both models might be similar, but the per-task breakdown reveals the cost-optimization path.

Slicing is the difference between knowing your model works on average and knowing exactly where it works and where it doesn't. Aggregate metrics give you confidence in the overall system. Sliced metrics give you the precision to optimize, to deploy selectively, to enforce quality standards per segment, and to catch the failures that matter most before they reach production. Pass rate, regression detection, and slicing are the three pillars of eval metrics, and trends over time tell you whether your system is improving or degrading across every dimension you care about.

---

*Next: 10.4 — Trend Analysis and Quality Trajectory*
