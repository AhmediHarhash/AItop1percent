# 15.6 — SLOs for Quality: Defining Eval Targets Like Uptime

Uptime has a number. Latency has a number. Error rate has a number. Quality needs a number too. If you treat uptime as a service-level objective with defined thresholds, alerts, and escalation paths but treat quality as a vague aspiration, you will ship systems that stay online and perform terribly. Quality is a reliability dimension. It gets the same rigor, the same measurement discipline, the same operational response as any other SLO. You define a target, you measure against it, you treat breaches as incidents, and you fix them before users notice or accept the consequences if you cannot.

A quality SLO is a quantitative threshold applied to an eval metric with a defined measurement window and a defined breach response. "Accuracy must be at least 92% over the last 500 production queries" is a quality SLO. "Quality should be good" is not. SLOs force precision. They make trade-offs explicit. They turn quality from a subjective judgment into an operational commitment. Teams that run production systems with uptime SLOs but no quality SLOs are measuring half the system and pretending they measured it all.

## Defining Quality Targets: What Does Good Enough Look Like

Start with the quality bar from problem framing. If you defined "good enough" as 90% correctness during design, that becomes your SLO target. If you never defined a quality bar, you do not have enough information to set an SLO. Go back to problem framing. Ask Product, ask stakeholders, ask users: what is the minimum acceptable quality for this system to provide value? That floor is your SLO.

Quality targets are task-specific. A summarization task might require 85% factual accuracy. A PII detection task might require 99% recall. A content moderation task might require 95% precision on policy violations. Different tasks carry different risks. The SLO reflects the risk. Set targets per task type, not one global quality number for the entire system.

Quality targets are also risk-tier specific. A Tier 1 system handling medical decisions needs 98% correctness. A Tier 3 system generating marketing taglines needs 80% correctness. The acceptable failure rate scales with the impact of failure. A wrong medical diagnosis kills someone. A mediocre tagline wastes a copywriter's time. Your SLO targets must reflect that difference or you are setting targets without understanding what they protect.

Define targets in collaboration with Product, Domain Experts, and stakeholders. Engineering proposes what is technically achievable. Product defines what is commercially viable. Domain Experts define what is professionally acceptable. Legal defines what is legally defensible. The SLO is the intersection of all four. If Engineering can only deliver 80% accuracy but Legal requires 95%, you do not launch. If Product wants 99% but users are satisfied with 90%, you do not over-invest. SLOs are negotiated commitments, not aspirations.

## SLO Components: Metric, Threshold, Window, and Response

Every quality SLO has four components. First, the metric: what are you measuring? Accuracy, precision, recall, refusal rate, hallucination rate, policy violation rate, average score from a rubric-based LLM judge. The metric must be measurable, reproducible, and directly tied to a quality dimension users care about. Vague metrics produce vague SLOs.

Second, the threshold: what is the minimum acceptable value? "Accuracy must be at least 90%" or "Hallucination rate must be below 5%" or "Refusal rate for in-scope queries must be below 2%." The threshold is a line. Above the line, the system is performing within spec. Below the line, it is breaching. Thresholds make quality binary. Either you are meeting the bar or you are not.

Third, the measurement window: over what time period or sample size do you measure? "Over the last 1,000 production queries" or "Over the last 24 hours" or "Over the current sprint." The window defines how quickly you detect degradation. A 24-hour window means you might serve bad outputs for 24 hours before the SLO breach triggers an alert. A 1,000-query window means you might breach after 500 queries if traffic is slow. Choose windows that balance detection speed against measurement stability.

Fourth, the breach response: what happens when you cross the threshold? Alert the on-call engineer. Page the ML team. Trigger a rollback. Disable the feature. Route traffic to a fallback model. Escalate to leadership. The response must be defined before the breach happens. If you wait until quality drops to decide what to do, you make decisions under pressure with incomplete information. Pre-defined responses turn breaches into operational incidents with known playbooks.

## Setting Thresholds: Baseline, Target, and Stretch

Quality SLOs often have three thresholds, not one. The baseline is the minimum acceptable performance. If quality drops below baseline, the system is not fit for production. The target is the expected steady-state performance. This is what you commit to delivering. The stretch is the aspirational goal. You aim for stretch but you do not block launch if you miss it.

A medical chatbot might set: baseline 92% correctness, target 95%, stretch 98%. If quality drops below 92%, you pull the feature or route everything to human review. If quality is between 92% and 95%, you investigate and fix but you do not panic. If quality is above 95%, you are meeting commitments. If you hit 98%, you are exceeding expectations. Three thresholds create three operational zones: crisis, investigate, and healthy.

For new systems, set conservative baselines and adjust upward as you learn. If you have no production data, you do not know what quality is achievable under real-world conditions. Start with a baseline you are 90% confident you can meet. After a month of production data, recalibrate. Maybe the baseline was too easy and you can raise it. Maybe it was too aggressive and you need to lower it or invest in quality improvements before tightening the SLO. SLOs are not static. They evolve with the system.

Publish SLOs internally and hold teams accountable to them. Quality SLOs should appear in the same dashboards as uptime SLOs. The on-call engineer should see both. Leadership should review both in weekly metrics reviews. If uptime SLO breaches trigger incident response but quality SLO breaches get ignored, you have sent a message: uptime matters, quality does not. That message becomes culture. Culture becomes product.

## Measurement Windows: Frequency, Latency, and Stability

The measurement window determines how fast you detect degradation. A one-hour window detects problems faster than a one-week window. But a one-hour window is also noisier. If your eval suite runs on a small sample, short windows produce volatile measurements that trigger false alarms. Long windows smooth out noise but delay detection. The trade-off is detection speed versus measurement stability.

For high-traffic systems, use short windows. If you serve 10,000 queries per hour, a one-hour window gives you a stable sample size and fast detection. If quality drops, you know within an hour. For low-traffic systems, use query-count windows instead of time windows. "Over the last 500 queries" ensures you always have enough data to measure accurately, even if 500 queries takes three days.

Combine time-based and count-based windows. Monitor quality over the last hour AND over the last 1,000 queries. If the hourly window breaches, you get fast detection. If the count window confirms the breach, you know it is real and not a statistical fluke. Dual windows reduce false positives while maintaining fast response.

Track SLO measurement latency. Measurement latency is the delay between when a query happens and when its quality score appears in your SLO dashboard. If you use human eval, latency might be 24 hours. If you use LLM-as-a-judge, latency might be 5 minutes. If you use rule-based checks, latency might be 30 seconds. Latency determines how stale your SLO metrics are. A 24-hour latency means your dashboard shows yesterday's quality. If quality degraded this morning, you will not know until tomorrow. Minimize measurement latency wherever possible.

## Breach Response: Alerts, Escalation, and Mitigation

Define breach severity levels and map them to response actions. A P0 breach is catastrophic: quality dropped below baseline, users are experiencing widespread failures, legal or safety risk is imminent. Response: page on-call, escalate to leadership, consider emergency rollback or feature disable. A P1 breach is serious: quality dropped below target but above baseline, user impact is noticeable but not catastrophic. Response: alert on-call, investigate within 2 hours, file incident ticket. A P2 breach is a warning: quality is trending toward target but has not breached yet. Response: log the trend, review in next sprint planning.

Automate breach detection and alerting. Your eval pipeline runs continuously. Every time it completes a measurement window, it compares the result to the SLO thresholds. If a threshold is breached, the pipeline sends an alert to the appropriate channel: page on-call for P0, Slack alert for P1, dashboard annotation for P2. Humans do not watch dashboards 24/7. Automation watches for them.

Link SLO breaches to incident response workflows. A quality SLO breach is an incident, the same as an uptime SLO breach. It gets a ticket, an owner, a timeline, a root cause analysis, and a mitigation plan. Treat quality incidents with the same operational rigor as infrastructure incidents. If you do not, quality will always lose to uptime when priorities conflict.

Build fallback mitigations for common breach scenarios. If accuracy drops below baseline, route queries to a more capable model or to human agents. If refusal rate spikes, relax the refusal threshold temporarily while you investigate. If hallucination rate breaches, add a disclaimer to outputs warning users to verify. Fallbacks let you maintain partial service while you fix the root cause. They are not solutions. They are damage control.

## SLO Reporting: Dashboards, Trends, and Accountability

Build a quality SLO dashboard that shows current performance, threshold lines, breach history, and trends over time. The dashboard should answer three questions in five seconds: Are we meeting our SLOs right now? How close are we to breaching? How has quality trended over the past month? If the dashboard requires ten clicks and five minutes of interpretation, no one will use it.

Report SLO performance in regular team reviews. Quality SLOs should appear in sprint retros, monthly business reviews, and quarterly planning. Show the metrics. Show the breach history. Show the mitigation actions. Treat quality performance as a first-class deliverable, not a footnote. If quality is only discussed when it breaks, you are managing reactively. If it is discussed every sprint, you are managing proactively.

Track SLO compliance as a percentage: the fraction of measurement windows where you met your target. If you define a 95% accuracy target and you met it in 47 out of 52 weeks, your SLO compliance is 90%. Compliance percentage is a single number that summarizes a year of quality performance. Leadership understands percentages. Use them.

Publish SLOs externally if you are building a platform or API. External SLOs are commitments to customers. They come with penalties for non-compliance: SLA credits, contract breaches, reputational damage. External SLOs force you to set realistic targets and invest in the infrastructure to meet them. Internal SLOs can be aspirational. External SLOs cannot. If you are not confident enough to publish your SLO externally, you are not confident enough to rely on it internally.

## Quality SLOs as a Cultural Shift

Adopting quality SLOs is not a technical change. It is a cultural change. It requires Product to commit to specific quality targets and accept the cost of meeting them. It requires Engineering to measure quality with the same rigor as uptime. It requires Leadership to treat quality breaches as incidents that demand root cause analysis and mitigation. It requires the entire organization to agree that quality is not a nice-to-have. It is a reliability dimension.

The resistance will come from ambiguity. Product will say "we do not know what good quality is yet." Engineering will say "quality is too subjective to measure precisely." Leadership will say "we need to move faster and we cannot block on quality." All three objections are risk-avoidance disguised as pragmatism. If you do not know what good quality is, you are shipping a system with no success criteria. If quality is too subjective to measure, you are admitting you cannot tell whether your system works. If you cannot block on quality, you are saying quality does not matter. SLOs force you to confront those contradictions and make explicit choices.

Start with conservative SLOs and tighten them as the system matures. It is easier to raise a bar than to lower it. If you set an aggressive SLO on day one and miss it every week, the SLO loses credibility and teams stop paying attention. If you set a realistic SLO and meet it consistently, you build trust in the measurement system. Then you can raise the bar and teams will believe the new target is achievable.

Quality SLOs turn eval from a pre-launch checklist into an operational discipline. You do not just test before shipping. You measure continuously, you detect degradation automatically, you respond when thresholds breach, and you hold teams accountable to targets. The next question is how you build the infrastructure to run those measurements at scale — because SLOs are only as good as the pipelines that feed them.

