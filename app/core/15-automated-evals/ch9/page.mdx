# Chapter 9 — Calibration and Drift Detection

Automated evals drift. The LLM judge you calibrated in January behaves differently after a model update in March. The golden set that represented your user distribution six months ago no longer matches today's queries. The heuristic thresholds you tuned for one product version fail silently on the next. Calibration is not a one-time setup — it is continuous maintenance. This chapter covers why evals drift, how to build calibration loops that compare automation to human judgment, how to schedule recalibration, how to detect drift before it corrupts your quality signal, and how to decide when an eval needs tuning versus complete rebuilding.

---

- 9.1 — Why Evals Drift Over Time
- 9.2 — Calibration Loops: Comparing Automation to Humans
- 9.3 — Scheduled Recalibration Cadences
- 9.4 — Drift Detection Metrics and Thresholds
- 9.5 — Judge Model Update Impact Assessment
- 9.6 — Ground Truth Evolution and Eval Staleness
- 9.7 — A/B Testing Eval Changes
- 9.8 — Calibration Documentation and Audit Trails
- 9.9 — The Silent Drift Problem: Catching Invisible Degradation
- 9.10 — When to Rebuild vs Recalibrate

---

*The eval that was 94% accurate last quarter might be 78% accurate today — and your dashboard will not tell you unless you check.*
