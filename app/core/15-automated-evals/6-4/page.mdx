# 6.4 — Multi-Step Workflow Evaluation

The engineering team stares at the logs. The workflow succeeded through three steps — retrieved the customer data, validated the account status, checked payment history. Then step four ran. The agent decided the customer was eligible for a refund, initiated the payment, and sent a confirmation email. Every individual action looked correct in isolation. But the customer had already received a refund two weeks earlier. The workflow never checked for duplicate refunds. The eval suite tested each step independently. It never tested whether the steps made sense as a sequence.

This is the central problem of multi-step workflow evaluation. A workflow is not a collection of independent actions that happen to run in order. It is a coherent sequence where each step depends on the state created by previous steps, where the correctness of step five assumes that steps one through four succeeded and produced valid outputs, where a mistake in step two silently poisons every step that follows. You cannot evaluate a workflow by testing each step in isolation. You must evaluate the workflow as a system — verifying that each step produces the correct output given the actual state at that point, that the sequence itself makes logical sense, that the workflow detects and handles deviations, that intermediate states are valid and internally consistent.

## The Illusion of Step Independence

Most teams start by evaluating each step independently. They build test cases for the retrieval step — does it fetch the right data? They build test cases for the validation step — does it correctly identify valid accounts? They build test cases for the decision step — does it apply the refund logic correctly? Every step passes its tests. The workflow ships. Then production reveals that the steps interact in ways the isolated tests never captured.

The retrieval step returns a list of transactions. The validation step expects the list to be sorted by date. The retrieval step does not guarantee sort order. In ninety percent of cases, the data happens to arrive sorted. In ten percent, it does not. The validation step processes the unsorted list and makes decisions based on the assumption that the most recent transaction appears first. The decision is wrong. The workflow completes successfully. The output is garbage. No individual step failed. The sequence failed.

This is why step independence is an illusion. Each step operates on the output of the previous step. If step two assumes something about the structure or properties of step one's output, and step one does not guarantee those properties, the workflow has a latent failure mode that only appears under certain conditions. Evaluating steps independently cannot catch these failures because the failure is not in any single step. The failure is in the contract between steps — the implicit assumptions about what the previous step produces and what the next step requires.

## Workflow Trace Analysis

**Workflow trace analysis** means capturing the complete execution path of a multi-step workflow — every step that ran, every decision the agent made, every intermediate state, every tool call, every branching point — and verifying that the sequence makes sense as a whole. You are not just checking that each step produced a valid output. You are checking that the workflow followed a logical path from input to final output, that it made the right decisions at each branch, that it did not skip required steps or execute steps in an order that violates dependencies.

The trace includes every intermediate state. After step one, what data existed? After step two, how did that data change? After step three, what new information became available? The eval examines each transition. Did step two have access to all the information it needed from step one? Did step three correctly interpret the output of step two? Did the workflow ever reach a state that should have triggered an error but did not? Did it ever execute a step that should have been skipped based on prior state?

A financial services company built an agent that processed loan applications. The workflow had seven steps: verify identity, check credit score, validate income, assess debt-to-income ratio, determine eligibility, calculate offer terms, generate approval letter. The eval tested each step independently. Every step passed. In production, fifteen percent of approval letters contained offer terms that did not match the applicant's actual eligibility. The workflow trace revealed why. Step four calculated a debt-to-income ratio based on the income from step three. Step five determined eligibility based on the ratio. But step three sometimes returned an annualized income and sometimes returned a monthly income, depending on the data source. Step four did not normalize the input. It calculated the ratio using whatever value step three returned. When step three returned monthly income, the ratio was twelve times too high. Step five saw the inflated ratio and downgraded eligibility. Step six calculated offer terms based on the downgraded eligibility. The final output was internally inconsistent — the applicant qualified for a better rate than the letter offered, but the workflow never caught the mismatch because no single step failed its isolated test.

The fix was not changing any individual step. The fix was adding trace validation that verified the consistency of the entire workflow state at each transition point. After step three, the trace validator checked that the income value included a unit marker. After step four, it verified that the ratio matched the expected range given the income and debt values. After step six, it verified that the offer terms aligned with the eligibility decision from step five. The eval became a sequence of state assertions, not a collection of isolated step tests.

## Intermediate State Validation

Every step in a workflow produces an intermediate state. That state is input to the next step. If the intermediate state is invalid — missing required fields, containing contradictory values, violating domain constraints — the next step will either fail outright or produce an incorrect result based on corrupted input. Evaluating workflows means validating every intermediate state, not just the final output.

The intermediate state is not always visible. Some agents pass data between steps using internal memory or context that does not appear in logs. Some agents transform data between steps in ways that lose information. The eval pipeline must instrument the workflow to expose intermediate state at every step boundary. This usually means modifying the agent architecture to emit structured events after each step, capturing the full state before it transitions to the next step.

A healthcare company ran a multi-step workflow that generated prior authorization requests. Step one extracted patient information from the EHR. Step two retrieved the treatment history. Step three looked up the requested procedure code. Step four determined whether prior auth was required. Step five generated the request form. Step six submitted it to the payer. The workflow succeeded ninety-eight percent of the time. Two percent of submissions were rejected by the payer for "incomplete information." The team traced the failures back to step two. When a patient had a long treatment history, step two summarized it to fit within the context window. The summarization sometimes dropped critical details — previous failed treatments, contraindications, prior denials. Step four made the auth determination based on the summarized history. Step five generated a request that omitted the dropped details. Step six submitted an incomplete form.

The eval that caught this validated the intermediate state after step two. It checked that the summarized treatment history included all critical events flagged by clinical guidelines — prior denials, adverse reactions, failed treatments, contraindications. If the summary dropped any critical event, the eval failed before the workflow proceeded to step three. This forced step two to revise its summarization logic to preserve critical information even when the full history exceeded the context limit. The fix was architectural — step two learned to store critical events in structured memory outside the summary — but the eval that surfaced the problem validated intermediate state, not final output.

## Workflow Deviation Detection

The workflow has an expected path. For a given input, you expect the agent to execute a specific sequence of steps in a specific order. But agents are not deterministic. They make decisions at runtime based on the information available at each step. Sometimes those decisions lead to deviations from the expected path — skipping a step, executing steps out of order, repeating a step multiple times, taking a branch that should only be triggered under rare conditions.

**Workflow deviation detection** means defining the expected execution path for each test case and flagging any workflow run where the actual path differs from the expected path. The deviation itself is not necessarily a failure. Sometimes the agent makes a correct decision to deviate. But every deviation is a signal that requires review. Either the deviation is correct and the expected path was too rigid, or the deviation is incorrect and the agent made a bad decision.

A legal tech company built an agent that drafted contract amendments. The expected workflow: parse the original contract, identify the clauses to be modified, generate revised language, verify consistency with the rest of the contract, format the amendment, produce the final document. The eval defined this as the canonical path. In production, twelve percent of workflows deviated. Some skipped the consistency verification step. Some repeated the clause identification step multiple times. Some jumped directly from parsing to formatting without generating revised language.

The team reviewed every deviation. Some were legitimate — if the amendment only changed formatting or corrected a typo, the agent correctly skipped the clause modification steps. Some were errors — the agent misunderstood the requested change and skipped revision entirely, producing an amendment that did not modify anything. The deviation detection eval surfaced both. The team tightened the decision logic for when to skip steps and added trace assertions that verified that if a step was skipped, the input met the conditions that made skipping valid. The deviation rate dropped to three percent, all legitimate shortcuts.

Deviation detection works by logging the actual execution path for every workflow run and comparing it to a reference path defined in the test case. The comparison flags three types of deviations: missing steps, extra steps, and out-of-order steps. Missing steps mean the agent skipped something it should have done. Extra steps mean the agent executed something unnecessary or redundant. Out-of-order steps mean the agent violated a dependency — executing step five before step three when step five requires step three's output. The eval categorizes each deviation and escalates unexpected deviations for manual review.

## The Compound Error Problem

Errors compound across workflow steps. A small mistake in step two becomes a larger mistake in step four and a catastrophic mistake by step seven. The agent does not know that step two produced a bad output. It trusts the intermediate state and builds on it. Each subsequent step magnifies the original error until the final output is completely wrong, and the root cause is buried six steps back in the execution trace.

A logistics company ran a multi-step workflow that optimized delivery routes. Step one clustered delivery addresses by geography. Step two assigned each cluster to a vehicle. Step three calculated the optimal route within each cluster. Step four estimated arrival times. Step five generated driver instructions. The final output was a set of routes with time estimates and turn-by-turn directions. In production, eight percent of routes included impossible time estimates — the agent scheduled a driver to be in two places twenty miles apart within fifteen minutes. The error originated in step one. The clustering algorithm occasionally assigned two geographically distant addresses to the same cluster if they shared a zip code prefix. Step two assigned both to one vehicle. Step three tried to route them and produced a path that required impossible speeds. Step four calculated arrival times based on the impossible path. Step five generated instructions that told the driver to violate traffic laws. The final output was nonsense, but every step after step one faithfully executed on corrupted input.

The eval caught this by validating the output of step one before allowing the workflow to proceed. It checked that every cluster met geographic constraints — maximum distance between any two addresses, minimum density, no clusters split across highways or rivers. If step one produced an invalid cluster, the eval failed immediately, before step two compounded the error. The team fixed the clustering logic, and the route quality improved across the board.

Compound error detection requires failing fast. You cannot wait until the workflow completes to discover that step one was wrong. You must validate each step's output before the next step consumes it. This means instrumenting the workflow with assertions at every step boundary — preconditions that verify the input meets requirements, postconditions that verify the output meets guarantees. If any assertion fails, the workflow halts, and the eval logs the failure at the step where it occurred, not at the end of the workflow.

## Sequence Logic Verification

A workflow is not just a sequence of steps. It is a sequence that encodes logic. If condition A, execute step three. If condition B, skip to step five. If the result of step two exceeds a threshold, repeat step two with adjusted parameters. The correctness of the workflow depends on whether the agent executes the right branches in the right order based on the actual conditions at each decision point.

**Sequence logic verification** means testing that the agent makes correct branching decisions based on intermediate state. You define test cases that trigger each branch and verify that the workflow takes the expected path. You define edge cases where the branching condition is ambiguous and verify that the agent resolves ambiguity consistently. You define adversarial cases where the wrong branch leads to serious failures and verify that the agent never takes the wrong path.

An insurance company built a claims processing workflow. The logic: if the claim amount is under ten thousand dollars, auto-approve. If the amount is between ten thousand and fifty thousand and the claimant has no prior claims, auto-approve. If the amount exceeds fifty thousand or the claimant has prior claims, route to a human adjuster. The workflow had three branches. The eval tested all three. It included boundary cases — claims at exactly ten thousand dollars, exactly fifty thousand dollars. It included edge cases — claimants with one prior claim, claimants with fifty prior claims. It included adversarial cases — claims that were just under the auto-approve threshold but had fraud indicators.

The eval discovered that the workflow mishandled boundary cases. A claim for exactly ten thousand dollars took the first branch — auto-approved. A claim for ten thousand and one dollars took the second branch — checked prior claims, then auto-approved if none existed. This meant a ten thousand dollar claim and a ten thousand and one dollar claim were evaluated under different rules. The boundary condition was ambiguous. The team redefined it: if the amount is less than ten thousand dollars, auto-approve. If the amount is greater than or equal to ten thousand and less than fifty thousand, check prior claims. The logic became deterministic at the boundaries, and the eval verified that every boundary case took the correct branch.

Testing the sequence logic means defining test cases that exercise every branch, every loop, every conditional. You are not testing individual steps. You are testing the control flow — verifying that the agent follows the intended logic path based on the actual runtime state. This is the difference between unit testing and integration testing. Unit tests verify that each step works in isolation. Sequence logic tests verify that the steps compose into a coherent workflow that behaves correctly under all conditions.

Multi-step workflow evaluation is where isolated component tests give way to end-to-end system verification, where you stop asking whether each step is correct and start asking whether the workflow as a whole produces the right outcome through the right sequence of actions. The next challenge is ensuring that when something does go wrong in that sequence, the workflow recovers gracefully instead of cascading into catastrophic failure.

