# 6.6 — Loop and Infinite Recursion Detection

The Infinite Loop Trap is the silent killer of agent workflows. The agent executes a step, evaluates the result, decides the result is insufficient, and executes the step again. Then again. Then again. The loop runs until the context window fills, the rate limit is exhausted, or the cost threshold triggers an emergency shutdown. The agent never terminates on its own because it never recognizes that it is stuck. In the worst cases, a single looping workflow consumes thousands of dollars in API costs before anyone notices.

Loops are not always bugs. Productive iteration is a core agent capability. The agent generates a draft, critiques it, revises it, and repeats until the output meets quality criteria. This is useful work. The problem is distinguishing productive iteration from stuck loops — knowing when the agent is making progress toward a goal and when it is repeating the same failed action indefinitely because it cannot recognize that the approach is not working. Your eval pipeline must detect loops early, classify them as productive or stuck, and terminate stuck loops before they cause damage.

## The Anatomy of an Infinite Loop

An infinite loop occurs when the agent's termination condition is never satisfied. The agent is designed to repeat a step until some condition becomes true — the output exceeds a quality threshold, an external validation passes, a user confirms satisfaction. But the condition cannot be met given the current approach, the current inputs, or the current state of the world. The agent does not realize this. It keeps trying the same thing, expecting a different result.

A content generation agent was designed to write product descriptions that met brand guidelines. The workflow: generate a draft, evaluate it against a style rubric, revise if the score is below 0.90, repeat until the score meets the threshold. The eval tested happy path cases where the draft scored 0.85 on the first attempt and 0.92 after one revision. The agent revised once and terminated. The test passed.

In production, the agent looped indefinitely on fifteen percent of products. The draft scored 0.60. The agent revised. The score improved to 0.65. The agent revised again. The score improved to 0.68. The agent revised again. The score improved to 0.70. After twenty iterations, the score plateaued at 0.78, still below the 0.90 threshold. The agent continued revising. The output stopped improving. The agent did not notice. It generated forty-seven revisions before hitting the maximum token limit and crashing. The product description was never finalized. The cost for that single workflow exceeded two hundred dollars.

The root cause: the agent lacked the context to meet the brand guidelines. The guidelines required specific terminology that was not in the training data. The agent could not generate compliant descriptions no matter how many times it revised because it did not know the required vocabulary. The termination condition was unreachable. The agent should have recognized this after the score stopped improving and escalated to a human. Instead, it looped until resource exhaustion.

## Loop Detection Heuristics

**Loop detection heuristics** are rules that flag a workflow as potentially stuck based on execution patterns. The most common heuristics: iteration count, progress stagnation, output similarity, cost accumulation, and time elapsed. None of these perfectly distinguishes productive iteration from stuck loops, but together they provide strong signals that a workflow needs review.

**Iteration count** is the simplest heuristic. If a step has been repeated more than a threshold number of times, flag it. The threshold depends on the task. A creative writing agent might legitimately revise a story fifteen times. A data extraction agent should never need to retry the same API call more than five times. The eval defines iteration limits per step type and fails any workflow that exceeds them.

**Progress stagnation** means the output is not improving with each iteration. The agent repeats the step, but the evaluation score, the user feedback, or the validation result does not change. This indicates the agent is stuck. The eval tracks the metric being optimized — quality score, validation pass rate, user rating — and flags workflows where the metric plateaus across three or more consecutive iterations. If the score goes from 0.60 to 0.65 to 0.68 to 0.70 to 0.70 to 0.70, the workflow is stuck at 0.70 and should terminate.

**Output similarity** means the agent is generating nearly identical outputs on each iteration. If revision three and revision four differ by only punctuation or minor phrasing, the agent is not making meaningful progress. The eval computes similarity between consecutive outputs — using embedding distance, edit distance, or semantic similarity — and flags workflows where consecutive outputs are more than ninety-five percent similar. This catches loops where the agent rephrases the same content without substantively improving it.

**Cost accumulation** is a hard stop. If a single workflow has consumed more than a cost threshold, terminate it regardless of whether it appears to be making progress. A research agent that calls GPT-5 forty times in one session might be doing productive work, but the cost is unsustainable. The eval enforces a maximum spend per workflow and kills any run that exceeds it. This prevents runaway costs even if the loop detection heuristics fail to catch the problem.

**Time elapsed** is another hard stop. If a workflow has been running for more than a maximum duration, terminate it. A customer service conversation should not take thirty minutes. A document generation workflow should not take two hours. The eval sets time limits based on the expected workflow duration and kills any workflow that runs over. This prevents workflows from running indefinitely in the background, consuming resources long after the user has moved on.

A legal document review agent used all five heuristics. Iteration limit: ten revisions per document section. Progress stagnation: if the compliance score does not improve by at least 0.02 across two consecutive iterations, terminate. Output similarity: if consecutive revisions have cosine similarity above 0.97, terminate. Cost limit: fifty dollars per document. Time limit: twenty minutes per document. The agent was allowed to iterate freely within these bounds. If any limit was hit, the workflow terminated, and the current output was flagged for human review. The combination of heuristics caught ninety-eight percent of stuck loops before they caused cost or latency issues.

## Recursion Depth Limits

Recursive workflows are workflows where a step can call itself — the agent executes a task, determines that the task requires a subtask, executes the subtask using the same workflow, and returns the result to the original task. Recursion is powerful for decomposing complex problems, but unbounded recursion is catastrophic. If the base case is never reached or the recursion depth is not limited, the workflow recurses infinitely until it exhausts memory, context, or API limits.

**Recursion depth limits** are hard caps on how many levels deep a recursive call can go. If the workflow reaches the maximum depth, it terminates the recursion and either returns a partial result or escalates. The limit prevents infinite recursion while allowing legitimate recursive problem-solving within reasonable bounds.

A software engineering agent used recursion to debug code. The workflow: run the code, check for errors, if errors exist, identify the root cause, generate a fix, apply the fix, recurse. The agent could call itself to debug the fix if the fix introduced new errors. This was useful when the initial fix was close to correct and only needed minor adjustment. It was catastrophic when the initial fix was fundamentally wrong and every subsequent fix introduced new errors.

The eval set a recursion depth limit of three. The agent could recurse twice — fix the original code, fix the fix if needed, fix the second fix if needed. On the fourth level, the workflow terminated and escalated to a human. The eval tested this with code that could not be fixed through iteration — code with unsatisfiable constraints, code that required architectural changes, code with conflicting requirements. The agent attempted three fixes, hit the recursion limit, and escalated. Without the limit, it would have recursed until the context window filled with hundreds of broken fix attempts.

Recursion limits are enforced by the workflow runtime, not the agent itself. The agent does not know how deep the recursion stack is. The runtime tracks the call depth and kills the workflow when the limit is reached. The eval verifies that the runtime correctly enforces the limit by running test cases designed to recurse indefinitely and checking that they terminate at the expected depth.

## Distinguishing Productive Iteration from Stuck Loops

Not every loop is stuck. The agent that revises a draft ten times and improves the quality score from 0.60 to 0.95 is doing productive work. The agent that revises a draft ten times and the score moves from 0.60 to 0.62 is stuck. The eval must distinguish between the two without killing productive workflows.

The key signal is whether each iteration produces measurable improvement. If the agent is making progress toward the termination condition — the score is increasing, the validation failures are decreasing, the user feedback is improving — the loop is productive. If the termination condition is not getting closer, the loop is stuck.

A customer service agent used iteration to resolve support tickets. The workflow: propose a solution, validate it against the customer's issue, revise if validation fails, repeat until validation passes. The eval defined productive iteration as any loop where the validation failure count decreased with each iteration. Initial proposal: validation fails on four criteria. Revision one: validation fails on two criteria. Revision two: validation passes. This is productive iteration. Total cost: three LLM calls. Total time: eight seconds.

Contrast with a stuck loop: Initial proposal: validation fails on four criteria. Revision one: validation fails on four criteria. Revision two: validation fails on three criteria. Revision three: validation fails on four criteria. The validation count is not decreasing monotonically. The agent is not converging. After three iterations with no clear progress, the eval terminated the loop and escalated to a human agent. The customer received a response in twenty seconds instead of waiting for the agent to loop indefinitely.

Productive iteration has a trajectory. Each step moves the agent closer to the goal. Stuck loops have no trajectory. The agent churns without converging. The eval detects the difference by tracking the goal metric over time and fitting a trend line. If the trend is flat or oscillating, the loop is stuck. If the trend is consistently improving, the loop is productive.

## Cost Explosion from Loops

A single stuck loop can consume more resources than a thousand successful workflows. The agent calls an LLM on every iteration. If the loop runs for fifty iterations, the workflow makes fifty LLM calls. If the LLM is GPT-5, each call costs ten cents. The workflow costs five dollars. If ten workflows loop simultaneously, the bill is fifty dollars. If this happens at scale — hundreds of workflows per hour, fifteen percent of which loop — the monthly cost reaches six figures before anyone notices.

A customer support platform deployed an agent in December 2025. Average cost per conversation: twelve cents. Average conversation length: four exchanges. The agent handled ten thousand conversations in the first week. Total cost: twelve hundred dollars, in line with projections. In week two, a software update introduced a bug in the termination logic. The agent stopped recognizing when a conversation was resolved. It continued generating follow-up responses even after the customer stopped replying. The average conversation length increased to forty exchanges. The average cost per conversation increased to one dollar twenty. The weekly cost jumped to twelve thousand dollars. The finance team noticed when the bill hit thirty thousand dollars. The engineering team identified the bug, rolled back the deployment, and added loop detection to the eval pipeline to catch termination logic failures before they reached production.

The eval now includes cost-based circuit breakers. Every workflow has a maximum cost budget. If the cumulative API cost exceeds the budget, the workflow terminates immediately, and an alert is fired. The budget is set conservatively — twice the expected cost for a normal workflow. Any workflow that exceeds the budget is either looping or doing something unexpected and warrants investigation. The circuit breaker prevents a single bad workflow from consuming thousands of dollars before detection.

## Early Termination Strategies

When the eval detects a stuck loop, it must terminate the workflow. The termination strategy depends on the workflow state and the cost of partial outputs. The three strategies: immediate termination, graceful termination with partial output, and escalation to human review.

**Immediate termination** kills the workflow and returns an error. This is appropriate when the workflow is in an unrecoverable state — the loop has consumed all available retries, the cost limit has been exceeded, or the time limit has been reached. The user receives a failure message, and the workflow logs include the full execution trace for debugging. Immediate termination prevents further resource consumption but provides no value to the user.

**Graceful termination with partial output** stops the loop and returns the best output generated so far. This is appropriate when the workflow has produced something useful but cannot reach the termination condition. The agent generated ten revisions of a document, and the quality score plateaued at 0.85, below the target of 0.90. The eval terminates the loop and returns the 0.85 version with a flag indicating it did not meet the full quality bar. The user gets a useful output and can decide whether to accept it or request human review.

**Escalation to human review** stops the loop and routes the workflow to a human operator. This is appropriate when the task is high-stakes or when the partial output is not useful without further refinement. A medical diagnosis agent that loops without reaching a confident conclusion should not return a low-confidence diagnosis. It should escalate to a human clinician. The eval detects the loop, terminates the workflow, and creates a ticket for human review with the full execution trace and intermediate outputs.

The choice of termination strategy is encoded in the eval configuration. Each workflow type defines its preferred strategy based on the use case. The eval enforces the strategy when loop detection heuristics trigger. This ensures that stuck loops are handled consistently and that workflows fail safely instead of consuming unbounded resources.

Loop detection is the last line of defense against workflows that never finish. With multi-step workflows validated, error recovery tested, and loops contained, the next question is ensuring that workflows stop at the right time — that termination conditions are correctly defined and consistently enforced.

