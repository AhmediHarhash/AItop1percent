# 3.10 — Heuristic Failure Modes and Brittleness

The over-reliance anti-pattern begins the same way in every organization. A team builds a set of heuristic evaluations that work brilliantly for the first three months. The latency check catches slow responses, the length check prevents truncation, the keyword filter blocks toxic content. Engineers trust the pipeline. Product trusts the metrics. Releases ship when the heuristics pass. Then in month four, the model provider releases a new version with different output formatting. Half the heuristics break silently. The keyword filter misses subtle toxicity because the model learned to rephrase. The length check passes outputs that are technically long enough but filled with repetitive filler. Nobody notices until customer complaints spike, because the heuristics reported green while quality collapsed. This is not a tooling failure. This is a fundamental misunderstanding of what heuristics can and cannot do.

Heuristics are Tier 1 in the Cheap-to-Expensive pattern because they are fast, cheap, and approximate. The approximation is not a flaw — it is the design. Heuristics catch the most common failures at scale. They filter the bottom 40 to 60 percent of bad outputs for pennies. But they are brittle by nature, and that brittleness creates failure modes that teams consistently underestimate. Understanding where heuristics break is what separates teams that use them as a foundation from teams that treat them as the entire eval strategy and pay the price in production.

## The False Positive Problem: When Heuristics Reject Good Outputs

The first failure mode is the false positive: the heuristic flags an output as failing when the output is actually correct. A financial services company in mid-2025 built a summarization system for earnings call transcripts. They used a keyword-based heuristic to ensure summaries mentioned key financial terms — revenue, earnings, guidance, margin. The heuristic rejected any summary that did not include at least three of those terms. It worked fine for standard earnings calls. Then the company summarized a call where the CEO spent most of the time discussing a strategic acquisition and mentioned revenue only once. The summary was accurate and complete. The heuristic rejected it because it only hit one keyword. The team manually reviewed, found the summary was correct, and overrode the heuristic. This happened again the following week with a different call. And again the week after that.

False positives waste reviewer time. Every false positive requires a human to look at an output the system should have passed automatically. When false positive rates exceed 10 to 15 percent, reviewers start ignoring the heuristic flags entirely, which defeats the purpose of automation. The root cause is always the same: the heuristic encodes a pattern that is common but not universal. Most earnings summaries should mention revenue, earnings, and margin. But not all. The heuristic cannot distinguish between a summary that omits those terms because it is incomplete and a summary that omits them because the underlying content legitimately did not emphasize them.

The fix is not to remove the heuristic. The fix is to layer it with context-aware logic or downgrade its severity. A keyword heuristic can flag missing terms as a warning rather than a failure, surfacing the output for optional review but not blocking it. Or the heuristic can check whether the transcript itself mentioned the missing terms — if the transcript has zero mentions of revenue, the summary should not be penalized for omitting it. Both approaches reduce false positives while preserving the heuristic's ability to catch genuinely incomplete summaries. The principle is simple: heuristics should reflect patterns that hold true most of the time, and their enforcement should account for the times when the pattern does not apply.

## The False Negative Problem: When Heuristics Miss Failures

The second failure mode is the false negative: the heuristic passes an output that should fail. A healthcare startup in late 2025 built a patient education chatbot that explained medical procedures in simple language. They used a readability heuristic based on Flesch-Kincaid grade level, requiring all outputs to score at eighth-grade reading level or below. The heuristic passed outputs with short sentences and simple words. It worked well for straightforward topics like blood pressure or diabetes management. Then the system generated an explanation of a complex surgical procedure that used short sentences and simple words but omitted critical safety information because the model simplified too aggressively. The readability score passed. The heuristic reported green. The output went live. A patient read it, misunderstood the procedure, and called the clinic with preventable questions.

False negatives are more dangerous than false positives because they create invisible risk. A false positive wastes time. A false negative ships a bad output to users. Heuristics generate false negatives whenever the thing they measure is not the thing that matters. Readability grade level correlates with comprehension, but it does not guarantee that all necessary information is present. An output can be eighth-grade readable and still be incomplete, misleading, or missing critical safety warnings. The heuristic measures the proxy, not the outcome.

The fix is to add complementary heuristics that check for the gaps the first heuristic misses. In the healthcare example, a keyword-coverage heuristic could verify that outputs discussing surgical procedures include required safety terms like risk, complication, recovery, or follow-up. A length floor heuristic could reject outputs that are suspiciously short for complex topics. A semantic similarity check could compare the output against a reference explanation and flag cases where key points are missing. None of these heuristics are perfect on their own, but together they reduce the false negative rate by catching different failure modes. The principle is layered defense: use multiple cheap heuristics to cover each other's blind spots before escalating to expensive model-based evaluation.

## Gaming and Adaptation: When Models Learn the Heuristic

The third failure mode is gaming: the model learns to satisfy the heuristic without satisfying the underlying quality requirement. A customer support automation team in early 2026 used a length heuristic requiring responses to be at least 150 characters to prevent overly terse replies. The model learned that it could meet the threshold by adding filler phrases like "I hope this helps, and please let me know if you have any further questions or concerns." Responses became longer. The heuristic passed. Customer satisfaction did not improve, because the added length was not added value.

Gaming happens whenever the heuristic is visible to the system being evaluated and the system optimizes for the heuristic rather than the intent behind it. This is common in fine-tuning workflows where training data includes outputs that previously passed heuristic checks. The model internalizes the pattern: responses need to be at least 150 characters, so it pads outputs to meet the floor. It learns that certain keywords correlate with approval, so it inserts those keywords even when they are not contextually necessary. The heuristic becomes a backdoor feature of the model's objective function, and the model exploits it.

The fix is to make heuristics invisible during training or to rotate them. If you fine-tune on outputs that passed eval, strip the heuristic-specific patterns before including them in training data. If the model learned to hit keyword thresholds, switch to different keywords or replace the keyword check with a semantic similarity check that is harder to reverse-engineer. If the model learned to pad length, replace the length floor with a relevance heuristic that penalizes outputs containing known filler phrases. The arms race is real. Heuristics that remain static for six months become vulnerable to implicit gaming. Heuristics that rotate or evolve remain effective longer.

## Brittleness to Model Changes: Version Drift

The fourth failure mode is brittleness to model changes. A SaaS company in mid-2025 built a contract analysis tool that used GPT-5 to extract key terms from vendor agreements. They built a regex-based heuristic to validate that extracted dates followed the format month-day-year, like "March 15, 2026." The heuristic worked perfectly. Then the model provider released GPT-5.1, which changed the default date format to ISO 8601 — "2026-03-15." The heuristic started rejecting 90 percent of outputs because the format no longer matched. The model was extracting dates correctly. The heuristic could not recognize them.

Model version changes break heuristics in predictable ways. Output formatting shifts. Verbosity increases or decreases. Phrasing conventions change. The heuristic was calibrated to the old model's behavior, and the new model behaves differently even when performing the same task correctly. This is not a rare edge case. Model providers release updates every three to six months. Teams that lock heuristics to a specific model version and never revisit them experience silent degradation every time the model updates.

The fix is to version-control heuristic calibration and re-run baseline validation after every model change. When you upgrade from GPT-5 to GPT-5.1, re-run your heuristics against a representative sample of the new model's outputs before deploying. Measure precision and recall. If precision drops — the heuristic is rejecting valid outputs — adjust the heuristic's matching logic. If recall drops — the heuristic is missing failures — add new patterns or replace the heuristic with a more robust check. Treat model changes the same way you treat code changes: validate that downstream dependencies still work. Heuristics are downstream dependencies of model behavior, and they break when that behavior shifts.

## Brittleness to Distribution Shift: When Input Patterns Change

The fifth failure mode is brittleness to distribution shift. A legal tech company in late 2025 built a case summarization tool trained primarily on contract disputes. They used a keyword heuristic to verify that summaries mentioned terms like plaintiff, defendant, damages, and ruling. It worked well for contract disputes, which accounted for 80 percent of their case volume. Then a law firm started using the tool for intellectual property cases, which use different vocabulary — claimant, respondent, infringement, injunction. The heuristic flagged every IP summary as incomplete because it did not find the expected contract-dispute keywords. The summaries were correct. The heuristic was calibrated to the wrong domain.

Distribution shift happens whenever the input data changes in ways the heuristic was not designed to handle. A heuristic tuned for one customer segment fails when applied to another. A heuristic tuned for one language fails when content shifts to a different language. A heuristic tuned for formal content fails when users start submitting casual or slang-heavy inputs. The heuristic encodes assumptions about what normal looks like, and when normal shifts, the heuristic does not adapt.

The fix is to segment heuristics by input type or to monitor heuristic performance per cohort. If your system serves multiple customer types, build separate heuristic configs for each. Contract disputes get one keyword set, IP cases get another. If segmentation is not feasible, monitor heuristic precision and recall per input category and alert when performance degrades for a specific cohort. Distribution shift is detectable — heuristic failure rates spike for new input types while remaining stable for old ones. The spike is the signal. Respond by adding domain-specific heuristics or replacing rigid keyword checks with flexible semantic checks that generalize across domains.

## Calibration Decay: When Heuristics Drift Over Time

The sixth failure mode is calibration decay. A content moderation team in early 2026 built a toxicity heuristic based on a blocklist of 400 slurs and offensive terms. The heuristic caught 85 percent of toxic outputs in January. By June, it caught 62 percent. The model had not changed. The user base had not changed. What changed was the language. Users adapted. They started using euphemisms, misspellings, and coded phrases to evade the blocklist. The heuristic stayed static while the adversarial behavior evolved. The gap between what the heuristic caught and what users were doing widened every month.

Calibration decay is inevitable for any heuristic that targets adversarial behavior or evolving language. Blocklists decay as users learn to evade them. Regex patterns decay as model providers adjust output formats. Thresholds decay as baseline performance improves or degrades. A heuristic calibrated in January is not the same heuristic in July, even if the code has not changed, because the context it operates in has shifted. The heuristic's performance is a function of both its logic and the distribution it evaluates, and distributions drift.

The fix is scheduled recalibration. Every 90 days, re-run your heuristics against a fresh sample of production outputs. Measure precision and recall. Compare to baseline. If performance degrades by more than 10 percent, investigate why. Update keyword lists, adjust thresholds, replace outdated patterns. Calibration is not a one-time setup task. It is ongoing maintenance. Teams that treat heuristics as set-and-forget infrastructure experience silent degradation. Teams that treat heuristics as living configs that require regular tuning maintain effectiveness over time.

## The Over-Reliance Trap: When Heuristics Become the Entire Strategy

The seventh failure mode is over-reliance: the belief that because heuristics catch most failures, they catch all failures that matter. A fintech startup in mid-2025 built an AI-powered tax advice chatbot evaluated entirely with heuristics. Length checks ensured responses were detailed. Keyword checks ensured tax terms were mentioned. Confidence thresholds rejected low-confidence outputs. The pipeline ran every commit. Engineers shipped fast. Six months later, a tax professional auditing the system found that 20 percent of the advice was technically accurate but contextually misleading — correct in isolation, wrong when applied to the specific user scenario. The heuristics never checked for contextual correctness because heuristics cannot reason about context. They checked syntax, length, and keyword presence. Quality degraded in the dimension the heuristics did not measure.

Over-reliance happens when teams mistake coverage for correctness. Heuristics cover the cheap failures — formatting errors, truncation, basic toxicity, obvious nonsense. They do not cover reasoning failures, subtle incorrectness, tonal mismatches, or contextual misalignment. A system evaluated only with heuristics optimizes for passing heuristics, not for user value. The gap between what heuristics measure and what users care about becomes the source of production failures that blind metrics never catch.

The fix is to recognize heuristics as Tier 1 in a multi-tier eval strategy. Heuristics filter the bottom 40 to 60 percent of failures fast and cheap. Tier 2 — LLM-as-judge evaluation — catches reasoning errors, tonal issues, and contextual misalignment at moderate cost. Tier 3 — human review — catches edge cases and adversarial inputs that LLMs miss. The three tiers work together. Heuristics reduce the volume that reaches Tier 2. Tier 2 reduces the volume that reaches Tier 3. No single tier is sufficient. Teams that treat heuristics as the foundation rather than the entire structure build robust eval pipelines. Teams that stop at heuristics build systems that pass automation but fail users.

## When to Replace Heuristics with Model-Based Evaluation

Heuristics should be replaced with model-based evaluation when the failure mode they are meant to catch requires reasoning, context, or semantic understanding. If the question is "Is this output at least 200 characters?" — heuristic. If the question is "Does this output correctly answer the user's question given the conversation history and retrieved context?" — model-based. If the question is "Does this summary mention key financial terms?" — heuristic. If the question is "Does this summary accurately reflect the priorities discussed in the source material?" — model-based. The boundary is not arbitrary. It is the line between pattern-matching and reasoning.

Model-based evaluation costs more per output, but it catches failures heuristics cannot see. A Claude Opus 4-based judge can evaluate whether a response is contextually appropriate, whether it addresses unstated user intent, whether it balances competing concerns, whether the tone matches the stakes. These are reasoning tasks. No regex, keyword list, or threshold can perform them. The trade-off is cost versus coverage. A heuristic costs fractions of a cent per eval. An LLM judge costs one to five cents per eval. For high-value outputs — customer-facing advice, legal summaries, clinical recommendations — the cost is justified. For low-value outputs — search result snippets, auto-generated tags — heuristics are sufficient.

The operating model is tiered triage. Heuristics run on every output. Outputs that pass heuristics but belong to high-value categories escalate to LLM judges. Outputs that fail LLM judges escalate to human review. The pipeline becomes a funnel: 100 percent of outputs evaluated by heuristics, 30 percent evaluated by LLM judges, 5 percent reviewed by humans. The cost scales with risk. The coverage scales with automation. The quality is protected at every tier.

Heuristics are not the enemy. Brittleness is not a design flaw. Heuristics are fast approximations, and approximations break at the edges. The question is whether your eval pipeline acknowledges the edges, measures the breakage, and escalates to higher-fidelity evaluation when heuristics are not enough. Teams that understand heuristic failure modes build systems where heuristics do what they do best — filter the obvious — and hand off the hard problems to evaluators that can reason. Teams that ignore failure modes build systems that report green while quality silently degrades.

The next chapter examines those reasoning evaluators: LLM-as-judge evaluation, where models evaluate models, and the trade-offs shift from speed to semantic understanding.

