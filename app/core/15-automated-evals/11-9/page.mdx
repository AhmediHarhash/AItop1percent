# 11.9 — Debugging Eval Failures in Production

The alerts fire at 2:47am. Your customer support eval suite just dropped from a 94% pass rate to 68% in the last hour. Production traffic is still flowing. Users aren't complaining. The model version hasn't changed. But something broke, and you're the one paged to figure out what. You pull up the dashboard, scan the failing cases, and see responses that look perfectly reasonable. The eval is marking them as failures, but they read like improvements. You have no idea if the eval broke, the model broke, or something else entirely shifted beneath you. This is not a code bug. This is an eval failure, and it requires a different kind of investigation.

Eval failures in production come in two shapes, and only one of them wakes you up at night. The first is the **false negative** — the eval starts failing when it shouldn't, throwing alerts for outputs that are actually fine. The second is worse: the **false positive**, where the eval keeps passing while production quality silently degrades. The false negative is noisy and annoying. The false positive is silent and dangerous. Your debugging process has to catch both. When an eval that's been stable for months suddenly changes behavior, the cause is almost never a bug in the eval code itself. It's drift — in the model, in the judge, in the traffic, or in the assumptions baked into your thresholds. Debugging eval failures means tracking down which part of a distributed system shifted without telling you.

## Step One: Determine If the Eval Is Wrong or the Output Is Wrong

The first question is not "what changed" — it's "who's lying." You have an eval that says the output is bad. You need to know if the eval is correct. The fastest way to answer this is to manually review a sample of the failing cases yourself. Not all of them — ten to twenty representative failures is enough. Read the input, read the output, and decide: is this output actually bad, or is the eval misfiring? If the output genuinely looks wrong — confusing, incorrect, off-tone, missing required information — then the eval is doing its job and the model degraded. If the output looks fine or even better than before, then the eval broke. This sounds obvious, but teams skip this step constantly and waste hours debugging the wrong system.

When you review those ten failing cases, you're not just looking for quality. You're looking for patterns. Are all the failures in a specific domain or format? Are they all edge cases your eval wasn't designed to handle? Are they all cases where the model changed its output structure slightly — maybe it started adding a politeness phrase the eval didn't expect, or it reformatted dates in a way that breaks your regex-based checker? If the pattern is structural, the eval is brittle. If the pattern is semantic, something actually degraded. The distinction matters because it dictates your next move. A brittle eval needs tolerance updates. A degraded model needs a rollback or mitigation.

You also need to check whether the eval is measuring the right thing in the first place. Sometimes an eval starts failing because the model improved in a way the eval wasn't designed to recognize. A summarization eval that penalizes outputs longer than 200 words will fail when the model starts producing denser, slightly longer summaries that users actually prefer. A tone eval calibrated for formal responses will fail when the model starts matching user informality. If your manual review shows that the failing outputs are better than the passing ones, your eval didn't break — it became obsolete. The model evolved past it. This happens more often than teams admit, especially in domains where user preferences shift quickly.

## Step Two: Check for Recent Changes

Once you know whether the eval or the output is wrong, the next step is to map the timeline. What changed in the six hours before the failure rate spiked? Start with the obvious: did you deploy a new model version, update a prompt, or modify an eval configuration? Check your deployment logs, your prompt version history, and your eval commit history. If any of those changed within a day of the failure, that's your prime suspect. A model version bump can shift output distributions in ways that break evals calibrated to the previous version. A prompt change can alter response structure enough to trip format-based checks. An eval threshold adjustment can turn what used to be a marginal pass into a hard fail.

But the changes aren't always in systems you control. If you're using a third-party model via API, the provider might have updated the model without announcing it. OpenAI, Anthropic, and Google all reserve the right to improve models in place. What was GPT-5 yesterday might not be exactly the same GPT-5 today. If your failure spike happened without any changes on your side, check the provider's changelog and status page. If they pushed a model update within 48 hours of your failure, that's likely your cause. The fix is either to adjust your eval to the new output distribution or to pin to a specific model snapshot if the provider offers that option.

The other common invisible change is dataset drift. If your eval suite includes a static test set, and the test set hasn't been updated in six months, it might no longer represent production traffic. User behavior shifts. Product features change. New edge cases appear. Your eval keeps testing the old world while production has moved on. Check your production logs for distribution changes. Are users suddenly asking questions in a different format? Are they using new vocabulary or referencing new features you haven't added to your test cases? If yes, your eval didn't break — it aged out. The fix is not to tweak the eval, it's to refresh the dataset.

## Step Three: Compare to Human Judgment on the Same Cases

Automated evals are proxies for human judgment. When they fail, you need to know if they still correlate with the ground truth. Pull the same ten to twenty failing cases you reviewed manually and run them through a human eval process — not you alone, but at least two independent reviewers who don't know the cases failed the automated check. Ask them to score the outputs on the same criteria your automated eval measures. If the humans agree with the automated eval, the eval is still valid and the model genuinely degraded. If the humans disagree, the eval lost calibration.

Calibration loss happens in two directions. The eval can become too strict, failing outputs that humans rate as acceptable. Or it can become too lenient, passing outputs that humans rate as poor. The too-strict case is the one that generates alerts. The too-lenient case is the silent killer that lets quality decay without triggering any alarms. When you compare human and automated judgments, you're checking both. If your automated pass rate is 92% but your human pass rate on the same sample is 78%, your eval is passing things it shouldn't. If your automated pass rate is 68% but your human pass rate is 91%, your eval is failing things it shouldn't. Either way, the eval needs recalibration.

The comparison also reveals whether your eval criteria still match what users care about. Sometimes an eval starts failing because it's measuring something users don't actually value anymore. A customer support eval might penalize responses that don't include a case number, but if your product stopped requiring case numbers three months ago, the eval is enforcing a policy that no longer exists. Human reviewers will tell you this. They'll say "I don't care about the case number, the response is helpful." That's a signal to update the eval spec, not to adjust the threshold.

## Step Four: Check Judge Model Calibration

If you're using an LLM-as-a-judge eval, the judge itself might have drifted. LLM judges are not static. If you're calling a hosted model, the provider can update it. If you're running your own fine-tuned judge, context window changes or prompt drift can shift its behavior. The way to check this is to re-run a **calibration set** — a small, stable set of cases with known-good human labels that you use to verify judge consistency over time. If you don't have a calibration set, you need one. It should be 50 to 100 cases that span the quality range, with human-verified scores. Run your current judge on those cases and compare its scores to the original human scores. If correlation dropped, the judge drifted.

Judge drift has a few common causes. The most frequent is a model version update. If you were using Claude Opus 4 as your judge and Anthropic released Opus 4.1, and you didn't pin your version, your judge changed without you noticing. The new version might score cases differently — stricter on tone, looser on factual accuracy, more sensitive to formatting quirks. The fix is to either pin to a specific model version or to re-calibrate your judge on the new version and adjust thresholds accordingly.

Another cause is prompt drift. If your judge prompt is dynamically constructed — maybe you inject examples or context based on the task type — and something in that construction logic changed, the judge sees a different prompt and behaves differently. Check your judge prompt logs. Compare the prompts used today to the prompts used a week ago. If they differ, trace the change. Sometimes a well-intentioned prompt improvement makes the judge stricter or changes its interpretation of edge cases. That's not necessarily wrong, but it means your historical baseline no longer applies.

The third cause is judge contamination. If your judge has ever seen the test cases during training or fine-tuning, it might memorize answers or develop biases. This is rare for general-purpose judges but common for task-specific fine-tuned judges. If your failure spike happened after you fine-tuned a judge on a dataset that overlapped with your production eval set, the judge might be scoring based on memorized patterns instead of actual quality. The fix is to ensure strict separation between judge training data and eval test cases, and to retrain the judge if contamination is confirmed.

## Step Five: Check Dataset Representativeness

Your eval suite passes or fails based on a test set. If that test set no longer matches production traffic, the eval results stop being meaningful. This is **distribution shift**, and it's one of the most common root causes of unexplained eval failures. The way to check for it is to compare the statistical properties of your test set to a recent sample of production traffic. Look at input length, vocabulary, task type distribution, user intent, and edge case frequency. If production traffic shifted toward longer inputs but your test set still uses short examples, your eval is testing the wrong distribution. If users started asking multilingual questions but your test set is English-only, your eval is blind to a growing failure mode.

Distribution shift happens gradually, which makes it hard to catch. Users don't wake up one day and start behaving completely differently. They shift incrementally — asking slightly longer questions, using slightly different phrasing, referencing features that didn't exist when you built the test set. Over six months, those small shifts compound into a meaningful divergence. Your eval still runs, still produces scores, but the scores no longer predict production quality because the eval is measuring a dataset that looks like last year's users, not this year's.

The fix is to refresh your test set on a regular cadence — quarterly at minimum, monthly if traffic patterns change quickly. Refreshing doesn't mean throwing away the old test set. It means sampling new cases from production, labeling them, and either adding them to the existing set or replacing the oldest cases. The goal is to keep the test set representative of current traffic. If your eval suite has 5,000 cases and you replace 500 per quarter, you'll turn over the full set every two and a half years while maintaining continuity for trending and historical comparison.

You also need to check for **edge case inflation**. Sometimes production traffic develops a new edge case that your eval never anticipated. A summarization system that worked fine for news articles starts failing when users feed it legal documents. A customer support bot that handled product questions breaks when users start asking about account security. If your failure spike is concentrated in a specific input type, and that input type is new or rare in your test set, your eval isn't catching the failure mode that matters. The fix is to add those edge cases to the test set and decide whether they're critical enough to adjust your overall thresholds or whether they need a separate eval track.

## Step Six: Check Threshold Appropriateness

Your eval doesn't just measure quality — it makes a binary decision based on a threshold. If your threshold is set at 0.85 and the eval scores a case at 0.83, that's a fail. But thresholds are not universal truths. They're calibrated to a specific baseline — the model version, the traffic distribution, the judge behavior at the time you set the threshold. When any of those baselines shift, the threshold can become inappropriate. A threshold that was right three months ago might be too strict or too lenient today.

The way to check threshold appropriateness is to look at score distributions over time. Plot your eval scores for the past 30 days and compare them to the scores from when you first set the threshold. If the entire distribution shifted — maybe the median score used to be 0.89 and now it's 0.81 — then your threshold is either too high or the model genuinely degraded. If the distribution shape changed — maybe you used to have a tight cluster around 0.90 and now you have a bimodal distribution with peaks at 0.95 and 0.75 — then something fundamental changed in how the model or eval behaves, and a single threshold no longer makes sense.

Threshold decay happens when a threshold is set based on a model's initial performance and never revisited. The model improves, the average score rises, and the threshold that used to filter out the bottom 10% of outputs now filters out the bottom 2%. The eval still passes, but it's no longer catching meaningful quality issues because it's calibrated to an outdated baseline. The opposite also happens: the model degrades slightly, the average score drops, and the threshold that used to pass 95% of outputs now passes only 85%. The eval starts failing more often, but the failures are marginal cases that users don't actually care about. In both scenarios, the threshold needs recalibration based on current performance and current user expectations.

The recalibration process is not "make the threshold looser so the eval passes." It's "validate the threshold against current human judgment and current production quality." Run a human eval on a fresh sample of production traffic. Calculate the score at which human reviewers transition from "acceptable" to "unacceptable." That's your new threshold. If it's different from your old threshold, update it and document why. Threshold changes are not admissions of failure — they're maintenance.

## Root Cause Categories

When you finish the six-step investigation, you should land on one of five root cause categories. **Model change** means the model version, parameters, or behavior shifted. This includes intentional updates, unintentional provider changes, and emergent behavior drift. The fix is to either roll back, adjust the eval to the new model, or accept the new behavior if it's actually better. **Judge change** means the LLM judge or rule-based scorer shifted. This includes model version updates, prompt changes, and calibration drift. The fix is to re-calibrate the judge or pin to a stable version. **Traffic change** means the production input distribution shifted. This includes new user behaviors, new edge cases, and seasonal or product-driven traffic changes. The fix is to refresh the test set and potentially add new eval tracks.

**Threshold decay** means the threshold is no longer appropriate for current performance levels. This includes both cases where the model improved past the threshold and cases where the model degraded but the threshold was too lenient to catch it. The fix is to recalibrate thresholds based on current human judgment. **Contamination** means the eval test set leaked into training data, the judge saw the test cases, or the eval criteria are no longer independent of the optimization target. The fix is to rebuild the test set, retrain the judge with clean splits, or redesign the eval to measure different criteria.

Most eval failures are multicausal. You might have a model update that shifted output structure, combined with a judge that's slightly too strict, combined with a test set that's slightly stale. The six-step process helps you disentangle the causes and prioritize fixes. If the model genuinely degraded, that's the urgent fix — roll back or mitigate. If the eval drifted, that's a medium-priority fix — recalibrate and monitor. If the test set aged, that's a long-term fix — refresh on a cadence. Don't try to fix everything at once. Fix the highest-impact cause first, verify the fix, then move to the next one.

## The Debugging Log

Every eval failure investigation should be documented in a **debugging log** — a shared record that captures the failure mode, the investigation steps, the root cause, and the fix. The log is not just for historical reference. It's a pattern recognition tool. When you see the same failure mode three times in six months, that's a signal that something systemic is broken. Maybe your threshold recalibration process is too slow. Maybe your test set refresh cadence is too infrequent. Maybe your judge calibration suite is missing key edge cases. The log reveals these patterns in a way that individual incident responses don't.

The debugging log should include the failure timestamp, the eval suite and metric that failed, the failure rate before and after the spike, the hypothesis tested during investigation, the root cause identified, the fix applied, and the resolution timestamp. It should also include the secondary effects — did fixing this eval break another eval? Did recalibrating the threshold change your production deployment gate? Did refreshing the test set reveal a new failure mode? These details matter because eval systems are interconnected. A fix in one place can cause a regression somewhere else, and the log helps you track those dependencies.

The log also serves as training material. When a new engineer joins the team and sees an eval failure for the first time, they can search the log for similar past incidents and learn the investigation process. When you're deciding whether to invest in better judge calibration tooling, you can scan the log and see how often judge drift was the root cause. When a stakeholder asks "how often do our evals fail?" you can answer with data, broken down by cause. The log turns individual incidents into institutional knowledge.

## Postmortem Process

When an eval failure causes production impact — quality degraded, users complained, revenue was lost, or trust was damaged — you need a postmortem. The postmortem is not about blame. It's about understanding what allowed the failure to reach production and what safeguards need to be added. The postmortem should answer five questions. What happened? Why did the eval fail to catch it? Why did the secondary safeguards fail? What could have detected it earlier? What changes will prevent this class of failure in the future?

The postmortem reveals systemic gaps. Maybe your eval suite caught the quality drop, but the alert threshold was set too high so it didn't fire. Maybe the eval didn't catch it because the test set didn't include the failure mode. Maybe the eval caught it, the alert fired, but the on-call engineer dismissed it as a false positive because there was no investigation playbook. Each of these is a different fix. The first needs threshold tuning. The second needs test set expansion. The third needs process documentation and training. Without the postmortem, you fix the symptom — the specific eval that failed — without fixing the system that let it fail.

The postmortem should include action items with owners and deadlines. "Improve eval coverage" is not an action item. "Add 200 test cases covering legal document summarization by end of quarter, owner: DataEng" is an action item. "Make evals more robust" is not an action item. "Re-calibrate judge on current production traffic and set new thresholds by next sprint, owner: MLEng" is an action item. The postmortem document should be shared with Engineering, Product, and any stakeholder who was affected by the production impact. It should also be added to the debugging log so future investigations can reference it.

Postmortems are not punishment. They're learning. The teams that run postmortems after every eval-related production incident build better systems because they treat each failure as a data point, not a one-off mistake. The teams that skip postmortems repeat the same failures every quarter because they never analyzed why the safeguards didn't work.

---

When you've debugged the eval failure and identified the root cause, the next step is not just to fix it — it's to close the loop so the same failure can't happen again. Subchapter 11.10 covers closed-loop remediation: the process of turning eval failures into automated fixes, updated test cases, and improved monitoring so the system learns from every incident.

