# Chapter 5 — Reference-Based and Ground Truth Evals

Reference-based evaluation compares outputs to known-correct answers. When you have a golden answer set, you can measure whether your model produces outputs that match — exactly, semantically, or factually. This is the most grounded form of automated evaluation because you are comparing to truth, not to another model's opinion. But reference-based evals have their own challenges: golden sets are expensive to build, they become stale as requirements evolve, annotators disagree on what counts as correct, and many tasks have multiple valid answers that exact matching cannot handle. This chapter covers how to build and maintain golden datasets, how to handle label disagreement, how to choose between exact and semantic similarity, and how to evaluate creative tasks where no single reference can capture correctness.

---

- 5.1 — Golden Answer Sets: Building and Maintaining Ground Truth
- 5.2 — Golden Dataset Sampling Strategy: What to Add Next and Why
- 5.3 — Label Disagreement and Adjudication Workflows
- 5.4 — Golden Set Lifecycle and Ownership
- 5.5 — Exact Match vs Semantic Similarity Scoring
- 5.6 — Embedding-Based Similarity Metrics
- 5.7 — BLEU, ROUGE, and Classical NLP Metrics
- 5.8 — Factual Consistency Checking
- 5.9 — Citation-to-Claim Alignment Scoring
- 5.10 — Expected Tool Call Verification
- 5.11 — Multi-Reference Evaluation Strategies
- 5.12 — When References Are Wrong: Stale Ground Truth
- 5.13 — Reference-Free Evaluation for Creative Tasks

---

*The golden set that was correct six months ago might be wrong today. Ground truth requires maintenance, not just creation.*
