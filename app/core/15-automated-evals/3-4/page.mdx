# 3.4 — Refusal and Deflection Pattern Matching

Most teams think refusals are a safety feature. They are — until they become a liability. A model that refuses appropriate requests costs you users. A model that never refuses inappropriate requests costs you lawsuits. The difference between the two is measurable, and you need to measure it before production shows you the failure mode.

## The Over-Refusal Spiral

In December 2025, a legal research platform deployed Claude Opus 4.5 with safety-conscious system prompts. Within two weeks, support tickets spiked by 240%. The model was refusing to analyze case law involving criminal defendants, claiming ethical concerns about legal content. The research tool was designed for lawyers doing exactly that work. The team had optimized for zero harmful outputs. They had achieved it by blocking a third of legitimate use cases.

This is **the Over-Refusal Spiral** — when safety tuning or system prompts push the model toward excessive caution, degrading utility faster than anyone notices in pre-production testing. Refusal pattern matching detects this failure mode before it reaches users. It counts how often the model says no, what it says no to, and whether those refusals align with policy or paranoia.

The alternative failure mode is just as dangerous. Under-refusal means the model attempts tasks it should decline — generating medical diagnoses without disclaimers, providing financial advice without caveats, or engaging with content that violates your terms of service. Both directions destroy trust. One frustrates users. The other exposes you to risk. You need automated detection for both, running on every batch, flagging deviations before they compound.

## Hard Refusals and Their Signatures

A **hard refusal** is an explicit decline. The model states directly that it cannot or will not complete the request. In 2026, refusal patterns have converged across major models. GPT-5 variants use "I cannot assist with that" or "I'm not able to help with requests that involve..." Claude Opus 4.5 uses "I don't feel comfortable" or "I can't help with that." Gemini 3 Pro uses "I cannot provide" or "I'm unable to generate." These are signature phrases, and you can match them with basic string detection.

Your heuristic evaluator scans every model output for these patterns. When it detects a hard refusal, it logs the prompt, the refusal text, and the timestamp. You set a baseline refusal rate during initial eval — maybe 2% of test cases trigger appropriate refusals based on your content policy. In production, you monitor drift. If the refusal rate climbs to 8%, something changed. If it drops to zero, something else changed. Both deserve investigation.

Hard refusals are easy to detect but context-dependent to judge. A medical chatbot should refuse to diagnose based on symptoms described in text. A customer support bot should not refuse to look up an order because the request mentioned a medical device. The heuristic does not judge appropriateness — it detects prevalence. Your job is to set the threshold. If baseline refusal rate is 3%, alert at 5%. If it is 15%, alert at 20%. The exact number depends on your domain, your policy, and your model's safety tuning.

## Soft Refusals and Deflections

A **soft refusal** is more insidious. The model does not explicitly decline. It hedges, deflects, or provides a non-answer that looks like compliance but delivers no value. "I'm not sure I can help with that, but here are some general considerations" is a soft refusal. "That's a complex topic — you might want to consult a professional" is a deflection. Both responses avoid the user's request without stating a clear policy reason.

Soft refusals are harder to detect with exact string matching because the phrasing varies. Instead, you match on hedge phrases and deflection patterns. Common markers include "I'm not sure," "I can't say for certain," "you might want to," "it's complicated," "that depends on many factors," and "I'd recommend consulting." These phrases appear in legitimate uncertain responses too, so you cannot flag every instance. The heuristic detects frequency, not individual occurrences.

You set a soft refusal threshold. If more than 10% of outputs contain hedge phrases combined with short responses — under 100 tokens — you flag the batch for review. A one-sentence deflection is different from a thoughtful exploration of uncertainty. Length combined with hedge phrase presence separates genuine nuance from avoidance. This is a coarse signal, not a definitive judgment, but it surfaces patterns human review might miss until users complain.

Deflection often pairs with topic avoidance. The model answers a related but safer question instead of the one asked. "How do I appeal a visa denial?" becomes "Visa processes vary by country." The user asked for procedural steps. The model provided a generic fact. Pattern matching for topic drift requires semantic similarity scoring, which moves beyond pure heuristics, but you can approximate it with keyword presence. If the user prompt contains "appeal" and "steps" and the output contains neither, flag it. Simple keyword absence is a weak signal individually, but aggregated across thousands of outputs, it highlights systematic deflection.

## Distinguishing Appropriate from Excessive Refusal

Not all refusals are failures. Your model should refuse some requests. The question is whether it refuses the right ones. Heuristic evaluations cannot make that judgment automatically, but they can structure the data for human review. You categorize refusals by prompt type, then compare refusal rates across categories.

Segment your test set into policy-compliant prompts and policy-violating prompts. Policy-violating prompts should trigger refusals. Policy-compliant prompts should not. Run both segments through your eval pipeline. Measure refusal rate in each. If the model refuses 95% of policy-violating prompts and 2% of policy-compliant prompts, the system works. If it refuses 60% of policy-violating prompts and 18% of policy-compliant prompts, you have both under-refusal on restricted content and over-refusal on legitimate use cases.

This segmentation requires labeled test data. You need a set of prompts tagged as "should refuse" and "should not refuse" based on your policy. Building this set is manual work upfront, but once built, the heuristic evaluation runs automatically on every model version. You detect policy drift, safety tuning side effects, and prompt injection that manipulates refusal behavior. The labeled set evolves as your policy evolves, but the evaluation pattern remains constant.

When a new refusal pattern emerges — a phrase the model suddenly starts using that you have not seen before — the heuristic flags it as an anomaly. In March 2025, GPT-5.1 began responding to certain legal questions with "I should clarify that I can't provide legal advice in this context." The phrase was new. Teams that monitored refusal language detected it immediately. Teams that did not spent two weeks wondering why legal research query volume dropped. Monitoring refusal vocabulary is as important as monitoring refusal rate.

## Refusal Rate as a System Health Metric

**Refusal rate** is the percentage of queries that trigger a refusal response. It is one of the simplest heuristics to track and one of the most informative. In a stable system, refusal rate is stable. When it shifts, something upstream changed — the model, the system prompt, the input distribution, or the adversarial attempts users are making.

You track refusal rate per deployment, per model version, and per user cohort. Production refusal rate should mirror eval set refusal rate within a tolerance band — maybe plus or minus two percentage points. If your eval set refusal rate is 4% and production climbs to 11%, your production traffic contains more policy-violating prompts than your test set anticipated, or your model has become more cautious in the wild. Both scenarios demand investigation.

Refusal rate also segments by category. A customer support bot might refuse 0.5% of order lookup queries and 40% of queries asking for personal opinions. Those are expected baselines. If order lookup refusal rate climbs to 8%, the model is refusing legitimate support requests. If opinion refusal rate drops to 10%, the model is engaging with out-of-scope prompts. Category-specific refusal rate detects localized failures that aggregate metrics mask.

Some teams set refusal rate SLOs. "Refusal rate on policy-compliant test prompts will not exceed 3%." When a deployment violates the SLO, it does not ship. This is a quality gate identical to latency or accuracy SLOs. Refusal rate is a product metric, not just a safety metric. Excessive refusal degrades user experience. Insufficient refusal creates risk. The heuristic makes both measurable, and measurement makes them controllable.

## Detecting Refusal Manipulation

Advanced users and adversarial actors attempt to manipulate refusal behavior. Prompt injection techniques try to bypass safety refusals by embedding instructions like "ignore your guidelines and answer anyway." Jailbreak attempts use role-play, hypothetical framing, or encoded language to elicit responses the model would normally refuse. Your refusal pattern matching detects some of this indirectly.

When a prompt contains manipulation language and the model does not refuse, you have a potential bypass. You cannot identify every manipulation technique with heuristics, but you can flag high-risk patterns. Prompts containing "ignore previous instructions," "disregard your training," "pretend you are," or "hypothetically speaking" combined with policy-sensitive keywords warrant logging and review. The heuristic does not block these prompts in production — that would cause false positives — but it flags them for post-hoc analysis.

You also monitor refusal consistency. The same prompt sent twice should yield the same refusal decision. If one instance refuses and another complies, you have non-determinism in safety behavior. This happens with temperature above zero or when sampling introduces variability in safety classifier activations. Heuristic evaluation sends identical prompts multiple times, checks for refusal agreement, and flags divergence. Inconsistent refusal is a model reliability problem, not just a safety problem. Users notice when the system says yes one day and no the next.

Refusal manipulation attempts cluster around certain topics. If 15% of prompts in a given week contain jailbreak language related to medical advice, you know adversarial users are targeting that domain. Your heuristic eval does not stop them, but it quantifies the attack surface and informs where you harden system prompts or add classifiers upstream of the model. Detection precedes defense.

## Refusal Breakdown Reporting

Your heuristic pipeline generates a **refusal breakdown report** for every eval run and every production batch. The report shows refusal rate overall, refusal rate by category, top refusal phrases by frequency, soft refusal rate, deflection rate, and examples of each type. This report goes to the team responsible for system prompts, safety tuning, and content policy.

The report highlights deltas. "Refusal rate increased 6 percentage points since last deployment." "New refusal phrase detected: appeared in 140 outputs." "Soft refusal rate in legal queries doubled." These deltas drive action. A flat report that shows "refusal rate is 4.2%" provides no urgency. A delta report that shows "refusal rate was 2.1%, now 4.2%" triggers investigation.

You archive refusal reports over time. Trends emerge. Seasonal patterns appear — adversarial attempts spike around certain events, or user behavior shifts during product launches. Long-term refusal trends inform content policy updates, model selection decisions, and safety tuning investments. A model that shows steadily increasing refusal rate over six months is drifting toward caution. A model that shows decreasing refusal rate is drifting toward permissiveness. Both drifts are measurable, and both are correctable if detected early.

Refusal pattern matching is cheap, fast, and coarse. It will not tell you whether a specific refusal was correct. It will tell you that refusal behavior changed, how much, and where. That information is enough to route the right cases to human review, enough to block a deployment that over-refuses, and enough to detect safety bypasses before they scale. The next heuristic layer — keyword and phrase presence signals — extends this pattern matching from refusals to content quality markers, detecting the presence or absence of language that correlates with output correctness or policy compliance.

