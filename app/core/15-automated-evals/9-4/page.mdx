# 9.4 — Drift Detection Metrics and Thresholds

The monitoring dashboard shows green for weeks. Pass rate at 94%, same as last month. Latency stable. Cost stable. Then a product manager runs a manual spot check and finds that half the outputs are wrong in a way the eval suite never caught. The drift was invisible until it mattered. The pipeline ran every day. The metrics looked fine. But somewhere between the last calibration and today, the relationship between what the judge measures and what users need quietly broke. The team thought they had continuous monitoring. What they actually had was continuous measurement of the wrong thing.

**Drift detection** is the practice of identifying when an eval suite's measurements stop reflecting production reality — when pass rates stay stable but user satisfaction degrades, when scores rise but escalations increase, when the metrics themselves become uncoupled from the outcomes they were designed to predict. Drift is not model failure. Drift is measurement failure. The model might be performing exactly as it did three months ago, but the evaluation system no longer captures whether that performance is good enough. Most teams discover drift through escalations or revenue loss. Elite teams detect drift before users notice, using statistical signals that reveal when calibration assumptions have expired.

## The Four Types of Eval Drift

Drift manifests in distinct forms, each requiring different detection strategies. **Judge drift** occurs when the LLM judge's behavior changes — a provider updates the model weights, context handling shifts, or instruction-following patterns degrade. The same prompt that produced reliable binary decisions in January returns inconsistent results in March. Teams see this as increased variance in repeated evaluations of identical test cases. A case that scored 8 out of 10 five times in a row now returns 8, 6, 9, 5, 8 across five runs. Judge drift is often provider-driven, invisible in release notes, and affects all evals using that judge simultaneously.

**Criteria drift** happens when the evaluation criteria become misaligned with user needs. The criteria were correct when written. User expectations evolved. Regulatory guidance changed. Competitive context shifted. The team is still measuring tone and accuracy, but users now care most about response speed and citation depth. The eval suite passes, the product fails. Criteria drift is organizational — it reflects a gap between what Product believes users need and what users actually need. Teams detect criteria drift by tracking the correlation between eval pass rates and user satisfaction metrics. When pass rates rise but satisfaction falls, criteria have drifted.

**Distribution drift** occurs when production traffic patterns diverge from eval dataset composition. The eval suite was built with 60% transactional queries, 30% informational, 10% edge cases. Production traffic shifted to 40% transactional, 50% informational, 10% edge. The eval still measures the right things, but weights them incorrectly. A model optimized to pass the eval performs worse in production because the production distribution no longer matches the eval distribution. Teams detect this by comparing eval performance segmented by query type to production performance segmented the same way. When eval predicts one rank ordering and production shows another, distribution has drifted.

**Ground truth drift** happens when the examples used as reference answers become outdated. Policies change. Product behavior changes. The company's position on edge cases evolves. The eval suite still compares outputs to ground truth, but the ground truth itself is no longer correct. A customer service eval compares responses to a policy document from nine months ago. The policy changed twice since then. The model follows the new policy. The eval marks it wrong. Ground truth drift is the most insidious form because it penalizes correct behavior and rewards outdated behavior. Teams detect it by periodically re-validating ground truth against current documentation, policies, and stakeholder consensus.

## Statistical Signals for Drift Detection

Drift detection requires comparing current eval behavior to historical baseline behavior using metrics sensitive to subtle shifts. **Score variance over time** tracks whether repeated evaluations of the same test cases produce increasing variance. For each case in the golden set, run the eval five times per week and compute the standard deviation of scores across those five runs. Average that standard deviation across all cases. Plot the average weekly. When average variance rises from 0.4 to 1.2 over six weeks, judge drift is occurring. The judge is becoming less consistent, even though mean scores might remain stable. This is an early warning signal that precedes visible quality degradation.

**Correlation decay** measures how strongly eval scores predict production outcomes. Track the Pearson correlation between eval pass rate and a production metric like escalation rate, user satisfaction score, or task completion rate. Calculate correlation weekly over a rolling 30-day window. When correlation drops from negative 0.82 to negative 0.61, criteria or distribution drift is likely. The eval is still producing scores, but those scores no longer predict the thing that matters. Correlation decay is the single most important drift signal for business-critical systems. It directly measures whether the eval remains predictive of real-world success.

**Segment performance divergence** compares how eval scores trend across different query types, user cohorts, or input categories. Segment both eval results and production metrics by the same dimensions — query complexity, user tier, language, task type. For each segment, track eval pass rate and production success rate weekly. When eval pass rate for complex queries rises from 78% to 84% but production success rate for complex queries falls from 72% to 66%, distribution drift is confirmed. The eval has become overoptimistic for that segment. Divergence analysis requires consistent segment tagging in both eval and production logging, but it isolates drift to specific slices of traffic.

**Score distribution shift** detects changes in the shape of the score distribution, independent of mean or median. Compute the full distribution of eval scores — not just pass/fail, but the continuous scores if available — and compare current distribution to baseline using Kolmogorov-Smirnov test or Jensen-Shannon divergence. When the distribution shifts from roughly normal centered at 7.8 to bimodal with peaks at 5.2 and 9.1, even if the mean stays near 7.8, something fundamental has changed. Bimodality often signals that the judge is applying criteria inconsistently or that the dataset now contains two distinct populations the eval treats differently. Distribution shift testing is computationally cheap and catches drift that summary statistics miss.

## Threshold Calibration and Alert Design

Drift metrics are continuous. Alerts are binary. The threshold determines whether drift is noise or signal. **Baseline volatility estimation** sets thresholds based on historical natural variation. For each drift metric, calculate the baseline standard deviation over a stable period — ideally four to eight weeks when no known changes occurred. Set the warning threshold at two standard deviations above baseline, critical threshold at three standard deviations. If variance historically fluctuates between 0.3 and 0.5, a warning at 0.7 and critical at 0.9 prevents alert fatigue while catching meaningful shifts. Baseline volatility estimation assumes the metric has consistent noise characteristics. If variance is seasonal or workload-dependent, use quantile-based thresholds instead.

**Sustained threshold breaches** prevent alerts on transient spikes. Drift is rarely instant. It accumulates over days or weeks. Trigger alerts only when a metric exceeds threshold for N consecutive measurements or M out of N measurements in a rolling window. For variance drift, require three consecutive daily measurements above threshold before alerting. For correlation decay, require seven out of ten daily measurements below threshold. Sustained breach logic filters one-off anomalies caused by dataset composition quirks, judge API transient errors, or production traffic anomalies unrelated to drift. It delays alerts by days but eliminates 80% of false positives.

**Multi-metric confirmation** requires evidence from multiple drift signals before escalating. Variance might spike due to a noisy batch of test cases. Correlation might dip due to a holiday traffic pattern. But when variance spikes AND correlation decays AND segment divergence appears simultaneously, drift is confirmed. Build alert tiers: yellow alerts fire on single-metric threshold breaches, orange alerts fire when two metrics breach, red alerts fire when three or more breach. Red alerts get paged to on-call. Yellow alerts go to Slack for investigation Monday morning. Multi-metric confirmation reduces false positives dramatically but requires investing in multiple orthogonal drift detection methods.

**Manual override escape hatches** acknowledge that statistical thresholds cannot anticipate every scenario. Provide a mechanism for engineers to manually trigger a drift investigation even when metrics are within thresholds, or suppress alerts when metrics breach but drift is known and acceptable. A provider announces a judge model update. Variance spikes. The team validates outputs manually and confirms quality is stable. They suppress variance alerts for two weeks while recalibrating thresholds. Manual override prevents alert fatigue during known changes and empowers engineers to act on intuition when they spot drift patterns statistics have not yet captured.

## Continuous Drift Monitoring Infrastructure

Drift detection must run continuously, automatically, and with minimal latency between drift occurrence and detection. **Daily eval reruns on fixed golden sets** provide the stable baseline needed for variance tracking. Select 200 to 500 representative cases from the eval dataset, mark them as the golden set, and never change them. Rerun the eval on this golden set daily, storing scores in a time-series database. This creates a controlled experiment where the inputs are constant, so any score variation must come from judge behavior change, not input variation. Golden set reruns are cheap — 500 cases evaluated daily costs 15 dollars per month with GPT-5-mini as judge. The value is early detection of judge drift weeks before production impact becomes visible.

**Real-time correlation tracking** compares eval predictions to production outcomes with minimal delay. For every production request, log whether the eval would have passed it and whether the user outcome was successful. Aggregate hourly into pass rate and success rate. Compute correlation every six hours over the trailing 72 hours. Detect correlation decay within a day of onset. Real-time tracking requires instrumentation that joins eval results to user session outcomes, which means eval must run in-line or near-line to production requests. For high-volume systems, sample 5% to 10% of traffic for correlation tracking to keep costs manageable while maintaining statistical power.

**Automated segment tagging pipelines** ensure that every eval case and every production request is tagged with segment metadata — query type, complexity tier, language, user cohort. Tagging enables divergence detection. The pipeline extracts features from the input, classifies it into segments using lightweight models or rules, and writes segment IDs to logs. Consistency is critical. If eval uses a rule-based complexity classifier but production uses an embedding-based classifier, segment definitions diverge and divergence metrics become meaningless. Shared tagging logic deployed to both eval and production environments ensures apples-to-apples comparison.

**Drift dashboard and historical views** surface drift metrics to the team responsible for eval health. The dashboard shows each metric over the trailing 90 days with threshold bands overlaid. Engineers see variance climbing from 0.5 to 1.1 over eight weeks, crossing the warning threshold two weeks ago. They see correlation dropping from negative 0.78 to negative 0.64 over the same period. They see segment divergence appearing in the legal query segment but not others. The historical view provides context. A spike that looks alarming in isolation might be consistent with previous spikes that resolved naturally. A gradual trend that looks subtle might represent the third occurrence of the same pattern. The dashboard does not make decisions. It equips humans to make decisions faster.

## False Positive Management and Alert Tuning

Drift detection that cries wolf loses credibility. Teams stop investigating alerts. Drift goes unaddressed until production fails. **Post-alert retrospectives** review every drift alert to determine whether it was accurate, whether the response was appropriate, and whether threshold tuning would have prevented false positives without missing true positives. After each alert, the engineer who investigated writes a two-paragraph summary: what caused the alert, whether drift was real, what action was taken. After 20 alerts, patterns emerge. 70% of variance alerts during weekend traffic are false positives because weekend traffic has different characteristics and eval dataset was built on weekday traffic. The team adjusts thresholds to be less sensitive during weekends or builds a separate weekend baseline.

**Seasonality and traffic pattern adjustment** prevents alerts on predictable fluctuations. Retail systems see traffic composition shift during holiday shopping. Educational systems see query patterns shift between academic terms. Healthcare systems see seasonal illness patterns. Drift metrics reflect these shifts even when eval quality remains stable. Model drift seasonality by computing separate baselines for each season, day of week, or event type, then compare current metrics to the seasonally appropriate baseline instead of a global baseline. This requires at least one full cycle of historical data to establish seasonal patterns, but it cuts false positive rates by 40% to 60% in domains with strong seasonality.

**Threshold ratcheting based on team capacity** acknowledges that alert volume must match investigation capacity. A three-person team cannot investigate 15 drift alerts per week. Set thresholds conservatively to generate five alerts per week maximum. Monitor the true positive rate. If 80% of alerts are actionable, thresholds are well-calibrated. If 40% are actionable, tighten thresholds to reduce volume. If 95% are actionable but production incidents suggest missed drift, loosen thresholds. Threshold tuning is not a one-time activity. It is continuous recalibration based on team feedback and incident learnings. The goal is sustainable alerting — frequent enough to catch drift early, sparse enough that every alert gets investigated.

**Graduated response protocols** match alert severity to response urgency. Yellow alerts post to Slack with a 48-hour investigation SLA. Orange alerts create a Jira ticket assigned to the eval owner with a 24-hour response SLA. Red alerts page on-call immediately with a four-hour investigation SLA. Graduated response prevents alert fatigue on low-severity drift while ensuring critical drift gets immediate attention. The protocol also defines what investigation means at each tier. Yellow requires reviewing recent changes and checking production metrics. Orange requires running a manual audit of 50 recent cases. Red requires immediate eval suite freeze, manual validation of production outputs, and exec escalation if user impact is confirmed.

Drift detection transforms eval pipelines from static measurement tools into living quality systems that adapt to changing reality. The metrics, thresholds, and monitoring infrastructure described here detect drift weeks before it damages user experience, giving teams time to recalibrate judges, update criteria, refresh ground truth, or rebalance datasets. Drift is inevitable in production systems. Detection is not. The next subchapter examines how to assess the impact of judge model updates before they propagate through your eval suite and compromise calibration across dozens of pipelines simultaneously.
