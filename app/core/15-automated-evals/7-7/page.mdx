# 7.7 — Tool Abuse and Unintended Action Detection

The agent executed a database deletion command at 11:42 PM on a Saturday. The deletion was syntactically correct, properly authenticated, and passed all existing validation checks. It removed 340,000 customer records. The agent had been asked to "clean up test data from the staging environment." Production and staging shared the same database connection pool. No human reviewed the action before execution. The company discovered the loss when customer login failures spiked Monday morning.

The engineering team had implemented comprehensive tool-use logging. They had built retry logic, timeout handling, and error recovery. They had tested the agent extensively on sample tasks. What they had not built was a system to detect when a technically correct action was contextually catastrophic. The agent did exactly what the code allowed. The problem was not a bug. The problem was that the system treated all database deletions as equally valid as long as they compiled.

Tool abuse by AI agents is not typically malicious. It is almost always a failure of constraint design. The agent optimizes for the goal you specified using the tools you provided without the contextual judgment you assumed it had. When you give an agent a powerful tool and insufficient guardrails, you are not trusting the agent. You are gambling that the distribution of tasks you tested against includes every dangerous edge case you will encounter in production. That gamble has a consistent failure mode: the first time the agent encounters a novel situation that requires human judgment, it takes the action that seems locally optimal and globally disastrous.

## The Categories of Tool Misuse

Tool abuse falls into predictable patterns. **Scope violation** occurs when an agent applies a tool to a broader set of targets than intended. The agent authorized to modify user preferences modifies admin settings. The agent authorized to delete test records deletes production data. The agent authorized to send email to support ticket contacts sends email to the entire customer base. The scope was implicit in your mental model. The agent has no mental model. It has permissions and instructions.

**Rate violation** happens when the agent uses a tool at a frequency or volume that causes system harm even though each individual action is valid. The agent retries a failed API call 4,000 times in six minutes. The agent generates 200 support tickets for a single user issue because each attempt to resolve the problem is logged as a new ticket. The agent sends 50 emails to the same customer because the task instruction said "ensure the customer is notified." Each action in isolation is correct. The aggregate is abuse.

**Dependency violation** occurs when the agent uses tools in an order or combination that violates implicit system invariants. The agent deletes a user record before canceling their active subscriptions, leaving orphaned billing entries. The agent updates a database schema while a migration is running. The agent modifies a configuration file that another process is reading, causing a race condition. The agent does not understand system state or transaction boundaries. It sees tools as independent operations, not as participants in a stateful system.

**Context violation** is the most dangerous category and the hardest to detect. The action is technically correct, scoped correctly, executed at a reasonable rate, and respects dependencies — but it is catastrophically wrong given the broader situation the agent does not perceive. The agent approves a refund for a user who is simultaneously committing fraud because the refund request in isolation meets policy. The agent schedules maintenance during a known high-traffic event because the calendar shows the time slot as available. The agent sends a cheerful promotional email to a customer whose account notes indicate they just reported a data breach. The agent cannot read the room because it has no room. It has the narrow slice of context you provided in the prompt and tool schema.

## Pre-Action Safety Classification

Detecting tool abuse requires evaluating actions before execution, not after. **Pre-action classification** treats every tool call as a candidate requiring safety review. The moment the agent decides to invoke a tool, the system captures the tool name, the arguments, the current conversation context, and any relevant system state. This capture happens before the tool executes. A classifier evaluates the proposed action against a set of safety rules.

The classifier is often itself an LLM, but it operates under a different prompt and different constraints than the agent. The agent's job is to accomplish the user's goal. The classifier's job is to identify danger. The agent is optimistic and goal-driven. The classifier is paranoid and conservative. You give the classifier the proposed action, a description of the tool's capabilities, the conversation history that led to the decision, and a set of danger patterns to watch for. The classifier returns a risk score and a justification. If the score exceeds a threshold, the action is blocked and flagged for review.

This approach introduces latency. Every tool call now requires a classifier invocation before execution. For high-frequency, low-risk tools, this overhead is unacceptable. The solution is **tiered classification**. Tools are categorized by risk. Read-only tools that query data without modification execute without classification. Low-risk tools that modify isolated user-scoped state undergo lightweight rule-based checks. High-risk tools that can affect multiple users, delete data, or trigger external actions undergo full LLM-based classification. The tiering is manual and requires careful judgment. The cost of a false negative — failing to block a dangerous action — is almost always higher than the cost of a false positive — blocking a safe action and requiring human override.

## Dangerous Action Pattern Libraries

Pre-action classification is only as good as the danger patterns it knows to look for. **Pattern libraries** codify the categories of harm your system has seen or anticipates. Each pattern includes a description, a set of signals that indicate the pattern might apply, and examples of past incidents that match the pattern. The library is version-controlled, reviewed by both engineering and domain experts, and updated every time a new failure mode is discovered.

A scope violation pattern for a customer support agent might flag any action that affects more than ten users in a single invocation. A rate violation pattern might flag any tool that is called more than five times in ten seconds for the same user. A dependency violation pattern might flag any database write that occurs within thirty seconds of a schema change. A context violation pattern might flag any promotional message sent to a user whose recent tickets include the words "breach," "lawsuit," or "cancel." These patterns are heuristics, not proofs. They generate candidates for review. A human or secondary classifier decides whether the candidate is truly dangerous.

The quality of the pattern library determines the effectiveness of the system. A library with three generic patterns catches nothing. A library with 300 hyper-specific patterns generates so many false positives that operators start ignoring alerts. The mature library has 20 to 50 well-tuned patterns that cover the 80% of tool abuse cases your domain encounters. Each pattern has a documented precision and recall measured against historical incidents. Patterns with precision below 30% are disabled or refined. Patterns with recall below 50% are investigated — if a pattern is supposed to catch a known failure mode and is missing half the cases, the pattern definition is incomplete.

## Action Safety Gates and Human-in-the-Loop

Some actions should never execute without human approval, regardless of how confident the agent is. **Action safety gates** enforce this by routing certain tool calls to a human review queue before execution. The agent proposes the action. The system blocks execution and notifies a human reviewer. The reviewer sees the proposed action, the context that led to it, the classifier's risk assessment, and any relevant system state. They approve, modify, or reject the action. Only after approval does the tool execute.

Gates are expensive. Every gated action introduces human latency, human cost, and the risk that the human is unavailable when the agent needs to act. Gates are not a general solution. They are a mitigation for the highest-risk tools in the highest-stakes domains. A financial trading agent might gate any trade above a certain dollar threshold. A healthcare agent might gate any medication recommendation that contradicts a known allergy. A legal document agent might gate any filing that has not been reviewed by a human attorney. The gate is acknowledgment that the domain contains risks the automated system is not yet competent to evaluate.

The implementation challenge is making the review process low-friction enough that it does not bottleneck operations while rigorous enough that reviewers take it seriously. If the review queue consistently contains 200 flagged actions and 198 of them are false positives, reviewers will stop reading carefully. They will click approve without investigation. The gate becomes theater. The solution is high-precision gating. Only actions that are genuinely ambiguous or high-risk should reach the queue. If your false positive rate is above 20%, the gating rules are too broad or the classifier is not tuned. Tighten the gate criteria until the queue represents real decisions, not noise.

## Tool Abuse Metrics and Feedback Loops

Measuring tool abuse requires tracking both detected and undetected incidents. **Detected abuse** is any action that was blocked or flagged by the classification system. You measure the rate of blocked actions per thousand tool calls, the distribution of block reasons, and the override rate — how often a human reviewer approved an action the classifier flagged as dangerous. A high override rate suggests the classifier is too conservative. A low override rate suggests it is well-calibrated or that reviewers are not reviewing carefully.

**Undetected abuse** is harder to measure because by definition it was not caught. The proxy is incident reports. Any tool-related failure that reaches production is by definition an undetected abuse case. You log the incident, trace it back to the tool call that caused it, and evaluate whether the existing pattern library and classification rules should have caught it. If yes, the classifier failed. If no, the pattern library was incomplete. The incident becomes a training example. You add it to the pattern library, write a test case that reproduces the failure, and verify that the updated system would now block the action.

The feedback loop is continuous. Every blocked action that a human overrides is labeled as a false positive. Every production incident caused by tool misuse is labeled as a false negative. These labels retrain the classifier if it is a learned model or inform updates to the rule set if it is rule-based. The system's precision and recall improve over time as it sees more examples of what danger looks like in your domain. The teams that treat tool abuse detection as a static problem build systems that catch June's mistakes and miss July's. The teams that treat it as a learning system build defenses that adapt.

## Designing Tools to Reduce Abuse Surface

The most effective tool abuse mitigation is designing tools that are hard to misuse in the first place. **Narrowly scoped tools** are safer than broadly scoped tools. A tool that deletes a single record by ID is safer than a tool that accepts a SQL WHERE clause. A tool that sends an email to a user in the current conversation context is safer than a tool that accepts arbitrary recipient lists. Narrow tools limit the agent's ability to cause large-scale harm with a single invocation.

**Idempotent tools** are safer than stateful tools. A tool that sets a user preference to a specific value can be called repeatedly without compounding damage. A tool that increments a counter or appends to a list can cause unbounded harm if called in a loop. When possible, design tools such that repeated invocations do not amplify risk. **Reversible tools** are safer than irreversible tools. A tool that archives data is safer than a tool that deletes it. A tool that drafts an email for review is safer than a tool that sends it immediately. Reversibility creates space for catching mistakes before they become permanent.

Rate limiting belongs in the tool, not just in the monitoring layer. A deletion tool should refuse to execute if it has been called more than a threshold number of times in a rolling time window. An email tool should refuse to send if it has already sent to the same recipient within the past hour. These limits are not perfect — a determined attacker or a truly confused agent can still cause harm by waiting — but they prevent the most common abuse mode, which is rapid repetition of a harmful action because the agent is stuck in a loop.

The principle is defense in depth. The tool is the first line of defense. The pre-action classifier is the second. The human review gate is the third. Monitoring and incident response are the fourth. No single layer is sufficient. A mature system assumes every layer will fail occasionally and builds enough redundancy that a single failure does not cascade into catastrophe.

Adversarial eval suites are the mechanism for discovering abuse cases before they occur in production, and the next subchapter covers how to build, maintain, and automate them.

