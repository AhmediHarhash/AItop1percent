# 9.3 — Scheduled Recalibration Cadences

Recalibration happens on two timelines: scheduled cadences and event-driven triggers. Scheduled cadences ensure you detect slow drift before it accumulates into major calibration failure. Event-driven triggers ensure you detect fast drift from known changes before they degrade eval reliability. Both are necessary. Cadences alone miss sudden changes. Triggers alone miss gradual decay. Together, they keep your eval system calibrated across model updates, distribution shifts, and ground truth evolution.

The cadence you choose depends on how fast your system changes, how high your stakes are, and how much recalibration costs. A customer support system that updates models monthly, launches new features weekly, and handles 200,000 queries per day needs weekly or biweekly recalibration. A contract analysis system that updates models quarterly, changes ground truth infrequently, and handles 5,000 queries per day can recalibrate monthly or quarterly. The faster your system changes, the faster drift accumulates, and the more frequently you must recalibrate to stay ahead of it.

## Weekly Recalibration for High-Velocity Systems

Weekly recalibration is the most aggressive cadence. It is expensive, operationally intensive, and necessary for systems where drift can cause user-facing failures within days. A fraud detection system, a content moderation system, or a customer-facing AI assistant that updates models frequently and where quality degradation has immediate business consequences needs weekly calibration checks.

Weekly recalibration does not mean you retrain models or overhaul rules every week. It means you measure agreement every week. Most weeks, agreement stays within acceptable range and no action is needed. Some weeks, agreement drops, triggering investigation and recalibration. The weekly measurement gives you early warning. If agreement drops from 91 percent to 88 percent in one week, you investigate immediately. You identify the cause — model update, traffic spike in a new query type, ground truth change — and decide whether to recalibrate now or monitor for another week. If agreement drops further the following week, you recalibrate. If it stabilizes or recovers, you document the anomaly and continue monitoring.

The operational cost of weekly recalibration is human review time. A 500-example calibration dataset requires ten to fifteen hours of human review per week. You need dedicated reviewers, clear labeling criteria, and a process to deliver labeled data to the engineering team within 48 hours of collection. For a high-stakes system, this cost is a small fraction of the cost of undetected quality degradation. For a lower-stakes system, it is prohibitive.

Weekly cadences also create noise. Production traffic fluctuates. Some weeks are unrepresentative — a viral post drives unusual query types, a holiday changes user behavior, a bug causes temporary traffic spikes. If you measure agreement during an unrepresentative week, the measurement reflects the anomaly, not true calibration. You see agreement drop, investigate, find no systemic issue, and waste time. The solution is to separate signal from noise. You track agreement as a trend, not as individual data points. A single week's drop is a signal to watch. Two consecutive weeks of drops is a signal to investigate. Three consecutive weeks is a signal to recalibrate.

## Monthly Recalibration for Moderate-Velocity Systems

Monthly recalibration is the most common cadence. It balances cost, responsiveness, and operational feasibility. A monthly cycle gives drift time to accumulate into measurable signal while catching it before it becomes severe. Most production AI systems — customer support, document analysis, code generation, writing assistance — change fast enough that quarterly recalibration is too slow and slow enough that weekly recalibration is overkill.

A monthly cycle works as follows. In the first week of each month, you sample 500 to 1,000 examples from the previous month's production traffic. You use stratified sampling to ensure the sample represents the full distribution. Human reviewers label the examples according to current quality criteria. Labeling takes one to two weeks. In the third week of the month, you run your automated eval system on the same examples. You measure agreement. You compare current month's agreement to the previous three months' agreement. You identify trends — stable, declining, improving — and segments where agreement diverged from the overall pattern.

If agreement is stable and above threshold — 90 percent or higher, or whatever threshold your team has validated — you take no action. You document the measurement, update your calibration tracking dashboard, and schedule the next cycle. If agreement dropped but remains above threshold — say, from 92 percent to 89 percent — you investigate the cause. You analyze disagreement patterns. You identify whether the drop is broad or concentrated in specific segments. You decide whether to recalibrate immediately or monitor for another month. If agreement dropped below threshold, you recalibrate.

Monthly recalibration is cheap enough to sustain indefinitely and frequent enough to catch drift before it causes operational problems. The human review cost is 20 to 40 hours per month — manageable for a team with dedicated annotators or for a team that rotates recalibration duty among engineers. The lag time — two to three weeks between sampling examples and measuring agreement — is acceptable for most systems. Drift that occurs mid-month is detected by the end of the month. By the time you recalibrate and deploy, four to six weeks have passed since drift started. For moderate-stakes systems, that lag is tolerable.

## Quarterly Recalibration for Stable Systems

Quarterly recalibration is appropriate for systems that change slowly, where drift accumulates gradually, and where the cost of drift is low relative to the cost of frequent recalibration. A legal research tool that updates its knowledge base twice a year, a financial analysis system that uses a stable model, or an internal tool with low user volume and forgiving quality requirements can recalibrate quarterly without significant risk.

Quarterly cadences reduce human review costs by 75 percent compared to monthly cadences. Instead of labeling 500 examples every month, you label 1,000 examples every three months. The per-cycle cost is higher, but the annual cost is much lower. The trade-off is detection lag. Drift that starts in January is not measured until March, not investigated until April, and not corrected until May. For four to five months, the eval system operates in a drifted state. If drift is slow and stakes are low, this is acceptable. If drift can degrade quality noticeably within weeks, quarterly recalibration is too slow.

Quarterly recalibration also increases the risk of compounding drift. A model update in January causes minor drift. A distribution shift in February causes additional drift. Ground truth changes in March cause more drift. By the time you measure agreement in late March, drift has accumulated from three separate sources. The agreement drop is large — from 91 percent to 79 percent — and the recalibration effort is correspondingly larger. You must address all three sources of drift simultaneously. This takes longer and costs more than addressing each source incrementally as it occurred.

The operational pattern for quarterly recalibration is the same as monthly, but the scope is larger. You sample 1,000 to 2,000 examples from the previous quarter. You label them over two to three weeks. You measure agreement. If agreement is acceptable, you take no action and wait another quarter. If agreement dropped, you recalibrate. Because quarters are long, you also run lightweight spot checks between quarterly cycles — sample 100 examples mid-quarter, label them, measure agreement on the small sample. If the spot check shows major drift, you escalate to an unscheduled recalibration instead of waiting for the quarterly cycle.

## Event-Triggered Recalibration

Scheduled cadences catch slow drift. Event-triggered recalibration catches fast drift from known changes. An event is any change that is likely to affect calibration: model update, fine-tuning deployment, major product launch, ground truth refresh, regulatory change, or a user complaint spike that suggests quality degradation.

When an event occurs, you trigger recalibration immediately, regardless of where you are in the scheduled cadence. The recalibration follows the same process — sample examples, label them, measure agreement — but it happens out of cycle. If you recalibrate monthly and a model update happens mid-month, you run an event-triggered recalibration within one week of the update. You measure agreement on 200 to 500 examples drawn from production traffic after the update. You compare post-update agreement to pre-update agreement. If agreement stayed stable, the update did not cause drift and you return to the normal cadence. If agreement dropped, you investigate and recalibrate.

Event-triggered recalibration prevents a specific failure mode: deploying a change that breaks calibration and not discovering it until the next scheduled cycle. A team updates from GPT-5 to GPT-5.1 in mid-February. Their next scheduled recalibration is in early March. The model update caused the new model to produce longer, more detailed answers. The eval automation was tuned for GPT-5's concise style. It now fails correct answers because they are too verbose. For two weeks, the automation produces misleading scores. Engineers see eval pass rates drop and assume the new model is worse. They tune prompts to shorten responses. Shortened responses pass evals but are actually lower quality — they omit necessary detail. By the time the March recalibration detects the drift, the damage is done. Event-triggered recalibration after the model update would have caught the drift within days, not weeks.

The cost of event-triggered recalibration is flexibility in your human review process. You cannot wait two weeks for labels. You need reviewers who can turn around 200 to 500 examples within 48 hours. This requires either dedicated reviewers on standby or a pool of on-call reviewers who can be activated when an event occurs. The cost is higher than scheduled recalibration, but the value — catching drift before it compounds — justifies it for high-stakes systems.

## Recalibration Budgets

Recalibration is not free. It consumes human review time, engineering time, and sometimes compute cost for retraining models. A realistic recalibration budget accounts for scheduled cycles and event-triggered cycles. If you recalibrate monthly and expect two event-triggered recalibrations per quarter, you budget for 14 to 16 recalibration cycles per year. Each cycle requires 20 to 40 hours of human review and 10 to 20 hours of engineering work. Annually, that is 280 to 640 hours of human review and 140 to 320 hours of engineering effort.

For a team with dedicated annotators, this is manageable. For a small team where engineers must also do labeling, it is a significant tax. The budget determines the cadence you can afford. A team that can allocate 400 hours per year to recalibration can afford monthly cadences plus event-triggered recalibrations. A team that can allocate 100 hours per year can afford quarterly cadences with no event-triggered recalibrations. The cadence you need and the cadence you can afford are not always the same. When they diverge, you face a choice: increase the budget, accept slower recalibration and higher drift risk, or reduce the scope of your automated eval system.

Some teams try to reduce recalibration costs by automating parts of the process. They use model-graded evals to pre-score examples, then only send low-confidence cases to human review. This reduces human review volume by 40 to 60 percent. The trade-off is that you are using the automation to filter the calibration dataset. If the automation has drifted, it will filter out the examples that reveal the drift. You end up measuring agreement on a biased sample. The measurement shows falsely high agreement because you excluded the cases where automation fails. Automation-assisted filtering is useful for reducing costs in production eval workflows, but it is dangerous in recalibration workflows. Recalibration must measure the full distribution, not a pre-filtered subset.

## The Cost of Under-Calibrating

Teams under-invest in recalibration because the cost is visible and immediate — hours spent labeling, hours spent engineering — while the cost of drift is invisible and delayed. Under-calibrated eval systems produce three kinds of damage. First, false confidence. Your team trusts eval scores that are wrong. They make decisions based on misleading data. They deploy changes that degrade quality because the eval system said quality improved. They roll back improvements because the eval system said quality degraded. Each bad decision compounds.

Second, wasted effort. Engineers investigate eval failures that are not real failures. They tune prompts to optimize for drifted metrics. They debug issues that do not exist. A team spends 40 hours investigating why eval pass rates dropped after a model update. The drop was caused by calibration drift, not by model degradation. The 40 hours were wasted. If they had run event-triggered recalibration, they would have spent 10 hours recalibrating and saved 30 hours of investigation.

Third, eroded trust. Once your team stops trusting eval scores, the eval system becomes ceremonial. It runs because process requires it, but it does not inform decisions. Engineers ignore eval results and rely on manual testing. Product managers ignore eval metrics and rely on user feedback. Leadership ignores quality dashboards and relies on intuition. The eval infrastructure still costs money — compute, storage, human review for production evals — but it no longer delivers value. At that point, the eval system is pure cost with no benefit.

The economic comparison is straightforward. Recalibrating monthly costs 300 to 400 hours per year. Under-calibrating costs thousands of hours in wasted investigation, bad decisions, and reduced trust. The return on investment for recalibration is positive for any system where eval scores influence decisions.

## Automated Recalibration Triggers

Advanced teams build automated triggers that detect calibration drift without waiting for scheduled cycles. The triggers monitor proxy metrics — eval pass rate volatility, disagreement rate between multiple eval methods, user feedback trends — and flag anomalies that suggest drift. When a trigger fires, the team investigates and decides whether to run an unscheduled recalibration.

One trigger is pass rate volatility. You track daily eval pass rates. You calculate a rolling seven-day average and a rolling standard deviation. If pass rate drops more than two standard deviations below the mean for three consecutive days, the trigger fires. This suggests something changed — model update, distribution shift, or drift. You investigate. If the drop is explained by a known event and is expected, you document it and continue monitoring. If the drop is unexplained, you run a rapid recalibration check on 100 to 200 examples to see if agreement dropped.

Another trigger is cross-method disagreement. You run two independent eval methods on the same outputs — say, a rule-based eval and a model-graded eval. You measure how often they agree. If agreement between methods is normally 85 percent and it drops to 78 percent, one or both methods have drifted. You investigate which method drifted and recalibrate it. Cross-method triggers catch drift earlier than single-method monitoring because they compare two independent signals.

A third trigger is user feedback divergence. You track user thumbs-up and thumbs-down ratings. You compare user ratings to eval scores. If user ratings and eval scores normally agree 80 percent of the time and agreement drops to 68 percent, your eval system has drifted away from user preferences. You recalibrate to realign with current user expectations.

Automated triggers reduce the lag between drift onset and detection. Scheduled monthly recalibration detects drift with up to four weeks of lag. Automated triggers detect drift within days. The cost is engineering effort to build and maintain the trigger logic and the operational overhead of responding to false positives. Not every pass rate drop signals drift. Some are noise. Some are legitimate quality changes. Triggers must be tuned to minimize false positives while catching real drift.

The question now is not whether to recalibrate on a cadence, but how to operationalize drift detection so that you catch calibration failures before they degrade the eval system's reliability and your team's trust in it.

