# 9.10 — When to Rebuild vs Recalibrate

When do you tweak your eval suite and when do you throw it out? Every team with a mature evaluation system eventually faces this question. You have a suite that was calibrated six months ago. It runs clean. Metrics are stable. But the product changed direction. The model changed twice. User complaints no longer match what your evals measure. The team spends more time explaining why the eval does not catch issues than fixing the issues themselves. Is this a calibration problem you can fix with threshold adjustments and new test cases, or is the entire eval architecture wrong for what your system has become?

The answer determines whether you spend two weeks tuning or three months rebuilding. Get it wrong and you either waste months reconstructing something that only needed minor updates, or you spend quarters patching an eval suite that cannot evolve to cover what your product does now. The decision is not obvious. Calibration and rebuild both involve changing test cases, updating thresholds, and adding coverage. The difference is architectural. Recalibration assumes your eval model is sound and just needs adjustment. Rebuild assumes the model itself no longer fits reality.

## The Recalibration Zone

Recalibration is the right choice when your eval architecture still aligns with your product, but the specifics have drifted. Your evaluation model — the dimensions you measure, the task types you cover, the quality tiers you enforce — still matches what your system does. You are not measuring the wrong things. You are measuring the right things with outdated assumptions about what good looks like.

A financial services company ran evals for a contract analysis system that extracted clauses, categorized risk, and flagged non-standard terms. The eval suite measured precision and recall on clause extraction, classification accuracy on risk categories, and false positive rates on flagging. Six months in, the team noticed that precision thresholds set at 92 percent were passing outputs that users rejected. The business had tightened compliance requirements. What used to be acceptable margin of error was now regulatory risk. The eval dimensions were correct — precision, recall, false positives. The thresholds were wrong. They recalibrated. New precision floor at 96 percent. New false positive ceiling at 0.8 percent. New test cases reflecting tighter compliance language. Two weeks of work. The suite worked again.

Recalibration is appropriate when the failure mode is threshold drift, coverage gaps, or stale test data — not architectural mismatch. You see it in predictable patterns. Your eval passes outputs that users reject, or fails outputs that users accept, but the reasons map cleanly to your existing metrics. You just need sharper thresholds or more representative test cases. Your task taxonomy still covers what the system does. Your quality dimensions still matter. Your eval tiers still reflect product risk. The structure is sound. The parameters need updating.

You recalibrate when the product evolves within the boundaries your eval was designed for. A customer support assistant adds five new product categories. Your eval already measures response relevance, policy adherence, and tone consistency. You do not need new dimensions. You need test cases for the new categories and possibly tightened thresholds if the new categories carry higher risk. A summarization system changes source documents from news articles to technical research papers. Your eval already measures factual accuracy, completeness, and readability. You do not need a new eval model. You need test cases from the new domain and recalibrated expectations for what constitutes complete coverage of a technical document versus a news article.

The hallmark of a recalibration scenario is that your eval team can describe the fix in terms of the existing structure. We need to raise the threshold here. We need more test cases for this task type. We need to add a subcategory to this dimension. We need to update golden outputs for this risk tier. You are working within the system, not redesigning it.

## The Rebuild Zone

Rebuild is necessary when your eval architecture no longer matches what your product does or what quality means for your users. This is not a threshold problem. This is not a coverage gap. This is a fundamental mismatch between the model of quality your eval enforces and the model of quality your product requires.

A legal research assistant launched with evals focused on retrieval accuracy and citation correctness. The eval measured whether the system returned the right cases and cited them properly. That was the product in version one — a search and citation tool. Eighteen months later, the product had evolved into a reasoning assistant that synthesized arguments from case law, identified conflicting precedents, and recommended litigation strategies. The original eval suite still ran. It still measured retrieval and citation. But users were complaining about reasoning gaps, contradictory recommendations, and failure to flag jurisdiction-specific nuances. None of those dimensions existed in the eval. The eval architecture was built for a search product. The product was now a reasoning product. Recalibration could not fix that. They rebuilt.

You know you need a rebuild when your eval does not capture the failure modes users actually experience. A content moderation system initially evaluated per-item accuracy — did the model correctly classify this post as violating policy or not. The product matured into a context-aware system that considered user history, thread context, and community norms. Users started reporting that the system over-moderated new users, under-moderated repeat offenders, and made inconsistent decisions within the same thread. The eval still measured per-item accuracy. It had no concept of consistency across items, no ability to evaluate context-aware behavior, no coverage of user-level or thread-level patterns. The eval model was item-centric. The product was context-centric. That is a rebuild.

Rebuild is the right answer when the task taxonomy no longer reflects what the system does. Your product added agentic workflows and your eval only covers single-turn question answering. Your product added multi-step reasoning and your eval only measures final answers. Your product added real-time personalization and your eval only measures static output quality. You cannot recalibrate a single-turn eval into a multi-turn eval. You cannot recalibrate a static eval into a personalization eval. The architecture has to change.

Rebuild is also the right answer when your quality dimensions are missing what matters. You measure fluency and coherence but not factual grounding, and hallucinations are your top user complaint. You measure task completion but not user satisfaction, and users complete tasks but hate the experience. You measure average quality but not worst-case quality, and your Tier 1 use cases fail in ways your eval never detects. You can add thresholds and test cases, but if the dimension is not in the eval model, you are not measuring it. That requires rebuild.

The test is simple. If a senior engineer looks at your eval suite and says, we would need to add three new core dimensions, restructure the task taxonomy, and change how we define quality tiers — that is a rebuild. If they say, we need to tighten thresholds and add 200 test cases for these new scenarios — that is recalibration.

## The Decision Framework

The decision framework starts with three diagnostic questions. First: do the quality dimensions your eval measures still align with the quality dimensions your users care about? If your users complain about reasoning quality and your eval measures retrieval quality, the answer is no. If your users complain about inconsistency and your eval measures per-item accuracy, the answer is no. If your eval metrics improve but user satisfaction declines, the answer is no. Misalignment on dimensions requires rebuild.

Second: does your task taxonomy still cover what your system does? If your product performs five task types and your eval covers all five, the taxonomy is intact. If your product added multi-step workflows, multi-turn conversations, or personalization and your eval treats everything as single-turn static tasks, the taxonomy is broken. If your product now handles edge cases that do not fit any category in your taxonomy, the taxonomy is incomplete. Taxonomy mismatch requires rebuild. Taxonomy gaps within an otherwise sound taxonomy require recalibration.

Third: can you describe the necessary changes using your existing eval structure, or do you need a new structure? If the fix is new thresholds, new test cases, updated golden outputs, and expanded coverage within existing dimensions — that is recalibration. If the fix is new dimensions, new task types, new quality models, and a rethinking of what your eval is fundamentally measuring — that is rebuild.

A rebuild is expensive. It typically takes two to four months for a mature system, depending on complexity. You are designing a new eval model, creating new test datasets, implementing new measurement logic, recalibrating all thresholds from scratch, and re-baselining your entire system. You lose historical continuity — your new metrics do not compare cleanly to your old metrics, so trend analysis resets. You disrupt your release process because the new eval may initially be stricter or more lenient than the old one, creating false regressions or false improvements until recalibrated. You may need to retrain your team on what the new metrics mean and how to interpret them.

Recalibration is fast. It typically takes one to three weeks. You are adjusting within a known system. Your team already understands the dimensions, the taxonomy, the architecture. Historical trends stay intact because you are measuring the same things with updated parameters. Your release process stays stable because changes are incremental. The risk is that recalibration becomes a trap. You keep patching an eval that no longer fits, adding workarounds and exceptions, until the eval suite is a mess of special cases that nobody trusts.

The decision heuristic: if more than 40 percent of your user-reported issues fall outside the dimensions your eval measures, rebuild. If more than 30 percent of your product functionality falls outside your task taxonomy, rebuild. If your eval team spends more time explaining why the eval does not catch real problems than improving the eval, rebuild. If the fixes you need would require changing the eval schema, adding new top-level dimensions, or redefining quality tiers, rebuild. Otherwise, recalibrate.

## Signs You Have Outgrown Your Eval

Certain patterns signal that your eval architecture no longer serves your product. One: your eval metrics trend up but user satisfaction trends down. This means you are optimizing for the wrong signal. Your eval measures something that no longer correlates with user value. You need to rebuild around the dimensions that actually matter.

Two: your team starts maintaining a separate, informal quality model that diverges from the eval. Product reviews outputs using criteria the eval does not measure. Trust and Safety escalates issues the eval never flags. Customer Success tracks failure modes the eval does not cover. When your org develops a shadow quality model, your eval has become obsolete. Rebuild to formalize what the org actually cares about.

Three: new features consistently require eval exceptions or workarounds. You add a feature and realize your eval cannot measure it, so you carve out an exception. You add another feature and carve out another exception. Six months later, 40 percent of your product surface area is eval-exempt. That is not an eval. That is a legacy system you are avoiding. Rebuild to cover the full product.

Four: your eval suite has more special-case logic than general-case logic. You have per-model threshold overrides, per-task-type exceptions, per-region adjustments, per-customer carve-outs, and conditional evaluation paths that apply only in specific combinations of conditions. This is a sign that your core eval model is too rigid or too narrow, and you have been patching around it rather than fixing the foundation. Rebuild with a more flexible architecture.

Five: you cannot onboard a new engineer to the eval suite in under a week. Complexity is fine if it maps to product complexity. But if the complexity is accidental — a tangle of legacy decisions, half-finished migrations, and unexplained exceptions — you have eval debt. Rebuild to simplify.

Six: your eval suite takes longer to update than your product does. Product ships a new feature in two weeks. Eval takes six weeks to add coverage. That lag means your eval is too brittle, too manual, or too architecturally mismatched to the product. Rebuild with extensibility in mind.

## The Cost of Each Approach

Recalibration costs time, but it is bounded. You are updating thresholds, adding test cases, and possibly expanding coverage. For a mature system, expect one to three weeks of eval engineering work. You may need to re-baseline current behavior, run side-by-side comparisons with the old thresholds, and adjust multiple times to find the right settings. But you are not rebuilding infrastructure. Your eval pipeline, your metric definitions, your reporting dashboards — all stay intact. Deployment is usually a config change or a data update, not a code migration.

Rebuild costs months. You are designing a new evaluation model from scratch. That includes defining new dimensions, creating a new task taxonomy, sourcing or creating new test datasets, implementing new scoring logic, building or adapting infrastructure to support the new model, and recalibrating all thresholds. For a complex system, this can be a three to six month project with two to four engineers. You also pay a migration cost. You need to run the old and new evals in parallel for weeks to ensure the new eval does not introduce regressions or blind spots. You need to retrain your team on the new metrics and their meaning. You need to update all documentation, dashboards, and alerting thresholds. You may need to re-baseline your release gates because the new eval measures quality differently.

The hidden cost of rebuild is continuity loss. Your historical trend data becomes hard to interpret because you are measuring different things. You cannot directly compare month-over-month quality if the quality definition changed. You lose institutional knowledge embedded in the old eval — the edge cases it caught, the thresholds that were carefully tuned over years, the implicit assumptions about what mattered. You have to rebuild that knowledge in the new system.

The hidden cost of recalibration is opportunity cost. If you recalibrate when you should rebuild, you waste time optimizing an eval model that does not fit. Worse, you may degrade trust in the eval system. Teams start ignoring eval results because they know the eval is measuring the wrong things. Once trust is lost, even a rebuilt eval takes months to regain credibility. The team has learned to route around the eval, and changing that behavior is harder than building the eval itself.

The strategic calculus: recalibration is cheaper in the short term, but rebuild is cheaper if the current eval cannot grow with the product. If you are six months from a major product shift, and your current eval cannot measure what the product will do after that shift, rebuild now rather than recalibrating twice. If your product is stable and the eval just needs tuning, recalibrate now and defer rebuild until the next major product evolution.

## The Eval Lifecycle Model

Evaluation systems have a natural lifecycle. They launch with a clear model of quality that fits the product at launch. Over months or years, the product evolves — new features, new task types, new quality expectations. The eval evolves too, through recalibration. Thresholds tighten. Test cases expand. Coverage grows. For a while, recalibration keeps the eval aligned.

Eventually, the product diverges too far from the eval's original design. The task taxonomy no longer fits. The quality dimensions miss what matters. The eval architecture is optimized for a product that no longer exists. At that point, recalibration stops being effective. You need a rebuild. You design a new eval model that fits the current product, deploy it, and start the cycle again.

The typical lifecycle for a fast-moving AI product: initial eval design takes one to three months. The eval operates effectively with quarterly recalibrations for twelve to twenty-four months. Around the two-year mark, or after a major product pivot, the eval needs a rebuild. The rebuild takes three to six months. The new eval operates effectively with recalibrations for another twelve to twenty-four months. Then the cycle repeats.

Some products stabilize and the eval does not need frequent rebuilds. A tier-three internal tool might run the same eval for five years with only minor recalibrations. Other products evolve rapidly and need more frequent rebuilds. A frontier AI assistant shipping new capabilities every quarter might rebuild evals annually.

The key is recognizing where you are in the lifecycle. If you are in the recalibration phase, recalibrate aggressively. Do not let eval drift accumulate. If you are at the rebuild threshold, commit to the rebuild. Do not try to stretch recalibration beyond its useful life. The worst mistake is staying in permanent recalibration mode, endlessly patching an eval that no longer works, because rebuild feels too expensive. That is how you end up with an eval suite nobody trusts.

## Rebuild Execution Strategy

When you commit to a rebuild, treat it as a product launch. You are building a new eval system, and it needs the same rigor you would apply to a customer-facing feature. Start with requirements gathering. Interview Product, Engineering, Trust and Safety, Customer Success, and domain experts. Ask: what quality dimensions matter now that did not matter a year ago? What failure modes do we see in production that the current eval does not catch? What does good look like for the product we have today, not the product we launched with?

Design the new eval model before writing any code. Define the quality dimensions, the task taxonomy, the eval tiers, the success criteria for each tier. Get sign-off from stakeholders. This is the foundation. If you get the model wrong, the implementation does not matter.

Build the new eval in parallel with the old eval. Do not replace the old system until the new one is proven. Run both systems side by side for at least four weeks. Compare results. Investigate divergences. Understand where the new eval is stricter, where it is more lenient, and why. Use the old eval as a sanity check. If the new eval suddenly flags 40 percent of outputs that the old eval passed, you need to understand whether that is legitimate increased strictness or a miscalibration.

Recalibrate the new eval before switching over. Your initial thresholds are guesses. Run the new eval against production traffic, analyze distributions, and set thresholds based on actual data. Do not lock in thresholds during the design phase. Let data guide the final calibration.

Migrate gradually. Start by using the new eval for monitoring only — it produces metrics but does not block releases. Once confidence is high, add it as an advisory gate — it flags issues but teams can override. Once trust is established, promote it to a blocking gate. This phased rollout reduces risk and builds team buy-in.

Sunset the old eval cleanly. Archive its data, document its history, and preserve the institutional knowledge it captured. Do not just delete it. The old eval represented years of learned quality expectations. That knowledge should inform the new eval even if the structure changes.

## When Rebuild Is Premature

Rebuild is not always the answer, even when the eval feels inadequate. Some signs that you should recalibrate instead: your eval has only been running for six months. It has not had time to accumulate edge cases, tune thresholds, or build institutional knowledge. Unless the product changed drastically in those six months, the eval probably just needs more calibration cycles.

Your eval is missing coverage for a new feature, but the rest of the eval still works. Add coverage for the new feature. Do not rebuild the entire system. Eval systems are modular. You can add new task types or dimensions without redesigning everything.

Your team has not invested in recalibration. If you have never updated thresholds, never added test cases, never expanded coverage since launch — your eval is not broken. It is neglected. Try a serious recalibration effort before concluding you need a rebuild.

Your dissatisfaction is about tooling, not the eval model. Slow execution, bad dashboards, hard-to-update test data — those are infrastructure problems, not eval model problems. Fix the tooling. Do not conflate bad developer experience with bad eval design.

The decision to rebuild should be based on architectural mismatch, not frustration. Frustration is a signal, but it is not proof. Investigate whether the frustration comes from a fundamental misalignment between eval model and product reality, or from execution issues that recalibration and tooling improvements can fix.

Your eval suite is a living system. It must evolve with your product. Recalibration is evolution within a stable architecture. Rebuild is evolution to a new architecture. Both are necessary. The skill is knowing which one to apply when. Misjudge it and you either waste months on unnecessary work or spend quarters failing to fix a broken foundation. Get it right and your eval system remains the trusted foundation of your quality assurance, no matter how fast your product changes.

The next challenge is making eval results visible, interpretable, and actionable for every team that depends on them. Metrics and dashboards turn raw eval data into decisions. Chapter 10 covers how to build reporting infrastructure that earns trust and drives action.

