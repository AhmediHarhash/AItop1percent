# 7.3 â€” Medical and Legal Risk Evaluation

Most teams believe that if their model passes general safety evals for toxic and harmful content, they have covered their risk surface. This assumption is wrong. General safety evals detect universal harms: slurs, violence, self-harm. They do not detect domain-specific harms that look harmless to a generalist model but are catastrophic in context. A medical agent that says "headaches usually resolve on their own" passes every toxic content filter and every harmful instruction filter. It also kills users who are experiencing stroke symptoms. A legal advice agent that says "you can represent yourself in court" passes every safety eval designed for consumer products. It also leads to wrongful convictions, lost custody cases, and financial ruin. Domain-specific risk evaluation is not an enhancement. It is a requirement for any system operating in medicine, law, finance, or other high-stakes fields where incorrect advice has irreversible consequences.

The challenge is that domain-specific harms require domain expertise to detect. Your engineering team cannot write a medical risk eval without clinical training. Your ML team cannot write a legal risk eval without understanding case law, statutes, and jurisdictional variance. The only path to effective domain-specific safety evaluation is collaboration with domain experts who understand both the risks and the nuances. This is expensive, slow, and non-negotiable. The alternative is shipping a product that passes all your automated evals and harms users in ways your eval suite never tested for.

## Medical Advice Risk Detection

Medical advice is any output that could influence a user's health decisions. This includes symptom triage, medication information, treatment suggestions, diagnosis explanations, and wellness recommendations. The harm surface is massive. Incorrect symptom triage delays emergency care. Incorrect medication information causes overdoses, dangerous interactions, or treatment failures. Incorrect treatment suggestions lead users to attempt ineffective or harmful interventions. Incorrect diagnosis explanations cause users to dismiss serious conditions or panic over benign ones. Incorrect wellness recommendations create long-term health consequences.

The detection logic starts with classification: is this output medical advice? This is harder than it sounds because users often ask health questions indirectly. "I have a headache" is clearly medical. "My head hurts when I stand up" is clearly medical. "I feel weird after taking my pills" is clearly medical. "I am tired all the time" is ambiguous. It could be a medical question about fatigue as a symptom, or it could be a lifestyle question about stress and sleep. Your eval must classify both as medical because the downside risk of treating a medical question as non-medical is that the model gives casual advice that ignores a serious underlying condition.

Once classified as medical, the output is evaluated against three criteria: does it provide a diagnosis, does it recommend treatment, and does it triage urgency correctly? Any output that does any of these three must be validated by a clinician. You cannot automate medical safety evaluation without clinical review. The best you can do is automate the triage: flag all outputs that contain diagnostic language, treatment recommendations, or urgency assessments, and route them to a clinician for review within 24 hours. If your product generates 10,000 medical outputs in your eval suite, and 30 percent contain diagnostic or treatment language, you need a clinician to review 3,000 examples before you can deploy. This is the cost of operating in a regulated, high-stakes domain.

The clinician review follows a rubric. For diagnosis-related outputs, the clinician checks: does the output suggest a specific condition, does it rule out serious conditions prematurely, does it use language that could be misinterpreted as a formal diagnosis? An output that says "this sounds like a migraine" is not technically a diagnosis, but users interpret it as one. An output that says "headaches are rarely serious" rules out serious conditions prematurely. Both fail the eval. For treatment-related outputs, the clinician checks: does the output recommend a specific medication or intervention, is the recommendation evidence-based, could the recommendation cause harm if the user has a contraindication the model does not know about? An output that says "try ibuprofen" is a treatment recommendation. If the user has a bleeding disorder the model does not know about, ibuprofen could cause life-threatening hemorrhage. The output fails.

For urgency triage, the clinician checks: does the output correctly identify red flag symptoms that require immediate care, does it minimize symptoms that could be serious, does it direct the user to appropriate care? Red flag symptoms include chest pain, difficulty breathing, sudden severe headache, loss of consciousness, uncontrolled bleeding, and any symptom a medical algorithm flags as high-risk. If the model's output fails to mention emergency care for any of these, it fails the eval. If the output says "monitor your symptoms at home" in response to chest pain, it fails catastrophically. The threshold for urgency triage is perfection. You cannot deploy a model that misses even one red flag symptom in your eval suite.

The regulatory context matters. In the United States, the FDA regulates medical device software, including clinical decision support tools. If your agent provides diagnostic or treatment recommendations, it may be classified as a medical device and subject to FDA 510(k) clearance or De Novo authorization. The EU Medical Device Regulation applies similar standards. Your medical safety eval must produce documentation that demonstrates compliance with these regulations. This means every eval example must be logged, every clinician review must be documented, and every failure must be tracked with root cause analysis. The eval results are not just internal quality metrics. They are regulatory evidence.

## Legal Advice Risk Detection

Legal advice is any output that could influence a user's legal decisions. This includes interpretation of statutes or contracts, explanation of legal processes, assessment of legal risk, and guidance on legal strategy. The harm surface is as large as the medical domain but harder to detect because legal language overlaps with everyday language. A model that says "you should talk to a lawyer" is not giving legal advice. A model that says "this contract clause means X" is giving legal advice, even if it disclaims that it is not a lawyer. Users rely on the interpretation regardless of the disclaimer.

The detection logic starts the same way: classify whether the output contains legal advice. This requires a definition of legal advice that is broader than what attorneys use. For safety evaluation purposes, legal advice is any output that interprets legal text, predicts legal outcomes, or recommends legal action. "This statute prohibits X" is interpretation. "You are likely to win this case" is prediction. "You should file a motion to dismiss" is recommendation. All three are legal advice for eval purposes, even if they would not constitute unauthorized practice of law under state bar rules.

Once classified as legal advice, the output is evaluated by an attorney licensed in the relevant jurisdiction. Jurisdiction matters enormously in legal evaluation because the same question has different answers in different states, different countries, and different legal systems. A question about employment law has one answer in California and a different answer in Texas. A question about contract law has one answer under common law and a different answer under civil law. If your product serves users in multiple jurisdictions, you need attorneys licensed in each jurisdiction to review examples relevant to that jurisdiction. This is prohibitively expensive for global products, which is why most legal AI products explicitly limit their service to one jurisdiction or disclaim all legal advice entirely and operate purely as document retrieval tools.

The attorney review follows a different rubric than medical review because legal harm is different from medical harm. Medical harm is usually physical. Legal harm is financial, relational, or liberty-related. The attorney checks: is the legal interpretation accurate under current law, does the output fail to mention critical exceptions or recent case law, does the output recommend an action that could harm the user's legal position? An output that says "verbal contracts are not enforceable" is flatly wrong in most jurisdictions and could cause a user to ignore a binding agreement. An output that says "you do not need a lawyer for small claims court" is technically true in most jurisdictions but ignores that self-represented litigants lose at much higher rates. Both fail.

The liability question is central. When a legal advice agent gives incorrect guidance, who is liable: the user who relied on it, the company that deployed it, the attorneys who reviewed the eval suite, or the model provider? The answer is unsettled and varies by jurisdiction. Some states have strict unauthorized practice of law statutes that impose liability on anyone providing legal advice without a license, which could include the company even if the company disclaims that the model is not a lawyer. Other states apply a standard of reasonable reliance: if a reasonable user would rely on the output as legal advice, the provider is liable for harm caused by incorrect advice. Your legal safety eval must account for this liability risk, and your attorney reviewers must be prepared to testify that the eval suite was comprehensive and the review was thorough if your company is sued.

The practical outcome is that most companies exit the legal advice space entirely or operate only in narrow, low-risk areas like legal document generation with heavy disclaimers. The ones that remain invest heavily in legal safety evals. A mid-sized legal AI company operating in contract review has a standing team of four attorneys who review 20,000 eval examples per quarter, spending approximately 2,000 hours on review. The cost is over 400,000 dollars per year. The alternative is one incorrect contract interpretation that leads to a multi-million-dollar lawsuit. The eval cost is insurance, and it is cheaper than the downside.

## Financial Advice Risk Detection

Financial advice is any output that could influence a user's financial decisions. This includes investment recommendations, tax guidance, retirement planning, debt management, and insurance selection. The harm surface is narrower than medical or legal but the financial consequences are often larger. A user who follows incorrect medical advice may recover. A user who follows incorrect financial advice may lose their retirement savings, face IRS penalties, or default on debt with no recovery path.

The detection logic is the same: classify whether the output contains financial advice, then route flagged outputs to a financial expert for review. The classification is tricky because general financial education is not financial advice, but the line is thin. "Index funds have lower fees than actively managed funds" is education. "You should invest in index funds" is advice. "Based on your age, you should allocate 70 percent to stocks" is personalized advice, which is regulated more strictly than general advice. Your eval must flag all three categories because users do not distinguish between them.

The expert review is performed by a CFP, CFA, or licensed financial advisor depending on the advice type. The reviewer checks: is the financial information accurate under current tax law and market conditions, does the output fail to mention risks or fees, does the output recommend an action that is inappropriate for the user's situation based on the information provided in the conversation? An output that says "you should maximize your 401k contributions" is good general advice but terrible specific advice if the user mentioned they have high-interest credit card debt. An output that says "Roth IRAs have no income limits" is factually wrong as of 2026 and could cause the user to make an excess contribution and face penalties.

The regulatory context is simpler than medical or legal because financial advice is regulated at the federal level by the SEC and FINRA in the United States, and by similar bodies in other countries. If your agent provides personalized investment advice, it is subject to the Investment Advisers Act and your company must register as an investment adviser or operate under an exemption. If your agent provides general financial education, it is not regulated, but the line between education and advice is determined by the SEC based on factors including personalization, specificity, and whether the user could reasonably interpret the output as a recommendation. Your financial safety eval must document which side of the line each output falls on, and your expert reviewers must be able to defend that classification to a regulator.

The practical outcome is similar to legal advice: most companies either exit the personalized financial advice space or invest heavily in expert review. A robo-advisor that provides automated investment recommendations employs a team of CFPs to review eval outputs, update the eval dataset quarterly as tax laws change, and validate that the model's recommendations comply with fiduciary duty standards. The cost is high but the alternative is regulatory enforcement, investor lawsuits, and reputational damage that destroys the business.

## Domain-Specific Safety Classifiers

The expert review model works but does not scale. If your product generates a million medical outputs per day, you cannot review them all manually. The solution is domain-specific safety classifiers: models trained to detect medical, legal, or financial risk without requiring expert review of every output. These classifiers are not replacements for expert review in pre-deployment evals. They are tools for post-deployment monitoring and for scaling expert review by pre-filtering outputs that are obviously safe or obviously risky.

A domain-specific safety classifier is trained on thousands of labeled examples of safe and unsafe outputs, labeled by domain experts. For medical risk, the training data includes 20,000 outputs labeled by physicians as safe, risky, or critical. Safe outputs are general health education with no diagnostic or treatment content. Risky outputs contain diagnostic or treatment language but are technically correct and unlikely to cause harm. Critical outputs contain incorrect medical information, miss red flag symptoms, or recommend harmful actions. The classifier learns to distinguish these categories and returns a risk score for each new output.

The classifier is used in two ways. In pre-deployment evals, it pre-filters your 50,000-example eval suite down to the 5,000 examples that the classifier flags as risky or critical. Your clinical reviewers focus on those 5,000 instead of reviewing all 50,000, reducing review time by 90 percent without reducing coverage. The 45,000 examples flagged as safe are spot-checked at a 10 percent sample rate to validate the classifier is not missing risks. In post-deployment monitoring, the classifier scores every production output and alerts when the percentage of risky or critical outputs exceeds your baseline. If your baseline is two percent risky and 0.1 percent critical, and today's traffic is five percent risky and 0.3 percent critical, you investigate immediately.

The limitation is that domain-specific classifiers are expensive to build and maintain. Training a medical risk classifier requires labeling 20,000 examples at 50 dollars per example for physician time, a total cost of one million dollars. The classifier must be retrained annually as clinical guidelines change, adding another 200,000 dollars per year. This is only feasible if you operate at scale in the domain. A startup with 10,000 users cannot justify this investment. A health system with 10 million patient interactions per year can. The threshold is roughly one million domain-specific outputs per year. Below that, expert review is more cost-effective. Above that, a domain-specific classifier is essential.

## The Liability Question

The question every company building in high-stakes domains must answer is: if our model harms a user, who is liable? The legal answer varies by jurisdiction, but the practical answer is always the same: you are. The model provider may share liability under a failure-to-warn theory or a defective product theory, but you deployed the model, you chose to operate in the domain, and you chose the safety controls. The user will sue you first. Your insurance will pay the settlement or judgment, and your premiums will increase until the product is uninsurable.

The liability risk shapes every aspect of domain-specific safety evaluation. You cannot deploy a model in a high-stakes domain without the ability to prove, in court, that you took every reasonable precaution to prevent harm. The eval suite is your primary evidence. When the plaintiff's expert witness testifies that your model's output was negligent, your defense is not "the model usually works." Your defense is "we tested the model on 50,000 examples of this exact scenario, reviewed by board-certified experts, and the model passed every eval." The eval logs, the expert review documentation, and the versioned eval dataset are evidence. If you cannot produce them, you lose.

This means your domain-specific safety evals must be litigation-ready. Every eval run is logged with a timestamp, model version, eval dataset version, and results. Every expert review includes the reviewer's credentials, the date of review, and the specific examples reviewed. Every failure includes root cause analysis and documentation of the fix. Every deployment includes a safety sign-off from a qualified expert stating that the model is safe to deploy based on eval results. This is not engineering best practice. This is legal risk management.

The insurance question is related. Most professional liability policies exclude AI systems entirely or limit coverage to systems that meet specific safety standards. If you want insurance for a medical AI product, the insurer will require documentation of your safety evaluation process, evidence that you are using evals designed by clinicians, and proof that you have a process for continuous monitoring and rapid response to safety failures. Your eval system is not just a technical requirement. It is an insurance requirement. Without it, you are self-insuring against catastrophic liability, which no rational company does.

## Regulatory Requirements for High-Risk Domains

The regulatory landscape for high-stakes AI systems changed dramatically in 2025 and 2026. The EU AI Act classifies medical diagnosis and treatment systems as high-risk, requiring conformity assessments, risk management systems, and post-market monitoring before deployment. The FDA's clinical decision support software guidance requires validation evidence for any system that provides diagnostic or treatment recommendations. The FTC's health breach notification rule requires disclosure of any incident where a health app shares user data inappropriately. Your domain-specific safety evals must produce the evidence these regulators require.

For medical systems, the FDA requires validation on a dataset representative of the intended use population. If your agent is designed for primary care triage, your eval dataset must include the demographic distribution, the symptom distribution, and the acuity distribution of primary care patients. If your eval dataset contains only young, healthy users with minor complaints, it is not representative, and the validation is insufficient. The FDA also requires ongoing monitoring and reporting of adverse events. If a user reports that your model gave harmful advice, you must investigate, determine whether the harm was foreseeable, and report it to the FDA within 30 days if it meets the criteria for a medical device adverse event. Your post-deployment safety evals are the monitoring system that detects these events.

For legal systems, the regulatory landscape is less mature but emerging. Several states are considering licensing requirements for legal AI products, modeled on the unauthorized practice of law statutes. The ABA has issued guidance that legal AI providers should ensure their systems do not provide advice that constitutes practicing law without a license. The practical requirement is that your system either stays on the safe side of the education-advice line, or it operates under the supervision of a licensed attorney who reviews outputs before they reach users. Your legal safety evals document which side of the line each output falls on.

For financial systems, the SEC and FINRA have published guidance requiring that robo-advisors disclose their limitations, avoid conflicts of interest, and maintain records of all advice provided. Your financial safety evals must include checks for disclosure language, checks for conflicts of interest in recommendations, and logging of all outputs for the required retention period. If an investor sues claiming your model recommended an unsuitable investment, the SEC will review your eval logs to determine whether you had controls in place to prevent unsuitable recommendations. If your evals did not test for suitability, you failed the regulatory standard.

The common thread across all three domains is evidence. Regulators do not accept "we used best practices" as a defense. They require documentation that you identified risks, designed evals to test for those risks, validated the evals with domain experts, and monitored for failures continuously. Your domain-specific safety evals are the system that produces that evidence. Without them, you cannot operate legally in a high-stakes domain.

---

*Medical, legal, and financial risk evaluation is the hardest safety eval category because it requires domain expertise, regulatory compliance, and litigation-ready documentation. But it is not the last category. Policy compliance evaluation ensures your model adheres to the behavioral boundaries your organization has set.*
