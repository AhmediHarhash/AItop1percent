# 6.8 — Latency and Performance Behavioral Metrics

Latency is not a technical metric — it is a behavioral one. How long an agent takes to complete a task reveals how it makes decisions, how it explores options, and how it manages tool calls. Two agents can achieve identical task outcomes with wildly different latency profiles. One makes three tool calls in four seconds. The other makes fifteen tool calls in forty seconds. From a success-rate perspective, they're equivalent. From a behavioral perspective, the second agent has a reasoning problem. It's exploring unnecessary paths, retrying actions that already succeeded, or failing to recognize when it has enough information to proceed. Your eval pipeline must measure latency as a signal of reasoning efficiency, not just as a performance constraint.

## Latency as a Reasoning Diagnostic

When an agent takes too long to complete a task, the first question isn't "how do we make it faster?" The first question is "why is it slow?" The latency distribution tells you what the agent is doing during execution. A task with a ten-second median and a sixty-second p99 has high-variance behavior — some inputs trigger exploration patterns that the agent can't escape efficiently. A task with a consistent eight-second completion time across all inputs has predictable behavior, even if eight seconds is slower than you want. Variance is a reasoning issue. Consistency is a performance issue. Your eval must separate the two.

You measure latency at multiple levels. Total task latency is the time from task start to termination. Per-step latency is the time spent in each reasoning or tool execution phase. Time-to-first-action is how long the agent takes to make its first tool call. Time-to-decision is how long it takes to choose a stopping condition after all necessary information is available. Each latency component surfaces different behavioral patterns. Long time-to-first-action suggests over-planning. Long per-step latency with many steps suggests inefficient exploration. Long time-to-decision suggests the agent can't recognize when it's done.

Your eval tracks these components for every task and builds latency profiles by task type. Retrieval tasks should have short time-to-first-action — the agent knows it needs to search, and the first step is obvious. Multi-step orchestration tasks should have longer time-to-first-action but shorter per-step latency — planning takes time, but execution is efficient. When a task's latency profile doesn't match its type, that's a behavioral anomaly. A retrieval task with long time-to-first-action is overthinking. An orchestration task with highly variable per-step latency is making execution decisions that should have been resolved during planning.

## Time-to-First-Token and Perceived Responsiveness

For interactive agents, time-to-first-token is the most critical latency metric. It's the delay between user input and the first visible sign that the agent is doing something. Users perceive systems as responsive if they see output within 300 milliseconds, even if the full response takes ten seconds. They perceive systems as broken if nothing happens for three seconds, even if the final response is perfect. Time-to-first-token determines whether users trust that their request was heard.

You measure time-to-first-token separately from total completion time. An agent might have a two-second time-to-first-token and a twelve-second total latency — acceptable for most use cases. An agent with an eight-second time-to-first-token and a ten-second total latency feels unresponsive, even though it's faster overall. The eval tracks both metrics and flags cases where time-to-first-token exceeds thresholds that create poor user experience. For chat-style interfaces, that threshold is typically 500 milliseconds. For agents executing complex tasks, it's one to two seconds. Beyond those thresholds, users start questioning whether the system is working.

Time-to-first-token also reveals architectural issues. If the agent spends six seconds planning before making its first tool call, that planning is happening synchronously when it could be streamed or parallelized. If the agent spends four seconds loading context before generating any output, the context retrieval is a bottleneck. Your eval doesn't just measure time-to-first-token — it attributes the delay to a specific phase of execution. Is the model slow? Is the tool call slow? Is the orchestration layer slow? Each component gets a latency budget, and the eval reports which component is consuming the budget.

## Per-Step Latency Budgets

A multi-step task is only as fast as its slowest step. If nine steps complete in 500 milliseconds each and one step takes eight seconds, the total latency is dominated by the outlier. Your eval must measure per-step latency and identify which steps are bottlenecks. A step that consistently takes longer than its budget is either calling a slow tool, performing redundant work, or retrying failed actions. The eval captures the step type, the tool involved, and the outcome, so you can diagnose whether the latency is inherent to the operation or a result of inefficient behavior.

You set per-step latency budgets based on task requirements and tool characteristics. A database query should complete in 200 to 800 milliseconds. A document retrieval should complete in 300 milliseconds to 1.5 seconds. A code execution sandbox should return results in one to three seconds. When a step exceeds its budget, the eval logs it as a latency violation. If the violation happens because the tool is fundamentally slow, you know you need to optimize the tool or choose a different one. If the violation happens because the agent retried the tool three times unnecessarily, you know you have a behavioral issue.

Your eval also tracks whether latency violations cluster around specific inputs or conditions. If retrieval steps are slow only when the query is ambiguous, the agent is compensating for poor query formulation by doing multiple searches. If execution steps are slow only when the code is complex, the sandbox might need more resources. If every third attempt at a specific task type is slow, there's a caching or state management issue. Latency violations aren't random — they're symptoms of underlying problems, and your eval's job is to surface the pattern so you can address the root cause.

## Latency SLOs for Agents

Service level objectives for latency define what "fast enough" means for each task type. A chatbot responding to a simple question has a latency SLO of one to two seconds. An agent summarizing a long document has an SLO of ten to fifteen seconds. An agent orchestrating a multi-step workflow has an SLO of thirty to sixty seconds. These SLOs are not arbitrary — they're based on user expectations, task complexity, and the cost of waiting. Your eval enforces these SLOs by flagging any task that exceeds its target latency, regardless of whether it succeeded.

You define SLOs at the 50th, 90th, and 99th percentiles. The p50 latency represents typical performance. The p90 latency represents performance under load or with moderately complex inputs. The p99 latency represents worst-case performance that still needs to be acceptable. A task might have an SLO of three seconds at p50, six seconds at p90, and twelve seconds at p99. If p99 latency hits twenty seconds, the task is violating its SLO, and you need to investigate whether it's a behavioral issue, a tool issue, or an input complexity issue.

Your eval tracks SLO compliance over time and across agent versions. A model update that improves task accuracy but degrades p90 latency by 40% is a regression, even if it doesn't break SLOs outright. A prompt change that reduces p99 latency from fifteen seconds to eight seconds is a major win, even if p50 latency stays the same. SLO compliance is a first-class metric alongside task success rate. An agent that succeeds 95% of the time but violates latency SLOs on 30% of tasks is not production-ready.

## Performance Regressions as Eval Failures

When you update a model, change a prompt, or modify orchestration logic, latency can regress even if task success rates improve. The new configuration might explore more options before deciding, retry actions more conservatively, or call additional tools for verification. Each change adds latency. If the latency increase crosses SLO thresholds, the change is a regression regardless of accuracy gains. Your eval pipeline must treat latency regressions as failures, not as acceptable trade-offs.

You detect regressions by comparing latency distributions before and after a change. If p50 latency increases by more than 15%, that's a regression. If p99 latency doubles, that's a critical regression. If the percentage of tasks violating SLOs increases by more than 5 percentage points, that's a regression. The eval doesn't just report "latency got worse" — it quantifies how much worse, at which percentiles, and for which task types. You need that level of detail to decide whether the accuracy improvement justifies the latency cost.

The hardest regressions to catch are when latency improves on average but degrades for a subset of tasks. The new model might be faster on simple inputs but slower on complex ones. The p50 latency drops, the p99 latency spikes, and the overall distribution looks acceptable. Your eval must break down latency by input complexity, task type, and success outcome. If latency improves for tasks that succeed but degrades for tasks that fail, the agent is spending more time on hard cases — potentially a good trade-off. If latency degrades uniformly across all outcomes, the change is inefficient.

## Latency Attribution and Root Cause Analysis

When a task is slow, your eval must identify where the time went. You break total latency into components: model inference time, tool execution time, network latency, orchestration overhead, and idle time between steps. Each component gets measured separately. If 80% of latency is model inference, you need a faster model or a smaller prompt. If 60% of latency is tool execution, you need faster tools or fewer tool calls. If 30% of latency is idle time, the orchestration layer is introducing unnecessary delays.

You track latency attribution across tasks and build a profile of where time is spent. Retrieval tasks should spend most of their latency in tool execution — the model inference is fast, but database queries take time. Reasoning tasks should spend most of their latency in model inference — the agent is thinking, not calling tools. If a retrieval task is spending 50% of its latency in model inference, the agent is overthinking simple lookups. If a reasoning task is spending 70% of its latency in tool execution, it's calling tools when it should be reasoning.

Your eval also measures parallelization opportunities. If the agent makes three tool calls sequentially when they could run in parallel, the latency is three times higher than necessary. The eval identifies these cases by analyzing tool dependencies in the execution trace. If two tool calls don't share inputs or state, they're parallelizable. If the agent executed them sequentially anyway, that's a scheduling inefficiency. The eval logs these cases and calculates the potential latency savings if the agent parallelized correctly.

## Latency Tolerance and User Context

Not all latency is equal. A user asking a chatbot for a weather forecast expects a response in one second. A user asking an agent to analyze a hundred-page legal document expects a response in thirty seconds. The same agent latency can be perceived as instant in one context and unacceptably slow in another. Your eval must account for user expectations when defining what "too slow" means.

You encode latency tolerance as part of task metadata. High-urgency tasks have tight latency budgets. Low-urgency tasks have loose budgets. Interactive tasks have stricter time-to-first-token requirements than batch tasks. The eval uses these budgets to determine whether a task's latency is acceptable for its context. A ten-second task that violates a five-second SLO is a failure. A ten-second task with a twenty-second SLO is a success, even though the absolute latency is identical.

Your eval also tracks whether agents adjust their behavior based on latency constraints. If a task specifies a strict latency budget, does the agent make fewer tool calls, skip optional verification steps, or terminate earlier when it has enough information? If the agent treats all tasks the same regardless of urgency, it's not responding to latency signals. A production-grade agent should be capable of trading accuracy for speed when the user's context demands it, and your eval must verify that this trade-off happens appropriately.

Latency and performance metrics reveal how efficiently an agent executes, how it makes trade-offs between speed and thoroughness, and whether its reasoning patterns are predictable or chaotic. But latency only measures time — it doesn't measure impact. The next behavioral dimension is side effects: what the agent changed, whether those changes were intended, and whether they introduced risks that task success rates alone don't surface.

