# 1.2 — The Core Categories of Automated Evaluation

You have 10,000 customer support responses to evaluate. Some need to check if a refund amount matches the order total. Some need to judge if the tone is empathetic. Some need to verify that the response follows a seven-step policy escalation process. No single evaluation method handles all three. You need different tools for different jobs.

Automated evaluation is not one technique. It is a family of techniques, each suited to different quality dimensions. Teams that treat evaluation as a single category end up running expensive, slow checks on cases where a simple rule would suffice — or running cheap, unreliable checks on cases that need real judgment. The result is eval pipelines that cost too much, run too slow, and miss critical failures anyway. The fix is understanding the categories and matching the right technique to the right problem.

## Rule-Based Evaluation: Hard Constraints with Zero Ambiguity

Rule-based evaluation checks for properties that are either true or false. Did the output include a required field? Is the response length within bounds? Does the text contain any banned phrases? Is the format valid JSON? These are binary checks. There is no gray area. The rule is either satisfied or violated. A customer support response that fails to include a ticket number when policy requires one is objectively wrong. You do not need a language model or a human reviewer to make that call. You need a rule.

Rule-based checks are fast, cheap, and deterministic. They run in milliseconds. They cost effectively nothing. They produce the same result every time. They catch entire categories of failure instantly. A legal compliance system that must include specific disclosures in every output can enforce that requirement with a rule: check if the disclosure text appears verbatim. If it does not, fail the output. This is not subjective judgment. It is policy enforcement. The check runs on every output. It never misses. It never gets tired. It never interprets the requirement creatively.

The limitation is that rules only catch what they explicitly check for. A rule that checks for the presence of a disclosure does not check whether the disclosure is in the right location, whether it is formatted correctly, or whether the rest of the response contradicts it. A rule that checks response length does not check whether the response is helpful. Rules are narrow. They catch the specific failure modes you encoded. They miss everything else. This is fine. You are not trying to evaluate all quality dimensions with rules. You are trying to catch the failures that are always failures, no matter the context.

A healthcare company built a symptom checker that must never recommend self-medication for chest pain. The policy is absolute. Any output that includes chest pain as an input and suggests over-the-counter medication as a response is a failure. The eval team wrote a rule: if input contains chest pain keywords AND output contains medication recommendation keywords, fail. The rule runs on every output. It has caught 27 violations in production over two years — outputs that would have been dangerous medical advice. The rule produces false positives occasionally, flagging cases where the model correctly advised seeing a doctor but mentioned medication in a historical context. The team accepts the false positives. The cost of manually reviewing 30 flagged cases per year is trivial compared to the risk of missing a real violation. The rule does one thing and does it perfectly.

## Heuristic Evaluation: Approximate Quality Signals

Heuristics are rules with flexibility. They do not enforce hard constraints. They estimate quality based on observable patterns. A heuristic might check if a response includes citations, if it uses formal language, if it has a certain keyword density, if it matches expected length distributions. Heuristics are not ground truth. They are signals. A response with citations is not automatically high-quality, but citation presence correlates with quality for certain task types. A response that is too short might be low-effort. A response that repeats the same phrase five times might indicate a model failure.

Heuristics are faster and cheaper than judgment-based evaluation but more sophisticated than binary rules. They catch patterns that matter across many cases. A customer support eval might use a heuristic: responses that do not acknowledge the customer's specific issue by name score lower. This is not a hard rule — some high-quality responses are generic — but it is a useful signal. Responses that fail this heuristic warrant closer review.

A financial summarization system uses heuristics to flag low-confidence outputs. The heuristic checks if the summary includes hedge words like "might," "possibly," or "unclear" more than twice. High-quality summaries state facts clearly. Summaries that hedge excessively often indicate the model could not extract clear information from the source document. The heuristic does not auto-fail these outputs. It flags them for human review. Approximately 60% of flagged summaries turn out to have issues — the model was guessing or hallucinating. The heuristic is not perfect, but it concentrates human review time on the outputs most likely to have problems.

Heuristics work best when you have learned patterns from historical data. After reviewing thousands of outputs, you notice that responses under 50 words in a specific task category are almost always low-quality. You encode that as a heuristic. You notice that responses that do not use any domain-specific terminology are often off-topic. You encode that as a heuristic. Heuristics are crystallized experience. They are the patterns human reviewers have learned but cannot apply at scale. Automation applies them to every output.

The limitation is that heuristics are proxies, not truth. They correlate with quality but do not define it. A response can pass all your heuristics and still be wrong. A response can fail your heuristics and still be excellent. Heuristics reduce the search space. They help you prioritize. They do not replace judgment.

## LLM-as-Judge Evaluation: Scalable Subjective Judgment

LLM-as-judge evaluation uses a language model to score outputs on dimensions that require interpretation. Is this response empathetic? Is the tone appropriate for a professional email? Does this summary accurately capture the key points of the source document? These are not binary checks. They require reading the output, understanding context, and applying judgment. Human reviewers can do this. Language models can approximate it.

The pattern is straightforward. You write a judge prompt that defines the evaluation criteria. You provide the output to evaluate and any necessary context — the user query, reference materials, policy guidelines. The judge model returns a score or a binary decision. You run this at scale. The same judge prompt evaluates every output in a category. You get consistent application of criteria across volume humans cannot match.

LLM-as-judge is slower and more expensive than rules or heuristics but faster and cheaper than human review. A judge call might take 500 milliseconds and cost a fraction of a cent. A human review takes 60 seconds and costs dollars. For tasks that need judgment on thousands of outputs per day, the economics are clear. You cannot afford human review. You cannot tolerate the latency. LLM-as-judge makes subjective evaluation scalable.

A content moderation system uses GPT-5-mini as a judge to evaluate whether customer service responses are polite and professional. The judge prompt defines "polite" with examples of acceptable and unacceptable tone. It includes edge cases — firm but professional responses that should pass, passive-aggressive responses that should fail. The judge model evaluates 80,000 outputs per day. It flags approximately 2% as failing the tone requirement. Human moderators review the flagged outputs. The judge achieves 91% agreement with human reviewers on the flagged cases — high enough to be useful, low enough that human review is still necessary for final decisions. The system works because the judge is not making final accept-reject decisions. It is triaging. It is finding the 2% of outputs that need human attention, sparing reviewers from evaluating the 98% that are clearly fine.

The risk with LLM-as-judge is that the judge model has failure modes of its own. It can be biased. It can misinterpret instructions. It can hallucinate reasons for scores that do not reflect reality. It can drift as the underlying model updates. You are not eliminating subjectivity. You are encoding it in a prompt and running it at scale. This is useful, but it is not ground truth. The judge's opinion is a signal, not a fact. Treat it as one part of your eval pipeline, not the whole thing.

## Reference-Based Evaluation: Comparing Against Known Good

Reference-based evaluation compares an output to a reference answer. If you have a dataset of questions and known-correct answers, you can evaluate new outputs by measuring how closely they match the references. This is common in traditional NLP tasks. Machine translation systems compare generated translations to human-written reference translations. Summarization systems compare generated summaries to human-written summaries. The closer the match, the higher the score.

Metrics like BLEU, ROUGE, and exact match are reference-based. They measure overlap between the generated output and the reference. These metrics are fast and deterministic. They require no model calls. They produce repeatable scores. The challenge is that reference-based metrics often correlate poorly with human judgment for generative tasks. A translation can differ from the reference in wording but be equally correct. A summary can include different details and still capture the key points. Exact match is too strict. BLEU and ROUGE are noisy proxies.

Reference-based evaluation works best when there is a single correct answer or a small set of acceptable answers. A system that extracts structured data from invoices can compare extracted values to ground truth values. Invoice number, total amount, vendor name — these have correct answers. If the extracted value matches the reference, the extraction succeeded. If it does not match, it failed. This is clean. Reference-based evaluation for exact-match tasks is reliable and cheap.

For tasks with many valid outputs, reference-based evaluation is less useful on its own. A customer support response can be helpful in a dozen different ways. Comparing it to a single reference response misses the variation. You can address this by having multiple references per input, but that is expensive to create and still does not cover all valid responses. Most teams use reference-based metrics as one signal among many, not as the sole eval method.

## Behavioral Evaluation: Checking System Properties Through Interaction

Behavioral evaluation tests how the system behaves across inputs. It does not evaluate individual outputs in isolation. It evaluates patterns. Does the system refuse harmful requests? Does it maintain consistent persona across a conversation? Does it refuse to answer the same question asked five different ways? Behavioral checks often involve multi-turn interactions or adversarial probing.

A chatbot that should never provide financial advice can be tested behaviorally by submitting 100 different phrasings of requests for investment recommendations. If it refuses all 100, the behavioral check passes. If it provides advice on any of them, the check fails. You are not evaluating the quality of the advice. You are evaluating whether the refusal boundary holds.

Behavioral evaluation is critical for safety and policy enforcement. A customer service bot that must hand off to a human for billing disputes can be tested by submitting 50 billing dispute scenarios. If it hands off correctly in all 50, the behavior is correct. If it tries to handle the dispute itself in any of them, the behavior is wrong. This is not a content quality check. It is a policy compliance check.

Behavioral checks are slower than single-output evaluations because they often require multiple interactions. They are harder to automate because they require orchestrating sequences of inputs. But they catch failure modes that single-output evals miss. A model might handle individual queries correctly but fail to maintain state across a conversation. A model might refuse a direct harmful request but comply with the same request phrased indirectly. Behavioral evaluation stresses the boundaries of the system's capabilities and policies.

## Choosing the Right Category for Each Quality Dimension

No single eval category handles all quality dimensions. The question is which category to use for which problem. Rule-based evaluation for hard constraints and required fields. Heuristics for fast pattern-based filtering. LLM-as-judge for dimensions that need subjective judgment at scale. Reference-based for tasks with correct answers. Behavioral for policy boundaries and multi-turn interactions.

Most production eval pipelines use all five categories. They layer them. Rules catch the obvious failures instantly. Heuristics flag the suspicious cases. LLM-as-judge evaluates the subjective dimensions. Reference checks validate the objective ones. Behavioral tests stress the boundaries. Each category does what it does well. Together they provide coverage across the quality space.

The mistake is using a single category for everything. Teams that try to encode all quality checks as rules end up with brittle, unmaintainable rule sets that miss nuanced failures. Teams that rely entirely on LLM-as-judge spend too much money and wait too long for results on cases that a simple rule could have handled in milliseconds. Teams that use only heuristics miss the failures that do not match known patterns. The solution is multi-category evaluation. Match the tool to the job.

---

The next step is understanding how to layer these categories efficiently, running cheap checks first and expensive checks only when necessary.
