# 29.2.1 — The Three Organizational Models: Centralized, Hub-and-Spoke, and Embedded

The CTO stands at the whiteboard. Three boxes, three labels, three arrows. Three ways to organize AI governance across a company with four thousand employees, eleven business units, and thirty-eight AI systems in production. The room holds the VP of Engineering, the General Counsel, the head of data science, and the CISO. Everyone agrees that the current approach — which is no approach at all, just individual teams making individual decisions about individual deployments — has reached its limit. The question is what replaces it. The CTO draws the first box: one team, central authority, everything flows through them. She draws the second box: a central hub that sets standards and provides tools, with local teams executing in each business unit. She draws the third box: governance distributed entirely into product and engineering teams, no central function at all. Each model solves one problem and creates another. The room will spend the next two hours arguing about which set of trade-offs they can live with. That argument, replicated in thousands of enterprises between 2024 and 2026, is the argument that defines how AI governance actually works in practice.

There is no universally correct answer. The right model depends on your organization's size, regulatory exposure, AI maturity, and the degree of trust between central functions and business units. But understanding the mechanics of each model — not just the label, but how it actually functions under pressure — is what separates an intentional governance design from one that defaults into dysfunction.

## The Centralized Model: One Team, Total Authority

In the **centralized model**, a single governance team owns all AI oversight. Every AI system, regardless of which business unit builds or operates it, must go through this central team for risk assessment, approval, monitoring, and lifecycle management. The team sets the standards, conducts the reviews, operates the model registry, and holds the deployment gate. No AI reaches production without their sign-off.

The advantages are real and significant. Consistency is the most obvious: every AI system is evaluated against the same criteria by the same people using the same risk framework. There is no ambiguity about what "approved" means because the same team defines it every time. Expertise concentration follows naturally. A centralized team of ten governance specialists develops deeper knowledge of regulatory requirements, risk patterns, and evaluation methods than any individual business unit could build on its own. Accountability is clean. When the regulator asks "who oversees your AI systems?" you point to one team with one leader and one charter. Board reporting is straightforward because a single team owns the aggregated view of the entire AI portfolio.

The disadvantages are equally real and become more severe as the organization scales. The central team becomes a bottleneck. If you have thirty-eight AI systems in production, eight new deployments in the pipeline, and a five-person governance team, the math does not work. Reviews take weeks. Teams game the system by deploying first and seeking approval later, or by reclassifying their AI system as "a rules-based tool" to avoid the queue. The central team is also disconnected from business context. A governance analyst reviewing a claims-processing model and a content-recommendation model on the same day applies the same framework to both, but the risk landscape, the customer impact, and the regulatory exposure are wildly different. Without deep domain knowledge, the central team either over-governs low-risk systems or under-governs high-risk ones — sometimes both in the same week.

The centralized model works best for organizations with a small AI portfolio, typically fewer than fifteen to twenty systems in production. It works for heavily regulated industries — banking, healthcare, insurance — where the cost of inconsistency is higher than the cost of delay. And it works for organizations in the early stages of AI governance, where the centralized team is building the standards, templates, and risk frameworks that will eventually be distributed to business units. Think of it as the scaffolding: essential during construction, but not the permanent structure.

## The Hub-and-Spoke Model: Standards at the Center, Execution at the Edge

The **hub-and-spoke model** places a central governance team — the hub — responsible for setting enterprise-wide standards, maintaining the risk framework, operating shared infrastructure like the model registry, and providing tools and training. Business unit teams — the spokes — are responsible for executing governance locally: conducting risk assessments, managing deployment gates, monitoring production systems, and reporting back to the hub. The hub governs how governance is done. The spokes govern what gets done.

This is the dominant model at enterprise scale in 2026, and for good reason. It solves the two fatal problems of the centralized model — bottleneck and context — without abandoning the consistency that distributed models sacrifice. The hub defines what a risk assessment must include, what thresholds trigger escalation, what evidence a deployment package requires, and what monitoring cadence applies at each risk tier. The spokes apply those standards with the domain expertise that the central team lacks. A spoke team in the healthcare division understands HIPAA implications of a specific data flow in ways that no central generalist could. A spoke team in the consumer product division understands the reputational dynamics of a customer-facing chatbot in ways that a centralized risk analyst reviewing a spreadsheet cannot.

The scalability advantage compounds as the organization grows. When a new business unit launches an AI initiative, the hub does not need to hire new staff. It provides the spoke with the standards, the templates, the training, and the tooling, and the spoke staffs its own governance capacity. IBM's research on AI operating models found that centralized or hub-and-spoke models yielded roughly thirty-six percent higher AI ROI than fully decentralized approaches, primarily because standardized risk practices reduced rework, incident costs, and regulatory friction.

The disadvantage is drift. When fifty governance practitioners across twelve business units are applying the same framework, interpretations diverge. One spoke might classify a system as medium risk that another spoke would classify as high risk. Escalation thresholds become elastic. Standards that the hub defined with precise language get reinterpreted at the edges into looser practices. This is not malice. It is organizational entropy. Without active countermeasures, the hub-and-spoke model decays into the embedded model with better documentation.

Active countermeasures include mandatory calibration sessions — quarterly meetings where spoke teams review each other's risk assessments to norm on severity ratings and escalation triggers. They include hub-conducted audits of spoke decisions, not to override them, but to identify divergence patterns and update standards where ambiguity exists. They include shared dashboards that give the hub real-time visibility into spoke activity: how many assessments are being conducted, how many systems are in production, what risk distribution looks like across the enterprise. And they include rotation programs, where spoke practitioners spend time in the hub and hub practitioners embed temporarily in spokes, building the shared language and calibration that prevent drift from becoming fracture.

## The Embedded Model: Governance Lives Where the Work Lives

In the **embedded model**, there is no central governance team. Governance responsibilities are distributed entirely to product and engineering teams. Each team that builds AI is also responsible for assessing its risks, managing its approvals, monitoring its production behavior, and reporting its compliance posture. The governance function is not a separate organization — it is a capability embedded in every team that ships AI.

The advantages are speed and context. There is no handoff delay because the people building the system are the same people assessing its risk. There is no context loss because the governance practitioner is the product manager or the engineering lead who understands the domain, the data, and the customer. Decisions happen in hours, not weeks. Deployment velocity is maximized because there is no external gate to wait for.

The disadvantages are severe enough that most enterprises abandon this model within twelve to eighteen months of attempting it. The first problem is inconsistency. Without a central standard, every team develops its own definition of risk, its own assessment process, its own monitoring practices, and its own threshold for what requires escalation. Some teams are rigorous. Others are perfunctory. The organization has no way to tell which is which until an incident reveals the answer. The second problem is visibility. No single person or function has a portfolio view of all AI systems, their risk profiles, and their compliance status. When the regulator asks "what AI are you running?" you are back to the scenario from Chapter 1 — sending emails to every division head and hoping they respond honestly. The third problem is what you might call **shadow governance gaps** — the quiet spaces between teams where nobody owns the question. Who governs the model that two business units share? Who monitors the data pipeline that feeds three different AI systems? Who decides when a cross-functional risk requires cross-functional review? In the embedded model, the answer is often nobody, because each team's governance scope stops at its own boundary.

The embedded model works, genuinely works, in only one context: very small organizations with very few AI systems, where the people building AI can see the entire landscape because the landscape is small enough to see. A fifteen-person startup with two AI features does not need centralized governance. The CTO knows every system, every data flow, every risk. The moment the organization grows past the point where one person can hold the entire AI portfolio in their head — usually around fifteen to twenty AI systems or five to seven separate teams building AI — the embedded model fractures.

## Choosing the Right Model for Your Organization

The choice between these models is not permanent. Most enterprises that end up with hub-and-spoke governance started somewhere else and transitioned. The typical trajectory moves through three stages, and understanding the transition signals is more useful than memorizing the end state.

Stage one is the startup phase: fewer than ten AI systems, one or two teams building AI, low regulatory exposure. Governance is embedded because it has to be — there aren't enough people to staff a central function, and the CEO or CTO can personally oversee every deployment. The signal to transition out of this stage is the moment you discover an AI system in production that leadership didn't know about. That discovery means the portfolio has outgrown the individuals who were tracking it informally.

Stage two is the centralization phase: ten to forty AI systems, growing regulatory scrutiny, first compliance requirements. The organization builds a central governance team, typically three to eight people, who create the initial risk framework, model registry, and deployment gate. This phase is necessary because the standards don't exist yet and someone needs to build them. The signal to transition out of this stage is when the central team becomes a bottleneck. Review times exceed two weeks. Teams start gaming classifications to avoid the queue. The backlog of governance requests is growing faster than the team can process them.

Stage three is the hub-and-spoke phase: forty or more AI systems, multiple business units with AI capabilities, mature standards and tooling. The central team evolves into the hub, retaining ownership of standards, tooling, training, escalation review, and portfolio visibility. Business units staff their own governance practitioners — either dedicated roles or governance responsibilities embedded into existing product and engineering roles — who execute the framework locally. This is where most large enterprises land by 2026, and the organizations that designed the transition intentionally arrive in better shape than those that stumbled into it after a crisis.

## Why the Org Model Matters More Than the Policy

A common mistake is spending months perfecting the governance policy while leaving the organizational model as an afterthought. The opposite priority is correct. A mediocre policy executed by the right organizational model produces better outcomes than a perfect policy executed by the wrong one. The policy tells you what governance should do. The organizational model determines whether governance actually happens. If the policy says "all high-risk systems require a risk assessment" but the organizational model has no one with the time, authority, or incentive to conduct that assessment, the policy is irrelevant.

The organizational model also determines the political dynamics of governance. In the centralized model, governance is a gatekeeper — it controls access to production. Business units either partner with it or fight it. In the hub-and-spoke model, governance is a service provider at the hub and a peer responsibility in the spokes — it enables deployment rather than blocking it, but it still holds escalation authority. In the embedded model, governance is whatever each team makes of it — sometimes rigorous, sometimes invisible. These political dynamics shape the culture of AI development more than any policy document ever will. When governance is a gatekeeper, teams learn to avoid it. When governance is a service, teams learn to use it. When governance is invisible, teams learn to ignore it.

The next step in building the right organizational model is deciding where to house the central expertise — and the most common answer in 2026, the AI Center of Excellence, turns out to be harder to get right than most leaders expect.
