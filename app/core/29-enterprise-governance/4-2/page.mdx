# 29.4.2 — Risk Classification Under the EU AI Act: Prohibited, High-Risk, Limited, and Minimal

Most teams assume that classifying their AI systems under the EU AI Act is a straightforward exercise. Read the categories. Match your systems. Move on to compliance. In practice, classification is the single most contested and ambiguous step in the entire compliance process. A customer support chatbot that routes users to human agents sounds like minimal risk — until it handles insurance claims and starts influencing coverage decisions, which pushes it toward high-risk. A resume parsing tool that extracts structured data from CVs sounds like a utility — until you realize it feeds a scoring system that ranks candidates for hiring managers, which puts it squarely in Annex III's employment category. The classification you assign to each system determines every obligation that follows: whether you need a conformity assessment, whether you need human oversight mechanisms, whether you must register the system in the EU database, and whether your organization faces fines for non-compliance. Get the classification wrong and you either over-invest in compliance for systems that do not require it, or you under-invest for systems that do — and the second mistake is the one that ends careers.

## The Four-Tier Risk Architecture

The EU AI Act organizes AI systems into four risk tiers, each carrying different obligations. The architecture is deliberately graduated: the higher the potential harm, the heavier the regulatory burden. This is not arbitrary. It reflects a policy choice that a spam filter should not bear the same compliance cost as a system that determines whether someone qualifies for a mortgage. The tiers are: prohibited, high-risk, limited risk, and minimal risk.

**Prohibited AI practices** are banned entirely. You cannot build them, deploy them, or place them on the EU market under any circumstances. **High-risk AI systems** are permitted but must meet the full set of requirements defined in Chapter III of the Act — risk management, data governance, documentation, logging, human oversight, accuracy, robustness, cybersecurity, conformity assessment, and post-market monitoring. **Limited risk AI systems** must meet specific transparency obligations — primarily ensuring that users know they are interacting with an AI system. **Minimal risk AI systems** face no specific obligations under the Act, though voluntary codes of conduct are encouraged. The vast majority of AI systems deployed globally fall into the minimal risk category. The systems that matter most for compliance — and where most organizations will spend their effort — are the high-risk systems.

## Prohibited Practices: The Hard Lines

The Act draws absolute lines around eight categories of AI that are considered to pose unacceptable risk. These prohibitions have been enforceable since February 2, 2025. There is no compliance pathway for prohibited systems — they must be decommissioned or fundamentally redesigned so they no longer fall within the prohibited category.

The first prohibition covers AI systems that deploy subliminal techniques beyond a person's consciousness, or deliberately manipulative or deceptive techniques, to materially distort behavior in a way that causes or is reasonably likely to cause significant harm. The second covers AI systems that exploit vulnerabilities of specific groups — age, disability, or social or economic situation — to materially distort their behavior in a harmful way. The third prohibits social scoring: AI systems that evaluate or classify people based on their social behavior or personal characteristics, where the resulting score leads to detrimental treatment that is unjustified or disproportionate. The fourth prohibits AI systems that assess the risk of an individual committing criminal offenses solely based on profiling or personality traits, without being based on objective and verifiable facts directly linked to criminal activity.

The fifth prohibition covers real-time remote biometric identification in publicly accessible spaces for law enforcement, with narrow exceptions for specific serious crimes, missing persons searches, and imminent terrorist threats. The sixth prohibits the untargeted scraping of facial images from the internet or CCTV footage to build or expand facial recognition databases. The seventh prohibits emotion recognition in the workplace and educational institutions, with exceptions for medical or safety purposes. The eighth prohibits biometric categorization systems that categorize people based on biometric data to infer sensitive characteristics such as race, political opinions, trade union membership, religious beliefs, sex life, or sexual orientation — again with narrow exceptions for law enforcement filtering of lawfully acquired datasets.

If any of your systems perform functions that resemble these categories, your legal and governance teams need to evaluate them immediately. The fines for operating a prohibited system are the Act's highest: up to 35 million euros or seven percent of global annual turnover.

## High-Risk: Where Most Compliance Effort Concentrates

High-risk AI systems are defined through two pathways. The first pathway, under Article 6 paragraph 1, covers AI systems that serve as safety components of products already regulated under existing EU harmonized legislation — medical devices, machinery, toys, lifts, radio equipment, civil aviation, motor vehicles, and similar categories listed in Annex I. If your AI system is a safety component of a product that requires a third-party conformity assessment under existing EU product law, that AI system is automatically high-risk.

The second pathway, under Article 6 paragraph 2, covers AI systems in specific use-case areas listed in Annex III. This is where most enterprise AI systems encounter the high-risk classification. Annex III defines eight domains with specific use cases.

**Biometrics** covers AI systems intended for biometric categorization according to sensitive or protected attributes, and AI systems intended for emotion recognition outside the prohibited contexts.

**Critical infrastructure** covers AI systems used as safety components in the management and operation of critical digital infrastructure, road traffic, or the supply of water, gas, heating, or electricity.

**Education and vocational training** covers AI systems that determine access or admission to educational institutions at all levels, evaluate learning outcomes, assess the appropriate level of education for an individual, and monitor or detect prohibited behavior during tests.

**Employment, workers management, and access to self-employment** covers AI systems used for recruitment, application filtering, candidate evaluation and ranking, task allocation based on individual behavior or personal traits, and performance monitoring or evaluation in work-related relationships. This category catches a surprisingly broad range of enterprise AI systems. Any AI tool that influences hiring, performance review, or workforce allocation decisions is likely high-risk.

**Access to essential private services and public services** covers AI systems used to evaluate creditworthiness, evaluate eligibility for public assistance benefits, assess risk in life and health insurance, and dispatch or prioritize emergency first response services.

**Law enforcement** covers AI systems used for individual risk assessment, polygraphs and similar tools, evaluation of evidence reliability, profiling in crime detection, and crime analytics for searching complex data sets.

**Migration, asylum, and border control management** covers AI systems used for polygraphs and similar tools, assessment of security risks posed by individuals, examination of applications for asylum, visa, or residence permits, and identification of individuals in the context of migration.

**Administration of justice and democratic processes** covers AI systems used to assist judicial authorities in researching and interpreting facts and law and in applying the law, and AI systems intended to influence the outcome of elections or referendums.

## The Classification Challenge: Why Ambiguity Is the Norm

The categories in Annex III seem clear enough when described at a high level. In practice, mapping your actual AI systems to these categories requires judgment calls that are anything but clear.

Consider a large language model that powers an internal knowledge base for HR managers. The system answers questions about company policies, summarizes employee handbook provisions, and helps managers draft performance improvement plans. Is this a high-risk system under the employment category? The system does not evaluate candidates or make hiring decisions. But it helps managers draft documents that directly influence employment decisions. If a manager asks the system to help frame a termination rationale, and the system generates language that shapes the decision, the system is arguably influencing employment outcomes even though it was not designed to do so.

Or consider a customer service AI that handles insurance inquiries. Most of the time it answers questions about policy terms and helps customers file claims. But when it triages claims by urgency and routes high-priority cases to senior adjusters, it is making decisions about the allocation of essential private services. Does the routing function make the entire system high-risk, or only the routing component?

The Act includes an important nuance in Article 6 paragraph 3: a high-risk AI system in Annex III is not classified as high-risk if it does not pose a significant risk of harm to health, safety, or fundamental rights, including by not materially influencing the outcome of decision-making. This exception applies when the AI system performs a narrow procedural task, improves the result of a previously completed human activity, detects decision-making patterns without replacing or influencing human assessment, or performs a preparatory task to an assessment relevant to the use cases listed in Annex III. But claiming this exception requires documented justification. You cannot simply assert that your system does not materially influence outcomes — you must demonstrate it, and the demonstration must withstand regulatory scrutiny.

The European Commission was required by February 2, 2026 to provide guidelines with a comprehensive list of practical examples of use cases that are high-risk and not high-risk. These guidelines help reduce ambiguity, but they cannot eliminate it. Every organization's AI portfolio contains systems that sit in gray areas, and the classification for those systems requires a reasoned analysis, documented in enough detail to serve as evidence if a regulator questions the decision.

## Documenting Classification Rationale

The classification decision is not just an internal exercise. It is a governance artifact that must be documented, justified, and defensible. When a regulator asks why you classified a system as minimal risk rather than high-risk, your answer cannot be "we discussed it and agreed." It must be a documented analysis that explains which Annex III category was considered, why the system does or does not fall within the category's scope, and if you applied the Article 6 paragraph 3 exception, what evidence supports the claim that the system does not materially influence decision-making.

The documentation should include the system's intended purpose, the actual use cases observed in production, the types of decisions or outputs the system influences, the populations affected by those outputs, and the rationale for the classification conclusion. This documentation must be maintained over time because classification can change. A system that starts as a simple search tool may evolve into a recommendation engine that influences employment decisions as features are added. Each significant change to the system's functionality or deployment context should trigger a reclassification review.

Organizations with large AI portfolios should maintain a classification register — a centralized record of every AI system, its assigned risk tier, the rationale for that assignment, the date of the most recent classification review, and the individual or committee responsible for the decision. This register serves double duty: it is the foundation for your compliance program and it is the first document a regulator will ask to see.

## Limited Risk: The Transparency Tier

Limited risk AI systems face a single primary obligation: transparency. Users must be informed that they are interacting with an AI system. This applies to three specific categories. First, chatbots and conversational AI — anyone interacting with your AI-powered customer service agent must be told they are talking to an AI, not a human. Second, systems that generate or manipulate content, including deepfakes — AI-generated images, audio, or video must be labeled as artificially generated or manipulated. Third, emotion recognition systems and biometric categorization systems that are not prohibited must inform the individuals being subjected to them.

The transparency requirements seem simple, but they have engineering implications. Your chatbot needs a disclosure mechanism that fires before the interaction begins, not buried in terms of service. Your content generation systems need watermarking or metadata that identifies outputs as AI-generated. Your emotion recognition deployments, if they fall outside the prohibited categories, need explicit notification to subjects. These are not policy decisions — they are feature requirements that must be designed, built, tested, and monitored.

## Minimal Risk: Not Zero Obligation

The minimal risk tier carries no specific obligations under the Act. Most AI systems in the world — recommendation algorithms, spam filters, search engines, game AI, weather prediction models — fall into this category. But minimal risk does not mean zero risk, and it does not mean your organization has no governance responsibilities for these systems. The Act encourages voluntary codes of conduct for minimal risk systems, and your organization's internal governance framework should still apply risk-proportionate oversight to these systems. A spam filter that accidentally blocks critical business communications can cause real harm, even if the Act does not regulate it.

The practical implication for your compliance program: every AI system must be classified, even those that land in the minimal risk tier. The classification register should include all systems, not just the regulated ones. When a regulator asks for your AI inventory, a complete register that includes minimal risk systems demonstrates comprehensive governance. A register that only includes high-risk systems raises the question of whether you have systems you failed to classify.

The classification framework determines the regulatory burden. For most enterprise organizations, the high-risk requirements in Chapter III of the Act represent the core compliance challenge — the engineering work, the documentation work, and the organizational work that must be complete before the August 2026 deadline. The next subchapter breaks those requirements into an operational checklist that your engineering and governance teams can execute against.
