# 29.6.2 — Training Data Governance: Provenance, Consent, and Quality Controls

In late 2024, a mid-sized insurance company in Germany assembled a dataset of two million policyholder claims to train a model that would automate initial claim assessments. The data team spent four months cleaning, formatting, and validating the dataset. The model performed well in testing. Three weeks after deployment, the company's data protection officer received an inquiry from the regional supervisory authority asking under what legal basis the company had used policyholder data — submitted for claims processing — to train an artificial intelligence model. The company's privacy notice covered data processing for claims handling. It said nothing about machine learning. The DPO could not identify a valid legal basis for the training use. The model was pulled from production. The four months of data preparation and the model itself became sunk costs. The root cause was not technical. It was governance: nobody had verified that the consent basis for the data covered its intended use before the data entered the training pipeline.

This is the pattern. Teams focus on data quality — is the data clean, is it formatted correctly, does the model perform well — while treating provenance and consent as afterthoughts. By the time someone asks "do we have the right to use this data for this purpose," the model is already built. Training data governance means asking that question before a single example enters the pipeline, and having the infrastructure to answer it.

## Provenance: Knowing Where Your Data Actually Came From

**Data provenance** is the chain of custody from original source through every transformation to the point where data enters a training set. In traditional systems, provenance answers "where did this data come from?" For AI training data, it must answer a harder set of questions: who created this data, under what terms was it collected, what processing was applied, who has touched it since, and do the rights that applied at collection still apply for the intended use?

Provenance tracking for training data requires capturing metadata at every stage. At collection, you record the source — whether it is internal operational data, purchased from a vendor, scraped from the web, contributed by users, or generated synthetically. You record the date of collection and the conditions under which it was obtained. At processing, you record every transformation: filtering, deduplication, anonymization, augmentation, format conversion. At validation, you record quality checks performed, issues found, and decisions made about inclusion or exclusion. At ingestion into the training pipeline, you record the specific dataset version, the training run identifier, and the model version that resulted.

This is more metadata than most organizations are accustomed to tracking for any single data asset. But without it, you cannot answer the questions that regulators, auditors, and your own legal team will ask. When the EU AI Act requires a sufficiently detailed summary of training data — a requirement that took effect for general-purpose AI models in August 2025, with a mandatory template published by the European Commission — you need this metadata to generate accurate documentation. When a data subject exercises their right to know whether their data was used for training, you need this metadata to answer them. When a copyright holder challenges your use of their content, you need this metadata to demonstrate your due diligence.

## The Consent Maze

Consent for AI training is not a single question. It is a layered problem where the answer depends on the data source, the jurisdiction, the relationship with the data subject, and the intended use of the model.

**Internal operational data** is the most common training data source and the most commonly mismanaged from a consent perspective. When customers interact with your product, they generate data — transactions, support tickets, usage patterns, communications. Your privacy policy almost certainly covers using this data to provide and improve the service. Whether "improve the service" includes training a machine learning model depends on the specificity of your privacy notice, the expectations of reasonable users, and the interpretation of your local data protection authority. CNIL's guidance from 2024 warns that vague improvement language is unlikely to constitute valid consent for model training. The safer approach is explicit: update your privacy notice to specifically mention machine learning and model training as processing purposes, and ensure that the legal basis — whether consent, legitimate interest, or contractual necessity — genuinely applies.

**Purchased datasets** introduce vendor risk. When you buy a dataset from a third-party provider, you inherit the consent and rights chain from that provider. If the provider scraped data without consent, purchased it from another provider who misrepresented rights, or collected it under terms that do not extend to your use case, you bear the compliance risk. Vendor due diligence for training data must include verification of the provider's collection methods, the legal basis for the data, the rights granted in the license, and whether the license explicitly permits use for model training. A dataset licensed for "analytics and research" may not cover commercial model training. A dataset licensed for "internal use" may not cover a model deployed to external customers.

**Web-scraped data** is the most governance-intensive category. From 2026, the EU AI Act requires AI developers to check whether a data source has a copyright reservation under the Copyright Directive and to exclude or license that content before using it for training. Even absent copyright restrictions, web-scraped data often contains personal information that was posted in a specific context — a forum comment, a product review, a social media post — with no expectation that it would be used to train a commercial AI model. The governance challenge is not just legal. It is reputational. Public backlash against companies found to be training on user-generated content without meaningful consent has been significant and swift.

**Synthetic data** — data generated by models rather than collected from real sources — appears to sidestep consent issues. It does not. If synthetic data is generated by a model that was itself trained on data with consent limitations, the synthetic data inherits the provenance challenges of the original training data. Governing synthetic data requires tracing its lineage back through the model that generated it to the data that trained that model. We cover synthetic data governance in detail in subchapter 29.6.5.

## Quality Controls Beyond Traditional Standards

Training data quality controls must go beyond accuracy and completeness to address the dimensions that determine whether a dataset will produce a model that behaves as intended.

**Representativeness** is the first AI-specific quality dimension. A training dataset must represent the population the model will serve. If you are building a customer service model for a global user base, a training set drawn entirely from North American English speakers will produce a model that underperforms for the rest of your users. Representativeness audits examine the distribution of key attributes — language, geography, demographic characteristics, use case types — and compare them to the expected production distribution. Gaps are not just quality issues. They are governance risks, because a model that performs poorly for specific populations may violate anti-discrimination regulations.

**Label accuracy** is the second dimension. Supervised training depends on labels — the "correct answers" that teach the model what to produce. Labels created by annotators who lack domain expertise, who received ambiguous instructions, or who were under time pressure produce models that learn the wrong patterns. A legal document classification model trained on labels created by annotators who cannot distinguish a contract amendment from a contract addendum will confidently misclassify documents in production. Quality controls for labels include inter-annotator agreement measurement, domain expert review of label samples, and ongoing label audits as the training set grows.

**Freshness** is the third dimension. A model trained on data from 2023 and deployed in 2026 may have learned patterns that no longer apply. Customer behavior shifts. Product catalogs change. Regulatory requirements evolve. Stale training data produces models that are confidently wrong about the current state of the world. Quality controls for freshness include dating every training example, setting maximum age thresholds based on how quickly the domain changes, and implementing refresh pipelines that introduce current data on a defined schedule.

**Coverage gaps** are the fourth dimension. These are the scenarios your model will encounter in production that are not represented in your training data. A fraud detection model trained primarily on credit card fraud may miss emerging fraud patterns in digital wallet transactions or cryptocurrency payments. Coverage gap analysis compares known production scenarios against training data composition and identifies categories where the model will be operating without adequate training signal. These gaps must be documented and addressed before deployment — either by acquiring additional training data or by implementing fallback mechanisms for scenarios the model is not equipped to handle.

## The Governance Pipeline

Training data governance is not a one-time review. It is a pipeline of checkpoints that data must pass through before it reaches a model. The pipeline has four stages. At collection, you verify provenance and legal basis. At processing, you verify that transformations preserve the integrity of provenance metadata and do not introduce bias. At validation, you verify representativeness, label quality, freshness, and coverage. At training ingestion, you verify that the specific dataset version is documented, that all upstream checkpoints have been satisfied, and that the training run is linked to the validated dataset.

Each checkpoint produces an artifact — a record of what was verified, by whom, when, and what the outcome was. These artifacts form the compliance documentation that regulators, auditors, and your own governance board require. They are not paperwork for paperwork's sake. They are the evidence that your organization exercises responsible control over the data that shapes your AI systems' behavior. Without them, you are training models on data you cannot fully account for, under rights you have not fully verified, with quality you have not fully assessed.

The next subchapter examines data lineage for AI — how to trace the path from raw data source to model decision, and why traditional lineage tools cannot make that journey alone.
