# 29.8.2 — AI Incident Classification: Severity Frameworks for AI-Specific Failures

In late 2025, a mid-size insurance company discovered that its automated claims processing model had been systematically undervaluing claims from policyholders in certain zip codes — zip codes that correlated strongly with race and income level. The pattern had persisted for four months. When the data science team escalated the issue, the incident management team classified it as a SEV-3 based on their standard framework: no system outage, no data breach, fewer than five hundred customer complaints received. The engineering team scheduled a fix for the following sprint. Three weeks later, a state insurance regulator opened a formal investigation, a class action lawsuit was filed on behalf of twelve thousand affected policyholders, and the company's general counsel told the board that the total exposure exceeded forty million dollars. The incident was a SEV-3 in the software world. In the AI world, it was a catastrophe that the severity framework was blind to.

## Why Software Severity Frameworks Fail for AI

Traditional severity frameworks classify incidents along two axes: system availability and user impact measured by volume. A SEV-1 means the system is completely down and all users are affected. A SEV-2 means significant degradation affecting many users. A SEV-3 means partial degradation with limited user impact. A SEV-4 means a minor issue with minimal impact. This framework works brilliantly for infrastructure because availability and user volume capture the dimensions that matter most.

AI incidents break this framework because the dimensions that matter most are entirely different. A biased lending model can be one hundred percent available, serving every request within its latency SLA, with zero errors in any infrastructure log — and still be producing discriminatory outcomes that violate fair lending law. Under the traditional severity framework, this system is healthy. Under any framework designed for AI, this system is in crisis. The gap between what the severity framework sees and what is actually happening is where organizational damage accumulates. The MIT AI Risk Repository, which now catalogs over sixteen hundred classified risks, demonstrates how wide the spectrum of AI failure modes has become — far beyond what any availability-based severity framework was designed to capture.

## The Five Dimensions of AI Incident Severity

An effective AI incident classification framework assesses severity across five dimensions that traditional frameworks ignore. Each dimension captures a different axis of potential damage, and the overall severity is determined by the highest-scoring dimension, not the average. This is a critical design decision. An incident that scores low on four dimensions but high on regulatory exposure is not a medium-severity incident. It is a high-severity incident with narrow but acute risk.

**Harm type** is the first dimension. Accuracy degradation — the model is returning worse answers — is the most common and typically the least severe in isolation. Bias and discrimination — the model is producing systematically different outcomes for different populations — carries legal and regulatory weight that accuracy degradation does not. Data exposure — the model is leaking training data, personal information, or confidential content in its outputs — triggers breach notification requirements under GDPR and state privacy laws. Unauthorized action — an AI agent taking actions beyond its intended scope — creates liability that extends beyond the model to the entire system architecture. Hallucination leading to real-world harm — a medical recommendation, legal advice, or financial guidance that a person acts on — carries consequences that cannot be undone with a model rollback.

**Harm scope** is the second dimension. An incident affecting a single individual is fundamentally different from one affecting a demographic group or an entire user population. The insurance company's zip-code bias affected twelve thousand policyholders across a specific demographic. A single hallucinated response to a customer query affects one person. The response protocols, the stakeholders involved, and the communication requirements differ dramatically between individual-scope and population-scope incidents. Population-scope incidents almost always involve Legal and Communications. Individual-scope incidents often do not, unless the individual is high-profile or the harm is severe enough to attract attention on its own.

## Reversibility: The Dimension That Drives Urgency

**Harm reversibility** is the third dimension and one of the most important for determining response urgency. Some AI harms are reversible — a bad product recommendation can be corrected, a miscategorized support ticket can be reclassified, an incorrect content moderation decision can be overturned on appeal. Other AI harms are partially reversible — a denied loan application can be reconsidered, but the applicant may have already lost the house they were bidding on. A rejected job candidate can be re-evaluated, but the position may have been filled. The harm is formally reversed but the real-world consequences remain.

And some AI harms are irreversible in any meaningful sense. A medical recommendation that a patient follows cannot be un-followed. A leaked piece of personal data cannot be un-leaked. A wrongful arrest based on a facial recognition match cannot give someone back the night they spent in jail. When harm is irreversible, every hour of delayed response compounds the damage in ways that no remediation can fully address. This is why reversibility must be assessed in the first minutes of triage, not during a leisurely post-incident analysis. The containment decision — whether to roll back the model, disable the feature, or take the system offline — depends heavily on whether the harm is still accumulating or has already been done.

## Regulatory Exposure: The Clock That Cannot Be Paused

**Regulatory exposure** is the fourth dimension. Some AI incidents trigger mandatory reporting obligations with fixed deadlines that begin ticking the moment you become aware of the incident, regardless of whether your investigation is complete. Under the EU AI Act, providers of high-risk AI systems must report serious incidents — those resulting in death, serious damage to health, serious disruption to critical infrastructure, or violation of fundamental rights — to the relevant market surveillance authority within fifteen days. The European Commission's September 2025 draft guidance on serious incident reporting clarifies that the clock starts when the provider establishes, or reasonably should have established, a causal link between the AI system and the harm. Under GDPR, personal data breaches must be reported to the supervisory authority within seventy-two hours. Under US financial regulations, certain algorithmic decision-making failures must be disclosed to relevant regulators.

The regulatory exposure dimension does not just affect severity classification — it drives the entire response timeline. An incident that triggers a fifteen-day reporting obligation cannot spend twelve of those days in an engineering investigation queue. Legal and Compliance must be notified immediately so they can begin preparing the regulatory report in parallel with the technical investigation. Organizations that treat regulatory reporting as a post-resolution activity — something that happens after the fix ships — routinely miss filing deadlines, which transforms a manageable incident into a regulatory violation with its own separate penalties.

## Reputational Sensitivity and the Public-Facing Factor

**Reputational sensitivity** is the fifth dimension. A bias incident in an internal document summarization tool carries minimal reputational risk because external users never see the outputs. The same bias in a public-facing chatbot, a hiring portal, or a consumer-facing recommendation engine creates immediate reputational exposure. The reputational dimension also depends on the domain. AI failures in healthcare, education, criminal justice, and financial services generate disproportionate public attention because people perceive these domains as consequential and personal. An equivalent technical failure in an internal logistics optimization tool barely registers.

Reputational sensitivity determines who must be in the incident response — specifically, whether your communications team needs to prepare holding statements, whether your executive team needs to be briefed, and whether your social media monitoring should be escalated. A purely internal incident with no regulatory exposure might involve only Engineering and the AI governance team. A public-facing incident in a sensitive domain might require Engineering, Legal, Compliance, Communications, the Chief Risk Officer, and the CEO — all within hours. The cost of over-escalating is a few people losing an hour of their evening. The cost of under-escalating is your CEO learning about a regulatory investigation from a news article.

## Mapping Dimensions to Response Protocols

The classification framework becomes operational when it maps dimensional scores to concrete response actions. A practical approach uses three severity tiers derived from the five dimensions.

**AI-Critical** incidents involve irreversible harm, population-level scope, regulatory reporting obligations, or high reputational sensitivity in any combination. These trigger immediate cross-functional mobilization — an incident commander is assigned, Legal and Compliance are notified within one hour, Communications prepares a holding statement, the model or feature is suspended pending investigation, and regulatory reporting timelines are tracked from the moment of classification.

**AI-Major** incidents involve reversible but significant harm, group-level scope, possible but uncertain regulatory exposure, or moderate reputational risk. These trigger the full incident response process with a designated lead, Legal notification within four hours, and an accelerated investigation timeline of forty-eight to seventy-two hours.

**AI-Standard** incidents involve individual-scope accuracy degradation with no regulatory or reputational dimensions. These follow the normal engineering incident process with the addition of AI-specific root cause analysis.

The person on call at two in the morning needs to be able to classify an incident into one of these tiers within fifteen minutes. That means the framework must be documented as a decision tree, not a committee process. The on-call engineer asks five questions — one for each dimension — and the highest-severity answer determines the tier. They do not need to fully understand the regulatory implications of GDPR Article 22 to know that the incident involves personal data in model outputs and therefore scores high on regulatory exposure. The decision tree converts domain-specific complexity into actionable classification that any trained responder can perform under pressure.

## The Decision Tree: Making Classification Fast Under Pressure

A decision tree that nobody has practiced using is a document, not a capability. The on-call responder at two in the morning needs a classification tool that works in fifteen minutes, not a committee process that requires gathering six stakeholders. The decision tree must translate the five dimensions into a sequence of yes-or-no questions that any trained responder can answer. Does the incident involve personal data in model outputs? If yes, regulatory exposure is high. Is the affected system public-facing in a sensitive domain? If yes, reputational sensitivity is high. Could someone suffer harm they cannot recover from? If yes, reversibility is low. The highest-severity answer across all five questions determines the tier.

Every engineer, ML platform operator, and on-call responder who might encounter an AI incident must walk through the classification framework during onboarding and in quarterly tabletop exercises. The exercise does not need to be elaborate. Present a scenario — "the content moderation model has been auto-approving posts that violate community guidelines for the past forty-eight hours, and a journalist has noticed" — and ask the responder to classify it using the five dimensions. If they reach the correct tier in under fifteen minutes, the framework is working. If they hesitate, ask clarifying questions that the decision tree does not answer, or misclassify the reputational dimension, the framework needs refinement. Classification accuracy improves with practice, not with longer documents.

## Keeping Classification Current

The classification framework is not a static document. New regulations change the regulatory exposure dimension — the EU AI Act's August 2026 enforcement date will reclassify incidents that were previously unregulated into the mandatory-reporting category. New deployment contexts change the reputational sensitivity dimension — expanding a model from internal use to customer-facing use elevates every incident involving that model. New harm patterns emerge as AI systems take on more consequential decisions, particularly as agentic AI systems become more common and "unauthorized action" moves from a theoretical risk category to a regular occurrence.

The framework must be reviewed quarterly by the AI governance body and updated immediately when regulatory deadlines arrive or when a post-incident analysis reveals a classification gap. Every post-incident review should include a specific question: "Did the classification framework correctly capture the severity of this incident?" If the answer is no — if the incident was more severe than the framework predicted, or if a dimension was missing — the framework must be updated before the next incident tests it.

The most common failure in AI incident classification is not getting the framework wrong. It is not having one at all and defaulting to the software severity framework that was never designed for these dimensions. The insurance company that classified a forty-million-dollar regulatory exposure as a SEV-3 did not make a classification error. They made the more fundamental error of using a classification system that could not see the dimensions that mattered. Build the framework before you need it. You will not have time to design it during the incident.

The classification tells you how severe the incident is. The next subchapter covers what you actually do about it — the response playbook that translates severity into coordinated action across every function that needs to move.
