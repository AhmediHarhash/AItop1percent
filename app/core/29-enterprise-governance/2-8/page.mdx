# 29.2.8 — Scaling Governance Across Business Units Without Creating Bureaucracy

Governance that is consistent across business units is necessary. Governance that is identical across business units is a mistake. This distinction separates organizations where governance scales from organizations where governance either collapses into meaningless paperwork or calcifies into a bottleneck that every team resents. The temptation at the executive level is uniformity: one process, one template, one approval path, applied everywhere. Uniformity feels fair. It looks clean in a board presentation. It is also the fastest way to ensure that governance becomes performative — technically followed and practically useless — because the business unit deploying a customer-facing clinical AI system and the business unit using a summarization tool for internal meeting notes do not face the same risks, operate under the same regulations, or need the same depth of review.

The insurance division processes health records under HIPAA. The marketing division generates ad copy using publicly available brand guidelines. The lending division makes credit decisions subject to fair lending laws. Applying identical governance to these three use cases means one of two outcomes. Either the governance is designed for the highest-risk unit and crushes the lower-risk units under unnecessary process, or it is designed for the lowest-risk unit and leaves the highest-risk unit dangerously undergoverned. Neither outcome is acceptable. The solution is a governance architecture that enforces a common standard while allowing local adaptation — a structure where the principles are universal but the execution is contextual.

## What Must Be Standardized

Standardization is the foundation. Without it, every business unit invents its own governance language, its own risk categories, and its own approval criteria, and the organization loses the ability to compare, aggregate, or audit AI risk at the enterprise level. The elements that must be standardized are the ones where inconsistency creates systemic risk or makes enterprise-level oversight impossible.

**Risk classification** must be standardized. Every business unit must use the same tier system — the same criteria for what makes a system Tier 1, Tier 2, or Tier 3, and the same consequences attached to each tier. If the lending division calls a system "high-risk" and the marketing division calls a functionally similar system "medium-risk" because they use different classification criteria, the organization has no meaningful way to aggregate its risk exposure. A centrally defined risk taxonomy, with clear criteria tied to data sensitivity, decision impact, regulatory exposure, and customer-facing status, ensures that when leadership asks "how many high-risk AI systems do we have across the organization," the answer means the same thing everywhere.

**Documentation requirements** must be standardized at the schema level. Every AI system, regardless of which business unit deploys it, must produce the same core documentation: a system description, a data flow map, a risk assessment, an evaluation summary, an ownership record, and an incident response plan. The depth and detail of each document will vary by risk tier, but the categories must be universal. This is what makes enterprise audit possible. When the internal audit team reviews AI governance across business units, they need to be able to compare like with like. If the insurance division documents risk assessments in a three-page narrative and the marketing division documents them in a one-paragraph Slack message, comparison is impossible and audit is theater.

**Incident reporting** must be standardized because incident patterns often cross business-unit boundaries. A model failure in one division may indicate a vulnerability in a shared model or a shared data pipeline that affects other divisions. Standardized incident reporting — with consistent severity classifications, consistent escalation triggers, and a centralized incident registry — ensures that an incident in one part of the organization is visible to the governance function and can be evaluated for cross-cutting impact. The 2025 wave of enterprises discovering that shadow AI tools had proliferated independently across multiple business units proved that without centralized incident visibility, the same failure can repeat in division after division while each believes it is an isolated event.

**Audit trail format** must be standardized for the same reason documentation must be standardized: enterprise-level oversight requires a common language. Every governance decision — every risk classification, every review outcome, every exception approval, every escalation — must be recorded in a format that the central governance team and internal audit can query, aggregate, and analyze. This means a shared governance platform or, at minimum, a shared data schema that all business units feed into. When the board asks "how many AI governance exceptions were granted across the organization in the last quarter, and what risk categories did they cover," the governance team must be able to answer without conducting a weeks-long survey of every business unit.

## What Must Be Localized

Localization is where governance becomes usable. The standardized elements establish what governance asks. The localized elements determine how governance happens in each specific context. Forcing a single "how" onto every business unit ignores the operational reality that different divisions have different workflows, different risk profiles, different regulatory obligations, and different team structures.

**Review workflows** must be adapted to local team structures. The insurance division may have a dedicated AI team with an embedded risk analyst who can conduct initial reviews before escalation to the central governance body. The marketing division may have a single data scientist using a vendor API, with no dedicated governance capacity. The review workflow for each unit must match its capacity. Demanding that the marketing team run the same multi-stage review process as the insurance team means either the marketing team ignores the process entirely or spends more time on governance paperwork than on the actual AI work. The central team defines the review standards — what questions must be answered, what evidence must be produced — and the business unit defines the workflow that produces those answers within their operational reality.

**Risk tier thresholds** need domain-specific calibration within the universal taxonomy. The standardized risk classification defines the criteria, but the specific thresholds may shift by domain. In the lending division, any model that influences a credit decision is automatically Tier 3 regardless of other factors, because fair lending regulations create a compliance floor that doesn't exist in other domains. In the marketing division, a text generation model that produces ad copy might be Tier 2 based on its customer-facing nature, but not Tier 3, because the regulatory exposure is lower and the consequences of a failure are reputational rather than legal. The central team sets the classification framework. The business unit, in consultation with the central team, applies domain-specific calibration that reflects their regulatory environment and risk appetite.

**Domain-specific risk factors** add a layer to the risk assessment that the universal framework cannot anticipate. A healthcare business unit needs to evaluate whether an AI system could produce outputs that constitute clinical advice, a risk factor that doesn't exist in any other division. A financial services unit needs to evaluate whether a model's training data reflects historical lending bias, a risk factor specific to their regulatory environment. A customer support unit needs to evaluate whether an AI agent could make commitments the company is legally obligated to honor, the exact scenario that played out in the Air Canada chatbot case. These domain-specific factors must be defined by the business unit with guidance from the central governance team, and they must be incorporated into the risk assessment alongside the universal criteria.

## The Hub-and-Spoke Model in Practice

The organizational structure that best supports standardization with local adaptation is the **hub-and-spoke model**. The central governance team is the hub. Each business unit has a governance liaison — or for larger units, a small local governance team — that forms the spoke. The hub sets standards, provides tooling, and maintains enterprise-level oversight. The spokes execute governance within their business units, adapt the standards to local context, and serve as the first line of governance engagement for their product and engineering teams.

The hub's responsibilities are specific: maintain the risk classification taxonomy, publish and update governance policies, provide governance tooling and templates, train business-unit liaisons, conduct calibration exercises to ensure consistent standards, manage the enterprise AI inventory, run the governance board for high-risk escalations, produce enterprise-level governance reporting, and track regulatory developments across jurisdictions. The hub is a service function, not an approval gate. Its job is to make governance easy for the spokes, not to review every decision the spokes make.

The spokes' responsibilities are equally specific: conduct initial risk assessments for new AI deployments in their unit, ensure documentation meets the standard schema, manage the local review workflow for Tier 1 and Tier 2 systems, escalate Tier 3 systems to the central governance board, report incidents into the centralized registry, maintain the local AI inventory as part of the enterprise inventory, and serve as the governance point of contact for their engineering and product teams. The spoke is empowered to make decisions within its domain. A Tier 1 deployment in the marketing division does not need to wait for the central governance team's approval. The local liaison assesses it, confirms it fits within pre-approved patterns, documents the assessment, and clears it to ship. The central team sees the record in the enterprise inventory but does not gate the decision.

The hub-and-spoke model only works if the liaisons in the spokes are properly equipped. This means training, calibration, and support. Training gives liaisons the skills to conduct risk assessments, apply the classification framework, and produce documentation that meets the standard. Calibration ensures that a Tier 2 classification in one business unit means the same thing as a Tier 2 classification in another. The most effective calibration mechanism is a quarterly exercise where the central team presents anonymized case studies and every liaison classifies them independently, followed by a discussion of any discrepancies. Calibration exercises prevent the gradual drift that occurs when liaisons operate in isolation — each slowly adapting the standards to their unit's preferences until the enterprise-level consistency is gone.

## The Governance Franchise Model

A useful framing for this structure is the **Governance Franchise Model**. Think of the central governance team as a franchise headquarters and each business unit as a franchise location. Headquarters provides the brand, the playbook, the training materials, the quality standards, and the supply chain. Each franchise location operates independently within those boundaries, adapting to its local market while maintaining the brand standard. A customer walking into any franchise location expects a consistent experience. The franchise owner has autonomy over staffing, scheduling, and local marketing, but not over the core product, the quality controls, or the brand identity.

In governance terms, the franchise headquarters provides the risk taxonomy, the policy framework, the documentation templates, the governance tooling platform, the training curriculum, and the quality standards for governance reviews. Each business unit operates within those boundaries. They hire their own liaison, run their own review workflows, and apply domain-specific risk factors. But they use the central taxonomy, they document in the central schema, they report into the central inventory, and they meet the central quality standards. The central team audits the spokes not to approve every decision but to ensure the standards are being applied consistently and the quality of governance work meets the bar.

The franchise model makes one thing explicit that the hub-and-spoke model sometimes leaves implicit: the relationship is contractual. The central team and each business unit operate under a governance service-level agreement that defines what the central team provides, what the business unit is responsible for, and what the performance standards are for both. The central team commits to responding to escalations within a defined timeline, providing updated templates within a defined cycle, and running calibration exercises on a defined cadence. The business unit commits to conducting risk assessments for every new deployment, maintaining current documentation, reporting incidents within a defined window, and participating in calibration exercises. When either side fails to meet its commitments, the SLA provides the escalation mechanism.

## Preventing Drift Without Creating Bottlenecks

The central challenge of any distributed governance model is drift. Over time, business units adapt the standards so aggressively that the standards become unrecognizable. One unit decides that certain documentation fields are "not applicable" and stops filling them out. Another unit starts approving Tier 2 systems through an informal process that skips the documentation step. A third unit classifies systems as Tier 1 that the central team would classify as Tier 2, because the local liaison has developed a different risk intuition from operating in isolation.

Drift is natural and must be actively managed, not punished. The tools for managing drift are audit, calibration, and feedback loops — not centralized approval gates that turn the hub into a bottleneck.

Audit means the central team periodically reviews a sample of governance decisions from each business unit. Not every decision — that defeats the purpose of distributed governance. A representative sample, reviewed quarterly, comparing the business unit's risk classifications, documentation quality, and review outcomes against the central standards. When the audit reveals drift, the response is calibration — a conversation with the liaison about the discrepancy, an update to the training materials if the standard was ambiguous, or a policy clarification if multiple units drifted in the same direction, which usually means the standard was unclear rather than the units were negligent.

Feedback loops close the circle. The central team must actively solicit feedback from the spokes about what is working and what isn't. When a template is so cumbersome that three business units are filling it out poorly, the problem is the template, not the units. When an escalation path is so slow that liaisons start approving systems locally to avoid the delay, the problem is the escalation path, not the liaisons. Governance drift is often a signal that the central standards don't work in practice, and the fix is to improve the standards rather than to tighten enforcement of broken ones.

## Governance SLAs by Risk Tier

The governance franchise model requires explicit performance standards, and the most important standard is speed. Governance that takes twelve weeks to review a deployment is governance that teams will circumvent. The way to prevent circumvention is to define and enforce SLAs by risk tier.

Tier 1 systems — low-risk, fitting pre-approved patterns, using non-sensitive data — should clear governance in two to five business days. The local liaison reviews the documentation, confirms the pattern match, and approves. No central team involvement required. If Tier 1 is taking longer than five days, the process has a design flaw.

Tier 2 systems — moderate risk, customer-facing or using sensitive data, requiring evaluation evidence — should clear governance in ten to fifteen business days. The local liaison conducts the initial review, the governance engineer validates the technical controls, and the approval is granted at the spoke level with a record visible to the hub. If Tier 2 reviews consistently exceed fifteen days, either the documentation requirements are too heavy for the risk level or the review process has unnecessary handoffs.

Tier 3 systems — high-risk, consequential decisions, regulated data, cross-border deployment — require central governance board review and should complete within twenty to thirty business days. This timeline includes the local team's preparation of documentation, the central team's technical review, the governance board's deliberation, and any required remediation. Tier 3 reviews are inherently heavier because the stakes justify the investment. But thirty days should be the ceiling, not the starting point. Organizations that allow Tier 3 reviews to stretch beyond thirty days without a specific, documented reason find that teams either avoid building high-risk AI systems entirely or build them and skip governance, both of which are worse than a rigorous thirty-day review.

Publishing these SLAs internally does two things. First, it sets expectations for product and engineering teams so they can plan deployment timelines that include governance review rather than treating governance as an unpredictable delay. Second, it creates accountability for the governance team itself. When the SLA says ten to fifteen days for Tier 2 and the actual average is twenty-two days, the governance team must investigate and fix the process. SLAs make governance a measurable service rather than an opaque process.

## Escalation Paths When Local and Central Disagree

Disagreement between local governance liaisons and the central governance team is inevitable and healthy. The local liaison understands their domain's operational reality. The central team understands the enterprise-level risk picture and regulatory requirements. When these perspectives conflict, the organization needs a defined escalation path — not a power struggle.

The most common disagreement is risk classification. A business unit classifies a system as Tier 2. The central team reviews it during audit and believes it should be Tier 3. The escalation path should be structured: the liaison presents their classification rationale, the central team presents theirs, and if agreement cannot be reached, the decision escalates to the governance board, which includes both technical and business representation. The board's decision is final and becomes precedent — a documented ruling that guides future classification decisions in similar cases. Over time, the library of precedent decisions reduces the frequency of disagreements because both sides can reference prior rulings.

A less common but more contentious disagreement is policy applicability. A business unit argues that a particular governance policy does not apply to their domain or that applying it would be disproportionately burdensome relative to the risk. This is a legitimate objection. The escalation path for policy applicability should include a formal exception process: the business unit documents the exception request, including the specific policy, the rationale for the exception, the alternative controls they propose, and the residual risk if the exception is granted. The central team evaluates the request and either grants the exception with conditions, denies it with rationale, or proposes a modified approach. Every granted exception has an expiration date and a review trigger. Exceptions are not permanent. They are time-bound accommodations that the central team monitors for continued appropriateness.

The worst outcome is a disagreement that has no resolution mechanism — where the local team and the central team simply stop communicating and the local team proceeds without governance approval. This is the failure mode that leads to shadow governance, where business units maintain the appearance of compliance while operating outside the actual standards. The prevention is structural: make the escalation path fast, fair, and final. Fast means the escalation resolves within days, not weeks. Fair means both sides present their case and a neutral party decides. Final means the decision sticks and both sides implement it. When the escalation mechanism works, disagreement strengthens governance. When it doesn't, disagreement destroys it.

## Making the Model Sustainable

Scaling governance across business units is not a one-time design exercise. It is an ongoing operating model that requires continuous investment in three areas: liaison development, standard evolution, and executive sponsorship.

Liaison development means treating the business-unit governance role as a professional investment, not a part-time assignment stapled onto someone's existing job. The liaison needs dedicated time, training, access to the central team, and career recognition for the governance work they do. Organizations that assign governance liaison duties to a junior analyst who already has a full-time role find that governance becomes the first thing dropped when workload increases. The liaison role must have explicit time allocation — at minimum twenty percent for smaller units, fifty percent or more for larger or higher-risk units — and the work must appear in their performance evaluation.

Standard evolution means the central team revisits and updates the governance framework on a regular cycle, incorporating feedback from the spokes, adapting to new regulatory requirements, and simplifying processes that have proven unnecessarily burdensome. A governance framework that was right when it launched becomes wrong over time if it is treated as finished. Quarterly reviews of the framework, informed by data on SLA performance, audit findings, liaison feedback, and regulatory changes, keep the standards relevant and the spokes engaged.

Executive sponsorship means a senior leader — the Chief AI Officer, the CTO, or the head of enterprise risk — visibly owns the governance program, defends its budget, resolves escalations that exceed the governance board's authority, and communicates its value to the broader organization. Without executive sponsorship, governance programs atrophy. Business units deprioritize governance work when nobody senior is watching. Budgets get trimmed during quarterly reviews because governance doesn't have a revenue line. Engineering leaders push back on governance requirements because they perceive no executive support for the function. The executive sponsor doesn't need to run the program. They need to show the organization that the program matters.

Organizational structure determines whether governance is operational or theoretical. The teams, roles, authority models, and scaling mechanisms covered in this chapter are the machinery that makes governance real. But machinery without direction produces motion, not progress. The next chapter addresses the governance frameworks and standards that give the machinery its purpose — the specific policies, taxonomies, and decision criteria that define what good governance actually requires.
