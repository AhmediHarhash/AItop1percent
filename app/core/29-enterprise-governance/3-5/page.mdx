# 29.3.5 — AI Policy Design: Writing Policies That People Actually Follow

**The Shelf Policy** is the most common and most expensive anti-pattern in AI governance. It is a fifty-page document written by lawyers for auditors, formatted like a regulatory filing, stored in a SharePoint folder that nobody visits, and referenced only when an executive asks "do we have an AI policy?" The answer is technically yes. The impact is functionally zero. The Shelf Policy exists in every industry, at every company size, and it persists because it satisfies the wrong audience. It satisfies the person who needs to check a compliance box. It does not satisfy the engineer who needs to know, this afternoon, whether the model they just built can go into production.

A major European bank discovered the cost of a Shelf Policy in early 2025. The bank had a comprehensive AI governance policy — sixty-three pages, approved by the board, aligned with the NIST AI RMF, mapped to ISO 42001 clauses, reviewed by external counsel. It covered everything: data governance, model validation, bias testing, monitoring, incident response, regulatory reporting. The policy had been published for fourteen months. When regulators asked the bank's ML engineering teams how they applied the policy to their daily work, every team gave a different answer. Two teams were unaware the policy existed. Three teams had read the summary but not the full document. One team had developed its own internal guidelines that contradicted the policy on key points. The bank had a policy. It did not have governance. The difference cost them a seven-figure remediation effort and a formal supervisory finding.

## What Makes a Policy Actionable

The distance between a Shelf Policy and an actionable policy is not length. It is not formatting. It is whether the person who needs to follow the policy can answer three questions after reading it: what exactly am I required to do, how do I demonstrate that I did it, and what happens if I don't?

An actionable policy has **clear scope** — it defines precisely which teams, which AI systems, and which activities it covers. Not "all AI activities" but "all AI systems that generate, modify, or classify content that is presented to external customers, including but not limited to chatbots, recommendation engines, automated email generation, and document summarization." The engineer reading this scope can immediately determine whether their system falls within it. If the scope is ambiguous, the policy fails at the first sentence because the reader does not know whether it applies to them.

An actionable policy has **specific requirements** — not aspirations, not principles, not goals, but requirements that can be verified as met or unmet. "Teams must evaluate model outputs for bias" is a principle. "Before deployment, teams must run the standard bias evaluation suite against the model's outputs on the demographic evaluation dataset, achieve a maximum disparate impact ratio of one point two across all protected categories, and submit the evaluation results to the governance team through the evaluation portal" is a requirement. The difference is that the second version tells the engineer exactly what to do, exactly what threshold to hit, and exactly where to submit the evidence. There is nothing left to interpret.

An actionable policy has **measurable compliance** — it defines how the organization will verify that teams are following the policy, what evidence demonstrates compliance, and how frequently compliance is assessed. "Compliance is assessed quarterly through automated pipeline checks and governance team review of evaluation submissions. Systems that have not submitted evaluation evidence within ninety days of their last assessment are flagged as non-compliant and escalated to the system owner's director." The team knows they will be checked. They know the cadence. They know the consequence of falling behind.

## The Structure of a Policy That Works

Every AI policy should follow a consistent structure that practitioners can navigate without reading the entire document. Consistency across policies means that once someone learns how to read one policy, they can quickly find what they need in any other.

The first section is **purpose and scope**. Two paragraphs maximum. Why does this policy exist, and who does it apply to? The purpose states the governance objective in business terms — not "to ensure responsible AI" but "to ensure that customer-facing AI systems produce outputs that are accurate, unbiased, and compliant with applicable regulations before deployment and throughout their operational life." The scope lists the specific categories of systems, teams, and activities covered. If the scope has exceptions, state them explicitly. "This policy does not cover internal-only AI tools used exclusively by employees for productivity enhancement, which are covered under the Internal AI Use Policy."

The second section is **definitions**. List every term that could be interpreted differently by different readers. "Customer-facing AI system," "deployment," "evaluation," "bias," "high-risk" — define them once, precisely, and use them consistently throughout the policy. This is not pedantic. It is the single most effective way to prevent teams from interpreting the policy differently. When a policy says "before deployment" and the engineering team defines deployment as "promoted to the production environment" while the governance team defines it as "made available to end users," the resulting gap creates compliance ambiguity that erodes the entire policy.

The third section is **requirements**. This is the core of the policy. Each requirement states what must happen, who is responsible, when it must happen in the development lifecycle, and what evidence of compliance must be produced. Organize requirements by lifecycle phase — design, development, evaluation, deployment, monitoring, retirement — so that a team working in a specific phase can quickly find the requirements relevant to their current work. Each requirement should be testable: an auditor should be able to look at the evidence and determine, without subjective judgment, whether the requirement was met.

The fourth section is **compliance and enforcement**. How is compliance assessed? What happens when a team is found non-compliant? What is the escalation path? What are the consequences? This section is where most policies go silent, and the silence is fatal. A policy without enforcement provisions is a suggestion. Enforcement does not mean punishment — it means accountability. The consequences should be proportional: a first violation might trigger a governance review and remediation plan. A pattern of violations might trigger a deployment freeze on the team's AI systems until compliance is restored. A deliberate circumvention of governance controls should trigger a formal escalation to senior leadership. State these consequences clearly. Teams that know the consequences make different choices than teams operating in an enforcement vacuum.

The fifth section is **review and updates**. When is this policy next reviewed? Who owns the review? How are policy changes communicated? This section ensures the policy remains current. A policy written in January 2025 that has not been updated by mid-2026 is almost certainly out of alignment with the current regulatory environment, the organization's current AI portfolio, and the state of the technology.

## Writing for Practitioners, Not Auditors

The Shelf Policy fails because it is written for the wrong audience. It is written for the person who will audit the organization and needs to verify that a policy exists. The actionable policy is written for the person who will follow the policy and needs to understand what it requires of them.

The difference is in the prose. Audit-oriented policies use passive constructions that obscure responsibility. "Bias assessments shall be conducted prior to deployment." Who conducts them? When, exactly? Using what methodology? Practitioner-oriented policies use active constructions that name the actor and the action. "The model owner runs the bias evaluation suite on the final candidate model before submitting a deployment request. The evaluation results are submitted through the governance portal and reviewed by the governance team within five business days."

The difference is in the specificity. Audit-oriented policies state requirements at a level of abstraction that passes a compliance check but provides no operational guidance. "Appropriate monitoring must be implemented for all AI systems." Practitioner-oriented policies state requirements at a level of specificity that tells the team what to build. "Customer-facing AI systems must log all inputs and outputs, report latency and error rate metrics to the central monitoring dashboard, and trigger an automatic alert if the model's output distribution shifts by more than ten percent from the baseline distribution measured at deployment."

The difference is in the format. Audit-oriented policies are dense documents that bury requirements in paragraphs of context. Practitioner-oriented policies make requirements visually scannable. Each requirement is a discrete statement. Requirements are numbered for easy reference. Cross-references between policies use specific requirement numbers, not vague phrases like "as described in the data governance policy."

Writing for practitioners does not mean dumbing down the policy. It means respecting the reader's time. An engineer who needs to check one requirement should not have to read fifteen pages of context to find it. A product manager who needs to understand the deployment requirements for a new AI feature should be able to locate those requirements within two minutes. If it takes longer, the policy is not serving its audience, and people will stop consulting it.

## The Policy Hierarchy: How Multiple Policies Relate

No single policy can cover every aspect of AI governance. Organizations need multiple policies — for data governance, for model development, for deployment and monitoring, for incident response, for vendor management of third-party AI, for acceptable internal use of AI tools. The question is how these policies relate to each other.

The **foundational policy** is the organization's overarching AI governance policy. It states the principles, defines the governance structure, establishes the risk classification methodology, and references the domain-specific policies that contain detailed requirements. Think of it as the constitution — the document from which all other policies derive their authority. This policy is approved at the board or executive committee level and changes rarely.

**Domain policies** address specific governance areas in detail. The AI data governance policy covers training data requirements, data quality standards, privacy obligations, and data retention rules. The AI model development policy covers evaluation requirements, bias testing, documentation standards, and approval processes by risk tier. The AI deployment and monitoring policy covers deployment gates, production monitoring requirements, and performance degradation response procedures. Each domain policy references the foundational policy's principles and risk classification but provides the operational detail that the foundational policy intentionally omits.

**Procedural documents** sit beneath domain policies and describe exactly how teams execute the policy requirements. These are not policies themselves — they are playbooks, runbooks, templates, and checklists. The bias evaluation procedure describes exactly which tools to use, which datasets to evaluate against, how to interpret the results, and where to submit the evidence. The deployment checklist itemizes every gate that must be passed before a model goes live. These documents change frequently as tools and processes evolve, and they are owned by the teams that use them rather than by the governance team.

The common mistake is mixing these levels. When procedural detail ends up in the foundational policy, the policy becomes too rigid and too long. When the domain policy is too abstract, teams lack the specificity to comply consistently. When procedural documents are not linked to the policies they implement, teams follow the procedure without understanding the policy rationale, which means they cannot adapt when they encounter a situation the procedure does not cover.

## The Review Cadence: Keeping Policies Current

An outdated policy is worse than no policy at all. A team that follows an outdated policy believes it is compliant when it is not. A team that discovers a policy is outdated loses trust in the governance system and starts making its own judgment calls about which parts of which policies to follow — which is exactly the inconsistency governance is designed to prevent.

Build a mandatory review cadence. The foundational AI governance policy is reviewed annually by the governance team, with any changes approved by the executive committee. Domain policies are reviewed semiannually, with input from the teams that implement them. Procedural documents are reviewed quarterly by their owning teams, with changes communicated to the governance team for consistency checking.

Beyond the scheduled reviews, certain events trigger immediate policy review. A new regulation taking effect — like the EU AI Act's August 2026 enforcement deadline for general-purpose AI model obligations — triggers a review of every policy that touches regulatory compliance. A significant AI incident, either internal or at a peer organization, triggers a review of the relevant domain policy to assess whether the existing requirements would have prevented the incident. The adoption of a new category of AI system, such as moving from assistive AI to agentic AI, triggers a review of whether existing policies adequately cover the new risk profile.

Every policy should display its last review date, its next scheduled review date, and the name of the person or team responsible for the review. This metadata is not administrative trivia. It is a trust signal. A practitioner who sees that the policy was last reviewed three months ago trusts it more than a practitioner who sees a review date from eighteen months ago. The review date is a commitment that the governance team takes the policy seriously enough to keep it current.

## Common Policy Failures and How to Avoid Them

Beyond the Shelf Policy, several other failure modes recur across organizations.

**The Aspirational Policy** states what the organization wishes it could do rather than what it can actually enforce. "All models must undergo comprehensive red-team testing before deployment" is aspirational if the organization has no red-team capability, no testing methodology, and no budget for external red-team services. Aspirational policies create cynicism. Teams read them, realize the organization cannot support the requirement, and conclude that the governance team is disconnected from operational reality. Write policies that match your current capability, with a roadmap for expanding the requirements as capabilities mature.

**The Copy-Paste Policy** is lifted from an external framework, another organization's published policy, or a consultant's template without adaptation to the organization's specific context. These policies are easy to spot because they reference organizational structures that do not exist ("the AI Ethics Board" when no such board has been established), tools that have not been deployed ("submit results through the model registry" when no model registry exists), or regulatory requirements that do not apply to the organization's jurisdiction. Every borrowed policy must be adapted. Every reference must be verified. Every requirement must be implementable within the organization's actual capabilities.

**The Static Policy** is reviewed and approved once, then never updated. By the time it is eighteen months old, the AI landscape has shifted underneath it. Models referenced in the policy are deprecated. Tools named in the procedures have been replaced. Regulatory requirements have evolved. The policy still says "GPT-4" when the organization is using GPT-5.2. The policy still references a risk classification methodology that the governance team replaced six months ago. Teams stop reading it because they know it is stale. Build the review cadence before you publish the first version.

Well-designed policies are necessary. They are not sufficient. A policy that no one enforces is a Shelf Policy with better formatting. The next subchapter addresses the mechanism that separates documentation from governance: how to enforce policies at scale through technical controls, process controls, and organizational accountability.