# 29.5.7 — Model Monitoring and Post-Deployment Oversight

The **set-and-forget anti-pattern** kills more AI systems than bad training data. A team builds a model, validates it, documents it, pushes it through every governance gate, and deploys it to production. Then they move on to the next project. Six months later, the model is still running. Its performance has drifted twelve points below the threshold that governance approved. The underlying API provider has swapped the base model twice. A new regulation has reclassified the use case from limited-risk to high-risk. The documentation still describes the system as it existed on launch day. Nobody noticed because nobody was watching — not from a technical monitoring perspective, which may have been active, but from a governance perspective. The system was compliant on the day it shipped. It has not been compliant for months.

Governance does not end at deployment. It shifts from prospective to continuous. Pre-deployment governance asks whether a system should go live. Post-deployment governance asks whether a system should stay live. The second question is harder because it must be asked repeatedly, across every system in your portfolio, for as long as each system operates. Organizations that invest heavily in deployment gates but neglect post-deployment oversight are building a governance program with a front door and no walls.

## What Governance Monitoring Tracks

Technical monitoring, covered in Section 17, tracks latency, error rates, throughput, and model performance metrics. Governance monitoring tracks a different set of signals — signals that determine whether a deployed system remains compliant, correctly classified, and within the boundaries that justified its deployment.

**Compliance status currency** is the first signal. When a system was deployed, it was classified under a specific regulatory regime and assessed against specific requirements. Governance monitoring tracks whether those requirements have changed. The EU AI Act's high-risk system requirements became enforceable in August 2026. A system deployed in March 2026 under a pre-enforcement compliance posture needs to be reassessed once enforcement begins. If your governance monitoring does not track regulatory change against deployed systems, you will discover compliance gaps through regulatory inquiry rather than through internal review.

**Risk classification currency** is the second signal. Risk classifications are not permanent. A customer service chatbot classified as limited-risk at deployment becomes high-risk when the organization expands it to handle insurance claims or medical appointment scheduling. A model classified as minimal-risk when it served internal users moves to a different tier when it starts serving external customers. Governance monitoring reviews whether the conditions that determined each system's risk tier still hold. When the conditions change, the classification must be updated and the system must be re-evaluated against the requirements of its new tier.

**Documentation freshness** is the third signal. The EU AI Act requires that technical documentation for high-risk systems be kept up to date throughout the system's lifecycle. Article 18 specifies that providers must retain documentation for ten years after the system has been placed on the market. Documentation that was accurate on deployment day and has not been updated since is a compliance liability. Governance monitoring tracks when documentation was last reviewed, whether it reflects the current state of the system, and whether required documentation elements — model cards, risk assessments, conformity declarations — are current.

**Performance against validated thresholds** bridges governance and technical monitoring. During pre-deployment validation, the governance process approved the system based on specific performance criteria: accuracy above a certain level, fairness metrics within defined bands, safety test pass rates above defined floors. Governance monitoring tracks whether the system continues to meet those thresholds. A system that was approved with 92 percent accuracy and now operates at 79 percent is not just a technical problem. It is a governance problem because the approval was conditioned on performance the system no longer delivers.

**Drift detection alerts** track whether the system's behavior has shifted from its validated baseline. Drift comes in multiple forms: data drift when the input distribution changes, concept drift when the relationship between inputs and outputs shifts, and model drift when the provider updates the underlying model. Any form of drift can invalidate the governance approval because the system being monitored is no longer the system that was validated.

**Incident reports** complete the monitoring picture. Every time a deployed system generates a safety incident, a compliance concern, a customer complaint, or an output that required human override, that event must flow back into the governance monitoring layer. Individual incidents are data points. Patterns of incidents are governance signals that may trigger re-validation, risk reclassification, or retirement.

## The Governance Monitoring Cadence

Effective governance monitoring operates at four cadences, each serving a different purpose.

**Daily automated checks** run without human intervention. These verify that monitoring infrastructure is active, that compliance evidence is being generated, that no critical alerts have been missed, and that systems classified as high-risk have active dashboards with current data. Daily checks are cheap and mechanical. Their purpose is to catch infrastructure failures — a monitoring pipeline that stopped running, an evidence generation job that failed, a dashboard that is showing stale data.

**Weekly summary reviews** are lightweight human reviews of the automated monitoring output. The governance team reviews the week's alerts, confirms that no system has crossed a threshold that requires action, and triages any ambiguous signals. Weekly reviews take thirty to sixty minutes for a portfolio of twenty to forty systems. They are not deep investigations. They are triage — sorting the week's signals into "no action needed," "investigate further," and "escalate immediately."

**Monthly governance reviews** go deeper. Each month, a subset of the portfolio receives a focused review that examines compliance status, documentation currency, performance trends, and any incidents from the prior month. The subset is selected based on risk tier — high-risk systems are reviewed monthly, medium-risk systems are reviewed every other month, and low-risk systems are reviewed quarterly. The monthly review produces a written assessment for each system reviewed, documenting its current governance posture and any required actions.

**Quarterly comprehensive assessments** cover the entire portfolio. Every system, regardless of risk tier, is assessed against its governance requirements. Quarterly assessments also review the governance monitoring process itself: are the automated checks catching what they should? Are the weekly reviews effective? Are monthly reviews producing actionable findings? The quarterly assessment is where the governance team steps back from individual systems and evaluates whether the monitoring program as a whole is functioning.

## When Monitoring Triggers Re-Validation

Not every monitoring alert requires action. But certain events require the system to go back through the validation process — a partial or full re-run of the pre-deployment governance review.

Performance below validated thresholds triggers re-validation when the drop is sustained rather than transient. A single day of degraded performance during an unusual traffic spike is a technical event. Three consecutive weeks of performance below the approved threshold is a governance event because the system no longer meets the conditions under which it was approved.

Drift beyond defined tolerances triggers re-validation because drift means the system's behavior has changed in ways that the original validation did not assess. The re-validation must evaluate whether the drifted system still meets compliance, fairness, and safety requirements — not just whether the performance metrics recovered.

Regulatory change triggers re-validation when the change affects the system's compliance posture. A new requirement, a new interpretation of an existing requirement, or a new enforcement action in the system's domain means the original compliance assessment may no longer be sufficient.

Provider model updates trigger re-validation for any system that depends on a third-party model accessed through an API. When the provider updates the underlying model — which major providers do regularly, sometimes without advance notice — the system you validated is no longer the system running in production. The re-validation must confirm that the updated model still meets the governance criteria that the original model satisfied.

## Monitoring Evidence as Compliance Artifacts

Every piece of governance monitoring output is a compliance artifact. The daily check logs prove that monitoring was active. The weekly review records prove that humans were reviewing the automated output. The monthly assessments prove that each system received focused governance attention. The quarterly reviews prove that the organization maintains a comprehensive, ongoing governance practice.

Under the EU AI Act, providers of high-risk AI systems must establish post-market monitoring systems and maintain post-market monitoring plans. Article 72 specifies that these systems must actively and systematically collect, document, and analyze relevant data on the performance of high-risk systems throughout their lifetime. Your governance monitoring cadence — daily, weekly, monthly, quarterly — produces exactly the evidence that this requirement demands.

The organizations that treat governance monitoring as overhead rather than evidence production are the organizations that scramble when an auditor asks to see their post-deployment oversight records. The organizations that design their monitoring to produce compliance artifacts from the start are the organizations that hand the auditor a structured evidence package and move on. The difference is not effort. The effort is roughly the same. The difference is intent — whether you monitor to check a box or monitor to produce proof.

## The Connection to Risk Acceptance Expiry

Governance monitoring connects directly to the Risk Acceptance With Expiry concept introduced in the previous chapter. When a VP accepted the residual risk of a deployment and that acceptance was given a ninety-day expiry, it is governance monitoring that provides the evidence for the renewal decision. Did the system perform within its validated thresholds during the acceptance period? Did the risk classification remain current? Did any incidents occur that change the risk profile? Without governance monitoring, risk acceptance renewal becomes a rubber stamp. With it, renewal becomes an informed decision based on evidence from the system's actual operating history.

A system that survives ongoing governance monitoring — that continues to meet its thresholds, that remains correctly classified, whose documentation stays current, and whose incidents remain within acceptable bounds — is a system that earns continued trust. A system that fails governance monitoring is a system that needs intervention. And a system that governance monitoring cannot evaluate — because the monitoring infrastructure broke, because nobody was reviewing the output, because the evidence pipeline stopped producing — is a system operating without oversight. In a regulated environment, that is not a monitoring gap. It is a compliance gap. The next subchapter addresses what happens at the end of a model's life — when monitoring reveals that a system should no longer be running at all.
