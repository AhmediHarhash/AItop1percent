# 29.7.7 — The AI Risk Register: Building and Maintaining a Living Risk Inventory

Why do most risk registers become shelfware? Because they are built to satisfy an audit, not to govern a system. A compliance team spends three weeks populating a spreadsheet during a regulatory review, labels every risk with a likelihood and impact score, emails it to stakeholders for sign-off, and files it in a shared drive. Twelve months later, the organization has deployed four new AI systems, deprecated two, and changed providers for a third. The risk register still describes the world as it existed the day it was written. It is not wrong in the way a broken tool is wrong. It is wrong in a more dangerous way — it creates confidence in coverage that does not exist.

The AI risk register is the single artifact that connects risk identification, assessment, treatment, and monitoring into one living system. When it works, it is the operational backbone of your governance program. When it fails, every other governance process — board reporting, incident management, regulatory response — is flying blind.

## What Belongs in an AI Risk Register

Every entry in your AI risk register should capture enough information that a reviewer who has never seen the risk before can understand it, assess it, and act on it. The minimum set of fields begins with a unique **risk identifier** that never changes, even if the risk description evolves. Next comes a plain-language risk description that explains what could go wrong and why, written for a non-technical reader. Each risk should map to a category from your organization's AI risk taxonomy, as described in subchapter 29.7.1 — bias, security, compliance, operational, financial, reputational, or whatever categories you have defined.

Beyond the basics, each entry needs an affected systems field listing every AI system or deployment that the risk applies to, because a single risk — say, training data containing personally identifiable information without valid consent — can apply to multiple systems simultaneously. The register should capture a likelihood assessment, an impact assessment, and a financial exposure estimate derived from the quantification methods covered in subchapter 29.7.3. These three fields together give stakeholders the ability to prioritize without additional analysis.

Every entry must also include a treatment plan describing what the organization is doing about the risk — accepting it, mitigating it, transferring it, or avoiding it — along with a treatment status that tracks whether the plan is not started, in progress, or complete. A residual risk level captures what remains after treatment, because a mitigated risk is not a zero risk. And two date fields — the last review date and the next scheduled review date — ensure that no entry is allowed to age without scrutiny.

## The Field Most Registers Miss: The Risk Owner

The most critical field in the register is the one most organizations get wrong: the **risk owner**. A risk owner is not a team. It is not a department. It is not "Engineering" or "Compliance" or "the AI governance committee." It is a named individual who is personally accountable for monitoring that risk, ensuring the treatment plan is executed, escalating when conditions change, and reporting on status when asked. If your risk register lists "Data Science Team" as the owner of a model bias risk, nobody actually owns it. A team cannot be paged at two in the morning when a monitoring alert fires. A department cannot explain to the board why a treatment plan stalled.

Assigning individual ownership does two things. First, it creates accountability that is impossible to diffuse. When the quarterly review reveals that a high-risk entry has not been updated in six months, the conversation is with a person, not an abstraction. Second, it creates an information channel. The risk owner is someone who understands the specific system, the specific threat, and the specific mitigation well enough to detect when conditions change. When a model is retrained on new data, the risk owner for training data provenance should be among the first to know — not because an automated alert told them, but because they are embedded in the workflow.

## Structuring the Register: By System, by Category, or Hybrid

Organizations organize their risk registers in one of three ways, and the choice matters more than it appears. A system-organized register groups all risks by AI deployment — all risks for the customer service chatbot are together, all risks for the underwriting model are together. This makes it easy for system owners to see their full risk profile but makes it hard to see cross-cutting risks like regulatory compliance gaps that affect every system simultaneously.

A category-organized register groups risks by type — all bias risks together, all security risks together, all compliance risks together. This is better for the governance team and for regulatory reporting but makes it harder for individual system owners to see their specific exposure. The hybrid approach, which is what most mature organizations adopt by 2026, maintains a single register with both dimensions as filterable views. Every risk entry has both a system tag and a category tag, and the register can be sliced either way depending on the audience. The board sees a category view that highlights the organization's top risks by type. The engineering team sees a system view that highlights their deployment's specific risk profile. The data is the same. The perspective shifts.

## Keeping the Register Alive

The difference between a living register and shelfware is operational integration. A living register is fed by automated signals, not manual updates. Your model monitoring system detects performance drift that exceeds a threshold — that signal should automatically update the likelihood assessment for the relevant risk entry. A regulatory body publishes new enforcement guidance — a compliance analyst should trigger reassessment of every entry tagged with the relevant regulatory category. A system is retrained, a new data source is added, a deployment is expanded to a new market — each of these events should trigger a review of the affected risk entries.

The cadence of human review matters as much as the automated feeds. High-risk entries — those with financial exposure estimates in the top quartile or residual risk levels rated as severe — should be reviewed quarterly at minimum. Every entry in the register, regardless of risk level, should be reviewed annually. The quarterly reviews are substantive: has the likelihood changed? Has the treatment plan made progress? Is the financial exposure estimate still accurate given what the organization has learned? The annual reviews are comprehensive: is this risk still relevant? Should it be retired because the system was decommissioned? Should it be upgraded because the threat landscape shifted?

Version control is non-negotiable. Every change to a risk entry — a likelihood update, a treatment status change, a new risk owner assignment — should be timestamped and attributed. When a regulator asks why a risk was downgraded from high to medium in October, you need to show who made that decision, what evidence supported it, and what approval process it went through. A register without version history is a register that cannot withstand regulatory scrutiny. In 2026, with the EU AI Act's enforcement mechanisms active and high-risk system obligations approaching their August compliance deadline, the ability to demonstrate a complete audit trail for every risk decision is not optional — it is a regulatory expectation.

## Integration with Incident Management

The most powerful signal your risk register can receive is an incident. Every AI incident — a model producing biased outputs, a data breach traced to a training pipeline, a regulatory complaint about an automated decision — should trace back to one or more entries in the risk register. The incident report should reference the risk identifier. The risk register entry should be updated to reflect that the theoretical risk has materialized, with the incident details linked for context.

This linkage works in both directions. When an incident occurs and maps cleanly to a register entry, it validates the risk identification process and provides real-world data to refine likelihood and impact estimates. When an incident occurs and does not map to any register entry, it reveals a gap — a risk that the organization failed to identify. That gap is itself valuable information. It means the risk taxonomy needs updating, the risk assessment process needs broadening, or the team needs additional domain expertise.

The inverse pattern is equally important. If a risk register entry has been rated as high-likelihood for two years but has zero linked incidents, one of two things is true. Either the treatment plan is working and the risk is effectively mitigated — in which case the residual risk level should be updated to reflect that success. Or the monitoring systems are not detecting the risk materializing — in which case the organization is exposed and does not know it. Both possibilities require investigation. An entry with no incidents is not necessarily good news.

Over time, this bidirectional linkage transforms your risk register from a theoretical inventory into an empirically calibrated one. Likelihood scores become grounded in actual incident frequency rather than expert estimation. Impact scores reflect real financial damage rather than hypothetical scenarios. The register matures from a document of assumptions into a document of evidence — and that evidence is what gives the register credibility with the board, with regulators, and with the engineering teams who need to trust it enough to use it.

## Common Failure Modes

Three failure modes destroy risk registers more often than any others. The first is **completeness theater** — the register has hundreds of entries, each with every field populated, but the entries are generic. "Risk: model may produce biased outputs. Likelihood: medium. Impact: medium. Treatment: monitor." This entry describes every AI system ever built. It provides no actionable information. It exists to make the register look thorough, not to govern risk. Every entry should be specific enough that a reader can identify the system, the scenario, the exposure, and the mitigation without needing to ask follow-up questions.

The second failure mode is **owner abandonment**. Risk owners are assigned at register creation but never held accountable for updates. Six months later, some owners have changed roles, some have left the organization, and some have forgotten they own risks at all. The fix is process: risk ownership must be part of the onboarding checklist for new role holders, the offboarding checklist for departing employees, and the quarterly review agenda.

The third failure mode is **register sprawl** — the organization maintains multiple risk registers across different teams, departments, or tools, with no central view. The compliance team has one register in their GRC platform. The engineering team has another in a project management tool. The data science team has a third in a shared spreadsheet. No single person can see the full picture. When the board asks "what are our top AI risks?" the answer requires manually reconciling three sources that use different taxonomies, different scoring methods, and different update frequencies. The fix is governance: one register, one taxonomy, one platform, multiple views.

## The Register as the Foundation of Risk Communication

Your risk register is not an internal compliance tool. It is the foundation of every risk conversation your organization has — with the board, with regulators, with auditors, with customers who ask about your AI governance practices. When the board asks about AI risk exposure, the answer comes from the register. When a regulator asks how you identified and treated a specific risk, the register entry — with its history of assessments, treatment plans, and linked incidents — is the primary evidence. When an auditor evaluates your governance maturity, the register's completeness, currency, and integration with operational systems is one of the first things they examine.

Building a risk register takes weeks. Maintaining it takes discipline. But the alternative — governing AI risk through memory, intuition, and ad hoc spreadsheets — is not governance at all. It is hope dressed in process language. The organizations that govern AI well in 2026 are not the ones with the most sophisticated risk models. They are the ones with the most honest, most current, and most operationally integrated risk registers — the ones where every entry represents a real risk, owned by a real person, connected to real monitoring, and reviewed on a real schedule.

The register captures risks as they are understood at a point in time. The next subchapter addresses what happens between those points — continuous risk monitoring that detects when the risk landscape shifts faster than any review cycle can capture.
