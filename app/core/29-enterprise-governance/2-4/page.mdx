# 29.2.4 — Cross-Functional AI Governance: Engineering, Product, Legal, Risk, and Compliance

The incident report listed the root cause as "model hallucination." The customer-facing financial advisor chatbot had fabricated three investment projections, complete with percentage returns and timeframes, and presented them as the firm's official guidance. A client acted on one of them and lost $220,000. The forensic review took eleven days, involved four outside law firms, and consumed the better part of a quarter's worth of executive attention. But when the dust settled and the leadership team sat in a windowless conference room staring at the timeline of decisions that led to the deployment, the real root cause was not technical. It was organizational. Five functions — Engineering, Product, Legal, Risk, and Compliance — had each assumed someone else was responsible for AI safety. Engineering assumed Product had validated the use case against regulatory requirements. Product assumed Legal had reviewed the output constraints. Legal assumed Risk had classified the deployment tier. Risk assumed Compliance had established monitoring controls. Compliance assumed Engineering had built guardrails into the model itself. Nobody had. The model hallucinated because it was a language model and that is what language models do when they are not constrained. The failure was that no function owned the constraint.

This pattern — the **diffusion of responsibility** — is the single most common structural failure in enterprise AI governance. It does not happen because people are careless. It happens because AI systems cross every functional boundary an organization has. A customer-facing chatbot is simultaneously an engineering system, a product feature, a legal exposure, a risk surface, and a compliance obligation. No single function has the expertise or the authority to govern it alone. And when no function is explicitly assigned specific governance responsibilities, the default is not shared ownership. The default is no ownership.

## Why AI Governance Cannot Live in Any Single Function

The temptation is strong to assign AI governance to whichever function screams loudest after the first incident. If the breach was a data leak, governance lands in Information Security. If the incident was a regulatory fine, governance lands in Compliance. If the failure was a biased output, governance lands in a hastily formed Ethics team. Each of these placements solves one problem while creating five others.

When governance lives solely in Engineering, it optimizes for technical controls — model validation, output filtering, latency budgets — but underweights regulatory exposure, contractual risk, and user impact. Engineers are superb at building guardrails. They are not trained to assess whether a model deployment in Germany requires a Data Protection Impact Assessment under GDPR or whether the output format creates an implied warranty under consumer protection law. When governance lives solely in Legal, every deployment becomes a contract review. Legal teams are thorough but slow, and they default to risk avoidance over risk management. A legal-owned governance function can paralyze an AI program, rejecting deployments that carry manageable risk because the legal framework has no mechanism for accepting risk — only for identifying it.

When governance lives solely in Compliance, it becomes a checkbox exercise. Compliance teams are built to demonstrate adherence to known requirements, not to evaluate novel risks from systems that behave probabilistically. The compliance function can tell you whether you filed the right form. It cannot tell you whether your model's behavior will drift in ways that violate the spirit of the regulation even though the form was filed correctly. When governance lives solely in Risk, it produces risk registers that nobody reads and heat maps that nobody acts on, because enterprise risk frameworks were designed for deterministic systems with predictable failure modes, not for probabilistic systems whose behavior changes with every input.

The only governance model that works is cross-functional governance where each function contributes what it is uniquely qualified to contribute, and no function is asked to cover ground it is not equipped to cover.

## What Each Function Contributes

Understanding the specific contribution of each function eliminates the ambiguity that creates responsibility gaps. This is not about vague collaboration. It is about precise, named responsibilities.

**Engineering** contributes technical controls and validation. This means model evaluation pipelines, output filtering, performance monitoring, drift detection, access controls, and the infrastructure that makes governance enforceable. Engineering does not decide whether a model should be deployed. Engineering ensures that if the decision is made to deploy, the technical controls are in place to make that deployment safe. Engineering also owns the technical evidence that other functions depend on — evaluation results, monitoring dashboards, incident response tooling.

**Product** contributes use-case risk assessment and user impact analysis. Product management understands who the user is, what they will do with the output, what the consequences are if the output is wrong, and how the feature is positioned in the market. A model that generates internal summaries for analysts carries different user risk than the same model generating advice for retail consumers. Product owns this distinction. Product also owns the decision about whether a use case should exist at all — whether the value proposition justifies the risk surface.

**Legal** contributes regulatory mapping, contractual review, and liability assessment. Legal determines which regulations apply to a specific deployment based on jurisdiction, data type, user population, and output type. Legal reviews vendor agreements to ensure that data processing terms cover the actual data flows. Legal assesses whether the model's outputs could create legal liability — implied advice, discriminatory impact, privacy violations. Legal does not make technical decisions. Legal tells you what the law requires and what the legal consequences are of different design choices.

**Risk** contributes enterprise risk integration. The risk function ensures that AI risks are not managed in isolation but are integrated into the organization's broader risk framework. This means AI risks appear in the same risk registers, use the same severity scales, and receive the same executive attention as cybersecurity risks, financial risks, and operational risks. Risk also owns the risk appetite framework — the organizational decision about how much residual risk the company is willing to accept across its AI portfolio.

**Compliance** contributes evidence generation, audit readiness, and regulatory reporting. Compliance ensures that the controls other functions build actually produce the evidence that regulators and auditors will demand. When Engineering builds a bias evaluation pipeline, Compliance ensures the results are stored in an auditable format with timestamps, versioning, and chain-of-custody. When Legal determines that a deployment requires a conformity assessment under the EU AI Act, Compliance manages the documentation process and ensures the assessment is complete before the deadline.

## The RACI Model for AI Governance Decisions

The **RACI model** — Responsible, Accountable, Consulted, Informed — is the standard tool for assigning decision roles, and it is particularly valuable for AI governance because AI decisions naturally span functions. Without RACI clarity, cross-functional governance degenerates into meetings where everyone discusses but nobody decides.

For a new AI deployment, the RACI might look like this in practice. Product is Responsible for the use-case risk assessment, defining who the users are, what impact a failure would have, and whether the use case should proceed. Engineering is Responsible for the technical evaluation — model performance, safety testing, infrastructure readiness. Legal is Consulted on regulatory requirements and contractual implications. Risk is Accountable for the overall risk classification — they own the final risk tier assignment that determines the governance path. Compliance is Informed of the deployment and Responsible for ensuring the governance evidence is complete and stored. The executive sponsor — typically a VP or SVP — is Accountable for the deployment decision itself, meaning they are the person who says "ship it" and accepts the consequences.

The critical discipline is that Accountable means exactly one person. Not a committee. Not a shared responsibility. One named individual who can be asked "did you approve this?" and whose answer is on the record. When Accountability is shared, it is owned by nobody. Forrester's research on AI governance RACI structures found that organizations with explicit single-point accountability for AI deployment decisions resolved governance conflicts forty percent faster than those using consensus-based approval.

## Preventing the "Everyone Is Responsible So Nobody Is" Failure Mode

The financial advisor chatbot incident at the beginning of this subchapter is the textbook example of what governance scholars call the **bystander effect** applied to organizational decision-making. When five functions each believe the others are covering a risk, the probability that any one function takes action drops with each additional function involved. The more people in the room, the less likely anyone is to act.

You prevent this with three structural mechanisms. First, explicit gap analysis at the design phase. Before any AI system enters development, the cross-functional governance team maps every governance requirement to a specific function and a specific person. Not a team — a person. The mapping is documented and signed. If any requirement has no named owner, development does not start. This feels bureaucratic. It takes about two hours. It prevents the two-month incident response that follows when a requirement falls through the gap.

Second, negative confirmation. Instead of asking each function "are you handling this?" — which invites the assumption that someone else will — you ask each function "confirm that this specific item is NOT your responsibility." When a function sees that nobody else is claiming a requirement, the gap becomes visible before it becomes dangerous. Negative confirmation forces each function to actively disclaim responsibility rather than passively assuming others have it covered.

Third, a governance coordinator role. This is not a decision-maker. It is a process owner — someone whose job is to ensure that the RACI is complete, that every requirement has a named owner, that handoffs between functions happen cleanly, and that nothing falls between the seams. In mature governance programs, this role sits in the AI governance team or the Chief AI Officer's organization. The coordinator does not need deep expertise in every function. They need deep expertise in the process of cross-functional coordination.

## Cross-Functional Governance Workflows

Abstract responsibility assignments only work when they are embedded in concrete workflows. Four recurring governance decisions require cross-functional coordination, and each one needs a defined sequence of actions.

For **new deployments**, the workflow starts with Product submitting a use-case risk assessment. Legal reviews the regulatory landscape and flags applicable requirements. Risk assigns a risk tier based on the use-case assessment and legal input. Engineering conducts technical evaluation against the requirements that Legal and Risk have defined. Compliance verifies that the evidence generated during evaluation meets audit standards. The deployment decision goes to the Accountable executive, who has a complete package: use-case assessment, regulatory requirements, risk tier, technical evaluation results, and compliance sign-off.

For **incidents**, the workflow starts with Engineering detecting and containing the technical issue. Product assesses user impact — how many users were affected, what harm occurred, what communication is needed. Legal evaluates legal exposure — potential regulatory notification requirements, contractual breach, litigation risk. Risk updates the risk register and assesses whether the incident changes the risk tier of the system. Compliance manages regulatory notifications if required and ensures the incident is documented in the audit trail. This sequence happens in hours, not weeks, because the roles and handoffs are predefined.

For **model changes** — upgrading a model version, switching providers, or retraining on new data — the workflow requires Engineering to evaluate the change against the same standards as the original deployment. Product confirms that the use-case risk assessment is still valid for the changed model. Legal confirms that contractual and regulatory requirements are still met. Risk determines whether the change alters the risk tier. If the risk tier changes, the deployment goes back through the full governance path. If it does not, an expedited review is sufficient.

For **vendor adoption** — adding a new model provider or AI tool — Legal leads with contract review and data processing terms. Risk assesses the vendor against the organization's third-party risk framework. Engineering evaluates technical integration, security posture, and performance. Product assesses whether the vendor's capabilities match the use-case requirements. Compliance ensures that the vendor relationship is documented in the vendor registry with all required evidence.

## Making Cross-Functional Governance Fast

The most common objection to cross-functional governance is that it slows everything down. Five functions reviewing every deployment means five calendars to coordinate, five approval queues, and five potential bottlenecks. This objection is valid if governance is implemented as sequential review gates where each function must complete its review before the next function starts. It is invalid if governance is implemented as parallel workflows where functions review simultaneously and escalate only when they find issues.

The organizations that make cross-functional governance work treat it as a product engineering problem, not a process compliance problem. They build shared platforms where all five functions can see the current state of a governance review in real time. They set service-level expectations — Legal has three business days to complete a regulatory review for standard-risk deployments, two business days for expedited. They create tiered paths where low-risk deployments require lighter cross-functional review and high-risk deployments get the full treatment. They measure governance throughput — average days from submission to deployment decision — and they hold governance functions accountable for speed as well as thoroughness.

A cross-functional governance model that adds thirty days to every deployment will be circumvented by the third deployment. A cross-functional governance model that adds three days to standard deployments and ten days to high-risk deployments will be adopted because three days is a reasonable price for a deployment that has been reviewed by every function that needs to review it. The goal is not to eliminate review time. The goal is to make review time proportionate to risk and fast enough that teams do not build workarounds.

The cross-functional model defines who contributes to governance decisions. But contribution is not the same as authority. The next subchapter addresses the question that cross-functional governance raises but does not answer: who has the right to make the final call — to ship, to block, to override, and to accept the risk that remains.
