# 29.9.2 -- Internal Audit for AI: Building the Function from Scratch

In early 2025, the internal audit team at a mid-sized insurance company received a board directive: audit the organization's AI systems. The team had deep experience in SOX compliance, IT general controls, and operational auditing. They approached AI the way they approached everything else -- pull the control documentation, sample transactions, test whether controls operated as designed. Six weeks later, they delivered a report that found all documented controls "operating effectively." Three months after that, a regulatory examination discovered that the company's claims-processing model had been approving claims at systematically different rates for policyholders in certain zip codes, a pattern strongly correlated with race. The internal audit had tested whether the model was deployed through the approved change management process. It had never tested whether the model's outputs were fair. Every control the auditors examined was, technically, operating. The system was still producing discriminatory outcomes.

This is the failure pattern that plays out when audit teams treat AI systems like traditional IT systems. The controls they know how to test are real controls, and testing them is valuable. But AI introduces an entirely new category of risk -- output risk -- that traditional IT audit methodology was never designed to assess. Building an internal audit function that can genuinely assure AI governance requires rethinking what you audit, how you audit it, and what skills your auditors need.

## Why Traditional IT Audit Methodology Fails for AI

Traditional IT audit methodology rests on a deterministic assumption: if the control is designed correctly and operates consistently, the outcome will be correct. Test the control, confirm it ran, verify the output matches expectations. This works for access management, change control, backup procedures, and data center operations because these systems produce predictable, repeatable results.

AI systems violate this assumption at every level. A model deployed through a perfectly executed change management process can still produce biased outputs. A data pipeline that passes every data quality check can still contain historical bias that the model learns and amplifies. An evaluation suite that covers every documented test case can still miss failure modes that only emerge in production with real-world input distributions. The controls are necessary -- you absolutely need change management, data validation, and evaluation gates. But passing those controls does not guarantee that the system behaves acceptably. The traditional audit approach of testing controls and inferring outcome quality from control quality produces false assurance for AI systems.

The fix is not to abandon control testing. It is to add output testing. Internal audit for AI must test not only whether the governance process was followed but whether the system's actual outputs meet the organization's stated standards for accuracy, fairness, safety, and reliability. This is a fundamental expansion of audit scope that requires new skills, new tools, and new relationships between the audit team and the technical teams that build and operate AI systems.

## The AI Audit Universe: What Internal Audit Should Cover

Your **AI audit universe** is the complete inventory of auditable topics related to AI governance. Defining it explicitly prevents the common failure of auditing only what your team already knows how to audit while ignoring the areas where risk is highest.

The universe spans seven domains. First, the **model inventory and risk classification** -- does the organization maintain a complete and current inventory of all AI systems, and is each system classified by risk tier using consistent criteria? Organizations that cannot produce an accurate model inventory on demand are not ready for any other audit. Second, **data governance** -- are training data sources documented, is data quality validated before use, is data lineage traceable, and are data retention and deletion policies enforced? Industry surveys consistently show that roughly seventy percent of enterprises report only moderate to limited visibility into data lineage, making this a high-likelihood finding area. Third, **model validation and evaluation** -- are models evaluated against documented acceptance criteria before deployment, are evaluation datasets representative, and are evaluation results retained as evidence? Fourth, **deployment and change controls** -- does deployment follow a documented approval process, are model versions tracked, and can the organization reproduce any deployed model state? Fifth, **production monitoring and drift detection** -- is model performance monitored in production, are drift detection mechanisms active, and do monitoring alerts trigger documented response procedures? Sixth, **vendor and third-party AI management** -- are third-party AI systems subject to the same governance requirements as internally developed systems, and are vendor contracts adequate? Seventh, **regulatory compliance** -- does the organization's AI governance satisfy applicable regulatory requirements, and can compliance be demonstrated with evidence?

Each domain should map to specific controls that can be tested. The model inventory domain might include controls like "all AI systems are registered within thirty days of deployment" and "risk classifications are reviewed annually." The audit tests whether those controls are operating -- and critically, whether the risk classifications are accurate, not just documented.

## Building the AI Audit Plan: Risk-Based Prioritization

You cannot audit everything every year. Risk-based prioritization determines which AI systems and governance domains receive audit attention in each cycle. The prioritization should weight three factors: the risk tier of the AI system, the maturity of the governance controls around it, and the time since last audit.

High-risk AI systems -- those making decisions with legal effects on individuals, operating in regulated domains, or processing sensitive data -- should be audited annually. Medium-risk systems should be audited every eighteen to twenty-four months. Low-risk systems can operate on a three-year audit cycle supplemented by automated monitoring. New AI systems should receive their first audit within six months of production deployment, before patterns and shortcuts have time to calcify.

Within each audit engagement, scope the work to cover both control testing and output testing. A well-structured AI audit engagement for a high-risk system typically takes three to four weeks and includes documentation review in the first week, control testing and interview sessions in the second week, output and performance testing in the third week, and report drafting and management response in the fourth week. For a medium-risk system, compress this to two weeks by reducing the depth of output testing. For a low-risk system, a focused one-week review of documentation and monitoring adequacy is sufficient.

Present the annual AI audit plan to the audit committee alongside your traditional audit plan. This signals that AI assurance is a permanent capability, not a one-time project. The plan should specify which systems will be audited, which domains each engagement will cover, the estimated hours, and any co-sourcing requirements for technical expertise the internal team does not yet possess.

## AI Audit Skills: What Your Auditors Need to Learn

Your auditors do not need to become machine learning engineers. They need to become informed questioners who can assess whether technical teams are following sound practices and whether governance controls are producing the intended outcomes. The skill set breaks into four areas.

The first area is **model fundamentals**. Auditors need to understand, at a conceptual level, how models are trained, what evaluation metrics mean, what overfitting and underfitting look like, what data drift is and why it matters, and what the difference is between a model performing well on test data and performing well in production. They do not need to read model code. They need to understand the lifecycle well enough to know where failures are most likely.

The second area is **bias and fairness assessment**. Auditors need to understand demographic parity, equalized odds, and other fairness metrics well enough to interpret evaluation results and ask whether the right metrics were chosen for the use case. They need to recognize when a model has not been tested for fairness across relevant demographic groups, and they need to understand that a model can be accurate overall while being systematically unfair to a subpopulation.

The third area is **data quality and lineage**. Auditors need to know what questions to ask about training data provenance, consent, representativeness, and labeling quality. They need to understand that data quality issues propagate through the model and into production outputs, making data governance a first-order audit concern rather than an IT housekeeping detail.

The fourth area is **prompt and output evaluation**. For systems using large language models, auditors need to understand how prompts shape outputs, what guardrails and safety filters are, and how to assess whether output quality monitoring is adequate. This is newer territory even for experienced ML teams, and auditors who can evaluate it effectively add significant value.

## The Three-Year Capability Building Roadmap

Building internal AI audit capability is a multi-year effort. Trying to go from zero to independent in a single year typically produces superficial audits that miss the highest-risk areas or, worse, deliver false assurance.

**Year one: awareness and foundation.** Train the full audit team on AI fundamentals -- a two-day workshop covering the model lifecycle, common risk areas, and the AI audit universe. Select one or two AI systems for your first audit engagements and co-source these with an external firm that has AI audit expertise. Your internal auditors participate in every phase, learning methodology through practice. The deliverables from year one are completed audit reports, a documented AI audit methodology tailored to your organization, and an honest assessment of the skills gaps that remain.

**Year two: guided independence.** Your internal team leads AI audit engagements with external specialists available for consultation on technical areas -- output testing, fairness assessment, data pipeline review. Expand coverage to four or five AI systems. Begin building repeatable test procedures for the most common control areas: model inventory accuracy, deployment approval compliance, monitoring effectiveness. Train one or two auditors as AI audit specialists with deeper technical skills through specialized courses like the IIA's AI auditing curriculum or ISACA's AI-focused certifications. By the end of year two, your team should be able to independently audit the governance and control domains, with external support needed primarily for deep technical output testing.

**Year three: independent capability.** Your internal team executes the full AI audit plan independently, bringing in external specialists only for the most technically complex engagements or when independence requirements demand it. The AI audit methodology is documented, tested, and integrated into the standard audit planning process. Your AI audit specialists can interpret evaluation results, assess fairness metrics, and evaluate data governance at a level that produces findings your technical teams take seriously.

This timeline assumes dedicated investment. If your audit function is already stretched thin with flat budgets and expanding mandates -- which describes forty-three percent of internal audit functions as of 2025 -- you will need to make explicit trade-offs about what existing audit coverage to reduce to make room for AI assurance. That is a conversation for the audit committee, not a decision the Chief Audit Executive should make alone.

## Common Audit Findings in AI Governance Programs

After enough AI audit engagements, patterns emerge. Knowing the most common findings helps auditors focus their testing and helps management anticipate remediation needs.

The most frequent finding is **incomplete or stale model inventories**. Organizations deploy AI systems faster than they register them. The inventory says there are twelve AI systems in production. The infrastructure team's deployment logs show nineteen. The seven unregistered systems were never risk-classified, never evaluated against governance standards, and are operating without the monitoring the governance framework requires.

The second most common finding is **undocumented risk acceptances**. The governance framework requires evaluation before deployment. A model was deployed without completing the full evaluation suite because the launch deadline was immovable. Someone decided the risk was acceptable, but nobody documented who made the decision, what risk was accepted, or when the decision should be revisited. This is the Risk Acceptance With Expiry problem from Chapter 7 -- risk acceptances that were never given an expiration date and have been silently accepted indefinitely.

Third, **evaluation data that does not represent production conditions**. The model was evaluated on a carefully curated test set that does not reflect the input distribution the model encounters in production. Accuracy on the test set is ninety-four percent. Accuracy on production inputs -- which include edge cases, adversarial inputs, and distribution shifts the test set never captured -- is closer to eighty-one percent. Nobody noticed because nobody compares evaluation performance to production performance systematically.

Fourth, **monitoring that exists but is not reviewed**. Dashboards are built, alerts are configured, but nobody is assigned to review them on a regular cadence. Drift alerts fired six times in the past quarter. None were investigated. The monitoring exists as a governance artifact but does not function as a governance control.

Fifth, **vendor AI systems operating outside the governance framework**. The organization has robust governance for internally developed AI, but the customer service chatbot from a SaaS vendor, the resume screening tool from an HR tech provider, and the document extraction model from a legal tech platform were all procured without AI governance review. They operate in production, making decisions that affect customers and employees, with no evaluation, no monitoring, and no risk classification.

## Co-Sourcing: Bridging the Gap with External Expertise

Co-sourcing -- engaging external specialists to work alongside your internal audit team -- is the most effective way to build capability while still delivering assurance. Unlike fully outsourcing AI audits, co-sourcing keeps your internal team involved in every phase, building knowledge transfer into the engagement model.

Structure co-sourcing engagements so that the external specialist leads the technical testing while your internal auditors lead the governance and control assessment. The external specialist evaluates model outputs, reviews fairness metrics, and assesses data pipeline integrity. Your internal team tests inventory completeness, deployment approval compliance, monitoring effectiveness, and documentation quality. Both teams contribute to the findings and the report. Over two or three engagements, your internal team absorbs enough technical methodology to reduce the scope of external support.

Choose co-sourcing partners who invest in knowledge transfer, not partners who treat your engagement as a black box that only they can open. The goal is to build your own capability, not to create a permanent dependency on external expertise. Every co-sourced engagement should produce a documented methodology artifact that your internal team can use in future engagements.

The internal audit function is one piece of the assurance puzzle. The next subchapter examines the other piece -- third-party audits that provide the independence your board, your customers, and your regulators increasingly demand.
