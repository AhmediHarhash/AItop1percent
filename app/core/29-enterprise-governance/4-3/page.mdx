# 29.4.3 — High-Risk AI System Requirements: The Compliance Checklist

The EU AI Act's requirements for high-risk AI systems are defined across Articles 9 through 15 and Article 17. Read as legal text, they are dense, cross-referential, and abstract. Read as engineering specifications, they are remarkably concrete. Each article describes a capability your system must have, a process your organization must run, or an artifact your teams must produce and maintain. This subchapter translates each requirement into operational terms — what your teams must actually build, document, and sustain — so that compliance becomes an engineering program rather than a legal exercise. If you have aligned your governance program with ISO 42001 or the NIST AI Risk Management Framework, you already have partial coverage of several requirements. The gap analysis at the end of this subchapter will help you identify what you have and what you still need.

## Article 9: Risk Management System

Article 9 requires a risk management system that operates as a continuous, iterative process throughout the entire lifecycle of the high-risk AI system. This is not a one-time risk assessment before deployment. It is a standing function that identifies risks, estimates them, evaluates them, and implements mitigation measures — and then does it again as the system, its context, and its user population evolve.

Operationally, your risk management system must do four things. First, identify and analyze the known and reasonably foreseeable risks that the system poses to health, safety, and fundamental rights. This includes risks during intended use and risks during conditions of reasonably foreseeable misuse. Second, estimate and evaluate the risks that may emerge when the system is used in accordance with its intended purpose and under conditions of reasonably foreseeable misuse. Third, evaluate other risks that may emerge based on post-market monitoring data. Fourth, adopt appropriate and targeted risk management measures to address identified risks.

The most common failure here is treating risk management as a document rather than a process. A risk register created before launch and never updated does not satisfy Article 9. The regulation explicitly requires that the risk management system be "planned and run throughout the entire lifecycle" with "regular systematic review and updating." Your compliance evidence must show not just the initial risk assessment but the cadence of reviews, the triggering events for reassessment, and the specific actions taken when new risks were identified. If your AI system has been in production for six months and your risk management documentation has not changed since launch day, that gap will be visible to any auditor.

The risk management measures themselves must follow a hierarchy. Risks must be eliminated through design and development choices where technically feasible. Where elimination is not feasible, adequate mitigation and control measures must be implemented. Where residual risks remain despite mitigation, the provider must supply information and, where appropriate, training to deployers. Throughout this hierarchy, the provider must give due consideration to the technical knowledge, experience, education, and training expected of the deployer, and to the reasonably foreseeable environment in which the system will be used.

## Article 10: Data and Data Governance

Article 10 establishes requirements for the data used in training, validation, and testing of high-risk AI systems. If your system uses any of these data processes — and virtually every machine learning system does — this article applies.

Training, validation, and testing datasets must be subject to data governance and management practices that address the relevant design choices, data collection processes, data preparation operations such as annotation and labeling, the formulation of relevant assumptions about the information the data is supposed to measure, an assessment of the availability and quantity of data needed, an examination of possible biases that are likely to affect the performance of the system, the identification of possible data gaps and how they can be addressed, and appropriate measures to detect and address biases. The datasets must be relevant, sufficiently representative, and to the best extent possible free of errors and complete in view of the intended purpose.

For engineering teams, this translates to documented data lineage, bias auditing processes, data quality metrics, and version-controlled dataset management. You must be able to demonstrate where your training data came from, how it was processed, what quality checks were applied, what biases were identified and how they were addressed, and what gaps exist and why they are acceptable given the system's intended purpose. If you use synthetic data, you must document the generation methodology and its potential limitations. If you use data from multiple sources, you must document how those sources were evaluated for consistency and representativeness.

The Article 10 requirement that datasets be "sufficiently representative" is one of the most operationally demanding provisions in the entire Act. Representative of what? Of the population the system will affect. If your high-risk AI system evaluates loan applications across the EU, your training data must be representative of the demographic, geographic, and economic diversity of EU loan applicants. If your system is deployed in Germany but trained primarily on UK data, that representativeness gap is a compliance vulnerability. Documenting how you evaluated representativeness, what gaps you found, and what measures you took to address them is not optional — it is the evidence that satisfies this requirement.

## Article 11: Technical Documentation

Article 11 requires providers to draw up technical documentation before the system is placed on the market or put into service, and to keep it up to date. The content of the documentation is specified in Annex IV and is extensive. It must include a general description of the AI system, a detailed description of the elements of the system and the process for its development, detailed information about the monitoring, functioning, and control of the system, a description of the appropriateness of the performance metrics, a detailed description of the risk management system, a description of the changes made to the system through its lifecycle, a list of the harmonized standards applied, a copy of the EU declaration of conformity, and a detailed description of the system for evaluating the AI system performance in the post-market phase.

This is not your internal engineering documentation. It is a compliance artifact designed to enable a regulator or notified body to assess your system's conformity with the Act. Think of it as a dossier that tells the complete story of your AI system — what it does, how it was built, what data it uses, what risks were identified, how those risks are managed, how performance is measured, and how the system is monitored after deployment. Annex IV runs to several pages of detailed requirements. Most organizations will need a dedicated documentation template that maps each Annex IV requirement to a section of the technical documentation, and a process that keeps the documentation current as the system evolves.

The documentation must be maintained for the entire period the system is on the market or in service. If your system operates for five years, the documentation must be maintained for five years — updated each time a significant change is made to the system, its data, or its deployment context. This has implications for your documentation infrastructure. Manual documentation processes that worked for a single system will not scale to a portfolio of twenty high-risk systems, each requiring continuous updates.

## Article 12: Record-Keeping and Automatic Logging

Article 12 requires that high-risk AI systems be designed and developed with capabilities enabling the automatic logging of events over the duration of the system's lifetime. The logging capabilities must enable the tracing of the system's operation throughout its lifecycle and must, at minimum, include the recording of the period of each use, the reference database against which input data has been checked, the input data for which the search has led to a match, and the identification of the natural persons involved in the verification of the results.

For engineering teams, this means building structured, immutable, timestamped logging into the system from the design phase — not retrofitting it after deployment. The logs must be retained for a period appropriate to the intended purpose and applicable legal obligations. The deployer must keep logs automatically generated by the system for a minimum of six months unless otherwise provided by applicable law.

The practical challenge is volume. A high-risk AI system that processes thousands of requests per day generates enormous log volumes. Your logging infrastructure must handle this volume without degrading system performance, must store logs in a format that is queryable by compliance and audit functions, and must protect the logs against tampering. If a regulator asks you to produce the logs for a specific decision the system made eight months ago, you must be able to retrieve them. This is not a theoretical scenario — it is exactly the kind of request a national supervisory authority will make when investigating a complaint.

## Article 13: Transparency and Provision of Information to Deployers

Article 13 requires that high-risk AI systems be designed and developed to ensure that their operation is sufficiently transparent to enable deployers to interpret the system's output and use it appropriately. Providers must accompany the system with instructions for use that include the provider's identity and contact details, the system's characteristics, capabilities, and limitations of performance, the changes to the system that have been pre-determined by the provider, the human oversight measures, the computational and hardware resources needed, the expected lifetime of the system and necessary maintenance, and the levels of accuracy, robustness, and cybersecurity against which the system has been tested and validated.

This requirement means you cannot ship a high-risk AI system as a black box. The deployer must receive enough information to understand what the system does, what it does not do, where it performs well, where it performs poorly, and how to monitor its operation. If your system's accuracy degrades on specific demographic groups, specific input types, or specific deployment contexts, the instructions for use must disclose those limitations. If the system requires specific computational resources to maintain its validated performance levels, that must be documented. The standard here is not "enough information for a data scientist to understand the model architecture." It is enough information for the deployer — who may not have a data science team — to use the system safely and in compliance with the Act.

## Article 14: Human Oversight

Article 14 requires that high-risk AI systems be designed and developed so that they can be effectively overseen by natural persons during their period of use. Human oversight must aim to prevent or minimize the risks to health, safety, or fundamental rights that may emerge when a high-risk system is used in accordance with its intended purpose or under conditions of reasonably foreseeable misuse.

The Act specifies that human oversight measures must enable the individuals performing the oversight to correctly understand the relevant capacities and limitations of the system, be able to duly monitor the system's operation, remain aware of the possible tendency of automatically relying on or over-relying on the output of the system — known as automation bias — and be able to decide not to use the system in any particular situation, override or reverse the output, and intervene in or interrupt the system's operation. For certain high-risk systems, the oversight must include the ability for two or more natural persons to independently verify the system's output before it takes effect.

Operationally, this means your system needs a human-in-the-loop or human-on-the-loop mechanism that is not just architectural window dressing. The oversight mechanism must work in practice: the human overseer must have the information, the tools, and the authority to intervene. If your system processes five thousand decisions per hour and the "human oversight" consists of a single analyst reviewing a summary dashboard once per day, that is not effective oversight — it is a compliance fiction. The overseer must be able to understand individual decisions, identify problematic ones, and take action before harm occurs. The design of the oversight mechanism must account for the volume, speed, and complexity of the system's operations.

## Article 15: Accuracy, Robustness, and Cybersecurity

Article 15 requires that high-risk AI systems achieve an appropriate level of accuracy, robustness, and cybersecurity, and perform consistently in those respects throughout their lifecycle. The accuracy levels and metrics must be declared in the instructions for use. The system must be resilient to errors, faults, or inconsistencies that may occur within the system or its operating environment. Technical solutions to address AI-specific vulnerabilities must include measures to prevent and control for attacks that attempt to manipulate the training dataset — data poisoning — attacks that attempt to manipulate inputs designed to cause the model to make errors — adversarial examples — and attacks that exploit model or hardware vulnerabilities.

This article transforms your model monitoring, adversarial testing, and security practices from engineering best practices into legal obligations. You must define accuracy benchmarks before deployment, monitor against those benchmarks in production, and document any degradation and the corrective actions taken. You must conduct robustness testing — not just standard test-set evaluation but evaluation under adversarial conditions, distribution shift, and edge cases. And you must implement cybersecurity protections specific to AI vulnerabilities, which go beyond traditional application security to include protections against prompt injection, training data poisoning, model extraction, and membership inference attacks.

## Article 17: Quality Management System

Article 17 requires providers of high-risk AI systems to put a quality management system in place. The QMS must include a strategy for regulatory compliance, techniques and procedures for the design and development of the system, techniques and procedures for testing and validation, technical specifications including standards applied, systems and procedures for data management, the risk management process, post-market monitoring procedures, procedures for reporting serious incidents, communication with national competent authorities and other relevant parties, systems and procedures for record-keeping, resource management including supply chain measures, and an accountability framework.

The QMS requirement ties all the other articles together into a managed system. It is not enough to have a risk management process, a data governance process, a documentation process, and a monitoring process. You must have a quality management system that governs all of them — ensuring they are documented, followed, reviewed, and continuously improved. Organizations that hold ISO 9001 or ISO 42001 certifications already have the management system infrastructure that this article demands. The specific AI requirements from Articles 9 through 15 plug into that management system as domain-specific controls.

## Conformity Assessment, CE Marking, and Registration

Before placing a high-risk AI system on the EU market, providers must conduct a conformity assessment to verify that the system meets all applicable requirements. For most Annex III high-risk systems, the provider can conduct this assessment internally — the Act calls this a self-assessment based on internal control, defined in Annex VI. For certain systems, particularly biometric identification systems, a third-party assessment by a notified body is required.

After passing the conformity assessment, the provider must draw up an EU declaration of conformity and affix the **CE marking** to the system. The CE marking is the visible signal that the system has been assessed and found compliant. Providers must also register the system in the EU database before placing it on the market — a publicly accessible database that enhances transparency and enables regulatory oversight.

## The Gap Analysis: Where Existing Frameworks Help

If your organization has implemented ISO 42001, you have a management system that covers many of the Act's structural requirements — risk management processes, documentation practices, monitoring, and continual improvement. The gap will be in the AI-specific details: the Annex IV documentation requirements are more prescriptive than ISO 42001's general provisions, the logging requirements in Article 12 demand specific technical capabilities, and the conformity assessment process follows EU-specific procedures.

If your organization follows the NIST AI Risk Management Framework, you have a conceptual architecture that maps well to Articles 9 through 15. The Govern function maps to quality management and accountability. The Map function maps to risk identification and data governance. The Measure function maps to accuracy, robustness, and monitoring. The Manage function maps to mitigation and incident response. The gap will be in the compliance formalization: NIST AI RMF is voluntary and flexible, while the Act demands specific documented outputs in specific formats.

Organizations with neither framework face the largest gap but also the clearest path: build toward the Act's specific requirements, and the resulting system will naturally align with both ISO 42001 and NIST AI RMF. The Act's requirements are the most prescriptive of the three, so building for Act compliance inherently covers much of what the frameworks require.

The next subchapter covers the GPAI obligations and the Code of Practice — the requirements that apply not to deployers of high-risk systems but to providers of the general-purpose AI models that increasingly power them.
