# 29.5.6 — Third-Party and Vendor Model Governance: Managing Models You Did Not Build

The production dashboard looks normal at 8:47 AM. By 9:15, the support queue has tripled. Customers are reporting that the automated claims assistant is giving contradictory answers — approving coverage for scenarios it denied yesterday, citing policy clauses that do not exist. The engineering team checks the system. Nothing has changed on their side. No deployment, no configuration update, no data pipeline failure. Then someone notices the API response headers: the model version identifier is different from yesterday. The vendor updated the underlying model overnight. No notification. No changelog. No migration window. The model powering your most customer-facing system changed while your team slept, and the first people to notice were your customers.

This is not a hypothetical. OpenAI retired the GPT-5 and GPT-5.1 series in early 2026, replacing them with GPT-5.2 across all API endpoints, including agents that had been built against the earlier model versions. Organizations that had validated their systems against GPT-5.1 behavior woke up to a different model without having approved the change. For teams with strong governance, this was a disruption. For teams without it, this was a crisis they discovered only through user complaints.

## The Fundamental Governance Gap: You Do Not Control the Model

Most enterprises in 2026 run AI systems powered by models they did not build. GPT-5.2 from OpenAI, Claude Opus 4.6 from Anthropic, Gemini 3 from Google, Llama 4 Maverick self-hosted from Meta — these are the engines behind customer service bots, document processing pipelines, coding assistants, and analytical tools across thousands of organizations. The convenience is real. The governance challenge is equally real.

When you build a model internally, you control everything: the training data, the architecture, the fine-tuning process, the deployment schedule, and the decision of when and whether to update. When you use a vendor model, you control almost none of this. You cannot inspect the model's weights. You cannot audit its training data. You cannot prevent the vendor from updating the model. You cannot verify the vendor's claims about the model's capabilities except through your own testing. Your documentation of the model depends entirely on what the vendor chooses to disclose. This is the **vendor model governance gap**, and closing it requires a fundamentally different approach than governing models you built yourself.

The governance gap is not just operational — it is legal. Under the EU AI Act, the **deployer** of a high-risk AI system bears compliance obligations regardless of whether the deployer built the model. If you deploy a vendor model in a high-risk context — hiring, credit, insurance, medical triage — you are responsible for ensuring the system meets the Act's requirements for risk management, documentation, human oversight, and transparency. The vendor provides the model. You provide the compliance. If the vendor's documentation does not meet the standard required by the Act, you must fill the gap yourself or stop using the model in that context. Article 26 makes this explicit: deployers must verify that the provider has complied with their obligations and must implement appropriate human oversight measures based on the provider's instructions for use. If those instructions are insufficient, the deployer's obligation does not disappear — it intensifies.

## Vendor Model Updates: The Silent Governance Breach

The most acute governance challenge with vendor models is uncontrolled updates. When a vendor updates a model, the behavior of every system built on that model changes — and the change happens without your approval, without your testing, and often without your knowledge.

This is not a theoretical risk. Model updates routinely change output formatting, alter the balance between conciseness and verbosity, shift the model's tendency to refuse certain requests, change the model's performance on specific task types, and modify the model's calibration on confidence expressions. Any of these changes can break downstream systems that depend on specific model behaviors. A document extraction pipeline that parses structured outputs may fail when the model's output format shifts. A medical triage chatbot that was validated to refuse to provide diagnoses may start offering them after an update changes the model's refusal calibration. A legal research assistant that was tested to cite real cases may begin hallucinating case citations at a higher rate under a new model version.

Your governance framework must treat every vendor model update as a potential revalidation trigger. This means three things. First, you must have monitoring in place that detects when a vendor model's behavior has changed — not just when the vendor announces a change, because vendors do not always announce changes promptly or comprehensively. Behavioral monitoring tracks output characteristics over time and alerts when statistical properties of the outputs shift beyond defined thresholds. Second, you must have a revalidation process that can be triggered quickly when a change is detected. Your critical evaluation suite — the tests that validate the model meets your quality, safety, and compliance standards — must be runnable on demand, not just during the initial deployment pipeline. Third, you must have a rollback or fallback plan that allows you to maintain service while revalidation is underway. For API-based models, this may mean pinning to a specific model version where the vendor supports version pinning, or maintaining a secondary provider that can absorb traffic if the primary provider's model degrades. For self-hosted models, this means maintaining the previously validated model version and the ability to revert within hours.

## Documentation Gaps and the Deployer's Dilemma

Vendor model documentation is improving but remains far below what regulators require from deployers of high-risk AI systems. Model providers typically publish model cards that describe the model's general capabilities, known limitations, intended use cases, and high-level performance benchmarks. Some providers publish system cards with additional detail on safety evaluations and red-teaming results. These documents are useful. They are not sufficient.

The EU AI Act requires technical documentation covering the system's design, development process, training data characteristics, testing methodology and results, risk management measures, and post-market monitoring plans. For a model you built, you produce this documentation yourself. For a vendor model, you depend on the vendor — and what the vendor provides rarely covers the full scope of what the Act requires. Model cards do not typically disclose training data composition at the level of detail Article 10 demands. Performance benchmarks in model cards are measured on the vendor's evaluation sets, not on your specific use case, user population, or deployment context. Safety evaluations in system cards describe the vendor's red-teaming methodology but not the specific risks relevant to your industry, your regulatory environment, or your customer base.

This creates the **deployer's dilemma**: you are legally responsible for documentation and compliance regarding a model you cannot fully inspect and a training process you did not control. The practical resolution is to layer your own documentation on top of the vendor's. The vendor's model card becomes one input into your system-level documentation, which also includes your own use-case-specific risk assessment, your own evaluation results on representative data from your domain, your own fairness and bias analysis for your specific user population, your own adversarial testing results, and your own monitoring plan. You document not just the model but the full system — the model plus your prompts, your retrieval pipeline, your guardrails, your human oversight mechanisms, and your output validation logic. The system is what you deploy. The system is what you must document.

## Contractual Requirements: What to Demand from Model Providers

Your vendor agreements must include governance provisions that go beyond standard software licensing. Model providers are not traditional software vendors. Their product changes behavior over time, their product's quality varies across use cases, and their product's risk profile is shaped by how you use it. Your contracts must reflect this reality.

At minimum, your vendor agreements should include five categories of requirements. First, **change notification**: the vendor must notify you in advance of model updates, deprecations, and behavioral changes, with enough lead time for you to evaluate the impact and revalidate if necessary. Thirty days is a reasonable minimum for planned changes. Second, **documentation commitments**: the vendor must provide and maintain model documentation at a level of detail sufficient for your compliance obligations, including training data characteristics, evaluation methodology, known limitations, and performance benchmarks that are updated when the model changes. Third, **performance guarantees**: the vendor must commit to specific performance characteristics — latency, availability, throughput — and define the remedy when those guarantees are not met. For quality guarantees, which are harder to contractualize, the agreement should at minimum require the vendor to maintain the model's performance on defined benchmark tasks. Fourth, **incident reporting**: the vendor must notify you within a defined timeframe of any security incident, data breach, or discovered vulnerability that affects the model or the data it was trained on. Fifth, **data handling commitments**: the vendor must specify how your input data and output data are handled — whether inputs are used for training, how long data is retained, where data is processed geographically, and what access controls apply.

Getting these terms requires negotiation leverage, and not every organization has it. Large enterprises with significant API spend can negotiate custom terms. Smaller organizations may need to rely on the vendor's standard terms and compensate for gaps with their own monitoring and documentation. Regardless of what you can negotiate, you must document what the vendor has committed to and what gaps remain. Those gaps become risks in your model risk assessment — risks that you manage through your own controls rather than through vendor commitments.

## Vendor Risk Assessment: Evaluating Model Providers as Third-Party Risks

Your model providers are third-party risks, and your governance framework must evaluate them as such. A vendor risk assessment for an AI model provider should cover dimensions that go beyond what a standard software vendor assessment includes.

Evaluate the vendor's **governance maturity**: does the provider have a published responsible AI framework? Do they conduct and publish safety evaluations? Do they have a dedicated trust and safety team? Do they have an incident response process for model-related issues? A provider with no public safety evaluation and no published approach to responsible AI is a governance risk, regardless of how good the model's benchmark scores look.

Evaluate the vendor's **transparency**: does the provider disclose training data sources or categories? Do they publish model cards or system cards? Do they provide evaluation results on standardized benchmarks? Do they disclose known limitations? Greater transparency reduces your documentation gap. Opacity increases it.

Evaluate the vendor's **stability and reliability**: how often does the provider make breaking changes? What is their track record on uptime and latency commitments? Do they provide version pinning so you can control when you adopt updates? A provider that updates models frequently without version pinning and without advance notification forces you to run continuous revalidation — which is operationally expensive and governance-intensive.

Evaluate the vendor's **regulatory posture**: is the provider preparing for EU AI Act compliance? Do they classify themselves as a GPAI provider and comply with the corresponding obligations? Will they provide the documentation that the Act requires providers to share with downstream deployers? A provider that ignores regulatory requirements shifts the compliance burden entirely to you.

## Multi-Vendor Strategy and Governance Portability

Vendor lock-in is a governance risk, not just a technical risk. If your entire AI infrastructure depends on a single model provider, a provider outage becomes your outage. A provider policy change becomes your emergency. A provider price increase is non-negotiable. And a provider model degradation has no immediate mitigation path.

A **multi-vendor strategy** means maintaining validated integrations with at least two model providers for your critical systems. This does not mean using different providers for different systems — it means having a tested, validated alternative for the same system. If your customer service bot runs on Claude Opus 4.6, you should have a validated configuration using GPT-5.2 or Gemini 3 Pro that can be activated within hours. The alternative does not need to be identical in performance. It needs to be adequate — meeting your minimum quality and safety thresholds — so that you can maintain service while addressing an issue with the primary provider.

From a governance perspective, multi-vendor strategy requires governance portability. Your validation criteria, your evaluation suites, your monitoring dashboards, and your documentation templates must be provider-agnostic. They describe what the system must do, not which model powers it. When you switch providers, you run the same validation suite against the new provider and produce the same documentation artifacts. The governance process does not change. Only the model underneath changes.

The investment in governance portability pays dividends beyond vendor risk mitigation. It makes model upgrades faster because the validation process is standardized. It makes cost optimization easier because you can evaluate alternative providers without building a new governance process for each one. And it provides leverage in vendor negotiations because the provider knows you have a viable alternative. Organizations that build vendor-specific governance processes pay the cost of that specificity every time they need to change providers. Organizations that build portable governance processes pay the cost once and benefit repeatedly.

## The Shared Responsibility Model

Vendor model governance ultimately operates under a shared responsibility model. The vendor is responsible for the model — its training, its safety evaluation, its documentation, and its ongoing maintenance. You are responsible for the system — the application you built on top of the model, the context in which it operates, the users it serves, and the compliance obligations it must meet. The vendor cannot comply on your behalf. You cannot build on your behalf what the vendor must provide.

The governance challenge is managing the boundary between these responsibilities. Where the vendor's responsibility ends and yours begins must be documented clearly in your risk assessment, your compliance documentation, and your vendor agreements. When something goes wrong — and it will — the documented boundary determines whether the investigation focuses on the vendor's model behavior or your system's design, configuration, and monitoring. Without that boundary, every incident becomes a finger-pointing exercise. With it, every incident has a clear investigation path.

The next subchapter covers what happens after deployment — model monitoring, post-deployment oversight, and the governance mechanisms that catch problems your pre-deployment validation did not anticipate.
