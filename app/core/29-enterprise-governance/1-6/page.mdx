# 29.1.6 — The AI Decision Stack: How AI Proposals Move from Idea to Deployment

A product manager has an idea for an AI feature. A support lead wants to automate ticket routing. A data scientist proposes a model that could predict customer churn and trigger retention offers. What happens next?

In most organizations, what happens next is chaos. The product manager Slacks their engineering lead, who says "sounds cool, let's prototype it." Someone starts building. Weeks later, the prototype works, and now the question of deployment arises. Who decides if this goes to production? Nobody is sure. Does legal need to see it? Probably, but nobody wants to wait. Does it need a bias evaluation? The team isn't sure what counts as "bias evaluation" for a churn prediction model. Is there a security review? There is for software deployments, but nobody knows if AI has a separate track. The prototype sits in limbo. The product manager escalates to their VP, who escalates to another VP, who asks "why isn't this live yet?" and the answer — the honest answer — is that nobody knows what path this idea is supposed to follow from concept to production.

This is not a people problem. It is a structural problem. The organization has no defined mechanism for how AI proposals move through evaluation, approval, and deployment. Every proposal invents its own path. Every path is different. And the result is exactly what you would expect: some systems reach production with no review at all because a determined team routed around every checkpoint, while other systems die in approval limbo because a cautious team couldn't figure out who had the authority to say yes.

## Defining the AI Decision Stack

**The AI Decision Stack** is the defined process by which an AI proposal moves from initial idea through risk classification, technical validation, legal and compliance review, business sign-off, deployment authorization, and post-deployment monitoring. Each layer in the stack has defined inputs, defined outputs, defined decision rights, and defined escalation paths. The stack is not a suggestion. It is the structural backbone of operational governance — the mechanism that turns governance policy into governance practice.

The stack has seven layers. Not every proposal passes through every layer with the same depth. A low-risk deployment that fits a pre-approved pattern may pass through several layers in hours. A high-risk deployment that affects regulated decisions may spend weeks in specific layers. But every proposal enters the stack at the top and exits at the bottom, and the evidence produced at each layer creates the audit trail that governance requires. The stack is the same for a two-person startup scaling its first model and a fifty-thousand-person enterprise managing hundreds of AI systems. The difference is the depth of review at each layer, not the structure itself.

## Layer One: Idea Intake and Use-Case Registration

Every AI proposal begins with registration. This is not bureaucracy. It is the moment the organization gains visibility into what is being built. Without registration, AI systems appear in production with no prior knowledge, no risk classification, and no ownership record. With registration, every proposal is captured in a system of record before a single line of code is written.

The intake captures a small set of essential information. What is the proposed use case? What problem does it solve? Who are the intended users? What data will the system process? Will it make or influence decisions that affect people — hiring, lending, healthcare, pricing? Who is proposing it, and which team will build and maintain it? The answers do not need to be detailed engineering specifications. They need to be honest descriptions of intent, sufficient for the next layer to classify risk accurately.

Registration also serves as the organization's AI inventory. Every system that passes through intake is cataloged. Over time, this inventory becomes one of the most valuable governance assets the organization owns, because it answers the question regulators, auditors, and board members will inevitably ask: what AI systems are you running, and where? Organizations without intake processes discover their AI inventory only during crises or compliance audits, and the discovery process is always more expensive and more embarrassing than registration would have been.

## Layer Two: Risk Classification

Once registered, the proposal is classified by risk tier. Risk classification determines how much review the proposal receives at every subsequent layer, so getting it right here is critical.

Most organizations use three or four tiers. The exact labels vary, but the logic is consistent. The lowest tier covers AI systems that process non-sensitive data, do not make or influence consequential decisions, and operate within pre-approved patterns. Summarizing internal meeting notes, classifying internal support tickets, suggesting code completions for developers — these are typically lowest-tier use cases. The highest tier covers AI systems that process sensitive or regulated data, make or influence decisions with legal, financial, or health consequences for individuals, or operate in domains where errors create significant harm. Loan approval recommendations, clinical decision support, hiring screening, automated content moderation at scale — these are highest-tier use cases.

Classification is not a subjective judgment call. It is driven by a rubric with specific criteria: what data is processed, what decisions are influenced, who is affected, what the consequences of errors are, and what regulatory requirements apply. The rubric should be concrete enough that two independent reviewers classifying the same proposal would arrive at the same tier at least ninety percent of the time. If classification depends on who is doing the classifying, the rubric needs refinement.

The EU AI Act provides a useful external framework, classifying AI systems as minimal, limited, high, or unacceptable risk based on their use case and impact domain. Organizations operating in or selling to EU markets need their internal classification system to align with the Act's categories, particularly because the August 2026 enforcement deadline for high-risk systems requires documented evidence of risk classification. But even organizations outside the EU benefit from a structured classification approach, because it turns "how carefully should we review this?" from a political question into a procedural one.

## Layer Three: Technical Validation

Technical validation answers a specific question: does this system meet the organization's quality and safety bars for its risk tier? The evidence required scales with the tier. A lowest-tier system might need only a basic accuracy evaluation on a representative test set. A highest-tier system might need bias evaluation across protected attributes, adversarial robustness testing, performance evaluation across demographic subgroups, and a red-team exercise.

The key principle of this layer is that validation criteria are defined before the team starts building, not negotiated after. Each risk tier has a documented set of quality gates: what evaluations must pass, what thresholds must be met, what evidence must be produced. The team knows from the moment of risk classification exactly what they need to demonstrate. This is another instance of governance as acceleration: when the team knows the bar before they start building, they can design their system to meet it. When the bar is negotiated after the fact, every launch becomes a debate about whether the evidence is sufficient.

Technical validation also covers model-specific concerns that traditional software review misses. What happens when the model receives out-of-distribution inputs? What is the system's behavior when the underlying model is updated by the provider? Does the system have appropriate fallback behavior when the model fails or returns low-confidence outputs? What monitoring will detect degradation after deployment? These are questions that a standard software readiness review does not ask, and they are the questions that distinguish AI governance from general IT governance.

## Layer Four: Legal and Compliance Review

Legal and compliance review determines what regulatory, contractual, and legal obligations apply to the proposed system and whether the system's design satisfies them. This is not a rubber stamp. It is a substantive review that examines data processing agreements, regulatory classification, intellectual property considerations, liability allocation, and disclosure requirements.

For a system processing EU personal data, this layer evaluates GDPR obligations: lawful basis for processing, data minimization, right to explanation for automated decisions, data protection impact assessment requirements. For a system used in healthcare, it evaluates HIPAA compliance and any FDA implications. For a system used in financial services, it evaluates fair lending requirements, model risk management expectations from regulators, and SOX implications if the system influences financial reporting. For a system used in employment decisions, it evaluates the growing patchwork of state and local AI-in-hiring laws, several of which now require bias audits and candidate notification.

The output of this layer is not a yes-or-no decision. It is a set of conditions: the system may proceed if these specific controls are implemented, these disclosures are made, and these monitoring requirements are met. These conditions flow forward into the deployment plan and become part of the ongoing governance requirements for the system.

The legal review layer is where many organizations experience the most friction, and the friction is almost always caused by the same problem: legal was not involved until the system was ready to ship. When legal reviews a finished system, every finding becomes a potential redesign. When legal reviews a proposal at the classification stage, findings shape design before code is written. The AI Decision Stack deliberately places legal review before deployment authorization specifically to surface constraints early, when they are cheap to address, rather than late, when they require rework.

## Layer Five: Business Sign-Off

Technical validation confirms the system meets quality bars. Legal review confirms it meets regulatory requirements. Business sign-off answers a different question: who accepts the residual risk?

Every AI system carries residual risk — the risk that remains after all controls are in place. A loan recommendation model might pass bias evaluation and legal review, but it still carries the risk that it will make a recommendation that harms a customer in a way nobody anticipated. A clinical summarization tool might pass accuracy evaluation, but it still carries the risk that a clinician relies on a summary that omits a critical finding. Controls reduce risk. They do not eliminate it. Someone in the organization needs to consciously accept the remaining risk, and that someone needs to have the authority to do so.

Business sign-off assigns accountability. For highest-tier systems, sign-off typically requires a senior business leader — a VP or above — who understands both the system's value and its risk profile. For lower-tier systems, sign-off may sit with a product or engineering lead. The critical principle is that the person signing off is not the person who built the system. Builders are optimistic about their creations. Governance requires a separate perspective. The signer is accepting risk on behalf of the organization, and they need to see the full picture: what the system does, what could go wrong, what controls exist, and what residual risk remains.

## Layer Six: Deployment Authorization

Deployment authorization is the formal go or no-go decision. It is not a new review. It is a verification that all prior layers have been completed satisfactorily. The deployment authority — often a governance team member or a designated release authority — confirms that intake is complete, risk classification is documented, technical validation evidence meets the tier's requirements, legal conditions have been incorporated, and business sign-off is recorded. If all evidence is present and all conditions are met, the system is authorized for production deployment.

This layer exists to prevent the gap between individual approvals and actual deployment. It is common for a system to receive legal approval conditionally, for the conditions to be partially addressed, and for the system to go live anyway because nobody checked whether all conditions were satisfied. Deployment authorization is the checkpoint that closes this gap. It is the final gate, and its sole purpose is to verify completeness.

For low-risk systems following pre-approved patterns, deployment authorization can be automated. The governance tooling layer checks that all required evidence exists, all evaluations passed their thresholds, and all sign-offs are recorded, and if everything checks out, authorization is granted without human review. For high-risk systems, deployment authorization requires explicit human confirmation from a designated authority. The goal is to make the lowest-risk deployments as fast as possible while ensuring the highest-risk deployments receive the scrutiny they warrant.

## Layer Seven: Post-Deployment Monitoring

Authorization does not end governance. It shifts governance from prospective to continuous. Post-deployment monitoring is the layer that ensures the system remains governed after it is live — that the controls validated before deployment continue to hold as the system operates in the real world with real data and real users.

Post-deployment monitoring covers several dimensions. Performance monitoring tracks whether the system's quality metrics remain within the thresholds established during technical validation. Drift detection identifies when the distribution of inputs or outputs shifts meaningfully from what was evaluated. Incident tracking captures and routes any reports of harmful, inaccurate, or unexpected system behavior. Compliance monitoring verifies that the legal conditions imposed during review continue to be met — that disclosures are being made, that human oversight is being maintained, that data handling remains within approved boundaries.

The output of post-deployment monitoring feeds back into the stack. When monitoring detects degradation, the system may need to return to technical validation for re-evaluation. When regulatory requirements change, the system returns to legal review. When the system's use case expands beyond its original scope, it returns to risk classification. The stack is not a one-way pipeline. It is a cycle. Systems move through it initially for deployment authorization, and they return to specific layers throughout their operational lifetime as conditions change.

## Making the Stack Real

The AI Decision Stack is not valuable as a concept. It is valuable as an operational system. This means it needs tooling: a system of record where proposals are registered, risk classifications are documented, evidence is stored, approvals are recorded, and monitoring is tracked. It needs defined roles: who performs risk classification, who conducts technical validation, who provides legal review, who grants business sign-off, who authorizes deployment. It needs service-level agreements: how long should each layer take for each risk tier. And it needs executive sponsorship: someone with sufficient authority to enforce that every AI deployment passes through the stack, including the ones built by teams that would prefer to skip it.

The stack is designed to accommodate both extremes. A low-risk system using a pre-approved pattern can move through all seven layers in three to five business days, with most layers requiring only lightweight evidence and automated verification. A high-risk system in a regulated domain might take four to eight weeks, with substantive review at every layer. The goal is not uniform speed. The goal is proportional rigor — the right level of review for the right level of risk, applied consistently across every AI system the organization deploys.

The AI Decision Stack gives you the process. But process without structure is fragile. It depends on individuals remembering roles, following paths, producing evidence. The next subchapter introduces the five layers of governance — ownership, policy, decision rights, oversight, and learning — that give the Decision Stack its institutional permanence and make governance survive leadership changes, team turnover, and organizational restructuring.
