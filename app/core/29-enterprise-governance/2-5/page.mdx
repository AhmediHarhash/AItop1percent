# 29.2.5 — Decision Rights: Who Ships, Who Reviews, Who Overrides, Who Accepts Risk

Every AI governance failure can be traced to a decision that nobody was explicitly authorized to make. Not a wrong decision. Not a careless decision. A decision that was never formally assigned to any person, any role, or any body. The model went to production because an engineer merged the code and nobody told them they needed approval. The vendor was adopted because a product manager signed a free-tier agreement and nobody told them it required legal review. The incident was handled by whoever happened to be online at 2 AM because nobody had defined an escalation path. In each case, the governance framework existed on paper. What did not exist was a clear, unambiguous answer to the question: who is authorized to make this decision?

**Decision rights** are the most important structural element of AI governance. More important than policies, more important than review boards, more important than tooling. Policies tell you what should happen. Review boards tell you who discusses what. Tooling helps you execute. But decision rights tell you who decides — and without that clarity, everything else is advisory. A governance program without explicit decision rights is a discussion group. It produces opinions. It does not produce outcomes.

## The Four Critical Decision Categories

AI governance involves hundreds of decisions, but they cluster into four categories that require distinct authorities, distinct evidence, and distinct accountability structures. Getting these four right covers ninety percent of the governance surface area. Getting even one wrong creates the kind of ambiguity that produces incidents.

The first category is **ship decisions** — who authorizes a model, a feature, or a system to go live in production, exposed to real users, processing real data. The second is **review decisions** — who evaluates the system before deployment and determines whether it meets the organization's standards. The third is **override decisions** — who can bypass a governance gate, skip a review step, or approve an exception, and under what conditions. The fourth is **risk acceptance decisions** — who accepts the residual risk that remains after all controls have been applied, and takes organizational responsibility for the consequences if that risk materializes.

These four categories are not sequential. They interact. A ship decision depends on a completed review. An override decision short-circuits the review. A risk acceptance decision changes what counts as an acceptable review outcome. Understanding these interactions is what separates governance that works from governance that produces paperwork.

## Ship Decisions: Who Authorizes Deployment

The ship decision is the moment when an AI system crosses from internal to external, from testing to production, from "our problem" to "our customers' experience." It is the most consequential governance decision because it is irreversible in a practical sense — once a model is serving real users, the harm window is open regardless of how fast you can roll back.

The authority to make ship decisions must be matched to risk tier. For low-risk deployments — an internal summarization tool, a code completion assistant used only by developers — the ship decision can sit with the engineering lead or product manager who owns the system. They sign off, the evidence is recorded, and the deployment proceeds. For medium-risk deployments — customer-facing features with limited blast radius, such as search ranking improvements or content recommendation changes — the ship decision should require both the product owner and a governance review confirming that standard controls are in place. For high-risk deployments — systems that affect financial decisions, medical recommendations, hiring outcomes, or operate in regulated domains — the ship decision requires an executive sponsor at the VP level or above, with sign-off from Legal and Risk confirming regulatory compliance and risk classification.

The mistake organizations make is applying the same ship authority to every deployment. When a VP must approve the deployment of an internal meeting summarizer, the VP approval becomes meaningless because they are signing off on twenty low-risk deployments a month and cannot give meaningful attention to any of them. When an engineering lead can ship a high-risk financial advisory model without executive review, the organization has delegated existential risk to someone without the authority or the organizational context to evaluate it. Tiered authority matches the gravity of the decision to the seniority and cross-functional visibility of the decision-maker.

Every ship decision must be recorded with four elements: who approved it, when they approved it, what evidence they reviewed before approving, and what risk tier the deployment was classified as. This record is not bureaucratic overhead. It is the single most important artifact in any future audit, regulatory inquiry, or incident investigation. The organization that can produce a clean deployment approval record for every AI system in production is an organization that can survive regulatory scrutiny. The organization that cannot is hoping nobody asks.

## Review Decisions: Who Evaluates Before Deployment

Ship decisions depend on review decisions. Someone must evaluate the system and determine that it meets standards before the ship authority can approve deployment. The review function is where technical expertise, domain knowledge, and governance criteria intersect.

Review authority should not rest with the team that built the system. This is the most violated principle in AI governance, and the violation is always rationalized the same way: "nobody understands the system better than the team that built it." That is exactly the problem. The team that built the system is the least likely to identify the risks they didn't consider, the assumptions they baked in, and the failure modes they designed around without noticing. Independent review — where "independent" means at minimum a different team, and for high-risk systems a different reporting line — catches the blind spots that self-review misses.

The review function answers three questions. First, does the system meet technical standards? This covers model performance, safety testing, output quality, latency, and infrastructure readiness. Second, does the system meet governance requirements? This covers risk classification, regulatory compliance, data handling, and documentation completeness. Third, are the residual risks understood and documented? No system is risk-free. The review function does not demand zero risk. It demands that the remaining risks are identified, quantified where possible, and explicitly communicated to the person who will make the ship decision and the person who will accept the risk.

Review decisions must come with a clear outcome: approved, approved with conditions, or rejected. "Approved with conditions" is the most useful outcome because it allows deployment to proceed while requiring specific mitigations — deploy with enhanced monitoring for thirty days, restrict to a subset of users for the first two weeks, require manual review of outputs exceeding a confidence threshold. Conditions must be specific and time-bounded. "Monitor closely" is not a condition. "Review a random sample of fifty outputs daily for fourteen days and escalate if error rate exceeds three percent" is a condition.

## Override Decisions: Who Can Bypass a Gate

Every governance framework needs an override mechanism. This sounds counterintuitive — why build gates if people can bypass them? Because the alternative is a governance system that breaks under pressure. When a critical business deadline requires a deployment that has not completed the full review cycle, when a market event demands an immediate model change, when a customer escalation requires a fix that cannot wait for the standard approval path — the organization will bypass governance with or without a formal mechanism. The question is whether the bypass is documented, authorized, and accountable, or whether it happens in the shadows and creates untracked risk.

The **Emergency Override Protocol** should define three things. First, who can invoke it. This should be a short list — typically a VP or above, or the Chief AI Officer, or a designated on-call governance authority. The list should be named individuals, not roles, because in an emergency you need to reach a specific person, not figure out who currently holds a role. Second, what conditions justify an override. Not every deadline is an emergency. The protocol should define the threshold: imminent customer harm, regulatory deadline, material revenue impact, security incident. "We promised the customer it would ship Friday" is not an override condition. "The current system is actively generating harmful outputs and the fix requires deploying an updated model" is. Third, what happens after the override. Every override creates governance debt. The protocol must require that the bypassed review is completed within a defined window — typically five business days — and that the override is logged with the justification, the authorizing individual, and the post-override review timeline.

Organizations that lack a formal override mechanism do not prevent overrides. They guarantee that overrides happen without documentation, without accountability, and without the post-override review that catches whatever the bypassed gate would have caught. The most dangerous overrides are the ones nobody knows happened.

## Risk Acceptance Decisions: Who Accepts Residual Risk

After the review is complete, after the conditions are defined, after the controls are in place, there is always residual risk. The model might hallucinate in an edge case the evaluation did not cover. The training data might contain biases the fairness tests did not detect. The vendor might change the underlying model in ways the contract does not prevent. Residual risk is not a failure of governance. It is the reality of deploying probabilistic systems in complex environments. The governance question is not how to eliminate residual risk. It is who accepts it on behalf of the organization.

Risk acceptance is the decision that most governance frameworks leave undefined, and it is the one that causes the most damage when it is ambiguous. If nobody formally accepts the residual risk of a deployment, then when that risk materializes — when the model produces a harmful output, when the data leak occurs, when the regulatory inquiry arrives — the organization has no record of a conscious, informed decision to proceed despite the known risk. This makes every incident look like negligence rather than a calculated trade-off, because there is no evidence that anyone weighed the risk and decided the value justified proceeding.

Risk acceptance authority must sit at a level commensurate with the potential impact. For a low-risk internal tool where the worst outcome is wasted employee time, a product manager can accept the residual risk. For a high-risk customer-facing system where the worst outcome is financial harm to consumers, regulatory sanction, or reputational damage, risk acceptance belongs at the executive level — a VP, a Chief Risk Officer, or a member of the AI Risk Committee. The person accepting risk must have the organizational authority to absorb the consequences of that risk materializing. An engineer who accepts the residual risk of a deployment that later causes a million-dollar regulatory fine has neither the authority nor the accountability to make that acceptance meaningful.

## Risk Acceptance With Expiry

Here is the concept that transforms how mature organizations handle risk acceptance. **Risk Acceptance With Expiry** means that every risk acceptance decision has a shelf life. A VP who accepted the residual risk of a financial advisory model in March accepted it for that model's behavior at that time, with that training data, in that regulatory environment, serving that user population. Six months later, three things have changed. The model provider released a new version with different behavior characteristics. The EU AI Act's high-risk system requirements became enforceable. The user base expanded from professional investors to retail consumers. The March risk acceptance is no longer valid because the conditions under which it was granted no longer exist.

Time-bounded risk acceptance changes organizational behavior in three important ways. First, it forces periodic re-evaluation. When a risk acceptance expires — typically every ninety days for high-risk systems and every hundred and eighty days for medium-risk systems — the system must go through a lightweight re-review. Has the model changed? Has the regulatory landscape shifted? Has the user population evolved? Has the threat environment changed? If the answer to any of these is yes, the risk acceptance must be renewed with updated analysis, not rubber-stamped with a reference to the original.

Second, it creates an automatic inventory of systems that need attention. When you maintain a register of risk acceptances with expiration dates, you have a rolling queue of systems that require governance review. This is far more effective than periodic audits that try to review everything at once. The expiry-based approach ensures that high-risk systems are reviewed frequently and that no system goes indefinitely without governance attention.

Third, it protects the organization legally. When a regulator asks "were you aware of the risks of this system?" the organization can produce a record showing that the risks were assessed, accepted by a named authority, and re-evaluated at defined intervals. This is the difference between an organization that made an informed decision and an organization that deployed a system and forgot about it. Courts and regulators treat these two situations very differently.

The expiry period should be matched to the volatility of the system and its environment. A model using a self-hosted, frozen model version in a stable regulatory environment might warrant a hundred-and-eighty-day expiry. A model using a third-party API where the provider updates the underlying model quarterly, deployed in a jurisdiction where AI regulation is actively evolving, might warrant a sixty-day expiry. The point is not to create busywork. The point is to ensure that risk acceptance remains a conscious, current decision rather than a historical artifact that nobody revisits.

## Encoding Decision Rights in Systems

Decision rights that exist only in a policy document will be violated the first time someone is under time pressure and cannot find the document. The organizations that make decision rights operational encode them in the systems their teams use. The deployment pipeline knows who can approve a deployment at each risk tier and will not proceed without the right approval. The governance platform tracks risk acceptance records with expiration dates and generates alerts when renewals are due. The incident management system routes escalations to the predefined authorities based on the system's risk classification.

This encoding does not replace judgment. It prevents the most common failure mode: someone making a decision they were not authorized to make, not because they were malicious, but because they did not know the decision required a different authority. When the system enforces decision rights, the conversation shifts from "who should have approved this?" after an incident to "who needs to approve this?" before a deployment. That shift — from retrospective blame to prospective authorization — is the difference between governance as punishment and governance as protection.

Decision rights tell you who is authorized to decide. But many of the most consequential governance decisions — especially those involving novel use cases, unclear risk classifications, or conflicting functional perspectives — require more than a single authority. They require deliberation by a body with diverse expertise and defined authority. The next subchapter examines the governance bodies that handle these decisions: review boards, risk committees, and ethics councils.
