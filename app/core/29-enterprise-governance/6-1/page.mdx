# 29.6.1 — Why Data Governance for AI Is Different from Traditional Data Governance

Traditional data governance knows where your data lives. AI data governance must know what your data became. That single distinction — between data at rest and data transformed into model behavior — is why every organization that applies its existing data governance framework to AI without modification discovers, usually painfully, that the framework covers less than half of what actually needs governing.

Traditional data governance matured over two decades around a clear mental model. Data lives in databases, warehouses, and lakes. It has a schema, a location, an owner, and access controls. You can query it, update it, move it, and delete it. Governance means knowing what data you have, who can access it, how long you keep it, and what regulations apply. The tools are mature: data catalogs, access control lists, lineage trackers, retention policies. By 2026, most enterprises have sophisticated governance programs that handle structured and semi-structured data well. The problem is that AI breaks the assumptions these programs were built on, and it breaks them in ways that are not immediately obvious.

## Data Stops Being Data

The first and most fundamental difference is that AI training transforms data into something that is no longer data in the traditional sense. When you train a model on a dataset of customer support tickets, those tickets do not sit inside the model the way rows sit inside a database. The information from those tickets is encoded into the model's weights — millions or billions of numerical parameters that collectively represent patterns the model learned. You cannot query the model's weights to ask "which customer tickets are in here." You cannot run a SELECT statement to find a specific record. You cannot update a single data point without retraining. The data has been metabolized. It has become the model.

This transformation has governance consequences that traditional frameworks do not anticipate. **Data-at-rest governance** assumes data can be located, inspected, modified, and deleted. Model weights defy all four assumptions. You cannot locate specific training examples within the weights. You cannot inspect whether a particular record influenced a particular output. You cannot modify the model to reflect an update to a single training example without retraining or fine-tuning. And you cannot delete a specific piece of information from the weights with the precision you can delete a row from a database. The governance tools and processes built for data at rest do not translate to data that has been absorbed into model parameters.

## The Consent Problem Changes Shape

Traditional data governance handles consent through a relatively clear framework. You collect data under a stated purpose. You process it for that purpose. If the purpose changes, you need new consent or a compatible legal basis. The consent model assumes that you can enumerate the purposes, that the data subject can understand what will happen to their data, and that you can demonstrate compliance by showing how data was used.

AI complicates every part of this. When a company collects customer data to provide a service — purchase history, support interactions, usage patterns — the consent basis typically covers service delivery and improvement. Whether that same consent covers training a machine learning model is a legal question that regulators across different jurisdictions answer differently. The French data protection authority, CNIL, published guidance in 2024 specifically addressing this gap, noting that organizations cannot assume consent obtained for one purpose automatically extends to model training. The issue is not hypothetical. In 2023, Italy temporarily banned ChatGPT over consent and transparency concerns. In 2025, the European Data Protection Board coordinated enforcement actions specifically targeting the right to erasure, with thirty-two data protection authorities across more than twenty countries collaborating on how AI complicates deletion rights.

The consent problem deepens when you consider that a model trained on data under one consent basis may then be used for purposes far beyond what any individual data subject anticipated. A model trained on customer support data to improve response quality might later be fine-tuned for sales recommendations, used to predict churn, or deployed in a product the original customers never interacted with. Each of these downstream uses may require its own consent analysis, but the data is already in the weights.

## Data Quality Means Something Different

In traditional data governance, **data quality** is measured along well-defined dimensions: accuracy, completeness, consistency, timeliness, validity. A customer record with a correct email address, a complete name, and a valid phone number is high-quality data. The metrics are straightforward and the tools for measuring them are mature.

For AI, data quality includes all of these dimensions plus several that traditional governance never considered. A dataset can be perfectly accurate, complete, and consistent by traditional standards and still be dangerously biased for AI purposes. If your customer support training data comes overwhelmingly from English-speaking users, the resulting model will underperform for non-English speakers — not because the data was wrong, but because it was unrepresentative. If your medical imaging dataset draws from three hospitals in the same region, the model may learn patterns specific to that population and fail when deployed elsewhere. If your financial fraud detection data contains labels generated by a previous system that was itself biased, the new model inherits and potentially amplifies those biases.

Traditional data quality tools will not catch these problems because they are not data quality problems in the traditional sense. They are dataset composition problems, label quality problems, and distribution mismatch problems. Governing data quality for AI means adding representativeness audits, bias detection, label accuracy verification, and distribution analysis to the quality framework — capabilities that most traditional data governance programs do not include.

## Deletion Is No Longer Delete

The GDPR's right to erasure — Article 17 — gives data subjects the right to have their personal data deleted. In traditional systems, deletion is operationally straightforward. You find the records, you delete them, you verify they are gone. The operation is atomic, verifiable, and complete.

In AI systems, deletion is none of these things. If a data subject requests erasure and their data was used to train a model, you can delete the original training record. But the model's weights still reflect the patterns learned from that data. The model may still be able to generate outputs that resemble or reference the deleted data. In extreme cases, large language models have been shown to memorize and reproduce specific training examples verbatim. Deleting the source data does not delete the model's knowledge of it.

The practical options are uncomfortable. You can retrain the model from scratch without the deleted data — effective but potentially costing tens of thousands to millions of dollars depending on model size, and impractical if erasure requests are frequent. You can attempt **machine unlearning**, a family of techniques that aim to remove the influence of specific training examples from a model's weights without full retraining. As of 2026, machine unlearning has progressed significantly in research — surveys cataloging dozens of approaches have been published, and techniques using parameter-efficient methods like LoRA adapters have shown promise. But the field still lacks formal guarantees of completeness. Neither the GDPR nor the EU AI Act specifies exactly what "erasure" means when data has been absorbed into model weights. This legal ambiguity means governance teams must make judgment calls about what constitutes adequate compliance — and document those decisions defensively.

## Data Lineage Must Extend to Model Behavior

Traditional data lineage tracks data from source to destination: this column in this report came from this table which was loaded from this file which was received from this vendor. The lineage is concrete, queryable, and auditable.

AI data lineage must track something far more complex: the relationship between training data and model behavior. This is not a technical refinement. It is a conceptual leap. You need to know not just "this data was included in the training set" but "how did this data influence the model's outputs." The first question is answerable with good metadata practices. The second is an open research problem. Influence functions, which estimate how much a specific training example affected a model's predictions, exist but are computationally expensive and approximate. For large language models with billions of parameters, precise influence attribution is currently infeasible at scale.

The governance implication is that you must build the best lineage infrastructure you can while accepting that perfect lineage — full traceability from raw data to model output — is not yet technically achievable. Practical lineage for AI includes dataset versioning so you know exactly which data went into which training run, training run metadata capturing hyperparameters and configuration, input-output logging in production to track what the model actually generates, and documentation of data processing steps between collection and training. This gives you directional lineage: you can say which datasets influenced which model version, even if you cannot pinpoint which specific training example caused a specific output.

## The Governance Gap

The result of all these differences is a governance gap. Your existing data governance program covers data collection, storage, access, quality, and retention. It does not cover data transformation into model weights, consent for training versus storage, AI-specific quality dimensions like representativeness and bias, deletion obligations when data is embedded in parameters, or lineage from data to model behavior. These are not edge cases. They are the core governance challenges for any organization building or deploying AI.

Closing this gap does not mean abandoning your existing data governance framework. Your existing program is necessary — it governs the data before it enters the AI pipeline. But it is not sufficient. You need an AI data governance layer that extends traditional governance into the model lifecycle: governing data as it is selected for training, as it is processed and transformed, as it becomes part of a model, and as that model is deployed, monitored, and eventually retired. The EU AI Act's Article 10 explicitly requires data governance measures for high-risk AI systems, including examination of data for biases, identification of data gaps, and documentation of data preparation choices. The August 2026 compliance deadline for high-risk systems means this is not a future consideration. It is a current obligation.

The next subchapter examines training data governance specifically — how to build provenance tracking, consent verification, and quality controls into the pipeline that feeds your models.
