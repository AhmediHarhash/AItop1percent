# 29.6.6 — Copyright and Intellectual Property in AI Training Data

The legal team is three hours into reviewing the training data manifest for a new fine-tuned model. The manifest lists 847 data sources. Most are internally generated — customer service transcripts, product documentation, internal knowledge base articles. But scattered across the list are entries that raise questions nobody prepared for. A web scrape of industry forums that captured thousands of copyrighted blog posts. A dataset purchased from a third-party vendor with licensing terms that predate the existence of generative AI. A collection of technical manuals from a partner company, shared under a collaboration agreement that permits "internal use" but says nothing about model training. The legal team does not know whether training a model on these materials constitutes infringement. Nobody does with certainty, because the case law is still being written.

This is the reality of copyright governance for AI in 2026. The legal questions are profound and largely unresolved. The litigation is active and accelerating. The regulatory frameworks are new and untested. And every organization training or fine-tuning AI models must make governance decisions today under legal uncertainty that will not be resolved for years.

## The Core Legal Question: Is Training a Model Copyright Infringement?

Copyright protects the expression of ideas, not the ideas themselves. When you train a model on a copyrighted text, the model does not store a copy of the text — it learns statistical patterns from it. The model's weights encode relationships between words, concepts, and structures derived from potentially billions of training examples. No single copyrighted work is stored in a retrievable form. The question is whether this process — the extraction of patterns from copyrighted material to create a new system capable of generating novel text — constitutes copying, transformation, or something the law has not yet categorized.

In the United States, the primary legal framework is the **fair use** doctrine, which permits unlicensed use of copyrighted material under certain conditions. Fair use is evaluated through four factors: the purpose and character of the use, the nature of the copyrighted work, the amount used in relation to the whole, and the effect on the market for the original. Each factor is subject to interpretation, and courts have not yet issued a definitive ruling on how they apply to AI training at scale. The argument for fair use is that training is transformative — the model creates something fundamentally new from the patterns it extracts, and no individual work is reproduced. The argument against is that the scale of copying is unprecedented, the commercial purpose is clear, and model outputs can compete directly with the markets for the original works.

In the European Union, the legal framework is different. The EU Copyright Directive provides a **text and data mining** exception that allows reproduction of lawfully accessible works for text and data mining purposes. This exception exists in two forms. Article 3 permits text and data mining for scientific research by research organizations and cultural heritage institutions. Article 4 permits text and data mining by anyone, including commercial entities, unless the rightsholder has expressly reserved their rights in a machine-readable format. The critical question for AI training is whether model training qualifies as text and data mining under the directive, and the prevailing legal interpretation in 2026 is that it does — but with the important caveat that the Article 4 exception does not apply when rightsholders have opted out.

## Active Litigation: The Cases That Will Define the Law

Several major lawsuits filed between 2023 and 2025 are working their way through courts and will likely produce the legal precedents that define how copyright applies to AI training.

**The New York Times v. OpenAI and Microsoft**, filed in December 2023, is the highest-profile case. The Times alleges that OpenAI trained GPT models on millions of its copyrighted articles without permission and that the models can reproduce Times content nearly verbatim. OpenAI's defense centers on fair use and the transformative nature of training. The case has produced significant procedural developments. In 2025, a court order required OpenAI to preserve and produce approximately twenty million ChatGPT output logs as potential evidence of verbatim reproduction. Summary judgment briefing is expected to conclude by April 2026, meaning the first substantive fair use ruling could come in mid-to-late 2026. The outcome will not resolve the question definitively — appeals are certain — but it will set the first major judicial marker.

**Getty Images v. Stability AI** addresses the same question for image generation. Getty alleges that Stability AI trained Stable Diffusion on more than twelve million copyrighted photographs, their captions, and their metadata without licensing them. In November 2025, the UK High Court issued the first major judicial ruling on AI training and copyright in the Getty case, finding that Stability AI's scraping and use of Getty's images went beyond what the UK's text and data mining exception permits. This ruling, while specific to UK law, signals that courts are willing to find infringement when AI companies train on copyrighted material without authorization.

No court anywhere in the world has yet issued a final ruling on whether AI training constitutes fair use under US law. Legal analysts expect the first substantive fair use decisions no earlier than the summer of 2026. Until then, every organization using copyrighted material in training operates under legal uncertainty.

## The EU AI Act and the GPAI Code of Practice: A Regulatory Framework Arrives

While the courts deliberate, the EU has created a regulatory framework that imposes concrete obligations on organizations that train AI models. The GPAI Code of Practice, published on July 10, 2025, includes a dedicated copyright chapter that translates the EU AI Act's requirements into practical measures.

**Measure 1.1** requires GPAI providers to implement and regularly update a copyright policy that clearly defines responsibilities within the organization. This is not a suggestion. It is a documented, auditable requirement.

**Measure 1.2** addresses the web crawling that feeds many training pipelines. Providers must ensure that their web crawlers access only lawfully accessible works, do not circumvent technological protection measures, and exclude piracy sites identified by EU authorities. This means your training data collection infrastructure must have technical controls that enforce these requirements — not just policies that state them.

**Measure 1.3** requires providers to identify and comply with rights reservations expressed through machine-readable protocols. The two primary mechanisms are robots.txt files, which website operators use to signal whether crawling is permitted, and other machine-readable opt-out protocols that are still being standardized. If a rightsholder has reserved their rights through a recognized protocol, your training pipeline must respect that reservation. This requires technical integration between your data collection systems and the opt-out mechanisms, and governance processes that verify compliance.

**Measure 1.4** addresses model outputs. Providers must implement proportionate safeguards to reduce the risk that the model generates copyright-infringing outputs, and must prohibit infringing use in their terms of service.

Beyond these measures, the EU AI Act requires GPAI providers to publish a summary of training data content. The European Commission's AI Office published the template for this summary on July 24, 2025, and the obligation to publish took effect on August 2, 2025. Models placed on the market before that date have until August 2, 2027, to publish the summary. This transparency requirement means your training data governance is not just an internal compliance matter — it is a public disclosure obligation.

## Building Governance for Legal Uncertainty

The hardest part of copyright governance for AI is that the law is still being written. You must make governance decisions today that will be evaluated against legal standards that do not yet exist. This is uncomfortable. It is also unavoidable. The answer is not to wait for legal clarity before governing. The answer is to build governance that is defensible under multiple possible legal outcomes.

Start with a **training data inventory** that classifies every data source by its licensing status. Some data is clearly licensed for model training — data you created internally, data you purchased with explicit training rights, open-source datasets with permissive licenses. Some data is clearly not licensed — copyrighted material you scraped without authorization from sources that have opted out. Most data falls in a grey zone — material accessed from the open web without explicit authorization but without explicit prohibition, data licensed under terms that predate AI and do not mention model training, data from partners under agreements that cover "internal use" but do not define whether training constitutes use.

For clearly licensed data, document the license and proceed. For clearly prohibited data, remove it from your training pipeline and document the removal. For grey-zone data, your governance framework must define how your organization handles the risk. Some organizations adopt a conservative posture and exclude all grey-zone data. Others adopt a risk-managed posture, using grey-zone data while documenting the legal analysis, monitoring litigation outcomes, and maintaining the ability to retrain without the grey-zone data if the legal landscape shifts against them. Your legal team must drive this decision, and the decision must be documented, reviewed periodically, and updated as case law develops.

## Opt-Out Mechanisms and the Compliance Infrastructure

The EU framework's emphasis on opt-out mechanisms creates a practical governance requirement: your training data pipeline must be able to detect and comply with opt-out signals, and you must be able to demonstrate that compliance to regulators.

This means building technical infrastructure that checks robots.txt files and other machine-readable reservation protocols before crawling, logs compliance decisions for every source accessed, maintains a registry of sources that have opted out and ensures they are excluded from current and future training runs, and can retroactively identify and remove data from sources that opted out after you collected their content. The retroactive requirement is the hardest. If a website adds a robots.txt directive today excluding AI training crawlers, and your training dataset already contains material from that website collected six months ago, your governance framework must define whether you remove the existing data, exclude it from future training runs, or take another remediation action. The GPAI Code of Practice implies forward-looking compliance, but organizations should prepare for the possibility that regulators will expect retrospective compliance as well.

## The Organizational Challenge: Engineering Needs Data, Legal Needs Compliance

Copyright governance for AI creates a structural tension between the teams that build models and the teams that manage legal risk. Engineering teams need large, diverse training datasets to build effective models. Every data source you exclude on copyright grounds reduces the training set. Legal teams need to minimize copyright exposure. Every data source you include without clear licensing creates potential liability. Neither team is wrong. The tension is real, and governance must manage it rather than pretend it does not exist.

The resolution is process, not principle. Principles like "respect copyright" and "build effective models" do not resolve the tension because they point in different directions. Process resolves it by making the trade-off explicit and decision-rights clear. Your governance framework should require legal review of training data sources before they enter the pipeline, define escalation paths for grey-zone data, assign decision authority for risk acceptance, and document every decision with its rationale. When the legal landscape eventually clarifies, you will either find that your decisions were correct or you will have a clear record of what needs to change. Either way, you will have acted responsibly under uncertainty — which is ultimately what governance under legal ambiguity demands.

The next subchapter addresses what happens after data has served its purpose in training: data retention and deletion challenges, where the model's need for stable training history collides with regulatory requirements to minimize and eventually delete personal data.
