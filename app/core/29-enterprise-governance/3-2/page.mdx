# 29.3.2 — NIST AI Risk Management Framework: The Four Functions Applied to Enterprise AI

The governance lead opens the NIST AI RMF document for the first time. It is seventy-three pages. The language is careful, dense, and deliberately general. She reads about "characteristics of trustworthy AI systems" and "socio-technical considerations" and "risk tolerance determination processes." Every sentence is defensible. Every paragraph is thorough. And after an hour of reading, she still has no idea what to do on Monday. She knows the framework has four functions — Govern, Map, Measure, Manage — and she can explain what each one means in the abstract. What she cannot do is translate those functions into the specific decisions, workflows, and infrastructure that her organization needs to govern fifty-three AI systems across seven business units. That translation — from NIST's conceptual architecture to enterprise operational reality — is what this subchapter teaches.

The NIST AI RMF is the right starting point for most organizations building governance from scratch. Its strength is precisely its generality: it works for a fifty-person startup with two AI features and a fifty-thousand-person bank with three hundred AI systems. But generality without operationalization produces shelf-ware — a beautifully mapped compliance matrix that nobody opens after the initial assessment. To make the framework useful, you need to understand not just what each function says but what it demands from your organization in terms of people, processes, and infrastructure.

## Govern: The Function That Makes Everything Else Possible

The Govern function is where most organizations fail first and fail hardest. It covers organizational culture, accountability structures, workforce diversity, stakeholder engagement, and the policies that define how the organization approaches AI risk. In NIST's language, Govern is the connective tissue that applies across all other functions. In practical terms, Govern answers the question: who owns AI risk at your company, and what authority do they have?

Operationalizing Govern means building at least four concrete artifacts. The first is a **governance charter** — a document, approved by executive leadership, that defines the scope of AI governance, the roles responsible for it, the authority those roles carry, and the escalation paths when disagreements arise. The charter is not the governance policy itself. It is the constitutional document that gives the governance function legitimacy. Without it, governance practitioners have opinions but no authority. With it, they have a mandate that every business unit must respect.

The second artifact is a **roles and responsibilities matrix** that maps governance accountabilities to specific people, not titles. "The risk committee reviews high-risk systems" is not actionable. "Sarah Chen, VP of Engineering, reviews all Tier 1 risk assessments within five business days and has authority to block deployment" is actionable. The matrix must cover the full lifecycle: who conducts the initial risk classification, who performs the detailed assessment, who approves deployment, who monitors production, who triggers incident response, and who decides when a system must be retrained or decommissioned.

The third artifact is an **AI risk appetite statement** — a document, endorsed by the board or executive committee, that articulates how much AI risk the organization is willing to accept and in what domains. "We do not deploy AI that makes final decisions about customer credit, insurance claims, or medical diagnoses without human review" is a risk appetite statement. "We accept that AI-generated marketing content may occasionally require manual correction and do not require pre-publication human review for non-regulated content categories" is another. Risk appetite statements constrain the decisions that every other function produces. Without them, every risk assessment is a judgment call with no anchor.

The fourth artifact is a **training and awareness program** that ensures everyone who builds, deploys, or manages AI systems understands their governance responsibilities. This does not mean a single annual compliance video. It means role-specific training — engineers learn about the deployment gate process and the monitoring requirements, product managers learn about the risk classification system and the escalation triggers, and executives learn about their accountability for portfolio-level risk. The NIST AI RMF explicitly identifies workforce diversity and AI expertise as governance requirements, recognizing that organizations cannot manage risks they do not understand and cannot understand risks if the people making governance decisions lack technical literacy.

## Map: Understanding What You Have and What It Touches

The Map function is the most underestimated of the four. It covers context identification, impact analysis, and the characterization of AI systems within their broader operational environment. In practice, Map answers: what AI do we have, what does it do, who does it affect, and what could go wrong?

The foundational output of Map is the **AI inventory** — a comprehensive, maintained, accurate catalog of every AI system in the organization. We covered the AI inventory in detail in Chapter 1. But Map goes beyond inventory. It requires you to characterize each system's context: what data it uses, what decisions it influences, who is affected by those decisions, what the impact would be if it fails, and what the impact would be if it succeeds in ways you did not intend. This last category — unintended success — is where generative AI creates novel risk. A content generation model that produces marketing emails exactly as designed can still create regulatory risk if those emails contain claims that violate advertising standards in jurisdictions your marketing team didn't consider.

Operationalizing Map means conducting a **contextual impact assessment** for every AI system, calibrated to the system's risk tier. For Tier 1, high-risk systems, the assessment is deep: a multi-page analysis that examines data lineage, decision scope, affected populations, regulatory exposure, failure modes, and misuse scenarios. For Tier 3, low-risk systems, the assessment is a single-page classification form that confirms the system's risk tier and documents the rationale. The assessment is not a one-time exercise. It is repeated when the system's scope changes, when its data sources change, when it is deployed to new geographies or customer segments, and on a fixed cadence — annually for Tier 3 systems, quarterly for Tier 1 systems.

The Map function also requires you to identify and engage **stakeholders** — not just internal teams, but the people affected by your AI systems. For a customer service chatbot, stakeholders include customers, support agents, the customer experience team, the legal team, and the data privacy function. For an underwriting model, stakeholders include applicants, underwriters, actuaries, regulators, and the fairness and bias team. Mapping stakeholders is not a symbolic exercise. It determines whose perspectives are included in risk assessment, whose concerns shape monitoring requirements, and whose feedback triggers re-evaluation.

## The GenAI Profile: Extending Map and Measure for Generative AI

In July 2024, NIST released AI 600-1, the **Generative Artificial Intelligence Profile** — an extension of the AI RMF specifically designed for the risks that generative AI introduces. The profile does not replace the four functions. It adapts them by identifying twelve risk categories unique to or amplified by generative AI and mapping more than two hundred specific actions across Govern, Map, Measure, and Manage.

The twelve risk categories include confabulation and hallucination, data privacy violations, harmful bias and homogenization, information security threats, intellectual property and copyright concerns, obscene or degrading content, CBRN information access, and environmental impact. For each category, the profile prescribes specific mapping, measurement, and management actions. For hallucination risk, for example, the profile calls for structured evaluation of the system's factual accuracy across domains, monitoring of hallucination rates in production, and documentation of the system's limitations in user-facing disclosures.

If your organization uses generative AI — and by 2026, nearly every enterprise does — the GenAI profile is not optional reading. It is the most specific, actionable guidance NIST has produced for the risks that most urgently concern enterprise leadership. Treat it as an extension of your Map and Measure processes: for every generative AI system in your inventory, assess it against the twelve risk categories and document which categories apply, what the exposure level is, and what controls are in place.

## Measure: The Function That Keeps You Honest

The Measure function is where governance moves from intention to evidence. It covers the tools, techniques, and methodologies for assessing AI risk — not once, but continuously. In NIST's language, Measure employs quantitative, qualitative, or mixed-method approaches to analyze, assess, benchmark, and monitor AI risk and related impacts. In practical terms, Measure answers: how bad is this risk, and is it getting better or worse?

Operationalizing Measure requires you to define **metrics** for each risk category you identified in Map. If you mapped bias risk for an underwriting model, your Measure function defines how bias is measured — which demographic groups, which statistical tests, what threshold constitutes an unacceptable disparity. If you mapped hallucination risk for a customer support chatbot, your Measure function defines how hallucination is detected — automated factual accuracy checks, human review sampling rates, escalation rates from customers who received incorrect information.

The critical mistake is measuring once. Many organizations conduct a thorough risk assessment before deployment, document the results, and never measure again. The NIST framework explicitly positions Measure as a continuous function. AI risk is not static. Models drift. Data distributions shift. User behavior evolves. Regulatory requirements change. A system that was measured as low-risk at deployment can become high-risk six months later without any change to the system itself — because the world it operates in changed. Continuous measurement means defining monitoring cadences, setting alert thresholds, and building the infrastructure to collect measurement data from production systems automatically.

The Measure function also requires you to define **assessment methods** — and to be honest about their limitations. Self-assessment, where the team that built the system also evaluates its risk, produces consistently optimistic results. Independent assessment, where a separate function evaluates the system, produces more accurate results but requires more organizational capacity. Third-party assessment, where an external firm evaluates the system, provides the highest credibility but the highest cost. Most organizations use a tiered approach: self-assessment for Tier 3 systems, independent internal assessment for Tier 2 systems, and independent or third-party assessment for Tier 1 systems. The tier determines the method, and the method determines the credibility of the measurement.

## Manage: Closing the Loop

The Manage function is where risk assessment turns into risk response. It covers the allocation of resources to mapped and measured risks, the implementation of controls, and the processes for monitoring and adjusting those controls over time. Manage answers the question that every other function builds toward: now that we know the risk, what do we do about it?

Operationalizing Manage means building a **risk response catalog** — a documented set of responses that your organization deploys for different types of risk at different severity levels. Risk responses fall into four categories. You can **mitigate** the risk by implementing controls that reduce its probability or impact — adding a human review layer, implementing content filtering, restricting the model's decision scope. You can **transfer** the risk by shifting accountability to another party — purchasing insurance, requiring customers to accept terms of service that allocate liability, using a model provider whose contract includes indemnification. You can **accept** the risk by documenting the decision to proceed without additional controls — a valid choice for low-severity, well-understood risks that fall within your stated risk appetite. Or you can **avoid** the risk by not deploying the system at all — the correct response when a risk exceeds your appetite and no available control reduces it to an acceptable level.

The Manage function also requires **incident response planning** specific to AI systems. Traditional incident response assumes a system is either working or broken. AI systems introduce a third state: working incorrectly in ways that are difficult to detect. A model that produces biased outputs is not down. It is up, serving users, and causing harm while every availability monitor shows green. Your AI incident response plan must account for this: it must define how silent failures are detected, who is empowered to halt a production model, what the communication protocol is for AI-specific incidents, and how the organization learns from incidents to update its Map, Measure, and Govern artifacts.

The most overlooked element of Manage is the feedback loop back to Govern. When you manage a risk — when you implement a control, accept an exposure, or halt a deployment — that decision generates information that should update your governance posture. If you are consistently accepting bias risks because your organization lacks the tooling to measure bias effectively, that is a signal that the Govern function needs to allocate resources for bias measurement infrastructure. If you are consistently blocking deployments because risk assessments take too long, that is a signal that the Govern function needs to streamline the assessment process or staff the assessment team. The four functions are not a pipeline with Govern at the start and Manage at the end. They are a cycle, and the organizations that close the cycle — feeding Manage decisions back into Govern — are the ones whose governance programs improve over time rather than ossifying into compliance theater.

## Making It Real: The NIST Implementation Cadence

The framework does not prescribe an implementation timeline, but enterprise experience between 2024 and 2026 has produced a pattern that works. The first quarter focuses on Govern: charter, roles, risk appetite, and training. The second quarter focuses on Map: inventory, contextual assessments, and stakeholder identification. The third quarter focuses on Measure: metric definition, assessment methods, and monitoring infrastructure. The fourth quarter focuses on Manage: risk response catalog, incident response planning, and the feedback loop. After the first year, the cycle repeats — but now each iteration refines rather than creates, and the refinement is informed by a full year of operational data.

This cadence is not a waterfall. You do not wait until Govern is perfect to start Map. You build Govern to the minimum viable level, start Map, and iterate Govern as Map reveals gaps. You start Measure before Map is complete, because the act of measuring exposes mapping inaccuracies that would otherwise remain hidden. The functions are concurrent, not sequential. But the emphasis shifts across quarters to ensure that no function receives all the attention while others atrophy.

The NIST AI RMF is a thinking framework, not a doing framework. It gives you the architecture for how to think about AI risk management. What it does not give you is the certifiable evidence that you are actually doing it — and that is precisely what ISO 42001 provides, which we will explore in the next subchapter.
