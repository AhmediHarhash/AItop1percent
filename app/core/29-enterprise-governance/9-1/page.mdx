# 29.9.1 -- The AI Audit Landscape: Internal Audit, External Audit, and Regulatory Examination

Every AI governance program eventually faces the same question: who checks whether the controls actually work? The answer is not one function but three, each with a different mandate, different authority, and different expectations for what "working" means. Internal audit tests whether your own policies are being followed. External audit provides independent assurance to boards, customers, and regulators that your claims hold up under scrutiny. Regulatory examination tests whether your compliance meets the legal standard -- and unlike the first two, this one comes with enforcement power. Understanding how these three functions differ, where they overlap, and what each demands from your organization is the foundation of any serious assurance program.

## Three Pillars, Three Mandates

**Internal audit** operates as your organization's own assurance function, typically reporting to the audit committee of the board rather than to management. In the traditional Three Lines Model published by the Institute of Internal Auditors, internal audit occupies the third line -- independent assurance that the first line (operational management) and second line (risk and compliance functions) are performing their roles effectively. For AI systems, this means internal audit tests whether your model risk classification is accurate, whether your data governance controls are operating, whether your deployment approvals followed the documented process, and whether your monitoring is actually catching what it claims to catch. Internal audit does not build the controls. It tests them.

**External audit** -- sometimes called third-party audit -- brings independence that internal audit cannot provide. When your board needs assurance that your AI governance program meets a recognized standard like ISO 42001, or when a customer's procurement team requires evidence that your AI systems have been independently validated, internal audit's word is not sufficient. External auditors bring credibility precisely because they have no organizational incentive to find favorable results. The market for third-party AI auditing is growing rapidly, with firms like Holistic AI, Credo AI, BABL AI, and the Big Four accounting firms all building dedicated AI audit practices. KPMG became one of the first Big Four firms in the United States to receive ISO 42001 AI management system certification in late 2025, signaling that these capabilities are maturing beyond boutique specialists.

**Regulatory examination** is neither advisory nor voluntary. When the SEC examines your firm's use of AI in investment decisions, or when an EU market surveillance authority assesses your high-risk AI system's conformity with the AI Act, the examiner is not offering recommendations. They are determining whether you comply with binding legal requirements, and non-compliance carries penalties. The SEC's 2026 examination priorities explicitly include AI as an operational risk area and flag "AI washing" -- misleading claims about AI capabilities -- as an enforcement concern. These are not theoretical risks. They are active examination priorities that examiners are trained to pursue.

## How AI Auditing Differs from Traditional IT Auditing

If you have been through SOX IT audits, SOC 2 examinations, or ISO 27001 certifications, you might assume AI auditing is a natural extension. It is not. Traditional IT auditing operates in a deterministic world. You test whether a control exists, whether it was executed, and whether it produced the expected result. Access controls either restrict the right users or they do not. Backup procedures either run on schedule or they do not. Change management either requires approvals or it does not. The control either passes or fails, and the auditor's judgment is binary.

AI systems break this model in three fundamental ways. First, the outputs are probabilistic. A model that correctly classifies ninety-one percent of inputs is not failing -- it is operating within its expected performance range. The auditor cannot simply check whether the output was correct. They must assess whether the accuracy rate is acceptable for the risk context, whether the nine percent error rate concentrates in protected populations, and whether the organization has adequate monitoring to detect when that accuracy degrades. This requires statistical reasoning that most IT auditors were never trained to perform.

Second, the systems are opaque in ways that traditional IT systems are not. An auditor examining a rules-based decision engine can read the rules and verify they implement the stated policy. An auditor examining a neural network cannot read the decision logic because it is distributed across billions of parameters with no human-interpretable mapping between inputs and outputs. The auditor must rely on behavioral testing -- examining what the model does across a representative set of inputs -- rather than structural testing of the decision logic itself. This shifts auditing from inspection to experimentation.

Third, AI systems are data-dependent in a way that changes the scope of the audit. Auditing a traditional application means examining the code and configuration. Auditing an AI system means examining the training data, the evaluation data, the production input distribution, the data pipeline integrity, the labeling process, and the ongoing data monitoring. The data is not an input to the system -- it is part of the system. An audit that examines only the model and its deployment without examining the data is incomplete in a way that would never be tolerated in a financial audit that ignored the underlying transactions.

## The IIA AI Auditing Framework

The Institute of Internal Auditors published its AI Auditing Framework, most recently updated in September 2024, to help internal audit functions navigate these differences. The framework is organized around the IIA's Three Lines Model and covers the full governance landscape: strategic alignment between AI initiatives and organizational objectives, ethical risk assessment including bias and fairness, data governance across the AI lifecycle, technical resourcing and capability, third-party and vendor controls, and ongoing monitoring and reporting.

The framework provides practical starting points. Its "Getting Started" guidance helps auditors assess whether AI is being used across the organization, map which departments are deploying AI tools, identify the teams responsible for AI governance, and evaluate whether board-level strategy alignment exists. The Practitioner's Guide offers a checklist spanning AI strategy, data governance, cyber risks, training, vendor controls, and reporting -- cross-referenced against established frameworks like the NIST AI Risk Management Framework, COSO, and COBIT. For audit teams that have never examined an AI system, this framework provides the vocabulary and the structure to begin. For audit teams with some AI experience, it provides the rigor to move from ad hoc reviews to systematic assurance.

## Assurance Levels: Matching Rigor to Risk

Not every AI system warrants the same depth of assurance. A chatbot answering frequently asked questions about office hours does not need the same audit intensity as a model making credit decisions or screening medical images. The assurance approach should scale with the risk classification of the system.

At the lightest level, low-risk systems receive a governance review -- confirming that the system is registered in the model inventory, that a risk classification was performed, and that basic monitoring exists. The audit is documentation-focused and can be completed in days. At the next level, medium-risk systems receive a controls assessment -- testing whether the stated controls are operating effectively, whether evaluation data is current, and whether deployment followed the approved process. This requires access to technical artifacts and typically takes one to two weeks. For high-risk systems, a full assurance engagement is warranted -- independent testing of model performance across demographic groups, validation of the training data pipeline, stress testing under adversarial conditions, and a comprehensive review of the governance documentation. This engagement may take four to eight weeks and often requires specialized technical expertise that the audit team may not possess internally. For the highest-risk systems -- those making autonomous decisions with legal effects on individuals -- external independent audit should supplement internal assurance, providing the level of independence that boards, regulators, and affected individuals have the right to expect.

## The 2026 Reality: Regulatory Examination Is Here

The regulatory examination landscape has shifted from aspirational to operational. The EU AI Act's obligations for high-risk AI systems take effect on August 2, 2026. Article 43 establishes conformity assessment procedures that require providers to demonstrate compliance through either internal control processes or, for certain categories, assessment by a notified body. Providers of high-risk AI systems listed under Annex III must follow documented conformity assessment procedures, maintain quality management systems, and produce technical documentation that demonstrates compliance with the Act's requirements. Notified bodies will conduct periodic audits to verify that quality management systems are maintained and applied, and may carry out additional testing of AI systems for which certificates were issued.

In the United States, the SEC's Division of Examinations has integrated AI into multiple 2026 priority categories. AI is no longer treated as an "emerging technology" curiosity. It is assessed under cybersecurity governance, automated investment tools, operational resiliency, and disclosure accuracy. Examiners will evaluate whether firms' actual AI usage matches their representations to clients and regulators -- meaning that marketing claims about AI capabilities will be tested against operational reality. Financial services firms that overstate their AI capabilities in marketing materials or Form ADV disclosures face enforcement risk not for the AI itself but for the misleading claims about it.

## The Capability Gap and How to Close It

The gap between what organizations need to audit and what their audit teams can actually assess is significant. Industry surveys from 2025 indicate that only four percent of Chief Audit Executives report substantial progress implementing AI auditing capabilities, and only eighteen percent of internal audit functions are using AI within their own processes in any capacity. More than eighty-five percent of audit leaders report relying on external partners to augment their capabilities. Meanwhile, forty-three percent of internal audit functions saw no budget increase and eighteen percent saw reductions -- even as the scope of what they are expected to cover expanded dramatically.

Closing this gap requires deliberate investment in three areas. First, upskill your existing audit team. Auditors do not need to become data scientists, but they need to understand model fundamentals well enough to ask the right questions -- what training data was used, how was performance evaluated, what monitoring is in place, how are edge cases handled. The IIA's training courses on AI auditing, updated in July 2025, provide a structured curriculum. Second, hire or embed technical specialists. At least one person on your audit team -- or available to it on demand -- should be able to review model evaluation results, interpret fairness metrics, and assess data pipeline integrity at a technical level. Third, co-source with external specialists during the capability build. Engaging a firm with AI audit expertise for your first two or three AI audit engagements lets your internal team learn methodology in practice rather than in theory.

The audit landscape for AI is no longer a future concern. It is an operational reality that most organizations are racing to build capability for, often with less budget and fewer skilled people than the task demands. The next subchapter dives into how to build that internal audit capability from scratch -- starting with the audit universe, the skills your team needs, and a realistic three-year roadmap to independence.
