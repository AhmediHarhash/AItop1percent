# 29.4.1 — The EU AI Act: Structure, Scope, and What It Requires from Your Organization

The EU AI Act is not a guidance document. It is not a set of recommendations. It is a regulation with the force of law, backed by fines of up to 35 million euros or seven percent of global annual turnover — whichever is higher. If your AI systems produce outputs that are used by anyone in the European Union, this regulation applies to your organization regardless of where you are headquartered, regardless of where your servers run, and regardless of whether you have a single employee on European soil. This is the most consequential AI regulation in the world, and as of 2026, it is enforceable.

Most organizations that will be affected by the EU AI Act still treat it as a legal concern — something for the compliance team to track and the lawyers to interpret. That posture is dangerously wrong. The Act's requirements are engineering requirements. They demand specific technical capabilities: logging, monitoring, documentation, human oversight mechanisms, risk management systems, and conformity assessments. The compliance team cannot build these. The legal team cannot deploy them. Your engineering and governance teams must understand the Act's structure, scope, and operational demands well enough to build the systems that produce compliance evidence continuously — not retroactively when a regulator asks for it.

## The Structure of the Act

The EU AI Act was formally adopted in March 2024 and entered into force in August 2024. It is organized into thirteen titles, though the ones that matter most for your engineering and governance teams are concentrated in a handful of them. Title I establishes definitions and scope. Title II defines the prohibited AI practices — the systems you cannot build at all. Title III contains the requirements for high-risk AI systems, which represent the heaviest compliance burden and the area where most enterprise AI organizations will spend the majority of their compliance effort. Title IV addresses transparency obligations for certain AI systems. Title V covers general-purpose AI models, including the obligations that apply to foundation model providers. Titles VI through XIII deal with governance structures, enforcement mechanisms, delegated acts, and transitional provisions.

The Act's architecture follows a risk-based approach. Not all AI systems face the same requirements. A spam filter and an AI system that evaluates job applicants carry fundamentally different risks, and the Act regulates them differently. The four risk tiers — prohibited, high-risk, limited risk, and minimal risk — determine what your organization must do for each AI system it builds or deploys. We will cover the risk classification system in detail in the next subchapter, but the structural principle matters here: the Act does not impose a single compliance standard on all AI. It imposes graduated requirements based on the potential harm a system can cause. Your first compliance task is classifying every AI system in your portfolio into the correct tier.

## Key Definitions Your Teams Must Know

The Act introduces a precise vocabulary that your organization must adopt, because the definitions determine who is responsible for what. The most important definitions are not academic — they carry direct compliance obligations.

An **AI system** under the Act is a machine-based system designed to operate with varying levels of autonomy, that may exhibit adaptiveness after deployment, and that, for explicit or implicit objectives, infers from the inputs it receives how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments. This definition is deliberately broad. It covers your chatbot, your recommendation engine, your fraud detection model, your resume screening system, and your document summarization pipeline. If you are building systems that use machine learning or large language models, those systems almost certainly qualify.

A **provider** is an entity that develops an AI system or a general-purpose AI model — or has one developed on its behalf — and places it on the market or puts it into service under its own name or trademark. If you build a model and sell access to it, you are a provider. If you contract a third party to build a model and you release it under your brand, you are still the provider. Providers carry the heaviest obligations under the Act: risk management, data governance, technical documentation, conformity assessment, post-market monitoring, and serious incident reporting.

A **deployer** is an entity that uses an AI system under its own authority, except where that use is for personal non-professional purposes. If you take an off-the-shelf AI model from a provider and integrate it into your product, you are a deployer. Deployers have their own obligations — not as extensive as providers, but substantial. You must use the system in accordance with the provider's instructions. You must ensure human oversight. You must monitor the system's operation and report serious incidents. And crucially, if you modify a high-risk AI system in a way that changes its intended purpose, you become the provider of that modified system and inherit all provider obligations. This reclassification trap catches organizations that fine-tune commercial models or repurpose them for use cases the original provider did not intend.

**Importers** bring AI systems from outside the EU into the European market. **Distributors** make AI systems available within the EU without being the provider or importer. Both carry verification obligations — they must ensure the provider has completed the required conformity assessments and documentation before the system enters the market.

## Territorial Scope: The Extraterritorial Reach

If your organization is based in the United States, Japan, Singapore, or anywhere else outside the European Union, do not assume the Act does not apply to you. The EU AI Act's territorial scope is explicitly extraterritorial, and it reaches further than even the GDPR in some respects.

The Act applies to three categories of organizations. First, providers that place AI systems on the EU market or put them into service in the EU, regardless of where the provider is established. If your company is headquartered in San Francisco and you sell an AI product to a customer in Berlin, you are subject to the Act. Second, deployers that are established or located within the EU. Third — and this is the broadest reach — providers and deployers established outside the EU where the output produced by the AI system is used in the Union. The GDPR's extraterritorial reach required that organizations target EU residents with goods or services. The AI Act has no such targeting requirement. If your AI system's outputs are used by anyone in the EU, even indirectly, the Act can reach you.

For a multinational enterprise, this means practically every AI system in your global portfolio needs to be assessed against the EU AI Act. Even systems that were not designed for the European market may produce outputs that reach EU users through downstream integrations, partner relationships, or global product deployments. The compliance assessment must start with an honest mapping of where your AI outputs actually end up, not where you intend them to go.

## The Compliance Timeline: What Has Already Happened and What Is Coming

The Act's obligations do not all take effect at once. They phase in over a multi-year timeline, and understanding where you are on that timeline is essential for prioritizing your compliance work.

February 2, 2025 marked the first enforcement milestone. As of that date, the prohibited AI practices defined in Title II are enforceable. Any organization operating an AI system that falls into a prohibited category — social scoring systems, manipulative AI that exploits psychological vulnerabilities, certain real-time biometric identification systems in public spaces — is already in violation. The AI literacy requirement also took effect on this date, meaning organizations must ensure that staff operating and overseeing AI systems have a sufficient level of AI literacy relevant to their role.

August 2, 2025 was the second milestone. The obligations for general-purpose AI models — the GPAI rules in Title V — became applicable. Providers of foundation models must comply with transparency requirements, copyright compliance, and technical documentation obligations. For GPAI models classified as posing systemic risk, additional obligations around safety evaluation, adversarial testing, incident reporting, and cybersecurity protections apply. The GPAI Code of Practice, published in July 2025, provides the detailed compliance guidance for these obligations.

August 2, 2026 is the deadline that most enterprise AI organizations are working toward right now. On this date, the full requirements for high-risk AI systems take effect. Conformity assessments, quality management systems, risk management systems, technical documentation, human oversight requirements, accuracy and robustness obligations — all of it becomes enforceable. However, the European Commission's Digital Omnibus proposal from November 2025 proposed deferring the Annex III high-risk system deadline until harmonized standards and common specifications are available, with a long-stop date of December 2, 2027. As of early 2026, the Omnibus is still working through the legislative process. Prudent organizations are not relying on the extension. They are building toward August 2026 while monitoring the legislative outcome, because being ready early carries no penalty while being caught unprepared carries enormous risk.

August 2, 2027 brings the final wave: obligations for high-risk AI systems that are components of larger products already covered by EU harmonized legislation, such as medical devices, automotive systems, and aviation equipment. Providers of GPAI models that were placed on the market before August 2025 must also be fully compliant by this date.

## What the Act Requires: The Operational View

At its core, the EU AI Act requires your organization to do six things for any high-risk AI system, and it requires you to prove you are doing them.

First, establish a risk management system that runs continuously throughout the AI system's lifecycle. Not a one-time risk assessment before launch — an ongoing, documented process that identifies risks, evaluates them, mitigates them, and monitors residual risk over time.

Second, implement data governance that ensures your training, validation, and testing datasets are relevant, representative, and as free from errors as possible given the intended purpose. The Act does not expect perfect data, but it expects documented, systematic data quality management.

Third, produce and maintain technical documentation sufficient for authorities to assess the system's compliance. This is not a product specification or an internal design document. It is a specific set of information defined in Annex IV of the Act, and it must be kept up to date for the entire period the system is on the market.

Fourth, build automatic logging capabilities so the system records events throughout its operation in a manner that enables traceability. The logs must be retained for a period appropriate to the system's intended purpose and applicable legal obligations.

Fifth, design the system to enable human oversight. Someone must be able to understand, monitor, and when necessary intervene in the system's operation. The level of oversight must be proportionate to the risks the system poses.

Sixth, ensure the system achieves appropriate levels of accuracy, robustness, and cybersecurity. Performance must be maintained throughout the system's lifecycle, and degradation must be detected and addressed.

Beyond these six pillars, providers must establish a quality management system, perform conformity assessments before placing the system on the market, affix CE marking to indicate conformity, register the system in the EU database, implement post-market monitoring, and report serious incidents to the relevant authorities without undue delay.

## The Shift from Optional to Mandatory

Before the EU AI Act, AI governance was a choice. A good choice, an increasingly expected choice, but a choice nonetheless. Organizations that invested in AI governance did so because they valued risk management, because enterprise customers demanded it, or because they anticipated regulation. Organizations that did not invest in governance faced reputational risk but no legal penalty.

That era is over. The EU AI Act transforms AI governance from an organizational best practice into a legal obligation with financial teeth. The fine structure is designed to be painful at any scale: 35 million euros or seven percent of global turnover for prohibited practice violations, 15 million euros or three percent for other violations, and 7.5 million euros or one percent for providing incorrect information to authorities. For a company with two billion euros in annual revenue, the maximum fine for a prohibited practices violation is 140 million euros. These numbers are not theoretical. The EU has demonstrated through GDPR enforcement that it is willing to impose significant fines — Meta received a 1.2 billion euro GDPR fine in 2023, Amazon received a 746 million euro fine in 2021.

The organizations that treated AI governance as optional now face a compliance gap that cannot be closed overnight. The technical capabilities the Act requires — continuous risk management, automated logging, human oversight mechanisms, conformity-ready documentation — take months to build. Teams that started governance work in 2024 are well positioned. Teams starting in mid-2026 are already behind, regardless of whether the Omnibus extends the deadline.

The next subchapter breaks down the risk classification system that determines which of these requirements apply to each of your AI systems — and why classification is more difficult than it appears.
