# 29.4.6 — The US Regulatory Patchwork: Federal Policy, State Laws, and Enforcement Reality

The EU has one comprehensive AI law. The United States has hundreds of fragments. No federal statute equivalent to the EU AI Act exists. No single agency owns AI regulation. No unified risk classification system tells you which of your AI systems face mandatory compliance requirements. Instead, the US offers a shifting mosaic of executive orders that change with administrations, sector-specific agency guidance that may or may not carry enforcement weight, state laws that vary dramatically in scope and ambition, and a growing body of enforcement actions brought under statutes that never mention artificial intelligence by name. For organizations operating in both the EU and the US, this asymmetry creates a specific kind of governance challenge: you must build compliance infrastructure precise enough to satisfy the EU AI Act's detailed requirements while flexible enough to accommodate a US landscape where the rules are still being written by dozens of different authors.

## The Federal Posture: Pro-Innovation, Limited Prescription

The current federal approach to AI regulation in the US is shaped by two executive orders that point in opposite directions. In October 2023, President Biden signed Executive Order 14110, which established reporting requirements for developers of advanced AI models, directed federal agencies to develop AI safety guidelines, and created a framework for managing AI risks across government. In January 2025, the incoming Trump administration revoked Executive Order 14110, replacing the safety-focused framework with a pro-innovation posture that emphasized reducing regulatory barriers to AI development and deployment.

On December 11, 2025, President Trump signed a new executive order titled "Ensuring a National Policy Framework for Artificial Intelligence." This order does three things that matter for your compliance planning. First, it asserts federal authority over AI regulation and signals that the administration views state-level AI laws as potential barriers to innovation. Second, it directs the Attorney General to establish an AI litigation task force to challenge state AI laws deemed inconsistent with federal policy, including on grounds of unconstitutional regulation of interstate commerce. Third, it directs the Federal Trade Commission to issue a policy statement by March 2026 describing how the FTC Act applies to AI and when state laws are preempted by federal authority over deceptive practices.

The practical effect in early 2026 is regulatory uncertainty at the federal level. The executive order articulates a direction — less prescriptive regulation, more reliance on existing enforcement mechanisms — but executive orders are not legislation. They can be challenged in court, reversed by a future administration, or rendered moot by congressional action. Building your compliance program around the assumption that the current federal posture will persist indefinitely is a risk your governance team should explicitly acknowledge and document.

## Sector-Specific Federal Enforcement: Where the Action Is

The absence of a comprehensive federal AI law does not mean the absence of federal enforcement. Existing agencies are applying existing statutes to AI-related conduct, and several have made AI a declared enforcement priority for 2026.

The **Securities and Exchange Commission** has elevated AI to a top examination priority for the 2026 reporting season. The SEC is focused on two areas: AI-related disclosures by public companies and the use of AI in financial services. Companies that make material claims about AI capabilities in filings or marketing materials face scrutiny under existing securities fraud and disclosure requirements. Broker-dealers and investment advisers using AI for trading, portfolio management, or client recommendations face examination under existing fiduciary and suitability rules. The SEC has not created new AI-specific rules. It is applying existing rules to AI conduct — which means the obligations are real even though they are not labeled as AI regulation.

The **Federal Trade Commission** has been the most active federal enforcement body on AI issues. The FTC has brought enforcement actions under Section 5 of the FTC Act — which prohibits unfair or deceptive practices — against companies making misleading claims about AI capabilities, a practice the agency has labeled **AI washing**. The FTC has also pursued enforcement in areas including AI-generated fake reviews, algorithmic pricing practices, and AI systems that violate consumer privacy commitments. The December 2025 executive order directs the FTC to formalize how the FTC Act applies to AI, which will provide clearer guidance but also expand the agency's articulated enforcement surface.

The **Equal Employment Opportunity Commission** monitors AI use in hiring, promotion, and workforce management for disparate impact and unlawful discrimination against protected classes under Title VII of the Civil Rights Act. While the incoming administration removed some earlier EEOC technical assistance documents from the agency's website, the underlying legal obligations have not changed. If your AI hiring tool produces disparate impact against a protected class, you face liability under Title VII regardless of whether the EEOC's guidance documents are currently published. The law exists independently of the guidance.

The **Food and Drug Administration** regulates AI and machine learning-based software as medical devices. The FDA has cleared or authorized hundreds of AI-enabled medical devices and maintains a framework for evaluating AI systems that learn and adapt after deployment. If you are building clinical AI, the FDA's regulatory requirements are specific, mature, and actively enforced. The **Department of Health and Human Services** and **Office for Civil Rights** enforce HIPAA in contexts where AI systems process protected health information.

## State-Level Legislation: The Growing Patchwork

While the federal government has stepped back from prescriptive AI regulation, state legislatures have accelerated. The result is a growing and increasingly complex patchwork of state-specific requirements.

The **Colorado AI Act** (SB 24-205) is the most comprehensive state AI law enacted to date. Originally set to take effect in February 2026, its effective date was delayed to June 30, 2026 following an amendment signed in August 2025. The law requires developers and deployers of **high-risk AI systems** to exercise reasonable care to prevent algorithmic discrimination. Deployers must conduct impact assessments before deploying high-risk systems, provide consumers with notice that AI is being used in consequential decisions, and establish a governance program. Developers must provide deployers with documentation about the system's capabilities, limitations, and known risks of algorithmic discrimination. The law's definition of high-risk aligns broadly with use cases similar to those in the EU AI Act's Annex III — employment, education, financial services, healthcare, housing, insurance, and legal services. Colorado's law is notable because the December 2025 federal executive order specifically named it as an example of state regulation the administration considers excessive, making it a likely target for the Attorney General's AI litigation task force.

**California** has been prolific in AI-related legislation, though its approach is fragmented across multiple bills rather than unified in a single act. The California Transparency in Frontier Artificial Intelligence Act took effect January 1, 2026, requiring transparency disclosures from developers of frontier AI models. California has also enacted requirements around AI-generated deepfakes, AI use in political advertising, and AI transparency in specific sectors. Governor Newsom vetoed SB 1047, a broader AI safety bill, in September 2024, signaling that even in regulation-friendly California, the scope and approach of AI oversight remains contested.

**Texas** enacted the Responsible Artificial Intelligence Governance Act, which also took effect January 1, 2026, establishing governance requirements for state agency use of AI and certain private sector obligations. **Illinois** has the Biometric Information Privacy Act, which predates the AI era but applies directly to AI systems that collect or process biometric data — face recognition, voiceprints, fingerprint analysis. BIPA's private right of action and per-violation damages have generated substantial litigation and settlements, making it one of the most consequential privacy laws for AI companies operating in the US. **New York City's Local Law 144** requires employers using automated employment decision tools to conduct annual bias audits and provide notice to candidates, creating a compliance obligation specific to hiring AI within one municipality.

## The Preemption Question

The central unresolved question in US AI regulation is whether federal policy will preempt state laws. The December 2025 executive order signals the administration's intent to assert federal supremacy, but an executive order cannot preempt state legislation on its own. Congressional action is required for statutory preemption. As of early 2026, the proposed TRUMP AMERICA AI Act — introduced by Senator Marsha Blackburn — represents the most ambitious congressional attempt to codify the executive order's preemption framework into legislation. The bill would preempt state laws regulating frontier AI developers' management of catastrophic risk and largely preempt state laws addressing digital replicas, while expressly preserving state authority over child safety, AI infrastructure permitting, government procurement, and other specified areas.

Whether this legislation passes, and in what form, remains uncertain. Until federal preemption is enacted through statute, existing state AI laws remain enforceable. The executive order itself acknowledges this — it expressly does not preempt otherwise lawful state AI laws in the areas it carves out. Your compliance program cannot assume preemption that has not yet happened. If you deploy AI systems in Colorado, California, Illinois, New York City, or Texas, the state-level requirements apply to you today regardless of the federal administration's preferences.

## Building a Program That Works Across Both Regimes

The practical challenge for US-based companies operating globally is building a compliance program that satisfies the EU AI Act's specific requirements for EU-facing systems while simultaneously navigating the US patchwork for domestic operations. The most effective approach is what governance teams call the **high-water-mark strategy**: design your compliance infrastructure to meet the most demanding set of requirements you face across all jurisdictions, then adapt downward for jurisdictions with lighter obligations.

In practice, this means your EU AI Act compliance program becomes the foundation. If you have built a risk management system, technical documentation, conformity assessments, and post-market monitoring to satisfy the EU AI Act's high-risk requirements, you have already met or exceeded the requirements of most US state laws. Colorado's impact assessment requirement is a subset of the EU's conformity assessment. New York City's bias audit requirement is a subset of the EU's data governance and non-discrimination requirements. The additional work for US compliance is mapping your existing compliance artifacts to the specific requirements of each applicable state law and filling any gaps — which, if you have built for the EU standard, tend to be narrow.

Where the US adds requirements the EU does not is in sector-specific regulation. The FDA's requirements for clinical AI are more detailed and domain-specific than anything in the EU AI Act. SEC disclosure requirements for AI use in financial services create obligations that the EU AI Act does not address. HIPAA's requirements for protected health information apply to AI systems in ways that the EU AI Act does not specifically prescribe. Your compliance architecture must layer these sector-specific requirements on top of your general AI governance framework rather than treating them as separate programs.

## The Enforcement Reality

The most important thing to understand about US AI regulation in 2026 is that enforcement is already happening. State attorneys general have brought enforcement actions against companies deploying discriminatory AI systems. The FTC has imposed consent orders and fines on companies making deceptive AI claims. The EEOC has investigated AI hiring tools for disparate impact. The SEC has issued guidance that amounts to a warning: AI disclosures are under scrutiny. The absence of a comprehensive federal AI law does not mean the absence of regulatory risk. It means the risk is distributed across multiple agencies, multiple statutes, and multiple jurisdictions — which makes it harder to track, not easier to ignore.

The organizations that treat the US regulatory patchwork as permission to delay governance investment are the ones most likely to face enforcement action. The organizations that build comprehensive governance infrastructure — even without a single law telling them exactly what to build — are the ones that will navigate both the current patchwork and whatever federal legislation eventually emerges. The regulation beyond the EU and the US is equally important for organizations with global ambitions. The next subchapter examines how the UK, Singapore, Canada, Japan, and emerging frameworks in other regions are shaping the global AI compliance landscape your organization must navigate.
