# 29.7.1 — AI Risk Taxonomy: Categorizing the Risks Your Organization Actually Faces

The most dangerous AI risks are the ones your risk framework was not designed to see. Traditional enterprise risk taxonomies — built for credit exposure, supply chain disruptions, and cybersecurity breaches — assume risks are discrete, identifiable, and owned by a single department. AI risks are none of these things. They are probabilistic, emergent, and cross-functional. A hallucinating customer service model is simultaneously a model risk, a reputational risk, a compliance risk, and a legal risk, and no single team in your organization owns all four dimensions. Until you build a risk taxonomy purpose-built for AI, you are categorizing risks into buckets that were designed for a different era and missing the ones that will actually hurt you.

## Why Generic Enterprise Risk Frameworks Fail for AI

Your existing enterprise risk management framework likely follows a structure inherited from financial services regulation or ISO 31000. It works well for risks that are static, binary, and attributable to a single root cause. A server goes down — that is operational risk. A vendor raises prices — that is third-party risk. A regulation changes — that is compliance risk. The risk has a cause, an owner, and a response plan.

AI risks defy this model in three fundamental ways. First, they are probabilistic rather than binary. A model does not either work or fail. It produces outputs that are correct some percentage of the time, and that percentage shifts as input distributions change, as the model drifts, and as user behavior evolves. You cannot say "the model failed" the way you can say "the server went down." You can only say "the model's accuracy dropped from ninety-one percent to eighty-three percent over six weeks, and we did not notice until a customer complaint triggered an investigation." Second, AI risks are emergent. They arise from the interaction between the model, the data, the deployment context, and user behavior in ways that nobody predicted during development. A model that performs flawlessly on test data produces biased outputs when deployed to a new demographic — not because of a bug, but because the training data did not represent that population. Third, AI risks are cross-functional. Model drift is an engineering problem, but its consequences land on legal, compliance, product, and the executive team. No single risk owner can manage an AI risk end to end, which means AI risks fall between the cracks of frameworks that assign each risk to one department.

## Model Risk: The Technical Core

A governance-ready AI risk taxonomy needs at minimum seven categories, each capturing risks that traditional frameworks either misclassify or miss entirely. **Model risk** is the first and most technical category. It covers hallucination, where the model produces confident but fabricated outputs. It covers drift, where model performance degrades as the real world diverges from the training distribution. It covers bias, where the model produces systematically different outcomes for different populations. And it covers memorization, where the model retains and can reproduce specific training examples, including sensitive personal data. Model risk is the category most engineering teams understand, but it is also the category most frequently underestimated by governance teams who assume that if the model passed its initial evaluation, the risk is managed.

The subtlety of model risk is that it is continuous rather than discrete. Your model does not suddenly become biased or start hallucinating on a particular date. These behaviors exist on spectrums that shift over time. A model might hallucinate on two percent of queries at launch and six percent three months later as user behavior diversifies beyond the distribution the model was tested on. Governance teams accustomed to thinking in binary — the system is compliant or it is not — must learn to think in gradients when managing model risk. The question is never "does this model hallucinate" but "at what rate does this model hallucinate, on which input types, and is that rate within our tolerance."

## Operational and Compliance Risk

**Operational risk** covers availability, latency, throughput, and dependency on external providers. When your customer-facing product depends on an API call to a model hosted by a third party, your uptime is no longer entirely within your control. In January 2025, multiple organizations experienced cascading failures when a major model provider's API suffered an extended outage, and the companies that had no fallback models or graceful degradation paths lost hours of customer-facing functionality. Operational risk for AI also includes the risk that your inference infrastructure cannot handle traffic spikes, that your model serving layer introduces latency that violates service level agreements, and that a model update deployed through your CI/CD pipeline introduces a regression that is not caught until production.

**Compliance risk** is the category that boards understand most viscerally because it comes with specific financial penalties. The EU AI Act establishes fines of up to thirty-five million euros or seven percent of global annual turnover for prohibited AI practices, fifteen million euros or three percent for other violations, and seven point five million euros or one percent for providing misleading information to regulators. The August 2026 enforcement deadline for high-risk AI systems means compliance risk is not theoretical — it is a countdown. Beyond the EU AI Act, sector-specific regulations layer additional compliance obligations. Financial services firms face scrutiny from the SEC, which included AI-related examination priorities in its 2026 fiscal year guidance. Healthcare organizations face HIPAA implications when AI processes protected health information. Employment-related AI decisions fall under existing anti-discrimination law, and the EEOC has signaled active interest in algorithmic hiring tools.

## Reputational, Strategic, and Data Risk

**Reputational risk** is harder to quantify but often more damaging than regulatory fines. When a public-facing AI system produces biased, offensive, or visibly wrong outputs, the damage spreads through social media within hours. The Air Canada chatbot incident in 2024, where a customer was given incorrect information about bereavement fare policies and the airline was held liable for the chatbot's statements, demonstrated that reputational damage from AI failures is not limited to tech companies. Any organization deploying customer-facing AI carries this risk, and the reputational impact scales with the sensitivity of the domain. A biased hiring algorithm generates far more public outrage than a biased product recommendation engine.

**Strategic risk** captures the competitive dimension. If your competitors adopt AI capabilities faster, more effectively, or more responsibly than you do, the strategic gap compounds over time. But strategic risk also runs in the opposite direction. Organizations that rush AI deployment without adequate governance face the risk of a high-profile failure that forces a retreat, destroying internal confidence in AI and setting the organization back years. The taxonomy must capture both sides: the risk of moving too slowly and the risk of moving too recklessly.

**Data risk** encompasses leakage, poisoning, and consent violations. Training data that contains personal information creates privacy exposure. Retrieval-augmented generation systems that pull from sensitive internal documents can surface confidential information in model outputs. Data poisoning — where an adversary corrupts training data to influence model behavior — is a risk that most organizations have not assessed because it sits at the intersection of security and machine learning, a boundary that few teams patrol.

## Third-Party Risk and the Vendor Dependency Problem

**Third-party risk** deserves its own category because most organizations in 2026 consume AI primarily through vendor APIs and hosted models rather than training models from scratch. When you build on a third-party model, you inherit every risk that model carries — its biases, its failure modes, its training data composition — without the visibility to assess those risks directly. When the vendor updates the model behind the API, your system's behavior changes without your consent, your testing, or your approval. When the vendor deprecates a model version, you face a forced migration on their timeline, not yours. And when the vendor raises prices or changes terms, your cost structure shifts in ways your original business case did not anticipate.

The NIST AI Risk Management Framework's Map function provides a useful starting point for structuring this taxonomy, emphasizing the need to identify AI-specific risks in context rather than abstracting them into generic categories. The EU AI Act's own risk classification — unacceptable, high, limited, and minimal risk — provides the regulatory layer. But neither framework is sufficient on its own. The NIST framework is voluntary and principle-based. The EU AI Act is mandatory but jurisdiction-specific. Your taxonomy must incorporate both while adding the operational specificity that your organization needs to actually manage risk.

## Building a Taxonomy That Maps to Your AI Portfolio

A taxonomy that sits in a governance document but does not connect to your actual AI systems is decorative. The taxonomy becomes operational only when every AI system in your portfolio is mapped to the risk categories it touches. Start by inventorying every AI system in production — the model registry discussed in Chapter 5 is the foundation for this work. For each system, assess which of the seven categories apply and at what severity. A low-risk internal text summarization tool might carry moderate model risk, low compliance risk, and negligible reputational risk. A high-risk automated lending decision system carries severe model risk, severe compliance risk, severe reputational risk, and significant data risk.

The mapping produces a risk profile for each system and a risk heat map across the portfolio. The heat map reveals concentration risk — if fifteen of your twenty AI systems depend on the same third-party model provider, your portfolio-level third-party risk is far higher than any individual system assessment would suggest. It reveals category gaps — if nobody assessed data poisoning risk for any of your production systems, you have an unexamined exposure across the entire portfolio. And it reveals the systems that carry the highest aggregate risk, which should receive the most governance attention and the most monitoring investment.

The mapping is not a one-time exercise. New AI systems enter the portfolio. Existing systems change scope, data sources, or deployment contexts. Regulations evolve. The GPAI Code of Practice, finalized in July 2025, introduced obligations for general-purpose AI model providers that ripple downstream to every organization using those models. Your taxonomy must be a living artifact — reviewed quarterly at minimum, updated whenever a new AI system enters production, and reassessed whenever a regulatory change or an incident reveals a risk category you underweighted.

## Common Taxonomy Mistakes

Three mistakes recur when organizations build their first AI risk taxonomy. The first is **risk conflation** — lumping all AI risks into a single "AI risk" category within the existing enterprise framework. This produces entries like "AI system failure" in the enterprise risk register, which is about as useful as a single entry for "technology failure." It tells the board nothing actionable. The seven categories exist precisely because the mitigations, the owners, and the escalation paths differ for each one. Model bias requires different expertise, different tools, and different response protocols than vendor API deprecation.

The second mistake is **static severity assignment**. Teams classify each risk as high, medium, or low at the time of the initial assessment and then never revisit the classification. But AI risk severity changes as the system evolves, as the regulatory environment shifts, and as the organization's deployment context changes. A model classified as medium compliance risk in early 2025 may need reclassification to high by mid-2026 as the EU AI Act's high-risk system obligations take effect. Severity must be dynamic, tied to the continuous risk scoring described in the next subchapter.

The third mistake is **ignoring interconnections**. AI risks do not exist in isolation. A model drift event that degrades accuracy is a model risk. But if the degraded accuracy produces biased outcomes in a regulated domain, it becomes a compliance risk. If a user screenshots the biased output and posts it publicly, it becomes a reputational risk. The taxonomy must account for these cascade paths — the way one risk category can trigger another — because the financial exposure of a cascading incident is multiplicative, not additive.

## The Organizational Home for Risk Taxonomy

The final question is ownership. A risk taxonomy owned exclusively by the risk management function tends to be thorough on compliance and financial exposure but blind to technical model risk. A taxonomy owned exclusively by the engineering team tends to capture model drift and performance degradation but miss the legal and reputational dimensions. The most effective approach assigns ownership to the cross-functional AI governance body — the committee described in Chapter 2 — with explicit input requirements from engineering, legal, compliance, product, and security. Each risk category has a designated subject matter expert who is responsible for keeping that category current, but the taxonomy as a whole is governed collectively.

The review cadence matters as much as the ownership structure. Quarterly reviews of the full taxonomy keep it current, but event-driven updates are equally important. When a competitor suffers a public AI failure, your taxonomy should be reviewed for analogous exposure within days, not at the next quarterly meeting. When a new regulation is proposed or finalized, the compliance risk dimension must be updated immediately. When your organization launches a new AI system or expands an existing one into a new domain, the portfolio mapping must be refreshed before the system goes live. A taxonomy that updates only on schedule is a taxonomy that falls behind reality — and in the AI risk landscape of 2026, reality moves fast.

The taxonomy tells you what risks exist. The next subchapter examines how to assess those risks — moving beyond static checklists to methods that capture the probabilistic, emergent nature of AI risk.
