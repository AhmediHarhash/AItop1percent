# 29.6.4 — PII and Regulated Data in AI Training and Inference

PII that enters a model's training data does not stay as data. It becomes capability — the model's ability to generate text that looks like your customers' private information. A database stores a Social Security number in a row you can query, audit, and delete. A model that trained on that same number has absorbed it into billions of weight parameters, where it cannot be located, cannot be isolated, and cannot be surgically removed. This distinction is the reason every PII governance framework built for traditional databases fails when applied to AI systems without fundamental adaptation.

The stakes are not abstract. In April 2023, Samsung discovered that employees had pasted proprietary source code and internal meeting transcripts directly into ChatGPT. The data entered OpenAI's system, was potentially retained for training, and could not be retrieved or deleted. Samsung banned employee use of generative AI tools entirely and began developing internal alternatives. That incident involved trade secrets, not customer PII, but the mechanism is identical. When regulated personal data — patient records, financial account details, children's information — enters an AI training pipeline through the same kind of careless or uninstrumented path, the consequences shift from embarrassing to legally catastrophic.

## Training Memorization: The Problem You Cannot See

**Training memorization** occurs when a model learns specific training examples so precisely that it can reproduce them verbatim or near-verbatim in its outputs. Every large language model memorizes to some degree. Research has demonstrated that models can regurgitate phone numbers, email addresses, physical addresses, and other PII from their training data when prompted with the right context. The risk is not theoretical. It is measurable, documented, and proportional to how many times a specific data point appeared in the training set.

Memorization is worse when data is repeated. A customer's email address that appears once across a ten-billion-token training corpus is unlikely to be memorized. The same address appearing three hundred times across customer service logs, support tickets, and CRM exports has a dramatically higher memorization probability. This is why deduplication matters for privacy, not just for training efficiency. Organizations that dump raw operational databases into training pipelines without deduplication are effectively amplifying the memorization risk for their most active customers — the people who interact with the company most frequently and therefore appear in the data most often.

Detection is difficult. You cannot inspect a model's weights and find the PII inside them. The only reliable method is **extraction testing**: systematically prompting the model with contexts that might trigger memorized content and measuring whether PII appears in outputs. This is expensive, incomplete by nature, and needs to be repeated after every training run. A model that passes extraction testing today may fail it tomorrow with a different set of prompts. Extraction testing reduces risk. It does not eliminate it.

## Differential Privacy: The Technical Control and Its Limits

**Differential privacy** is the strongest technical guarantee against memorization. Applied to model training through a technique called DP-SGD — differentially private stochastic gradient descent — it adds mathematically calibrated noise to the training process. The noise ensures that no single training example can significantly influence the model's learned parameters, which limits the model's ability to memorize any individual data point.

The guarantee is real and it is formal. A model trained with strong differential privacy, measured by a parameter called epsilon, provides a mathematical bound on how much any individual's data affected the model. Lower epsilon values mean stronger privacy. The problem is that the guarantee comes at a cost. Models trained with strong differential privacy — epsilon values near one — typically show significant accuracy degradation. Industry experience consistently shows performance drops of five to fifteen percentage points on standard benchmarks at epsilon values below ten. For some applications, this trade-off is acceptable. For applications that need both high accuracy and strong privacy, the trade-off is punishing.

There are also practical implementation challenges. DP-SGD requires careful handling of batch sampling, gradient clipping, and noise calibration. Research published in 2025 identified a gap between the theoretical privacy guarantees of DP-SGD and its typical implementation: the random batch sampling that the theoretical guarantees depend on is rarely implemented correctly in standard deep learning frameworks. Teams that believe they are training with differential privacy may be getting weaker guarantees than they think. Privacy engineering is not a flag you set in a configuration file. It requires expertise, validation, and ongoing verification.

Beyond accuracy costs, differential privacy can widen performance gaps across demographic subgroups. If the training data contains fewer examples from a particular population, the noise added by DP-SGD degrades the model's performance on that population more than on well-represented groups. A clinical AI system trained with differential privacy might perform well on common conditions affecting majority populations while degrading substantially on rare conditions or minority demographics. This fairness interaction means that privacy controls cannot be evaluated in isolation — they must be assessed alongside equity metrics.

## PII Detection and Filtering in Training Pipelines

Before data enters a training pipeline, your governance framework must require PII detection and filtering. This is the first line of defense, and it is far cheaper and more reliable than trying to remove PII from a trained model after the fact.

**PII detection** uses a combination of pattern matching, named entity recognition, and machine learning classifiers to identify personal data in text. Pattern matching catches structured PII — Social Security numbers, credit card numbers, phone numbers, email addresses — with high reliability. Named entity recognition catches names, addresses, and organizations. Machine learning classifiers catch less structured PII — medical descriptions that identify a specific patient, financial narratives that identify a specific account holder.

No detection system is perfect. False negatives — PII that the detector misses — mean regulated data enters the training pipeline undetected. False positives — non-PII flagged as personal data — mean useful training data is removed unnecessarily, reducing dataset quality. Your governance framework must define acceptable false negative and false positive rates based on the regulatory sensitivity of the data. For HIPAA-protected health information, the false negative tolerance should be near zero, even if that means accepting a high false positive rate and losing training data. For general business data in a low-regulation context, a higher false negative rate may be acceptable if the potential harm is limited.

Filtering can take two forms. **Redaction** replaces detected PII with placeholder tokens — a name becomes a generic person token, an address becomes a location token. This preserves the structure and context of the training example while removing the identifying information. **Exclusion** removes the entire training example if it contains PII above a severity threshold. Redaction preserves more training data. Exclusion provides stronger privacy guarantees. Your governance framework should specify which approach applies to which data categories, based on regulatory requirements and risk tolerance.

## Inference-Time PII Risks

Training is not the only exposure surface. Inference — the process of running a deployed model to generate outputs — creates its own PII risks that your governance framework must address separately.

The first risk is **output leakage**: a model generating PII that it memorized during training. Even with PII filtering in the training pipeline, some memorization may occur. Output scanning — applying PII detection to model outputs before they reach the user — is the inference-time equivalent of input filtering. Every response the model generates passes through a PII detector, and responses containing potential PII are either redacted or blocked. This adds latency to every request, typically five to twenty milliseconds per scan, but the cost is trivial compared to the regulatory exposure of serving a customer their neighbor's medical record.

The second risk is **input exposure**: users submitting PII into the AI system through their prompts. A customer asking a support chatbot "my account number is 4829-3847-2938 and I need to dispute a charge" has just entered a credit card number into your system. If your system logs prompts for quality monitoring, retraining, or debugging — and most production AI systems do — that credit card number is now stored in your logs. If those logs feed back into training pipelines, the number could end up memorized by the next model version. Your governance framework must define how user inputs are handled: what is logged, how long logs are retained, whether PII in inputs is redacted before logging, and whether logged inputs can ever flow into training data.

The third risk is **context window contamination**: in systems where previous conversation turns are included in the prompt context, PII from one user's session could theoretically persist and appear in another user's session if session isolation is not properly implemented. This is an engineering failure, not a model behavior, but your governance framework must require session isolation testing as part of the deployment validation process.

## Regulatory Requirements That Shape Your Framework

Three regulatory frameworks have the most direct impact on PII governance in AI systems, and your governance framework must address each explicitly.

**GDPR** applies to any organization processing personal data of EU residents, regardless of where the organization is headquartered. Article 17 establishes the right to erasure — the right to have personal data deleted. For AI systems, this creates a fundamental tension: if a person's data was used to train a model, deleting the data from your databases does not delete its influence on the model's weights. The Article 29 Working Party and its successor, the European Data Protection Board, have not yet issued definitive guidance on whether retraining a model constitutes adequate erasure. Your governance framework should treat this as an unresolved risk and document your approach — whether that means never training on data subject to erasure requests, retraining models when erasure requests are received, or arguing that the model's inability to produce identifiable outputs satisfies the erasure requirement. Legal counsel must sign off on whichever position you take.

**GDPR Article 22** restricts automated decision-making — decisions based solely on automated processing that produce legal effects or similarly significant effects on individuals. If your AI system makes decisions about credit, employment, insurance, or access to services without meaningful human involvement, Article 22 requires that you provide the data subject with an explanation of the logic involved and the right to contest the decision. This does not prohibit AI-assisted decision-making, but it requires that a human with real authority be involved in decisions with significant impact, and that the individual affected can challenge the outcome.

**HIPAA** governs protected health information in the United States. If your AI system processes, stores, or generates PHI — and clinical AI systems almost certainly do — HIPAA's Security Rule requires administrative, physical, and technical safeguards. Training an AI model on PHI requires a Business Associate Agreement with any third party involved in the training process, including cloud compute providers. Inference outputs that contain PHI must be encrypted in transit and at rest. Audit trails must track who accessed what PHI and when. De-identification under HIPAA's Safe Harbor method requires removing eighteen specific categories of identifiers — and that de-identification must happen before data enters the training pipeline, not after.

California's AB 2013, which took effect on January 1, 2026, requires developers of generative AI systems to publish documentation about their training data, including whether datasets contain personal information. This transparency obligation means your training data governance is no longer just an internal concern. It is a disclosure requirement.

## The Organizational Challenge: AI Teams and Data Access

The deepest governance challenge with PII in AI is not technical. It is organizational. Engineering teams building AI systems need broad data access to build effective models. Traditional application teams access data through well-defined APIs and database queries with role-based access controls. AI teams need to access large volumes of raw data for training — data that often includes PII, operational details, and sensitive business information that no traditional application team would ever see.

This access pattern breaks the principle of least privilege that most data governance frameworks depend on. A machine learning engineer building a customer service model needs access to millions of customer service transcripts. Those transcripts contain customer names, account numbers, complaint details, and potentially health or financial information. The engineer does not need any individual customer's PII to build the model — they need the patterns and language across millions of conversations. But the raw data they access contains all of it.

Your governance framework must address this gap with controls that are specific to AI workflows. Training data preparation environments should be separate from production data environments, with PII detection and filtering applied at the boundary. Engineers should access filtered or redacted training data, not raw production data. Access to raw data should require explicit approval with documented justification and time-limited access windows. Automated pipelines that move data from production into training environments should have PII filtering as a mandatory, non-bypassable step — not an optional configuration that individual engineers can disable.

The Samsung incident was a failure of this organizational boundary. Engineers with access to sensitive data used an external AI tool without governance controls because no governance controls existed for that workflow. Building the controls after the incident is more expensive, more disruptive, and less effective than building them before the first engineer pastes sensitive data into a prompt. Your governance framework must anticipate the access patterns that AI development creates and design controls around them before teams find the path of least resistance around the controls you have.

The next subchapter addresses a different data governance challenge: when the training data itself is generated by machines, not collected from real sources, and the governance implications of synthetic data in AI pipelines.
