# 29.10.10 -- Multi-Team and Multi-Product Governance: Shared Standards with Local Autonomy

The governance lead pulls up the risk classification that the lending team submitted for their new loan decisioning model. It is labeled "Tier 2 -- Moderate Risk." She then opens the classification the marketing team submitted for their new ad targeting model. Also labeled "Tier 2 -- Moderate Risk." She opens the third, from the fraud detection team, for a transaction monitoring system. "Tier 2 -- Moderate Risk." All three teams used the same classification framework. All three teams arrived at the same tier. But the lending model makes credit decisions that are regulated under fair lending laws. The marketing model personalizes ads with no regulatory exposure. The fraud model flags suspicious transactions where false positives freeze customer accounts. These are not the same risk. The teams interpreted the framework differently because they live in different contexts, use different vocabularies, and face different consequences for failure. Three teams, one framework, three incompatible interpretations -- and each team is confident that theirs is correct.

This is the central problem of multi-team AI governance. You need consistency so that a "Tier 3" system means the same thing across the organization. You also need local context so that a lending team's risk assessment reflects lending-specific regulatory exposure, not a generic rubric written for all domains. Get the balance wrong in either direction and governance breaks. Too much centralization creates a bottleneck where every classification requires the central team to learn domain-specific context they do not have. Too much decentralization produces the scenario above: shared labels with no shared meaning.

## What Must Be Standardized

Certain governance elements must be consistent across every team and every product in the organization. These are the non-negotiable shared standards, and they are fewer than most governance teams think.

**Risk classification methodology** is the first. Not the risk thresholds -- those can vary by domain -- but the methodology. Every team must assess the same dimensions: who can be harmed, how severely, how many people are affected, what regulatory obligations apply, and how reversible the harm is. The dimensions are universal. The answers differ by domain. A lending team and a marketing team both assess severity of harm, but a lending team's severity scale reflects credit denial while a marketing team's reflects ad irrelevance. The methodology is shared. The domain-specific calibration is local.

**Documentation format** is the second. Every AI system in the organization should produce documentation in the same structure so that auditors, governance reviewers, and executives can read any system's documentation without learning a new format. The content differs by domain, but the sections, the required fields, and the evidence structure are consistent. When an auditor reviews system forty-seven, they should find the same documentation architecture as system three, even though the systems serve different teams and different purposes.

**Incident reporting** is the third. When an AI system causes harm or near-harm, the reporting path, the severity classification, and the escalation criteria must be identical across teams. An incident in the fraud team should flow through the same reporting structure as an incident in the marketing team. Inconsistent incident reporting means inconsistent organizational learning -- the fraud team learns from its failures, the marketing team learns from its failures, but neither learns from the other's.

**Audit trail requirements** are the fourth. What must be logged, how long logs must be retained, what evidence must be preserved for regulatory review -- these are organizational commitments that cannot vary by team. A regulator does not ask one business unit for evidence. They ask the organization.

**Minimum control standards** are the fifth. Every AI system, regardless of team or domain, must meet baseline requirements: monitoring is active, a rollback mechanism exists, an owner is named, evaluation results are documented. These minimums are the governance floor. Teams can exceed them. No team can operate below them.

## What Should Be Localized

Everything else should be adapted to local context. Attempting to centralize domain-specific governance decisions is where multi-team programs stall.

**Risk tier thresholds** should reflect domain-specific consequences. A Tier 3 designation in the lending team might require regulatory impact assessment and fairness testing across protected classes. A Tier 3 designation in the marketing team might require privacy impact assessment and opt-out mechanism verification. Both are high-risk. The specific controls differ because the specific harms differ.

**Review workflows** should match team cadence. An infrastructure team that deploys weekly needs a different review rhythm than a research team that deploys quarterly. Forcing all teams into a single review cadence either slows the fast teams unnecessarily or rushes the careful teams dangerously.

**Domain-specific evaluation criteria** belong to the domain. The lending team knows what fairness metrics matter for lending decisions. The content moderation team knows what safety benchmarks matter for user-generated content classification. The governance team sets the requirement that evaluation must occur. The domain team determines the evaluation criteria.

## The Hub-and-Spoke Operating Model

The operating model that balances standardization with localization is **hub-and-spoke governance**. The central governance team -- the hub -- owns the shared standards: methodology, documentation format, incident reporting, audit trails, minimum controls. They also own the governance technology platform, the pattern catalog, and the aggregate portfolio view. The hub does not review every system. It designs the system that reviews systems.

Each major team or business unit has a **governance liaison** -- the spoke. The liaison is embedded in the team, understands the domain context, and adapts the central standards to local needs. They conduct first-line risk assessments using the central methodology with domain-specific calibration. They ensure documentation meets the central format while containing domain-relevant content. They escalate novel risks or classification uncertainties to the hub.

The liaison is not a full-time governance role in most organizations. It is a responsibility carried by a senior engineer, a technical lead, or a product manager who spends ten to twenty percent of their time on governance activities. The critical requirement is that the liaison has enough domain expertise to make credible risk judgments and enough organizational trust that the team takes governance seriously when the liaison raises concerns.

A technology company with eight product teams and over forty AI systems in production implemented hub-and-spoke governance with a central team of three and eight liaisons. Before the model, the central team was reviewing every system -- a process that took an average of twenty-three days per review and generated constant friction with product teams who felt the central reviewers did not understand their domain. After implementing hub-and-spoke, the average review time for standard deployments dropped to seven days. The liaisons handled eighty percent of reviews locally. The central team focused on high-risk systems, cross-team calibration, and governance system improvements. Product team satisfaction with the governance process doubled in their quarterly survey.

## Cross-Team Calibration

Even with shared methodology, different teams will interpret risk differently. Cross-team calibration is the mechanism that catches and corrects this drift before it produces the scenario that opened this subchapter.

**Calibration sessions** bring governance liaisons from all teams together quarterly to review a set of recent risk classifications. Each liaison presents two or three of their team's recent classifications, explaining the reasoning. The group discusses disagreements -- not to override local judgment, but to surface inconsistencies that signal methodology gaps. When the lending team's Tier 2 and the marketing team's Tier 2 represent fundamentally different risk levels, the calibration session is where that discrepancy becomes visible.

The output of each calibration session is a set of calibration notes that refine the shared methodology. Maybe the severity scale needs domain-specific anchors -- concrete examples of what "high severity" means in lending vs. marketing vs. fraud detection. Maybe a new risk dimension needs to be added because one team identified a risk type that the methodology did not address. Maybe the thresholds need adjustment because early classifications were too conservative or too lenient. The methodology evolves through calibration, not through top-down decree.

## Shared Services and Reusable Assets

Central governance teams create the most value when they build reusable assets that every team can leverage. Pre-approved patterns, documented in the pattern catalog from subchapter 10.3, are the most impactful. When the central team approves a deployment pattern once and publishes it for all teams, every subsequent team saves weeks of review time.

Beyond patterns, shared services include governance templates that teams fill in rather than creating from scratch, training programs that teach consistent risk assessment methodology, tooling for automated risk classification and evidence validation, and a library of evaluation benchmarks appropriate to common use cases. Each shared service reduces the per-team cost of governance while improving consistency. The economic logic is simple: if eight teams each spend twenty hours building their own risk assessment template, the organization has spent a hundred and sixty hours on work that a single well-designed template could have eliminated.

## Managing Acquisitions

When your organization acquires a company with its own AI systems, you inherit one of three situations: they have governance that is incompatible with yours, they have governance that partially overlaps with yours, or they have no governance at all. Each requires a different integration approach.

For acquisitions with no governance, the priority is immediate: inventory every AI system, assign owners, conduct rapid risk assessments, and identify any systems that pose regulatory or reputational risk requiring immediate intervention. This is triage, not full governance integration. Full integration happens over the subsequent two to three quarters as the acquired team adopts your shared standards, documentation format, and review processes.

For acquisitions with incompatible governance, the challenge is political as much as operational. The acquired team has invested in their framework and believes it works. The integration approach should map their existing classifications to your methodology, identify gaps in either direction, and negotiate a migration timeline that respects their existing commitments while converging on your shared standards. Mandating immediate adoption of your framework without acknowledging what they built generates resentment and compliance resistance.

For acquisitions with partially overlapping governance, the opportunity is learning. They may have solved problems you have not encountered. Their methodology may include risk dimensions yours lacks. The integration should be a two-way conversation that strengthens both frameworks rather than a one-way migration that assumes yours is superior.

## The Federated Governance Council

At the organizational level, multi-team governance needs a coordination mechanism above the hub-and-spoke model. The **federated governance council** fills this role. It includes the central governance lead, one representative from each major team or business unit, and cross-functional members from legal, compliance, and security.

The council meets monthly or quarterly, depending on the pace of AI expansion. Its purpose is not to review individual systems -- that is the spoke's job. Its purpose is to set and evolve the shared standards, review cross-team calibration results, approve changes to the methodology, address governance conflicts between teams, and ensure that the governance program reflects the organization's evolving risk landscape.

The council also provides buy-in. When standards are set by a central team without input from the teams that must follow them, compliance is grudging. When standards are set by a council that includes representatives from every major team, compliance is ownership. The representative who helped design the risk methodology goes back to their team and champions it -- not because the central team told them to, but because they shaped it.

The final subchapter addresses the question that every governance conversation eventually reaches: what does this cost, and how do you justify the investment to leadership that wants to spend every dollar on product?
