# 29.5.4 — Model Validation: Testing, Approval Workflows, and Pre-Deployment Gates

In late 2024, a mid-size insurance company deployed a claims triage model that routed incoming claims to adjusters based on predicted complexity and estimated payout. The model had been tested against a held-out evaluation set and achieved 91 percent accuracy on claim categorization. Leadership approved deployment based on that single metric. Within six weeks, regulators flagged the system after policyholders in three zip codes — all majority-minority communities — experienced denial rates 2.4 times higher than the portfolio average. The model had learned to associate geographic proxies with claim outcomes, and nobody had tested for demographic disparities because the validation process did not require it. The company pulled the model, paid $2.8 million in remediation and legal costs, and spent four months rebuilding a validation framework that should have existed before a single prediction reached production. The accuracy metric was real. The model genuinely categorized claims correctly 91 percent of the time. But accuracy was the wrong question, and nobody had defined the right questions before development began.

## The Governance Principle: Define the Bar Before You Build

**Model validation** is the governance process that confirms an AI model meets the organization's quality, safety, and compliance standards before it enters production. The critical word is "before." Validation criteria must be established before development begins, not negotiated after the model is built and the team is eager to ship. When the bar is defined retroactively, it bends to fit whatever the model can achieve. When the bar is defined in advance, the model must rise to meet it.

This is not about slowing teams down. It is about ensuring that every team knows exactly what "ready for production" means before they write a single line of training code. The validation criteria become the contract between the development team and the organization. If the model meets the criteria, it ships. If it does not, the team knows what to fix. There is no ambiguity, no negotiation, no pressure to lower the bar because the deadline is next week.

The practical mechanism is a **validation specification** — a document created during the project's design phase that defines every test the model must pass, every metric that must meet a threshold, every analysis that must be completed, and every reviewer who must sign off. The specification is reviewed and approved by the governance function before development begins. Any change to the specification after development starts requires the same approval process. This prevents the most common governance failure: the quiet erosion of validation standards to accommodate a model that almost meets the original bar.

## Risk-Tier-Based Validation

Not every model needs the same level of scrutiny. A model that recommends blog articles to users poses different risks than a model that approves mortgage applications. Your validation framework must scale its requirements to the risk the model poses — and the risk tier, discussed in earlier chapters, is the mechanism that determines how much validation is enough.

For **low-risk systems** — internal productivity tools, content recommendations with human curation, search ranking — the validation specification typically requires accuracy evaluation against a representative test set, basic performance benchmarking across key segments, and a review by the owning team's technical lead. The approval workflow is lightweight: one technical reviewer, one product owner, documented results in the model registry. The total validation process might take two to three days.

For **medium-risk systems** — customer-facing chatbots, automated email classification, pricing recommendations that influence but do not determine outcomes — the specification adds requirements. You need fairness testing across defined demographic groups, performance evaluation under distribution shift scenarios, a review of failure modes and their user impact, and sign-off from both a technical reviewer and a governance representative. The total process typically runs one to two weeks.

For **high-risk systems** — credit decisions, medical triage, hiring screening, insurance underwriting, any system the EU AI Act classifies as high-risk under Annex III — the specification becomes comprehensive. You need everything the medium tier requires plus adversarial robustness testing, red-teaming for harmful outputs, demographic subgroup analysis across every protected characteristic relevant to the use case, stress testing under edge-case inputs, documentation sufficient for regulatory inspection, and approval from an independent validation team that did not participate in development. At regulated financial institutions, this aligns with the independent validation requirement from SR 11-7, which we cover in the next subchapter. The total process runs two to six weeks depending on system complexity.

The risk tier is not optional. Every model in your registry must have a risk classification, and that classification determines which validation specification template applies. If a team disputes the classification, the dispute is resolved before development begins — not when the model is waiting at the deployment gate.

## The Approval Workflow: Who Reviews, Who Approves, What Evidence Is Required

Validation is not just testing. It is testing plus judgment plus accountability. The approval workflow defines who must review the validation evidence, who has the authority to approve deployment, and what evidence package must be assembled before any reviewer sees it.

A well-designed approval workflow has three stages. First, the development team completes all validation tests and assembles the evidence package. This package includes the validation specification, the test results for every required metric, any failure analysis for metrics that did not initially pass, documentation of any changes made to the model after initial testing, and a summary of known limitations. Second, the designated reviewers evaluate the evidence. For low-risk systems, this is one technical reviewer. For high-risk systems, this is an independent validation team plus a governance representative plus, in regulated industries, a model risk management function. Third, the approver — the person with formal authority to approve deployment — makes the decision based on the reviewers' assessments. The approver's identity and the approval decision are recorded immutably in the model registry.

The key principle is separation of concerns. The people who built the model should not be the people who approve it for deployment. At minimum, the approver must be someone who did not participate in development. For high-risk systems, the entire validation review should be conducted by people who are organizationally independent of the development team. This is not bureaucracy. It is the mechanism that prevents confirmation bias from masquerading as validation.

## Pre-Deployment Gates as Automated Checkpoints

Manual approval workflows work for organizations deploying a handful of models per year. They break when you are deploying dozens. **Pre-deployment gates** are automated checkpoints that verify specific validation requirements before a model can proceed through the deployment pipeline. They are the technical enforcement layer beneath the governance process.

A pre-deployment gate works like a CI/CD quality gate in software engineering. Before a model artifact can be promoted from staging to production, the deployment pipeline checks a set of conditions. Does the model have a signed validation specification in the registry? Have all required test suites been executed and recorded? Do all metrics meet their defined thresholds? Has the required approval been recorded? Is the model documentation complete? If any condition fails, the pipeline stops. No human can override the gate without a formal exception process that is itself logged and auditable.

The most effective organizations layer their gates. The first gate checks documentation completeness — no model proceeds without a completed model card and validation specification. The second gate checks automated test results — accuracy, latency, fairness metrics must all meet thresholds. The third gate checks human approvals — the required reviewers must have recorded their decisions. Each gate catches a different category of readiness failure, and the layering ensures that no single point of failure can let an unvalidated model reach production.

## Common Validation Failures

Even organizations with formal validation processes make predictable mistakes. The most dangerous is **non-representative test data** — evaluating a model on data that does not reflect the population the model will serve in production. A hiring model tested on historical data from a single office in San Francisco will not reveal how it performs on candidates from rural markets, different educational systems, or non-English-dominant backgrounds. The fix is explicit: the validation specification must define the demographic, geographic, and use-case segments the test data must cover, and the data team must verify coverage before testing begins.

The second most common failure is **single-metric validation** — the insurance company problem from the opening. Accuracy alone tells you nothing about fairness, robustness, calibration, or failure-mode severity. Your validation specification must define metrics across multiple dimensions: task performance, fairness across protected groups, calibration of confidence scores, robustness under input perturbation, and latency under expected load. A model that scores 93 percent on accuracy but shows a 15-percentage-point performance gap between demographic groups has not passed validation, no matter how good the headline number looks.

The third failure is **skipping adversarial testing**. Standard test sets measure how the model performs when inputs look like the training data. Adversarial testing measures how the model performs when someone is trying to break it — or when production inputs drift in unexpected directions. For any model that accepts user-generated input, adversarial testing is not optional. Prompt injection, input manipulation, and edge-case exploitation are production realities, and a model that was never tested against them is a model that will fail in ways your standard metrics never predicted.

The fourth failure is **validation theater** — running all the required tests but treating the results as a checkbox rather than a decision input. If a fairness test reveals a disparity and the team documents the disparity but deploys anyway without mitigation, the validation process has produced evidence of a problem while providing no governance value. Validation must have teeth. A failed test must trigger a defined response: remediation, escalation, or a formal exception with documented justification and compensating controls.

## Integrating Validation with the AI Decision Stack

Model validation sits at Layer 3 of the AI Decision Stack — Technical Validation. It connects downward to the development process (Layer 4, where the model is built) and upward to the business approval process (Layer 2, where the deployment decision is made in the context of business risk and strategic alignment). Validation is the bridge that translates technical test results into governance evidence that business decision-makers can act on.

This means the validation output must be interpretable by non-technical stakeholders. A validation report that consists entirely of metric tables and confusion matrices is useful for the technical reviewers but useless for the governance board member who needs to approve a high-risk deployment. The validation summary should translate technical results into risk language: "The model meets all accuracy thresholds. Fairness testing identified a 4-percentage-point performance gap between age groups, which is within our defined tolerance of 5 percentage points. Adversarial testing identified two prompt injection vectors that were mitigated in version 2.3. The independent validation team recommends approval with the condition that the age-group gap is monitored monthly in production." That summary gives a governance reviewer enough information to make a decision without requiring them to interpret a ROC curve.

The next subchapter covers model risk management and how the principles of SR 11-7 — originally designed for financial models — apply to AI systems of every kind.
