# 29.4.4 — GPAI Obligations and the Code of Practice: What Model Providers Must Do

Who is responsible when an AI system built on someone else's model causes harm? If your customer service chatbot hallucinates legal advice because the underlying foundation model was trained on questionable data, who owns that failure — you, or the company that built the model? The EU AI Act answers this question with a framework that most organizations have not fully internalized: both parties have obligations, and neither can point at the other to escape them. The Act creates a distinct regulatory category called **General-Purpose AI** — GPAI — and imposes specific obligations on the companies that provide these models, separate from the obligations placed on the companies that deploy AI systems built on top of them. Understanding this distinction is not optional. If you build products on GPT-5, Claude Opus 4.6, Gemini 3, or any other foundation model, you are a downstream deployer with obligations that depend directly on your model provider meeting theirs.

## What Counts as a GPAI Model

A **GPAI model** under the EU AI Act is an AI model that displays significant generality, is capable of competently performing a wide range of distinct tasks regardless of how it was placed on the market, and can be integrated into a variety of downstream systems or applications. This definition captures every major foundation model on the market in 2026. GPT-5, Claude Opus 4.6, Gemini 3 Pro, Llama 4 Maverick, DeepSeek V3.2, Mistral Large 3 — all qualify. The definition is deliberately broad. It covers models released through APIs, models released as open weights, and models distributed through licensing agreements. If a model is general-purpose and can be integrated into downstream systems, it is a GPAI model regardless of how it reaches you.

The Act does not regulate GPAI models the same way it regulates high-risk AI systems. GPAI model providers are not required to conduct conformity assessments or apply CE markings. Instead, they have a separate, parallel set of obligations focused on transparency, documentation, and — for the most capable models — safety. This parallel track matters because it means your foundation model provider has compliance obligations they must meet independently of anything you do as a deployer.

## Two Tiers: Standard GPAI and Systemic Risk

The Act creates two tiers of GPAI obligation. All GPAI model providers face a baseline set of requirements. A smaller subset faces additional, more demanding requirements because their models are classified as presenting **systemic risk**.

The systemic risk classification is triggered by a quantitative threshold: any model trained using a cumulative compute of ten to the power of twenty-five floating point operations or more is presumed to present systemic risk. As of early 2026, this threshold captures models from OpenAI, Anthropic, Google DeepMind, and likely xAI. The European Commission can also designate a model as presenting systemic risk based on qualitative criteria — the model's capabilities, its reach across the EU market, the number of registered users, or other factors the Commission determines are relevant. Providers must notify the Commission within two weeks of reaching or reasonably foreseeing that their model will reach the compute threshold. A provider can attempt to rebut the presumption of systemic risk, but the burden of evidence falls entirely on them.

This two-tier structure means that most organizations deploying AI systems are using GPAI models with systemic risk — because the most commercially popular models are the ones trained with the most compute.

## Obligations for All GPAI Providers

Every provider of a GPAI model, regardless of systemic risk classification, must meet three categories of obligation. First, they must create and maintain **technical documentation** that describes the model's capabilities, limitations, training process, and intended and foreseeable uses. This documentation must be comprehensive enough that downstream deployers — you — can understand what the model does, what it does not do well, and how to integrate it responsibly into your own systems. The documentation must be stored securely for at least ten years and made available to the EU AI Office upon request.

Second, GPAI providers must establish a **copyright compliance policy**. This means they must document their approach to identifying and respecting copyright reservations, including using state-of-the-art technologies to comply with opt-out mechanisms. They must provide a sufficiently detailed summary of the training data used for the model — not the full dataset, but enough information for rights holders to understand whether their content was included. They must only scrape lawfully accessible content and must not deliberately circumvent technical protections on copyrighted material. For organizations consuming GPAI models, this obligation matters because downstream liability for copyright infringement in AI-generated outputs remains an active legal question across jurisdictions.

Third, GPAI providers must meet **transparency obligations** toward downstream deployers. They must provide information and documentation sufficient to enable you to understand the model's capabilities and limitations and to comply with your own regulatory obligations. This is the critical link in the value chain. Your ability to meet your obligations as a high-risk AI system provider or deployer depends on the information your GPAI model provider gives you.

## Additional Obligations for Systemic Risk Models

Providers of GPAI models with systemic risk face four additional requirements on top of the baseline obligations. They must conduct and document **model evaluations** — standardized assessments of the model's capabilities and limitations, including adversarial testing to identify risks. They must assess and mitigate **systemic risks** that the model may pose, including risks to public health, safety, fundamental rights, and societal well-being. They must establish and maintain **incident reporting procedures** — when a serious incident involving the model occurs, the provider must report it to the AI Office without undue delay. And they must implement adequate **cybersecurity protections** for the model and its infrastructure.

These additional obligations are significant because they require systemic risk providers to actively search for dangers in their own models, not merely document what the models do. The adversarial testing requirement means providers must try to break their own models in structured, documented ways. The incident reporting requirement means providers must maintain monitoring infrastructure that detects when their models cause or contribute to serious harm. For organizations using these models, the systemic risk obligations create a compliance dependency: if your provider cuts corners on model evaluation or incident reporting, the risks flow downstream to you and your users.

## The GPAI Code of Practice: A Compliance Pathway

The **GPAI Code of Practice**, published on July 10, 2025, provides a voluntary but strategically important compliance pathway for GPAI model providers. The Code was developed through a structured drafting process involving model providers, downstream deployers, civil society, academic experts, and the European Commission's AI Office. The final version is organized into three chapters: Transparency, Copyright, and Safety and Security. The Transparency and Copyright chapters apply to all GPAI model providers. The Safety and Security chapter applies only to providers of systemic risk models.

What makes the Code strategically important is the **presumption of conformity** it provides. The European Commission and the AI Board confirmed in August 2025 that the Code is adequate for demonstrating compliance with the AI Act's GPAI obligations. A provider that signs the Code and adheres to its commitments benefits from reduced administrative burden, fewer information requests from the AI Office, and greater legal certainty than providers who attempt to demonstrate compliance through alternative means. The presumption is not absolute — signing the Code does not make a provider immune from enforcement — but it significantly shifts the burden. The AI Office will account for Code commitments when assessing potential fines.

Crucially, the Code allows partial adoption. A provider that adheres to the Transparency and Copyright chapters but not the Safety and Security chapter receives the presumption of conformity only for transparency and copyright obligations. This flexibility means you should ask your model providers not just whether they signed the Code, but which chapters they committed to.

## What Downstream Deployers Need from Providers

If you are building AI systems on top of GPAI models — which, in 2026, describes the majority of organizations deploying AI — you occupy the **downstream deployer** role in the Act's value chain. Your obligations as a deployer of a high-risk AI system are independent of your provider's obligations. You must conduct your own risk assessments, implement your own monitoring, and maintain your own documentation. But your ability to do this well depends on the information your GPAI model provider gives you.

You need technical documentation sufficient to understand the model's training data sources, known limitations, performance characteristics, and failure modes. You need transparency about the model's copyright compliance so you can assess your own exposure. You need assurance that the provider has conducted evaluations and adversarial testing — particularly if you are deploying the model in a high-risk use case where a systematic failure could cause serious harm. Without this information, you are building a compliant system on an opaque foundation, which is a contradiction the Act explicitly addresses by placing documentation obligations on the provider.

## The Vendor Contract Imperative

The practical implication is blunt: your vendor contracts must require your GPAI model providers to share the documentation and transparency information you need to meet your own obligations. If your contract with OpenAI, Anthropic, Google, or any other provider does not include clauses requiring them to provide technical documentation, training data summaries, copyright compliance information, and evaluation results, you have a compliance gap that no amount of internal governance can fill. The Act places the documentation obligation on the provider, but the Act does not enforce your contract. You must enforce it yourself.

This means your procurement and legal teams need to understand GPAI obligations well enough to negotiate contracts that produce the right deliverables. A generic enterprise agreement that covers uptime and data processing terms is not sufficient. You need specific contractual commitments to provide the information described in Articles 53 and 55 of the Act, along with mechanisms for updates when the model changes. If your provider refuses to include these commitments, that refusal is itself a risk factor you need to document and escalate to your governance board.

The GPAI framework fundamentally changes how organizations should evaluate model providers. Technical capability and pricing are no longer sufficient selection criteria. Regulatory compliance posture — whether a provider has signed the Code of Practice, what documentation they provide, whether they conduct and share adversarial testing results — is now a procurement requirement on equal footing with performance benchmarks. With the provider obligations understood, the next subchapter addresses the deadline that makes all of this urgent: August 2, 2026, and what must be ready by then.
