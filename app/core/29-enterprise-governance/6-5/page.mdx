# 29.6.5 — Synthetic Data Governance: When Machines Generate Your Training Data

If your training data was generated by another AI model, who is responsible for its quality, its biases, and its legal status? The question sounds philosophical. It is not. It is the operational governance question that every team using synthetic data must answer before that data enters a training pipeline, and the answer is more complicated than most organizations expect.

**Synthetic data** is data generated by AI models rather than collected from real-world sources. A team that needs ten thousand customer service conversations to fine-tune a support bot can generate them using a large language model instead of collecting and annotating real conversations. A healthcare organization that cannot share patient records can generate synthetic patient data that preserves statistical patterns without containing any real patient's information. A financial services company that needs fraud examples can generate synthetic fraud scenarios rather than waiting for real fraud to occur. The use cases are compelling, the generation is fast, and by 2026 synthetic data has become a standard component of training pipelines across industries.

The governance challenge is that synthetic data inherits risks from the model that generated it, introduces new risks that real data does not carry, and exists in a regulatory grey zone that is only beginning to be clarified.

## Provenance: Where Did This Data Come From?

Real data has a clear provenance chain. Customer service transcripts came from your call center. Medical records came from your EHR system. Financial transactions came from your payment processor. You can trace each record to its origin, verify its authenticity, and document its chain of custody. Synthetic data has a fundamentally different provenance: it came from a model. That model was trained on some other data, which was itself collected or generated through some process you may or may not have visibility into.

This provenance gap creates governance problems at every level. When you fine-tune a model on synthetic data generated by GPT-5.2, the quality and characteristics of your training data depend on GPT-5.2's training data — which OpenAI does not fully disclose. If GPT-5.2 was trained on data that included copyrighted material, biased datasets, or factual errors, those characteristics propagate into your synthetic data and from there into your fine-tuned model. You have introduced a dependency on a system you cannot audit, trained on data you cannot inspect, with characteristics you cannot fully verify.

Your governance framework must require **provenance documentation** for all synthetic data. At minimum, this documentation should capture which model generated the data, which version of the model was used, what prompts or instructions were used to generate the data, when the data was generated, what quality filters were applied after generation, and what validation was performed against known ground truth. This documentation enables traceability — when a problem surfaces in the fine-tuned model, you can trace it back through the synthetic data to the generation process and identify whether the problem originated in the generation model, the generation prompts, or the post-generation filtering.

## Quality: The Amplification Problem

The most dangerous assumption about synthetic data is that it is "good enough" because it was generated by a capable model. This assumption ignores a fundamental property of synthetic data generation: the model generates what it has learned, and what it has learned includes the biases, gaps, and errors in its own training data.

A large language model asked to generate customer service conversations will produce conversations that reflect the patterns in its training data. If its training data over-represented complaints about shipping and under-represented complaints about accessibility, the synthetic data will carry the same imbalance. If its training data contained stereotypical language patterns associated with particular demographics, the synthetic conversations will reproduce those stereotypes. The model does not generate neutral data. It generates data that reflects its own learned distribution, biases included.

This is the **amplification problem**: when you use synthetic data to train a new model, you are training on the generating model's biases, not on reality. Worse, if you generate synthetic data, train a model on it, and then use that model to generate more synthetic data for the next round of training, the biases compound with each generation. This is sometimes called **model collapse** — the progressive degradation of model quality through recursive training on synthetic data. Research from 2024 and 2025 documented this phenomenon across language models, image generators, and tabular data synthesizers. By the third or fourth generation, the synthetic data distribution has drifted substantially from the original real-data distribution, and models trained on it perform measurably worse on real-world tasks.

Your governance framework must require quality validation for all synthetic data before it enters a training pipeline. Validation should compare the synthetic data's statistical properties against a held-out sample of real data. Distribution matching — verifying that the synthetic data's feature distributions, label distributions, and correlation structures align with real data — is necessary but not sufficient. You also need task-level validation: does a model trained on this synthetic data perform comparably to a model trained on equivalent real data when evaluated on a real-world test set? If synthetic data degrades task performance, it does not matter how realistic it looks statistically.

## Privacy: The False Comfort of Synthetic Data

One of the primary motivations for synthetic data is privacy. If the data was generated by a model rather than collected from real people, it should not contain anyone's personal information. This reasoning is intuitive. It is also dangerously incomplete.

Synthetic data generated from real data can leak information about the real data it was derived from. A model trained on real patient records and then used to generate synthetic patient records may produce records that are close enough to real patients to enable **reconstruction attacks** — techniques that work backward from the synthetic data to identify or partially reconstruct real individuals in the original dataset. The risk is highest when the synthetic data is generated to be highly realistic, because realism and privacy are in direct tension. The more faithfully the synthetic data reproduces the patterns in the real data, the more information it carries about the real individuals in that data.

The EDPB's Opinion 28/2024 addressed this directly, introducing a three-step test for determining whether synthetic data qualifies as anonymous under GDPR. First, was the data derived from personal data? If yes, the generation process itself constitutes processing of personal data and requires a legal basis. Second, can individuals in the original dataset be identified from the synthetic data, either directly or through combination with other available information? Third, are there technical and organizational measures in place to prevent re-identification? The opinion made clear that simply calling data "synthetic" does not make it anonymous. The privacy properties depend on the generation process, the characteristics of the output, and the controls around its use.

NIST followed with SP 800-226, which provided guidance on evaluating differential privacy claims for synthetic data. The publication clarified acceptable ranges for the epsilon parameter and established that similarity metrics alone — measuring how closely synthetic records resemble real records — are not adequate proxies for privacy. A 2025 consensus panel endorsed membership and attribute disclosure rates as the preferred privacy indicators for synthetic data, replacing the earlier reliance on similarity scores that had given teams false confidence in their privacy protections.

Your governance framework must treat synthetic data as potentially re-identifiable unless proven otherwise through formal privacy evaluation. For synthetic data generated from regulated datasets — HIPAA-protected health information, financial data subject to GLBA, children's data subject to COPPA — the privacy evaluation must be rigorous, documented, and reviewed by both privacy engineers and legal counsel before the synthetic data is used in training.

## Regulatory Status: A Moving Target

Regulators are catching up to synthetic data, but the regulatory landscape in 2026 is still fragmented and evolving. No major jurisdiction has issued a comprehensive synthetic data regulation. Instead, existing frameworks are being interpreted and extended to cover synthetic data, creating a patchwork of guidance that varies by jurisdiction and sector.

The EU approach treats synthetic data generation as data processing when the generation model was trained on personal data. This means GDPR applies to the generation process itself, even if the output is anonymous. The legal basis for training the generation model on personal data must be established before the generation begins. The GPAI Code of Practice, finalized in July 2025, requires model providers to document whether their training data includes synthetic components and to describe the generation methodology.

In the United States, California's AB 2013 requires disclosure of whether synthetic data was used in training generative AI systems, effective January 1, 2026. The disclosure must describe the role of synthetic data and how it was generated. Sector-specific regulators have also begun weighing in. The UK Financial Conduct Authority issued guidance on synthetic data use in financial services, emphasizing the need for validation against real-world outcomes and warning against over-reliance on synthetic data for model validation.

For your governance framework, the regulatory uncertainty means you cannot assume synthetic data is unregulated. Design your governance as if synthetic data carries the regulatory obligations of the most sensitive real data it was derived from. If your synthetic customer service data was generated by a model trained on real customer interactions that contained PII, govern the synthetic data under your PII policies until a formal privacy evaluation demonstrates otherwise. This conservative approach costs more in process overhead, but it prevents the scenario where a regulatory clarification retroactively makes your synthetic data pipeline non-compliant.

## Labeling: Knowing What Is Synthetic

A training pipeline that mixes real and synthetic data without distinguishing between them creates an audit nightmare. When a model exhibits unexpected behavior in production — biased outputs, factual errors, distribution shift — you need to determine whether the problem originates in the real data, the synthetic data, or the interaction between them. If your training manifests do not track which examples are synthetic and which are real, this investigation is impossible.

**Synthetic data labeling** means tagging every synthetic example with metadata that identifies it as synthetic, records the model that generated it, records the generation timestamp, and links to the generation configuration. This metadata should be preserved through every stage of the pipeline — from generation through filtering, validation, augmentation, and final training set assembly. When a training example is used, you should be able to trace it back to its origin in a single query.

Labeling also enables ratio management. Industry experience consistently shows that training sets work best when synthetic data supplements real data rather than replacing it. Teams that have tested different ratios typically find that synthetic data beyond forty to sixty percent of the training set begins to degrade model quality on real-world tasks, though the exact threshold depends on the domain and the quality of the synthetic data. Without labeling, you cannot measure your synthetic-to-real ratio, and you certainly cannot manage it.

## Governance Requirements: The Synthetic Data Policy

Your synthetic data governance policy should codify five requirements. First, **provenance documentation**: every synthetic dataset must have a record of its generation model, generation process, and validation results. Second, **quality validation**: synthetic data must be validated against real data before use, with documented acceptance criteria and rejection thresholds. Third, **privacy evaluation**: synthetic data derived from regulated data must undergo formal privacy evaluation before it is cleared for use. Fourth, **labeling and tracking**: all synthetic data must be labeled as synthetic in training manifests and trackable through the full pipeline. Fifth, **ratio limits**: training pipelines must define and enforce maximum synthetic-to-real ratios based on validated performance thresholds for each use case.

These requirements add process overhead to synthetic data generation. That overhead is the cost of governing a data source that introduces model dependencies, amplification risks, privacy uncertainties, and regulatory ambiguities that real data does not. Teams that skip this governance because synthetic data "is not real data" are the teams that discover too late that their model memorized the generating model's biases, that their privacy assumptions were wrong, or that a new regulation retroactively applies to their entire training pipeline.

The next subchapter turns to a governance challenge that predates synthetic data but has been transformed by AI: copyright and intellectual property in training data, where the legal landscape is being rewritten in real time.
