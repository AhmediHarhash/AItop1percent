# 29.9.7 -- Evidence Management: Building the Audit Trail That Regulators Expect

The auditor sits across the table and asks a simple question. "Show me the risk assessment that was conducted before this model was deployed to production." The room goes quiet. The head of engineering looks at the compliance lead. The compliance lead opens a laptop and starts searching a shared drive. Someone suggests checking Confluence. Someone else remembers an email thread from the previous spring. Forty minutes later, the team produces a document that might be the risk assessment, but it is undated, unsigned, and stored in a personal folder belonging to someone who left the company four months ago. The auditor writes a note. The note does not say "risk assessment found."

This scene plays out in organizations that built governance frameworks with real controls, real review processes, and real oversight. The controls existed. The reviews happened. The decisions were made by the right people for the right reasons. But when the moment came to prove it, the evidence was scattered, incomplete, and unverifiable. **Evidence management** is the unsexy infrastructure that makes every other governance capability real. Without it, your audit readiness, your continuous assurance, and your regulatory compliance are performative. You can have the best governance framework in the industry. If you cannot produce the proof, it does not exist in the eyes of any auditor or regulator who matters.

## What Constitutes Governance Evidence

Governance evidence is not a single type of artifact. It is the complete documentary record of how your organization made decisions about AI systems, tested those decisions, monitored outcomes, and responded when something went wrong. The categories are broader than most teams realize.

**Decision records** capture who approved what and when. Model deployment approvals, risk classification decisions, exception grants, policy overrides, data usage approvals, vendor model selections. Every decision that affected an AI system's behavior, scope, or risk profile should leave a record that identifies the decision maker, the information they had at the time, the alternatives they considered, and the rationale for their choice. **Approval chains** document the sequence of reviews a decision passed through, including any conditional approvals and the evidence that conditions were subsequently met. **Risk assessments** document the identified risks, their likelihood and severity, the mitigations applied, and the residual risk that was accepted. **Test results** capture the output of control testing, evaluation runs, bias audits, and security assessments, including not just the pass or fail outcome but the methodology, the test data used, and the date of execution. **Monitoring data** captures the ongoing performance, fairness, and safety metrics that demonstrate your system behaved as expected between audits. **Exception approvals** capture every instance where a governance control was bypassed, waived, or modified, including who authorized the exception and why.

Each of these categories matters because auditors do not audit systems. They audit decisions. They want to know that the right people considered the right information and made defensible choices. The evidence is what lets you demonstrate that chain.

## Evidence Architecture: Centralized vs. Distributed

The architectural question is whether to centralize all governance evidence in a single repository or to collect evidence where it naturally lives and index it centrally. Both approaches work. Both can fail. The choice depends on your organization's existing tooling landscape and your appetite for change management.

A centralized repository means every governance artifact is stored in one system designed for immutability, versioning, and retrieval. The advantage is simplicity: auditors get one place to search, metadata is consistent, and retention policies apply uniformly. The disadvantage is adoption friction. Engineers do not want to leave their CI/CD pipeline to log an approval in a separate system. Product managers do not want to write risk assessments in a tool they use for nothing else. If adoption is low, the centralized repository becomes a graveyard of incomplete records while the real evidence lives in Jira tickets, Slack threads, and email chains.

A distributed collection model accepts that evidence will be generated in existing tools — your deployment pipeline, your model registry, your ticketing system, your communication platforms — and builds a central index that catalogs what evidence exists, where it lives, and how to retrieve it. The advantage is lower adoption friction. Teams continue using their existing tools, and the evidence management layer pulls metadata into a searchable index. The disadvantage is complexity: you must build and maintain integrations, handle format inconsistencies, and ensure that evidence in distributed systems is not deleted or modified after the fact.

Most mature organizations in 2026 use a hybrid approach. High-criticality evidence — risk assessments, deployment approvals, regulatory submissions — lives in a centralized, immutable store. Operational evidence — test results, monitoring data, evaluation outputs — lives in existing systems with automated indexing into the central catalog. The central catalog is the auditor's entry point. It answers the question "does evidence exist for this decision?" and provides a link to the canonical artifact, wherever it is stored.

## Immutability and Integrity

Evidence that can be modified after the fact is not evidence. It is narrative. Regulators and auditors require governance artifacts to be tamper-resistant, timestamped, and attributable to a specific person or system. This is not a theoretical concern. In regulatory proceedings, contested evidence creates more organizational damage than missing evidence. Missing evidence suggests a gap in your process. Modified evidence suggests a gap in your integrity.

Immutability does not require blockchain. It requires write-once storage with integrity verification. Object storage with versioning enabled ensures that every artifact retains its complete version history. Cryptographic hashing at the time of creation lets you verify that an artifact has not been altered since it was stored. Audit logging on the evidence store itself records every access, every retrieval, and every attempted modification. The combination of these three mechanisms — versioned storage, hash verification, and access logging — provides the tamper-resistance that auditors expect without introducing exotic technology that your team cannot maintain.

Attribution is equally important. Every artifact must identify who created it, when it was created, and in what context. An undated risk assessment without an author is nearly worthless to an auditor. Automated evidence capture — where your deployment pipeline automatically records the approver, the timestamp, and the evaluation results alongside the deployment event — eliminates the attribution gap that manual processes inevitably create.

## Retention Requirements and the GDPR Tension

The EU AI Act, under Article 18, requires providers of high-risk AI systems to keep documentation for ten years after the system has been placed on the market or put into service. This includes technical documentation, quality management system records, conformity assessment documentation, and records of any changes approved by notified bodies. The ten-year clock starts from the last unit placed on the market or the last day the system was in service, whichever is later. If your model served its last customer in 2028, you must retain the documentation until 2038.

This creates a direct tension with GDPR, which requires that personal data be deleted when it is no longer necessary for the purpose for which it was collected. The resolution lies in understanding what the AI Act requires you to retain. The ten-year obligation applies to documentation and metadata — the technical specifications, the risk assessments, the test results, the governance decisions. It does not require you to retain the raw training data or the personal data that flowed through the system. You can and should delete personal data according to GDPR timelines while retaining the governance documentation that describes how that data was used, protected, and monitored. Building this distinction into your retention architecture from day one saves your legal team from an impossible reconciliation exercise later.

Sector-specific requirements add additional layers. Financial services firms in the US face SEC and FINRA recordkeeping rules that may require seven-year retention of communications related to AI-driven investment decisions. Healthcare organizations must retain records related to clinical decision support for the duration required by HIPAA and state medical records laws, which can extend to thirty years in some jurisdictions. Map your retention requirements by sector and jurisdiction before you design your storage architecture, because retrofitting retention policies onto an existing evidence store is significantly harder than building them in from the start.

## The 24-Hour Test

Can your organization produce any governance artifact within twenty-four hours of a regulatory request? This is the evidence management readiness test, and most organizations fail it the first time they try. The failure is rarely because the evidence does not exist. It is because the evidence cannot be found, cannot be verified, or cannot be assembled into a coherent narrative within the time constraints regulators impose.

Run the test internally before a regulator runs it for you. Select a random AI system from your inventory. Ask your governance team to produce, within one business day, the complete evidence package: the risk classification and the rationale behind it, the most recent risk assessment, the deployment approval and the evaluation results that supported it, the most recent control test results, the current monitoring dashboard or data export, and any incidents or exceptions related to the system in the last twelve months. Time the exercise. Identify where the delays occur. Most teams find that the evidence exists across five or more systems, that no single person knows where all of it lives, and that assembling the package requires coordination across teams that have never practiced this retrieval together.

Organizations that pass the 24-hour test consistently share three characteristics. They maintain a per-system evidence index that maps every required artifact to its storage location and retrieval method. They assign evidence ownership so that someone is accountable for each artifact category's completeness. And they run the retrieval exercise quarterly, treating it as a drill rather than an emergency.

## Common Evidence Management Failures

Five patterns account for most evidence management failures. **Scatter** is the most common: evidence exists but lives in dozens of tools with no central index, making retrieval a treasure hunt. **Context loss** occurs when evidence is stored without the metadata needed to interpret it — a test result file with no record of what model version was tested, what dataset was used, or what the pass criteria were. **Verification gaps** emerge when evidence cannot be authenticated — unsigned documents, undated assessments, artifacts in editable formats stored without version control. **Vendor blind spots** appear when your evidence management covers internally developed models but ignores the governance documentation for vendor-provided AI services, creating a gap that is invisible until an auditor asks about your third-party models. **Temporal gaps** arise when evidence management starts at a specific date and everything before that date is unrecoverable — a common pattern when organizations adopt governance frameworks retroactively and discover that their earliest AI deployments have no documentation at all.

Each of these failures is preventable, but only if you address them before the audit, not during it. A quarterly evidence completeness review — checking that every AI system in your inventory has a complete, current, retrievable evidence package — catches gaps while there is still time to fill them. The review should produce an evidence coverage score for each system, and systems with low coverage should be flagged for immediate remediation or, in extreme cases, for decommissioning if the evidence gap cannot be closed.

## Building the System

The evidence management system does not need to be a custom-built platform. Many organizations in 2026 use a combination of existing tools — a document management system with versioning for high-criticality artifacts, their model registry for evaluation results, their CI/CD pipeline for deployment records, and a lightweight metadata catalog that indexes across all of them. The catalog is the critical piece. It provides the single pane of glass that auditors use to navigate your evidence landscape, and it enforces the metadata standards — artifact type, system identifier, creation date, author, retention period — that make evidence retrievable and interpretable.

Access controls on the evidence store matter. Governance evidence should be readable by audit and compliance teams, writable only through controlled processes, and deletable only by retention policy automation — never by individual users. The principle is simple: once evidence is created, no individual should be able to make it disappear. This is not about distrusting your team. It is about ensuring that when a regulator asks whether your evidence store can be tampered with, your answer is backed by architecture, not by trust.

The evidence management system is the foundation that makes regulatory inspection survivable. Without it, inspection preparation becomes a scramble. With it, inspection preparation becomes retrieval. The next subchapter addresses what happens when the regulator arrives — what they actually ask for, how to manage the interaction, and how to prepare before the first request lands on your desk.
