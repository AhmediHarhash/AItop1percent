# 29.8.7 — The Near-Miss Program: Learning from Failures That Almost Happened

In September 2025, a healthcare technology company deploying a clinical decision support model noticed something during a routine quality review. A human reviewer caught the model recommending a medication dosage that, if followed, would have exceeded safe limits for patients with renal impairment. The reviewer corrected the output, the patient never saw it, and no harm occurred. The reviewer mentioned it to her team lead, who noted it was "a close one" and moved on. No report was filed. No investigation was launched. No systemic analysis explored why the model generated that recommendation or whether similar outputs had slipped through without a reviewer catching them. Six months later, in March 2026, the same model produced a similar dosage error during an overnight period when reviewer staffing was reduced. This time, no human intercepted the output. A patient received the recommendation, followed it, and was hospitalized. The post-incident investigation found that the model had been generating dosage errors at a low but consistent rate for months. The signals were there. They were caught, corrected, and forgotten.

That six-month gap between the near-miss and the incident is where most governance programs fail. Not because they lack incident response playbooks or regulatory reporting procedures, but because they have no systematic mechanism for capturing the events that almost became incidents. The near-miss was the fire alarm. The organization heard it, silenced it, and went back to work.

## Why Near-Misses Are More Valuable Than Incidents

An incident tells you what went wrong after the damage is done. A near-miss tells you what almost went wrong while you still have time to prevent it. This distinction is not subtle. It is the difference between reactive governance and proactive governance, between an organization that learns from crises and one that learns before crises occur.

**Heinrich's Law**, derived from decades of industrial safety research, established a foundational ratio: for every major accident, there are roughly twenty-nine minor incidents and three hundred near-misses. Frank Bird expanded this in the 1960s based on analysis of 1.7 million accident reports, finding an even steeper pyramid — one serious injury for every six hundred near-misses. The exact ratios are debated, but the principle is universal and has been validated in aviation, nuclear safety, healthcare, and manufacturing. The base of the pyramid is always vastly larger than the top. Organizations that study the base prevent the top.

AI systems produce near-misses constantly. A content moderation model flags a borderline piece of content that a reviewer overrides. A recommendation engine suggests an inappropriate product that a business rule filter catches. A chatbot generates a hallucinated claim that a fact-checking layer removes before the user sees it. Each of these is a near-miss. Each reveals a failure mode in the model that your controls happened to catch this time. The question is whether your organization treats these as data or as noise.

## What Counts as an AI Near-Miss

Before you can build a near-miss program, you need a precise definition. A **near-miss** is any event where an AI system produced or nearly produced an output that would have caused harm — to a user, to the organization, to a third party, or to regulatory standing — but was prevented by an existing control, by an alert individual, or by luck. The "or by luck" clause matters. Some near-misses are caught by controls working as designed. Others are caught because someone happened to be looking at the right log at the right time. The second category is more important, because it reveals gaps in your controls.

Concrete examples help teams recognize near-misses in practice. A model generates a biased output for a protected demographic group, but a human reviewer catches it before it reaches the user. A data pipeline ingests a batch of records containing personally identifiable information that should have been excluded, but a downstream validation check flags the contamination before the data reaches training. A model's latency spikes to a level that would have triggered timeouts and degraded user experience, but the spike resolves before the autoscaler responds. A fraud detection model fails to flag a transaction pattern that, in retrospect, matched known fraud signatures, but the transaction happened to be caught by a separate manual review process. Each of these is an event that did not cause harm but easily could have under slightly different circumstances.

## The Reporting Problem: Why Near-Misses Disappear

The default state of any organization is that near-misses go unreported. This is not because people do not notice them. Engineers, reviewers, and operators see near-misses regularly. They correct the output, note it mentally, and continue with their work. The near-miss disappears into the day's tasks because reporting it feels like extra work with no clear benefit.

Three specific barriers suppress near-miss reporting. The first is friction. If reporting a near-miss requires filling out a formal incident report, scheduling a review meeting, or navigating a complex ticketing system, the effort exceeds the perceived value. The reviewer who caught the dosage error in the opening story would have had to spend twenty minutes filing a report that, as far as she knew, would sit in a queue. She had forty more reviews waiting. Rational behavior says to skip the report. The second barrier is ambiguity. Many near-misses exist in a gray zone where the person who noticed them is not sure whether the event qualifies as worth reporting. Was the model's output actually dangerous, or was it just suboptimal? Was the data quality issue a genuine risk, or was it within normal variation? Without clear criteria, people default to silence. The third barrier is the one discussed in the previous chapter on risk culture: fear that reporting a near-miss will attract blame, delay a project, or create work for the reporter's team.

## Building the Near-Miss Reporting Mechanism

The mechanism must be low-friction, clear, and safe. Low-friction means a near-miss report should take less than two minutes to submit. A short form with five fields — date, system involved, what happened, what prevented harm, and how severe the outcome could have been — is sufficient for initial capture. You can always follow up for details. But if the initial report requires a thirty-minute investigation, you will get no reports.

Clear means that every team knows exactly what to report and what not to report. Publish a near-miss definition with examples specific to your systems. Distribute a one-page reference card — not a twenty-page policy document — that gives five or six concrete scenarios and says "if you see something like this, report it here." Update the examples quarterly with real near-misses from your own organization, so the criteria stay grounded in actual experience rather than abstract categories.

Safe means that reporters are protected and recognized. A near-miss reporter is not creating a problem. They are surfacing a problem that already exists. Organizations that treat reporters as heroes — naming them in team meetings, referencing their contributions in performance reviews, sharing anonymized versions of their catches as training examples — see reporting rates increase dramatically. Organizations that treat reporters as problems see reporting rates collapse.

## Analyzing Near-Misses for Patterns

Individual near-misses are informative. Patterns across near-misses are transformative. Three near-misses in the same model within a month suggest a systemic issue that your controls are catching now but may not catch forever. Five near-misses involving the same data source suggest a pipeline problem that needs a permanent fix, not repeated last-minute catches. A cluster of near-misses following a model update suggests the update introduced a failure mode that testing did not detect. A spike in near-misses across multiple systems after a foundation model provider pushes an update suggests that the provider's change had broader effects than their release notes described.

The pattern analysis is where near-miss programs pay for themselves many times over. A fintech company that launched its near-miss program in early 2025 identified a pattern within three months: its fraud detection model was generating false negatives at elevated rates during the first seventy-two hours after each weekly data refresh. The pattern had been invisible because individual reviewers caught the errors and moved on. Aggregated across dozens of reports, the timing correlation was unmistakable. The team traced the root cause to a cache invalidation issue in the feature pipeline and deployed a fix. Without the near-miss program, that pattern would have remained hidden until a fraudulent transaction slipped through during one of those seventy-two-hour windows.

Assign someone — a governance analyst, a risk champion, a dedicated near-miss coordinator — to review incoming near-miss reports on a weekly cadence. Their job is not to investigate each individual report in depth. Their job is to spot patterns: recurring systems, recurring failure types, recurring timing, recurring conditions. When a pattern emerges, it escalates into a formal investigation. That investigation should produce a systemic fix — a monitoring rule, a validation check, a training data correction, a model constraint — that addresses the root cause, not just the symptom.

Track two metrics. The **near-miss reporting rate** measures cultural health: are people reporting? A rising rate is good. A flat rate after known deployments and changes is a warning. The **near-miss-to-incident ratio** measures control effectiveness: for every incident that reaches users, how many near-misses were caught beforehand? A high ratio means your controls are working. A low ratio — or worse, incidents with no preceding near-misses — means your controls have blind spots and your reporting has gaps.

## Feeding Near-Misses Into Governance Framework Updates

The near-miss program is not a standalone activity. It is a feedback mechanism for your entire governance framework. Every pattern identified in near-miss analysis should be traceable to a specific governance action: a new monitoring rule added, a validation check strengthened, a risk register entry updated, a playbook revised, or a training module created. If near-miss reports go into a database and nothing comes out the other side, people will stop reporting. The connection between "I reported something" and "the organization changed because of it" is what sustains the program.

This traceability also matters for regulatory compliance. Under the EU AI Act's post-market monitoring requirements, which enter full enforcement in August 2026, providers of high-risk AI systems must demonstrate that they have mechanisms for identifying and addressing risks after deployment. A near-miss program with documented governance actions resulting from pattern analysis is exactly the kind of evidence regulators expect. It shows not just that you have monitoring, but that monitoring produces systematic improvement.

Review the near-miss program's effectiveness quarterly. How many reports were received? How many patterns were identified? How many governance actions resulted? How many of those actions were verified as effective — meaning the specific near-miss pattern decreased or disappeared after the fix was implemented? This feedback loop transforms the near-miss program from a data collection exercise into a continuous improvement engine.

## The Near-Miss Maturity Curve

Most organizations start at level zero: no formal near-miss program, no reporting mechanism, no analysis. Level one introduces a reporting channel and basic tracking. Level two adds pattern analysis and connects findings to governance updates. Level three integrates near-miss data with automated monitoring, so that some categories of near-misses are detected automatically rather than depending on human observation. Level four — the target state — makes near-miss analysis a core input to every governance decision, from risk register updates to deployment approvals to model revalidation schedules.

The journey from zero to four takes most organizations eighteen to twenty-four months. But the value begins accumulating at level one. A single near-miss report that prevents a single incident can save the organization more than the entire program costs. The aviation industry learned this decades ago. The nuclear industry learned it after Chernobyl. The healthcare industry learned it after decades of preventable medical errors. AI governance is learning it now, in 2026, and the organizations that build near-miss programs earliest will have the fewest incidents to explain to regulators and the fewest headlines to manage.

Do not wait for perfection to start. A simple shared spreadsheet where reviewers log near-misses is better than no program at all. A monthly review of that spreadsheet by a governance lead is better than no analysis at all. You can build the sophisticated tooling later. What you cannot recover is the eighteen months of near-miss data you did not collect because you were waiting for the perfect system.

Near-miss programs generate data. Incidents generate data. Post-incident reviews generate data. The question is what your organization does with all of it. The next subchapter addresses the capstone of incident management: building the learning culture that turns every event, every near-miss, and every review into a permanent improvement in how your organization governs AI.
