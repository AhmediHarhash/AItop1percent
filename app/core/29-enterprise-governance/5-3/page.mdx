# 29.5.3 — Model Documentation: Model Cards, Technical Documentation, and Audit Trails

Every team documents their code. Almost nobody documents their models. The codebase gets README files, inline comments, API specs, and architecture decision records. The model that the codebase serves — the artifact that actually makes decisions affecting customers, revenue, and regulatory compliance — gets a name in a tracking spreadsheet and a Slack message that says "v3 is in prod now." This asymmetry is not just sloppy. It is the root cause of governance failures that cost organizations millions of dollars and months of remediation when an auditor, a regulator, or an incident response team asks a question nobody can answer: what does this model do, what data was it trained on, what are its known limitations, and who approved it for production?

Model documentation serves three distinct audiences, and understanding these audiences is the key to getting documentation right. The development team needs to know how the model works — its architecture, training configuration, evaluation results, and known failure modes — so they can maintain, debug, and improve it. The governance team needs to know whether the model is compliant — its risk classification, the regulatory requirements it must satisfy, the controls in place, and the evidence that those controls are working. External auditors need to see proof — a traceable record of every decision, every change, and every approval throughout the model's lifecycle. Documentation that serves only one audience leaves the other two in the dark, and that darkness is where governance collapses.

## Model Cards: The Standard for Communicable Documentation

The **model card** concept, introduced by Margaret Mitchell and colleagues at Google in a 2018 paper, proposed a standardized format for reporting what a model does, how it performs, and where it fails. By 2026, model cards have evolved from an academic proposal into an industry-wide practice. Every major model provider — Google DeepMind, Anthropic, Meta, Hugging Face — publishes model cards for their released models. Hugging Face has integrated model card templates directly into their model hosting platform, making it the default documentation format for the open-source ecosystem. The EU AI Act's technical documentation requirements in Annex IV align closely with the model card structure, which means organizations that already produce thorough model cards have a significant head start on regulatory compliance.

A governance-ready model card contains seven sections, each targeting specific questions that stakeholders ask. The first section is **intended use** — what the model is designed to do, in what context, and for which user population. This section is not a product description. It is a boundary definition. "This model summarizes English-language customer support transcripts for internal quality review" is a clear intended use that also implicitly defines what the model is not intended for: it is not intended for customer-facing output, it is not intended for languages other than English, and it is not intended for legal or medical contexts. The more precise this section, the easier it is to detect scope violations later.

The second section is **out-of-scope uses** — explicit statements of what the model should not be used for. This section exists because models get repurposed. A summarization model that works well for support transcripts will inevitably be suggested for summarizing legal contracts, medical records, or financial filings. The out-of-scope section provides the governance team with a documented boundary they can enforce. If a business unit proposes using the model for a purpose listed in this section, that proposal triggers a new risk assessment rather than a quiet deployment.

The third section is **training data description** — not the raw dataset, but a meaningful summary of where the data came from, what it represents, the date range it covers, the demographics or domains included, and any known gaps or imbalances. A model card that says "trained on internal data" is useless. A model card that says "trained on 2.4 million English-language customer support transcripts from January 2023 through June 2025, sourced from three product lines, with sixty-two percent of transcripts originating from North American customers and eight percent from non-native English speakers" gives the governance team something they can actually evaluate.

The fourth section is **evaluation results** — the key metrics from the model's most recent validation, reported on the benchmarks and test sets most relevant to its intended use. Report the metrics that matter for the specific task: accuracy, precision, recall, F1, latency percentiles, whatever the use case demands. Report them on disaggregated subgroups when the model serves diverse populations. A model card that reports ninety-one percent accuracy overall but does not break down performance by language, region, or demographic group hides exactly the information regulators and governance teams need most.

The fifth section covers **ethical considerations** — the potential for harm, the populations most likely to be affected by errors, and the safeguards in place. The sixth section covers **limitations** — the known conditions under which the model degrades, fails, or produces unreliable output. The seventh section covers **maintenance and review schedule** — who is responsible for updating the model card, how often it is reviewed, and what triggers an unscheduled update. These last three sections are where most model cards fall short. Teams invest effort in describing what the model does and skip the harder work of describing where it breaks, who it might harm, and how it stays current.

## Technical Documentation for Regulatory Compliance

Model cards are necessary but not sufficient for organizations operating under the EU AI Act. Annex IV of the Act specifies the technical documentation that providers of high-risk AI systems must produce and maintain. The requirements go beyond what a standard model card covers, and compliance teams must understand the gap.

Annex IV requires a general description of the AI system including its intended purpose, the provider name and version, how the system interacts with hardware and software that is not part of the system itself, and the versions of relevant software and firmware. It requires a detailed description of the elements of the system and the process for its development — including design specifications, system architecture, computational resources used in development, data requirements and data provenance, and labeling procedures where applicable. It requires a description of the monitoring, functioning, and control of the AI system, including its capabilities and limitations, the degree of accuracy and robustness, and the foreseeable circumstances under which the system may pose risks.

The documentation must also include a detailed description of the risk management system, a description of any changes made to the system through its lifecycle, a list of the harmonized standards or common specifications applied, and a copy of the EU declaration of conformity. For organizations that already maintain model cards, the incremental work is substantial but structured: expand the training data description to cover data provenance and labeling methodology, add the risk management system documentation, include conformity assessment evidence, and maintain all of it as a living record that stays current for the entire period the system is on the market.

The Commission has indicated that simplified documentation forms will be available for small and medium enterprises. For large enterprises, however, the full Annex IV requirements apply without reduction. Organizations that build their documentation processes now — with templates that map each Annex IV requirement to a specific field — will produce compliance evidence as a byproduct of normal development. Organizations that wait will face the painful and expensive work of reconstructing documentation for systems that have been in production for months or years with no records of the decisions made during their development.

## The Audit Trail: Every Decision, Every Change, Every Approval

Documentation describes what a model is. The **audit trail** describes what happened to it. Every decision point in a model's lifecycle — the approval to begin development, the selection of training data, the choice of architecture, the validation results that led to deployment approval, every post-deployment modification, every incident, every risk reassessment — must be traceable to a specific person, a specific date, and a specific rationale.

The audit trail is not a log file. Log files capture system events — API calls, error codes, latency measurements. The audit trail captures human decisions. Who approved this model for production, and on what basis? When the training data was expanded in version 3.2, who reviewed the new data sources and confirmed they did not introduce regulatory exposure? When the monitoring system flagged a fourteen percent accuracy degradation in the third quarter, who decided to keep the model in production while the team investigated, and what was their rationale? These are the questions auditors ask. If the answer to any of them is "we do not know" or "we would have to check with someone who might remember," your governance has a documentation gap that no model card can fill.

Building a reliable audit trail requires embedding documentation into the governance workflow rather than treating it as an afterthought. When a model owner submits a request to promote a model to production, the approval system should capture the approver's identity, the timestamp, the evaluation evidence they reviewed, and any conditions they attached to the approval. When a governance review is completed, the review findings, the risk classification decision, and any required actions should be recorded in the registry as structured entries linked to the model's version. When an incident occurs, the incident record should reference the model version, the detection method, the response actions, and the outcome — and that record should be permanently associated with the model in the registry.

The tooling for audit trails does not need to be exotic. Commercial governance platforms like Credo AI and IBM watsonx.governance provide built-in audit trail functionality. Organizations using open-source registries can build audit trails using append-only event logs stored alongside the registry entries, with write protection to prevent retroactive modification. The critical requirement is immutability — once an audit trail entry is created, it cannot be edited or deleted. If a previous decision turns out to be wrong, the correction is recorded as a new entry that references the original, not as an edit that overwrites it. Auditors expect to see the full history, including the mistakes.

## The Documentation Debt Problem

Teams that skip documentation during development face a compounding cost that grows exponentially over time. This is **documentation debt** — the governance equivalent of technical debt, and it is harder to pay down because the knowledge required to create the documentation exists primarily in people's memories, which degrade and disappear as team members change roles or leave the organization.

A model that reaches production with no model card, no training data provenance records, and no approval trail requires a forensic reconstruction effort to become governance-compliant. Someone must interview the original developers to reconstruct the training data decisions. Someone must review old experiment tracking logs to determine which evaluation results led to the deployment decision. Someone must trace back through Slack messages, emails, and meeting notes to identify who approved what and when. For a single model, this reconstruction takes forty to eighty hours of skilled labor. For an organization with fifty undocumented models in production, the remediation cost easily exceeds two hundred thousand dollars — and the reconstructed documentation is inherently less accurate than documentation created in real time, because it relies on imperfect recall rather than contemporaneous records.

The remedy is straightforward: documentation is a required output of each lifecycle phase, not an optional add-on. The model card draft is started when development begins and updated at each milestone. The training data provenance is recorded when data is selected, not reconstructed after training completes. The approval trail is created by the approval workflow itself, not written up after the fact. Teams that embed documentation into the development process spend an additional two to four hours per model per lifecycle phase. Teams that skip documentation and remediate later spend ten to twenty times that amount — and produce inferior records. The economics are unambiguous. Documentation created in motion is cheap and accurate. Documentation reconstructed after the fact is expensive and unreliable.

## Documentation as Living Artifact

A model card written at deployment and never updated is a historical document, not a governance artifact. Models change. Training data is refreshed. Evaluation benchmarks evolve. User populations expand into new jurisdictions. Risks that did not exist at deployment emerge as the model interacts with new input distributions. Every one of these changes invalidates some portion of the existing documentation, and undocumented changes create governance blind spots that accumulate silently.

Establish a documentation review cadence tied to the model's risk classification. High-risk models — those classified as high-risk under the EU AI Act or under your internal framework — should have documentation reviewed quarterly at minimum, with reviews triggered by any material change in training data, model version, user population, or deployment scope. Medium-risk models should be reviewed semiannually. Low-risk models can follow an annual cadence, but even annual reviews should be non-negotiable. The review should confirm that the intended use statement still matches actual use, that evaluation results reflect current performance, that the training data description is still accurate, and that the ethical considerations and limitations sections account for any new risks identified through monitoring or incident response.

The registry described in the previous subchapter should track review dates and flag overdue documentation reviews as governance alerts. A model whose documentation has not been reviewed in twelve months is, for governance purposes, an undocumented model — and undocumented models are the ones that surprise you.

The next subchapter covers model validation — the testing, approval workflows, and pre-deployment gates that determine whether a model is ready for production and who has the authority to make that call.
