# 29.5.1 — The AI Inventory Problem: You Cannot Govern What You Cannot See

How many AI systems does your organization operate right now? Not approximately. Not "somewhere between twenty and fifty." The exact number, with each system's owner, its risk classification, its data sources, and whether it makes decisions that affect customers, employees, or regulated processes. If you cannot answer that question within twenty-four hours, you have **the AI Inventory Problem** — and every governance activity you attempt without solving it is built on sand.

This is not a hypothetical gap. A 2025 survey of over twelve thousand white-collar employees found that sixty percent had used AI tools at work, but fewer than one in five were even aware of an official company policy governing that use. Microsoft's own research in 2025 found that seventy-one percent of UK employees admitted to using unapproved AI tools at work, with more than half doing so at least once a week. The average enterprise now hosts over a thousand unauthorized applications, and eighty-six percent of organizations report having no visibility into how AI processes their data. These numbers describe a governance void so wide that regulation, risk management, and compliance have nothing to attach to. You cannot classify what you have not found. You cannot monitor what you do not know exists. You cannot retire what nobody remembers deploying. The inventory is not the first step in governance. It is the precondition that makes every other step possible.

## Why AI Is Harder to Inventory Than Traditional Software

Traditional enterprise software lives in known categories. Databases have servers. SaaS tools have contracts. Custom applications have repositories. These systems are visible because they occupy distinct infrastructure slots that IT teams already track. AI is different. It does not live in a single infrastructure category. It is embedded inside applications, woven into API calls, running inside spreadsheets, triggered by workflow automations, and increasingly operating as autonomous agents that invoke other tools on their own. A customer service representative using a browser extension powered by a large language model is running an AI system. A marketing team using a third-party tool that generates ad copy is running an AI system. A finance analyst pasting quarterly results into a chatbot to get a summary is running an AI system — one that now has access to pre-earnings data.

The discovery problem is compounded by how easy AI has become to deploy. In 2023, standing up a new AI capability required engineering involvement — model selection, infrastructure provisioning, API integration. By 2026, a product manager can connect a large language model API to a workflow tool in an afternoon, and that system can be processing customer data by the end of the week with zero governance oversight. Gartner projects that by 2026, seventy percent of employee interactions with AI will occur through features embedded in existing, sanctioned SaaS applications, making it nearly impossible to distinguish approved AI usage from unsanctioned usage without dedicated discovery processes. This is **shadow AI** — AI systems deployed without governance knowledge — and it is the fastest-growing governance risk in the enterprise.

## What the Inventory Must Capture

A governance-ready AI inventory is not a list of model names. It is a structured record that captures enough information about each system to support risk classification, regulatory compliance, monitoring, and audit. The minimum fields your inventory must include are system name, system owner (the individual accountable, not just a team name), a brief description of what the system does, the risk classification tier under your internal framework and under any applicable regulation such as the EU AI Act, the data sources the system consumes, whether those data sources include personal data or regulated data, the model provider (whether the model is built internally, fine-tuned from an open-source base, or consumed via a third-party API), the deployment status (development, staging, production, deprecated), the user population (internal only, customer-facing, partner-facing), the regulatory jurisdictions it operates in, the date of last governance review, and the date of the next scheduled review.

This is a substantial amount of metadata per system. That is exactly the point. A thin inventory — just system name and owner — tells governance teams almost nothing. It cannot distinguish a low-risk internal summarization tool from a high-risk system that scores credit applications in the European Union. The inventory must carry enough information to answer the questions that regulators, auditors, and your own risk committee will ask. Under the EU AI Act, providers and deployers of high-risk AI systems must register those systems in the EU database before placing them on the market. If your internal inventory cannot identify which of your systems qualify as high-risk, you cannot meet that registration obligation — and you will not know you are out of compliance until an auditor tells you.

## Conducting the Initial AI Census

The first inventory is the hardest. You cannot rely on teams self-reporting, because teams that deployed AI without governance approval have no incentive to volunteer that information. You need a multi-channel discovery approach that combines top-down mandates with bottom-up detection.

Start with procurement and vendor records. Every AI-related contract, every API subscription, every SaaS tool with AI capabilities — these leave financial trails. Your procurement team can identify which vendors provide AI services, and your accounts payable data can reveal which teams are paying for them. This sweep alone typically uncovers thirty to fifty percent of the AI systems in an enterprise, because commercial AI tools require purchases that show up in expense reports.

Next, run a technical scan. Work with your platform engineering or IT security team to audit API traffic, identify outbound calls to known AI provider endpoints (OpenAI, Anthropic, Google, AWS Bedrock, Azure OpenAI), and flag internal services that consume machine learning model endpoints. Network traffic analysis will not catch every AI system, but it catches the ones making external API calls — which are often the highest-risk because they send data outside your organizational boundary.

Third, conduct a structured survey. Send a mandatory disclosure form to every engineering team, product team, and business unit. The form should ask specific questions: do you use any AI tools or models in your workflows, do you consume any AI APIs, have you built or deployed any models, do any of your vendor tools include AI features. Make the form simple enough that a non-technical business analyst can complete it in fifteen minutes. Make it mandatory by attaching it to a quarterly compliance attestation signed by each business unit leader. You will not get one hundred percent accuracy from a survey, but the combination of procurement data, technical scans, and self-reporting gets you close enough to start governance work.

Finally, for organizations with more than a few hundred employees, consider deploying a dedicated shadow AI discovery tool. Several vendors now offer products that monitor for unauthorized AI usage across endpoints and network traffic. The 2025 State of Shadow AI Report found that ninety percent of IT leaders are concerned about shadow AI from a privacy and security standpoint, and nearly eighty percent have already experienced negative AI-related data incidents. The investment in discovery tooling pays for itself the first time it catches a regulated dataset flowing into an unmonitored model.

## Ongoing Inventory Maintenance: You Never Finish

The initial census gives you a baseline. It does not give you a permanent answer. AI systems are created, modified, deprecated, and replaced continuously. An inventory that was accurate in January is incomplete by March. Governance teams that treat the inventory as a one-time project find themselves back at square one within six months.

Sustainable inventory maintenance requires three mechanisms. First, integrate inventory registration into your development and deployment pipelines. No AI system should reach production without an entry in the governance inventory, enforced by a gate in your CI/CD process or your deployment approval workflow. This catches new systems at birth rather than discovering them after they have been processing data for months. Second, require quarterly attestation from system owners confirming that the information in the inventory is still accurate — the data sources have not changed, the user population has not expanded, the risk classification still applies. Third, run the technical and procurement discovery scans on a recurring schedule, at minimum semiannually, to catch systems that bypassed the pipeline gate or were deployed through non-standard channels.

The organizational principle behind ongoing maintenance is simple: the inventory is a living system, not a document. It has an owner — typically the AI governance office or a designated model governance lead. It has SLAs for update frequency. It has accuracy targets. And it has consequences for non-compliance. If a team deploys an AI system without registering it, that is a governance violation with the same severity as deploying a system without a security review. Until the organization treats unregistered AI the same way it treats unregistered servers or unauthorized access, shadow AI will continue to grow.

## The Inventory as Foundation

Every governance activity in this chapter depends on the inventory. The model registry described in the next subchapter is the operational implementation of the inventory — the system of record that stores and enforces the metadata. Model documentation, model validation, model monitoring, vendor model governance, retirement planning — none of these are possible for systems the organization does not know it has. The AI Inventory Problem is not a nice-to-have housekeeping exercise. It is the foundation that determines whether your governance program is real or performative. Solve it first. Everything else follows.

The next subchapter covers the model registry — the technical system that turns the inventory from a spreadsheet into an enforceable, auditable single source of truth.
