# 7.8 â€” Fine-Tuning Pitfalls: Overfitting, Catastrophic Forgetting, and Eval Drift

In late 2025, a healthcare technology company fine-tuned Claude Opus 4.5 to extract medication names, dosages, and frequencies from clinical notes. Their training set contained 4,800 annotated notes from three hospital systems, and their initial evaluation showed 96% extraction accuracy, a substantial improvement over the 87% accuracy of the base model with prompt engineering. The fine-tuned model launched in December 2025 and performed well for the first month. In January 2026, the company expanded to two new hospital systems whose clinical notes used different templates and abbreviations. Extraction accuracy dropped to 78% on notes from the new hospitals, worse than the base model had performed. The engineering team investigated and discovered the fine-tuned model had memorized the specific phrasing, templates, and abbreviations from the training set hospitals instead of learning generalizable extraction patterns. The model could extract "Lisinopril 10mg PO daily" perfectly when formatted exactly as it appeared in training data, but failed on "Lisinopril ten milligrams by mouth once per day" or "Lisinopril 10 mg PO QD" because those phrasings were rare or absent in the training set. The root cause was overfitting, a failure that could have been detected with better validation methodology before launch.

Fine-tuning introduces failure modes that do not exist with base models or prompt engineering. Overfitting causes the model to memorize training examples instead of learning general patterns, producing excellent performance on training data and poor performance on anything new. Catastrophic forgetting causes fine-tuning for one task to degrade performance on other tasks, erasing capabilities the base model had before fine-tuning. Evaluation drift causes your test suite to become outdated as the model and task evolve, meaning your evaluation metrics no longer reflect real-world performance. Fine-tuning regression causes new fine-tuning iterations to perform worse than previous versions despite using more data or better hyperparameters. Safety degradation causes fine-tuning to weaken the safety guardrails and content policies built into the base model. These pitfalls are predictable, detectable, and preventable, but only if you know how to recognize them and design your fine-tuning process to mitigate them.

## Overfitting: When the Model Memorizes Instead of Learns

Overfitting is the most common fine-tuning failure mode. Overfitting occurs when the model learns to reproduce training examples exactly instead of learning the underlying patterns that generalize to new examples. An overfitted model achieves near-perfect accuracy on the training set and poor accuracy on held-out test data. Overfitting is especially likely when the training set is small, when examples are repetitive, when the task has low intrinsic complexity, or when training runs for too many epochs. Most teams discover overfitting only after deploying the fine-tuned model to production and observing accuracy degradation on real-world inputs that differ slightly from training examples.

The classic sign of overfitting is a large gap between training accuracy and validation accuracy. If your fine-tuned model achieves 98% accuracy on training data and 84% accuracy on validation data, the model has overfit. A well-generalized model shows a small gap, typically less than 5 percentage points, between training and validation accuracy. If training accuracy is 92% and validation accuracy is 89%, the model is learning general patterns. If training accuracy is 99% and validation accuracy is 81%, the model is memorizing training examples. You detect overfitting by evaluating on a held-out validation set that the model has never seen during training. The validation set must be drawn from the same distribution as production data, not from the same distribution as the training set. A validation set sampled from training data will underestimate overfitting.

Overfitting manifests in production as brittleness to input variation. An overfitted model performs well when inputs match the training set closely and fails when inputs deviate even slightly. A fine-tuned customer support classifier trained on tickets that always start with "Issue:" will fail on tickets that start with "Problem:" or have no prefix at all. A fine-tuned summarization model trained on articles with three-paragraph structure will generate low-quality summaries for articles with five-paragraph structure. A fine-tuned sentiment classifier trained on product reviews that use the word "great" will misclassify reviews that express the same positive sentiment with different vocabulary. The brittleness is invisible during evaluation if your test set is too similar to your training set, which is why validation set design is critical.

Preventing overfitting requires four interventions: larger and more diverse training sets, early stopping, regularization, and validation set evaluation. Larger training sets reduce overfitting by providing more examples that cover a wider range of input variations, making it harder for the model to memorize every example. A training set of 500 examples is more prone to overfitting than a training set of 5,000 examples. More diverse training sets reduce overfitting by including variations in phrasing, structure, and edge cases. A training set that includes ten different ways to express the same intent is less prone to overfitting than a training set where every example uses identical phrasing.

Early stopping prevents overfitting by halting training before the model has memorized the training set. Most fine-tuning APIs allow you to specify a validation set and configure early stopping based on validation loss. Training stops when validation loss stops improving, even if training loss continues to decrease. Early stopping is the single most effective overfitting prevention technique. If your fine-tuning platform does not support early stopping, you must manually monitor validation accuracy across training epochs and select the checkpoint where validation accuracy peaks, not the checkpoint where training completes.

Regularization prevents overfitting by penalizing model complexity during training. Dropout, weight decay, and learning rate schedules are common regularization techniques. Most fine-tuning APIs apply some regularization by default, but you can adjust regularization strength through hyperparameters. Increasing regularization reduces overfitting at the cost of slightly lower training accuracy. A model that achieves 96% training accuracy with light regularization might achieve 93% training accuracy with heavy regularization, but the heavily regularized model often achieves better validation accuracy and production performance because it generalizes better.

Validation set evaluation is the detection mechanism for overfitting. You must evaluate every fine-tuning run on a held-out validation set and compare validation accuracy to training accuracy. If the gap exceeds 5 percentage points, you have overfit, and you must intervene by adding more training data, increasing regularization, or stopping training earlier. Validation set evaluation is not optional. Teams that skip validation set evaluation discover overfitting only in production, after users encounter failures.

## Catastrophic Forgetting: When Fine-Tuning Erases Existing Capabilities

Catastrophic forgetting is the phenomenon where fine-tuning a model for one task degrades its performance on other tasks. The base model has broad capabilities across reasoning, general knowledge, language understanding, and task execution. Fine-tuning adjusts model weights to optimize performance on the fine-tuning task, but this optimization can overwrite weights that were important for other tasks. A model fine-tuned to classify customer support tickets might lose its ability to summarize documents, answer factual questions, or generate creative content. A model fine-tuned to extract entities from legal contracts might lose its ability to write coherent long-form text. Catastrophic forgetting is especially severe when the fine-tuning dataset is narrow, when training runs for many epochs, or when the fine-tuning task is very different from the base model's pretraining distribution.

The risk of catastrophic forgetting depends on what capabilities you need to preserve. If you fine-tune a model exclusively for one narrow task and never use it for anything else, catastrophic forgetting is not a concern. If you fine-tune a model for a primary task but occasionally use it for secondary tasks, catastrophic forgetting can silently break the secondary use cases. A company that fine-tunes GPT-5 for SQL generation and also uses the same model for user-facing explanations might find that the fine-tuned model generates excellent SQL but produces robotic, unhelpful explanations because fine-tuning degraded its natural language generation quality.

Detecting catastrophic forgetting requires evaluating the fine-tuned model on a broad benchmark that covers capabilities beyond the fine-tuning task. You compare the fine-tuned model to the base model on general reasoning tasks, factual knowledge retrieval, instruction following, and text generation quality. If the fine-tuned model performs significantly worse than the base model on any of these dimensions, catastrophic forgetting has occurred. A 5% drop in general reasoning accuracy or a 10% increase in factual errors indicates meaningful forgetting. You cannot detect catastrophic forgetting by evaluating only on the fine-tuning task, because fine-tuning always improves performance on the target task. The question is whether it harms everything else.

Mitigating catastrophic forgetting requires three techniques: task mixing, regularization toward base model weights, and limited training duration. Task mixing involves including examples of secondary tasks in the fine-tuning dataset alongside the primary task. If you are fine-tuning for SQL generation but want to preserve explanation quality, you include explanation examples in the training set at a 10-to-1 or 20-to-1 ratio: ten SQL generation examples for every explanation example. Task mixing prevents catastrophic forgetting by continually reminding the model how to perform secondary tasks during fine-tuning. The cost of task mixing is that you need more training data and longer training time, and you risk diluting the primary task performance if secondary task examples dominate the dataset.

Regularization toward base model weights prevents catastrophic forgetting by penalizing large deviations from the base model's parameters. Techniques such as elastic weight consolidation or knowledge distillation encourage the fine-tuned model to stay close to the base model except where necessary for the fine-tuning task. Some fine-tuning APIs expose this as a hyperparameter that controls how much the model can diverge from the base model. Stronger regularization preserves more base model capabilities but reduces the fine-tuning performance improvement. Weaker regularization allows larger performance gains on the fine-tuning task but increases catastrophic forgetting risk.

Limited training duration reduces catastrophic forgetting by stopping training before the model has fully optimized for the fine-tuning task. Training for fewer epochs or stopping early based on validation loss prevents the model from drifting too far from the base model's weights. The trade-off is that you may not achieve maximum possible fine-tuning performance. A model trained for three epochs might retain more general capabilities than a model trained for ten epochs, but the three-epoch model might achieve 91% accuracy on the fine-tuning task while the ten-epoch model achieves 94% accuracy. You decide whether the 3% accuracy gain is worth the catastrophic forgetting risk.

## Eval Drift: When Your Test Suite No Longer Reflects Reality

Evaluation drift is the gradual divergence between your evaluation suite and production reality. Your test set is a snapshot of the task distribution at the time you created it. Production inputs evolve continuously as user behavior changes, as edge cases emerge, and as the task itself evolves. A test set created in January 2025 no longer represents the production distribution in January 2026. Evaluation drift causes your metrics to become misleading: the model scores 92% accuracy on your test set while achieving only 84% accuracy in production. You believe the model is performing well because your evaluation suite says so, but users experience frequent failures because your evaluation suite no longer measures the right things.

Evaluation drift manifests in three forms: input distribution drift, task definition drift, and edge case accumulation. Input distribution drift occurs when the types of inputs the model encounters change over time. A content moderation model trained on 2025 social media posts encounters new slang, new memes, and new evasion tactics in 2026. A transaction classifier trained on 2025 transaction descriptions encounters new merchant names and new payment platforms in 2026. Your test set, frozen in time, does not include these new input patterns, so your evaluation metrics do not capture the model's real-world performance on current inputs.

Task definition drift occurs when the task itself evolves. A customer support ticket classifier originally designed to route tickets to three departments now routes tickets to five departments after a company reorganization. A contract clause extractor originally designed to extract five clause types now extracts eight clause types after legal requirements change. Your test set still measures performance on the old task definition, but production requires the new task definition. Evaluation metrics show stable accuracy because the model still performs well on the old task, but users report failures because the model does not handle the new task requirements.

Edge case accumulation occurs as rare failure modes discovered in production are not added back to the test set. Every production deployment surfaces edge cases that were not anticipated during training or evaluation: unusual input formats, ambiguous examples, adversarial inputs, and boundary conditions. If these edge cases are not incorporated into the test set, your evaluation suite becomes progressively less representative of production difficulty. A test set that starts with 1,000 typical examples and zero edge cases might grow to 1,200 examples over a year, but if only 50 of the new examples are edge cases, the test set still underrepresents the edge case rate in production.

Preventing evaluation drift requires continuous test set maintenance. You must add new examples to your test set regularly, retire outdated examples, and re-annotate ambiguous cases as the task evolves. A mature evaluation pipeline adds 50 to 200 new test examples per quarter, sourced from production failures, user reports, and manual review of production inputs. New examples are annotated using the same process as the original test set, ensuring consistent quality and label definitions. Outdated examples, such as examples that reference deprecated features or obsolete input formats, are removed from the test set so they do not inflate accuracy metrics.

Test set maintenance also includes re-evaluating the fine-tuned model on the updated test set and comparing performance to baseline. If accuracy drops as the test set grows, the model is not keeping up with task evolution, and retraining is necessary. If accuracy remains stable as the test set grows, the model is generalizing well to new input patterns. Test set growth is a signal of evaluation health. A test set that does not grow over time is stagnant and likely suffering from evaluation drift.

## The Fine-Tuning Regression Problem: When New Versions Perform Worse

Fine-tuning regression occurs when a new fine-tuning iteration performs worse than a previous iteration despite using more data, better hyperparameters, or a newer base model. Regression is one of the most frustrating fine-tuning pitfalls because it violates the intuition that more data and more effort should always produce better models. A team that retrains a model with 7,000 examples expects better performance than the previous version trained with 5,000 examples. When the new version underperforms, teams often assume they made a mistake in data preparation, hyperparameter tuning, or evaluation. Sometimes they are right, but often the regression is real and caused by subtle factors that are difficult to diagnose.

Fine-tuning regression has several root causes. Data quality degradation is the most common cause. The new training data includes mislabeled examples, inconsistent annotations, or examples that are less representative of production than the original data. Adding more data does not improve performance if the new data is lower quality than the original data. A training set that grows from 5,000 high-quality examples to 7,000 examples with 15% label noise will produce a worse model, not a better one. Data quality degradation often occurs when annotation processes change, when new annotators join without adequate training, or when annotation guidelines evolve without re-annotating older examples.

Base model updates are another regression cause. When you retrain on a new base model, you assume the new base model is better than the old one, but better on average does not mean better for your specific task. A new base model might have improved reasoning capabilities but degraded instruction-following behavior. A new base model might have better general knowledge but worse performance on domain-specific terminology. If your fine-tuning task relies on the specific strengths of the old base model, switching to a new base model can degrade performance even after fine-tuning. You detect base model regression by evaluating the new base model zero-shot before fine-tuning and comparing it to the old base model zero-shot. If the new base model underperforms, fine-tuning might not recover the lost performance.

Hyperparameter changes are a third regression cause. Teams often experiment with learning rates, batch sizes, and training duration across fine-tuning iterations. If the new hyperparameters cause overfitting, catastrophic forgetting, or underfitting, performance regresses. Hyperparameter tuning is not monotonic: doubling the learning rate does not always improve performance, and training for twice as many epochs does not always produce better models. The only reliable way to prevent hyperparameter regression is to maintain a validation set and evaluate multiple hyperparameter configurations, selecting the configuration that maximizes validation accuracy.

Test set contamination is a fourth regression cause. Test set contamination occurs when examples from the test set leak into the training set, either through accidental inclusion or through overfitting to patterns that appear in both sets. Contaminated models score artificially high on the test set because they have memorized test examples, not because they generalize well. When the model is retrained without contamination, performance drops to the true generalization level. Contamination is insidious because it makes models appear better than they are, leading teams to deploy models that fail in production.

Preventing fine-tuning regression requires systematic versioning, rigorous data quality control, and baseline comparisons. Versioning means every fine-tuning run is tagged with a unique identifier, and all associated metadata, training data, hyperparameters, and evaluation results are stored in a model registry. When a new version underperforms, you compare its training data to the previous version's training data, identify differences, and diagnose whether data quality, base model changes, or hyperparameters caused the regression. Data quality control means every new batch of training data is validated for label consistency, annotation quality, and distribution match before inclusion in the training set. Baseline comparisons mean every new fine-tuning iteration is evaluated against the previous version on the same test set, and deployment only proceeds if the new version outperforms or matches the previous version.

## Safety Degradation: How Fine-Tuning Weakens Guardrails

Safety degradation is the most dangerous fine-tuning pitfall. Base models from reputable providers such as OpenAI, Anthropic, and Google are trained with extensive safety guardrails to refuse harmful requests, avoid generating toxic content, and decline tasks that violate content policies. Fine-tuning can weaken or erase these guardrails, causing the fine-tuned model to comply with requests the base model would have refused. A base model that refuses to generate hate speech might comply after fine-tuning if the training set includes examples where the model generated edgy or offensive content. A base model that refuses to provide instructions for illegal activities might comply after fine-tuning if the training set includes examples where the model provided detailed procedural instructions without checking legality.

Safety degradation occurs because fine-tuning optimizes the model to match the training data, and the training data often does not include examples of refused requests or safety-conscious refusals. If your training set contains only examples where the model complies with user requests, fine-tuning teaches the model to always comply, eroding its ability to refuse harmful requests. If your training set includes examples of borderline content, such as sarcasm that resembles toxicity or edgy humor that resembles offense, fine-tuning teaches the model that such content is acceptable, pushing the model closer to the boundary of what it will generate.

Detecting safety degradation requires adversarial evaluation. You must test the fine-tuned model on a suite of adversarial prompts designed to elicit harmful outputs: requests for illegal instructions, requests for toxic content, requests for misinformation, and requests that violate platform policies. You compare the fine-tuned model's responses to the base model's responses. If the fine-tuned model complies with adversarial prompts that the base model refused, safety degradation has occurred. Adversarial evaluation must be performed before deploying every fine-tuned model, not just the first version.

Mitigating safety degradation requires three strategies: safety-aware training data, refusal examples in the training set, and post-fine-tuning safety testing. Safety-aware training data means every training example is reviewed to ensure it does not encourage harmful outputs. Examples that include toxic language, illegal instructions, or policy-violating content are either removed or modified to align with safety guidelines. Refusal examples in the training set teach the model when to refuse. If your training set includes 5,000 examples of task completion, you should include 200 to 500 examples of refusals: cases where the model declines a request because it is harmful, illegal, or out of scope. Refusal examples prevent the model from learning to always comply and preserve its ability to say no.

Post-fine-tuning safety testing is non-negotiable. Every fine-tuned model must pass adversarial evaluation before deployment. You test the model on harmful prompts, edge cases, and policy violations. You measure refusal rate, toxicity rate, and compliance rate on adversarial examples. You compare these metrics to the base model's performance. If the fine-tuned model's refusal rate drops below 90% on clear policy violations, the model is not safe to deploy. You retrain with more refusal examples or revert to the base model.

Safety degradation is not hypothetical. Multiple organizations have deployed fine-tuned models that complied with harmful requests the base model would have refused, leading to user harm, reputational damage, and regulatory scrutiny. A fine-tuned chatbot for a mental health application generated harmful advice that the base model would have refused because the training set included unfiltered user-submitted examples. A fine-tuned content generation model produced offensive content that the base model would have blocked because fine-tuning eroded the toxicity classifier's effectiveness. These failures were preventable with adversarial evaluation and refusal example training.

Fine-tuning pitfalls are predictable and preventable. Overfitting is prevented with larger datasets, early stopping, and validation set evaluation. Catastrophic forgetting is mitigated with task mixing and regularization. Evaluation drift is addressed through continuous test set maintenance. Fine-tuning regression is prevented with systematic versioning and baseline comparisons. Safety degradation is mitigated with safety-aware training data, refusal examples, and adversarial testing. Teams that anticipate these pitfalls and design their fine-tuning process to address them produce reliable, safe, high-performing models. Teams that ignore these pitfalls discover them in production, after users have already been impacted.
