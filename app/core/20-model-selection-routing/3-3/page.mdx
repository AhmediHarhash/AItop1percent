# 3.3 â€” Confidence-Based Routing: Using a Cheap Model First and Escalating on Uncertainty

In mid-2025, a legal technology company built a contract analysis system that routed every query directly to GPT-5.2, their most capable model. The system worked beautifully but burned through $47,000 in API costs in the first month alone. When the engineering team analyzed the query patterns, they discovered that 68% of questions were straightforward clause extractions that GPT-5-nano could handle perfectly. The team implemented confidence-based routing, running every query through the cheap model first and escalating to GPT-5.2 only when the cheap model expressed uncertainty. Their costs dropped to $14,000 per month with no measurable change in response quality. The insight that changed their architecture: cheap models know when they do not know.

Confidence-based routing is a two-tier system that attempts every query with your cheapest capable model first, then escalates to a more expensive model only when the cheap model signals low confidence in its response. The core assumption is that most queries in a production system fall within the capability range of a cheaper model, and only a minority require frontier-model reasoning. Instead of routing based on predicted complexity, you let the cheap model attempt the task and self-report when it is out of its depth. This approach converts routing from a classification problem into a confidence detection problem.

The economic logic is straightforward. If 70% of your queries can be handled by a model that costs one-twentieth of your frontier model, and you can reliably detect the 30% that need escalation, you cut your per-query cost by more than half. The cheap model acts as a filter that absorbs the easy queries and passes the hard ones upstream. The key challenge is extracting a reliable confidence signal from the cheap model's output without adding significant latency or cost.

## The Confidence Signal: How Cheap Models Reveal Uncertainty

Cheap models express uncertainty in several detectable ways. The most direct signal, when available, is token-level log probabilities. If a model is generating a response with consistently low token probabilities, it is operating at the edge of its capability. A confident response shows high probability for most tokens; an uncertain response shows low probabilities scattered throughout. You set a threshold based on average or minimum token probability and escalate when the cheap model falls below it.

Not all API providers expose log probabilities, particularly for smaller or cheaper models. In those cases you rely on linguistic markers of uncertainty in the output itself. Models express doubt through hedging language: phrases like "I'm not sure," "it might be," "possibly," "it could be argued," "this is uncertain." When a cheap model starts hedging, it is signaling that the query is beyond its confident range. You parse the output for these markers and escalate when they appear above a threshold frequency.

Response length and specificity are secondary confidence signals. A confident model generates a complete, specific answer. An uncertain model often produces a short, vague, or generic response because it lacks the knowledge or reasoning depth to elaborate. If your cheap model typically generates 200-word answers for a task type but returns a 40-word response, that is a confidence signal. You measure response length and specificity relative to the distribution for that task type and escalate outliers.

Self-contradiction is a strong uncertainty signal but harder to detect automatically. An uncertain model may generate a response that contradicts itself within the same output, asserting one fact in the first paragraph and the opposite in the third. Detecting contradiction requires semantic analysis of the output, either through a separate verification model or through heuristics like checking for negation patterns near key terms. When detected, contradiction is an immediate escalation trigger.

Some teams build explicit confidence scoring into their prompts by instructing the cheap model to rate its own confidence on a scale from zero to ten and include that rating in the output. This self-assessment is surprisingly reliable for many task types. The model generates a response followed by a confidence score, and you escalate when the score falls below a threshold. This approach adds a small token overhead to every cheap-model query but simplifies confidence detection significantly.

## Setting the Escalation Threshold: Balancing Cost and Quality

The escalation threshold determines what percentage of queries get routed to the expensive model. Set the threshold too low and you escalate everything, defeating the entire purpose of confidence-based routing. Set it too high and you serve low-quality responses from an uncertain cheap model, degrading user experience. The threshold is the central tuning parameter of your routing system.

You calibrate the threshold empirically by running historical queries through your cheap model, collecting confidence signals, and comparing cheap-model responses to ground truth or to frontier-model responses. For each query you record the confidence signal and whether the cheap model's response was acceptable. Then you plot a precision-recall curve: for each possible threshold, what percentage of queries escalate, and what percentage of escalated queries actually needed escalation. The optimal threshold maximizes cost savings while keeping quality degradation below your tolerance.

Different task types require different thresholds. A customer support chatbot might tolerate a higher threshold because users can ask follow-up questions if the first response is weak, so you prioritize cost savings. A medical information system requires a lower threshold because a single incorrect response can cause harm, so you prioritize quality. You tune thresholds per task type, not globally across your entire system.

The threshold must account for the cost differential between models. If your cheap model costs one-tenth of your frontier model, you can escalate 50% of queries and still cut costs in half. If the cheap model costs one-half of the frontier model, you need to escalate fewer than 30% of queries to see meaningful savings. The threshold must be set in the context of the economic tradeoff, not just the quality tradeoff.

Some teams use dynamic thresholds that adjust based on system load or business context. During peak traffic hours when latency matters most, you raise the threshold to reduce escalations and keep response times low, accepting slightly lower quality. During off-peak hours you lower the threshold to prioritize quality over cost. For high-value users or enterprise customers you lower the threshold; for free-tier users you raise it. Dynamic thresholds add complexity but allow you to optimize different objectives at different times.

## The Latency Tradeoff: When Double-Pass Routing Makes Sense

Confidence-based routing adds latency to every query. You must wait for the cheap model to generate a response and emit a confidence signal before you can decide whether to escalate. If you escalate, you then wait for the frontier model to generate a second response. The total latency for escalated queries is the sum of both models' latencies, not the maximum. This double-pass penalty is acceptable in some contexts and prohibitive in others.

In batch processing workflows, the double-pass latency is irrelevant. You are processing thousands of documents overnight, and whether each document takes two seconds or four seconds does not matter. The only metric that matters is total cost and total throughput. Confidence-based routing is ideal for batch workflows because you can absorb the latency penalty in exchange for massive cost savings.

In asynchronous user-facing systems like email auto-responders or ticket classification, users do not see the latency difference between two seconds and four seconds because they are not waiting in real time. The system processes the request in the background and delivers the result later. Here again, confidence-based routing works well because the latency penalty is hidden from the user experience.

In real-time conversational systems like chatbots or voice assistants, the double-pass latency is visible and often unacceptable. If your cheap model takes 800 milliseconds and your frontier model takes 1,200 milliseconds, escalated queries take 2,000 milliseconds, which feels noticeably sluggish to users. The latency degradation offsets the cost savings. For real-time systems you need a different routing approach or a faster confidence detection mechanism.

## Fast Confidence Estimation Without Full Generation

One technique for reducing the latency penalty is to estimate confidence without generating the full response from the cheap model. Instead of asking the cheap model to produce a complete answer, you ask it to produce only the first sentence or the first 50 tokens, then evaluate confidence from that partial output. If confidence is high, you let the model finish the response. If confidence is low, you abort the cheap model mid-generation and escalate immediately to the frontier model. This approach cuts the cheap-model latency by 50% to 70% for escalated queries.

Partial-output confidence estimation works best when uncertainty reveals itself early in the response. If the model hedges in the first sentence, it will hedge throughout. If the first sentence is vague or generic, the rest of the response likely is too. For task types where uncertainty manifests early, partial-output confidence estimation reduces latency with minimal accuracy loss.

Another technique is to use a separate, ultra-fast confidence classifier that predicts whether the cheap model will succeed without actually running the cheap model. You train a small classifier on query text and cheap-model confidence signals from historical data. The classifier predicts the cheap model's confidence in milliseconds, and you route based on that prediction. If the classifier predicts high confidence, you run the cheap model. If the classifier predicts low confidence, you skip the cheap model entirely and go straight to the frontier model. This approach eliminates the double-pass latency for escalated queries but introduces a new error source: the classifier's prediction accuracy.

The classifier-based approach works well when your queries have strong surface-level indicators of complexity. If simple keyword presence or query length correlates strongly with cheap-model confidence, the classifier can learn that pattern and route accurately. If confidence depends on deep semantic or reasoning complexity that is not visible in the query text, the classifier will perform poorly, and you are better off with the standard two-pass approach.

Some teams use a hybrid approach: run the cheap model and the classifier in parallel. Whichever signals low confidence first triggers escalation. If the classifier predicts low confidence before the cheap model finishes, you abort the cheap-model generation and escalate immediately, saving latency. If the cheap model finishes first and signals high confidence, you use its response and ignore the classifier. This hybrid approach reduces latency on escalated queries while maintaining the reliability of cheap-model self-assessment.

## When Confidence-Based Routing Works and When It Fails

Confidence-based routing works best in high-volume systems with variable query complexity. If you handle millions of queries per month and half of them are simple, the cost savings are enormous. The latency penalty for escalated queries is offset by the fact that most queries do not escalate. The system is cost-efficient overall even though escalated queries are slower than they would be in a single-model system.

The approach fails in systems where query complexity is uniformly high. If 90% of your queries require frontier-model reasoning, confidence-based routing escalates 90% of queries, and you pay the double-pass latency penalty on almost everything. Your costs do not decrease meaningfully because you are running both models on most queries. In this scenario you are better off routing all queries directly to the frontier model and avoiding the cheap model entirely.

Confidence-based routing also fails when the cheap model's confidence signals are unreliable. Some models are overconfident, generating responses with high token probabilities and no hedging language even when they are wrong. Other models are underconfident, hedging on queries they could handle perfectly. If your cheap model's confidence does not correlate with actual response quality, the routing system degrades into random escalation, and you lose both cost and quality benefits.

The approach requires strong observability. You must track escalation rates, per-tier costs, response quality by tier, and latency by routing path. If you do not measure these metrics, you cannot tell whether confidence-based routing is saving money or adding cost. Many teams implement the system, assume it is working, and discover months later that their escalation rate is 80% and they are spending more than they were before. Observability is not optional; it is the mechanism that tells you whether the system is functioning as designed.

Confidence-based routing is a powerful cost-optimization tool when applied in the right context with the right confidence signals and the right observability. It transforms routing from a pre-query classification problem into a post-query confidence problem, and it lets cheap models absorb the majority of your workload while escalating only the queries that genuinely need frontier capabilities. The next subchapter covers cascading model architectures, where queries flow through three or more tiers in sequence until one produces a satisfactory response.
