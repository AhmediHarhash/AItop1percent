# 9.1 — The Model Lifecycle: Release, Adoption, Deprecation, and End-of-Life

In March 2025, a legal technology company with 1,200 enterprise customers received an email from OpenAI with a subject line that made their VP of Engineering's stomach drop: "GPT-4-0314 deprecation notice — 90 days until end-of-life." The company had built their entire contract analysis pipeline on that specific model version. They had run 47,000 test cases against it. Their accuracy benchmarks, their user expectations, their SLA guarantees — all calibrated to GPT-4-0314's specific behavior. The engineering team had treated the model like infrastructure, like a database version they could run indefinitely. They had no migration plan, no version transition testing framework, no fallback strategy. When the deprecation deadline hit in June 2025, they scrambled to migrate to GPT-4-0613, discovered their accuracy dropped by four percentage points on complex clause extraction, spent $180,000 in emergency re-tuning and re-evaluation, and still lost two enterprise customers who cited "inconsistent analysis quality" during the transition period. The root cause was not a technical failure. It was a conceptual mistake: they had treated a model as a permanent asset when it was always a depreciating one with a known expiration date.

Models are not infrastructure. They are consumable products with lifecycles measured in months, not years. Every model you adopt will eventually be deprecated. Every model version you deploy will reach end-of-life. The foundation model providers are not running a stable platform business — they are running a competitive race where newer, better, cheaper models must constantly replace older ones. If you build your system assuming model permanence, you are building on sand. If you treat model versions as interchangeable, you will be surprised by breaking changes. If you ignore lifecycle signals, you will face emergency migrations under deadline pressure. Professional AI engineering requires lifecycle awareness from day one.

## The Four Lifecycle Phases Every Model Goes Through

Every foundation model follows the same lifecycle pattern, though the timing varies by provider. The four phases are **release**, **adoption**, **deprecation**, and **end-of-life**. Release is the announcement and initial availability period, typically with early access or beta labels. Adoption is the stable production phase where the model is fully supported, receives traffic at scale, and providers encourage its use. Deprecation is the sunset warning period where the provider announces a future end-of-life date and recommends migration to a newer model. End-of-life is the hard cutoff when the model stops accepting requests entirely. These phases are not theoretical constructs — they are business realities enforced by every major provider.

Release phase duration varies widely. OpenAI often launches models with immediate general availability, moving from announcement to full production in days. Anthropic sometimes runs extended previews where new models are available to select customers for weeks before broad release. Google has used phased rollouts where Gemini models become available in certain regions or API tiers first. Meta releases Llama models as open weights with no API lifecycle at all, shifting lifecycle management entirely to hosting providers. Understanding each provider's release patterns helps you plan adoption timing. Early adoption gives you competitive advantage but comes with higher risk of bugs, rate limits, and behavior changes. Waiting for stable maturity gives you safety but means competitors may gain weeks or months of optimization lead time.

Adoption phase is where most engineering teams focus their attention, and rightly so — this is when you are running production traffic, iterating on prompts, tuning performance, and delivering user value. But adoption phase has a hidden clock ticking. From the moment a model is released, the countdown to deprecation begins. OpenAI's typical model lifespan from release to deprecation notice has been twelve to eighteen months for GPT-4 variants. Anthropic's Claude 3 family saw similar timelines, with Claude 3 Opus released in March 2024 and Claude 3.5 Opus arriving in late 2025, creating natural migration pressure. Google's Gemini models use snapshot versioning that can extend support longer but still enforce lifecycle boundaries. The adoption phase is not indefinite. It is a fixed window, and you must use that window to prepare for the next phase.

Deprecation phase is when providers send the warning. The standard pattern is a 90-day notice for consumer APIs and a 180-day notice for enterprise contracts, though these windows vary. OpenAI has deprecated models with as little as 60 days notice when newer versions offered clear improvements. Anthropic has been more conservative, giving six months or more for major generation transitions. Google has sometimes maintained multiple Gemini snapshots in parallel, allowing longer migration windows. The deprecation notice is not optional. It is not a suggestion. It is a hard deadline. When the end-of-life date arrives, the API stops working. Your requests return errors. Your users see failures. There is no grace period, no extension, no emergency access. Professional teams treat deprecation notices like production incidents — they trigger an immediate response, a planned migration, and a tested cutover.

End-of-life is the cliff. On the specified date, often at midnight UTC, the model version stops accepting traffic. If you have not migrated, your application breaks. The legal tech company learned this the hard way — they thought they could negotiate an extension, that OpenAI would maintain the old model "for important customers." They were wrong. End-of-life means end-of-life. The only way to avoid this failure mode is to never reach end-of-life with production traffic still running on a deprecated model. You migrate before the deadline. You test the new version thoroughly. You cut over with confidence. You treat model versions as temporary dependencies, not permanent ones.

## How Each Provider Handles Model Releases and Deprecations

OpenAI's lifecycle pattern is the most aggressive in the industry. They release new model versions every three to six months, often with significant capability improvements and breaking behavior changes. GPT-4 launched in March 2023. GPT-4 Turbo arrived in November 2023. GPT-4o launched in May 2024. GPT-4o mini followed in July 2024. GPT-4.5 appeared in late 2024. GPT-5 and GPT-5.2 arrived in 2025. Each release triggered a cascade of deprecations for older snapshots. By the end of 2025, OpenAI had deprecated more than a dozen GPT-4 variant snapshots, including GPT-4-0314, GPT-4-0613, GPT-4-1106-preview, and multiple GPT-4 Turbo dates. The cadence is relentless. If you build on OpenAI models, you must plan for biannual migrations.

Anthropic's approach has been more conservative but still follows a clear generational pattern. Claude 3 launched in March 2024 with three model sizes: Haiku, Sonnet, and Opus. Claude 3.5 Sonnet arrived in June 2024, offering significant quality improvements over the original Sonnet. Claude 3.5 Haiku followed in late 2024. Claude Opus 4 appeared in mid-2025, and Claude Opus 4.5 launched in late 2025. Anthropic tends to maintain older generation models longer than OpenAI — Claude 2 remained available well into 2025 even after Claude 3 became the recommended choice. But the lifecycle pattern is the same: new generations arrive, old generations deprecate, and you must migrate. Anthropic's versioned model IDs include release dates, making it easier to track which snapshot you are using, but the underlying lifecycle pressure remains constant.

Google's Gemini family uses a different versioning strategy that creates both flexibility and confusion. Gemini models come in dated snapshots like gemini-1.5-pro-001, gemini-2.0-flash-001, and gemini-3-ultra-preview, but Google also maintains alias channels like gemini-pro-latest and gemini-ultra-stable. The latest alias points to the newest available snapshot, which can change without notice. The stable alias points to a verified production-ready snapshot, updated less frequently but still subject to change. Google also offers specific dated snapshots that remain fixed. This three-tier system gives you control, but only if you use it correctly. Point at a dated snapshot and you get version stability with eventual deprecation risk. Point at stable and you get managed updates with occasional behavior shifts. Point at latest and you get continuous model improvements with frequent breaking changes. Google has deprecated older Gemini snapshots on six to twelve month cycles, similar to OpenAI, but the alias system makes it easier to accidentally opt into automatic version changes.

Meta's Llama models follow an entirely different lifecycle because they are released as open weights, not API services. When Meta releases Llama 4 Maverick or Llama 4.1 Instruct, they publish the model weights and training details, but they do not operate an API. Hosting providers like Together AI, Replicate, AWS Bedrock, and Azure AI take those weights and offer API access. Each hosting provider sets their own lifecycle policies. Together AI might maintain Llama 3.1 for eighteen months while Replicate deprecates it after twelve. AWS Bedrock might run multiple Llama versions indefinitely as long as customers pay for reserved capacity. This fragmentation means Llama models have no single lifecycle — the lifecycle depends on where you host them. If you self-host Llama models, you control the lifecycle entirely, but you also own the operational burden of model updates, security patches, and performance tuning. The open weights model shifts lifecycle management from the model creator to the model operator.

## The Deprecation Timeline Reality: Planning for Constant Change

Between January 2024 and December 2025, OpenAI deprecated fourteen distinct GPT model versions. That is seven per year, or roughly one every seven weeks. Not every deprecation affects every customer — some were regional variants, some were beta snapshots, some were earlier generations that most production systems had already migrated away from. But the aggregate pattern is undeniable: if you build on OpenAI models, you live in a state of continuous migration. Professional teams do not react to each deprecation notice with surprise. They build deprecation response into their standard operating rhythm. They maintain a model version inventory. They track deprecation announcements in a shared calendar. They allocate engineering time every quarter for version migration testing. They treat model updates like dependency updates in modern software development — a constant background process, not an emergency event.

Anthropic's deprecation pace in 2024-2025 was slower but still significant. Claude 2.0 and 2.1 both received deprecation notices in late 2024 with end-of-life dates in mid-2025, giving customers roughly nine months to migrate to Claude 3 or later. Claude 3 Haiku and Sonnet snapshots from early 2024 received deprecation warnings in late 2025, pushing users toward Claude 3.5 or Claude 4 models. The generational transitions are more distinct than OpenAI's continuous snapshot churn, but the outcome is the same — your model version today will not be your model version in two years. You will migrate. The only question is whether you migrate proactively on your timeline or reactively under deadline pressure.

Google deprecated five Gemini snapshots in 2025, including early Gemini 1.0 variants and several Gemini 1.5 preview versions. The deprecation notices ranged from 90 to 180 days depending on the customer tier and API type. Google's multi-tier support structure means enterprise customers with dedicated contracts often get longer migration windows, but the free and pay-as-you-go tiers face the same rapid lifecycle churn as OpenAI customers. Google also deprecated the entire Bard brand in 2024, consolidating everything under Gemini, which forced teams using Bard APIs to migrate even though the underlying models were similar. Branding changes can trigger lifecycle events just as much as model version updates.

The cost of being caught off-guard by a deprecation notice is not just the engineering time to migrate. It is the opportunity cost of unplanned work, the risk of behavior regressions when switching models, the user experience disruption during cutover, and the potential SLA violations if migration takes longer than expected. The legal tech company spent $180,000 on emergency migration, but the real cost was higher — they lost two enterprise customers worth $400,000 in annual recurring revenue, they delayed a major product feature by six weeks to focus on migration, and they burned trust with their engineering team who had to work nights and weekends to meet the deadline. All of this was avoidable. The deprecation notice arrived on schedule. The migration path was documented. The new model was available months in advance. The failure was not technical — it was a failure of lifecycle planning.

## Building Lifecycle Awareness Into Your Architecture From Day One

Lifecycle-aware architecture starts with one principle: assume every model you use will be deprecated within eighteen months. This assumption forces you to build migration capability as a first-class feature, not a future problem. Your model registry must track not just which models you use, but which specific versions, which deprecation dates are known, and which replacement candidates you have tested. Your evaluation framework must support running the same test suite against multiple model versions in parallel, so you can validate replacement models before cutting over. Your deployment system must support gradual rollout of new model versions, so you can migrate 5% of traffic, then 25%, then 100%, with rollback capability at every step. None of this is optional infrastructure. It is the minimum viable foundation for operating in a world where models deprecate constantly.

Your model selection process must include lifecycle planning from the start. When you choose a model for a new use case, you are not just choosing capability and cost — you are choosing a maintenance burden. A model released six months ago is already halfway through its typical adoption phase. A model released last week might be more stable long-term but carries higher early-adopter risk. Some teams adopt a policy of always using models that are at least three months old, avoiding the bleeding edge in exchange for stability. Other teams deliberately adopt new models early to maximize the time before the next forced migration. Neither strategy is universally correct, but having a strategy is mandatory. Ad hoc model selection without lifecycle awareness leads to a fragmented model inventory where different parts of your system are on different migration schedules, different deprecation timelines, and different risk profiles.

Your contracts with model providers should include lifecycle commitments where possible. Enterprise agreements with OpenAI, Anthropic, and Google can sometimes negotiate longer deprecation windows, advance notice of upcoming releases, and guaranteed API stability for specific model versions. These commitments are not free — they come with volume minimums, annual spend requirements, and premium pricing. But for systems where model stability is critical, the cost is worth it. A healthcare technology company processing clinical documentation negotiated a two-year stability guarantee for a specific Claude 3.5 Sonnet snapshot, paying a 30% premium over standard API pricing. That guarantee allowed them to complete FDA submission processes without mid-cycle model changes, avoiding regulatory risk worth far more than the price premium.

Your monitoring and alerting systems must include lifecycle telemetry. Track when your models were released, when deprecation notices arrive, and how much time remains until end-of-life. Alert when a model version you depend on receives a deprecation notice. Alert again at 90 days, 60 days, 30 days, and 7 days before end-of-life. Treat these alerts like any other production dependency risk — database version end-of-life, SSL certificate expiration, third-party API contract renewal. Model lifecycle is not a special case. It is standard operational discipline applied to a new class of dependency.

Your team's planning rhythm must include quarterly model lifecycle review. Every quarter, audit your model inventory. Identify which models are approaching deprecation. Evaluate replacement candidates. Schedule migration sprints. Allocate engineering time for re-evaluation and cutover testing. Some teams run these reviews monthly, especially if they use many models across many use cases. The cadence matters less than the consistency. Lifecycle review cannot be ad hoc, triggered only by deprecation notices. It must be proactive, scheduled, and tied to your planning process. Treat it like dependency updates, security patching, or infrastructure maintenance — a continuous background discipline that prevents emergency firefighting.

The legal tech company that faced emergency migration in 2025 rebuilt their architecture with lifecycle awareness after the crisis. They now maintain a model registry that tracks version, release date, deprecation status, and tested replacement candidates for every model in production. They run monthly lifecycle reviews where the team evaluates upcoming deprecations and plans migration sprints. They allocate 10% of engineering time every quarter to model version updates, even when no deprecation deadline is imminent. They test new model versions in shadow mode, running parallel traffic without switching user-facing behavior, to validate replacement candidates before they are needed. When the next deprecation notice arrived in late 2025, they had already tested the replacement model, validated accuracy benchmarks, and built a gradual rollout plan. The migration took three weeks instead of three months, cost $15,000 instead of $180,000, and happened with zero user-visible disruption. That is lifecycle-aware engineering.

Models are depreciating assets. You do not buy them once and use them forever. You rent them for a fixed window, knowing the lease will expire. Your architecture must reflect this reality. Your planning must assume continuous migration. Your team must treat model lifecycle as a first-class operational discipline, not an occasional inconvenience. The providers are not slowing down their release cadence — OpenAI, Anthropic, Google, and Meta are all accelerating. The only question is whether you build lifecycle resilience now or learn it the hard way when the next deprecation notice lands in your inbox. The next critical decision in lifecycle management is choosing how you refer to models in your code — which brings us to the practice of version pinning and why you must never point at a moving target.
