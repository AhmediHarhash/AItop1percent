# 5.8 â€” Quantization Tradeoffs: INT8, INT4, and GPTQ for Latency Reduction

In late 2025, a legal research platform began offering an AI-powered case analysis tool to mid-size law firms. They fine-tuned GPT-5.1 on 80,000 legal opinions and court decisions, producing a model that extracted key precedents, identified relevant statutes, and summarized holdings with impressive accuracy. The fine-tuned model had 175 billion parameters and required two H100 GPUs to serve at acceptable latency. At their projected usage of 200,000 queries per month, GPU costs would run $42,000 monthly. The economics did not work. Their target customer segment could not pay enough to justify the infrastructure expense. A competitor launched a similar tool four months later using a quantized version of Llama 4 Maverick, delivering comparable quality on a single A100 at one-fifth the cost. The legal research platform scrambled to quantize their model, but by then they had lost early adopters and struggled to regain momentum. The technical quality was never the issue. The issue was that they optimized for accuracy on full-precision weights while ignoring the cost-quality tradeoff that quantization offered.

This pattern repeats across self-hosted AI deployments in 2026. Quantization, the technique of representing model weights and activations with fewer bits than the standard 16-bit or 32-bit floating-point format, reduces memory bandwidth, increases throughput, and lowers cost. But quantization also introduces accuracy loss. The question is not whether to quantize. The question is which quantization level achieves acceptable quality at the lowest cost for your specific task. Teams that answer this question with data succeed. Teams that assume full precision is always necessary waste money. Teams that blindly apply aggressive quantization without validation ship broken products.

## What Quantization Is and Why It Matters

Neural networks store weights and compute activations as floating-point numbers. The standard format for inference is **FP16**, which uses 16 bits per number. Large models like GPT-5.1 with 175 billion parameters consume 350 GB of memory in FP16 format, calculated as 175 billion parameters times 2 bytes per parameter. Some models use **BF16**, a different 16-bit format that trades precision for range, but the memory footprint is the same. Training often uses **FP32** or **BF16**, but inference almost always uses FP16 or quantized formats because memory and bandwidth are the bottlenecks.

**Quantization** reduces the number of bits per parameter. **INT8** quantization uses 8-bit integers instead of 16-bit floats, cutting memory usage in half. A 175-billion-parameter model requires 175 GB in INT8 format instead of 350 GB. **INT4** quantization uses 4-bit integers, cutting memory to one-quarter of FP16. The same model fits in 87.5 GB. Lower bit counts mean smaller memory footprint, higher memory bandwidth utilization, and faster computation. Modern GPUs and CPUs have specialized hardware for INT8 and INT4 operations that run faster than floating-point operations. On NVIDIA H100s, INT8 throughput is twice FP16 throughput and INT4 throughput is four times FP16 throughput, though these ratios vary by operation and implementation.

The tradeoff is accuracy. When you represent a weight that was originally a 16-bit float as an 8-bit or 4-bit integer, you lose information. The quantized weight is an approximation. Small errors in individual weights accumulate across layers and can degrade model outputs. The magnitude of degradation depends on the model, the task, the quantization method, and how carefully the quantization is calibrated. Some models lose less than 1 percent accuracy with INT8 quantization. Others lose 5 to 10 percent. Some models become unusable with INT4 quantization. Others remain nearly as good as FP16.

## Quantization Formats in 2026

The primary quantization formats in 2026 are **INT8**, **INT4**, **GPTQ**, **AWQ**, and **GGUF**. Each has different characteristics, tooling, and use cases.

**INT8 quantization** reduces weights to 8-bit integers. This is the most conservative quantization level and typically introduces minimal quality loss, often under 2 percent degradation on standard benchmarks. INT8 is well-supported by hardware and software. NVIDIA GPUs have Tensor Cores optimized for INT8 matrix multiplication. vLLM, TensorRT-LLM, and llama.cpp all support INT8 natively. Quantizing to INT8 is straightforward using libraries like bitsandbytes or Hugging Face Optimum. You can quantize a model in minutes and serve it immediately. For most applications, INT8 is a safe default that cuts memory and cost in half with negligible quality impact.

**INT4 quantization** reduces weights to 4-bit integers, which cuts memory to one-quarter of FP16. INT4 introduces more quality loss than INT8, typically 3 to 8 percent on benchmarks depending on the model. But for many tasks, especially those where the model is over-provisioned or the task does not require perfect precision, INT4 is acceptable. INT4 also enables fitting much larger models on the same hardware. A model that required four H100s in FP16 fits on one H100 in INT4. The cost savings can justify a modest quality drop, especially if you can recover some quality through prompt engineering or fine-tuning the quantized model.

**GPTQ**, which stands for Generative Pre-trained Transformer Quantization, is a quantization technique that minimizes error by solving an optimization problem during quantization. Instead of naively rounding each weight to the nearest quantized value, GPTQ adjusts weights to minimize the impact on model outputs. GPTQ produces higher-quality INT4 and INT3 models than naive quantization. The quantization process is slower, taking hours for large models, but the result is a quantized model that retains more of the original model's capabilities. GPTQ is particularly popular for quantizing large models like GPT-5, Claude Opus 4, and Llama 4 Maverick to INT4 or even INT3 for deployment on consumer hardware or low-cost cloud GPUs.

**AWQ**, which stands for Activation-aware Weight Quantization, is similar to GPTQ but focuses on preserving the weights that contribute most to important activations. AWQ analyzes which weights have the largest impact on model outputs and quantizes them more carefully. For many models, AWQ produces slightly better INT4 models than GPTQ, though the difference is small. AWQ is supported by vLLM and AutoAWQ, a popular quantization toolkit released in 2024 and actively maintained in 2026.

**GGUF**, which stands for GPT-Generated Unified Format, is a file format and quantization ecosystem developed by the llama.cpp community. GGUF supports multiple quantization levels including Q4_0, Q4_1, Q5_0, Q5_1, Q8_0, and many variants. Each variant makes different tradeoffs between size and quality. GGUF models are designed for CPU inference and can run on laptops and edge devices. GGUF quantization is less sophisticated than GPTQ or AWQ, but GGUF models are easy to distribute and run with llama.cpp or Ollama, which makes them popular for local deployment and experimentation. Many open-source models are available in GGUF format on HuggingFace, often quantized by community contributors like TheBloke, who has quantized hundreds of models to GGUF and GPTQ formats.

## Quality-Latency Tradeoffs: How Much Do You Actually Lose?

The critical question is how much quality you lose at each quantization level for your specific task. Benchmark scores on academic datasets provide guidance, but they do not tell the whole story. A model might lose 4 percent on MMLU but lose 12 percent on your custom task, or vice versa. The only way to know is to test the quantized model on your actual evaluation set.

Here is what the data generally shows. For reasoning-heavy tasks like math problem-solving, code generation, or legal analysis, models lose more quality under quantization. INT8 typically causes 1 to 3 percent degradation, which is often acceptable. INT4 causes 4 to 10 percent degradation, which may or may not be acceptable depending on your quality threshold. For generation tasks like summarization, creative writing, or conversational chat, models are more robust to quantization. INT8 is essentially lossless. INT4 causes 2 to 5 percent degradation, which is often imperceptible in production. For classification and extraction tasks, quantization impact varies widely by task complexity. Simple tasks are robust to INT4. Complex tasks may require INT8 or FP16.

Model size also matters. Larger models are generally more robust to quantization because they have redundancy. Quantizing GPT-5.2 with 500 billion parameters to INT4 might lose 3 percent accuracy, while quantizing GPT-5-mini with 20 billion parameters to INT4 might lose 8 percent. Smaller models are already more capacity-constrained and have less room for information loss. If you need to quantize a small model aggressively, you may need to fine-tune the quantized model on your task to recover quality.

Quantization method also matters. GPTQ and AWQ typically lose 30 to 50 percent less quality than naive rounding quantization for INT4. For INT8, the difference is smaller because 8 bits provide enough precision that even naive quantization works well. If you are quantizing to INT4, use GPTQ or AWQ. If you are quantizing to INT8, naive quantization is fine and faster.

The practical recommendation is this. Start by quantizing to INT8 and measuring the quality impact on your eval set. If the degradation is under 2 percent, deploy INT8. If you need more cost savings, quantize to INT4 with GPTQ or AWQ and measure again. If the degradation is under 5 percent, deploy INT4. If the degradation is 5 to 10 percent, decide whether the cost savings justify the quality loss. If the degradation exceeds 10 percent, stick with INT8 or FP16, or consider a larger model that is more robust to quantization.

## Benchmarking Quantized Models Against Full-Precision Baselines

Benchmarking quantized models requires discipline. You cannot rely on vibes or spot-checking a few examples. You must run your full evaluation set on both the full-precision baseline and the quantized model and compare metrics side by side. For structured extraction tasks, compare precision, recall, and F1. For generation tasks, use LLM-as-judge or human eval to compare quality distributions. For classification tasks, compare accuracy and confusion matrices. For reasoning tasks, compare correctness rates.

The process is straightforward but time-consuming. Load the full-precision model, run your eval set, record metrics. Quantize the model to INT8, load the quantized model, run the same eval set, record metrics. Compare. If the drop is acceptable, deploy INT8. If not, stick with FP16. Repeat for INT4 if needed. Budget several days for this process. Quantization itself is fast, but running evals on thousands of examples takes time, especially for large models.

One subtlety is that quantization can affect not just accuracy but also output distribution. A quantized model might achieve the same average accuracy as the full-precision model but fail on different examples. This means your failure modes change, which can be problematic if you have carefully tuned your system around the baseline's failure patterns. Always review failure modes, not just aggregate metrics. If the quantized model fails on business-critical edge cases, the aggregate accuracy does not matter.

Another subtlety is that quantization interacts with prompt engineering. A prompt that works perfectly with FP16 might work slightly worse with INT4 because the model's internal representations have shifted. If you deploy a quantized model, revalidate your prompts and adjust if necessary. Sometimes rephrasing a prompt can recover 1 to 2 percent of the quality lost to quantization.

## When Quantization Is Worth It and When It Is Not

Quantization is worth it when you are self-hosting and cost or memory is a constraint. If your model barely fits on your available GPUs, quantizing to INT8 or INT4 allows you to fit it comfortably and potentially reduce the number of GPUs required. If your GPU cost is high and you can tolerate a small quality drop, quantization cuts cost by 50 to 75 percent. If you are deploying on edge devices or consumer hardware where memory is limited, quantization is often mandatory.

Quantization is not worth it when you are using API providers. OpenAI, Anthropic, Google, and other API providers decide whether and how to quantize their models. You do not control this and you do not need to. If the provider quantizes their models internally for cost savings, they handle the quality validation and you benefit from lower prices or faster latency. Your job is to evaluate the API-provided model's quality, not to worry about quantization. If the API model's quality is insufficient, switch to a larger or different model. Do not try to quantize an API model because you do not have access to the weights.

Quantization is also not worth it for quality-critical tasks where even 1 percent degradation is unacceptable. Medical diagnosis, legal contracts, financial compliance, and safety-critical systems should use full-precision models unless you have extensively validated that quantization does not affect critical decisions. The cost savings are not worth the risk of a quantization-induced error causing harm or liability. For these applications, pay for full-precision inference or use larger GPUs.

For most other self-hosted applications, quantization is worth evaluating. The potential cost savings are large and the quality impact is often small enough to be acceptable. The legal research platform from the opening story quantized their fine-tuned GPT-5.1 model to INT4 using AWQ. Their accuracy on legal precedent extraction dropped from 91.2 percent to 88.7 percent, a 2.5 percentage point drop. They judged this acceptable because their users reviewed and validated all extracted precedents anyway, so the tool was assistive rather than autonomous. The quantized model fit on one H100 instead of two, cutting their GPU cost from $42,000 per month to $18,000 per month. The product became economically viable and they retained their customer base.

## The Quantization Decision Framework

Here is the decision framework for quantization. First, determine if you are self-hosting or using APIs. If APIs, skip quantization. If self-hosting, proceed. Second, measure your baseline quality on your eval set with the full-precision model. Third, quantize to INT8 and measure quality again. If degradation is under 2 percent, deploy INT8. If degradation is 2 to 5 percent, decide based on cost-quality tradeoff. If you need further cost reduction, quantize to INT4 with GPTQ or AWQ and measure quality. If degradation from FP16 to INT4 is under 5 percent, deploy INT4. If degradation is 5 to 10 percent, decide based on cost-quality tradeoff and consider whether prompt tuning or fine-tuning the quantized model can recover quality. If degradation exceeds 10 percent, stick with INT8 or FP16.

Fourth, validate failure modes. Ensure the quantized model does not fail catastrophically on edge cases that the full-precision model handled. Fifth, revalidate prompts. Adjust prompts if necessary to maintain quality. Sixth, monitor quality in production. Quantization effects can interact with distribution shift or new use cases, so track quality metrics continuously and be prepared to roll back to full precision if needed.

This framework is not theoretical. It is how every competent team approaches quantization in 2026. You cannot skip the measurement step. You cannot assume quantization will work or that it will fail. You must test on your data and make an informed decision.

## Quantized Model Sources and Tooling

The easiest way to get started with quantized models is to use pre-quantized models from HuggingFace. Many popular models are available in INT8, INT4, GPTQ, AWQ, and GGUF formats. Search for your model name plus the quantization format, and you will often find community-contributed quantized versions. TheBloke, a prolific contributor, has quantized hundreds of models including Llama 4 Scout, Llama 4 Maverick, Mistral Large 3, Qwen3-235B, and many others. These quantized models are ready to download and serve with vLLM, llama.cpp, or Ollama.

If you need to quantize a custom model or a model that does not have a pre-quantized version, use AutoGPTQ for GPTQ quantization or AutoAWQ for AWQ quantization. Both are Python libraries that take a HuggingFace model and produce a quantized version. You provide a calibration dataset, typically a few thousand examples from your training or evaluation data, which the library uses to optimize the quantization. Quantization time ranges from minutes for small models to hours for models above 100 billion parameters. Once quantization completes, you save the quantized model and serve it with vLLM or TensorRT-LLM.

For GGUF quantization, use llama.cpp's quantization tools. You load a model in HuggingFace format, convert it to GGUF, and quantize to your desired level. llama.cpp supports quantization on CPU and does not require a GPU, which makes it accessible for teams without ML infrastructure. Ollama, a user-friendly tool for running local models, also supports GGUF models and makes deployment trivial. You download a GGUF model, run `ollama run model-name`, and you have a local inference server.

For serving quantized models at scale, vLLM and TensorRT-LLM are the standard choices. vLLM supports AWQ and GPTQ out of the box as of version 0.5. You specify the quantized model path, and vLLM loads it and serves it like any other model. TensorRT-LLM supports INT8 and INT4 quantization with its own quantization toolkit. TensorRT-LLM quantization is optimized for NVIDIA GPUs and produces faster inference than GPTQ or AWQ, but it requires converting your model to TensorRT format, which is more complex. For most teams, vLLM with GPTQ or AWQ is the easiest and most flexible option.

## Quantization-Aware Training and Fine-Tuning

If you quantize a model and the quality drop is too large, you can recover quality by fine-tuning the quantized model on your task. **Quantization-aware training** or **quantization-aware fine-tuning** trains the model while simulating the quantization process, which allows the model to adapt to the quantization noise. This produces a quantized model that is more accurate than post-training quantization.

The process is more involved than post-training quantization. You use a library like PyTorch's quantization tools or bitsandbytes to insert fake quantization operations into your model during training. These operations round weights and activations during the forward pass to simulate quantization, but they allow gradients to flow during the backward pass. The model learns to compensate for quantization errors. After training, you convert the model to true quantized format. Quantization-aware fine-tuning can recover 50 to 80 percent of the quality lost to naive quantization, which makes aggressive quantization levels like INT4 or INT3 viable for tasks where they would otherwise be too lossy.

Quantization-aware training is not common in 2026 because post-training quantization with GPTQ or AWQ works well for most use cases. But for teams that need to deploy very small models or very aggressive quantization on quality-sensitive tasks, quantization-aware fine-tuning is a valuable technique. Budget additional training time and compute, but the payoff is a quantized model that performs much closer to the full-precision baseline.

## Mixed-Precision and Dynamic Quantization

Advanced quantization techniques include **mixed-precision quantization**, where different layers or weights are quantized to different levels. Critical layers that contribute most to accuracy remain in FP16 or INT8, while less critical layers are quantized to INT4 or INT3. This balances memory savings with quality preservation. Mixed-precision quantization is supported by TensorRT-LLM and some research tools, but it is not yet standard in vLLM or mainstream serving stacks. Expect mixed-precision support to mature over 2026 and become common in 2027.

**Dynamic quantization** quantizes activations at runtime rather than statically quantizing weights. This can provide better quality than static quantization for some models, especially for tasks with highly variable input distributions. Dynamic quantization is more complex to implement and adds runtime overhead, so it is less common than static weight quantization. For most applications, static weight quantization with GPTQ or AWQ is sufficient.

## The Quantization Landscape Going Forward

Quantization is a mature technique as of 2026, but the tooling and methods continue to evolve. INT4 quantization is now mainstream for self-hosted deployments. INT3 and even INT2 quantization are active research areas and may become practical for specific models and tasks by 2027. Quantization-aware training is becoming easier with better library support. Mixed-precision quantization will become standard for balancing cost and quality. And hardware support for low-bit quantization continues to improve, with new GPU architectures providing faster INT4 and INT2 operations.

The strategic takeaway is that quantization is not a one-time decision. It is an ongoing optimization. As models evolve, as your tasks change, and as quantization techniques improve, you should periodically reevaluate your quantization strategy. A model that could not be quantized to INT4 in 2025 might be quantizable with new techniques in 2026. A task that required FP16 in 2025 might work with INT8 after prompt tuning in 2026. Stay current with quantization research and tooling, and revisit your quantization decisions every six months.

Quantization is one of the highest-leverage optimizations available for self-hosted inference. It cuts cost and memory by 50 to 75 percent with often minimal quality impact. But it requires careful evaluation, disciplined benchmarking, and ongoing validation. Teams that treat quantization as a rigorous engineering decision succeed. Teams that either ignore it or apply it blindly fail. Use the framework, measure the results, and deploy the quantization level that works for your task.

In the next subchapter, we explore model routing, the technique of dynamically selecting which model to use for each request based on task characteristics, cost, and latency requirements.
