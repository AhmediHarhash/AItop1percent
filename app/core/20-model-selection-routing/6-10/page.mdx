# 6.10 — Tokenizer Differences Across Models: Cost, Context, and Compatibility Pitfalls

In August 2025, a legal document processing company built a routing system that selected models based on estimated token counts to optimize cost. They measured prompts using OpenAI's tiktoken library, calculated cost per request, and routed longer documents to cheaper models. The system worked flawlessly in testing with GPT models. Then they added Claude Opus 4.5 as a routing option. Within a week, they saw cost overruns of 340% on requests routed to Claude. The problem wasn't usage volume. The problem was that Claude's tokenizer produced 40% more tokens than tiktoken for the same legal text. Complex legal terminology, Latin phrases, and specialized punctuation all tokenized differently. The team's cost estimates were based on GPT token counts. Claude charged based on Claude token counts. Every routing decision was based on incorrect cost assumptions. They'd optimized for a token count that didn't exist. By the time they rebuilt the cost estimation with per-model tokenizers, they'd burned through their margin for the quarter. The failure wasn't in the routing logic. The failure was in assuming that tokens are a universal unit. They're not. Every model family uses a different tokenizer, and the same text produces different token counts across models. If your multi-model architecture doesn't account for tokenizer differences, every cost estimate, every context window check, every routing decision is built on false data.

## Why Tokenizers Differ and Why It Matters

Tokenizers convert text into numerical sequences that language models process. Different model families use different tokenization algorithms trained on different corpora with different vocabularies. OpenAI's GPT models use tiktoken, a byte-pair encoding tokenizer with a vocabulary optimized for English and common programming languages. Anthropic's Claude models use a SentencePiece-based tokenizer with a vocabulary that handles multilingual text differently. Google's Gemini models use a custom SentencePiece variant. Meta's Llama models use a SentencePiece tokenizer with yet another vocabulary. DeepSeek, Qwen, and Mistral models each use tokenizers tuned for their training data.

The vocabulary size alone doesn't determine token counts. GPT-4 uses approximately 100,000 tokens in its vocabulary. Claude uses a different count. Gemini uses a different count. What matters more is which text sequences are represented as single tokens versus multi-token sequences. A tokenizer trained heavily on code might represent "function" as one token and common code symbols as single tokens. A tokenizer trained more on natural language might split "function" into "func" and "tion" and require multiple tokens for code symbols.

For English prose on common topics, most tokenizers produce similar token counts, typically within 10-15% of each other. For specialized domains, the variance explodes. Legal documents with Latin terms, medical records with pharmaceutical names, code files with specific framework syntax, multilingual text mixing scripts, or highly technical content can show 50% or greater variance in token count across tokenizers. If your routing system treats tokens as a constant unit, you're making decisions based on a measurement that changes depending on which model you choose.

## Token Count Estimation for Routing Decisions

The correct approach is to estimate token counts using each candidate model's tokenizer before making routing decisions. If you're choosing between GPT-5.1, Claude Opus 4.5, and Gemini 3 Pro for a request, you must run the prompt through all three tokenizers, calculate cost based on each model's actual token count and pricing, and only then make the routing decision. This triples your tokenization overhead but ensures accurate cost estimates.

The naive implementation is to install tokenizer libraries for every model family. OpenAI provides tiktoken. Anthropic doesn't provide an official tokenizer library but the community has reverse-engineered approximations. Google provides tokenizers for Gemini. Meta provides tokenizers for Llama. You import all of them, tokenize in parallel, and use the results. This works but introduces dependency hell. Each tokenizer library has different installation requirements, different API interfaces, different edge case handling. You need fallback logic for when a tokenizer library is unavailable or fails.

The professional implementation is to maintain a tokenizer service that encapsulates all model-specific tokenizers behind a uniform API. Your routing logic sends the prompt text and a list of candidate models. The tokenizer service returns token counts for each model. The routing logic uses those counts to calculate cost and context window fit. When a new model is added, you update the tokenizer service to include that model's tokenizer, and the routing logic requires no changes. This separation of concerns keeps tokenization complexity from leaking into routing logic.

Some teams take a different approach: they maintain token count ratios relative to a reference tokenizer. They measure, across a representative sample of their production text, that Claude produces on average 1.2 times the tokens that GPT-5 produces for the same text. Gemini produces 0.95 times the tokens. They use GPT-5 token counts as the reference and scale for other models. This is faster than running multiple tokenizers but introduces error. The average ratio might be 1.2, but for any specific text, the actual ratio could be 0.9 or 1.5. If your routing decision is close between two models, this error margin might cause suboptimal choices.

## Context Window Management Across Tokenizers

Context window sizes are specified in tokens, but which tokens? GPT-5.2's 1 million token context window means 1 million GPT-5.2 tokens, not 1 million Claude tokens or 1 million Gemini tokens. If you have a conversation history that's 400,000 GPT tokens and you route to Claude, you can't assume it fits in Claude's 500,000 token window. You must re-tokenize with Claude's tokenizer. It might actually be 520,000 Claude tokens, exceeding the context window.

This becomes critical when routing long-context tasks. A user uploads a 200-page document and asks questions about it. Your system ingests the document, stores it, and routes questions to various models based on query type. If you check context window fit using GPT token counts, then route to Claude, the request might fail because the document doesn't actually fit in Claude's window when tokenized with Claude's rules. You need either conservative margins or per-model tokenization before every routing decision.

The conservative margin approach is to measure the worst-case tokenizer for your content and use that as the basis for context window checks. If Claude consistently produces 1.3 times the tokens that GPT produces for your document types, you treat all context windows as if they were divided by 1.3. This wastes capacity but guarantees safety. A 500,000 token Claude window becomes 385,000 tokens in your planning, leaving margin for the tokenization variance. You never route content that might exceed a model's actual capacity.

The per-model tokenization approach is to tokenize with the target model's tokenizer before every routing decision involving long context. This is computationally expensive. Tokenizing a 200-page document might take several seconds depending on the tokenizer implementation. You can amortize this cost by caching: tokenize the document once with all candidate model tokenizers, store the token counts, and reuse them for all subsequent routing decisions involving that document. When the document changes, invalidate the cache and re-tokenize.

## The Fits-in-One-Model-But-Not-Another Problem

Tokenizer differences create a perverse scenario: you have a prompt that fits comfortably in one model's context window but exceeds another model's window, even though the second model advertises a larger window. GPT-5.2 has a 1 million token window. Claude Opus 4.5 has a 500,000 token window but Claude's tokenizer is more efficient on certain text types. A highly technical document might be 450,000 GPT tokens but only 380,000 Claude tokens. It fits in Claude even though Claude's window is nominally smaller.

This breaks simplistic routing rules like "always use the largest context window model for long documents." The largest window by token count isn't necessarily the largest by capacity for your specific text. You must measure actual fit, not nominal capacity. Some teams discover this when they route all long-context tasks to GPT-5.2 and later realize they could have used cheaper models with smaller nominal windows because those models' tokenizers were more efficient on their content.

The opposite scenario is more dangerous: your text fits in the model you develop against but fails in production when routed to a different model. You test with GPT-5, confirm the prompt is 30,000 tokens, well under the 128,000 token window of GPT-5-mini, and set up routing to send these prompts to GPT-5-mini in production. But GPT-5-mini uses a different tokenizer configuration and the same prompt is actually 135,000 tokens. Every request fails with context window exceeded errors. Your testing didn't catch this because you measured tokens with the wrong tokenizer.

## Cost Estimation and Budget Control

Token-based pricing makes tokenizer differences a direct financial issue. As of early 2026, GPT-5.1 might charge 5 dollars per million tokens, Claude Opus 4.5 might charge 8 dollars per million tokens, and Gemini 3 Pro might charge 3 dollars per million tokens. If your text is 100,000 GPT tokens, 130,000 Claude tokens, and 95,000 Gemini tokens, the actual cost per request is 50 cents with GPT, 1.04 dollars with Claude, or 28.5 cents with Gemini. If you estimated cost using GPT token counts across all models, you'd think Claude costs 80 cents per request and Gemini costs 30 cents. Your routing decision based on those estimates would be correct by luck for Gemini but wrong for Claude.

Teams that run multi-model production systems with tight budget constraints must implement per-model token accounting. For every request, log the model used, the actual token count reported by that model's API, and the cost calculated from that count. Aggregate these logs to track spending per model, compare estimated versus actual costs, and detect token count estimation drift. If your estimates are consistently 20% low for Claude, you're underbudgeting for Claude usage.

Some APIs return token counts in their response metadata. OpenAI's API returns prompt_tokens and completion_tokens in the usage field. Claude's API returns token counts. Gemini's API returns token counts. If you rely on these counts for billing reconciliation, you can catch estimation errors in real time. If your pre-request estimate was 50,000 tokens and the API reports 65,000 tokens, you know your tokenizer estimate was wrong and can adjust your models.

Other teams implement token budgets per request type. A document summarization request gets a budget of 150,000 tokens all-in, prompt plus completion. Before routing, they check which models can complete the task within budget. This requires accurate tokenization: if you estimate the prompt at 80,000 tokens using GPT tokenization but route to Claude where it's actually 100,000 tokens, a response that would fit budget with GPT exceeds budget with Claude. You either reject valid requests or blow the budget, depending on your enforcement mechanism.

## Tokenizer-Aware Prompt Design

Prompt engineering must account for tokenizer differences when targeting multiple models. A prompt optimized for GPT's tokenizer might be inefficient when run through Claude's tokenizer. Specific word choices, punctuation patterns, and formatting can have different token counts across tokenizers. If your prompt includes large JSON examples because they're compact in GPT tokens, they might be much less compact in Claude tokens due to how Claude tokenizes brackets and special characters.

Some teams maintain model-specific prompt templates. The core instruction is the same but the formatting, examples, and structure are adjusted for each model's tokenizer and training. A Claude prompt might use more natural language phrasing because Claude's tokenizer handles prose efficiently. A GPT prompt might use more structured formatting because GPT's tokenizer handles common structural patterns efficiently. This doubles prompt maintenance but optimizes token usage per model.

Multilingual prompts face the biggest tokenizer sensitivity. If your system supports multiple languages and uses the same prompt structure across languages, token counts will vary wildly across models. English text might tokenize efficiently in all models because all were trained heavily on English. Japanese text might tokenize very differently: models trained with more Asian language data will use fewer tokens, models trained primarily on English will use many more tokens. A prompt that's 5,000 tokens in English might be 8,000 tokens in Japanese with one tokenizer and 12,000 tokens with another. Your routing logic must account for this or face unexpected context window and cost issues.

Code prompts are another high-variance domain. Different tokenizers handle programming languages differently based on their training data. A Python code snippet might tokenize efficiently in GPT because GPT was trained on massive code datasets. The same Python might be less efficient in a model trained more on natural language. Prompt templates that include code examples should be tested with all target models' tokenizers to ensure token counts are acceptable. You might need separate code-optimized prompts for models with inefficient code tokenization.

## Tools and Libraries for Multi-Tokenizer Environments

Several tools help manage tokenizer differences. The tiktoken library supports multiple OpenAI models. The transformers library from Hugging Face includes tokenizers for many open models. Google provides cloud libraries with Gemini tokenizers. DeepSeek and other providers often publish tokenizer configurations that can be loaded into standard libraries. The challenge is integrating these disparate tools into a unified system.

One effective pattern is to build a tokenization abstraction layer. Define an interface with a single method: count tokens for this text using this model. Implement that interface with adapters for each tokenizer library. Your routing logic calls the interface without knowing which library is used under the hood. This keeps tokenizer implementation details encapsulated and makes it easy to add new models or swap tokenizer libraries when better options emerge.

Caching is essential. Tokenizing large documents repeatedly is wasteful. Implement a token count cache keyed by content hash and model identifier. When you need token counts for a document, check the cache first. If present, use the cached count. If not, tokenize, cache the result, and return it. This is safe because tokenizers are deterministic: the same text always produces the same token count for a given model version. If a model updates its tokenizer, you version the cache key to force re-tokenization.

Some teams implement approximate token counting using heuristics. They measure average characters per token for each model on their production data, then estimate tokens as character count divided by characters per token. A GPT model might average 4 characters per token, Claude might average 3.5 characters per token. This is much faster than actual tokenization but introduces error. It's acceptable for rough filtering—eliminating models that clearly won't work—but not for final routing decisions or cost accounting.

## Handling Tokenizer Updates and Model Versioning

Model providers occasionally update tokenizers when releasing new model versions. GPT-4 and GPT-5 use different tokenizers. Claude 3 and Claude 4 use different tokenizers. When a model's tokenizer changes, all your token count assumptions for that model become invalid. Your cached counts, your cost estimates, your context window checks, all based on the old tokenizer, are now wrong.

The robust approach is to version tokenizers alongside models in your configuration. When you configure a model for routing, you specify not just "GPT-5" but "GPT-5 with tiktoken version X." When a provider updates the tokenizer, you add a new configuration entry for the updated version and gradually migrate. During migration, you maintain both tokenizers, re-measure token counts with the new tokenizer, compare to the old, and validate that routing decisions remain sound. Only after validation do you fully switch over.

Some teams discover tokenizer updates through production monitoring. Suddenly, costs for a model spike or context window errors increase. They investigate and realize the API is returning different token counts than their estimates. The model provider updated the tokenizer without clear notification. This is why you must always compare pre-request token estimates to post-request actual counts reported by the API. Divergence indicates either estimation errors or tokenizer changes, and either requires investigation.

Longer term, model providers are moving toward more standardized tokenization, but as of early 2026, the ecosystem remains fragmented. Every major model family still uses a proprietary tokenizer optimized for its training data and architecture. If you're building multi-model systems, tokenizer management isn't optional infrastructure. It's core to cost control, reliability, and correct routing decisions. Treat tokenizers as first-class components of your model metadata, measure token counts explicitly for each model, and never assume that a token is a token across providers.

With context format normalization and tokenizer management in place, your multi-model routing system has the foundation to make accurate, cost-effective decisions while preserving conversation quality and staying within technical constraints.
