# 1.9 — Model Families and Versioning: Understanding Generations, Snapshots, and Dated Releases

In March 2025, a customer support platform serving a healthcare technology company discovered that their AI-powered triage system had begun refusing to answer previously acceptable questions about medication scheduling. The team had not changed any code. Their prompt templates were identical. Their evaluation suite showed a sudden 31% drop in recall for routing patient inquiries to the correct department. The root cause took six days to identify: OpenAI had silently updated the weights behind the gpt-5 alias to point to a new snapshot with different refusal behavior. The team had been calling gpt-5 assuming it was stable. It was not. The incident cost them $127,000 in manual review overhead and damaged trust with three hospital system clients. The diagnosis was simple: they had treated a rolling alias as if it were a pinned version. They learned the hard way that gpt-5 is not a model — it is a pointer that moves.

Model versioning is not an implementation detail. It is a production risk surface that determines whether your system behaves consistently over time or drifts unpredictably. Every major provider uses a different versioning scheme, and every scheme has traps for teams that do not understand the distinction between families, generations, snapshots, and aliases. You must know what you are calling, what guarantees it carries, and when it will disappear.

## The Concept of Model Families

When you refer to GPT-5, you are not naming a single model. You are naming a **model family** — a collection of related models released under a shared brand, each optimized for different cost-performance tradeoffs. The GPT-5 family as of January 2026 includes GPT-5 (the flagship), GPT-5-mini (smaller, faster, cheaper), GPT-5-nano (edge-optimized), GPT-5.1 (incremental improvement), and GPT-5.2 (latest refinement). Each has different capabilities, different costs, and different versioning behaviors. Calling them all GPT-5 is like calling every car in a manufacturer's lineup by the brand name — it obscures critical distinctions.

Claude operates as a family with Opus, Sonnet, and Haiku tiers. As of January 2026, the Claude family includes Opus 4, Opus 4.1, Opus 4.5, Sonnet 4.5, and Haiku 4.5. Opus is the reasoning-heavy flagship. Sonnet is the balanced workhorse. Haiku is the fast, low-cost option. Each tier targets different use cases. Opus handles complex legal analysis and multi-step research. Sonnet handles conversational AI and content moderation. Haiku handles classification and simple extraction. Treating them as interchangeable because they all start with Claude is a category error.

Gemini follows a similar pattern. The Gemini family includes Gemini 2.5 (legacy), Gemini 3 Pro (flagship), Gemini 3 Flash (fast variant), and Gemini 3 Deep Think (extended reasoning mode). Each variant has different latency profiles, different token limits, and different pricing. Google deprecates older family members aggressively. Gemini 2.5 was deprecated eight months after Gemini 3 Pro launched. If you were on 2.5, you had a hard cutoff. Teams that did not plan migrations in advance faced forced rewrites under time pressure.

Meta's Llama 4 family includes base models (Llama 4 8B, Llama 4 70B, Llama 4 405B) and fine-tuned variants (Llama 4 Instruct, Llama 4 Code). DeepSeek's V3.2 includes a base model and chat-tuned variant. Qwen3 ships in multiple sizes. Every provider has a family structure, and every family has internal variation. You must know which member of the family you are using, because performance, cost, and deprecation timelines differ.

## Rolling Aliases versus Dated Snapshots

OpenAI uses a dual versioning system. You can call a **rolling alias** like gpt-5, which always points to the latest recommended version, or you can call a **dated snapshot** like gpt-5-2025-08-06, which is pinned to the exact weights trained and frozen on that date. The alias is convenient for prototyping. The snapshot is mandatory for production. If you call gpt-5, you are accepting that the model may change behavior without notice. If you call gpt-5-2025-08-06, you have a stability guarantee until the deprecation date.

The healthcare triage system used gpt-5 because it seemed simpler. They did not realize that OpenAI updates the alias every few weeks as new snapshots are released. When gpt-5 was redirected to a snapshot with stricter medical content policies, the refusal rate spiked. The team had no changelog, no migration plan, and no warning. They had opted into unpredictable drift.

Anthropic follows a similar pattern. You can call claude-opus-4 (rolling alias) or claude-opus-4-20250610 (dated snapshot). The alias tracks the recommended version for that tier. The snapshot is frozen. If you want stability, you pin to the snapshot. If you want automatic improvements, you use the alias. The choice is explicit. Most production teams pin to snapshots and test new snapshots in staging before promoting them.

Google's Gemini API uses snapshot identifiers like gemini-3-pro-20250115. There is no widely used rolling alias. You must specify a snapshot. When a new snapshot releases, you must explicitly migrate. This forces teams to manage version transitions deliberately, but it eliminates silent drift. The tradeoff is clear: you gain control, you lose convenience.

Meta's Llama models are versioned by training checkpoint, not by date. Llama 4 70B Instruct v1.3 is distinct from v1.4. Each checkpoint is permanently available via Hugging Face or direct downloads. There is no deprecation timeline because you host the weights yourself. The versioning problem shifts from provider changes to internal model registry hygiene. You must track which checkpoint is deployed where, and you must manage upgrades manually.

## Why You Must Pin to Dated Versions in Production

Production systems require predictable behavior. If your model's refusal policy, output format, reasoning depth, or tone shifts unexpectedly, your evaluations break, your guardrails misfire, and your user experience degrades. Pinning to dated snapshots prevents silent drift. You control when the model changes. You test the new version in staging. You verify that evaluations pass. You validate that guardrails still work. Only then do you promote to production.

A legal research SaaS company learned this in mid-2025 when an unplanned GPT-5 alias update caused their citation extraction pipeline to change format. The new snapshot used slightly different markdown for case law citations. Their regex parsers failed. They discovered the issue when a customer reported missing citations in a brief filed in federal court. The incident required a public apology, a refund, and a full audit of six weeks of generated briefs. The root cause was using gpt-5 instead of gpt-5-2025-05-12. They had no control over when the change occurred.

After the incident, they rewrote their deployment process. Every model call now uses a dated snapshot. The snapshot identifier is stored in their model registry. When a new snapshot is released, they create a feature branch, update the registry, run the full evaluation suite, and deploy to a canary environment. If evaluations pass and the canary shows no regressions, they promote to production. If evaluations fail, they investigate whether the new snapshot breaks assumptions or whether their evaluations need updating. Either way, the change is deliberate and controlled.

This process adds overhead. It requires monitoring OpenAI's release notes, testing new snapshots, and maintaining a rollback plan. But it eliminates surprise breakages. The team has not experienced an unplanned model-related incident since adopting snapshot pinning. The incremental cost of version management is far lower than the cost of a single production failure.

## The Deprecation Cycle and Migration Windows

Every model snapshot has a finite lifespan. Providers deprecate old snapshots to reduce infrastructure costs and push customers toward newer, better models. OpenAI typically provides six to twelve months of notice before deprecating a snapshot. Anthropic provides similar migration windows. Google deprecates more aggressively, sometimes giving only three to six months. You must track deprecation timelines and plan migrations in advance.

OpenAI publishes deprecation notices in their changelog and sends email notifications to API customers using deprecated snapshots. If you are on gpt-5-2025-02-14 and OpenAI announces deprecation for August 2026, you have six months to test and migrate to a newer snapshot. If you ignore the notice, your API calls will fail when the cutoff date arrives. There is no grace period. The model disappears.

A fintech startup discovered this the hard way in late 2025. They had built a transaction explanation feature using gpt-4o-2024-11-20. They received the deprecation notice in May 2025 but assumed they had time. They prioritized other features. In November 2025, their API calls started returning 404 errors. The model was gone. They had to emergency-migrate to gpt-5-2025-08-06 with no time for evaluation. The new model had different output formatting, which broke their downstream parsers. They spent three days firefighting to restore service. The incident was entirely avoidable.

After the failure, they implemented a deprecation tracking system. Every dated snapshot in their model registry has an expiration field. A cron job checks the registry weekly and flags snapshots within 90 days of deprecation. The team receives a Slack alert. They treat deprecation migrations as scheduled maintenance, not surprises. They test the replacement snapshot, validate evaluations, and deploy with a rollback plan. They have not missed a deprecation deadline since.

Anthropic's deprecation policy is similar. They announce timelines in advance and recommend migration paths. If claude-opus-4-20250215 is deprecated, they recommend moving to claude-opus-4-20250810 or claude-opus-4.5-20251022. The recommended path usually represents a capability upgrade, but it may also introduce behavior changes. You must test before migrating.

Google's aggressive deprecation timelines require faster response. When Gemini 3 Pro launched, Gemini 2.5 was marked for deprecation within eight months. Teams had less time to migrate. Some teams were forced to accelerate timelines or risk outages. The lesson is clear: if you build on Google models, you must budget time for frequent version upgrades. The models improve faster, but the operational overhead is higher.

## How Minor Version Bumps Can Break Your System

Not all version changes are created equal. A major version jump from GPT-5 to GPT-6 signals substantial architectural changes. A minor version bump from GPT-5.1 to GPT-5.2 suggests incremental improvements. But even minor bumps can introduce breaking changes. Refusal policies may tighten. Output formats may shift. Reasoning depth may increase or decrease. Tone may change. You cannot assume that 5.2 is a drop-in replacement for 5.1.

A travel booking platform experienced this in October 2025. They upgraded from gpt-5.1-2025-06-10 to gpt-5.2-2025-10-03 expecting a transparent improvement. The new snapshot had better reasoning for complex multi-city itineraries, which was positive. But it also had a stricter refusal policy for ambiguous requests. When users asked vague questions like "find me a cheap flight next week," the new model refused more often, asking for clarification. The old model had made assumptions and returned suggestions. The refusal rate increased from 4% to 19%. Customer satisfaction dropped. Support tickets spiked.

The team rolled back to the old snapshot within 24 hours. They then ran a detailed comparison, sending 10,000 historical queries to both snapshots and comparing outputs. They discovered that the new snapshot required more explicit constraints in the prompt. By adding a fallback instruction — "if the request is ambiguous, state your assumptions and provide suggestions anyway" — they reduced the refusal rate to 6%, better than the original baseline. They re-deployed the new snapshot with the updated prompt and saw improved itinerary quality with acceptable refusal rates.

The lesson is that minor version bumps require the same validation rigor as major upgrades. You must test behavior, not just trust version numbers. A .1 to .2 bump is not guaranteed to be safe.

## The Model Registry Concept

A **model registry** is an internal system of record that tracks which model version is deployed in which environment for which task. It answers questions like: What snapshot is running in production for customer support? What snapshot is running in staging for content moderation? What snapshot did we use to generate outputs in the audit log from three months ago?

Without a registry, you have no reliable way to reproduce past behavior or correlate model changes with performance shifts. If you notice a drop in classification accuracy, you cannot determine whether it was caused by a model update, a prompt change, or a data shift. If a customer disputes a generated output from two months ago, you cannot identify which model version produced it. The registry provides traceability.

A minimal registry is a configuration file or database table with columns for task name, environment, model identifier, deployment date, and deprecation date. When you deploy a new snapshot, you update the registry. When you roll back, you update the registry. When you investigate an incident, you query the registry to see what was running at the time.

A more sophisticated registry integrates with your deployment pipeline. Every model call logs the snapshot identifier to your telemetry system. Your evaluation harness reads snapshot identifiers from the registry and runs tests against the specified version. Your rollback script reverts the registry entry and redeploys the previous snapshot. The registry becomes the single source of truth for model versions across your infrastructure.

A large e-commerce company built a registry in early 2025 after experiencing multiple version-related incidents. They catalog every model snapshot used across 40 different AI features. Each feature has a primary model and optional fallback models. The registry tracks snapshot identifiers, cost per million tokens, average latency, evaluation pass rates, and deprecation dates. When they plan a migration, they query the registry to see which features are affected. When they investigate a bug, they query the registry to see what changed. The registry saved them an estimated $400,000 in incident response costs in 2025 alone.

## Documenting and Managing Model Versions Across Your Organization

As your AI surface area grows, the number of model versions in use grows. Different teams may choose different snapshots for different reasons. Product may prefer the latest snapshot for new features. Legal may prefer an older, well-tested snapshot for compliance-sensitive tasks. Engineering may run experiments on pre-release snapshots. Without centralized documentation, you lose visibility into what is running where.

The solution is a versioning policy and a shared registry. The policy defines rules: production systems must use dated snapshots, not aliases; snapshots must pass evaluation before deployment; deprecated snapshots must be migrated at least 30 days before cutoff; all snapshot changes must be logged in the registry. The registry enforces the policy by making version information visible and auditable.

A healthcare SaaS company formalized their versioning policy in mid-2025 after the silent alias update incident. They require that every AI feature specify its model version in a central YAML configuration file. The file is version-controlled. Changes require code review. Deployment scripts read the file and validate that the specified snapshot is not deprecated. If a snapshot is within 60 days of deprecation, the deployment fails with an error message directing the team to migrate.

They also implemented a quarterly model version review. Every team presents their current snapshot choices and justifies why they have not upgraded to newer versions. The review surfaces teams that are falling behind on migrations and teams that are moving too fast without adequate testing. The review creates accountability and ensures that versioning is treated as a shared operational concern, not an afterthought.

The overhead is real. Maintaining a registry, enforcing policies, and running reviews requires time. But the alternative is chaos. Teams that do not manage versions systematically experience unpredictable breakages, debugging nightmares, and loss of trust. Teams that manage versions deliberately experience stability, reproducibility, and confidence.

## Versioning for Open-Source and Self-Hosted Models

If you use open-source models like Llama 4, DeepSeek V3.2, or Qwen3, you control the weights. There is no API provider changing snapshots on you. But you still have a versioning problem. Which checkpoint did you download? Which fine-tuned variant are you running? Which quantized version are you serving? If you update the checkpoint, did you test the new version? If you roll back, do you know which version you rolled back to?

Self-hosted models require even stricter registry discipline because there is no external authority tracking versions for you. You must document the model artifact source, the download date, the checksum, the quantization method, the serving framework version, and the deployment date. If you fine-tune the model, you must version the fine-tuned weights separately and document the training data version and hyperparameters.

A legal tech company hosts Llama 4 70B Instruct internally for document review. They version every model artifact in their internal model store with semantic versioning: llama-4-70b-instruct-v1.3.2. The first digit is the upstream Llama release. The second digit is their fine-tuning iteration. The third digit is the quantization or serving optimization version. They treat model artifacts like software releases. Each version has a changelog, a test report, and a deployment record.

When they fine-tune a new version, they run a regression suite against the previous version. If the new version improves accuracy by 3% but increases refusal rate by 8%, they document the tradeoff and decide whether to deploy. If they deploy, they keep the old version in warm standby for 30 days in case they need to roll back. They have rolled back twice in 18 months, both times due to unexpected behavior discovered in production. The rollback was seamless because they had the previous version ready.

## Building Organizational Muscle for Version Management

Model versioning is not a one-time setup task. It is an ongoing operational discipline. Every new model release requires a migration decision. Every deprecated snapshot requires a migration plan. Every version change requires validation. Teams that treat versioning as a checklist item fail. Teams that treat versioning as a core competency succeed.

The muscle you must build includes: monitoring provider release notes, testing new snapshots in staging, running evaluation suites before promotion, maintaining a model registry, enforcing snapshot pinning policies, tracking deprecation timelines, documenting version changes, and running periodic version reviews. These are not optional hygiene tasks. They are the foundation of production stability.

A financial services company embedded versioning discipline into their sprint planning. Every sprint includes a model version review task. The team checks for new snapshots, checks deprecation timelines, and updates the registry. If a snapshot is nearing deprecation, they schedule migration work in the backlog. If a new snapshot offers meaningful improvements, they schedule evaluation and testing. Versioning is not an interrupt; it is planned work.

The result is that they have never experienced an unplanned model-related outage. They have migrated through six major provider version changes in 18 months with zero incidents. Their customers experience consistent behavior. Their engineers trust the system. Their compliance team can audit every model version used in regulated outputs.

Model families and versioning are not abstract concepts. They are the infrastructure layer that determines whether your AI system behaves predictably or drifts into chaos. You must understand the difference between families, generations, snapshots, and aliases. You must pin to dated snapshots in production. You must track deprecation timelines. You must test version changes before deploying them. You must maintain a registry. You must build organizational discipline around version management. If you do, your system will remain stable as the model landscape evolves. If you do not, you will firefight version-related incidents until you learn the lesson the hard way.

In the next subchapter, we examine the capability tiers that distinguish models across dimensions like reasoning, instruction following, creativity, factuality, and safety — and why the best model overall is often not the best model for your specific task.
