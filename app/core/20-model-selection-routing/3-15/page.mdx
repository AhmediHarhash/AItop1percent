# 3.15 — Router Drift: When Routing Quality Silently Degrades

In August 2025, a legal technology company discovered that their intelligent routing system had been making increasingly poor decisions for nearly four months. The router, which directed queries between a fast semantic search model and a slower reasoning model for complex legal analysis, was sending 61% of queries to the expensive model—up from 38% six months earlier. The team had assumed this reflected changing user behavior. When they finally investigated, they found something worse: the router had become fundamentally miscalibrated. A June model update had doubled the cheap model's capability for statute interpretation, but the router still assumed it needed the expensive model for those queries. Meanwhile, users had started asking more procedural questions, which the router's complexity classifier—trained on 2024 data—consistently overestimated. The company had spent an extra $340,000 over four months routing queries that the cheap model could have handled perfectly. The router's logic hadn't changed, but the world around it had, and nobody had been watching for that silent degradation.

This is **router drift**—the gradual erosion of routing quality that happens when your router's decision logic remains static while the environment it operates in evolves. Router drift is insidious because the system keeps working. Queries still get answers. Response times stay reasonable. But beneath the surface, you're either wasting money by over-routing to expensive models, degrading quality by under-routing to inadequate models, or both at once in different query segments. The router was optimized for a world that no longer exists, and every day that misalignment deepens.

Router drift differs from other forms of model drift because it operates at the meta level. Your primary models might be performing perfectly. Your evaluation metrics might show stable quality. But the layer that decides which model sees which query has quietly lost its calibration, and that miscalibration compounds across every request. By the time you notice the symptoms—a cost spike, a quality drop, a user complaint pattern—the drift has been accumulating for weeks or months.

## The Four Sources of Router Drift

Router drift emerges from four distinct but often overlapping sources. **Query distribution drift** happens when the types of queries your users send change over time. Your router was calibrated on historical query patterns, but user behavior shifts. A customer service application might see query complexity increase as users learn the system handles sophisticated requests. A research tool might see query specificity decrease as it gains mainstream adoption. A chatbot might see domain drift as the product expands into new use cases. Your router's complexity classifier, trained or configured on January query patterns, encounters June queries that follow different distributions. The classification boundaries that worked six months ago now misfire systematically.

You see query distribution drift most clearly in seasonal businesses. A tax preparation service's queries in February are fundamentally different from its queries in September. An e-commerce platform sees different query complexity during holiday shopping than during off-season browsing. An educational platform's query patterns shift between semesters. If your router was calibrated during one phase of the cycle and runs unchanged through the next phase, it will misroute predictably. The router learned that certain keywords correlate with complexity during tax season, but those same keywords during planning season represent simpler queries.

**Model capability drift** occurs when the models in your routing pool change their capabilities, but your router continues using outdated assumptions about what each model can handle. This happens most commonly with provider-side model updates. OpenAI releases GPT-5.2, which handles multi-step reasoning substantially better than GPT-5.1. Your router still uses a complexity threshold calibrated for GPT-5.1, so it over-routes to your expensive fallback model for queries that GPT-5.2 now handles easily. Anthropic updates Claude Opus 4.5 with better tool use, expanding the boundary of what the cheaper model can accomplish. Your router doesn't know this, so it continues escalating tool-heavy queries to an even more expensive model unnecessarily.

Model capability drift also works in reverse. A provider might degrade a model's capabilities to reduce their costs, or an update might introduce regressions in specific domains. Your router assumes the cheap model still handles certain queries well, but after the update it doesn't. You start seeing quality degradation in the queries routed to that model, but because the router logic hasn't changed, you don't immediately connect the degradation to routing decisions. You might blame the model generally, missing that the specific problem is routing queries that now exceed the model's capability.

**Cost drift** emerges when pricing changes but your router's cost-benefit calculations don't update. Model providers adjust pricing frequently. A model that cost 12 dollars per million tokens in January might cost 8 dollars in June due to efficiency improvements or competitive pressure. If your router makes decisions based on a cost ratio of expensive model to cheap model being 10 to 1, but that ratio is now 6 to 1, the optimal routing boundary has shifted. You should be routing more aggressively to the previously expensive model because it's now relatively cheaper, but your router doesn't know that.

Cost drift also happens when your usage volume crosses pricing tiers. Many providers offer volume discounts. At 100 million tokens per month, you might pay full price for both models in your pool. At 500 million tokens per month, you might get 30% off the expensive model but only 15% off the cheap model, changing the cost ratio and thus the optimal routing threshold. Your router makes decisions based on list pricing, but you're actually operating under volume pricing, creating a systematic bias toward under-utilizing the model that got the bigger discount.

**Feature drift** occurs when the signals your router uses to make decisions lose their predictive relationship to the outcomes you care about. Your router might use query length as a complexity signal, which worked well when long queries genuinely correlated with complex intent. But over time, users learned that longer queries get better results, so they started padding queries with context regardless of actual complexity. Query length no longer predicts complexity well, but your router still weights it heavily. Your classifier drifts from predictive to decorrelated.

Feature drift is particularly common in learned routers that use embedding similarity or clustering. The semantic space your users operate in shifts. New terminology emerges. Old terminology gains new meanings. Domain jargon evolves. Your router's feature representations, learned from 2024 data, no longer capture the semantic distinctions that matter in 2026. A router that classifies queries as technical versus general based on embeddings might drift as the boundary between technical and general conversation shifts in your user population.

## Detecting Router Drift in Production

Detecting router drift requires intentional monitoring that most organizations don't implement by default. The simplest detection method is **routing rate monitoring**—tracking what percentage of queries go to each model over time. A steady increase or decrease in routing rates, absent corresponding changes in product usage or user behavior you can explain, signals drift. If your expensive model was seeing 22% of queries in January and is seeing 34% in June, something has changed. Either your users are genuinely asking harder questions, or your router is miscalibrated. Routing rate monitoring won't tell you which, but it tells you to investigate.

Effective routing rate monitoring requires segmentation. Overall routing rates might look stable while specific segments drift significantly. Monitor routing rates by user cohort, by query topic, by time of day, by session type. A routing rate shift isolated to a specific segment reveals drift in how your router handles that segment's characteristics. If enterprise users see routing rate increases while consumer users don't, your router might be drifting on features that correlate with enterprise usage patterns. If evening queries route differently than morning queries of similar content, your router might have temporal features drifting.

More sophisticated drift detection uses **feature distribution monitoring**. Track the distributions of the features your router uses to make decisions. If your router considers query length, track the distribution of query lengths over time. If it considers embedding similarity to reference classes, track those similarity scores over time. If it considers token count or estimated complexity, track those distributions. When a feature distribution shifts significantly, your router's decision boundaries—calibrated on the old distribution—will produce different routing patterns even if the router logic hasn't changed. A shift in average query length from 47 tokens to 68 tokens might push a significant fraction of queries across a complexity threshold, changing routing rates without any change in actual complexity.

Feature distribution monitoring requires establishing baselines and alerting on deviations. Use statistical process control: calculate mean and variance for each feature over a baseline period, then alert when current values exceed two or three standard deviations from baseline. Use distribution distance metrics like KL divergence or Kolmogorov-Smirnov statistic to detect when the current feature distribution diverges from the baseline distribution. These techniques catch drift early, before it substantially impacts cost or quality.

The most direct drift detection method is **oracle comparison**. Periodically sample queries, run them through all models in your routing pool, and check whether the router's decision matches the decision you would make knowing all models' actual outputs. This is expensive—you're deliberately running queries through models the router chose not to use—but it's the ground truth test of routing quality. If your router sends a query to the expensive model, but the cheap model would have produced an equally good answer, that's a routing error. Track the rate of routing errors over time. An increasing error rate is definitive evidence of drift.

Oracle comparison requires defining what constitutes a routing error. For quality-based routing, a routing error is when the router chose model A but model B would have met the quality threshold. For cost-based routing, an error is when the router chose the more expensive model but the cheaper model would have succeeded. For hybrid routing, you need a combined metric that considers both cost and quality. Define your error criteria explicitly, automate the comparison, and track error rates as a key drift metric.

You can reduce oracle comparison costs by sampling strategically. Don't oracle-check every query. Sample 1% of queries, or 100 queries per day, or queries flagged by other drift signals. Stratified sampling is more efficient: sample proportionally from each routing decision. If 80% of queries go to the cheap model and 20% go to the expensive model, sample 80 queries from the cheap-model route and 20 from the expensive-model route. This ensures you catch drift in both routing directions, not just the majority case.

## Drift Correction Strategies

Once you've detected drift, correction requires updating the router's decision logic to reflect current reality. For query distribution drift, the correction is **recalibration**—retrain or reconfigure your router on recent data. If your router uses a learned classifier, retrain it on queries from the last month or quarter rather than the historical dataset it was originally trained on. If it uses rule-based thresholds, re-analyze recent queries to set thresholds that split current query complexity appropriately. Recalibration treats the current query distribution as the new normal and optimizes routing for it.

Recalibration frequency depends on how quickly your query distribution changes. In stable domains, quarterly recalibration might suffice. In rapidly evolving products, monthly or even weekly recalibration might be necessary. Automate recalibration by scheduling regular retraining jobs that pull recent production data, retrain the router, validate routing quality on a holdout set, and deploy the updated router if quality improves. This turns drift correction from a reactive fire drill into a routine maintenance task.

For model capability drift, correction requires **re-benchmarking**—test current model capabilities and update your router's assumptions. When a model updates, run your routing benchmark suite against the new version. Measure where the capability boundaries have moved. If the cheap model now handles 15% more of your complexity distribution, adjust your routing thresholds to take advantage of that. If the expensive model regressed in a specific domain, adjust routing to avoid sending those queries there or add a fallback route.

Re-benchmarking should happen automatically when you detect a model version change. Instrument your application to log model version identifiers in responses. When the version changes, trigger a benchmark run. Compare new benchmark results to previous results. Calculate the change in capability boundaries. Update routing configuration to match. This prevents capability drift from accumulating invisibly.

For cost drift, correction is straightforward: **update cost parameters**. When model pricing changes, update the cost values in your router's decision logic. If you're using cost-per-token ratios, recalculate them. If you're using cost-benefit optimization, update the cost side of the equation. If you're using budget-based routing, adjust budget allocations to reflect new prices. Cost parameter updates should happen immediately when pricing changes, not on a delayed calibration cycle.

Automate cost updates by pulling current pricing from provider APIs or configuration management systems. Don't hardcode prices in router logic. Store them in a configuration service that you update when providers announce price changes. Your router reads current prices dynamically, so updates take effect without code changes. This eliminates the delay between a price change and your router adapting to it.

For feature drift, correction requires **feature engineering review**—identify which features have lost predictive power and replace or reweight them. If query length no longer predicts complexity, reduce its weight or remove it. If embedding similarity has drifted, retrain embeddings on recent data or switch to a different semantic representation. If domain-specific features like keyword presence have drifted as terminology evolved, update the keyword lists or use more robust semantic features.

Feature engineering review is harder to automate than other drift corrections because it requires understanding why a feature drifted and what would work better. This is human work. Schedule it quarterly. Analyze feature importance in your routing model. Test candidate replacement features. Run A/B tests comparing current routing to routing with updated features. But you can automate the detection that triggers review: when feature distribution monitoring shows a feature has drifted significantly, create a task for the team to review that feature's continued relevance.

## Organizational Ownership of Drift Detection

Router drift is particularly dangerous because it falls into organizational gaps. It's not a model problem—your models are fine. It's not a product problem—your features work. It's not an infrastructure problem—your systems are up. It's a meta-layer problem, and meta-layer problems often have no clear owner. In the legal tech case, the ML team owned the models, the product team owned the features, the infrastructure team owned the routing system, and nobody owned routing quality. Drift accumulated for months because no team had drift detection in their charter.

Assign drift detection to your platform or ML infrastructure team. Make it an explicit responsibility: this team monitors routing quality, detects drift, and triggers correction processes. Give them the tools and authority to recalibrate routers, update configurations, and coordinate with model teams when capability drift requires response. Routing quality becomes a measured platform metric, like latency or error rate, with SLOs and on-call responsibilities.

Include routing drift metrics in your team dashboards and incident response. When routing error rates exceed thresholds, page someone. When feature distributions shift beyond control limits, create a ticket. When costs deviate from forecast in ways that correlate with routing changes, investigate. Treat drift as a production issue, not a research curiosity. The legal tech company now has a weekly drift review meeting where the platform team presents routing metrics and the ML team discusses any model updates that might affect routing. Drift that would have accumulated for months now gets caught in days.

Build drift detection into your continuous integration and deployment pipelines. Before deploying a router update, test it against recent production data to ensure it doesn't introduce new drift. Before deploying a model update, benchmark its impact on routing decisions. Include drift tests in your staging environment. Make drift a deployment gate: if routing error rates in staging exceed thresholds, block the deployment and investigate. This prevents drift from being introduced by changes that seemed unrelated to routing.

## Automated Drift Correction Pipelines

The most mature organizations build fully automated drift correction pipelines that detect drift, validate corrections, and redeploy updated routers without human intervention. An automated pipeline monitors routing metrics continuously. When it detects drift signals—routing rate changes, feature distribution shifts, oracle comparison errors—it triggers a correction workflow. For query distribution drift, it retrains the routing classifier on recent data. For cost drift, it fetches updated pricing and recalculates thresholds. For capability drift triggered by a known model update, it runs benchmarks and adjusts configurations.

The pipeline validates corrections before deployment. It runs the updated router against a holdout dataset, measuring routing error rates and cost impacts. It compares the updated router to the current router on recent production traffic. If the updated router improves metrics without degrading others, the pipeline deploys it to a canary environment—5% of production traffic. It monitors canary metrics for anomalies. If canary performance matches validation predictions, it promotes the updated router to full production. If canary metrics degrade, it rolls back and alerts humans to investigate.

Building this pipeline requires significant investment. You need automated retraining infrastructure. You need benchmark automation. You need canary deployment for routers, not just models. You need comprehensive metrics and alerting. But the payoff is eliminating the months-long drift accumulation that happens when correction depends on humans noticing subtle metric shifts and manually triggering updates. The legal tech company that lost $340,000 to four months of drift now runs automated weekly recalibration. Drift still occurs, but it's corrected within days, limiting the financial and quality impact.

Automated pipelines also enable faster iteration. When you can validate and deploy router changes safely and quickly, you can experiment with more sophisticated routing logic. You can test new features, try different thresholds, explore alternative models in your pool. Each experiment goes through the same validation and canary process, so you learn quickly whether it improves routing without risking production. Manual drift correction is so slow and risky that teams avoid making routing changes. Automated pipelines make routing a continuously optimizing system rather than a set-it-and-forget-it configuration.

## When to Check for Drift

Drift detection cadence depends on your environment's volatility. In stable environments with infrequent model updates and consistent user behavior, monthly drift checks might suffice. In volatile environments with frequent model updates, rapidly evolving user behavior, or competitive pressure to optimize costs aggressively, weekly or even daily drift checks are justified. The legal tech company now runs daily routing rate monitoring, weekly feature distribution analysis, and monthly oracle comparisons. This multi-cadence approach catches fast drift quickly while also detecting slow drift that takes weeks to become visible.

Trigger drift checks after known changes. When a model provider announces an update, run capability benchmarks immediately. When you launch a new product feature that might change query patterns, increase drift monitoring frequency for the next few weeks. When pricing changes, update cost parameters the same day. Event-triggered drift detection catches predictable drift sources before they accumulate.

But also run scheduled drift detection regardless of known changes, because unknown changes happen constantly. Users change behavior without telling you. Models update silently. Competitors launch features that shift how people use your product. Seasonal patterns emerge. Scheduled drift detection catches the drift you didn't predict. The combination of event-triggered and scheduled detection provides defense in depth.

## The Cost of Ignoring Drift

Ignoring router drift creates a slow-motion cost and quality disaster. Every day the router operates miscalibrated, you either waste money on unnecessary escalations or degrade quality through under-routing. In the legal tech case, the four-month drift cost $340,000 in direct overspend. It also cost user trust—some queries routed to the cheap model after capability drift actually needed the expensive model, and users received lower-quality answers. The company saw a 9% increase in clarification questions and a 14% increase in abandoned sessions during the drift period. They attributed it to product-market fit issues until they discovered the routing problem.

Router drift also compounds other problems. Miscalibrated routing makes cost forecasting impossible. You budget based on historical routing rates, but drift changes those rates, so actuals diverge from forecast. You can't distinguish legitimate usage growth from drift-induced inefficiency. Drift makes A/B testing unreliable. You test a new feature, but routing drift during the test period confounds the results. You can't tell if quality changes came from the feature or from the router's degradation. Drift makes incident response harder. When quality drops, you investigate the models, the prompts, the data—everything except the router, because routing is supposed to be stable. You waste time debugging the wrong layer.

The cost of drift detection and correction is tiny compared to the cost of ignoring drift. Implementing basic routing rate monitoring takes a few hours. Adding feature distribution monitoring takes a few days. Building oracle comparison sampling takes a week. Running monthly recalibration takes compute costs measured in hundreds of dollars. The legal tech company's automated drift detection system cost about 40 engineer-hours to build and costs roughly $800 per month to operate. It's already paid for itself many times over by preventing drift that would have cost tens of thousands per month.

Router drift is not inevitable. It's a consequence of treating routing as a static configuration rather than a dynamic system that requires ongoing calibration. When you monitor routing quality, detect drift early, correct it systematically, and assign organizational ownership, drift becomes a managed operational concern rather than a silent budget drain. The router remains aligned with the current environment, routing decisions stay optimal, and the meta-layer continues doing its job: sending each query to the model that handles it best.

Now that you understand how to build, evaluate, and maintain routing systems that remain calibrated over time, the next frontier is making those systems operate within budget constraints while maximizing the value delivered per dollar spent—the domain of cost optimization and ROI measurement.
