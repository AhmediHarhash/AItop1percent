# 5.2 â€” Streaming Responses: Time-to-First-Token as the Critical User Experience Metric

In mid-2025, a legal technology company launched an AI contract analysis tool that generated detailed summaries of 50-page agreements in under 12 seconds. Their evaluation showed that summary quality was excellent, with 91% of clauses correctly identified and categorized. The product team was thrilled. Lawyers who tested the feature said it felt slow and unresponsive. One tester said it was like "staring at a blank screen wondering if it crashed." The team had optimized for total response time, hitting their 12-second latency budget at P95, but they had not implemented streaming. Users saw nothing for 11 seconds, then the entire summary appeared at once. The psychological experience was waiting in uncertainty, then being overwhelmed by a wall of text. When the team added streaming and reduced time-to-first-token to 1.2 seconds, user satisfaction scores jumped from 3.1 to 4.4 out of 5, even though total response time stayed the same at 12 seconds. The perceived performance improved dramatically without any change to the model or backend speed.

The root cause was ignoring the psychology of waiting. Users judge responsiveness not by when the full answer arrives but by when they see evidence that the system is working. A 12-second wait with no feedback feels like an eternity and triggers anxiety: did my request go through, is the system broken, should I refresh the page. A 12-second wait where text starts appearing after 1 second feels like progress. Users settle in and read as the response builds. The total time is identical, but the experience is night and day. The team learned this after launch, through painful user feedback and a rushed re-implementation of their rendering pipeline. This is the pattern we prevent by treating time-to-first-token as the critical user experience metric for any conversational or generative AI feature, and implementing streaming from day one.

## Why Time-to-First-Token Matters More Than Total Latency

**Time-to-first-token**, or TTFT, is the elapsed time from when the user submits a request to when the first token of the response appears. For streaming responses, this is distinct from total response time, which is the time until the last token arrives. TTFT determines perceived responsiveness. Total response time determines how long users wait for the complete answer. For most conversational and generative tasks, perceived responsiveness drives satisfaction more than total completion time because users tolerate longer waits if they see incremental progress.

Research on human perception of waiting, dating back decades to studies of elevator wait times and phone hold music, consistently finds that uncertainty amplifies perceived wait time. When you do not know how long a wait will be or whether anything is happening, each second feels longer than when you see progress indicators. The same principle applies to AI responses. A 10-second wait with no output feels like 20 seconds subjectively. A 10-second wait where tokens stream in continuously feels like 5 seconds because users are engaged and processing information as it arrives, not anxiously wondering if the system is frozen.

Streaming transforms waiting from passive to active. When text appears progressively, users start reading immediately. They absorb the first sentence while the second sentence generates, and the third while they process the second. By the time the full response completes, they have already read most of it. The "wait" for the complete answer overlaps with the "read" time, effectively hiding latency behind cognitive processing. When the full response appears at once, users must wait first, then read, and the wait feels longer because they are doing nothing during it. This overlap effect is why streaming can make a 15-second response feel faster than an 8-second non-streamed response if the streamed response starts in under 2 seconds.

For short responses under 3 seconds, streaming provides less benefit because the total wait is short enough that users do not experience much anxiety. For responses over 5 seconds, streaming becomes critical. For responses over 10 seconds, streaming is mandatory unless you have explicit progress indicators like "Step 1 of 4: Analyzing document." Even with progress indicators, streaming is better because it provides actual content rather than abstract reassurance. Users prefer seeing real output to seeing "Processing..." messages.

Streaming also enables users to interrupt or refine queries earlier. If a user asks a question and realizes halfway through the streamed response that they asked the wrong thing, they can stop the generation and rephrase rather than waiting for the full response. This reduces wasted time and improves the conversational dynamic. Without streaming, users must wait for the complete, possibly irrelevant answer before they can course-correct. For multi-turn conversations, this friction compounds: users waste time in every turn where they realize early that the response is off-track but cannot interrupt.

## How Streaming Works in LLM APIs

Streaming is supported by all major LLM providers as of early 2026. The mechanism is conceptually simple: instead of waiting for the model to generate the entire response and returning it as a single HTTP response, the API returns tokens incrementally as they are generated, using server-sent events or chunked transfer encoding. The client receives each token as it arrives and appends it to the display. From the model's perspective, generation is unchanged: it still produces tokens sequentially. From the API perspective, tokens are emitted as soon as they are ready rather than buffered until completion.

OpenAI's API, Anthropic's API, Google's Gemini API, and open-source inference servers like vLLM and TGI all expose streaming via a parameter like `stream=true` or `stream=True`. When enabled, the API returns an event stream where each event contains one or more tokens. The client listens to the stream and processes events as they arrive. For HTTP-based APIs, this typically uses the `text/event-stream` content type, and clients use libraries like `fetch` with stream readers in JavaScript or `requests` with `iter_lines` in Python. For gRPC-based APIs, streaming is native to the protocol.

The tokens arrive in the order they are generated, which is left-to-right in the text. For English and most languages, this means the response builds naturally from the beginning to the end. The model does not know the full response in advance; it generates each token based on the prompt and all previously generated tokens. There is no "buffering" of a complete response before streaming begins unless you explicitly buffer on the client side, which defeats the purpose. The API sends tokens with minimal delay, often in chunks of 1 to 10 tokens at a time depending on network and API design. The user sees a smooth flow of text appearing, typically at a rate of 10 to 50 tokens per second depending on model speed.

Error handling in streaming is more complex than in non-streaming requests. If the model encounters an error mid-generation, such as hitting a content policy filter or running into a token limit, the stream terminates early with an error event. The client must handle partial responses gracefully: display what was received, show an error message, and allow the user to retry. You cannot roll back a streamed response the way you might discard a failed non-streaming response, because the user has already seen the partial output. Design your UI to make partial responses clearly distinguishable from complete responses, such as showing a spinner or pulsing cursor at the end of the streamed text until the stream closes successfully.

## Time-to-First-Token by Model and Provider in 2026

TTFT varies significantly by model and provider, driven by model size, infrastructure architecture, and load. Smaller models have faster TTFT because they have fewer parameters to activate and faster forward passes. Larger models have slower TTFT because their first token generation requires more computation. Reasoning models have highly variable TTFT because they may perform internal reasoning before emitting any tokens, and the reasoning time depends on query complexity. Provider infrastructure also matters: some providers optimize for throughput over latency and batch requests aggressively, increasing TTFT, while others optimize for responsiveness and prioritize low TTFT.

As of early 2026, typical TTFT values at P95 for a 1,000-token prompt are approximately as follows, based on publicly available benchmarks and production measurements. GPT-5-nano averages 150 to 250 milliseconds TTFT, making it the fastest option for chat interfaces. GPT-5-mini averages 200 to 350 milliseconds. Haiku 4.5 averages 180 to 300 milliseconds. These three models are optimized for speed and are the best choices when TTFT under 300 milliseconds is critical. GPT-5 averages 400 to 700 milliseconds, acceptable for conversational interfaces with a 1.5-second budget. Sonnet 4.5 averages 500 to 900 milliseconds. Gemini 3 Pro averages 600 to 1,000 milliseconds. Llama 4 Maverick, when self-hosted on optimized infrastructure, averages 300 to 600 milliseconds but can vary widely based on hardware. Claude Opus 4.5 averages 1,200 to 2,000 milliseconds, still acceptable for complex queries where users expect a thoughtful response.

Reasoning models have much higher and more variable TTFT. GPT-5.2 averages 2 to 8 seconds TTFT depending on problem complexity, as it performs chain-of-thought reasoning internally before generating the final response. DeepSeek R1 averages 3 to 15 seconds TTFT. Gemini 3 Deep Think averages 4 to 12 seconds. These models are unsuitable for conversational interfaces unless you redesign the UI to set expectations, such as showing "Thinking..." with an animated indicator during the reasoning phase, then streaming the final answer once reasoning completes. Even with good UI, users find these delays frustrating for simple queries, so reasoning models should be reserved for tasks where the quality improvement justifies the wait, like complex code generation or multi-step problem solving.

TTFT also depends on prompt length. Longer prompts require more processing before the first token generates, adding latency. A 10,000-token prompt might add 200 to 500 milliseconds to TTFT compared to a 1,000-token prompt for the same model. If your feature uses large prompts, such as passing entire documents as context, measure TTFT with realistic prompt sizes during evaluation. Do not rely on benchmarks with short prompts if your production prompts are 10x longer. Consider chunking or summarizing context to reduce prompt size if TTFT is too high.

Provider load impacts TTFT unpredictably. During peak hours or when a provider experiences high demand, TTFT can double or triple as requests queue. OpenAI, Anthropic, and Google generally maintain consistent TTFT under normal load but degrade during outages or viral usage spikes. Self-hosted models give you more control over TTFT consistency but require significant infrastructure investment to maintain low latency at scale. Monitor TTFT continuously in production and alert on P95 violations the same way you monitor total latency.

## Streaming Implementation Patterns

Implementing streaming on the backend is straightforward if you use a modern web framework. Most frameworks support server-sent events or streaming HTTP responses natively. In Python with FastAPI, you can define an endpoint that returns a `StreamingResponse` and yields tokens as they arrive from the model API. In Node.js with Express, you can set the response header to `text/event-stream` and call `res.write()` for each token. In Go, you can flush the response writer after each write to push data to the client immediately. The key is ensuring that your web server and any proxies between your backend and the client support streaming and do not buffer the response.

On the client side, JavaScript makes streaming easy with the Fetch API and ReadableStream. You call `fetch` with the streaming endpoint, get a reader from `response.body.getReader()`, and loop over chunks as they arrive, decoding and appending each chunk to the DOM. Libraries like Vercel's AI SDK and LangChain's JavaScript client abstract this further, handling the event stream parsing and token accumulation for you. For mobile apps, iOS and Android both support streaming HTTP with URLSession and OkHttp respectively, and you process the stream in a background thread and update the UI on the main thread.

A common mistake is buffering tokens on the client and only updating the UI every N tokens or every N milliseconds. This defeats the purpose of streaming. Users should see tokens as close to real-time as possible. Update the DOM or UI component immediately when each token or small batch of tokens arrives. Modern browsers and mobile platforms handle frequent small DOM updates efficiently, so there is no performance reason to batch aggressively. If you do batch, keep the batch size under 5 tokens and the batch interval under 50 milliseconds so users still perceive continuous flow.

Handle stream interruptions gracefully. Users may close the chat window, navigate away, or click a stop button while streaming. When this happens, cancel the request to the model API immediately to avoid wasting compute and cost. Most APIs support request cancellation via an AbortController in JavaScript or context cancellation in Go. Signal the backend that the user is no longer interested, and the backend should stop reading from the model stream and close the connection. This prevents orphaned requests that generate tokens no one will see.

Implement retries for transient stream failures. If the model API connection drops mid-stream due to network issues, you can retry the request from the beginning or, if your API supports it, resume from the last received token. Full retries are simpler but waste the work already done. Resumption requires tracking the conversation state and prompt carefully so the retry picks up where the interruption occurred. For user-facing chat, full retries are usually acceptable because the latency of re-streaming 5 seconds of tokens is tolerable. For long-running generations, resumption saves significant cost and time.

## When Streaming Helps and When It Does Not

Streaming is most valuable for free-form text generation where the output is human-readable as it builds. Chat responses, document summaries, explanations, creative writing, and code comments all benefit from streaming because users can read and understand partial outputs. Streaming provides immediate feedback, reduces perceived latency, and allows early interruption. For these tasks, always implement streaming unless you have a strong reason not to.

Streaming provides no benefit for structured outputs where partial results are not interpretable. If your model generates JSON and your application parses it, users cannot do anything with half a JSON object. You could stream the raw JSON text for transparency, but users will not understand it until it is complete and parsed. In practice, most applications wait for the full JSON, validate and parse it, and then display the result. The same applies to tool calls, function calls, or API responses: the output is not meaningful until it is complete and validated. For these cases, non-streaming requests are simpler and equally good from a user experience perspective.

Streaming also provides no benefit for very short responses under 2 seconds. The overhead of setting up a streaming connection and rendering tokens incrementally is wasted if the full response arrives before users even notice a delay. For autocomplete suggestions that return in 300 milliseconds, streaming adds complexity without improving experience. For yes-or-no answers or short factual responses, non-streaming is fine. Reserve streaming for responses you expect to take over 3 seconds or generate more than 100 tokens.

Some outputs are structured but still benefit from streaming for user experience reasons. If you generate a long markdown document with headers, lists, and tables, streaming lets users start reading the top sections while lower sections generate. This is a hybrid case: the content is somewhat structured, but it is human-readable incrementally. Implement streaming here, and render the markdown as it streams if your UI supports progressive markdown parsing. If rendering markdown is expensive, you might buffer tokens and re-render every 50 tokens to balance responsiveness with rendering cost.

Multi-step agent workflows complicate streaming. If your agent performs multiple LLM calls, retrieval steps, and tool invocations before returning a final answer, you can stream the final answer but not the intermediate steps unless you design the UI to show them. Some applications stream a narrative of the agent's progress: "Searching the knowledge base... Found 12 relevant documents... Analyzing documents... Generating summary..." This gives users transparency into what is happening but requires careful prompt design so the model produces streamable narration alongside its actual work. If you cannot stream intermediate steps meaningfully, at least stream the final answer once the agent is ready to produce it.

## The TTFT-Quality Tradeoff with Reasoning Models

Reasoning models present a fundamental tradeoff between TTFT and output quality. These models perform internal reasoning before generating the visible response, using techniques like chain-of-thought, search, or self-critique to improve answer accuracy. The reasoning process adds seconds or tens of seconds to TTFT because the model does not emit tokens during reasoning. From the user's perspective, there is a long silence, then the response starts streaming. This violates the principle that low TTFT drives satisfaction.

Some reasoning models expose their internal reasoning as part of the output, streaming the chain-of-thought before the final answer. GPT-5.2 and DeepSeek R1 can optionally return the reasoning trace, which lets you stream it to users. This makes the wait visible and somewhat interesting: users see the model "thinking out loud," which can build confidence in the answer and make the wait feel purposeful. However, reasoning traces are verbose and not always intelligible to non-technical users. A lawyer using a contract analysis tool may not care to read three paragraphs of chain-of-thought about clause interpretation; they just want the summary. For these users, streaming the reasoning is noise, and you are better off hiding it and showing a "Analyzing..." message.

Other reasoning models do not expose reasoning traces, only the final answer. For these, you cannot stream anything during the reasoning phase. Your only option is to show progress indicators: spinners, pulsing dots, or messages like "Thinking deeply about your question..." These are better than a blank screen but not as engaging as actual content. User-test your progress indicators to ensure they reduce anxiety rather than increasing it. Avoid overly playful or cutesy messages like "Pondering the mysteries of your query..." which can feel patronizing for professional tools.

When reasoning models do start emitting the final answer, stream it normally. The TTFT may be high, but once tokens start, users benefit from seeing them progressively. The total response time for reasoning models can be 15 to 30 seconds, and streaming the final answer over 5 seconds is much better than waiting 5 more seconds for the complete answer to return.

Consider routing to reasoning models only for queries where the quality benefit justifies the TTFT cost. If a user asks a simple factual question, route to a fast model like GPT-5 or Sonnet 4.5 with sub-1-second TTFT. If a user asks a complex multi-part question or explicitly requests detailed analysis, route to a reasoning model and set expectations with UI that signals "This will take a moment." This adaptive routing, which we will explore fully in Chapter 7, lets you optimize for quality when it matters and for TTFT when it does not.

## Measuring and Monitoring TTFT

Measuring TTFT requires instrumentation that captures timestamps when the request is sent and when the first token arrives. For non-streaming requests, this is simple: log the request timestamp and the response timestamp, and the difference is total latency. For streaming requests, log the request timestamp and the timestamp when the first token arrives on the client, and the difference is TTFT. Also log the timestamp when the last token arrives to calculate total response time. These three metrics, request time, first token time, and last token time, let you compute TTFT, total latency, and streaming duration, which is the time from first token to last token.

TTFT is more variable than total latency for the same model and input because it depends on the model's internal state and the provider's infrastructure state at the moment the request hits. A model that usually has 500-millisecond TTFT might occasionally spike to 2 seconds if the request lands on a cold server or if the provider's load balancer queues it. Monitor TTFT at P95 and P99, not just average, to catch these tail latencies. Set alerts for P95 TTFT violations the same way you alert on total latency violations.

Correlate TTFT with model choice, prompt length, and provider load. If you notice TTFT degrading over days or weeks, check whether your prompts have grown longer, whether the model provider has changed infrastructure, or whether your traffic has shifted to larger models. If TTFT spikes at specific times of day, check whether those correlate with peak provider load. If TTFT is consistently worse for certain users, check whether those users are geographically farther from the provider's servers or on slower network connections.

Measure TTFT from the user's perspective, not just the server's perspective. Server-side TTFT is the time from when your backend calls the model API to when it receives the first token. Client-side TTFT is the time from when the user clicks "Send" to when the first token renders in their browser. The difference is network latency between your backend and the model API, and between your backend and the client. Client-side TTFT is what users experience, so it is the metric you should optimize and alert on. Server-side TTFT is useful for debugging whether latency is driven by the model API or by your infrastructure.

Track the percentage of requests that stream successfully to completion versus those that fail mid-stream. Stream failures can happen due to network issues, client disconnections, or model API errors. A high stream failure rate degrades user experience because users see partial responses and have to retry. Investigate failures to determine root causes: are they transient network issues, client bugs, or provider instability? Implement retries and error messages that help users understand what went wrong and how to proceed.

## Client-Side Rendering of Streamed Tokens

Rendering streamed tokens efficiently on the client is critical to maintaining the responsiveness streaming provides. If your client-side code is slow to process tokens or update the UI, you negate the latency benefits of streaming. Each token should render within milliseconds of arriving so users perceive continuous flow. Poor rendering performance makes streaming feel stuttery or laggy, which is worse than not streaming at all.

In web applications, use efficient DOM updates. Appending text to an existing element is fast. If you are appending each token to a div's innerText or textContent, the browser handles this efficiently. Avoid expensive operations like re-rendering the entire message or re-parsing the full markdown on every token. If you must render markdown, use a streaming markdown parser that processes incremental input, or batch token updates every 5 to 10 tokens and re-render then. Libraries like react-markdown can be slow for large documents, so profile your rendering and optimize if necessary.

For mobile applications, update the UI on the main thread after processing tokens on a background thread. Receiving and decoding the stream should happen off the main thread to avoid blocking user interactions. Once you have a batch of tokens, dispatch to the main thread to update the text view. Keep batch sizes small, 5 to 10 tokens, so updates feel fluid. Test on low-end devices to ensure rendering performance is acceptable even on slower hardware.

Handle long outputs gracefully. If a response is 2,000 tokens, rendering all 2,000 tokens into a single DOM element can cause performance issues in some browsers. Consider virtualizing the output: only render the visible portion of the text and the immediate off-screen buffer, and render more as the user scrolls. This is more complex but necessary for very long outputs like full document generations or detailed reports. Alternatively, chunk outputs into collapsible sections so users see a high-level structure first and expand details on demand.

Show a cursor or indicator at the end of the streamed text to signal that generation is ongoing. A blinking cursor or a pulsing ellipsis helps users distinguish between a complete response and one still being generated. When the stream completes, remove the cursor. If the stream fails, replace the cursor with an error icon and a message. These small UI details make streaming feel polished and professional rather than ambiguous and confusing.

## When to Skip Streaming

There are specific cases where streaming adds complexity without sufficient benefit, and non-streaming is the better choice. If your responses are under 2 seconds and under 50 tokens, streaming is overkill. The latency is already acceptable, and the overhead of streaming infrastructure may not be worth it. For autocomplete, search suggestions, and short factual answers, non-streaming is simpler and equally good.

If your infrastructure does not yet support streaming and adding it requires significant refactoring, defer it until latency becomes a user complaint. If users are satisfied with non-streaming responses, do not prematurely optimize. Build streaming when you introduce longer-form generative features where TTFT becomes a bottleneck. Start simple, then add complexity when justified by user feedback or metrics.

If your model outputs structured data that is not human-readable until parsed and validated, non-streaming is appropriate. Streaming raw JSON or XML to users serves no purpose. Wait for the full output, parse it, handle errors, and display the result. Your latency budget should account for the full response time, and you can show a loading spinner while waiting.

If you are building a batch processing system or API where responses are consumed programmatically, streaming may not be needed. API clients can poll for results or use webhooks. Streaming is primarily a user experience optimization for interactive applications, not a requirement for all AI systems.

Streaming is a foundational technique for managing perceived latency in generative AI applications. It shifts the user experience metric from total response time to time-to-first-token, making long-running generations feel responsive and engaging. When implemented correctly, streaming reduces perceived wait times, allows early interruption, and improves satisfaction without requiring faster models or more expensive infrastructure. TTFT is now the critical metric for conversational and generative interfaces, and optimizing it is as important as optimizing total latency. Once you have defined latency budgets and implemented streaming, the next question is how to balance these latency requirements against cost constraints, which determine whether you can afford the models that meet your speed and quality needs.
