# 8.4 â€” Inference Infrastructure: vLLM, TGI, Triton, and Managed Endpoints

In mid-2025, a healthcare technology company decided to self-host their clinical summarization system using Llama 4 Maverick 70B. They'd run the economics and crossed the threshold where self-hosting would save roughly $420,000 annually compared to API costs. Their ML team selected Hugging Face's text-generation-inference framework, deployed it on four H100 GPUs, and launched to production. Within two weeks, they encountered a crisis. During peak morning hours when clinical staff logged in simultaneously across multiple time zones, request latency spiked to 12 to 18 seconds at the p95, far above their 3-second SLA. Off-peak latency was acceptable at 1.1 to 1.8 seconds, but the peak congestion rendered the system unusable during the hours that mattered most. The team investigated and discovered that TGI's request batching strategy was optimized for throughput, not latency. Requests queued behind long-running summarization tasks, creating head-of-line blocking. Switching to vLLM with its continuous batching and PagedAttention architecture dropped p95 latency to 2.4 seconds even during peak load. The model was the same. The hardware was the same. The inference framework was the difference between a production failure and a functioning system. Choosing your serving infrastructure isn't a minor implementation detail; it's an architectural decision that determines whether your self-hosted deployment delivers acceptable performance under real-world load.

## The Inference Framework Landscape in 2026

Serving large language models in production requires specialized software that sits between your application and your GPU hardware. This software, generically called an inference framework or inference engine, handles model loading, request batching, memory management, and response generation. In 2026, the dominant open-source framework for LLM inference is vLLM, a project originally developed at UC Berkeley and now widely adopted across the industry. vLLM introduced two foundational optimizations that make it the default choice for most production deployments: PagedAttention and continuous batching.

PagedAttention reimagines how the framework manages key-value cache during autoregressive decoding. Traditional inference engines allocate contiguous memory blocks for each request's KV cache, leading to fragmentation and wasted GPU memory. PagedAttention instead breaks the KV cache into fixed-size blocks that can be allocated and deallocated dynamically, similar to how operating systems manage virtual memory with pages. This architecture reduces memory waste by 60% to 80% compared to naive implementations, allowing you to serve more concurrent requests on the same hardware or serve larger models on smaller GPU clusters. For a Llama 4 Maverick 70B deployment, PagedAttention typically allows you to handle 2.2x to 2.8x more concurrent requests per GPU compared to earlier frameworks.

Continuous batching, also called iteration-level batching, allows the framework to add new requests to a batch as soon as any prior request in that batch completes, rather than waiting for the entire batch to finish before starting new work. In traditional static batching, if you're processing a batch of eight requests and seven complete quickly while one takes 10 seconds due to a very long output, the next batch of incoming requests waits idle for those 10 seconds. Continuous batching inserts new requests into the active batch immediately as slots free up, dramatically improving GPU utilization and reducing queuing latency for short requests. This optimization is particularly valuable for workloads with high variance in output length, which describes most production LLM applications.

vLLM also supports tensor parallelism and pipeline parallelism for distributing large models across multiple GPUs. Tensor parallelism splits individual weight matrices across GPUs, allowing you to serve models that don't fit in a single GPU's memory. Pipeline parallelism distributes entire layers across GPUs. For a 405B parameter model like Llama 4 Maverick 405B, you'll use tensor parallelism across eight H100s to keep latency low. For serving multiple smaller models simultaneously, pipeline parallelism can improve throughput. vLLM handles these configurations with straightforward command-line flags and supports quantization formats including AWQ, GPTQ, and SqueezeLLM. As of early 2026, vLLM is the inference engine of choice for production LLM deployments unless you have specific requirements it doesn't address.

The vLLM project moves quickly, with major releases every 4 to 6 weeks. This rapid iteration brings performance improvements and support for new model architectures, but it also introduces instability risk. Production teams need to balance staying current to benefit from optimizations against the risk of regressions in new releases. Most organizations adopt a 2 to 4 week lag, allowing the community to discover and fix critical bugs before upgrading production deployments. You'll need a staging environment where you can test new vLLM releases against your models and traffic patterns before promoting to production.

## Alternative Frameworks: TGI, Triton, and SGLang

Hugging Face text-generation-inference, commonly called TGI, offers a simpler deployment model than vLLM with fewer configuration options and tighter integration with the Hugging Face ecosystem. TGI is optimized for serving Hugging Face Hub models with minimal setup. You point TGI at a model ID, specify your hardware configuration, and it handles the rest. This simplicity makes TGI attractive for teams new to self-hosted inference or for rapid prototyping. However, TGI's batching and memory management are less sophisticated than vLLM's, resulting in lower throughput and higher memory consumption for identical workloads. In benchmark tests with Llama 4 Maverick 70B, TGI achieves roughly 65% to 75% of vLLM's throughput under high concurrency.

TGI does excel in one specific scenario: serving models with custom architectures or specialized tokenizers that vLLM doesn't yet support. When new model families release, Hugging Face typically adds TGI support within days, while vLLM support may lag by weeks. If you need to deploy a cutting-edge model immediately after release and can tolerate lower efficiency temporarily, TGI provides faster time-to-production. Once vLLM adds support for that model architecture, migrate your deployment to reclaim the performance headroom.

TGI also has stronger observability defaults out of the box. The framework exposes detailed metrics about batch composition, queue depth, and per-request timings without additional configuration. vLLM provides similar metrics but requires more manual setup. For teams without strong DevOps capabilities who need production observability quickly, TGI's batteries-included approach has value. However, the performance gap typically outweighs the observability convenience for high-traffic deployments.

NVIDIA Triton Inference Server takes a different architectural approach. Triton is designed for multi-model, multi-framework serving where you might host LLMs, traditional ML models, and custom business logic in a single infrastructure layer. Triton supports ONNX, TensorRT, PyTorch, TensorFlow, and custom backends, making it a natural choice if your organization needs to serve diverse model types from a unified platform. For pure LLM serving, Triton can use vLLM as a backend, giving you the performance of vLLM with Triton's model management, versioning, and ensemble capabilities. Use Triton when you're operating a centralized model serving platform that handles many different models and frameworks. For single-model or single-framework deployments, Triton's additional complexity and operational overhead aren't justified.

Triton's model ensemble feature allows you to chain multiple models in a directed acyclic graph, with Triton handling data flow between models. If your application requires an embedding model followed by a retrieval step followed by a generation model, Triton can orchestrate this pipeline with lower latency than making separate API calls. This capability is valuable for RAG systems and multi-stage reasoning pipelines. The trade-off is operational complexity. Triton's configuration files are verbose and error-prone. Debugging issues in multi-model ensembles requires understanding Triton's request lifecycle and data marshaling logic. Most teams find that managing orchestration in their application code provides more flexibility and debuggability than offloading it to Triton.

SGLang, a more recent entrant, specializes in constrained decoding and structured output generation. If your application requires LLM outputs to conform to specific schemas, formats, or grammars, SGLang can enforce these constraints during generation rather than requiring post-hoc validation and retry loops. For example, if you need every model response to be valid JSON matching a specific schema, SGLang guarantees that the generated tokens satisfy your schema constraints, eliminating malformed outputs. This capability delivers substantial latency and cost improvements for structured extraction and data generation tasks by avoiding the generate-validate-retry cycles that traditional inference engines require.

SGLang implements constrained decoding by maintaining a set of valid next tokens at each generation step based on your grammar or schema. The model only samples from this valid set, ensuring outputs always conform to your constraints. This approach works well for relatively simple schemas but can introduce latency overhead for complex grammars with large valid token sets at each step. As of early 2026, SGLang supports a narrower range of model architectures than vLLM and lacks some production hardening features like comprehensive health checks and graceful shutdown handling. Use SGLang for workloads where structured output is critical and where the quality improvements from guaranteed schema conformance justify the operational immaturity trade-off. For general-purpose serving, vLLM remains the safer choice.

## Managed Endpoint Options: AWS, Azure, GCP

If you've determined that self-hosting delivers better economics than API calls but you want to minimize operational burden, managed inference endpoints offer a middle path. AWS Bedrock custom model import allows you to upload fine-tuned or privately-licensed model weights and serve them through Bedrock's managed infrastructure. You don't manage servers, GPUs, or scaling policies. You configure your model, specify instance types and autoscaling thresholds, and AWS handles deployment, health monitoring, and request routing. You pay for provisioned capacity by the hour, typically at a 25% to 35% premium over raw EC2 GPU instance costs, but you avoid building and maintaining your own serving infrastructure.

Bedrock custom model import works well for organizations already standardized on AWS infrastructure who need to serve proprietary or fine-tuned models at moderate scale. The platform abstracts away most operational complexity but constrains your framework choices and limits visibility into serving metrics compared to self-managed infrastructure. Latency tends to be slightly higher than optimally-tuned vLLM deployments due to Bedrock's multi-tenant architecture, but the difference is typically 80 to 150 milliseconds, acceptable for most applications. If your use case demands absolute minimum latency or requires bleeding-edge inference optimizations, Bedrock's managed endpoints won't suffice. For teams prioritizing operational simplicity over maximum performance, Bedrock is a defensible choice.

Bedrock's autoscaling is conservative, designed to prevent cost overruns rather than minimize latency during spikes. If your traffic increases suddenly, Bedrock may take 3 to 8 minutes to provision additional capacity, during which time requests queue and latency degrades. You can pre-provision capacity to handle expected peaks, but then you're paying for idle capacity during troughs, negating some of the cost benefits. For applications with predictable traffic patterns, pre-provisioning works well. For spiky traffic, the autoscaling lag can be problematic.

Azure AI model catalog provides similar functionality within the Azure ecosystem. You deploy models from Azure's catalog or upload custom weights, select your VM SKU and instance count, and Azure provisions a managed inference endpoint. Azure's implementation integrates tightly with Azure ML, providing streamlined model versioning, A/B testing, and observability through Azure Monitor. Pricing structure mirrors AWS: you pay for provisioned capacity with a platform premium of 20% to 30% over raw VM costs. Azure's advantage over AWS is better integration with enterprise authentication and compliance tooling, making it attractive for organizations in regulated industries already using Azure Active Directory and Azure Policy for governance. The performance and operational characteristics are comparable to Bedrock.

Azure AI supports blue-green deployments natively, allowing you to run two model versions simultaneously and shift traffic gradually. This capability is valuable for validating model updates with live traffic before fully committing. AWS Bedrock supports this through manual configuration of multiple endpoints and weighted routing, but Azure's integrated approach is more straightforward. For teams regularly updating models and concerned about quality regressions, Azure's deployment primitives provide meaningful operational value.

GCP Vertex AI Model Garden offers the most flexibility among the major cloud managed endpoints. Vertex allows you to select from pre-configured serving frameworks including vLLM, TGI, and Triton, or bring your own containerized serving stack. This flexibility means you can run vLLM on managed infrastructure, getting most of the performance benefits of self-hosting with less operational burden. Vertex also provides strong integration with Google's TPU infrastructure, relevant if you're considering TPU-based inference for specific model architectures. The platform fee for Vertex managed endpoints runs 22% to 32% above raw GCP GPU instance costs. Vertex's observability and deployment tooling lags slightly behind Azure's but exceeds AWS Bedrock's, making it a strong option for organizations that prioritize performance tuning and metric visibility while still wanting managed infrastructure.

Vertex AI's support for custom containers is its key differentiator. If you've built custom serving logic, pre-processing pipelines, or post-processing filters that aren't supported by standard inference frameworks, you can package them in a container and deploy to Vertex. This flexibility allows gradual migration from self-managed to managed infrastructure, moving components one at a time rather than requiring a full replatform. The trade-off is complexity; managing custom containers requires understanding Vertex's container contract, health check expectations, and scaling signals.

All three managed endpoint platforms handle SSL termination, health checks, autoscaling, and basic monitoring, but none provide the deep request-level telemetry and framework-specific tuning options you get with self-managed vLLM deployments. Use managed endpoints when your organization lacks ML infrastructure expertise, when you're serving relatively standard models at moderate scale, or when you're willing to accept a 20% to 35% cost premium in exchange for not staffing a dedicated inference operations team.

## Quantization Support and Performance Trade-offs

Quantization reduces model weight precision to shrink memory footprint and increase throughput, at the cost of some output quality degradation. In 2026, the three dominant quantization formats for LLM inference are AWQ, GPTQ, and GGUF. AWQ, or Activation-aware Weight Quantization, uses 4-bit integer weights with careful calibration to minimize quality loss. GPTQ applies post-training quantization optimized for GPU deployment. GGUF, originally developed for llama.cpp, focuses on CPU and mixed CPU-GPU inference. For production GPU-based serving, AWQ and GPTQ are the primary options.

vLLM supports both AWQ and GPTQ quantized models with no code changes required. You load a pre-quantized model from Hugging Face Hub or convert your own weights using the respective quantization toolkits, then serve them through vLLM exactly as you would full-precision models. An AWQ-quantized Llama 4 Maverick 70B model consumes roughly 35GB of GPU memory compared to 140GB for the BF16 version, allowing you to serve the model on a single H100 instead of requiring two GPUs in tensor-parallel mode. Throughput improves by 1.7x to 2.2x due to reduced memory bandwidth requirements and faster matrix operations on integer arithmetic.

The quality cost of quantization varies by task. For extractive tasks like classification, summarization, and simple question-answering, INT4 quantization via AWQ typically degrades quality by 1% to 3% relative to full precision, measured by task-specific metrics. For generative tasks requiring nuanced reasoning or creative writing, degradation can reach 5% to 8%. For mathematical reasoning and code generation, some models show 10% to 15% degradation at INT4. Test quantization rigorously against your evaluation suite before deploying to production. If your metrics remain within acceptable bounds, quantization delivers substantial cost savings by reducing GPU requirements and increasing throughput per GPU.

GPTQ generally preserves quality slightly better than AWQ for the same bit width but requires longer quantization time and occasionally produces numerical instability on specific model architectures. AWQ is faster to quantize and more broadly compatible across model families. As a default strategy, start with AWQ for 4-bit quantization and fall back to GPTQ only if you observe unacceptable quality degradation. For 8-bit quantization, both methods perform comparably, and quality loss is negligible for nearly all tasks. Most production deployments in 2026 run 8-bit quantization for models under 100B parameters and 4-bit for models above 100B, balancing memory efficiency with acceptable quality.

FP8 quantization, enabled by H100 and H200 Tensor Cores, provides a middle ground between INT8 and FP16. FP8 preserves more dynamic range than INT8, reducing quality degradation while still delivering 1.4x to 1.8x throughput improvements and memory savings compared to BF16. vLLM supports FP8 quantization on compatible hardware. For models where INT4 degrades quality unacceptably but BF16 is too memory-intensive, FP8 is worth evaluating. The quantization process is simpler than AWQ or GPTQ and the quality-performance trade-off is favorable for many workloads.

The quantization decision is not binary. You can run different quantization levels for different model sizes in your routing stack. Serve smaller models like 7B or 13B parameter variants in full precision for maximum quality, serve 70B models in INT8, and serve 405B models in INT4. This tiered approach optimizes the quality-cost trade-off across your model portfolio. Your routing logic then sends requests to the smallest model that can handle the task at the required quality level, automatically benefiting from the quantization strategy appropriate for each model size.

## Scaling Strategies: Autoscaling, Queuing, and Load Distribution

Production inference infrastructure must handle variable load. Traffic patterns in real applications are rarely uniform. You see diurnal patterns, weekly patterns, seasonal patterns, and unpredictable spikes from marketing campaigns or external events. Your infrastructure needs to scale up during peaks and scale down during troughs to optimize cost. The two primary scaling strategies are horizontal pod autoscaling based on request queue depth and predictive scaling based on historical traffic patterns.

Horizontal pod autoscaling, supported natively in Kubernetes via HPA, monitors a target metric like request queue length or GPU utilization and adds or removes serving pods when thresholds are exceeded. For LLM inference, request queue depth is the most reliable scaling signal. Configure your autoscaler to add a new serving pod when average queue depth exceeds 15 to 20 requests and remove pods when queue depth stays below 5 requests for a sustained period, typically 8 to 12 minutes. This approach reacts to actual demand and handles unpredictable spikes effectively, but it introduces 3 to 6 minutes of elevated latency during scale-up events while new pods initialize and load model weights.

Model loading time is the primary bottleneck in autoscaling. A 70B parameter model in INT8 format is roughly 70GB on disk. Loading this into GPU memory takes 90 to 180 seconds depending on disk I/O throughput and GPU memory bandwidth. Add 30 to 60 seconds for the inference framework to initialize, compile CUDA kernels, and warm up KV cache. Your total cold-start time is 2 to 4 minutes, during which the new pod cannot serve traffic. This lag means that autoscaling works well for gradual traffic increases but poorly for sudden spikes. If your traffic doubles in 30 seconds, your existing pods will be overwhelmed for 3 to 4 minutes before new capacity comes online.

Predictive scaling uses historical traffic data to anticipate load and pre-scale infrastructure before demand arrives. If your application sees consistent morning traffic spikes at 8 AM Eastern time, predictive scaling adds serving capacity at 7:45 AM, ensuring infrastructure is ready when requests arrive. This approach eliminates scale-up latency during predictable peaks but requires robust historical data and stable traffic patterns. Most production systems use a hybrid strategy: predictive scaling for known daily and weekly patterns, with HPA as a backstop for unpredictable spikes.

Request queuing and load distribution logic is critical for multi-pod deployments. A naive round-robin load balancer distributes requests evenly across serving pods without considering their current state, leading to head-of-line blocking when one pod gets several long-running requests while others sit idle. A queue-depth-aware load balancer tracks the number of in-flight requests at each pod and routes new requests to the pod with the shallowest queue. This approach improves tail latency by 30% to 50% compared to round-robin but requires your load balancer to maintain per-pod state. In Kubernetes, implement this with a custom Envoy or NGINX configuration that queries pod metrics before routing, or use service mesh tooling like Istio or Linkerd that provides least-request load balancing out of the box.

For extremely latency-sensitive applications, consider dedicated routing pods that maintain persistent connections to all inference pods and track their queue state in memory. These routers can make sub-millisecond routing decisions based on real-time pod availability, eliminating the overhead of querying metrics on each request. This architecture adds complexity but can reduce routing latency by 5 to 15 milliseconds compared to metrics-based routing, meaningful for applications targeting sub-200-millisecond end-to-end latency.

## Production Hardening: Health Checks, Shutdown, and Rolling Updates

Inference serving pods must expose health check endpoints that your orchestrator queries to determine pod readiness and liveness. vLLM provides a built-in health endpoint that returns HTTP 200 when the model is loaded and the server is ready to accept requests. Configure your Kubernetes readiness probe to query this endpoint with a 10 to 15 second initial delay to allow model loading, then poll every 5 seconds. Liveness probes should use a longer interval, 20 to 30 seconds, and a higher failure threshold to avoid killing pods during temporary GPU hiccups.

Shallow health checks that only verify the HTTP server is responsive are insufficient for GPU inference workloads. A pod can pass a shallow health check while the GPU is hung, out of memory, or in a degraded state. Deep health checks that send a short test prompt and verify a valid response provide stronger confidence that the pod can serve production traffic. The trade-off is latency and cost; deep health checks consume GPU resources and take 200 to 500 milliseconds to complete. Most teams run shallow checks every 5 seconds for fast failure detection and deep checks every 60 seconds for thorough validation.

Graceful shutdown is non-negotiable for production LLM serving. When a pod receives a SIGTERM signal during scale-down or redeployment, it should stop accepting new requests, finish processing in-flight requests, and then exit cleanly. vLLM respects SIGTERM and drains active requests up to a configurable timeout, typically 60 to 120 seconds. Set your Kubernetes terminationGracePeriodSeconds to match this timeout plus a 15-second buffer. If a request is still running when the grace period expires, Kubernetes sends SIGKILL and the response is lost. For critical applications, implement request-level retries in your client code to handle these rare termination-induced failures.

During graceful shutdown, your load balancer needs to know immediately that the pod should not receive new traffic. Kubernetes removes the pod from service endpoints when SIGTERM is sent, but this change can take 3 to 8 seconds to propagate through kube-proxy and iptables rules. During this window, new requests may still route to the terminating pod. Configure your application to fail its readiness probe immediately on receiving SIGTERM, forcing faster removal from the load balancer pool. This technique, called preStop hook readiness probe manipulation, reduces the window where terminating pods receive new traffic.

Rolling updates allow you to deploy new model versions or inference framework updates without downtime. Configure your Kubernetes deployment with a RollingUpdate strategy, maxUnavailable set to 0, and maxSurge set to 1. This configuration ensures at least N pods remain serving traffic while one additional pod starts with the new version. Once the new pod passes health checks, Kubernetes terminates an old pod and repeats until all pods are updated. The process takes 8 to 15 minutes for a typical four-pod deployment given model loading time, but traffic continues flowing uninterrupted.

Test your rolling update process thoroughly in staging before attempting it in production. A misconfigured update that brings down all pods simultaneously will cause a customer-facing outage. Common mistakes include setting maxUnavailable too high, using health check configurations that pass prematurely before the model is fully loaded, and failing to account for model loading time in your update timeline. Script your rollout and practice it multiple times in staging until the process is fully automated and validated.

## Observability and Troubleshooting

Operating inference infrastructure requires deep observability into request-level metrics, GPU utilization, and framework internals. vLLM exposes Prometheus metrics including request count, request latency distribution, queue depth, active requests, GPU memory usage, and KV cache utilization. Export these metrics to your observability platform, whether that's Prometheus and Grafana, Datadog, or cloud-native solutions like CloudWatch or Azure Monitor. Key metrics to alert on include p95 and p99 request latency exceeding your SLA thresholds, request queue depth growing unbounded, GPU out-of-memory errors, and pod crash loops.

Latency metrics need to distinguish time-to-first-token from total latency. Time-to-first-token measures how long users wait before seeing any response, critical for streaming interfaces. Total latency measures end-to-end request duration, relevant for batch processing and non-streaming use cases. Alert on p95 time-to-first-token separately from p95 total latency. A spike in time-to-first-token indicates queuing or scheduling issues. A spike in total latency with stable time-to-first-token indicates issues with generation speed or excessively long outputs.

Beyond aggregate metrics, you need request-level tracing to understand why specific requests are slow or failing. Implement distributed tracing with OpenTelemetry, propagating trace context from your application through your load balancer and into vLLM. vLLM supports OpenTelemetry tracing in recent versions, emitting spans for request queuing time, model execution time, and response serialization time. When a user reports a slow response, query your tracing backend by request ID to see the exact breakdown of where latency occurred. Was the request queued for 4 seconds waiting for GPU availability? Did token generation take 8 seconds due to a very long output? Did the request get routed to a pod that was thrashing on memory? Tracing answers these questions definitively.

Log aggregation completes your observability stack. vLLM logs model loading events, configuration, warnings, and errors to stdout. Collect these logs with Fluent Bit or a cloud-native agent and ship them to a centralized logging platform like Elasticsearch, Splunk, or CloudWatch Logs. When troubleshooting inference failures, correlate logs, metrics, and traces to build a complete picture. A request failed with HTTP 500. Your metrics show a GPU memory spike at that timestamp. Your logs show an out-of-memory error. Your trace shows the failed request had a 15,000-token input, exceeding your expected maximum. Now you know to either increase GPU memory allocation or implement input length validation.

GPU-level metrics from NVIDIA DCGM provide visibility into hardware health that vLLM metrics don't cover. Monitor GPU temperature, power consumption, ECC errors, and PCIe throughput. Elevated temperatures can indicate cooling problems that will eventually cause hardware failure. ECC errors indicate memory corruption that can cause silent data errors or crashes. PCIe throughput bottlenecks indicate that your CPU-to-GPU data transfer is constrained, relevant for workloads with large input contexts. Export DCGM metrics alongside vLLM metrics to get a complete picture of your serving stack health.

## Choosing Your Inference Stack

For most organizations deploying self-hosted LLM inference in 2026, the default recommendation is vLLM on raw GPU instances with Kubernetes orchestration, Prometheus metrics, and OpenTelemetry tracing. This stack delivers maximum performance and flexibility while using battle-tested open-source components. You'll need strong ML infrastructure expertise to operate it effectively, but the performance and cost efficiency justify the investment if you've reached the scale where self-hosting makes economic sense.

If you lack that expertise or you're operating at moderate scale where a 25% to 35% platform fee is justified by reduced operational burden, use managed endpoints. Choose AWS Bedrock if you're AWS-native and prioritize simplicity. Choose Azure AI if you're in a regulated industry already using Azure compliance tooling. Choose GCP Vertex if you want managed infrastructure with vLLM performance and deeper metric visibility.

If you're serving models with custom architectures immediately after release, use TGI temporarily until vLLM adds support, then migrate. If you're building a multi-model platform serving LLMs alongside traditional ML models, use Triton with vLLM as the LLM backend. If your use case requires structured outputs with schema enforcement, evaluate SGLang for those specific workloads while using vLLM for general-purpose serving.

Regardless of your framework choice, implement quantization unless you've proven through rigorous evaluation that the quality cost is unacceptable. Start with INT8 quantization via AWQ for models above 30B parameters and INT4 for models above 100B. Measure quality degradation against your eval suite. If metrics remain acceptable, deploy the quantized version and reclaim the memory and throughput benefits.

Build your production infrastructure incrementally. Start with a single vLLM instance serving your model in staging. Add observability, health checks, and deployment automation. Load test until you understand throughput limits and failure modes. Only then deploy to production with multiple instances and autoscaling. Resist the temptation to build the full production architecture on day one. Inference infrastructure has many failure modes you won't anticipate until you've operated it. Learning these lessons in staging with time to iterate is far preferable to learning them during production outages.

With your inference infrastructure selected, configured, and hardened for production, the next challenge is routing requests intelligently across multiple models to balance cost, latency, and quality, a topic we turn to in the following subchapter.
