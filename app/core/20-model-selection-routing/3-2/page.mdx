# 3.2 — Complexity-Based Routing: Simple Queries to Small Models, Hard Queries to Frontier

In June 2025, a legal document review platform served corporate law firms processing contracts, merger agreements, and compliance filings. The system used GPT-5 to analyze every document, extracting key clauses, flagging risks, and summarizing obligations. Monthly inference costs ran $43,000 for 2.8 million document sections processed. The engineering team analyzed their query distribution and discovered something striking: 72% of sections were boilerplate clauses — standard indemnification language, governing law statements, severability clauses — that appeared nearly identically across hundreds of contracts. These sections required no reasoning, no legal judgment, and no creativity. A small fine-tuned model could extract the standard metadata just as accurately as GPT-5. The remaining 28% were bespoke clauses with complex conditions, nested obligations, and ambiguous terms that genuinely required frontier model reasoning. But the team had no mechanism to distinguish boilerplate from bespoke at ingestion time, so everything went to the expensive model. They built a complexity classifier that scored each section based on vocabulary diversity, clause structure, and presence of conditional logic. Sections scoring below a threshold routed to a fine-tuned Llama 4 13B model that cost one-fifteenth as much. Sections scoring above the threshold routed to GPT-5. Quality metrics held steady — accuracy on boilerplate sections remained at 98%, accuracy on complex sections remained at 94%. But monthly costs dropped to $16,800, saving $26,200 monthly or $314,400 annually. Complexity-based routing turned an unsustainable cost structure into a profitable one with zero quality loss.

Complexity-based routing is the most intuitive and most effective routing strategy for the majority of production systems. The principle is simple: estimate how hard each query is to answer correctly, send simple queries to cheap and fast models, send hard queries to frontier models. The challenge is estimating complexity accurately in real time before you run inference. You need a complexity signal that correlates strongly with model performance, that can be computed quickly from the raw query, and that generalizes across different types of queries within your domain. Get the signal right, and you save 40% to 60% on inference costs with no quality degradation. Get the signal wrong, and you route complex queries to cheap models that fail, or simple queries to expensive models that waste budget.

## How to Estimate Complexity: Signals and Heuristics

The most basic complexity signal is input length. Longer queries tend to be more complex because they contain more constraints, more context, or more ambiguity. A query with 20 tokens is usually simpler than a query with 500 tokens. You can set a threshold: queries under 100 tokens route to the small model, queries over 100 tokens route to the frontier model. This heuristic works surprisingly well for many domains because length correlates with complexity. But it breaks down when users write verbose simple queries or terse complex queries. A user asking for a password reset might write a 150-token explanation of the issue, which is still a simple query. A user asking for a legal interpretation might write a 40-token question that requires deep reasoning.

Vocabulary complexity is a stronger signal. Simple queries use common words and standard phrasing. Complex queries use rare words, technical jargon, or domain-specific terminology. You can measure vocabulary complexity by comparing the query tokens to a frequency table of common words. If 95% of the tokens appear in the top 10,000 most common English words, the query is probably simple. If 40% of the tokens are rare or domain-specific, the query is probably complex. This heuristic is more robust than length because it captures the conceptual sophistication of the query, not just its verbosity.

Presence of reasoning keywords is another heuristic. Words like "why," "because," "if-then," "unless," "however," "although," and "despite" signal conditional logic, causal reasoning, or nuanced judgment. Queries that contain these keywords are more likely to require frontier model capabilities. You can build a keyword list by analyzing historical queries and identifying terms that correlate with high complexity scores. This is a simple rule-based classifier, and it runs in microseconds. It is fast enough to use as a pre-filter before invoking a more sophisticated complexity model.

Number of constraints is a strong signal for structured tasks. A query that asks for a summary with no constraints is simpler than a query that asks for a summary in a specific format, with specific exclusions, in a specific tone, and under a specific token limit. You can count the number of constraint clauses in the query by matching patterns like "must include," "should not contain," "in the style of," and "no more than." Each constraint increases complexity because it narrows the solution space and requires the model to balance multiple objectives simultaneously.

Domain specificity is a complexity signal for specialized systems. A query about general knowledge is simpler than a query about quantum mechanics, tax law, or pharmacology. You can measure domain specificity by comparing the query terms to a domain-specific vocabulary. If the query contains three or more domain-specific terms, it is probably a complex domain query. If the query contains only general terms, it is probably a simple query that a small general-purpose model can handle.

## Heuristic Complexity Scoring: Rule-Based Classification

Heuristic complexity scoring combines multiple signals into a single score using hand-coded rules. You define a scoring function that adds points for each complexity indicator: add 10 points if the query is longer than 200 tokens, add 5 points for each reasoning keyword, add 3 points if vocabulary complexity is above a threshold, add 7 points for each constraint clause, add 15 points if domain-specific terms are present. The total score determines the routing decision. If the score is below 20, route to the small model. If the score is above 20, route to the frontier model. This is static rule-based routing, and it is the simplest form of complexity-based routing to implement.

Heuristic scoring is fast, deterministic, and interpretable. You can debug routing decisions by inspecting the score components. If a query routed incorrectly, you can see which signals contributed to the score and adjust the weights. Heuristic scoring does not require training data or machine learning infrastructure. You define the rules, deploy them, and they run consistently. The downside is that heuristic scoring does not adapt to changes in query patterns or model capabilities. If users start asking more complex questions, you must manually update the scoring rules. If your cheap model improves, you must manually lower the routing threshold. Heuristic scoring also struggles with interactions between signals — a query that is long but uses simple vocabulary might be less complex than a short query with rare terms, but a simple additive scoring function cannot capture this nuance.

Despite these limitations, heuristic complexity scoring is the right starting point for most teams. You can build it in a few days, deploy it with confidence, and iterate on the rules based on observed routing accuracy. Start with the simplest heuristics — length and reasoning keywords — and measure their correlation with model performance. If a query scores as simple and the small model answers it correctly, the heuristic worked. If a query scores as simple but the small model fails, the heuristic under-estimated complexity. Track these misclassifications and refine the rules. Over weeks or months, you accumulate enough data to justify building a more sophisticated ML-based complexity model.

## ML-Based Complexity Scoring: Training a Routing Classifier

ML-based complexity scoring uses a trained classifier to predict which model should handle each query. You collect historical data: for each query, log the query text, the model that was used, the response quality score, and whether the response met your quality threshold. Label each query as simple or complex based on the outcome: if the cheap model handled it successfully, label it simple; if the frontier model was needed, label it complex. Train a small classifier — a logistic regression model, a gradient boosted tree, or a lightweight neural network — to predict the complexity label from query features like length, vocabulary complexity, keyword presence, and constraint count. At inference time, the classifier scores each incoming query and routes it to the appropriate model.

ML-based complexity scoring adapts to changing query distributions because it learns from actual outcomes. If users start asking more complex questions, the classifier observes more failures on the cheap model and adjusts its decision boundary to route more traffic to the frontier model. If the cheap model improves after a provider update, the classifier observes more successes on queries that previously failed and adjusts the boundary to route more traffic to the cheap model. This adaptation happens automatically through periodic retraining. You retrain the classifier weekly or monthly on the latest query logs, and the updated model reflects the current state of your query distribution and model capabilities.

The training data must be representative of your production traffic. If you train on a biased sample — only queries from enterprise users, or only queries from a specific task type — the classifier will overfit to that distribution and route poorly on other queries. You need a balanced dataset with examples of simple and complex queries across all user tiers, task types, and query patterns. You also need enough volume — at least 10,000 labeled queries, preferably 100,000 or more — to train a classifier that generalizes well. If you do not have labeled data yet, you can generate it by routing a random sample of queries to both the cheap model and the frontier model, comparing their outputs, and labeling queries based on whether the cheap model matched the frontier model's quality.

The classifier must be fast because it runs on every query before inference. A classifier that takes 200 milliseconds to score a query negates the latency benefit of routing to a fast model. You need a classifier that scores queries in under 10 milliseconds. Logistic regression and gradient boosted trees meet this requirement. Small neural networks with one or two hidden layers also meet it. Large language models do not — using GPT-4o to predict whether a query should route to GPT-5 is both slow and expensive, and it defeats the purpose of routing. The routing classifier must be smaller and faster than the models it is routing to.

## The Calibration Challenge: What Counts as Complex Depends on Task and Model

Calibration is the hardest part of complexity-based routing. What counts as "complex" is not an absolute property of the query — it depends on the task and the cheap model's capabilities. A query that is complex for a 7B parameter model might be simple for a 13B parameter model. A query that is complex for summarization might be simple for extraction. A query that was complex in 2024 might be simple in 2026 as small models improve. Your complexity threshold must be calibrated to your specific cheap model on your specific task distribution.

Under-calibration — setting the complexity threshold too low — routes complex queries to cheap models that fail. This costs quality. If 10% of queries that score as simple are actually complex, and the cheap model fails on them, you have degraded quality on 10% of traffic. If that 10% includes high-value users or high-stakes queries, the damage is disproportionate. Under-calibration also generates negative feedback loops: users who get wrong answers from the cheap model resubmit their queries, often with more detail or frustration, which increases load and makes routing even harder. You save money on the initial inference but lose it on retries, support tickets, and churn.

Over-calibration — setting the complexity threshold too high — routes simple queries to expensive models unnecessarily. This costs money but not quality. If 30% of queries that route to the frontier model could have been handled by the cheap model, you are overspending by 30% on those queries. Over-calibration is safer than under-calibration because it does not degrade quality, but it is still waste. The goal of routing is to maximize cost savings without sacrificing quality, and over-calibration fails to capture available savings.

The right calibration is the threshold that maximizes cost savings subject to a quality constraint. You define an acceptable error rate — for example, no more than 2% of routed queries should fail quality checks — and you find the complexity threshold that achieves that error rate while routing as much traffic as possible to the cheap model. You do this empirically by A/B testing different thresholds on production traffic. Route 10% of traffic through Threshold A, 10% through Threshold B, and measure both quality and cost for each cohort. The threshold with the lowest cost that meets your quality constraint becomes your production threshold. You repeat this experiment monthly as query distributions and model capabilities evolve.

## Real-World Complexity Distributions: Most Traffic Is Simpler Than You Expect

One of the most consistent findings across production systems is that query complexity distributions are heavily skewed toward simplicity. Most teams discover that 60% to 80% of their queries are simple enough for a cheap model, even in domains that feel complex. A legal document review system expects every query to be hard, but 70% of queries are about boilerplate clauses. A medical Q&A system expects every query to require medical expertise, but 65% of queries are about appointment scheduling, billing, or general wellness. A code generation system expects every query to require reasoning, but 60% of queries are requests for standard library usage or syntax fixes that a small model handles perfectly.

This skew exists because most users ask simple questions most of the time. Complex queries are rare because they require more effort to formulate, more context to provide, and more expertise to evaluate. Users who ask complex questions are often power users or domain experts, and they represent a small fraction of your user base. The median query in most production systems is far simpler than the 90th percentile query, and routing strategies that optimize for the median achieve dramatic cost savings.

The 80/20 pattern is common: 80% of queries can be handled by a model that costs 5x to 10x less than the frontier model. Some systems see even more extreme distributions: 90% of queries routable to a cheap model, 8% requiring a mid-tier model, 2% requiring a frontier model. These distributions create enormous routing leverage. If you route the 90% correctly, you save 90% of your inference costs on that traffic. If you misroute the 2%, you degrade quality on a small but critical subset. The challenge is identifying the 2% accurately without over-routing the 90%.

## Measuring Routing Accuracy and Its Impact on Cost and Quality

Routing accuracy is the percentage of queries that route to the optimal model. A query routes correctly if it goes to the cheapest model that can answer it within your quality threshold. A query routes incorrectly if it goes to a model that fails to meet the quality threshold, or if it goes to a more expensive model than necessary. You measure routing accuracy by sampling routed queries and evaluating whether the chosen model was optimal. For queries routed to the cheap model, run the query through the frontier model as well and compare outputs. If the cheap model's output meets quality thresholds and matches the frontier model's output, the routing was correct. If the cheap model's output fails quality checks or diverges significantly from the frontier model, the routing was incorrect.

For queries routed to the frontier model, estimate whether a cheaper model could have handled them. This is harder to measure because you did not run the cheaper model, but you can use heuristics: if the query scored just above the complexity threshold, route a sample to the cheap model and measure success rates. If 95% of queries that scored just above the threshold succeed on the cheap model, your threshold is over-calibrated and you are overspending. If 30% of queries that scored just above the threshold fail on the cheap model, your threshold is well-calibrated.

Routing accuracy directly impacts both cost and quality. High routing accuracy maximizes cost savings by routing as many queries as possible to cheap models without causing failures. Low routing accuracy either costs quality by routing complex queries to cheap models, or costs money by routing simple queries to expensive models. A system with 95% routing accuracy on 80% simple traffic saves 76% of the potential cost savings with minimal quality impact. A system with 80% routing accuracy on the same traffic saves only 64% of the potential savings and incurs quality failures on 4% of queries.

Routing accuracy improves with better complexity signals, more training data, and tighter feedback loops. The signal quality depends on how well your features correlate with actual complexity. The training data quality depends on how representative your labeled examples are. The feedback loop speed depends on how quickly you detect routing errors and retrain the classifier. Teams that measure routing accuracy daily and retrain weekly achieve higher accuracy than teams that measure monthly and retrain quarterly. The difference compounds over time: a team that iterates faster learns faster and captures more value from routing.

## Under-Routing vs Over-Routing: Managing the Trade-Off

Under-routing — sending complex queries to cheap models — is the more dangerous error because it degrades quality. If a cheap model fails on a complex query, the user gets a wrong answer, a nonsense response, or a refusal. This damages trust, generates support load, and can cause churn. Under-routing is especially damaging on high-stakes queries — legal analysis, medical advice, financial decisions — where a wrong answer has real consequences. You cannot tolerate high under-routing rates in these domains. Your quality constraint must be strict, and your complexity threshold must be conservative.

Over-routing — sending simple queries to expensive models — is the safer error because it preserves quality at the cost of efficiency. If a frontier model handles a simple query, the user gets a correct answer, just at higher cost than necessary. Over-routing does not generate user complaints or support tickets. It just makes your system more expensive to operate. In early stages of routing deployment, over-routing is the right default. You set a conservative complexity threshold that routes only the most obviously simple queries to the cheap model, and you route everything else to the frontier model. You measure the success rate on routed queries, and if it is high, you gradually lower the threshold to route more traffic to the cheap model.

The trade-off between under-routing and over-routing depends on your cost constraints and quality requirements. If you have strict cost targets and can tolerate occasional quality failures, you set an aggressive threshold that routes more traffic to cheap models. If you have strict quality requirements and more flexible cost budgets, you set a conservative threshold that routes less traffic. Most production systems prioritize quality over cost, which means they err on the side of over-routing. The financial cost of over-routing is quantifiable and manageable. The reputational cost of under-routing is harder to measure and harder to recover from.

## Finding the Right Threshold: Empirical Calibration Through A/B Testing

The right complexity threshold is not a number you derive analytically — it is a number you discover empirically through experimentation. You start with a conservative threshold that routes only 20% to 30% of traffic to the cheap model. You measure quality on routed queries using your standard evaluation metrics — accuracy, coherence, completeness, user satisfaction. If quality holds steady, you lower the threshold to route 40% of traffic and measure again. If quality degrades, you raise the threshold back to the previous level. You iterate this process until you find the threshold that routes the maximum traffic to the cheap model while keeping quality within acceptable bounds.

A/B testing accelerates calibration by letting you test multiple thresholds in parallel. You split production traffic into cohorts: Cohort A uses Threshold 1, Cohort B uses Threshold 2, Cohort C uses Threshold 3. You measure cost and quality for each cohort over a week or two, and you adopt the threshold that delivers the best cost-quality trade-off. A/B testing reduces the time to find the optimal threshold from months to weeks because you test multiple candidates simultaneously rather than sequentially. It also reduces risk because you expose only a fraction of traffic to each experimental threshold.

Threshold calibration is not a one-time exercise. Query distributions shift over time as users change behavior, new features launch, and seasonal patterns emerge. Model capabilities shift as providers release updates. The threshold that was optimal in January might be suboptimal in March. You need continuous calibration — monthly or quarterly threshold reviews where you A/B test new candidates against the current production threshold and adopt the better one. This continuous optimization ensures that your routing policy stays aligned with current query distributions and model capabilities.

## Complexity-Based Routing as the Foundation for Multi-Tier Routing

Complexity-based routing is the foundation for more sophisticated multi-tier routing strategies. Once you have reliable complexity signals, you can extend them to route queries across three or more model tiers: small models for simple queries, mid-tier models for moderate complexity, frontier models for high complexity. You can combine complexity routing with user-tier routing: free users route to cheap models for simple queries and mid-tier models for complex queries; paid users route to mid-tier models for simple queries and frontier models for complex queries. You can layer task-type routing on top of complexity routing: simple summarization goes to a small summarization specialist, complex reasoning goes to a frontier generalist.

These multi-dimensional routing policies start with complexity as the primary signal and add secondary signals for disambiguation. Complexity tells you how much capability is needed. User tier tells you how much cost is justified. Task type tells you which model specialization is relevant. Combining these signals lets you route queries precisely to the model that offers the best cost-quality-latency trade-off for that specific query in that specific context. But you cannot build multi-dimensional routing without first solving single-dimensional complexity-based routing. Complexity is the signal with the highest information value, and it must work reliably before you layer additional signals on top.

Complexity-based routing is the most intuitive, most effective, and most widely deployed routing strategy in production systems today. It aligns model costs with query difficulty, ensures that frontier model capacity is reserved for queries that need it, and delivers massive cost savings with minimal quality impact. The next step is to extend complexity-based routing to incorporate user-tier signals, latency requirements, and provider failover strategies, creating a comprehensive routing policy that optimizes across multiple dimensions simultaneously.
