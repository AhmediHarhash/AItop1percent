# 10.3 â€” Model Cards for Production: Documenting Capabilities, Limitations, and Risk

In June 2025, a healthcare technology company discovered their clinical summarization system had been using GPT-4.5 Turbo for eight months with no formal documentation of what the model could reliably do, what it couldn't, or what risk mitigations were in place. When their compliance team began preparing for an external audit, engineering couldn't produce a single document explaining why that specific model was chosen, what quality thresholds it met, what failure modes had been observed, or what would happen if performance degraded. The audit team found references to "the model" in Slack threads, emails discussing "switching to the new one," and scattered benchmark screenshots in slide decks. No central source of truth existed. The company spent seven weeks reconstructing documentation from git history, production logs, and engineer interviews, delaying their certification by three months and costing them two enterprise contracts worth $4.7 million. The root cause wasn't negligence. It was the absence of a standardized artifact that every model in production must have: a production model card.

## The Model Card as the Standard Documentation Artifact

Every model you run in production needs a model card. Not eventually. Not for high-risk systems only. Every model, from the moment it enters evaluation to the moment you retire it. A **model card** is the canonical document that answers every operational, performance, and risk question about that specific model in your specific context. It is not optional documentation. It is the artifact that Legal reviews before sign-off, that Product references when planning features, that Engineering consults when debugging performance issues, that auditors request during compliance reviews, and that incident responders open when something goes wrong.

The distinction between academic model cards and production model cards is fundamental. Academic model cards, like those published by OpenAI or Anthropic with their model releases, describe what the model can do in general research contexts. They report benchmark scores on standard datasets, discuss training data composition, and outline broad ethical considerations. They are written for researchers and the general public. Production model cards describe what the model does in your environment, on your tasks, with your data, under your constraints. They report evaluation scores on your specific criteria, document observed failure patterns in your workloads, and detail your specific risk mitigations. They are written for your engineering team, your compliance team, your product team, and your auditors.

You write a production model card the moment a model becomes a candidate for production use. If you're evaluating three models for a new summarization feature, all three get draft model cards during evaluation. The cards document evaluation results, observed limitations, and projected operational characteristics. When you select one model for production, that card becomes the living production artifact. The other two cards remain as evaluation records explaining why you didn't choose them. This historical record is invaluable six months later when someone asks "why didn't we use Claude Opus 4.5 for this?" and you can point to the evaluation-phase model card showing it failed your latency requirements.

Production model cards are versioned and updated continuously. When you run a new evaluation cycle, the performance section updates. When you discover a new failure mode in production, the limitations section updates. When you implement a new mitigation, the risk section updates. The card is never frozen. It reflects the current operational reality of the model in your system, not a point-in-time snapshot from six months ago when you first deployed it.

## What a Production Model Card Contains

A complete production model card has seven sections: identification, capabilities, limitations, risk assessment, performance data, operational data, and update history. Each section serves a specific audience and answers specific questions.

The **identification section** specifies exactly which model this card documents. Not "GPT-4.5" but "GPT-4.5 Turbo (gpt-4.5-turbo-2025-04-15)" with the full model identifier. Not "Claude" but "Claude Opus 4.5 (claude-opus-4.5-20251101)." Model names are ambiguous. Model identifiers are precise. You include the provider, the model family, the specific version, and the date range this card covers. If you switch from gpt-4.5-turbo-2025-04-15 to gpt-4.5-turbo-2025-07-20, that's a new model card, not an update to the existing one. Provider model updates can change behavior significantly, and you document each version separately.

You also specify which systems use this model, for which tasks. "Used in the clinical summarization service for generating visit summaries from encounter notes. Serves approximately 15,000 requests per day across 40 hospital clients. Deployed to production on March 12, 2025. Replaces GPT-4.5 (gpt-4.5-2025-02-01) which was retired on March 11, 2025." This context tells everyone where the model lives and what it does.

The **capabilities section** documents what the model can do at what quality level for which tasks. This is not a list of general abilities from the provider's marketing page. This is empirical documentation of observed performance on your workloads. "Generates clinically accurate visit summaries from structured encounter notes in English for adult primary care visits. Median summary length: 180 words. Achieves 94% factual accuracy as measured by clinician review on the March 2025 evaluation set of 500 encounters. Correctly identifies and includes all critical clinical events (diagnoses, prescriptions, referrals) in 97% of cases. Maintains appropriate clinical tone and terminology as rated acceptable or better by domain experts in 96% of cases."

Capabilities are scoped precisely. "For adult primary care visits" is not the same as "for all clinical encounters." You have not evaluated this model on pediatric visits, emergency department notes, or surgical reports, so you do not claim capabilities there. Capabilities are quantified. "Clinically accurate" is meaningless. "94% factual accuracy as measured by clinician review" is a testable claim tied to a specific evaluation methodology. Capabilities reference your evaluation data, not provider benchmarks. The fact that GPT-4.5 scores 88.5 on MMLU tells you nothing about its ability to summarize your encounter notes.

You document capability boundaries explicitly. "Performs reliably on notes up to 4,000 tokens. Performance degrades on notes longer than 6,000 tokens as measured by increased omission rates (12% vs 3% baseline). Does not reliably handle notes in languages other than English. Has not been evaluated on handwritten transcription inputs or voice-to-text outputs." These boundaries prevent scope creep. When Product asks "can we use this model for Spanish-language summaries?" you point to the capability section showing it hasn't been validated for that, and you initiate a new evaluation cycle if they want to proceed.

The **limitations section** documents known failure modes, quality ceilings, and input types that degrade performance. Limitations are specific patterns you have observed in evaluation or production. "Occasionally omits medication dosage details when the encounter note lists more than eight concurrent medications. Observed in 4% of cases with eight or more medications vs 0.5% baseline. Tends to over-emphasize patient-reported symptoms relative to clinical findings when the note contains extensive patient quotes. Generates summaries that are too brief (under 120 words) for complex multi-problem visits approximately 8% of the time, requiring regeneration."

Limitations include input characteristics that cause problems. "Performance degrades when encounter notes contain extensive copy-pasted text from previous visits, leading to summaries that reference outdated information. Handles abbreviations inconsistently, sometimes expanding them and sometimes leaving them abbreviated, creating style inconsistencies within a single summary. Struggles with notes written by non-native English speakers containing grammatical errors, occasionally propagating those errors into the summary."

You document quality ceilings: the best performance you have observed even with prompt engineering and careful inputs. "Maximum factual accuracy observed across all evaluation cycles is 94%. This ceiling has been consistent since initial evaluation in January 2025 despite multiple prompt refinements. Human expert performance on the same task is 97-98%. The 3-4 percentage point gap represents the current limitation of automated summarization for this task with this model." This ceiling tells your team not to expect 99% accuracy from prompt tweaks. The model has a performance ceiling, and you've found it.

Limitations are not failures. They are documented boundaries that inform operational decisions. When you know the model struggles with notes over 6,000 tokens, you implement a preprocessing step that splits long notes or you route them to a different model. When you know it over-emphasizes patient quotes, you adjust your review workflow to flag summaries for notes with extensive quoted text. Limitations become guardrails.

## Risk Assessment: What Could Go Wrong

The **risk assessment section** documents what could go wrong, how likely it is, how severe the consequences would be, and what mitigations are in place. This is not hypothetical risk brainstorming. This is structured assessment of observed failure modes and potential failure modes tied to the specific use case.

You categorize risks by type: accuracy risks, safety risks, bias risks, privacy risks, availability risks, and cost risks. Accuracy risks are quality failures. "Risk: summary omits a critical prescription, leading to patient harm. Likelihood: rare (observed in 0.3% of evaluation cases). Severity: high (potential patient harm). Mitigation: all summaries reviewed by clinical staff before inclusion in patient record, automated checks flag summaries missing prescription sections." Safety risks are harmful outputs. "Risk: summary includes inappropriate or offensive language. Likelihood: very rare (not observed in 12,000 production summaries to date). Severity: medium (damages patient trust, requires correction). Mitigation: output content filtering blocks offensive terms, clinical review catches remaining cases."

Bias risks are systematic performance differences across demographic groups or input characteristics. "Risk: summary quality lower for notes written in non-standard English. Likelihood: moderate (observed in 8% of notes from non-native English speaking providers). Severity: medium (creates quality inconsistencies across provider groups). Mitigation: additional review workflow for summaries flagged as low-confidence, provider feedback loop to improve note standardization." You document bias risks even when the bias is attributable to input quality rather than model behavior, because the downstream effect is the same: inconsistent service quality.

Privacy risks are leakage or exposure of sensitive information. "Risk: model training data includes proprietary medical information that could be extracted through prompt injection. Likelihood: very low (provider states training data does not include customer data, verified through contract review). Severity: high (HIPAA violation, regulatory penalties). Mitigation: all prompts filtered for injection patterns, no patient identifiers included in prompts, outputs logged and monitored for anomalous content." Privacy risks are especially important when using third-party hosted models where you don't control training data or infrastructure.

Availability risks are service interruptions. "Risk: provider API outage prevents summary generation, blocking clinical workflows. Likelihood: low (provider SLA is 99.9%, observed uptime is 99.95%). Severity: high (clinicians cannot complete visits, patient care delayed). Mitigation: fallback to previous model version (GPT-4.5-2025-02-01) if primary model unavailable, manual summary workflow documented and trained." Cost risks are unexpected expense increases. "Risk: prompt length increases due to input drift, doubling per-request cost. Likelihood: low (input length monitored, alerts configured at 10% increase). Severity: medium (budget overrun). Mitigation: monthly cost review, input length caps enforced at API layer."

For each risk, you specify the current mitigation status: mitigated, partially mitigated, accepted, or under review. Mitigated risks have controls in place that reduce likelihood or severity to acceptable levels. Partially mitigated risks have some controls but residual risk remains. Accepted risks are known risks where mitigation cost exceeds risk cost, documented with stakeholder sign-off. Under review risks are newly discovered or newly prioritized risks where mitigation is being designed.

## Performance Data: Your Tasks, Not Provider Benchmarks

The **performance data section** is the empirical core of the model card. You document evaluation scores on your specific tasks using your specific criteria measured on your specific evaluation sets. Not provider benchmarks. Not academic datasets. Your data.

You report scores for every criterion in your evaluation rubric. "Factual accuracy: 94% (470 of 500 summaries rated fully accurate by clinician review). Completeness: 97% (485 of 500 summaries include all critical elements per completeness checklist). Tone appropriateness: 96% (478 of 500 summaries rated acceptable or better by domain experts). Conciseness: 89% (445 of 500 summaries meet length target of 150-250 words). Overall pass rate: 88% (440 of 500 summaries meet all four criteria)."

You include confidence intervals or error bars when sample sizes are small. "Factual accuracy: 94% (95% CI: 91-96%). Based on 500 summaries reviewed by three clinicians with inter-rater agreement of 92%." This quantifies measurement uncertainty and prevents overconfidence in point estimates.

You report performance breakdowns by input characteristics when you have stratified evaluation data. "Factual accuracy by note length: under 2,000 tokens 96%, 2,000-4,000 tokens 94%, 4,000-6,000 tokens 91%, over 6,000 tokens 87%. Factual accuracy by visit complexity: single-problem visits 97%, multi-problem visits 92%." These breakdowns reveal where the model performs best and where it struggles, informing routing decisions and quality assurance priorities.

You document evaluation methodology so the numbers are interpretable. "Factual accuracy measured by clinician review using a structured checklist of 15 factual elements (patient demographics, visit reason, clinical findings, diagnoses, prescriptions, referrals, follow-up plan). Summary rated accurate if all present elements in the note are correctly represented in the summary and no hallucinated elements are present. Three clinicians reviewed each summary independently. Disagreements resolved by majority vote."

You include comparison data when available. "Current model (GPT-4.5 Turbo April 2025 version): 94% factual accuracy. Previous model (GPT-4.5 February 2025 version): 91% factual accuracy. Human expert performance (experienced clinical staff): 97-98% factual accuracy. Baseline approach (template-based extraction): 78% factual accuracy." This context shows whether the model is an improvement over the previous approach and how close it is to human performance.

You report evaluation dates and declare when data is stale. "Performance data last updated: March 15, 2025. Based on evaluation set collected February 2025. Next scheduled evaluation: June 2025 (quarterly cycle). Data considered current until next evaluation." If you're reading a model card in January 2026 and the performance data is from March 2025, you know it's ten months old and may not reflect current behavior, especially if the provider has updated the model.

## Operational Data: Cost, Latency, and Constraints

The **operational data section** documents the practical characteristics of running this model in production: cost per query, latency percentiles, rate limits, context window, and any operational constraints.

Cost is reported as cost per query, not cost per token, because queries are the unit your finance team and your product team care about. "Median cost per summary: $0.04. Based on median input length of 2,400 tokens and median output length of 180 tokens at provider pricing of $10 per million input tokens and $30 per million output tokens. Monthly cost at current volume (15,000 requests per day, 450,000 per month): $18,000. Cost variance: 90% of requests fall between $0.03 and $0.06 due to input length variation."

You document cost trends. "Cost per request increased 15% from January 2025 ($0.035 median) to June 2025 ($0.04 median) due to input length growth. Input length increased from 2,100 tokens to 2,400 tokens as encounter note templates expanded. Cost per token unchanged (provider pricing stable). Cost projected to reach $0.045 per request by December 2025 if input length trend continues." This trend analysis informs budget planning and prompts investigation into why inputs are growing.

Latency is reported as percentiles, not averages. "Median latency (p50): 1.8 seconds. p95 latency: 3.2 seconds. p99 latency: 5.1 seconds. Measured from API request to complete response received. Includes network time, provider processing time, and response streaming time. Based on 450,000 requests logged in production May 2025." Percentiles matter because your worst-case user experience is determined by p95 or p99 latency, not median latency.

You document rate limits and how close you are to hitting them. "Provider rate limit: 10,000 requests per minute. Current peak usage: 400 requests per minute (4% of limit). Burst capacity sufficient for 25x current peak usage. No rate limit throttling observed in production to date." If you're using 80% of your rate limit at peak, that's a constraint that Product needs to know about before they plan a feature that doubles request volume.

You specify context window and typical utilization. "Model context window: 128,000 tokens. Typical prompt length: 2,400 tokens input plus 800 tokens for instructions and examples, total 3,200 tokens. Typical output length: 180 tokens. Total tokens per request: 3,380 (2.6% of context window). Maximum observed input: 8,200 tokens (6.4% of context window)." This utilization data shows you're nowhere near context limits, which means you have room to add examples or handle longer inputs without hitting constraints.

You document any operational constraints or quirks. "Model returns responses via streaming. Median time to first token: 0.4 seconds. Full response completes in 1.8 seconds median. System timeout configured at 10 seconds. 0.2% of requests exceed timeout and are retried. Retry success rate: 95%. Provider reports transient increased latency during model deployment windows (typically 2-4 hour windows twice per month, announced 24 hours in advance)."

## The Living Document: Updates and Ownership

A production model card is never finished. It evolves as you learn more about the model's behavior in production, as you run new evaluations, as you implement new mitigations, and as the provider updates the model. The **update history section** tracks this evolution.

Every significant change to the model card is logged with a date, author, and description. "March 15, 2025 (Sarah Chen, ML Engineering): Updated performance data with Q1 2025 evaluation results. Factual accuracy increased from 91% to 94%. Added new limitation: struggles with notes over 6,000 tokens. April 2, 2025 (James Rodriguez, Trust & Safety): Added bias risk: lower quality for non-standard English notes. Mitigation: additional review workflow implemented. May 10, 2025 (Sarah Chen): Updated operational data: cost increased to $0.04 per request due to input length growth. June 1, 2025 (Maya Patel, Product): Updated capabilities: now validated for urgent care visits (added 200 urgent care notes to evaluation set, performance comparable to primary care)."

This history serves two purposes. First, it creates an audit trail showing how your understanding of the model evolved. When an auditor asks "when did you discover the performance degradation on long notes?" you point to the April 2 update in the model card. Second, it shows the model card is actively maintained. A model card with no updates in twelve months is a red flag indicating the team isn't evaluating or monitoring the model.

Ownership is explicit. Every model card has a designated owner, typically the ML engineer or product engineer responsible for the system using the model. "Owner: Sarah Chen (ML Engineering). Backup: James Rodriguez (ML Engineering). Review cadence: quarterly evaluation cycle plus ad-hoc updates as issues discovered. Last reviewed: June 15, 2025. Next scheduled review: September 15, 2025." Clear ownership means someone is accountable for keeping the card current.

Review cadence is tied to your evaluation cycle. If you run quarterly evaluations, the model card updates quarterly with new performance data. If you run continuous monitoring with monthly deep dives, the card updates monthly. Ad-hoc updates happen whenever you discover a new limitation, implement a new mitigation, or observe a significant operational change.

## Templates, Tooling, and Team Adoption

Standardization is critical. Every model card in your organization should follow the same template with the same section structure so anyone can pick up any model card and know where to find the information they need. You create a model card template in Markdown or whatever documentation system your team uses, and you require every model to have a completed card before it can go to production.

The template includes prompts for each section to guide the author. Capabilities section prompt: "Document what the model can do, at what quality level, for which specific tasks. Include quantitative performance metrics from your evaluation. Scope capabilities precisely to the inputs and contexts you have validated. Do not claim capabilities you have not tested." Limitations section prompt: "Document observed failure modes, input characteristics that degrade performance, and quality ceilings. Be specific. Reference evaluation data or production incidents." These prompts ensure consistency and completeness.

Some teams maintain model cards as code in their repository alongside the system that uses the model. The model card lives in the service repo as a Markdown file, versioned with git, updated in pull requests, and reviewed as part of code review. This approach ensures the card stays synchronized with code changes and deploys. Other teams maintain model cards in a central documentation system like Confluence or Notion, organized by system or by model. Either approach works as long as the cards are discoverable and maintainable.

Tooling can help. Some teams build scripts that auto-populate the operational data section from production metrics: current cost per request, latency percentiles, rate limit utilization. These scripts pull data from your logging or observability system and update the model card weekly or monthly. Auto-population reduces maintenance burden and ensures operational data stays current. The performance data section still requires manual updates after evaluation cycles because evaluation involves human judgment and cannot be fully automated.

Adoption requires process integration. Model cards become part of your model selection process: you don't select a model without drafting its model card. They become part of your production readiness review: you don't deploy a model without a completed and reviewed model card. They become part of your incident response: when a model fails, the first document you open is its model card to understand known limitations and mitigations. They become part of your audit process: when auditors request documentation, you hand them model cards.

## Who Writes Model Cards and When

The ML engineer or product engineer responsible for integrating the model writes the initial model card draft during the evaluation phase. They document evaluation results, observed limitations, and projected operational characteristics. This draft is reviewed by the domain expert who helped design the evaluation criteria and by the product manager who owns the feature. The three-party review ensures the card accurately represents model capabilities and limitations from both technical and product perspectives.

Legal and Compliance review the risk assessment section before production deployment. They verify that risks are adequately characterized, that mitigations are sufficient, and that residual risks are acceptable given the use case. For high-risk applications like healthcare or financial services, Legal may require additional documentation or more stringent mitigations before sign-off.

After production deployment, the model card becomes the shared responsibility of Engineering and Product. Engineering updates the operational data and performance data sections. Product updates the capabilities section as new use cases are validated. Trust & Safety updates the risk assessment section as new risks are discovered or mitigations are implemented. The owner coordinates these updates and ensures the card remains coherent and current.

Model cards are reviewed in post-mortems after incidents. When a model fails in production, the incident review asks: was this failure mode documented in the model card? If yes, were the mitigations adequate? If no, why wasn't it anticipated, and what evaluation gaps does that reveal? The model card updates with lessons learned.

Model cards are reviewed in quarterly business reviews or engineering reviews. The team walks through the cards for all production models, highlights any models with stale data or incomplete sections, and prioritizes updates. This review ensures cards don't languish and that the organization maintains a current understanding of all models in production.

Your production model card is not overhead. It is the foundation of operational excellence and regulatory compliance. It is the artifact that ensures everyone in your organization understands what your models can do, what they can't do, what risks they carry, and what mitigations are in place. It is the artifact auditors request, the artifact Legal reviews, the artifact Product references, and the artifact Engineering maintains. Writing one for each model is not optional. It is professional practice.

The next critical layer of production model management is understanding the legal obligations that apply when you deploy general-purpose AI systems in 2026, particularly when those systems serve users in the European Union.