# 6.11 â€” Prompt Portability Layer: Normalizing System Prompts, Tool Schemas, and Output Constraints

In March 2025, a healthcare technology company deployed a medical intake assistant that had been carefully tuned over five months to work with Claude Opus 4. The system performed beautifully in production, maintaining 94% accuracy on extracting structured medical histories from patient conversations. Then their compute costs spiked when Anthropic announced a price increase, and Finance mandated an immediate migration to GPT-5 for all long-running tasks. Engineering copied the exact same prompts to the new provider, updated the API endpoint, and shipped. Within two days, accuracy had collapsed to 61%, patients were reporting confusing interactions, and the oncall engineer discovered that the system prompt's carefully crafted XML-tagged instructions, which Claude parsed perfectly, were being interpreted by GPT-5 as literal conversation text rather than structural guidance. The team spent three weeks rewriting every prompt in the codebase, only to face the same problem in reverse when they needed to A/B test Gemini 3 Pro for cost optimization. They had created dozens of provider-locked prompts with no systematic way to maintain them across providers.

The root cause was treating prompts as provider-specific artifacts rather than abstract specifications with provider-specific renderings. They had written instructions that worked for one model's particular parsing behavior instead of building a prompt portability layer that could translate intent into each provider's expected format. This is the central challenge of multi-provider architectures: you need the flexibility to switch models without rewriting your entire prompt library, but each provider has different conventions for system messages, tool schemas, output formatting, and instruction interpretation.

## The Prompt Portability Problem

Every major model provider has evolved different conventions for how they expect prompts to be structured. Claude responds well to XML-tagged instructions, explicit role definitions, and thinking sections. GPT models prefer markdown formatting, numbered lists, and JSON schema definitions in the system prompt. Gemini handles conversational system messages more naturally than formal instructions. Llama 4 models work best with few-shot examples embedded directly in the user message. DeepSeek V3.2 interprets chain-of-thought prompts differently than reasoning models like GPT-5.2 or Gemini Deep Think.

If you write a prompt that says "output your reasoning inside thinking tags, then provide your final answer inside answer tags," Claude will parse the XML structure correctly. GPT-5 will sometimes follow the convention but other times will output the literal text "thinking tags" in its response. Gemini 3 will ignore the instruction entirely unless you provide an example. This is not a matter of model quality, it is a matter of training data and fine-tuning choices that each provider made independently.

The naive solution is to maintain separate prompt files for each provider: `system_prompt_claude.txt`, `system_prompt_gpt.txt`, `system_prompt_gemini.txt`. This works until you have thirty tasks and five providers, at which point you are maintaining 150 prompt files. When you discover an improvement to your reasoning instructions, you need to manually apply the change across all 150 files, translating the concept into each provider's preferred syntax. This is unmaintainable at scale.

The professional solution is a prompt portability layer: an abstraction that lets you define prompt intent once and renders it into provider-specific formats. You write a canonical prompt specification that captures what you want the model to do, and the portability layer translates that specification into Claude's XML format, GPT's markdown format, Gemini's conversational format, and Llama's few-shot format. When you improve your reasoning instructions, you update the canonical spec once, and all provider renderings update automatically.

## Provider-Specific Rendering Patterns

A prompt portability layer needs to handle three major areas of provider divergence: system message structure, tool schema format, and output constraints.

System message structure varies dramatically. Claude treats the system message as a privileged instruction space separate from the conversation. GPT-5 merges system messages with the first user message conceptually. Gemini 3 uses system instructions as tone guidance more than hard constraints. Your portability layer needs to decide how to render the same logical instruction across these different models. If your canonical spec says "you are a medical intake assistant who must extract structured patient histories," Claude's rendering might use a formal role definition with XML-tagged responsibilities, GPT's rendering might use a numbered list of capabilities, and Gemini's rendering might use a conversational introduction.

Tool schema format is particularly painful because every provider uses a different JSON schema dialect. OpenAI's function calling expects parameters defined with type, description, and required fields in one structure. Anthropic's tool use expects input_schema as a separate object with a specific nesting. Google's function declarations use yet another format with different required fields. Your portability layer needs a canonical tool definition format, then renders it into each provider's expected schema.

Output constraints are where the biggest divergence appears. If you want structured JSON output, Claude works well with instructions like "output a JSON object with these exact fields" followed by an example. GPT-5 and GPT-5.1 support response_format with json_schema to enforce structure at the API level. Gemini 3 Pro has its own controlled generation format. Llama 4 models often need explicit few-shot examples showing the exact JSON structure. Your portability layer needs to take a logical output schema and render it as Claude prose instructions, GPT API-level constraints, Gemini controlled generation config, or Llama few-shot examples depending on which provider you are calling.

Temperature and sampling parameter names differ across providers. Claude uses temperature and top_p. OpenAI uses the same names but different default values. Google uses temperature and top_k instead of top_p. Some providers support frequency_penalty and presence_penalty, others do not. Your portability layer should let you specify sampling intent, like "prefer diverse creative responses" or "prioritize deterministic output," then map that to the appropriate parameter configuration for each provider.

## Building the Prompt Template System

The core abstraction is a prompt template that separates invariant content from provider-specific rendering. Start with a structured representation of prompt intent. Define sections like role, task_description, input_specification, output_specification, constraints, examples, and reasoning_instructions. Each section contains provider-neutral content.

Your role section might say "medical intake assistant specializing in oncology patient histories." Your task_description explains what the assistant does without specifying how to format the explanation. Your output_specification defines the logical schema you want: patient demographics, medical history timeline, current symptoms, medication list. Your constraints list requirements like "never make assumptions about symptoms not explicitly mentioned" without specifying whether to use XML tags or bullet points.

Then build provider-specific renderers. Each renderer takes the canonical template and produces a final system prompt, tool definitions, and API parameters for its target provider. The Claude renderer wraps constraints in XML tags, formats the output specification as a prose description with an example, and includes explicit thinking instructions. The GPT renderer uses markdown headers, defines output as a json_schema in the API call rather than the prompt, and skips the thinking instructions because GPT-5.2 has native reasoning modes. The Gemini renderer uses conversational tone, embeds output examples inline, and adds explicit few-shot demonstrations for complex extractions.

Tool definitions follow the same pattern. Define tools in a canonical format: tool name, description, parameter list with names, types, descriptions, and required flags. The Claude renderer converts this to Anthropic's tool use schema. The GPT renderer converts to OpenAI's function calling format. The Gemini renderer converts to Google's function declaration schema. When Google changes their schema format in a future API version, you update one renderer, not thirty tool definitions.

Output constraints are particularly important to abstract. If you want the model to output valid JSON matching a specific schema, represent that schema in a provider-neutral format. The Claude renderer generates prose instructions with an example. The GPT renderer uses response_format with json_schema. The Gemini renderer uses generationConfig with responseMimeType and responseSchema. The Llama renderer generates three few-shot examples showing valid outputs. All from the same canonical schema definition.

## Testing Prompt Portability

Prompt portability is not automatic. Just because you can render a prompt for multiple providers does not mean it will work equally well on all of them. You need systematic testing to validate that the rendered prompts produce equivalent behavior across providers.

Build a golden dataset for each task type. For the medical intake assistant, your golden set includes twenty real patient conversation transcripts with expert-labeled ground truth extractions. For each transcript, you know exactly which demographics should be extracted, which medications should be listed, which symptoms should be captured. This becomes your portability test suite.

Run the full golden set through each provider using their rendered prompts. Measure extraction accuracy, format compliance, error rates, and response latency. If Claude achieves 94% accuracy, GPT achieves 89%, Gemini achieves 91%, and Llama achieves 73%, you have a portability problem with Llama. Investigate whether the issue is the prompt rendering, the model's inherent capabilities, or a mismatch between your canonical schema and Llama's training distribution.

Test not just correctness but instruction following. Does each provider respect your constraints equally well? If your canonical prompt says "never make assumptions about symptoms not mentioned," does GPT-5 hallucinate medications as often as Claude Opus 4.5? If your constraint says "ask clarifying questions before making extractions when information is ambiguous," does Gemini 3 Pro actually ask questions or does it fill in defaults? Instruction-following fidelity varies significantly across providers, and your portability layer needs to account for this with provider-specific constraint rendering.

Test tool use consistency. If your task involves calling a check_drug_interaction tool, verify that all providers call it with the correct parameter structure, call it at the appropriate points in the conversation, and handle the tool response correctly. OpenAI and Anthropic have very different tool-calling patterns. Claude often calls multiple tools in parallel, GPT typically calls one at a time, and Gemini sometimes needs explicit prompting to use tools at all. Your portability layer may need provider-specific tool invocation guidance.

Regression test when you update the canonical template. If you improve your reasoning instructions, run the full golden set through all providers to ensure the change improved or maintained performance for each one. Sometimes an improvement for Claude degrades performance for GPT because the new phrasing triggers different behavior. You need visibility into these cross-provider effects.

## The Portability Versus Optimization Tradeoff

Perfect prompt portability and perfect per-provider optimization are mutually exclusive goals. A fully portable prompt that works identically across all providers necessarily leaves performance on the table for providers that could benefit from their unique features. A fully optimized prompt that exploits every provider-specific capability is by definition not portable.

The practical approach is selective optimization. Start with a portable baseline that works adequately across all providers. Then identify high-value tasks where provider-specific optimization delivers meaningful improvements. For these tasks, create provider-specific overrides in your portability layer.

A concrete example: your baseline reasoning instructions work fine for most tasks across all providers, but for a complex medical diagnosis task, you discover that Claude Opus 4.5 performs significantly better with explicit XML-tagged chain-of-thought sections, while GPT-5.2 performs better using its native reasoning mode with no thinking instructions at all. Your portability layer should support this. Define the baseline reasoning approach in your canonical template, then define provider-specific overrides for the diagnosis task. Claude gets the XML thinking tags, GPT gets the reasoning mode flag, other providers get the baseline.

Tool use is another area where selective optimization pays off. If you have a task that requires parallel tool calls and you know Claude handles parallel calls well while GPT does not, your portability layer can render Claude prompts that encourage parallel calls and GPT prompts that explicitly request sequential calls. Same logical task, different execution strategies optimized for each provider's strengths.

Output formatting benefits from selective optimization. For simple structured outputs like extracting a list of medications, the portable approach works fine. For complex nested structures like a full patient medical timeline with medications, diagnoses, procedures, and outcomes all cross-referenced, you might want Claude to use its structured output capabilities, GPT to use json_schema enforcement, and Gemini to use controlled generation, while Llama gets a simpler flattened schema that matches its training distribution better. The logical output is the same, the rendering is optimized per provider.

The key decision is where to draw the optimization line. If you optimize every prompt for every provider, you lose the maintainability benefits of portability. If you never optimize, you pay a performance tax across your entire system. The professional approach is to measure the performance gap between portable and optimized prompts for each task, then optimize selectively for tasks where the gap is large and the task is high-value.

## When to Standardize Versus When to Optimize

Standardize on portable prompts for foundational tasks that run across many providers and change frequently. Your content moderation prompts, your classification prompts, your simple extraction prompts should all be portable. These tasks are high-volume, well-understood, and benefit from consistent behavior across providers. The cost of maintaining provider-specific versions outweighs the performance gain from optimization.

Optimize per-provider for specialized tasks where model-specific features provide significant advantages. Your complex reasoning tasks, your long-context synthesis tasks, your tasks that require specific tool use patterns should be optimized. A medical diagnosis assistant running on Claude Opus 4.5 should exploit Claude's strengths: long context, structured thinking, parallel tool use. The same task running on GPT-5.2 should exploit its reasoning mode and function calling patterns. The cost of maintaining two versions is justified by the quality improvement.

Standardize for rapid experimentation. When you are still discovering the right prompt structure for a new task, start with a portable template that lets you test across multiple providers quickly. See which provider performs best, understand why, then decide whether to commit to that provider or build optimized versions for multiple providers. Portability accelerates the exploration phase.

Optimize when you have locked in a provider for a specific task based on cost, performance, or compliance requirements. If you have determined that all medical diagnosis tasks will run on Claude Opus 4.5 for the next year because of HIPAA compliance and model quality, invest in highly optimized Claude-specific prompts. The portability layer still provides value because you maintain the canonical schema and can render portable versions for testing or backup, but production uses the optimized version.

Standardize for multi-model pipelines where prompt consistency is critical. If your system uses three models in sequence, Claude for extraction, GPT for summarization, Gemini for final presentation, and the handoff between stages depends on consistent output formats, use portable prompts with strict schema enforcement. The risk of format drift across provider-specific optimizations outweighs the performance gain.

Optimize for single-model tasks where each provider handles a completely different workload. If Claude handles all your long-document analysis, GPT handles all your coding assistance, and Gemini handles all your conversational agents, there is no benefit to portability. Each task runs on one provider forever. Optimize aggressively for each provider's strengths and do not pay the abstraction tax.

## Maintaining the Portability Layer Over Time

Model providers update their APIs, introduce new features, and deprecate old formats constantly. In 2025 alone, OpenAI introduced structured outputs, deprecated legacy function calling, and changed default temperature values twice. Anthropic added tool use improvements and XML parsing enhancements. Google released Gemini 3 with entirely new configuration options. Your portability layer is not a one-time build, it is an ongoing maintenance surface.

Treat provider renderers as versioned components. When OpenAI releases a new API version, create a new GPT renderer that targets that version. Keep the old renderer for backward compatibility until you have migrated all tasks. This lets you adopt new provider features incrementally rather than forcing a big-bang migration.

Monitor provider updates for breaking changes. Subscribe to all provider changelogs, test your portability layer against new API versions in staging before they reach production, and maintain a provider compatibility matrix that shows which renderer version works with which API version. When Anthropic announces a deprecation, you have time to update the Claude renderer before the old format stops working.

Track provider-specific performance over time. Model providers update their underlying models without changing API versions. GPT-5.1 in January 2026 is not the same model as GPT-5.1 in June 2026 even though the API is identical. Your prompts may need adjustment as the model behavior shifts. Run your golden dataset through each provider monthly, track accuracy trends, and update renderers when you detect performance drift.

Version your canonical templates. When you discover a better way to structure reasoning instructions or output schemas, create a new template version rather than editing in place. This lets you migrate tasks incrementally and roll back if the new version causes regressions. Your portability layer should support multiple canonical template versions rendering to multiple provider versions simultaneously.

The next challenge is ensuring these carefully crafted multi-model architectures actually work as integrated systems, not just as collections of individually functional components, which is the focus of multi-model testing.
