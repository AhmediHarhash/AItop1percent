# 6.3 — Orchestrator-Worker Patterns: A Frontier Model Directing Cheaper Models

In September 2025, a legal technology company spent eleven weeks building what they called an intelligent document analysis pipeline. The system needed to process complex multi-jurisdiction contracts, extract structured data, identify potential compliance issues, and generate executive summaries. Their initial architecture sent every document through Claude Opus 4.5 for the full analysis workflow. The quality was exceptional. The cost was catastrophic: $847,000 in API fees across their first production quarter serving just 340 enterprise clients. Their unit economics showed they were losing $340 on every contract analyzed, even at their premium pricing tier. When the board demanded a path to profitability, the engineering team proposed downgrading to Haiku 4.5 across the board. Quality collapsed immediately—compliance detection dropped from 94% recall to 71%, and legal teams started reporting missed clauses that exposed clients to real liability risk. The company was trapped between financial ruin and professional negligence.

The root cause was architectural. They treated model selection as a binary choice: expensive and good, or cheap and bad. They never considered that most of their pipeline's work was simple structured extraction that Haiku handled perfectly, while only the compliance reasoning and risk assessment actually needed Opus-class capabilities. They didn't need to make every part of their system equally smart. They needed an architecture that matched model capability to subtask complexity—what's now called the **orchestrator-worker pattern**. This pattern uses a frontier model as the orchestrator that plans, delegates, and verifies, while cheaper models act as workers that execute well-defined subtasks. When the legal tech company finally rebuilt their pipeline this way, they cut costs by 73% while actually improving overall quality because the orchestrator could run multiple worker attempts and verify outputs. This chapter teaches you how to design, implement, and operate orchestrator-worker architectures in production.

## The Pattern Architecture

The orchestrator-worker pattern is structurally simple but operationally subtle. A single frontier model—typically Claude Opus 4.5, GPT-5, or Gemini 3 Deep Think—serves as the orchestrator. It receives the full user request, understands the complete context, decomposes the task into discrete subtasks, determines which subtasks require frontier reasoning versus simple execution, delegates the execution subtasks to cheaper worker models, receives and validates worker outputs, integrates the results, and produces the final response. The workers are smaller, faster, cheaper models like Haiku 4.5, GPT-5-mini, Gemini 3 Flash, or Llama 4 Scout. They never see the full task context. They receive narrow, well-specified instructions from the orchestrator, execute them, and return structured outputs.

The cost advantage comes from the ratio of orchestrator calls to worker calls and the price differential between frontier and worker models. If the orchestrator costs fifty cents per million tokens and workers cost two cents per million tokens, and you can decompose a task into one orchestrator call plus ten worker calls, you spend fifty cents on orchestration plus twenty cents on execution for seventy cents total, versus five dollars if you ran the entire task through the orchestrator ten times. The quality advantage comes from the orchestrator's ability to verify worker outputs, retry failed subtasks, and maintain coherent reasoning across the full task even when individual steps are handled by simpler models.

The architectural mistake most teams make is treating the orchestrator as a simple router. They build logic that says "if subtask is type A, send to worker model X; if type B, send to worker model Y." That's not orchestration—that's static routing with extra steps. Real orchestration means the frontier model exercises judgment. It looks at the specific content of this particular task, understands what could go wrong, decides which subtasks are safety-critical versus routine, chooses not just which model to use but how many attempts to allow and what verification to apply, and adapts its delegation strategy based on worker performance in real time. The orchestrator thinks. The workers execute.

## Task Decomposition: What the Orchestrator Delegates

The orchestrator's first job is decomposition. It must break the user's request into subtasks that are independent enough to parallelize, simple enough that worker models can handle them reliably, and well-specified enough that the orchestrator can verify the outputs without re-doing the work. Poor decomposition is the most common failure mode. Teams either decompose too finely—creating hundreds of micro-tasks with massive coordination overhead—or too coarsely—creating subtasks so complex that workers fail frequently and the orchestrator spends all its time on retries.

Effective decomposition follows capability boundaries, not workflow boundaries. Don't decompose based on "first do step one, then step two, then step three." Decompose based on "this part requires reasoning, this part requires retrieval, this part requires formatting, this part requires validation." The legal tech company's original pipeline had twelve sequential steps, which seemed like natural decomposition points. But when they analyzed capability requirements, they realized eight of those steps were pure structured extraction from explicit text, two were template-based formatting, one was retrieval from a static compliance database, and only one—the risk assessment—actually required legal reasoning. They collapsed the eight extraction steps into a single worker call with a detailed schema, handled formatting with string operations instead of LLM calls, moved retrieval out of the LLM pipeline entirely, and reserved Opus for the risk assessment. Twelve steps became one orchestrator call, one worker call, and two non-LLM operations.

The worker tasks that succeed most reliably are extraction, classification, formatting, and template filling. If the orchestrator can specify exactly what to extract, what schema to use, what categories to choose from, or what template to populate, worker models execute these tasks at frontier-level quality for a fraction of the cost. The worker tasks that fail most often are tasks requiring chain-of-thought reasoning, tasks requiring integration of information from multiple sources, tasks requiring domain expertise, and tasks requiring subjective judgment. When teams delegate these to workers, they get plausible-looking outputs that are subtly wrong—the kind of errors that pass superficial validation but cause downstream failures.

The orchestrator must also decide on execution order. Some subtasks must run sequentially because later tasks depend on earlier outputs. Others can run in parallel because they're independent. The cost-optimal strategy is maximum parallelization: send all independent subtasks to workers simultaneously, then integrate results. But the reliability-optimal strategy is often sequential validation: send the first subtask, verify the output, use that verified output as input to the second subtask, verify again, and so on. The orchestrator must balance speed and cost against error propagation risk. For high-stakes tasks, sequential verification is worth the latency penalty. For bulk processing, parallel execution with post-hoc validation catches enough errors at much lower cost.

## Communication Protocol: How Orchestrator and Workers Interact

The interface between orchestrator and workers determines system reliability. Most teams start with unstructured communication: the orchestrator generates a natural language instruction, the worker generates a natural language response, and the orchestrator tries to parse it. This works in demos and fails in production. When the worker's output format varies slightly—maybe it adds an explanation before the requested data, or uses different field names, or returns a list instead of a single item—the orchestrator's parsing breaks. You end up spending frontier model calls trying to parse worker outputs, which defeats the entire cost model.

Structured communication is not optional. The orchestrator must specify output schemas for worker tasks. The worker must return valid structured data—JSON without braces described in words, or XML-style tagged text, or any format that's deterministic to parse. The orchestrator validates schema compliance before attempting semantic validation. If the worker returns malformed output, the orchestrator retries with a more explicit schema specification, not with reasoning about what the worker might have meant. This discipline is what separates production orchestrator-worker systems from prototypes.

The schema specification itself must be part of the orchestrator's reasoning. Generic schemas don't work because different tasks need different structure. The orchestrator looks at the specific subtask, determines what fields are required, what types they should be, what validation rules apply, and generates a schema specification as part of the worker instruction. For extraction tasks, this might be a field list with descriptions and examples. For classification tasks, this might be an enumeration of valid categories with decision criteria. For generation tasks, this might be a template with labeled slots. The more precise the schema, the more reliably workers comply.

Context passing is the subtle challenge. The orchestrator has full context—it knows the user's original request, the business domain, the prior conversation history, and the results of other subtasks. Workers should not receive all of this context. Passing full context to workers destroys the cost model because you're paying for worker models to process thousands of tokens they don't need. But passing too little context means workers lack the information needed to execute their subtask correctly. The orchestrator must extract the minimal context subset required for each worker task. If the worker is classifying a clause type, it needs the clause text and the category definitions, but not the full contract or the user's original question. If the worker is extracting dates, it needs the relevant paragraph and the date format specification, but not the legal analysis from other clauses.

The communication pattern that works in production is orchestrator sends worker task ID, minimal context, structured output schema, and success criteria; worker returns task ID, structured output, and confidence score; orchestrator validates schema, validates semantics against success criteria, and either accepts output or retries with refined instructions. The task ID lets you track which worker calls correspond to which subtasks in logs. The confidence score lets the orchestrator decide whether to accept a marginal output or retry. The success criteria give workers explicit targets—not just "extract dates" but "extract dates in ISO 8601 format, return empty list if no dates found, flag ambiguous dates with confidence less than 0.8."

## When to Use This Pattern

Orchestrator-worker architectures make sense for complex multi-step tasks where most steps are simple, agent workflows where planning is expensive but actions are cheap, and any scenario where a small number of high-quality decisions control a large number of routine executions. The pattern does not make sense for simple single-step tasks, for tasks where every step requires frontier reasoning, or for real-time latency-critical applications where the orchestrator call adds unacceptable overhead.

The clearest signal that you need orchestrator-worker is when you look at your frontier model's token usage and realize most tokens are spent on repetitive subtasks, not on the core reasoning. A customer support system that uses GPT-5 to understand the user's question, retrieve relevant knowledge base articles, extract information from each article, synthesize the information into a response, and format the response for the user interface is spending maybe 20% of its tokens on understanding and synthesis—the parts that need GPT-5—and 80% on retrieval, extraction, and formatting that Gemini 3 Flash handles identically for one-twentieth the cost. Move the retrieval, extraction, and formatting to worker models, and your cost drops by 64% with no quality loss.

Agent workflows are the canonical use case. An agent that must plan a sequence of actions, execute each action, observe results, and replan based on outcomes is naturally orchestrator-worker. The planning and replanning require frontier reasoning. The action execution is often simple: call an API, run a search query, extract a specific field from a document, format a message. Using a frontier model for action execution is pure waste. The orchestrator plans, delegates actions to workers, observes worker outputs, updates its world model, and replans. This pattern is now standard in production agent systems from Anthropic, OpenAI, and Google, and it's why agent workflows became economically viable in 2025 after being cost-prohibitive in 2024.

The pattern also works for batch processing with variance in difficulty. Imagine processing ten thousand support tickets where 90% are simple requests that follow standard templates and 10% are complex edge cases requiring deep reasoning. A naive architecture runs all ten thousand through your frontier model. An orchestrator-worker architecture has the orchestrator triage each ticket, delegate the 9,000 simple tickets to worker models with template-based instructions, and handle the 1,000 complex tickets itself. Your cost drops by 85%, your latency improves because simple tickets get faster worker responses, and your quality stays high because complex tickets still get frontier reasoning.

Don't use orchestrator-worker for single-turn question answering where the answer requires synthesis of multiple information sources and deep reasoning. Don't use it for tasks where decomposition is harder than just solving the task directly. Don't use it for real-time applications where every millisecond matters, because the orchestrator call adds latency before any worker can start. And don't use it when your task success rate is already 98% with a mid-tier model—orchestration overhead costs more than the quality gain is worth.

## Cost Analysis: Overhead Versus Savings

The cost model is simple on paper and subtle in practice. You add one orchestrator call at frontier pricing, you replace N frontier calls with N worker calls at budget pricing, and you save money if the worker savings exceed the orchestrator overhead. The break-even point depends on the price ratio between frontier and worker models, the number of subtasks you can delegate, and the retry rate for worker tasks.

As of January 2026, Claude Opus 4.5 costs about fifty cents per million input tokens and one dollar fifty per million output tokens. Haiku 4.5 costs two cents input and six cents output. The ratio is 25x on input, 25x on output. If you can decompose a task into one Opus orchestrator call and ten Haiku worker calls, and each call processes similar token volumes, your input cost goes from fifty cents times eleven calls equals five dollars fifty with all-Opus down to fifty cents for orchestrator plus two cents times ten equals twenty cents for workers, total seventy cents. You save 87%. But this assumes perfect worker execution. If workers fail and require retries, the math changes.

Suppose 20% of worker tasks fail and require one retry. Now you need twelve worker calls instead of ten, costing twenty-four cents instead of twenty cents. You're still at ninety-four cents total, still saving 83%. But if worker tasks fail 50% of the time and require an average of two attempts each, you need twenty worker calls, costing forty cents, bringing your total to ninety cents—still much better than five dollars fifty, but the savings shrink as worker reliability drops. And if the orchestrator must re-decompose tasks when workers fail, you start paying for multiple orchestrator calls, and the model collapses.

The orchestrator overhead itself varies based on task complexity. Simple decomposition—"extract these five fields from this document"—costs a few thousand tokens. Complex decomposition—"analyze this legal contract, identify all compliance-relevant clauses, determine which regulations apply, assess risk for each clause, and synthesize an executive summary"—costs tens of thousands of tokens because the orchestrator must reason deeply about task structure. If orchestrator overhead is 5,000 tokens and worker tasks are 2,000 tokens each, and you delegate to ten workers, your orchestrator uses 5,000 tokens at fifty cents per million equals 0.25 cents, workers use 20,000 tokens at two cents per million equals 0.04 cents, total 0.29 cents. If you had run all ten tasks through Opus, you'd spend 25,000 tokens at fifty cents per million equals 1.25 cents. You save 77%.

But if orchestrator overhead is 30,000 tokens because task decomposition is complex, your orchestrator now costs 1.5 cents, workers still cost 0.04 cents, total 1.54 cents. You're only saving 23% compared to the all-Opus baseline. At some point, orchestration overhead exceeds worker savings, and the pattern becomes cost-negative. This happens when tasks are so complex that decomposing them requires nearly as much frontier reasoning as solving them directly, or when worker tasks are so large that the price differential doesn't matter, or when the number of subtasks is small—if you're only delegating two or three worker tasks, orchestrator overhead dominates.

The cost-optimal strategy is selective orchestration. Don't orchestrate every task. Use a fast classifier to triage incoming tasks into "simple enough for direct worker execution," "complex enough to need orchestration," and "too complex for worker delegation." Run simple tasks directly through worker models with no orchestrator. Run complex-but-decomposable tasks through orchestrator-worker. Run tasks that can't be decomposed directly through frontier models with no delegation. This three-tier architecture is what production systems converge to.

## Quality Control: Orchestrator Verification of Worker Outputs

The orchestrator's verification role is what makes this pattern reliable enough for production. Workers are cheaper because they're less capable. They make mistakes. The orchestrator must catch those mistakes before they propagate to users. Verification strategies range from simple schema validation to full semantic re-execution, with cost and reliability trading off at every point.

Schema validation is the baseline. The orchestrator checks that worker outputs conform to the specified structure. If the worker was supposed to return a list of dates and it returned a paragraph of text, the output is rejected immediately with no semantic analysis needed. Schema validation costs almost nothing—a few hundred tokens—and catches 30% to 40% of worker errors in typical production workloads. These are errors where the worker misunderstood the output format or hit a failure mode that caused it to generate explanatory text instead of structured data.

Semantic validation checks whether the output is plausible given the input. If the worker extracted five dates from a document and the orchestrator knows the document only mentioned three time references, the output is suspect. If the worker classified a support ticket as "billing issue" but the orchestrator sees no dollar amounts or payment terms in the ticket text, the classification might be wrong. Semantic validation requires the orchestrator to reason about the worker's task, which costs more tokens but catches another 20% to 30% of errors—cases where the worker returned well-formatted but incorrect data.

The expensive verification strategy is re-execution. The orchestrator performs the same task the worker just performed, compares outputs, and accepts the worker output only if it matches the orchestrator's answer. This catches nearly all worker errors but destroys the cost model because you're paying frontier prices for verification, which is the same cost as just doing the task with the frontier model in the first place. Re-execution makes sense only for high-stakes subtasks where errors are catastrophic and you want worker speed with frontier reliability. The legal tech company re-executes worker extraction for compliance-critical clauses but accepts worker outputs without re-execution for routine metadata extraction.

Selective re-execution based on confidence scores is the practical middle ground. Workers return confidence estimates with their outputs. The orchestrator accepts high-confidence outputs after schema validation only, accepts medium-confidence outputs after semantic validation, and re-executes low-confidence outputs. If 70% of worker tasks are high-confidence, 20% are medium-confidence, and 10% are low-confidence, you pay for re-execution on only 10% of tasks, which adds 10% to your worker costs but catches most errors before they reach users.

The verification strategy must be specified in the orchestrator's planning phase, not hard-coded. Different subtasks have different error tolerance. Extracting a document's title is low-stakes; getting it slightly wrong doesn't matter. Extracting the termination date from a contract is high-stakes; getting it wrong causes legal problems. The orchestrator assigns verification rigor based on subtask criticality. Low-stakes tasks get schema validation only. High-stakes tasks get semantic validation and confidence-based re-execution. This dynamic verification strategy is what makes orchestrator-worker reliable enough for production use in regulated domains.

## The Orchestrator Bottleneck Problem

The pattern's structural weakness is that the orchestrator is a serial bottleneck. Every task must pass through the frontier model for decomposition and verification. The orchestrator's latency and throughput limits system capacity. When request volume spikes, the orchestrator becomes the constraint. This is fine for workflows where orchestrator calls are rare relative to worker calls—one orchestrator call per ten worker calls means 90% of your compute is parallelizable. But for workflows where orchestrator and worker calls are one-to-one or one-to-two, the orchestrator is a throughput limiter.

The naive fix is to run multiple orchestrator instances in parallel. This works until you hit API rate limits on frontier models. As of January 2026, Claude Opus 4.5 and GPT-5 both have tier-based rate limits that cap requests per minute even for enterprise customers. If your orchestrator needs to process 10,000 tasks per minute and your rate limit is 5,000 requests per minute, you're bottlenecked. You can't just add more orchestrator instances because they all share the same rate limit pool.

The architectural fix is to batch orchestration when possible. Instead of one orchestrator call per task, send batches of ten or twenty tasks in a single orchestrator call. The orchestrator decomposes all twenty tasks, delegates all worker subtasks in parallel, and verifies all outputs. This reduces orchestrator calls by 20x, which usually eliminates the bottleneck. Batching works well for offline batch processing—analyzing a thousand contracts overnight—and works poorly for real-time interactive use cases where users expect sub-second response times.

The other fix is to cache orchestrator decisions. If the orchestrator has seen similar tasks before, it can reuse prior decomposition strategies instead of reasoning from scratch. A support ticket categorization system might see the same ticket types repeatedly. After the orchestrator has decomposed "billing question about refund policy" a hundred times, it can cache the decomposition: delegate to worker model with these instructions, verify with this schema. Cache hits skip the expensive orchestrator reasoning step and jump straight to worker delegation. Caching reduces orchestrator load by 40% to 70% in production systems with repetitive task distributions.

The wrong fix is to replace the orchestrator with a cheaper model. Teams try this when they hit orchestrator bottlenecks: "let's use Sonnet 4.5 instead of Opus 4.5 as orchestrator, it's faster and cheaper." Quality collapses. The orchestrator's job is hard—task decomposition, delegation decisions, output verification. Cheaper models can't do this reliably. They over-delegate tasks that need frontier reasoning, they under-specify worker instructions causing high retry rates, and they miss errors in verification. The cost savings from a cheaper orchestrator are destroyed by worker retries and escaped defects. If you're hitting orchestrator bottlenecks, the fix is batching and caching, not downgrading the orchestrator.

## Production Examples

The pattern appears in production across every domain where complex tasks have decomposable structure. Customer support systems use orchestrator-worker to handle tier-1 automation. The orchestrator understands the customer's question, retrieves relevant knowledge base articles using a worker model, extracts specific answers using another worker model, synthesizes a response with its own frontier reasoning, and formats the output with a worker model. The only frontier compute spent is on understanding and synthesis. Everything else runs on workers.

Content generation pipelines use it for high-volume production. A marketing technology company generates thousands of personalized emails daily. The orchestrator plans the email structure based on customer segment and campaign goals—frontier reasoning. Workers fill templates, retrieve personalization data, and format HTML. The orchestrator verifies that personalization is appropriate and messaging is on-brand, but doesn't write every sentence itself. This system generates content at 5x the volume of an all-frontier architecture at 30% of the cost.

Document processing workflows use orchestrator-worker for multi-stage analysis. The orchestrator reads a financial disclosure document, identifies sections requiring detailed analysis, delegates extraction of tables and figures to worker models, delegates summarization of standard sections to workers, performs its own analysis on risk factors and forward-looking statements—the high-value reasoning—and integrates all outputs into a final report. The workers handle 80% of the document volume, the orchestrator handles 20%, and the output quality matches all-frontier processing.

Agent systems use it universally. Any agent that plans actions, executes them, and replans based on results is orchestrator-worker by design. The agent's planning loop is the orchestrator. The action executors are workers. OpenAI's Operator, Anthropic's Claude Code, and Google's Gemini Agents all follow this architecture. The planning model is frontier. The action executors are task-specific and often much cheaper. This is the pattern that made production agents economically viable in 2025, and it's now the default architecture for any agent system running at scale.

The pattern's durability comes from its alignment with fundamental economics. Frontier models are expensive because frontier reasoning is expensive to produce—it requires massive training compute and careful post-training. But most real-world tasks don't need frontier reasoning in every step. They need it for planning, verification, and integration, while execution steps are routine. Orchestrator-worker exploits this structure. It concentrates expensive compute where reasoning matters and uses cheap compute everywhere else. As long as model pricing reflects capability—as long as frontier models cost more than budget models—this pattern will be cost-optimal for complex decomposable tasks. The next topic is the opposite approach: instead of delegating to cheaper models, run multiple models in parallel and aggregate their outputs to achieve reliability beyond any single model's capability.

