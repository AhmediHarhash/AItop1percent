# 1.10 — Capability Tiers: Reasoning, Instruction Following, Creativity, Factuality, and Safety

In September 2025, a content moderation platform serving a social media company selected Claude Opus 4.5 for harmful content detection based on its top ranking in overall benchmark performance. The model excelled at complex reasoning tasks, scored highest on MMLU-Pro, and dominated on multi-step logic challenges. The team assumed that the best overall model would be best for their task. It was not. After deploying to production, they discovered that Opus 4.5 had a 12% false positive rate on edge cases involving sarcasm, satire, and cultural context — significantly higher than GPT-5.2, which ranked lower overall but had been specifically optimized for safety and nuance detection. The false positives led to 4,700 erroneous content removals in two weeks, a user backlash, and a rushed rollback to GPT-5.2. The diagnosis was straightforward: they had optimized for the wrong capability dimension. They needed tier-1 safety performance, not tier-1 reasoning performance. Opus 4.5 was tier-1 in reasoning and tier-2 in safety. GPT-5.2 was tier-2 in reasoning and tier-1 in safety. The mismatch cost them $310,000 in reputation damage, manual review costs, and engineering time.

Capability tiers are not overall rankings. They are dimension-specific performance categories that map models to the demands of specific tasks. A model can dominate in reasoning while struggling in instruction following. Another can excel at creativity but hallucinate frequently. A third can be unmatched in safety but rigid in structured output. Treating capability as a single axis is a mistake. You must decompose capability into its constituent dimensions, map your task to the required tiers in each dimension, and select the model that meets your specific needs — not the model that wins the most benchmarks.

## The Concept of Capability Tiers

A **capability tier** is a performance category within a specific dimension. Tier-1 means best-in-class performance for that dimension as of the current model generation. Tier-2 means strong but not leading performance. Tier-3 means acceptable for low-stakes tasks but not reliable for high-stakes use cases. The tiers are not static. Every major model release reshuffles the tier assignments. A model that was tier-1 in reasoning six months ago may be tier-2 today because a competitor released a better model.

The key insight is that a model's tier varies by dimension. GPT-5.2 as of January 2026 is tier-1 in instruction following and safety, tier-1 in structured output, tier-2 in reasoning, tier-2 in creativity, and tier-1 in factuality. Claude Opus 4.5 is tier-1 in reasoning, tier-1 in creativity, tier-2 in instruction following, tier-2 in safety, and tier-1 in factuality. Gemini 3 Deep Think is tier-1 in extended reasoning, tier-2 in instruction following, tier-3 in creativity, tier-2 in safety, and tier-2 in factuality. Llama 4 405B Instruct is tier-2 in reasoning, tier-2 in instruction following, tier-3 in creativity, tier-3 in safety, and tier-2 in structured output.

When you select a model, you must identify which dimensions matter most for your task and choose the model that is tier-1 or tier-2 in those dimensions. If your task requires perfect adherence to complex system prompts and strict refusal of harmful requests, you prioritize instruction following and safety. If your task requires creative storytelling and stylistic variety, you prioritize creativity. If your task requires multi-step logical deduction, you prioritize reasoning. The model that ranks first overall may not rank first in the dimensions you care about.

## Reasoning: Multi-Step Logic, Math, and Code

**Reasoning** is the ability to perform multi-step logical deduction, mathematical problem-solving, and code generation that requires planning and abstraction. High reasoning capability means the model can chain dependencies, backtrack when assumptions fail, and arrive at correct conclusions even when the path is non-obvious. Low reasoning capability means the model takes shortcuts, makes logical errors, and fails on tasks that require more than two inference steps.

As of January 2026, the reasoning tier-1 models are Claude Opus 4.5, Gemini 3 Deep Think, and GPT-5.2. Opus 4.5 excels at open-ended problem decomposition and handles complex legal reasoning, scientific hypothesis generation, and strategic planning. Deep Think uses extended inference time to explore multiple solution paths and is unmatched for formal proof verification and competition-level math. GPT-5.2 has strong reasoning but is slightly more prone to overconfidence on ambiguous problems. All three are suitable for high-stakes reasoning tasks.

Tier-2 reasoning models include GPT-5-mini, Claude Sonnet 4.5, and Llama 4 405B Instruct. They handle most reasoning tasks competently but struggle on problems requiring more than five inference steps or deep domain-specific knowledge. Sonnet 4.5 is fast and cost-effective for intermediate reasoning like technical support triage, business logic validation, and moderately complex code review. GPT-5-mini is suitable for everyday math, basic logical inference, and simple code generation. Llama 4 405B is strong in specific domains where it has seen extensive training data but weaker in abstract reasoning.

Tier-3 reasoning models include GPT-5-nano, Claude Haiku 4.5, Gemini 3 Flash, and DeepSeek V3.2 base. They are adequate for classification, simple extraction, and single-step inference but fail on multi-step problems. You use them when reasoning is not the task's bottleneck.

If your task is multi-step mathematical proof checking, you need tier-1 reasoning. If your task is explaining a simple calculation to a user, tier-2 reasoning suffices. If your task is classifying transactions into categories, tier-3 reasoning is fine. The mistake is using tier-1 reasoning models for tier-3 tasks, paying 10x the cost for capability you do not need, or using tier-3 reasoning models for tier-1 tasks, causing frequent failures.

## Instruction Following: Complex System Prompts, Format Compliance, and Constraint Adherence

**Instruction following** is the ability to adhere to detailed, multi-part system prompts that specify format constraints, tone requirements, refusal criteria, and procedural rules. High instruction-following capability means the model respects all constraints simultaneously, even when they conflict or require subtle judgment. Low instruction-following capability means the model ignores parts of the prompt, misinterprets instructions, or reverts to default behavior when the prompt is complex.

GPT-5.2 and GPT-5 are tier-1 in instruction following as of January 2026. They handle system prompts exceeding 5,000 tokens with dozens of conditional rules and maintain compliance across long conversations. They are the best choice for tasks where strict adherence to format, tone, and constraints is mandatory — customer support scripts, regulated communication templates, and compliance-sensitive content generation.

Claude Opus 4.5 and Sonnet 4.5 are tier-2 in instruction following. They follow instructions well but occasionally drift on very long or ambiguous prompts. They are suitable for most use cases but require more prompt iteration to achieve the same constraint adherence as GPT-5.2.

Gemini 3 Pro and Llama 4 405B Instruct are tier-2 to tier-3 in instruction following. They handle straightforward instructions well but struggle with nuanced or conflicting constraints. If your system prompt says "be concise but thorough" or "refuse harmful requests unless the user provides medical credentials," these models may misinterpret the condition or default to overly cautious behavior.

If your task requires following a 3,000-token system prompt with 15 conditional refusal rules and format requirements for HIPAA-compliant messaging, you need GPT-5.2. If your task is a simple chatbot with a 200-token system prompt, tier-2 instruction following is adequate. Mismatching the tier to the task causes either over-spending or frequent failures to comply with constraints.

## Creativity: Open-Ended Generation, Style Variety, and Novel Combinations

**Creativity** is the ability to generate novel, stylistically varied, and engaging content in open-ended contexts like storytelling, marketing copy, and brainstorming. High creativity means the model produces unexpected yet coherent ideas, adapts style fluidly, and avoids repetitive phrasing. Low creativity means the model defaults to generic formulations, reuses the same sentence structures, and produces outputs that feel mechanical.

Claude Opus 4.5 is tier-1 in creativity as of January 2026. It generates rich narrative prose, adapts to stylistic constraints like "write in the style of early 20th-century journalism," and produces varied outputs even when given similar prompts. It is the model of choice for creative writing, marketing ideation, and content that must feel human and distinctive.

GPT-5.2 is tier-2 in creativity. It produces competent creative outputs but defaults to more formulaic structures. It is suitable for standard marketing copy, blog posts, and ideation where novelty is valued but not critical.

Gemini 3 Deep Think and Llama 4 405B are tier-3 in creativity. They prioritize correctness and reasoning over novelty. Their outputs are functional but often feel stiff or over-explained. You do not use Deep Think for creative writing; you use it for formal proofs.

If your task is generating product descriptions for an e-commerce site, tier-2 creativity is fine. If your task is writing narrative-driven ad copy that must stand out, you need tier-1 creativity. If your task is extracting data from invoices, creativity is irrelevant.

## Factuality: Grounding, Hallucination Rates, and Knowledge Currency

**Factuality** is the ability to produce outputs grounded in real information, minimize hallucinations, and reflect up-to-date knowledge. High factuality means the model rarely invents facts, qualifies uncertain claims, and avoids confidently asserting false information. Low factuality means the model fabricates citations, invents statistics, and presents guesses as facts.

GPT-5.2, Claude Opus 4.5, and Gemini 3 Pro are tier-1 in factuality as of January 2026. All three have knowledge cutoffs in mid-2025 and are updated regularly. They hallucinate less frequently than earlier generations and are more likely to say "I don't know" when uncertain. They are suitable for knowledge-intensive tasks like research summarization, technical documentation, and educational content.

Llama 4 405B and Claude Sonnet 4.5 are tier-2 in factuality. They perform well on common knowledge but hallucinate more often on obscure topics or recent events. They are adequate for general-purpose content but require fact-checking for high-stakes use cases.

Smaller models like GPT-5-mini, Haiku 4.5, and Gemini 3 Flash are tier-3 in factuality. They hallucinate frequently on knowledge-intensive tasks and should not be used for information retrieval or fact-dependent outputs without retrieval augmentation.

If your task is generating educational content for medical students, you need tier-1 factuality and must validate outputs against authoritative sources. If your task is generating conversational responses where occasional minor errors are acceptable, tier-2 factuality is fine. If your task is generating creative fiction, factuality is not a constraint.

## Safety: Refusal Rates, Harmful Content Prevention, and Jailbreak Resistance

**Safety** is the ability to refuse harmful requests, resist adversarial prompts designed to bypass restrictions, and avoid generating content that violates policies around violence, hate speech, illegal activity, or self-harm. High safety means the model refuses robustly even under sophisticated jailbreak attempts and errs on the side of caution. Low safety means the model is easily manipulated or generates harmful content when prompted indirectly.

GPT-5.2 is tier-1 in safety as of January 2026. It has the most sophisticated refusal logic, handles edge cases well, and resists jailbreak techniques like role-playing, hypothetical framing, and multi-turn manipulation. It is the best choice for consumer-facing applications, content moderation, and any use case where harmful outputs carry legal or reputational risk.

Claude Opus 4.5 and Sonnet 4.5 are tier-2 in safety. They refuse most harmful requests but are slightly more permissive on ambiguous cases. They are suitable for most use cases but require additional guardrails for high-risk applications.

Gemini 3 Pro is tier-2 in safety. Llama 4 405B is tier-3. It has weaker built-in refusal logic and relies on external guardrails for safety. Open-source models generally score lower on safety because they prioritize flexibility and user control over built-in restrictions.

If your task is a public-facing chatbot, you need tier-1 safety. If your task is internal research assistance for a controlled user base, tier-2 safety may suffice. If your task is code generation in a sandboxed environment, safety is less critical.

## Structured Output: JSON Reliability, Schema Adherence, and Function Calling Accuracy

**Structured output** is the ability to generate outputs that conform exactly to specified formats like JSON schemas, function calls, or structured templates. High structured output capability means the model produces valid JSON 99%+ of the time, adheres to field constraints, and handles optional fields correctly. Low structured output capability means the model frequently produces malformed JSON, omits required fields, or hallucinates extra fields.

GPT-5.2 and GPT-5 are tier-1 in structured output as of January 2026. They support native JSON mode and function calling with extremely high reliability. They are the best choice for API integrations, workflow automation, and any task where parsing failures cause downstream errors.

Claude Opus 4.5 and Sonnet 4.5 are tier-1 in structured output. They reliably produce valid JSON and handle complex schemas with nested objects and arrays.

Gemini 3 Pro is tier-2 in structured output. It performs well on simple schemas but struggles with deeply nested structures or schemas with many optional fields.

Llama 4 405B and smaller models are tier-2 to tier-3 in structured output. They require more careful prompting and validation to achieve reliable formatting.

If your task is extracting invoice data into a strict JSON schema for accounting software, you need tier-1 structured output. If your task is generating markdown-formatted summaries, structured output capability is less critical.

## Mapping Tasks to Required Capability Tiers

The correct model selection process starts by identifying which capability dimensions your task depends on and what tier is required in each dimension. A task is not just "hard" or "easy." It has a capability profile: reasoning tier-2, instruction following tier-1, creativity tier-3, factuality tier-1, safety tier-1, structured output tier-2.

A medical triage chatbot requires tier-1 safety, tier-1 factuality, tier-1 instruction following, tier-2 reasoning, tier-3 creativity, and tier-1 structured output. The model must refuse inappropriate medical advice, ground responses in accurate information, follow complex conditional prompts, reason through symptom logic, generate functional rather than creative text, and produce structured outputs for downstream systems. GPT-5.2 matches this profile. Claude Opus 4.5 does not because it is tier-2 in safety and instruction following.

A creative writing assistant requires tier-1 creativity, tier-2 reasoning, tier-2 instruction following, tier-3 factuality, tier-2 safety, and tier-3 structured output. The model must generate novel prose, handle narrative logic, follow stylistic constraints, tolerate fictional content, avoid harmful outputs, and produce freeform text. Claude Opus 4.5 matches this profile. GPT-5.2 does not because it is tier-2 in creativity.

A legal contract analysis tool requires tier-1 reasoning, tier-1 factuality, tier-1 instruction following, tier-3 creativity, tier-1 safety, and tier-2 structured output. The model must reason through legal dependencies, avoid hallucinating case law, follow detailed analysis prompts, produce functional rather than creative text, refuse to provide legal advice outside scope, and generate semi-structured summaries. Claude Opus 4.5 or GPT-5.2 both match this profile.

The mistake is selecting based on overall benchmark performance rather than task-specific capability profiles. The social media moderation platform selected Opus 4.5 because it ranked highest overall. But their task required tier-1 safety, not tier-1 reasoning. GPT-5.2 was the correct choice. The mismatch caused the incident.

## Why the Best Model Overall Is Often Not the Best Model for Your Task

Benchmark leaderboards rank models by aggregate performance across diverse tasks. A model that scores highest overall is strong across many dimensions but may not be strongest in the specific dimensions your task requires. If your task is narrow and well-defined, a model optimized for that task profile will outperform the overall leader.

GPT-5.2 leads in instruction following, safety, and structured output. Claude Opus 4.5 leads in reasoning and creativity. Gemini 3 Deep Think leads in extended reasoning and formal problem solving. None is universally best. Each dominates in specific capability dimensions.

If you select the overall leaderboard winner for every task, you overpay for unnecessary capabilities and underprovision the capabilities you need. You use Opus 4.5 for content moderation when GPT-5.2 has better safety. You use GPT-5.2 for creative writing when Opus 4.5 has better creativity. You use either for simple classification when Haiku 4.5 is faster and cheaper.

The correct approach is to define your task's capability profile, identify the minimum tier required in each dimension, and select the cheapest model that meets all minimum tiers. If you need tier-1 safety, tier-1 instruction following, and tier-2 reasoning, GPT-5.2 is the match. If you need tier-1 reasoning, tier-1 creativity, and tier-2 safety, Opus 4.5 is the match. If you need tier-2 reasoning and tier-3 everything else, Sonnet 4.5 or GPT-5-mini is the match.

## The Capability Tier List as of January 2026

This is a snapshot of capability tiers for major models as of January 2026. These tiers will shift with every model release. You must maintain your own tier assessments based on your evaluation results, not trust public benchmarks.

Reasoning: Tier-1 — Claude Opus 4.5, Gemini 3 Deep Think, GPT-5.2. Tier-2 — GPT-5-mini, Claude Sonnet 4.5, Llama 4 405B. Tier-3 — GPT-5-nano, Haiku 4.5, Gemini 3 Flash.

Instruction Following: Tier-1 — GPT-5.2, GPT-5. Tier-2 — Claude Opus 4.5, Sonnet 4.5, Gemini 3 Pro. Tier-3 — Llama 4 405B, DeepSeek V3.2.

Creativity: Tier-1 — Claude Opus 4.5. Tier-2 — GPT-5.2, Sonnet 4.5. Tier-3 — Gemini 3 Deep Think, Llama 4 405B.

Factuality: Tier-1 — GPT-5.2, Opus 4.5, Gemini 3 Pro. Tier-2 — Sonnet 4.5, Llama 4 405B. Tier-3 — GPT-5-mini, Haiku 4.5, Gemini 3 Flash.

Safety: Tier-1 — GPT-5.2. Tier-2 — Opus 4.5, Sonnet 4.5, Gemini 3 Pro. Tier-3 — Llama 4 405B, open-source models.

Structured Output: Tier-1 — GPT-5.2, GPT-5, Opus 4.5, Sonnet 4.5. Tier-2 — Gemini 3 Pro, Llama 4 405B. Tier-3 — smaller models.

These tiers are not authoritative rankings. They are informed estimates based on public benchmarks, vendor claims, and practitioner experience. You must validate them with your own evaluations on your own data. A model that is tier-1 on a public benchmark may be tier-2 on your specific task. A model that is tier-2 overall may be tier-1 for your narrow use case.

## How Capability Tiers Shift with Each Model Release

The tier assignments in this chapter are current as of January 2026. In six months, they will be outdated. GPT-6 may launch and dominate reasoning. Gemini 4 may improve safety. Claude Opus 5 may match GPT-5.2 in instruction following. Llama 5 may close the gap in factuality. Every major release reshuffles the tiers.

This means you cannot hardcode tier assignments into your selection logic. You must reevaluate tiers every quarter or after every major model release. If you selected GPT-5.2 in January 2026 because it was tier-1 in safety, you must verify that it remains tier-1 in July 2026. If a new model surpasses it, you must decide whether to migrate.

A content moderation company runs capability tier assessments quarterly. They send 5,000 test cases representing their task profile to every major model and measure performance across six dimensions. They update their tier rankings based on the results. In Q1 2025, GPT-5 was their tier-1 safety choice. In Q2 2025, GPT-5.2 replaced it. In Q3 2025, they tested an early preview of GPT-5.5 and found it offered no safety improvement, so they stayed on 5.2. In Q4 2025, Claude Opus 4.5 closed the safety gap, so they added it as a tier-1 alternative.

This process requires ongoing investment. You must allocate engineering time to run evaluations, analyze results, and update model selection decisions. But the alternative is operating on stale assumptions and missing opportunities to improve performance or reduce cost.

## Building Your Own Capability Tier Assessment

Public benchmarks like MMLU, HumanEval, and TruthfulQA provide directional guidance but do not reflect your specific task. A model that scores 92% on MMLU may score 78% on your internal medical knowledge evaluation. A model that excels on HumanEval may struggle with your organization's coding standards. You must build your own tier assessments.

The process is straightforward. First, define capability dimensions that matter for your task. Second, create evaluation datasets for each dimension with 500 to 2,000 examples. Third, send the datasets to candidate models and measure performance. Fourth, rank models by dimension and assign tiers. Fifth, select the model that meets your minimum tier requirements in all dimensions.

A legal research company built dimension-specific evaluations in mid-2025. For reasoning, they created 800 multi-step legal logic problems requiring case law synthesis. For factuality, they created 1,200 questions about statutes, precedents, and legal definitions with ground truth answers. For instruction following, they created 400 prompts with complex conditional constraints. They ran these evaluations against GPT-5, GPT-5.2, Opus 4.5, and Gemini 3 Pro.

The results: Opus 4.5 ranked first in reasoning, GPT-5.2 ranked first in factuality and instruction following, Gemini 3 Pro ranked third in all dimensions. They selected GPT-5.2 for production because factuality and instruction following were higher priority than reasoning for their core use case. Six months later, they reran the evaluations with new snapshots and confirmed that GPT-5.2 still met their tier requirements.

This approach requires infrastructure. You need evaluation datasets, a harness to run evaluations across models, metrics for each dimension, and a process to update assessments. But the investment is justified. Teams that rely on public benchmarks optimize for the wrong thing. Teams that build their own tier assessments optimize for their specific needs.

Capability tiers are the lens through which you map tasks to models. You cannot select models based on overall rankings or vendor marketing. You must decompose capability into dimensions, measure performance in each dimension, assign tier rankings, and match models to task profiles. The best model overall is often not the best model for your task. The right model is the one that meets your minimum tier requirements in the dimensions that matter and costs the least. If you select models based on capability profiles rather than leaderboard positions, you will spend less, ship faster, and avoid costly mismatches like the social media moderation platform experienced. The discipline of tier-based selection is not optional. It is the foundation of defensible model choice.

In the next subchapter, we turn to the operational constraints that shape model selection: cost, latency, throughput, and rate limits — and how to balance capability against infrastructure realities.
