# 5.10 â€” Latency Monitoring: P50, P95, P99 Tracking and Anomaly Detection

In mid-2025, a healthcare documentation platform serving 4,200 clinicians experienced a cascade failure that took 11 hours to diagnose. Response times for clinical note summarization, normally under 2 seconds, spiked to 14 seconds for 6% of requests. But the monitoring dashboard showed median latency at 1.8 seconds, unchanged from baseline. The team saw no alerts. Users saw degraded performance. Doctors waited. Some abandoned the tool mid-shift and returned to manual note-taking. By the time the engineering team noticed the problem, six hours after it started, 340 users had filed support tickets, trust in the product had eroded, and the root cause was still unknown. The issue was a regional capacity constraint at the model provider, affecting only requests routed to a specific availability zone. The median was unaffected because 94% of requests were unaffected. But the 6% that hit the degraded zone experienced catastrophic latency. The team had been monitoring only P50 latency, the 50th percentile, which is the median. They were not tracking P95 or P99, the 95th and 99th percentiles, which capture tail latency. They had no alerting on latency distribution changes. They had no anomaly detection. They were flying blind.

The lesson: median latency is not enough. Users experience the tail, not the average. A system that responds in 1.5 seconds for 90% of users and 18 seconds for 10% of users is not a 3-second system. It is a broken system for one in ten users. You must track latency at every percentile, detect anomalies in real time, correlate latency with quality and error rates, and enforce service level objectives. Latency monitoring is not a nice-to-have. It is the early warning system for production reliability.

## Why Continuous Monitoring Beats One-Time Benchmarks

One-time latency benchmarks tell you what the model can do under ideal conditions in a controlled environment. Continuous production monitoring tells you what the model actually does under real load with real users in real time. The two measurements are not comparable. Benchmarks are optimistic. Production is reality. A model that responds in 1.2 seconds during a benchmark test may respond in 2.8 seconds in production due to network latency, API rate limits, retries, cold starts, or provider-side throttling. You cannot predict production performance from benchmarks. You can only measure it.

Production latency is not static. It varies by time of day, day of week, geographic region, model version, provider capacity, network conditions, and input characteristics. A model that is fast at 3 AM Pacific time may be slow at 10 AM Eastern time when request volume peaks. A model that is fast for 50-token inputs may be slow for 1,500-token inputs. A model that is fast on Tuesday may be slow on Thursday because the provider deployed a new version with different performance characteristics. If you measure latency once and assume it stays constant, you will be wrong.

Continuous monitoring means measuring every request, aggregating latency distributions over time windows, and tracking changes. You do not sample. You do not estimate. You instrument every API call, record the latency, and store it in a time-series database. You calculate percentiles every minute, every five minutes, and every hour. You visualize the distributions on a dashboard. You set alerts for deviations from baseline. You review latency trends daily. This is not optional for production AI systems. It is the minimum standard for operational visibility.

A financial services company in early 2026 deployed GPT-5.1 for transaction risk analysis. They benchmarked latency at 1.8 seconds per request during testing. In production, P50 latency was 2.1 seconds, P95 was 4.3 seconds, and P99 was 8.7 seconds. The gap between benchmark and production P50 was 17%. The gap between P50 and P99 was 314%. If they had relied on the benchmark, they would have assumed the system met their 3-second SLO. In reality, 5% of requests exceeded the SLO by 40%, and 1% of requests exceeded it by 190%. Continuous monitoring revealed the problem. They investigated and found that requests with more than 800 tokens of input triggered rate-limiting at the provider, causing delays. They adjusted input chunking to stay under the threshold, and P95 latency dropped to 2.9 seconds, within SLO.

## What to Measure: TTFT, Total Response Time, and Per-Component Latency

Latency is not a single number. It is the sum of multiple components, each of which can degrade independently. You must measure time to first token, total response time, and per-component latency to understand where delays occur and how to fix them.

Time to first token, abbreviated TTFT, is the interval between sending the request and receiving the first token of the response. For streaming outputs, TTFT is the user-perceived start of the response. A system with 600-millisecond TTFT feels fast, even if the total response takes 3 seconds, because the user sees progress immediately. A system with 4-second TTFT feels broken, even if the total response completes in 5 seconds, because the user waits in silence for 4 seconds. TTFT is dominated by model inference startup time, queue wait time, and network round-trip time. If TTFT spikes, the problem is usually provider-side congestion or cold starts.

Total response time is the interval between sending the request and receiving the complete response. For non-streaming outputs, total response time is the only latency that matters. For streaming outputs, total response time still matters for tasks where the user waits for the full output before acting, such as document generation or code completion. Total response time is the sum of TTFT, token generation time, and network transfer time. Token generation time is proportional to output length and inversely proportional to model speed. If total response time spikes but TTFT does not, the problem is usually slow token generation or unexpectedly long outputs.

Per-component latency is the breakdown of total response time into each stage of the pipeline: preprocessing, API call, postprocessing, retry logic, and result formatting. You measure the duration of each stage separately and sum them to verify they match total response time. This decomposition reveals which component is the bottleneck. If API call time is 90% of total latency, optimization effort should focus on model selection or provider switching. If preprocessing is 40% of total latency, optimization should focus on input preparation logic. You cannot improve what you do not measure separately.

A customer support platform in late 2025 measured total response time at 3.2 seconds but did not instrument per-component latency. When users complained about slowness, the team assumed the model was the bottleneck and tested faster models. After switching to a faster model, total latency dropped only 8%, from 3.2 seconds to 2.9 seconds. They instrumented the pipeline and found that preprocessing, which involved database lookups and context assembly, took 1.4 seconds. The model API call took 1.1 seconds. Postprocessing and formatting took 0.7 seconds. The model was not the bottleneck. Preprocessing was. They optimized the database queries and reduced preprocessing to 0.6 seconds, dropping total latency to 2.4 seconds. Without per-component measurement, they would have optimized the wrong thing.

You log these metrics for every request. The log includes request ID, timestamp, user ID, task type, input token count, output token count, TTFT, total response time, and per-component durations. You store logs in a system that supports fast aggregation and querying, such as Elasticsearch, ClickHouse, or a managed observability platform like Datadog or New Relic. You query the logs to calculate percentiles, filter by user cohort or task type, and correlate latency with errors or retries.

## Percentile Tracking: P50, P95, P99 and Why Each Matters

Latency distributions are not normal. They are skewed. Most requests complete quickly. A minority of requests take much longer. The average latency is not representative of user experience, because the average is distorted by outliers. The median latency, also called P50, is the 50th percentile, meaning 50% of requests are faster and 50% are slower. The median is a better central tendency measure than the average, but it still hides the tail.

P95 latency is the 95th percentile, meaning 95% of requests are faster and 5% are slower. P95 tells you what the slowest 5% of users experience. If your P95 is 4.2 seconds and your SLO is 3 seconds, 5% of your users are seeing degraded performance. That is one in twenty users. Over a million requests per day, that is 50,000 slow requests. Those users notice. They complain. They churn. P95 is the threshold where user experience starts to degrade for a meaningful minority.

P99 latency is the 99th percentile, meaning 99% of requests are faster and 1% are slower. P99 tells you what the worst-case experience looks like for the unluckiest users. If your P99 is 9.8 seconds, one in 100 users waits nearly 10 seconds. That user may be your most important customer, a power user, or a reviewer. You cannot dismiss P99 as acceptable loss. One percent of requests is a large absolute number at scale, and the users who hit P99 latency are disproportionately vocal. P99 is also an early indicator of systemic problems. If P99 starts climbing, P95 and P50 will follow.

You track all three percentiles continuously. You plot them on the same time-series graph. You watch for divergence. If P50 stays flat but P95 and P99 increase, you have a tail latency problem. If all three percentiles increase together, you have a systemic latency problem. If P50 increases but P95 and P99 stay flat, you have a shift in the median, which usually means the model or infrastructure changed.

A legal technology platform in early 2026 tracked P50, P95, and P99 latency for contract generation. Baseline performance was P50 at 2.1 seconds, P95 at 3.8 seconds, P99 at 6.2 seconds. Over a two-week period, P50 stayed at 2.1 seconds, but P95 climbed to 5.4 seconds and P99 climbed to 11.7 seconds. The median user experience was unchanged. The tail user experience degraded by 42% at P95 and 89% at P99. The team investigated and found that a subset of requests, those involving contracts with more than 12 clauses, were triggering a slow path in their preprocessing logic. The slow path accounted for 6% of requests, which matched the gap between P95 and P50. They fixed the preprocessing bottleneck, and P95 dropped back to 3.9 seconds, P99 to 6.4 seconds.

You also track percentiles by user cohort, task type, and input characteristics. A system with good aggregate latency may have terrible latency for a specific user segment. Enterprise customers may experience worse latency than free-tier users if they send longer inputs. Mobile users may experience worse latency than desktop users if they have slower network connections. International users may experience worse latency than domestic users if the model provider has regional capacity differences. You segment your latency metrics to detect these patterns. If P95 for enterprise users is 6.8 seconds while P95 for free users is 2.9 seconds, you have a problem that aggregate metrics hide.

## Setting Up Latency Dashboards

A latency dashboard is the real-time view into your system's responsiveness. It displays current latency percentiles, historical trends, and breakdowns by component, task type, and user cohort. You build it on top of your time-series metrics database. You update it every minute. You display it on monitors in the engineering area. You review it in daily standups. It is the first place you look when users report slowness.

The dashboard has four sections: current status, historical trends, breakdowns, and alerts. Current status shows P50, P95, and P99 for the past five minutes, color-coded green if within SLO, yellow if approaching threshold, red if exceeding threshold. Historical trends show latency percentiles over the past 24 hours, 7 days, and 30 days, with markers for deployments, model changes, and incidents. Breakdowns show latency by task type, model, user cohort, and geographic region, so you can identify which segment is experiencing degradation. Alerts show active latency anomalies, SLO violations, and recent incidents, with links to incident postmortems.

A productivity tool company in mid-2025 built a latency dashboard with six panels: current P50/P95/P99 for all requests, latency by task type, latency by model, latency by region, TTFT vs total response time scatterplot, and per-component latency breakdown. The dashboard updated every 60 seconds. They set SLOs at P95 below 2.5 seconds and P99 below 4 seconds. The dashboard showed violations in red. When P95 spiked to 3.9 seconds for the document summarization task, the team saw it within 90 seconds, drilled into the latency-by-model panel, and found that the spike was isolated to GPT-5.1. They checked the provider status page, saw a reported incident, and temporarily routed summarization requests to Claude Sonnet 4.5. P95 dropped back to 2.2 seconds. The dashboard enabled sub-five-minute response to a latency incident.

You also build user-facing latency indicators. For internal tools, you display current P95 latency on the admin dashboard so product and support teams can see whether the system is healthy. For customer-facing products, you publish latency percentiles on a public status page, so customers know what to expect. Transparency builds trust. If users know that P95 latency is currently 3.1 seconds and climbing, they understand why the system feels slow, and they are less likely to assume it is broken.

The dashboard is not just for incident response. It is for capacity planning. If you see P95 latency trending upward over weeks, you know you are approaching a capacity limit. You can add infrastructure, switch to a faster model, or reduce traffic before you hit an SLO violation. If you see latency spikes every Monday at 9 AM, you can pre-scale capacity before the spike. The dashboard turns reactive firefighting into proactive operations.

## Anomaly Detection for Latency Spikes

Anomaly detection is automated monitoring that alerts you when latency deviates from normal patterns, before users complain. You define normal based on historical data, set thresholds for acceptable deviation, and trigger alerts when thresholds are crossed. Anomaly detection catches problems that static thresholds miss, such as gradual degradation or unexpected spikes during low-traffic periods.

The simplest anomaly detection is threshold-based alerting. You set fixed thresholds for P95 and P99 latency, such as 3 seconds for P95 and 5 seconds for P99. When either percentile exceeds the threshold, you trigger an alert. This works if your latency is stable and your SLO is well-defined. But it misses relative changes. If your baseline P95 is 2 seconds and it jumps to 2.8 seconds, that is a 40% increase and probably indicates a problem, but it does not cross a 3-second threshold. Threshold-based alerts would not fire.

Better anomaly detection uses statistical models. You calculate the mean and standard deviation of P95 latency over the past seven days, excluding known incidents. You define an anomaly as any value more than three standard deviations above the mean. This adapts to your baseline. If your typical P95 is 2.1 seconds with a standard deviation of 0.3 seconds, an anomaly is anything above 3 seconds. If your typical P95 is 4.5 seconds with a standard deviation of 0.7 seconds, an anomaly is anything above 6.6 seconds. The threshold adjusts to your system's normal behavior.

More sophisticated detection uses time-series forecasting. You train a forecasting model, such as ARIMA or Prophet, on historical latency data. The model predicts expected latency for the next hour based on time of day, day of week, and recent trends. You compare actual latency to the forecast. If actual latency exceeds the upper confidence bound of the forecast, you trigger an alert. This catches anomalies that are contextually unusual even if they are not statistically extreme. A P95 of 3.5 seconds might be normal at 2 PM on a weekday but anomalous at 2 AM on a Sunday.

A financial services company in late 2025 used Prophet to forecast P95 latency for transaction risk analysis. The model learned that P95 was typically 2.8 seconds at 10 AM and 1.9 seconds at 3 AM. When P95 hit 3.2 seconds at 3 AM, the threshold-based alert did not fire because 3.2 seconds was below the 4-second threshold. But the forecasting model flagged it as an anomaly because it was 68% above the expected 1.9 seconds for that time. The team investigated and found that a batch job had started at 2:45 AM, competing for API quota and slowing interactive requests. They moved the batch job to a separate API key with isolated quota, and the anomaly resolved.

You also detect anomalies in latency distribution shape. A normal distribution has most requests near the median and a thin tail. An anomaly might be a bimodal distribution, where requests cluster at two different latencies, indicating that some requests are hitting a slow path. You monitor the ratio of P99 to P50. If the ratio is stable at 2.5x for weeks and suddenly jumps to 5x, you have a tail latency anomaly even if P99 is still below your absolute threshold. The shape change is the signal.

Anomaly detection alerts go to on-call engineers via PagerDuty, Opsgenie, or a similar incident management system. The alert includes the current percentile values, the baseline values, the deviation magnitude, and a link to the latency dashboard for investigation. The alert is actionable. It does not just say latency is high. It says P95 latency is 4.2 seconds, 58% above baseline, affecting document generation tasks, investigate provider capacity or retry logic.

## Alerting Thresholds and SLO Enforcement

Service level objectives, abbreviated SLOs, are the promises you make about system performance. For latency, an SLO might be P95 below 2 seconds and P99 below 3.5 seconds for 99.5% of rolling 30-day windows. SLOs are not aspirations. They are contracts. If you violate an SLO, you have failed to deliver the experience you promised. Alerting thresholds enforce SLOs by notifying you when you are at risk of violation, so you can intervene before you break the contract.

You set alerting thresholds at two levels: warning and critical. The warning threshold is 80% of your SLO. If your SLO is P95 below 2.5 seconds, the warning threshold is 2 seconds. If P95 exceeds 2 seconds, you get a warning alert. You investigate, but you do not page on-call engineers at 3 AM. The critical threshold is 100% of your SLO. If P95 exceeds 2.5 seconds, you get a critical alert. You page on-call. You treat it as a production incident. The two-level system gives you time to fix problems before they become SLO violations.

You also track SLO burn rate, which is how quickly you are consuming your error budget. If your SLO allows 0.5% of requests to exceed the latency threshold, that is your error budget. If you are burning 0.1% per day, you will stay within budget for the month. If you are burning 0.8% per day, you will exhaust your budget in four days and violate your SLO. Burn rate alerts fire when the rate exceeds a safe threshold, such as 2x the steady-state rate. This catches slow degradation that does not trigger absolute threshold alerts.

A legal technology company in early 2026 set an SLO of P95 below 3 seconds for 99.9% of requests over a rolling 30-day window. They set a warning threshold at 2.4 seconds and a critical threshold at 3 seconds. They tracked burn rate and alerted if more than 0.2% of requests in any four-hour window exceeded 3 seconds, because 0.2% per four hours would exhaust the 0.1% monthly budget in 12 days. One week, P95 climbed from 2.1 seconds to 2.6 seconds over three days. The warning alert fired. The team investigated and found that a new model version had been deployed with slower inference time. They rolled back the model version, and P95 dropped to 2.2 seconds. The early warning prevented an SLO violation.

You publish SLO compliance metrics on your internal dashboard and, if appropriate, on a public status page. Transparency holds you accountable. If you promise P95 below 2.5 seconds and you deliver P95 at 2.8 seconds for a week, your users and stakeholders see it. You owe them an explanation and a fix. Publishing compliance metrics also builds trust when you meet your SLOs consistently. Users see that you deliver what you promise.

## Provider-Side Latency Variations

Model providers do not deliver consistent latency. Their performance varies by time of day, region, capacity, version, and operational incidents. You cannot control provider-side latency, but you can measure it, anticipate it, and mitigate it. Provider latency variation is one of the largest sources of production latency unpredictability.

Time of day effects are common. Providers experience peak load during business hours in their primary markets, typically North America and Europe. If you are using OpenAI's API, you may see slower responses between 9 AM and 5 PM Pacific time compared to overnight hours. If you are using Anthropic's API, you may see similar patterns. The magnitude of the variation can be 20% to 50% at P95. A model that responds in 1.8 seconds at 4 AM may respond in 2.7 seconds at 11 AM. You measure this by tracking latency by hour of day over weeks and plotting the distributions. If you see consistent patterns, you can plan for them by pre-scaling your retry logic, switching to faster models during peak hours, or batching non-urgent requests for off-peak processing.

Regional differences also matter. If you serve global users, requests from Europe may hit a European API endpoint with different capacity and latency characteristics than the North American endpoint. A request from Tokyo routed to a US-based API endpoint will have higher network latency than a request from California. Some providers offer regional endpoints. Some do not. You measure latency by user geography to detect regional disparities. If users in Asia-Pacific experience P95 latency 80% higher than users in North America, you either accept the disparity, choose a provider with better APAC infrastructure, or deploy regional inference infrastructure.

Provider capacity events are unpredictable. Providers occasionally experience partial outages, capacity constraints, or throttling that affects latency without causing total failures. The provider's status page may not report these events if uptime is not affected. You detect them by monitoring for sudden latency spikes that correlate across all your requests to that provider. A 3x jump in P95 over 10 minutes that affects all task types is almost always a provider-side issue. You mitigate by routing traffic to an alternative model or provider until the incident resolves.

A healthcare documentation platform in mid-2025 saw P95 latency for clinical note summarization spike from 2.3 seconds to 7.1 seconds over a 15-minute window. The spike affected all users, all regions, and all task types. They checked their own infrastructure and found no anomalies. They checked the model provider's status page and saw no reported incidents. They switched traffic to a backup model from a different provider. P95 dropped to 2.8 seconds. Two hours later, the original provider published a post-incident report acknowledging a regional capacity issue that caused elevated latency for 90 minutes. The platform's monitoring and failover prevented prolonged user impact.

You also monitor model version changes. Providers update models regularly, sometimes without notice. A new version may be faster or slower than the previous version. If you see a sudden latency shift that coincides with no changes on your side, it is likely a model version change. You contact the provider to confirm. You decide whether to accept the new latency profile or switch models. You cannot prevent provider changes, but you can detect and respond to them.

## Correlating Latency with Quality and Error Rates

Latency does not exist in isolation. It correlates with output quality and error rates. Slower requests often involve longer inputs, more complex reasoning, or retry logic after failures. Faster requests often involve simple queries or cached results. You must correlate latency with other metrics to understand whether latency spikes indicate problems or just harder tasks.

The first correlation to check is latency versus input length. Longer inputs take longer to process. If your P95 latency increases 40% over a week, and your average input token count also increases 40%, the latency change is explained by input complexity, not by system degradation. You control for input length by measuring latency per token or by segmenting latency metrics by input length bucket. If P95 latency for inputs under 500 tokens stays flat but P95 for inputs over 1500 tokens increases, you know the problem is specific to long inputs.

The second correlation is latency versus error rate. Requests that fail and retry take longer than requests that succeed on the first attempt. If your error rate increases from 2% to 8%, your median latency may stay flat, but your P95 latency may spike because the retried requests push up the tail. You plot error rate and P95 latency on the same graph. If they move together, retries are driving latency. You investigate why errors increased and fix the root cause.

The third correlation is latency versus output quality. In some systems, slower responses are higher quality because the model is doing more reasoning or generating longer, more complete outputs. In other systems, slower responses are lower quality because the model is struggling with a hard input and eventually timing out or producing a poor answer. You measure quality metrics, such as review pass rate or user satisfaction, alongside latency. If P95 latency increases and quality decreases, you have a problem. If P95 latency increases and quality increases, you may have a beneficial shift toward more thorough outputs.

A customer support platform in late 2025 tracked P95 latency and escalation rate together. Over two weeks, P95 latency increased from 2.4 seconds to 3.1 seconds, a 29% increase. Escalation rate increased from 5.8% to 7.2%, a 24% increase. The team hypothesized that the model was struggling with harder queries, taking longer and producing worse outputs. They segmented requests by intent complexity. Simple FAQs had stable latency and escalation rate. Complex troubleshooting queries had increased latency and escalation. The model was working harder on harder tasks and failing more often. The team routed complex queries to a more capable model, and both latency and escalation rate improved.

You build correlation views into your dashboard. You plot latency versus error rate, latency versus input length, and latency versus quality metrics on time-series graphs with dual y-axes. You look for co-movement. You set up alerts for divergences, such as latency increasing while quality decreases, which almost always indicates a problem.

## Latency SLOs and How to Enforce Them

Latency SLOs are commitments to your users about how fast the system will respond. A well-defined latency SLO specifies the percentile, the threshold, and the compliance target. For example: P95 latency will be below 2.5 seconds for 99.5% of requests over any rolling 30-day window. This SLO is measurable, time-bound, and specific. You can verify compliance by querying your metrics database.

Enforcing an SLO means monitoring compliance continuously, alerting on violations, investigating root causes, and fixing problems before you exhaust your error budget. You do not wait until the end of the month to check whether you met the SLO. You check daily. You track how much error budget you have remaining. You project whether you will stay within budget based on current burn rate. If you are on track to violate the SLO, you take action: optimize the slow path, switch to a faster model, add capacity, or reduce traffic to non-critical tasks.

A legal research platform in early 2026 committed to P95 latency below 3 seconds for 99.9% of requests. They tracked compliance daily. On day 12 of a 30-day window, they had consumed 0.06% of their 0.1% error budget, leaving 0.04% for the remaining 18 days. At current burn rate, they would stay within budget. On day 15, burn rate doubled due to a provider latency issue. They projected that they would exhaust the budget by day 23. They implemented emergency mitigations: they cached frequent queries, they routed complex requests to a secondary provider, and they batched low-priority requests for off-peak processing. Burn rate dropped, and they finished the month at 0.09% error consumption, meeting the SLO.

You also enforce SLOs by designing your system to degrade gracefully. If you cannot meet your latency SLO for all requests, you prioritize high-value users or high-stakes tasks. You route paid users to fast infrastructure and free users to slower infrastructure. You route safety-critical queries to high-priority queues and background analytics to low-priority queues. You enforce SLOs where they matter most and accept violations where they matter least.

SLO enforcement is not punitive. It is a feedback loop. When you violate an SLO, you run a postmortem, identify the root cause, implement a fix, and update your monitoring to detect similar issues earlier. Over time, your system becomes more resilient. Your SLO compliance improves. Your users trust that the system will perform as promised.

Latency monitoring is not a dashboard you check when users complain. It is the continuous heartbeat of your production system. You measure every request, track every percentile, detect every anomaly, and enforce every SLO. You correlate latency with quality, errors, and input characteristics. You respond to degradation within minutes, not hours. You publish compliance metrics transparently. You hold yourself accountable to your promises. This is how professional teams operate production AI systems. The next subchapter covers model capability benchmarks and how to interpret them for your specific use cases, so you can choose models based on what they actually do, not what vendors claim they do.
