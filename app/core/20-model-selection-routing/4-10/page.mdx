# 4.10 — The Diminishing Returns Curve: When a Better Model Is Not Worth the Price

In late 2025, a legal technology company serving 4,500 attorneys deployed contract review automation using Claude Opus 4.5, the most capable model available at the time. The feature flagged risky clauses, suggested revisions, and summarized obligations across 40-page agreements. Accuracy was excellent: 94% precision, 91% recall on a validation set of 2,000 contracts. The product team was thrilled. The cost was $0.18 per contract review. At 200,000 reviews per month, that was $36,000 in inference costs alone. Six months later, Anthropic released Claude Opus 4.5 Turbo, a faster variant with 98% of Opus 4.5's capability at 40% of the cost. The team tested it. Precision dropped to 92%, recall to 89%. The product manager demanded they stay on Opus 4.5. "We can't accept lower accuracy," she said. Engineering pushed back. The cost difference was $21,600 per month. The accuracy difference affected fewer than 3% of contracts, and those could be caught in the attorney review stage that happened anyway. After two weeks of debate, they ran a live A/B test with real attorneys. Attorneys flagged issues at the same rate in both cohorts. The 2% precision drop didn't translate to user-facing failures. The team switched to Opus 4.5 Turbo and saved $259,000 annually. The lesson wasn't that Opus 4.5 was bad. It was that the incremental accuracy gain from Opus 4.5 over Opus 4.5 Turbo didn't justify a 150% cost increase for this specific task. They were paying for capability they didn't need.

This subchapter covers the relationship between model capability and cost, the diminishing returns curve that defines when incremental improvements stop being worth the price, how to measure where you are on that curve for your specific tasks, and how to build decision frameworks that help teams choose the right model tier without over-spending or under-delivering.

## The Non-Linear Relationship Between Model Capability and Cost

Model capability and cost do not scale linearly. Doubling the cost of a model does not double its performance. Moving from a mid-tier model to a frontier model often increases cost by 5x to 10x while improving task performance by 10% to 25%. This non-linearity creates a strategic problem: at some point, the marginal improvement in quality is not worth the marginal increase in cost.

Consider a concrete example from early 2026 pricing. GPT-5-mini costs $0.02 per million input tokens and $0.06 per million output tokens. GPT-5 costs $0.30 per million input tokens and $0.90 per million output tokens. GPT-5 is 15 times more expensive than GPT-5-mini. For tasks where GPT-5 scores 95% accuracy and GPT-5-mini scores 88%, you're paying 15 times more for a 7 percentage point improvement. Whether that's worth it depends entirely on the task, the cost tolerance, and the business value of the accuracy delta.

For high-stakes tasks—medical diagnosis support, legal contract analysis, financial fraud detection—7 percentage points might be worth 15 times the cost because the cost of a single error exceeds the entire inference budget. For low-stakes tasks—email categorization, sentiment tagging, informal summarization—7 percentage points might not be worth even 2 times the cost because errors are cheap to correct or ignore. The same models, the same accuracy delta, the same cost ratio—completely different conclusions based on task context.

The non-linearity becomes more extreme at the high end. Moving from GPT-5 to GPT-5.2, the latest frontier variant, might cost 20% more for a 3% accuracy improvement. Moving from Claude Sonnet 4.5 to Claude Opus 4.5 might cost 400% more for a 12% improvement. The improvements compress as models approach theoretical performance limits. Going from 70% to 85% accuracy is easier than going from 90% to 95%, and far easier than going from 95% to 97%. The final percentage points cost exponentially more in both training compute and inference cost.

This non-linearity defines the **diminishing returns curve**: a plot of cost on the x-axis and task performance on the y-axis. Early on the curve, small cost increases yield large performance gains. Late on the curve, large cost increases yield small performance gains. Your job is to identify where on this curve your task sits, and whether you're operating in the region of steep gains or flat returns. Most teams operate too far right—paying for flat returns—because they default to the best available model without measuring whether cheaper alternatives would suffice.

## The Diminishing Returns Curve: Measuring Cost-Quality Tradeoffs

The diminishing returns curve is task-specific. You cannot plot a single curve for "GPT-5 versus GPT-5-mini" and apply it to all tasks. You must measure the curve for each task or task class you operate. This requires running evaluations across multiple model tiers and comparing performance against cost.

Start by selecting three to five candidate models spanning a cost range: a cheap model like GPT-5-nano, a mid-tier model like GPT-5-mini or Claude Sonnet 4.5, a high-tier model like GPT-5 or Claude Opus 4.5, and optionally a frontier model like GPT-5.2 or Gemini 3 Deep Think. Run each model on the same evaluation set—preferably the same set you use for baseline validation—and measure accuracy, precision, recall, or whatever metric defines success for your task. Record the cost per evaluation instance based on token usage and model pricing. Plot the results: x-axis is cost per instance, y-axis is task performance.

You now have a scatter plot with one point per model. Draw a curve through the points. On most tasks, this curve will be steep at the low end and flat at the high end. The cheap model scores 70% at $0.001 per instance. The mid-tier model scores 85% at $0.005 per instance. The high-tier model scores 91% at $0.02 per instance. The frontier model scores 93% at $0.05 per instance. The first step from cheap to mid-tier costs 5 times more and gains 15 percentage points. The second step from mid-tier to high-tier costs 4 times more and gains 6 percentage points. The third step from high-tier to frontier costs 2.5 times more and gains 2 percentage points. Diminishing returns.

Now overlay your success threshold. If your task requires 90% accuracy to meet business requirements, the high-tier model is the minimum viable choice. If your task requires 85%, the mid-tier model suffices. If your task tolerates 70%, the cheap model is fine. The curve tells you the minimum cost required to meet your requirement. Anything beyond that is over-spending.

You also overlay your cost tolerance. If your cost budget is $0.01 per instance, the high-tier model is affordable but the frontier model is not, even if the frontier model would improve quality. Cost tolerance acts as a vertical line on the curve: everything to the right of that line is out of bounds. The optimal model is the one that maximizes performance while staying left of your cost line and above your quality line.

This analysis takes two to four hours per task: running five models on a 500-instance eval set, computing metrics, recording costs, and plotting results. You do this once per task during initial model selection, and you repeat it quarterly as new models launch and pricing changes. The curve is not static. When Anthropic releases Claude Opus 4.6 or OpenAI releases GPT-5.3, the curve shifts. Maybe the new model delivers GPT-5 performance at GPT-5-mini cost. Maybe it delivers GPT-5.2 performance at GPT-5 cost. You re-run the analysis and update your model selection accordingly.

Teams that skip this analysis default to the most expensive model because it's the safest perceived choice. Teams that run this analysis often discover they can drop two tiers and save 80% of cost while losing only 5% of accuracy—a tradeoff every rational product manager would accept if presented with the data.

## The Good Enough Threshold: Defining Acceptable Performance

The **good enough threshold** is the minimum level of performance that satisfies your success criteria. It's the floor below which the feature fails to deliver value, and above which incremental improvements are nice-to-have but not required. Defining this threshold is a product and business decision, not a technical one. Engineering can measure performance. Product must decide what performance level is acceptable.

For a customer support chatbot answering FAQ questions, good enough might be 80% accuracy with a fallback to human agents for the other 20%. For a medical coding assistant suggesting diagnosis codes to billing specialists, good enough might be 95% accuracy because errors create compliance risk and rework cost. For an internal document search tool, good enough might be 70% recall because employees can iterate on queries and don't expect perfect results. The threshold depends on user tolerance for errors, the cost of errors, the availability of fallback mechanisms, and the competitive landscape.

You establish the good enough threshold by working backward from user impact. Ask: what happens when the model is wrong? If the user can easily ignore or correct the error, the threshold is low. If the error creates financial loss, legal liability, or safety risk, the threshold is high. If the error is invisible to the user because a human reviews all outputs, the threshold might be moderate because the human catches mistakes.

You validate the threshold with user research. Show users outputs at different quality levels—70%, 85%, 95%—and ask which is acceptable. Users often tolerate lower quality than product teams expect, especially if the alternative is no automation at all. A chatbot that answers 75% of questions correctly is better than no chatbot, even if it's not as good as a 95% chatbot. Users adjust their expectations based on context. They tolerate errors in informal tools and demand perfection in high-stakes tools.

Once you define the good enough threshold, you measure all candidate models against it. Models that fall below the threshold are eliminated regardless of cost. Models that exceed the threshold are ranked by cost. The cheapest model that clears the threshold is your default choice. You only choose a more expensive model if there's a documented reason: competitive differentiation, user preference, regulatory requirement, or stakeholder mandate.

The good enough threshold is not a one-time decision. You revisit it as user expectations evolve, as competitors raise the bar, and as models improve. What was good enough in 2024 might not be good enough in 2026. You track user satisfaction, error escalation rates, and competitive benchmarks, and you adjust the threshold accordingly. The threshold is a living target, not a static spec.

## When Frontier Models Add Marginal Value Over Mid-Tier Models

Frontier models—GPT-5.2, Claude Opus 4.5, Gemini 3 Deep Think, Llama 4 Maverick—represent the state of the art. They perform best on benchmarks, handle the most complex reasoning, and support the longest context windows. They also cost 5 to 20 times more than mid-tier models. For many tasks, the performance advantage is marginal. For some tasks, there's no advantage at all.

Frontier models add significant value on tasks that require deep reasoning, multi-step logic, ambiguity resolution, or novel problem-solving. Examples include complex legal analysis, research synthesis, strategic planning, technical debugging, and creative writing. On these tasks, frontier models outperform mid-tier models by 20 to 40 percentage points. The cost premium is justified because mid-tier models simply cannot complete the task at acceptable quality.

Frontier models add marginal value on tasks that are well-structured, repetitive, and within the training distribution of mid-tier models. Examples include entity extraction, classification, summarization of standard documents, sentiment analysis, and FAQ answering. On these tasks, frontier models outperform mid-tier models by 3 to 8 percentage points. The cost premium is harder to justify because mid-tier models already deliver 85% to 92% accuracy, which often exceeds the good enough threshold.

Frontier models add zero value on tasks where performance is bottlenecked by input quality, not model capability. If your task fails because users provide ambiguous queries or incomplete data, upgrading from GPT-5 to GPT-5.2 won't help. The model can't reason about information that isn't there. You're better off investing in prompt engineering, input validation, or user education than in a more expensive model.

You determine whether a frontier model adds value by running the diminishing returns analysis described earlier. If the frontier model scores 94% and the mid-tier model scores 91%, and your threshold is 90%, the frontier model adds 3 percentage points of margin above the threshold. Is that margin worth the cost? It might be if errors are catastrophic. It's probably not if errors are minor.

A common mistake is assuming frontier models are always better. They're better on benchmarks, which are designed to test the limits of model capability. They're not always better on your specific task, especially if your task is simpler than the benchmarks. A mid-tier model fine-tuned on your data often outperforms a frontier model with zero-shot prompts. A frontier model with poor prompts often underperforms a mid-tier model with optimized prompts. Model tier is one variable. Prompt quality, fine-tuning, context design, and input quality are equally important. You optimize the whole system, not just the model.

## Task-Specific Diminishing Returns Analysis: A Case Study

Consider a customer support ticket routing task. Incoming tickets are classified into one of 12 categories: billing, technical support, account management, feature requests, bug reports, security, compliance, sales, partnerships, refunds, cancellations, and other. The classification determines which team receives the ticket. Misclassification delays resolution and frustrates customers. The current model is GPT-5, which achieves 96% accuracy on a validation set of 10,000 tickets. The cost is $0.008 per ticket. At 500,000 tickets per month, that's $4,000 in inference costs.

The team runs a diminishing returns analysis. They test five models: GPT-5-nano, GPT-5-mini, Claude Sonnet 4.5, GPT-5, and Claude Opus 4.5. Results:

GPT-5-nano: 78% accuracy, $0.0005 per ticket, $250 per month for 500,000 tickets.

GPT-5-mini: 91% accuracy, $0.002 per ticket, $1,000 per month.

Claude Sonnet 4.5: 94% accuracy, $0.004 per ticket, $2,000 per month.

GPT-5: 96% accuracy, $0.008 per ticket, $4,000 per month.

Claude Opus 4.5: 97% accuracy, $0.015 per ticket, $7,500 per month.

The team plots the curve. The first jump from GPT-5-nano to GPT-5-mini costs 4 times more and gains 13 percentage points. The second jump from GPT-5-mini to Sonnet 4.5 costs 2 times more and gains 3 percentage points. The third jump from Sonnet 4.5 to GPT-5 costs 2 times more and gains 2 percentage points. The fourth jump from GPT-5 to Opus 4.5 costs 1.875 times more and gains 1 percentage point.

The team defines the good enough threshold at 92% accuracy, based on user research showing that 8% misclassification is tolerable given that agents can manually reroute tickets. Claude Sonnet 4.5 at 94% exceeds the threshold. It costs half as much as GPT-5 and delivers 98% of the accuracy. The team switches to Sonnet 4.5 and saves $24,000 annually.

Six months later, Anthropic releases Claude Sonnet 5, which delivers 96% accuracy at $0.003 per ticket, undercutting GPT-5 on both cost and performance. The team switches again. This iterative process—measure, compare, switch—runs quarterly as new models launch and pricing evolves.

The case study illustrates three principles. First, the optimal model is task-specific and time-specific. Second, the current model is often over-specified relative to the good enough threshold. Third, regularly re-running the analysis captures value from model evolution and pricing competition.

## The Cost-Quality Pareto Frontier

The **Pareto frontier** is the set of models where no other model is both cheaper and better. If model A costs less than model B and performs better, model B is dominated and should never be chosen. The Pareto frontier is the subset of models that represent optimal tradeoffs between cost and quality.

You construct the Pareto frontier by plotting all candidate models on a cost-quality graph and identifying the ones that form the upper-left boundary: lowest cost for a given quality level, or highest quality for a given cost level. Models that fall below and to the right of this boundary are dominated and eliminated from consideration.

For example, suppose you evaluate six models on a summarization task:

Model A: 80% quality, $0.001 per summary.

Model B: 85% quality, $0.003 per summary.

Model C: 83% quality, $0.004 per summary.

Model D: 90% quality, $0.008 per summary.

Model E: 92% quality, $0.015 per summary.

Model F: 91% quality, $0.020 per summary.

Models A, B, D, and E form the Pareto frontier. Model C is dominated by Model B: B is cheaper and better. Model F is dominated by Model E: E is cheaper and better. You eliminate C and F from consideration. Your choice is now between A, B, D, and E, based on your quality threshold and cost tolerance.

The Pareto frontier simplifies decision-making. Instead of comparing six models, you compare four. Instead of arbitrary debates about which model is "best," you frame the decision as a cost-quality tradeoff with clear data. Product and finance can see the options, see the tradeoffs, and make an informed choice based on business priorities.

You update the Pareto frontier quarterly as new models launch. When a new model enters the market, you test it and plot it. If it's on the frontier, it becomes a candidate. If it's dominated, you ignore it. The frontier evolves as the model ecosystem evolves. What was optimal in January 2026 might be suboptimal by April 2026. You stay on the frontier by re-evaluating regularly.

## How to Present Diminishing Returns Data to Stakeholders Who Want the Best Model

Product managers, executives, and domain experts often default to wanting "the best model." They equate best with most expensive or most capable. They resist choosing a cheaper model because it feels like compromising on quality. Your job is to reframe the conversation from "best model" to "best model for this task at this cost."

You present diminishing returns data in three steps. First, show the curve. Plot cost versus quality for all candidate models. Label the axes clearly: dollars per instance on the x-axis, accuracy percentage on the y-axis. Show the current model as a highlighted point. Show the Pareto frontier as a line connecting optimal models. This visualization makes the tradeoff concrete and undeniable.

Second, show the marginal cost and marginal benefit. Calculate the cost and quality delta between adjacent models. "Moving from Sonnet 4.5 to GPT-5 costs an additional $2,000 per month and improves accuracy by 2 percentage points. That's $1,000 per percentage point." Frame the decision as a unit economics question: how much are you willing to pay for one additional percentage point? This framing shifts the conversation from "should we use the best model?" to "is this marginal improvement worth this marginal cost?"

Third, anchor to the good enough threshold. Show where the threshold sits on the curve. Highlight all models that meet or exceed the threshold. Explain that any model above the threshold is viable, and the question is how much additional quality beyond the threshold you want to pay for. If the threshold is 90% and the mid-tier model delivers 92%, the frontier model at 95% is delivering 3 percentage points of "luxury performance." Is luxury worth 5 times the cost? Sometimes yes, usually no.

When stakeholders push back and insist on the most expensive model, ask them to quantify the value of the quality delta. "If we use GPT-5 instead of Sonnet 4.5, we gain 2 percentage points of accuracy. What is the business value of that 2%? Does it increase revenue, reduce churn, improve NPS, reduce support load?" If they can quantify it and the value exceeds the cost, use the more expensive model. If they can't quantify it, default to the cheaper model and revisit if user feedback indicates quality is insufficient.

You also present data on model evolution. Show stakeholders that model pricing and performance change quarterly. "If we choose GPT-5 today, we'll re-evaluate in three months when new models launch. We're not locked in forever. We're making the best decision for today's landscape and committing to continuous optimization." This reduces the perceived risk of choosing a cheaper model. It's not a permanent compromise. It's a temporary optimization that you'll revisit as the landscape evolves.

## Decision Framework: When to Upgrade Versus When to Stay

You need a formal decision framework that tells teams when upgrading to a more expensive model is justified and when staying with the current model is correct. This framework prevents both under-investment—staying on a cheap model that doesn't meet quality requirements—and over-investment—upgrading to an expensive model that delivers marginal value.

The framework has four decision gates. Gate one: does the current model meet the good enough threshold? If yes, proceed to gate two. If no, upgrade is mandatory. Gate two: does a cheaper model also meet the good enough threshold? If yes, downgrade to the cheaper model. If no, proceed to gate three. Gate three: does upgrading improve quality beyond the threshold by more than 5 percentage points? If yes, evaluate gate four. If no, stay with the current model. Gate four: does the quality improvement justify the cost increase based on business value? If yes, upgrade. If no, stay.

Gate four requires a cost-benefit calculation. You estimate the value of the quality improvement—reduced support costs, increased conversion, improved user satisfaction—and compare it to the inference cost increase. If the value exceeds the cost by at least 2x, upgrade. If not, stay. The 2x threshold accounts for uncertainty and opportunity cost. Marginal improvements that barely cover their cost are not worth the migration effort and ongoing monitoring.

You document this framework in a decision matrix and publish it to all product and engineering teams. Teams apply the framework whenever they consider changing models, whether that's upgrading, downgrading, or switching providers. The framework is not a rule that requires approval. It's a guide that ensures consistency and rigor. Teams can override the framework with documented justification, but they can't ignore it.

The framework also includes a trigger for re-evaluation. You re-run the diminishing returns analysis and re-apply the decision gates every quarter, or when any of the following events occur: a new model launches from a major provider, pricing changes by more than 20%, task requirements change, user feedback indicates quality issues, or costs exceed budget by more than 10%. These triggers ensure you don't ossify on a suboptimal model just because it was optimal six months ago.

The framework is not bureaucracy. It's a forcing function for evidence-based decision-making. It prevents teams from upgrading to the latest model just because it's new, and it prevents them from staying on an outdated model just because migration is inconvenient. It aligns model selection with business outcomes, not engineering fashion or product politics.

## The Long-Term Strategy: Optimize for the Curve, Not the Model

Model selection is not a one-time decision. It's a continuous optimization process. The models available today will be obsolete in 18 months. The pricing structures will change. The performance benchmarks will shift. Your task requirements will evolve. The optimal model for your task in January 2026 will not be optimal in July 2026 or January 2027.

Your long-term strategy is to build processes and infrastructure that let you ride the diminishing returns curve as it shifts. This means instrumenting your systems to measure cost and quality continuously, running quarterly model evaluations, maintaining vendor-agnostic abstractions so you can switch models without rewriting code, and training your teams to think in terms of cost-quality tradeoffs rather than model brand loyalty.

You invest in tooling that makes model comparison cheap and fast. Evaluation harnesses that run the same test set across five models in parallel. Dashboards that show cost per task and quality per task in real time. A/B testing frameworks that let you shadow-traffic new models against production workloads. These tools reduce the friction of re-evaluation from weeks to hours, which makes continuous optimization feasible.

You also invest in culture. You train product managers to ask "is this model still optimal?" every quarter. You train engineers to treat model selection as a configuration parameter, not an architectural commitment. You reward teams that downgrade to cheaper models without sacrificing quality, not just teams that adopt the latest models. You make cost efficiency a performance metric alongside feature velocity and system reliability.

The diminishing returns curve is the foundation of rational model selection. It replaces intuition with data, defaults with tradeoffs, and brand loyalty with business value. It prevents teams from overpaying for marginal improvements and underinvesting in critical quality thresholds. It's not the only factor in model selection—latency, context limits, compliance, and vendor relationships all matter—but it's the most neglected factor in organizations that default to the most expensive model and never question whether cheaper alternatives would suffice. Mastering the curve is what separates teams that spend $500,000 per month on AI from teams that spend $100,000 per month for the same business outcomes. The next step is applying these cost and quality tradeoffs to the broader problem of model routing, where you dynamically assign tasks to different model tiers based on real-time evaluation of difficulty and value, a strategy we explore in the chapters ahead.
