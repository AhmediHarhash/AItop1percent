# 10.4 â€” EU AI Act GPAI Obligations: What Model Selection Teams Must Do in 2026

In September 2025, a European SaaS company providing customer service automation tools received a formal inquiry from their national AI authority requesting documentation of their general-purpose AI model usage under the EU AI Act. Engineering assumed this applied only to companies training foundation models like OpenAI or Anthropic, not to companies using those models through APIs. They had no documentation prepared. The authority clarified: the company was a deployer of GPAI systems under Article 53 of the Act, and they were required to maintain technical documentation, conduct transparency assessments, and report incidents involving their deployed systems. The company spent four months reconstructing documentation covering which models they used, for what purposes, what safeguards they had implemented, and what risk assessments they had conducted. They faced a formal warning and a mandate to implement proper GPAI compliance processes within 60 days or face fines up to 15 million euros or 3% of global annual revenue. The root cause was a fundamental misunderstanding: the EU AI Act's General-Purpose AI provisions apply not just to model providers but to deployers, and if you are selecting and using foundation models in production systems serving EU users, you are a deployer.

## The EU AI Act GPAI Framework: What It Is and Why It Matters

The **EU AI Act** entered into force on August 1, 2024, with staggered implementation timelines through 2027. The provisions governing general-purpose AI models, known as the GPAI provisions, took effect February 2, 2025. These provisions create binding obligations for two categories of actors: **providers** of GPAI models (the organizations that train and release foundation models like GPT, Claude, Gemini, or Llama) and **deployers** of GPAI models (the organizations that integrate those models into products and services). If you are a model selection team choosing which foundation models to use in your production systems, you are a deployer, and you have legal obligations under the GPAI framework.

The **GPAI Code of Practice** was published by the European Commission on July 15, 2025, and provides detailed implementation guidance for both providers and deployers. This is not soft guidance. Article 56 of the EU AI Act specifies that adherence to the Code of Practice creates a presumption of conformity with GPAI obligations, meaning if you follow the Code, regulators will presume you are compliant. If you don't follow the Code, you must demonstrate alternative means of compliance, which is significantly harder. The Code was developed through a multi-stakeholder process involving industry, civil society, and national AI authorities, and it reflects the practical realities of deploying AI systems in 2026.

The **GPAI Q&A Guidance** was published by the European AI Office on September 30, 2025, answering 127 frequently asked questions about GPAI obligations. This guidance addresses ambiguities in the Act and the Code, clarifies enforcement approaches, and provides examples of compliance in specific sectors. For engineering teams, the Q&A is the most practically useful document because it translates abstract legal requirements into concrete actions.

The distinction between providers and deployers is the foundation of the GPAI framework. **Providers** are the organizations that train and make available general-purpose AI models. OpenAI is a provider for GPT models. Anthropic is a provider for Claude models. Google is a provider for Gemini models. Meta is a provider for Llama models. Providers have extensive obligations including transparency reporting, risk assessment, model evaluation, incident monitoring, and systemic risk evaluation for models with high-impact capabilities. These obligations are detailed in Articles 53 and 54 of the Act.

**Deployers** are the organizations that integrate GPAI models into AI systems that serve end users or make decisions. If you use GPT-4.5 in a customer service chatbot, you are a deployer. If you use Claude Opus 4.5 in a contract analysis tool, you are a deployer. If you use Gemini 3 in a content moderation system, you are a deployer. Deployers have transparency obligations, risk assessment obligations, incident reporting obligations, and technical documentation obligations. These obligations are detailed in Article 53(1)(b) and Article 53(3), and they apply regardless of whether you host the model yourself or access it through an API.

The critical insight for model selection teams is that your obligations as a deployer exist independently of whether your provider is compliant. You cannot outsource compliance to OpenAI or Anthropic. Even if your provider has perfect GPAI compliance, you still have your own obligations to document how you use the model, what safeguards you implement, what risks you assess, and what incidents you monitor. This is because the Act recognizes that the same model can be deployed safely in one context and unsafely in another, and deployers are responsible for safe deployment in their specific context.

## What Counts as a GPAI Model

A **general-purpose AI model** is defined in Article 3(63) of the Act as an AI model that displays significant generality, is capable of performing a wide range of distinct tasks, and is trained on large amounts of data. The definition includes both foundation models and fine-tuned versions of foundation models. GPT-4.5, Claude Opus 4.5, Gemini 3, Llama 4 Maverick, and other frontier models released in 2025 and 2026 all qualify as GPAI models. Fine-tuned versions of these models also qualify. If you fine-tune GPT-4.5 on your proprietary data, the resulting model is still a GPAI model, and you become both a deployer and potentially a provider depending on whether you make the fine-tuned model available to others.

Not every model is a GPAI model. Task-specific models trained from scratch for a single narrowly defined task do not qualify. If you train a BERT-based classifier for sentiment analysis on customer reviews, that is not a GPAI model because it is not general-purpose. If you train a custom neural network for fraud detection, that is not a GPAI model. The key distinction is generality and task range. GPAI models can perform many different tasks: text generation, summarization, translation, question answering, code generation, reasoning, and more. Task-specific models perform one task.

The **systemic risk threshold** defined in Article 51 creates a subcategory of GPAI models with particularly high impact potential. A GPAI model is classified as having systemic risk if it has high-impact capabilities, which the Act defines as capabilities that exceed those of the largest and most capable models currently on the market, or if it meets a computational threshold of 10^26 FLOPs (floating-point operations) used for training. As of January 2026, models like GPT-5.2, Claude Opus 4.5, and Gemini 3 Ultra are classified as systemic risk models. These models have additional obligations for providers including adversarial testing, model evaluation, cybersecurity measures, and reporting of serious incidents.

For deployers, the distinction between systemic risk models and other GPAI models affects your risk assessment and documentation obligations. If you deploy a systemic risk model, regulators expect more thorough risk assessment, more robust safeguards, and more detailed documentation because the model has higher capabilities and therefore higher potential for harm. This doesn't mean you can't use systemic risk models. It means you must justify why you need those capabilities, demonstrate that you've assessed the associated risks, and show that you've implemented appropriate mitigations.

## Obligations for Deployers: Transparency, Documentation, and Risk Assessment

Article 53(3) specifies deployer obligations. You must use GPAI models in accordance with the instructions for use provided by the provider. You must document how you integrate and use the model. You must conduct risk assessments appropriate to your use case. You must implement safeguards to prevent reasonably foreseeable misuse. And you must monitor for incidents and report serious incidents to your national AI authority.

The **instructions for use** requirement means you must review and follow the usage guidelines published by your model provider. OpenAI publishes usage policies for GPT models. Anthropic publishes acceptable use policies for Claude models. Google publishes responsible AI practices for Gemini models. These documents specify prohibited uses, required safeguards, and recommended practices. If your use case violates the instructions for use, you are non-compliant, period. You either change your use case or choose a different model.

Some teams assume the instructions for use are just legal boilerplate. They are not. Regulators treat violations of instructions for use as evidence of deployer negligence. In the Q&A Guidance, the European AI Office clarifies that if a deployer uses a GPAI model in a manner explicitly prohibited by the provider's instructions, and harm results, the deployer bears full liability regardless of provider compliance. This makes reviewing instructions for use a critical step in model selection. Before you select a model, you confirm your use case is permitted under the provider's instructions.

The **documentation requirement** is where model cards become legally significant. Article 53(3)(b) requires deployers to maintain technical documentation describing how the GPAI model is integrated into the AI system, what modifications or fine-tuning were applied, what inputs the model receives, what outputs it produces, what safeguards are implemented, and what monitoring is conducted. The GPAI Code of Practice specifies that this documentation must be updated whenever the system changes and must be available for review by national AI authorities upon request.

Your production model card, if written according to the structure described in the previous subchapter, satisfies most of the documentation requirement. It documents which model you use, for what purpose, with what safeguards, with what performance characteristics, and with what risk mitigations. You supplement the model card with system integration documentation describing how the model fits into your broader application architecture, what preprocessing and postprocessing you apply, and what monitoring and logging you conduct.

The **risk assessment requirement** is not one-time. You conduct risk assessment during model selection, before production deployment, and periodically during operation. The assessment identifies what could go wrong, how likely it is, how severe the consequences would be, and what mitigations reduce risk to acceptable levels. This is the risk assessment section of your model card, formalized and expanded.

The Code of Practice specifies risk categories that deployer assessments must address: accuracy and reliability risks, safety and security risks, bias and fairness risks, privacy and data protection risks, transparency and explainability risks, and environmental risks. You assess each category for your specific use case. A customer service chatbot has different risk profiles than a medical diagnosis assistant. A content moderation system has different risk profiles than a contract drafting tool. The assessment is contextual, not generic.

Risk assessment is documented and reviewed by Legal and Compliance before production deployment. For high-risk applications as defined in Annex III of the Act (employment decisions, credit scoring, law enforcement, critical infrastructure, education, healthcare), risk assessment must be particularly thorough and must consider the requirements in Articles 8-15 governing high-risk AI systems. If your GPAI deployment qualifies as a high-risk AI system, you have additional obligations including conformity assessment, registration in the EU database, and ongoing monitoring.

## Systemic Risk Models: Additional Considerations for Deployers

If you deploy a systemic risk GPAI model, the Code of Practice specifies additional deployer considerations. You must justify why your use case requires the capabilities of a systemic risk model. You must assess whether the model's high-impact capabilities introduce risks that cannot be adequately mitigated. You must implement safeguards specifically tailored to the model's advanced capabilities, such as monitoring for emergent behaviors, filtering for misuse attempts, and rate limiting for abuse prevention.

The justification requirement is practical. If your use case involves summarizing customer feedback emails, a systemic risk model like GPT-5.2 is likely overkill. GPT-4.5 Turbo or Claude Sonnet 3.5 would suffice. Regulators expect you to choose the least capable model that meets your requirements, not the most capable model available. This principle, known as **capability proportionality**, reduces risk by limiting the potential for unintended capabilities to cause harm.

If you do deploy a systemic risk model, your documentation must explain why. "We selected Claude Opus 4.5 for contract analysis because our evaluation showed that lower-capability models (Claude Sonnet 3.5, GPT-4.5 Turbo) failed to reliably identify nuanced liability clauses in complex multi-party agreements. Our evaluation set included 300 commercial contracts reviewed by expert attorneys. Claude Opus 4.5 achieved 94% accuracy on liability clause identification vs 81% for Sonnet 3.5 and 78% for GPT-4.5 Turbo. The 13-16 percentage point accuracy gain justifies the use of a higher-capability model given the high stakes of missed liability terms."

This justification demonstrates that you evaluated alternatives, that you have a clear performance delta, and that the performance gain is material to your use case. Regulators will not challenge this. They will challenge a deployment that says "we used the newest model because it's the best" with no comparative evaluation data.

## The Interaction with Existing Regulations: GDPR, Sector-Specific Rules, and Harmonization

The GPAI obligations exist alongside existing regulations, and you must comply with both. The most significant interaction is with **GDPR**, the General Data Protection Regulation. If your GPAI deployment processes personal data, you must comply with GDPR data protection principles: lawfulness, fairness, transparency, purpose limitation, data minimization, accuracy, storage limitation, integrity, and confidentiality.

GDPR Article 22 governs automated decision-making. If your GPAI system makes decisions that produce legal effects or similarly significantly affect individuals, you must provide transparency, allow human oversight, and enable individuals to contest the decision. This applies to hiring tools, credit scoring tools, insurance underwriting tools, and similar high-stakes applications. Your GPAI deployment must implement human-in-the-loop mechanisms that satisfy Article 22 requirements.

GDPR Article 35 requires Data Protection Impact Assessments for high-risk processing. If your GPAI deployment involves processing special category data (health, biometric, genetic data) or systematic monitoring of publicly accessible areas or vulnerable populations, you must conduct a DPIA before deployment. The DPIA overlaps with but does not replace the AI Act risk assessment. You conduct both, and you ensure consistency between them.

Sector-specific regulations apply on top of the AI Act. If your GPAI deployment operates in healthcare, you must comply with the Medical Device Regulation (MDR) and In Vitro Diagnostic Regulation (IVDR) if the system qualifies as a medical device. If your deployment operates in financial services, you must comply with MiFID II, PSD2, and other financial regulations. If your deployment operates in critical infrastructure, you must comply with the Network and Information Security Directive (NIS2). The AI Act does not replace these regulations. It adds to them.

The principle of **harmonization** means that when multiple regulations apply, you comply with the strictest requirements. If GDPR requires six-month data retention and your sector regulation requires three years, you retain for three years. If the AI Act requires quarterly risk assessment and your sector regulation requires monthly, you assess monthly. Regulators expect you to identify all applicable regulations and demonstrate compliance with each.

## Practical Compliance Steps for Engineering Teams

Compliance is not a Legal-only responsibility. Engineering teams own the technical implementation of GPAI compliance, and that implementation begins during model selection. You create a **GPAI compliance checklist** that every model selection decision must complete before production deployment.

The checklist includes: model identification and classification (is this a GPAI model, does it have systemic risk), provider compliance verification (has the provider published transparency documentation, is the provider adhering to the Code of Practice), instructions for use review (does our use case comply with provider usage policies), risk assessment completion (have we assessed and documented risks for this use case), model card creation (does this model have a complete production model card), technical documentation (is integration and safeguarding documented), monitoring plan (are we logging inputs, outputs, and performance metrics), incident response plan (do we have a process to detect and respond to failures), and Legal and Compliance sign-off (has Legal reviewed and approved deployment).

You complete this checklist for every model before production deployment. The checklist is stored as a formal record. When auditors arrive, you hand them completed checklists for all models in production. This demonstrates systematic compliance, not ad-hoc efforts.

You implement **logging and monitoring** that captures data required for incident reporting. The Act requires deployers to report serious incidents to national AI authorities within 15 days of becoming aware of them. A serious incident is an incident that leads to death, serious injury, serious harm to health, serious disruption of critical infrastructure, breach of fundamental rights, or serious environmental harm. You cannot report what you don't detect, so your monitoring must capture failure modes that could lead to serious incidents.

Your logging captures inputs, outputs, user feedback, model behavior anomalies, and system performance metrics. You implement alerting that notifies the responsible team when potential incidents occur. You define escalation paths so serious incidents reach Legal and Compliance quickly. You document these processes in your incident response playbook.

You establish a **periodic review cycle** for GPAI compliance. Quarterly reviews assess whether your risk assessments remain current, whether your model cards are up to date, whether your monitoring has detected any new failure modes, whether provider instructions for use have changed, and whether any regulatory guidance has been published that affects your obligations. This review is integrated with your evaluation cycle so compliance review and performance review happen together.

You train your team on GPAI obligations. Engineers understand that model selection is not just a technical decision but a compliance decision. Product managers understand that feature requirements must consider GPAI constraints. Legal and Compliance understand the technical implementation of safeguards so they can assess adequacy. This shared understanding prevents compliance from becoming a last-minute gate that blocks launches.

## What Auditors Actually Ask For

National AI authorities began conducting GPAI audits in mid-2025, and patterns have emerged in what auditors request. First, they ask for a **list of all GPAI models you deploy**, including model identifiers, versions, deployment dates, and use cases. This list must be current. If you retired a model three months ago but it's still on your list, that signals poor operational hygiene.

Second, they ask for **production model cards** for each deployed model. They review whether the cards are complete, whether they document risks, whether they specify safeguards, and whether they're up to date. They check the update history to confirm the cards are maintained. Stale model cards are a red flag.

Third, they ask for **risk assessment documentation**. They verify that you've assessed risks appropriate to your use case, that you've identified mitigations, and that Legal has reviewed and approved residual risk. They look for evidence that risk assessment is systematic, not a checkbox exercise.

Fourth, they ask for **evidence of compliance with instructions for use**. They compare your use case to the provider's published usage policies. If your use case is boundary-line, they ask for your interpretation and justification. They expect you to have reviewed instructions carefully, not skimmed them.

Fifth, they ask for **incident logs and response documentation**. They want to see that you monitor for failures, that you have an incident response process, and that you can detect serious incidents. They may ask whether any serious incidents have occurred and, if so, whether you reported them within the 15-day deadline.

Sixth, they ask about **data protection measures**. They verify that your GDPR obligations are met, that you have a data processing agreement with your model provider if applicable, that you limit data sent to models to what's necessary, and that you handle special category data appropriately.

Auditors are pragmatic. They recognize that GPAI compliance is new, that best practices are still emerging, and that perfect compliance is not expected in early audits. What they look for is evidence of good faith effort: systematic processes, documented decisions, maintained records, and responsiveness to issues. Teams that can demonstrate these characteristics generally pass audits even if some documentation gaps exist. Teams that have no processes, no records, and no awareness of their obligations face corrective action mandates and potential fines.

## Forward Compliance: Tracking Regulatory Evolution

GPAI compliance in 2026 is a moving target. The Code of Practice will be updated as providers and deployers gain experience. The Q&A Guidance will expand as new questions arise. National AI authorities will issue sector-specific guidance and enforcement priorities. Your compliance processes must adapt.

You designate someone on your team, often a senior engineer or engineering manager, as the **GPAI compliance lead**. This person subscribes to European AI Office announcements, monitors updates to the Code of Practice and Q&A Guidance, participates in industry working groups, and ensures your team stays informed of regulatory evolution. This is not a full-time role, but it requires consistent attention.

You establish a **regulatory update review process**. When new guidance is published, the compliance lead reviews it, identifies any changes that affect your obligations, and brings recommendations to the team. If the guidance clarifies that a practice you currently use is non-compliant, you adjust. If the guidance introduces new requirements, you implement them. This proactive approach prevents compliance gaps from accumulating.

You engage with industry groups and standards bodies. Organizations like the European Committee for Standardization (CEN) and the European Committee for Electrotechnical Standardization (CENELEC) are developing harmonized standards for AI systems. When these standards are published and cited in the Official Journal of the European Union, compliance with the standards creates a presumption of conformity with the Act. Participating in standards development or adopting published standards reduces compliance uncertainty.

Your GPAI compliance is not a one-time project. It is an ongoing operational discipline integrated with your model selection, deployment, monitoring, and incident response processes. It is not overhead. It is the cost of operating AI systems in the European market, and it is non-negotiable for teams serving EU users.

The legal obligations you face as a deployer exist alongside the technical obligations of maintaining model performance, managing costs, and ensuring reliability, and together these obligations form the complete picture of production model management in 2026.

