# 6.4 — Model Ensembles: Aggregating Outputs for Higher Reliability

In March 2025, a healthcare diagnostics company launched an AI system that analyzed radiology reports to flag potential missed findings before reports were finalized. The system needed to achieve 98% recall on critical findings—missing a cancer indication was unacceptable. Their initial implementation used GPT-5 and achieved 94.3% recall in production, which sounds impressive until you calculate that across 50,000 reports per month, they were missing 285 critical findings monthly. When they analyzed the errors, they found no pattern. GPT-5 didn't consistently fail on specific finding types or report formats. The errors were seemingly random—cases where the model just missed something it should have caught. Their medical advisory board refused to approve the system for production use. The company faced a choice: abandon the project or find a way to push recall above 98%.

The engineering team proposed an ensemble: run every report through three different frontier models—GPT-5, Claude Opus 4.5, and Gemini 3 Pro—and flag any finding that any model detected. The theory was that models make different errors, so if one model missed a finding, another would catch it. The medical team was skeptical. The finance team was horrified—this would triple their API costs. But the engineering team ran a retrospective analysis on 10,000 historical reports with known ground truth. GPT-5 alone achieved 94.3% recall. The three-model ensemble achieved 98.7% recall. The models' errors were indeed uncorrelated—when GPT-5 missed a finding, Opus or Gemini usually caught it. The false positive rate increased slightly, from 2.1% to 3.8%, but in this domain false positives were acceptable because radiologists reviewed all flagged findings anyway. The company deployed the ensemble in April 2025. It cost three times as much per report. It also caught 285 additional critical findings per month that the single-model system would have missed. The medical advisory board approved it immediately. This chapter teaches you when ensembles are worth the cost and how to implement them in production.

## The Ensemble Value Proposition

A **model ensemble** runs the same input through multiple models and combines their outputs to produce a final result that is more reliable than any single model. The reliability improvement comes from diversity: different models make different errors because they were trained on different data, use different architectures, or apply different reasoning strategies. When you aggregate outputs from diverse models, the errors cancel out while correct answers reinforce each other. This is the same principle used in statistical ensembles, weather forecasting, and investment portfolio diversification. Ensembles trade cost for reliability. You pay multiple API calls to reduce error rate.

The cost multiplier is straightforward. A two-model ensemble costs twice as much as a single model. A three-model ensemble costs three times as much. A five-model ensemble costs five times as much. If you're running an ensemble with models of different pricing tiers—say GPT-5, Claude Opus 4.5, and Gemini 3 Flash—your cost is the sum of all model costs, which might be 2.5x your baseline if Flash is much cheaper. But in practice, ensembles that mix capability tiers perform worse than ensembles of similar-capability models, because the weaker model's errors dominate. Effective ensembles use models of comparable quality, which means comparable cost.

The reliability improvement depends on error correlation. If two models make exactly the same errors, ensembling them provides zero benefit. You're paying twice as much for the same failure rate. If two models make completely independent errors—when one is wrong, the other is right 50% of the time—ensembling them significantly improves reliability. Real models fall between these extremes. Models from the same family—GPT-5 and GPT-5.1—tend to have correlated errors because they share training data and architecture. Models from different families—GPT-5 versus Claude Opus 4.5 versus Gemini 3 Pro—have more independent errors because they were built by different organizations with different design philosophies.

The ensemble is worth the cost when the value of increased reliability exceeds the cost of additional API calls. In the radiology flagging system, missing a critical finding costs the healthcare company somewhere between tens of thousands and millions of dollars in liability, patient harm, and reputation damage. Spending an extra two dollars per report to reduce miss rate by 75% is trivially cost-justified. In a content moderation system where false negatives allow harmful content to reach users and false positives anger users by removing legitimate content, ensembles might reduce both error types, making the platform safer and the user experience better. In a customer support chatbot where errors just mean slightly worse answers, tripling costs for marginally better quality is usually not justified.

## Voting Ensembles for Classification

The simplest ensemble strategy is majority voting for classification tasks. You run the input through three or five models, each model votes for a class, and you select the class with the most votes. This works when your task has discrete output categories: spam or not spam, positive or negative sentiment, category A versus B versus C. Voting ensembles are conceptually simple, easy to implement, and effective when individual model accuracy is above 80% and errors are reasonably independent.

The math is straightforward. If each model has 90% accuracy and errors are independent, the probability that a majority of three models is wrong is the probability that two or three models are wrong simultaneously. That's roughly 2.8% for three models with independent 90% accuracy, compared to 10% for a single model. You've reduced error rate by 72% by tripling costs. The improvement is larger when you use more models: with five models at 90% accuracy each, majority-vote error rate drops to about 0.86%, an error reduction of 91%. But you're now paying 5x costs.

In practice, model errors are not fully independent. Models trained on similar data with similar architectures make correlated errors. When you ensemble GPT-5, GPT-5.1, and GPT-5.2, you don't get independent errors—you get highly correlated errors because all three models share most of their training data and all use transformer architectures with similar designs. The error reduction you actually achieve is much less than the theoretical maximum. Empirical studies from late 2025 show that ensembles of models from the same family achieve error reductions of 20% to 40%, while ensembles of models from different families—OpenAI, Anthropic, Google, Meta—achieve error reductions of 50% to 70%. If you're building an ensemble, use models from different providers.

Weighted voting is the refinement. Instead of treating all model votes equally, you weight votes by model accuracy on your specific task. If GPT-5 achieves 92% accuracy on your validation set, Claude Opus 4.5 achieves 89%, and Gemini 3 Pro achieves 91%, you weight GPT-5's vote higher. This improves ensemble accuracy by 2% to 5% compared to unweighted voting. The downside is you need labeled validation data to calibrate weights, and weights must be recalibrated when models are updated. Most production teams start with unweighted majority voting and add weighting only if unweighted performance is close to but not quite meeting their target.

Tie-breaking is the operational detail that matters. With three-model ensembles, ties are impossible—you always get a 2-1 or 3-0 majority. With two-model ensembles, ties are common, and you need a tiebreaker rule. The simplest tiebreaker is "defer to the more accurate model," which you determine from validation data. A more sophisticated tiebreaker is to run a third model only when the first two disagree, creating a conditional three-model ensemble. You pay for three models only on the cases where models disagree, which might be 20% to 40% of inputs. This reduces average cost to 1.4x or 1.6x instead of 2x, while still achieving most of the reliability benefit.

## Best-of-N Sampling for Generation

For generation tasks—writing, summarization, question answering—voting doesn't work because outputs aren't discrete categories. Instead you use **best-of-N sampling**: generate N outputs from one or more models, evaluate all N outputs with a judge model or scoring function, and select the best one. This pattern is widely used in production for high-stakes generation where output quality matters more than cost.

The simplest version is best-of-N from a single model with temperature sampling. You generate five outputs from GPT-5 with temperature 0.7, run all five through a quality scoring function, and return the highest-scoring output. This improves output quality because temperature sampling introduces randomness, so the five outputs are different, and the best of five random samples is better than a single random sample. Empirical results show that best-of-5 improves quality metrics by 15% to 25% compared to single-sample generation, at 5x the cost. Best-of-10 improves quality by 25% to 35%, at 10x the cost. Returns diminish rapidly—best-of-20 is only marginally better than best-of-10.

The more powerful version is best-of-N across multiple models. You generate one output from GPT-5, one from Claude Opus 4.5, one from Gemini 3 Deep Think, score all three, and return the best. This captures model diversity instead of just sampling diversity. Different models have different strengths: GPT-5 might be better at creative writing, Opus 4.5 might be better at technical accuracy, Gemini Deep Think might be better at structured reasoning. The judge model or scoring function picks the output that best fits your quality criteria for this specific input. Multi-model best-of-N typically achieves 30% to 50% quality improvement over single-model generation, at 3x to 5x cost depending on how many models you use.

The critical component is the judge. The judge must evaluate outputs accurately and quickly, or the ensemble fails. Most production systems use a frontier model as judge—often the same model that generated one of the candidate outputs. You send the original input plus all N candidate outputs to the judge with instructions like "evaluate these three responses for accuracy, completeness, and clarity; return scores for each." The judge returns structured scores, and you select the highest-scoring output. This costs one additional model call per ensemble, so a three-model ensemble with a judge costs 4x a single model call.

Some teams try to save money by using a cheaper model as judge, like using Haiku 4.5 to judge outputs from Opus, GPT-5, and Gemini. This fails. Weaker models are not reliable judges of stronger models' outputs. The weaker judge systematically misjudges which output is best, often preferring the output that sounds most confident or uses simpler language, even when that output is factually wrong. If you're ensembling frontier models, you must use a frontier model as judge. The judge can be one of the models in the ensemble—you can use GPT-5 to judge outputs from GPT-5, Opus, and Gemini—as long as you design prompts to minimize self-preference bias.

Self-preference bias is real. Models prefer their own outputs about 60% to 70% of the time even when other models' outputs are objectively better, according to controlled studies from late 2025. You mitigate this by blinding the judge to which model generated which output and by using evaluation criteria that are as objective as possible. Instead of asking the judge "which response is best," ask "rate each response on factual accuracy, completeness, and relevance on a scale of 1 to 10." The judge returns numerical scores, and you select the highest-scoring output. Numerical scoring reduces self-preference bias from 65% to about 52%, which is close to random and acceptable.

## When Ensembles Improve Quality Versus When They Waste Money

Ensembles work when model errors are independent and when aggregation methods can distinguish correct from incorrect outputs. Ensembles fail when model errors are correlated, when the task is so hard that all models fail similarly, or when the aggregation method is unreliable. The decision to ensemble must be based on empirical analysis of error patterns in your specific domain, not on general intuition.

Ensembles are most effective for tasks where individual model accuracy is in the 85% to 95% range. Below 85%, models make so many errors that even ensembling doesn't push accuracy high enough for production use—you need better models or better task specifications, not ensembles. Above 95%, the remaining errors are often corner cases where all models fail, so ensembles provide minimal benefit. The sweet spot is tasks where models are pretty good but not quite good enough, and you need to close that 3% to 5% reliability gap.

Ensembles also work best when false positives and false negatives have asymmetric costs. In the radiology flagging system, false positives cost a few seconds of radiologist time reviewing a non-issue, while false negatives cost patient safety and legal liability. The ensemble increases false positives by 80% but decreases false negatives by 75%. This tradeoff is obviously worthwhile. In a spam filter, false positives cost user frustration when legitimate emails are blocked, while false negatives cost user frustration when spam reaches the inbox. The costs are more symmetric, and ensembles might increase total user frustration even if they reduce total error rate. You must analyze cost asymmetry before deploying ensembles.

Ensembles waste money when you're using models that are too similar. An ensemble of GPT-5, GPT-5.1, and GPT-5.2 achieves maybe 15% error reduction compared to GPT-5 alone, because the three models are highly correlated. You're paying 3x for 15% improvement. In most domains, you could achieve better than 15% improvement by upgrading to a better single model or by improving your prompts. The ensemble is cost-ineffective. Always check whether a better single-model configuration beats your ensemble before deploying.

Ensembles also waste money when your baseline model is already achieving acceptable performance. If GPT-5 gives you 97% accuracy and your requirement is 95%, ensembling to reach 98.5% accuracy provides no business value. You're already exceeding your target. Spending 3x to improve further is waste. The ensemble is only worthwhile when your baseline performance is below target and a single better model can't close the gap.

The other failure mode is when the task is fundamentally too hard for current models. If the best single model achieves 60% accuracy and you ensemble three models, you might reach 68% accuracy. That's a 20% error reduction, which sounds good in relative terms. But 68% accuracy is still unacceptable for most production use cases. The problem is not insufficient ensembling—the problem is that current models can't do this task reliably. You need to reframe the task, add more input context, provide examples, or wait for better models. Ensembling is not a substitute for solving the underlying capability gap.

## Ensemble Strategies: Majority Vote, Weighted Vote, Judge Model

You've already seen majority voting and judge-based selection. There are other ensemble strategies worth knowing. **Weighted voting** assigns different weights to different models based on their accuracy, their confidence scores, or their past performance on similar inputs. This improves accuracy by 3% to 8% compared to unweighted voting, at the cost of needing validation data to tune weights. Weighted voting works best when you have months of production data showing which models are more reliable for which input types.

**Confidence-weighted voting** uses each model's self-reported confidence score to weight its vote. If GPT-5 returns a classification with 95% confidence and Claude returns a different classification with 70% confidence, GPT-5's vote counts more. This works when models are well-calibrated—when their confidence scores accurately reflect their error rates. In practice, models are not well-calibrated out of the box. A model that reports 90% confidence might be wrong 20% of the time. You must calibrate confidence scores using validation data before using them for ensemble weighting. After calibration, confidence-weighted voting improves accuracy by 5% to 10% compared to unweighted voting.

**Cascading ensembles** run models sequentially, not in parallel. You start with the cheapest model, check its confidence score, and if confidence is above a threshold, you return its output immediately. If confidence is below threshold, you escalate to a more expensive model. If that model's confidence is high, you return its output. If not, you escalate to a third model or run a full ensemble. This pattern reduces average cost because most inputs are handled by cheap models, while uncertain inputs get the full ensemble treatment. A production content moderation system might run 70% of inputs through Haiku 4.5 with confidence threshold 0.95, escalate 25% to Sonnet 4.5, and run a full Opus-GPT-Gemini ensemble on the remaining 5%. Average cost is 1.4x the Haiku baseline instead of 3x for a full ensemble on every input.

**Judge model selection** is what you use for generation tasks. The judge evaluates N candidate outputs and picks the best. The judge can be a frontier model, a specialized reward model trained for your domain, or even a rule-based scoring function if your quality criteria are well-defined. Frontier model judges are most flexible but most expensive. Specialized reward models are cheaper and often better-calibrated for specific domains, but require training data and ongoing maintenance. Rule-based scoring works only when quality is measurable with explicit rules, like "output must include these three fields and be under 500 words and have no profanity."

The cutting-edge strategy as of early 2026 is **learned ensemble weights using a meta-model**. You train a small classifier that takes the input and all N model outputs as features and predicts which model is most likely correct for this input. The meta-model learns patterns like "GPT-5 is more reliable on technical questions, Claude is more reliable on creative tasks, Gemini is more reliable on multi-step reasoning." This approach requires significant training data—thousands of examples with ground truth labels—but achieves state-of-the-art ensemble performance, often 10% to 15% better than fixed-weight voting. It's used in production by companies processing millions of queries daily where the improved accuracy justifies the meta-model development cost.

## Selective Ensembles: Only Ensemble on Uncertain or High-Stakes Queries

The most cost-effective ensemble strategy is selective ensembling: run a fast triage model to decide which inputs need ensembling and which can be handled by a single model. This reduces average cost from 3x to 5x for full ensembling down to 1.2x to 1.8x while retaining most of the reliability benefit. Selective ensembling is the production standard for any high-volume application where ensembles improve quality but full ensembling is too expensive.

The triage decision is based on input characteristics, model confidence, or task criticality. For input-based triage, you run a fast classifier that predicts task difficulty. Easy inputs go to a single model. Hard inputs get ensembled. The classifier uses features like input length, vocabulary complexity, presence of domain-specific terms, and structural patterns. This classifier can be a simple logistic regression model trained on a few thousand labeled examples showing which inputs caused single-model errors. It runs in milliseconds and costs almost nothing.

Confidence-based triage runs a single model first, checks the confidence score, and triggers an ensemble only when confidence is below threshold. If GPT-5 returns an answer with 98% confidence, you accept it. If confidence is 75%, you run Claude and Gemini, compare all three outputs, and use a judge to select the best. This approach ensembles 15% to 30% of inputs in typical production workloads, reducing average cost to about 1.5x while catching most of the errors that single-model deployment would miss.

Criticality-based triage uses metadata about the request to decide ensemble policy. In the radiology system, reports flagged by human reviewers as "complex case" automatically trigger ensembles. Routine reports run through a single model. In a financial analysis system, queries about portfolio risk trigger ensembles because errors are costly, while queries about market news run through a single model because errors are low-stakes. Criticality metadata often comes from upstream systems—a customer support ticket marked "escalated" gets ensembled, routine tickets don't.

The optimal triage strategy depends on your cost-quality tradeoff. If you can afford to ensemble 50% of inputs, you set triage thresholds to route half of inputs to ensembles and half to single-model. If you can only afford to ensemble 10%, you tighten thresholds and ensemble only the hardest or highest-stakes cases. The triage model's job is to identify the 10% or 50% of inputs where ensembling provides the most value. This is a standard classification problem: train a model to predict which inputs benefit most from ensembling, using historical data showing which inputs caused single-model errors.

## When Ensembles Are Worth the Cost and When a Single Better Model Is Cheaper

The fundamental question is whether you should ensemble multiple models or just use a single better model. Ensembles cost 2x to 5x a single model call. Better models also cost more than baseline models—Claude Opus 4.5 costs 25x Haiku 4.5, GPT-5 costs 20x GPT-5-nano. The economic question is whether paying 3x for a three-model ensemble of mid-tier models gives you better quality per dollar than paying 25x for a single frontier model.

The answer depends on the task and the models available. For tasks where the frontier model achieves 98% accuracy and mid-tier models achieve 90%, ensembling three mid-tier models might get you to 94% accuracy at 3x cost, while the frontier model gives you 98% accuracy at 25x cost. The ensemble is cost-effective if 94% accuracy is good enough. If you need 98%, the single frontier model is cheaper than trying to ensemble your way there.

For tasks where model errors are highly correlated—all models fail on the same hard cases—ensembles provide little benefit. If GPT-5, Claude Opus, and Gemini all achieve 92% accuracy and all fail on the same 8% of inputs, ensembling them gets you maybe 93% accuracy because the errors overlap. You're paying 3x for 1 percentage point improvement. A better approach is to analyze the 8% failure cases and improve your prompts, add more context, or reframe the task. Ensembles are not a substitute for understanding and fixing failure modes.

For tasks where you need reliability beyond any single model's capability, ensembles are the only option. The radiology flagging system is the canonical example. No single model achieves 98% recall. The best single model is 94%. Ensembling three models reaches 98.7%. There is no "better single model" available that hits 98% at any price. The ensemble is not just cost-effective—it's the only way to meet the requirement. This is true for any safety-critical or compliance-critical application where error rates must be below single-model capabilities.

The rule is: try the best single model first. If it meets your requirements, stop. If it doesn't, analyze the errors. If errors are diverse and models from different providers make different errors, try an ensemble. If errors are concentrated in specific input types or failure modes, fix those failure modes with better prompts, more context, or task reframing. If you can't fix the failure modes and the best single model isn't good enough, ensembles are justified. But ensembles are a solution to a specific problem—uncorrelated errors preventing any single model from meeting requirements—not a general strategy to apply everywhere.

The cost discipline is to measure improvement per dollar. If a three-model ensemble costs three times as much and improves accuracy by 20%, your cost per accuracy point is 15x baseline. If upgrading to a better single model costs four times as much and improves accuracy by 25%, your cost per accuracy point is 16x baseline. The ensemble is slightly more cost-effective. But if the better single model costs four times as much and improves accuracy by 35%, your cost per accuracy point is 11.4x baseline, and the single better model wins. You must measure these tradeoffs empirically on your specific task with your specific quality metrics.

## Production Deployment Patterns

Ensembles in production require infrastructure for parallel execution, output aggregation, and monitoring. The parallel execution pattern is: receive input, fan out to N models simultaneously, collect all N outputs with timeout handling, aggregate outputs using your chosen strategy, return final result. You must handle cases where one or more models timeout or return errors. The standard policy is to proceed with whatever outputs you received—if you requested three models and only two responded within your timeout window, run your aggregation on two outputs instead of three.

Timeout handling is critical because ensemble latency is determined by the slowest model. If GPT-5 responds in 800ms, Claude responds in 1200ms, and Gemini responds in 2500ms, your ensemble latency is 2500ms plus aggregation overhead. This is unacceptable for real-time applications. You set a timeout—say 1500ms—and use whatever outputs arrived before timeout. In the example, you'd aggregate GPT-5 and Claude outputs and ignore Gemini. This degrades gracefully: you get a two-model ensemble instead of three, which still improves reliability over single-model, just not as much.

Monitoring ensemble performance requires tracking per-model accuracy, agreement rates, and contribution to final outputs. You log which models were called, what outputs they returned, whether outputs agreed, which output was selected by the judge, and whether the final output was correct based on any downstream feedback. This data lets you identify when models start making correlated errors—a sign that ensemble value is degrading—and when one model becomes consistently less reliable than others, indicating you should replace it.

Cost monitoring is essential because ensemble costs scale with volume. A system processing 10,000 queries daily with a three-model ensemble costs 30,000 model calls daily. If query volume spikes to 100,000 queries, you're now at 300,000 calls, and your API bill increases proportionally. You must set cost budgets and alerts. The adaptive strategy is to adjust ensemble size based on cost budgets: run three-model ensembles when you're under budget, drop to two-model ensembles if costs spike, and escalate to five-model ensembles for critical queries regardless of cost.

The next chapter explores cascade architectures, where you start with a cheap fast model and escalate to expensive slow models only when needed. This is related to selective ensembles but architecturally distinct: instead of deciding whether to ensemble, you decide which single model to use based on task difficulty and confidence.

