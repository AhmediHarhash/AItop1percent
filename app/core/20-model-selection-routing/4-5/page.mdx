# 4.5 â€” Batch Processing vs Real-Time: When Async Saves Fifty Percent or More

In September 2025, a content moderation platform serving twelve social media clients discovered they were spending $47,000 per month on real-time API calls to moderate user-generated content. Every uploaded image, comment, and profile update triggered an immediate call to GPT-5-mini for classification. The engineering team assumed real-time processing was mandatory because "moderation has to be instant." When their CFO forced a cost review, they discovered that 91% of their moderation volume had no time sensitivity whatsoever. Pre-uploaded content waiting in queues, bulk reclassification after policy updates, and nightly audits of flagged items could all wait minutes or hours. By routing those workloads to OpenAI's Batch API at fifty percent of real-time pricing, they cut their monthly bill to $26,000 with zero user-facing impact. The tragedy was not the initial architecture choice but the eighteen months they operated without questioning the real-time assumption.

The cost difference between real-time and batch API pricing is the single largest optimization lever most teams never pull. You are paying double for infrastructure you do not need. This subchapter teaches you how to identify batch-eligible workloads, architect for batch-first processing, and build hybrid systems that route intelligently between async and synchronous execution.

## The Batch API Cost Structure

Batch pricing exists because cloud providers can schedule your workload during off-peak capacity windows. OpenAI's Batch API charges fifty percent of standard real-time pricing for GPT-5, GPT-5.1, GPT-5.2, GPT-5-mini, and GPT-5-nano. Anthropic offers batch pricing at forty to fifty percent discounts for Claude Opus 4.5, Sonnet 4.5, and Haiku 4.5, depending on your completion window tolerance. Google's Gemini 3 Pro and Flash tiers offer batch modes at thirty-five to forty-five percent reductions. These are not promotional rates. These are structural pricing differences reflecting the economic reality of schedulable compute.

The mechanism is straightforward. You submit a batch of requests as a single JSONL file containing hundreds or thousands of individual prompts. The provider queues your batch, processes it during available capacity windows, and returns a JSONL file of responses within your specified completion window. Completion windows range from one hour to twenty-four hours depending on provider and tier. You poll for completion status or configure a webhook callback. There is no streaming, no incremental delivery, no request-level visibility until the entire batch completes.

The fifty percent discount is not a small optimization. It is the difference between a $100,000 annual model budget and a $50,000 budget for identical workload volume. For a system processing ten million inference calls per month, batch eligibility for even half your traffic represents $25,000 in monthly savings. These are not rounding errors. These are headcount-equivalent cost reductions.

The critical constraint is latency tolerance. Batch processing trades time for cost. If your workload can tolerate completion windows measured in minutes or hours instead of milliseconds, you qualify. If you need synchronous responses within seconds, you do not. The architectural challenge is separating these workloads and routing them to the appropriate execution path.

## Identifying Batch-Eligible Workloads

Most production systems contain far more batch-eligible work than their architects recognize. The content moderation platform discovered 91% batch eligibility only after instrumenting their request origins and measuring actual latency requirements. You must perform the same analysis. Instrument every inference call with metadata indicating whether the result is needed synchronously for a user-facing interaction or asynchronously for background processing.

Offline analysis is the canonical batch workload. Any task that generates insights for later review qualifies. A legal discovery platform analyzing ten thousand documents overnight for case relevance does not need real-time results. A compliance system re-scoring every customer interaction from the past quarter after a policy update operates entirely in batch mode. A research team running sentiment analysis on historical social media archives has zero latency sensitivity. These workloads should never touch real-time APIs.

Bulk classification is batch-eligible when results feed downstream systems rather than immediate user interactions. An e-commerce platform categorizing fifty thousand new product listings uploaded via CSV file has no user waiting for results. A CRM system enriching imported contact records with demographic predictions processes asynchronously. A fraud detection system re-evaluating historical transactions after deploying a new model runs entirely in batch. The distinguishing characteristic is decoupling between input arrival and output consumption.

Content moderation queues are hybrid workloads that many teams incorrectly implement as fully real-time. Pre-publication moderation of user uploads can run in batch if you buffer submissions for thirty seconds to two minutes before processing. A social platform receiving five hundred image uploads per minute can accumulate batches of three hundred to six hundred items, submit them every ninety seconds, and still deliver moderation verdicts within three to four minutes of upload. For most use cases, three-minute moderation latency is indistinguishable from thirty-second latency in user experience terms. The cost difference is fifty percent.

Nightly report generation, scheduled summarization, and periodic digest creation are pure batch workloads. A sales platform generating account health summaries for every customer overnight processes ten thousand accounts in batch. An analytics system creating natural language insights from daily metrics runs once per day in batch mode. A notification service composing personalized weekly update emails for three million users submits the entire workload as a single batch job. These tasks have explicit time boundaries and zero expectation of sub-second latency.

The pattern that reveals batch eligibility is temporal decoupling. If the time between input creation and output consumption is measured in minutes, hours, or days, you are batch-eligible. If a user is waiting synchronously for the result to proceed with their workflow, you are not. Most systems contain both patterns. Your architecture must separate them.

## Batch API Mechanics and Implementation Patterns

Using batch APIs requires different infrastructure patterns than real-time request-response flows. You submit work via file upload rather than HTTP POST. You poll for completion rather than waiting on open connections. You handle partial failures at batch granularity rather than request granularity. These differences demand architectural adaptation.

The submission process packages your inference requests into JSONL format with one request per line. Each line contains a unique identifier, the model name, the prompt, and any parameters like temperature or max tokens. For OpenAI, you upload this file to their file storage API, then submit a batch creation request referencing the file ID. The API returns a batch ID you use for status polling. Anthropic's flow is similar: upload JSONL, create batch, receive batch ID. Google Gemini uses Cloud Storage integration with batch job submission via API.

Completion windows define your latency-cost tradeoff. OpenAI offers one-hour, six-hour, and twenty-four-hour windows with pricing inversely proportional to window length. The fifty percent discount applies to twenty-four-hour windows. Shorter windows cost more but still undercut real-time pricing. Anthropic's batch API defaults to twelve-hour completion with optional six-hour and twenty-four-hour tiers. You select window length at batch submission time based on your downstream consumption deadlines.

Polling for completion uses exponential backoff to avoid rate limit exhaustion. Your orchestration layer checks batch status every thirty seconds initially, then every two minutes, then every five minutes until completion. Most providers offer webhook callbacks to eliminate polling overhead. When the batch completes, you download the results file, parse the JSONL, and match responses to original requests using your unique identifiers. This introduces a persistence layer requirement: you must store the mapping between batch IDs and downstream consumers waiting for results.

Partial failure handling is coarser-grained than real-time error handling. If three requests out of one thousand fail due to content policy violations or malformed prompts, the batch still completes with 997 successful responses. Your result parsing logic must handle missing entries gracefully. Some teams submit single-item batches for critical requests to isolate failures, but this defeats the cost optimization. Better to implement retry logic that resubmits failed items in a new batch after diagnosing the root cause.

Architecting for batch-first processing means inverting your request flow. Instead of application code calling the model API directly, application code writes requests to a queue. A batch orchestrator consumes the queue, accumulates requests until a size or time threshold triggers batch submission, submits the batch, polls for completion, and writes results back to a results queue or database. Downstream consumers read from the results store when ready. This decouples request generation from response consumption, enabling the temporal flexibility batch pricing requires.

The infrastructure overhead is nontrivial. You need durable request queues, batch orchestration workers, result storage with TTL-based cleanup, and monitoring for batch submission failures and completion delays. For teams processing fewer than ten thousand requests per day, this overhead may exceed the cost savings. For teams processing millions of requests monthly, the infrastructure investment pays for itself within weeks.

## Latency Tradeoffs and User Experience Impact

The fifty percent cost reduction comes from accepting higher latency. You must quantify the actual user experience impact before committing to batch architectures. Many teams discover their latency assumptions are unfounded.

A document analysis platform assumed users would abandon if processing took longer than sixty seconds. Instrumentation revealed that 94% of uploads came from automated integrations that polled for results every five minutes regardless of actual completion time. The remaining 6% of interactive users tolerated up to ten minutes before checking status. Switching to batch processing with six-hour completion windows had zero measurable impact on user satisfaction because the perceived latency was dominated by user behavior, not API response time.

An email personalization service feared that moving to batch processing would delay campaign sends. Analysis showed that marketers scheduled campaigns hours or days in advance, providing ample time for batch processing. The team implemented a hybrid flow: campaigns scheduled more than four hours out used batch APIs, while sends requested within four hours used real-time APIs for immediate processing. Over 80% of volume qualified for batch, cutting costs by forty percent with no user-facing change.

The anti-pattern is assuming latency requirements without measurement. Teams default to real-time APIs because "users expect fast responses" without instrumenting actual tolerance thresholds. You must measure how long users actually wait, how often they refresh or poll, and whether they can distinguish between two-second and two-minute response times in their workflows. For many background tasks, the answer is that latency beyond the first few minutes is imperceptible.

Visible progress indicators extend latency tolerance dramatically. A batch job that shows "processing 3,200 of 10,000 records" every thirty seconds keeps users engaged for minutes or hours. A simple "your request is queued, estimated completion in 4 minutes" message prevents abandonment. Many workflows that appear real-time-dependent actually tolerate substantial latency when you provide progress visibility and set accurate expectations.

The acceptable latency threshold varies by user persona and task context. Internal tools serving data analysts tolerate minutes to hours. Consumer-facing applications serving end users typically require seconds to low minutes. B2B platforms serving professional workflows fall in between. You must segment by persona and context rather than applying a single latency requirement across all workloads.

## Architecting Batch-First with Real-Time Fallback

The optimal architecture is not fully batch or fully real-time. It is a routing layer that sends each request to the appropriate execution path based on latency requirements and cost constraints.

A batch-first architecture defaults to async processing unless the request explicitly signals real-time need. Application code marks requests with priority flags: batch-eligible, real-time-preferred, or real-time-required. The routing layer queues batch-eligible requests for accumulation, attempts batch processing for real-time-preferred requests but falls back to real-time if batch capacity is exhausted, and immediately dispatches real-time-required requests to synchronous APIs. This surfaces the cost-latency tradeoff to application developers who understand their domain requirements.

Queue-based architectures provide the buffering layer batch processing requires. A content moderation system writes all moderation requests to a Kafka topic partitioned by priority. Batch consumers read from the batch-eligible partition, accumulate requests until reaching one thousand items or ninety seconds of accumulation time, submit a batch, and write results to a results topic. Real-time consumers read from the real-time-required partition and call synchronous APIs immediately. Application code consumes from the results topic regardless of which path processed the request. This decouples execution strategy from application logic.

The hybrid pattern combines batch and real-time processing for the same logical workload by splitting across time boundaries. An analytics platform generates dashboard insights in real-time for the current day's data but uses batch processing for historical recomputation. A recommendation engine updates user preferences in real-time as they interact but recomputes global item rankings nightly in batch. A fraud detection system scores new transactions in real-time but re-evaluates historical transactions quarterly in batch after model updates. Each use case processes the same type of request through different paths depending on temporal context.

Graceful degradation uses batch processing as a cost-saving fallback during peak load. When real-time API request volume exceeds your rate limits or budget thresholds, the system downgrades lower-priority requests to batch processing. A customer support platform processes VIP customer inquiries in real-time but queues standard-tier requests for batch processing during high-volume periods. This maintains service for critical users while containing costs during spikes.

The cost modeling for batch versus real-time split requires measuring the distribution of latency requirements across your workload. If 70% of your requests can tolerate ten-minute latency, 20% can tolerate two-minute latency, and 10% require sub-second latency, you route accordingly. At fifty percent batch pricing, sending 70% of traffic through batch APIs cuts total costs by 35%. If batch qualification reaches 90%, you save 45%. The relationship is linear: every percentage point of batch-eligible traffic delivers half a percentage point of cost reduction.

## Queue-Based Architectures for Batch Orchestration

Implementing batch processing reliably requires durable queuing, idempotent submission, and result correlation across async boundaries. The engineering overhead is higher than request-response flows, but the cost savings justify the investment at scale.

A durable request queue holds inference requests until batch submission. Amazon SQS, Google Cloud Pub/Sub, Azure Service Bus, or self-hosted Kafka provide the persistence layer. Each message contains the full inference request payload plus routing metadata: batch-eligible flag, deadline timestamp, result callback identifier. The queue guarantees at-least-once delivery, allowing batch consumers to fail and retry without losing requests.

Batch accumulators consume from the request queue and build batches based on size and time thresholds. A typical pattern accumulates until reaching one thousand requests or ninety seconds of wall-clock time, whichever comes first. This balances batch size for cost efficiency against latency for time-sensitive requests. The accumulator tracks partially-filled batches in memory with periodic checkpoints to durable storage. When a threshold triggers, the accumulator serializes the batch to JSONL, uploads to the provider's file storage, and submits the batch job.

Submission idempotency prevents duplicate batch creation during retries. Each batch receives a deterministic identifier derived from the hash of its contents or a UUID stored in the checkpoint. Before submitting, the orchestrator checks a deduplication table to ensure the batch ID has not been submitted previously. This allows retry logic to safely resubmit after transient failures without creating duplicate batches.

Completion polling runs in a separate worker pool that tracks submitted batches and queries their status. Each polling worker maintains a priority queue of active batches ordered by expected completion time. It queries the oldest batches first, applying exponential backoff for batches that remain in-progress. When a batch completes, the worker downloads the results file, parses JSONL, and writes individual results to a results store keyed by the original request identifier.

Result correlation maps responses back to the original requesters. The results store is typically a key-value database like Redis or DynamoDB with TTL-based expiration. Application code that submitted a request receives a request ID it uses to poll the results store. When the result appears, the application consumes it and proceeds. For event-driven workflows, the orchestrator publishes results to a results queue that downstream consumers subscribe to.

Error handling must account for partial batch failures and timeout scenarios. If a batch job fails entirely, the orchestrator resubmits the batch after logging the failure. If individual requests within a batch fail, the orchestrator writes error results to the results store so consumers can distinguish between pending and failed requests. If a batch exceeds its expected completion window by a configurable threshold, the orchestrator escalates to real-time fallback for critical requests or simply logs a delay warning for non-critical workloads.

Monitoring and observability track batch submission rate, completion latency distribution, failure rate, and cost per request. Dashboards show the percentage of requests routed to batch versus real-time, the average batch size, and the cost savings relative to all-real-time baseline. Alerts trigger when batch completion latency exceeds SLAs, when failure rates spike, or when queue depths indicate orchestrator capacity issues.

## The Hybrid Pattern: Batch What You Can, Stream What You Must

Most production systems serve multiple user personas with different latency tolerances. The optimal architecture uses batch processing for latency-tolerant workflows and real-time processing for latency-sensitive interactions, routing intelligently based on request context.

A legal discovery platform processes uploaded document sets in batch overnight for next-day delivery but offers real-time document analysis for urgent courtroom deadlines. The same document analysis model runs through different execution paths based on the user's selected priority tier. Batch processing handles 95% of volume at half the cost. Real-time processing serves 5% of volume at full cost but delivers sub-minute results when lawyers need them.

An e-commerce personalization engine computes product recommendations in batch nightly for email campaigns but generates real-time recommendations during active browsing sessions. The batch job processes ten million customer profiles, computes top-twenty recommendations per profile, and stores results in a cache. The real-time path computes recommendations on-demand for users currently on-site, incorporating their live click stream. Batch handles 85% of total recommendation volume. Real-time handles 15% but drives 60% of conversion value because it responds to immediate user intent.

A compliance monitoring system classifies all historical transactions in batch quarterly after regulatory updates but classifies new transactions in real-time to block suspicious activity before settlement. The batch job reprocesses hundreds of millions of records over several days. The real-time path processes thousands of transactions per minute with sub-second latency. Both use the same underlying model, but the execution path differs based on temporal context and risk profile.

The routing logic encodes business rules about latency requirements and cost tolerance. A simple decision tree checks whether the request originates from an interactive user session, whether the result is needed synchronously for downstream processing, and whether the deadline is within the next few minutes. Requests failing all three conditions route to batch. Requests meeting any condition route to real-time. More sophisticated routers incorporate user tier, request priority flags, current rate limit headroom, and budget burn rate to optimize the batch-real-time split dynamically.

Implementing hybrid routing requires infrastructure that supports both execution paths without duplication. Shared model configuration, prompt templates, and output parsing logic must work identically whether invoked via batch or real-time APIs. The router itself must be lightweight and low-latency to avoid adding overhead to real-time requests. A typical implementation uses a routing function that evaluates request metadata, writes to the appropriate queue or API, and returns a handle the caller uses to retrieve results.

The cost savings from hybrid architectures compound with scale. A platform processing fifty million inference calls monthly with 70% batch eligibility saves $87,500 per month at an average $0.005 real-time cost per request and fifty percent batch discount. At one hundred million calls, the savings double to $175,000 monthly. The infrastructure investment to build queue-based orchestration and hybrid routing is a one-time cost typically in the range of two to four engineer-weeks. The payback period at mid-scale is measured in weeks, not months.

## Cost Modeling for Batch versus Real-Time Split

Quantifying the cost impact of batch adoption requires modeling your workload distribution and pricing tiers. The key variables are total monthly request volume, percentage batch-eligible, real-time cost per request, and batch discount rate.

A baseline model calculates current all-real-time cost: total requests multiplied by real-time price per request. For a system running twenty million GPT-5-mini requests monthly at $0.003 per request, baseline cost is $60,000. If 60% of requests qualify for batch processing and batch pricing is fifty percent of real-time, batch-eligible cost drops from $36,000 to $18,000. Real-time-required cost remains $24,000. Total cost becomes $42,000, a thirty percent reduction from baseline.

The relationship is linear in batch eligibility percentage and discount rate. At seventy percent batch eligibility, savings reach thirty-five percent. At eighty percent batch eligibility, savings reach forty percent. The formula is straightforward: total savings equals batch eligibility percentage multiplied by batch discount rate. A seventy percent batch-eligible workload with a fifty percent batch discount yields thirty-five percent total cost reduction.

Real-world cost modeling must account for infrastructure overhead. Running queue-based orchestration on three medium-sized compute instances costs approximately $500 monthly. Operating a results store with several terabytes of monthly throughput costs $200 monthly. Monitoring and logging infrastructure adds $100 monthly. Total overhead is roughly $800 monthly. For a $60,000 baseline, $800 overhead is negligible. For a $5,000 baseline, overhead consumes sixteen percent of savings. Batch architectures make economic sense above approximately $10,000 in monthly model costs.

Latency distribution affects achievable batch eligibility. If your instrumentation reveals that 40% of requests come from synchronous user interactions requiring sub-second latency, your ceiling for batch eligibility is sixty percent. If only 5% of requests are latency-critical, you can reach 95% batch eligibility. Measuring latency requirements accurately is the prerequisite for realistic cost modeling.

The break-even analysis compares infrastructure investment against monthly savings. If building batch orchestration requires six engineer-weeks at a fully-loaded cost of $15,000 per week, total investment is $90,000. If monthly savings from batch adoption are $18,000, payback occurs in five months. If monthly savings are $50,000, payback occurs in under two months. High-volume systems justify the investment immediately. Lower-volume systems should evaluate whether simpler partial-batch patterns deliver sufficient savings without full orchestration infrastructure.

Partial-batch patterns include scheduled batch jobs that process daily or weekly workloads without continuous orchestration. A compliance system that re-scores all customer accounts weekly submits a single large batch job on Sunday nights rather than operating continuous batch accumulators. A content classification pipeline that processes user uploads every fifteen minutes submits fixed-interval batches rather than dynamic size-based accumulation. These simplified patterns deliver most of the cost savings with a fraction of the engineering complexity.

## When Batch Processing Does Not Apply

Batch APIs are not universally applicable. Several workload patterns cannot tolerate batch latency or do not achieve sufficient volume to justify orchestration overhead.

Interactive conversational agents require real-time streaming responses. A customer support chatbot cannot queue user messages for batch processing and deliver responses minutes later. The entire value proposition depends on sub-second interactivity. These workloads must use real-time APIs regardless of cost.

Low-volume workloads do not amortize infrastructure investment. A system processing five hundred requests daily saves $7.50 per day by moving to batch pricing at fifty percent discount and $0.01 per request. Monthly savings of $225 do not justify building queue-based orchestration. These workloads should use real-time APIs or adopt managed batch services if providers offer low-overhead options.

Workflows requiring immediate feedback loops cannot decouple request submission from result consumption. A code generation tool that iteratively refines outputs based on compiler errors needs real-time responses to proceed with the next iteration. A content generation pipeline that validates outputs and retries with adjusted prompts operates in tight request-response loops. Batch latency breaks the iterative cycle.

High-variability workloads with unpredictable latency sensitivity struggle with fixed routing rules. A system where some users tolerate delays while others demand immediacy, but you cannot distinguish at request time, cannot safely route to batch processing. You must either default to real-time for safety or accept user experience degradation for latency-sensitive users.

The decision framework is simple: if you can tolerate minutes to hours of latency, process millions of requests monthly, and invest in orchestration infrastructure, batch processing delivers massive cost savings. If you need sub-second responses, process low volumes, or lack engineering capacity for infrastructure work, stick with real-time APIs and optimize elsewhere.

The content moderation platform that opened this chapter exemplifies the ideal batch use case: high volume, low latency sensitivity, clear request-response decoupling. They identified 91% batch eligibility, invested four engineer-weeks in building orchestration infrastructure, and cut costs from $47,000 to $26,000 monthly. The payback period was three weeks. Eighteen months of operating without this analysis cost them over $300,000 in unnecessary API charges.

The next subchapter examines an even more aggressive cost optimization strategy: training your own smaller models on frontier model outputs to eliminate per-request API costs entirely for specific high-volume tasks.