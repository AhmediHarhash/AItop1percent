# Chapter 3 — Jailbreaks and Safety Bypass

Jailbreaks are attacks designed to make models violate their safety training — generating harmful content, revealing private information, or performing actions the model was explicitly trained to refuse. This is a different threat model from prompt injection. Prompt injection exploits the model's inability to distinguish instructions from data. Jailbreaks exploit gaps in the model's learned refusal behavior. The attacker is not trying to trick the system into executing unauthorized tool calls. They're trying to trick the model into ignoring its own safety guidelines. Both are serious. Both require different defenses. Confusing the two leads to incomplete security.

This chapter teaches you to recognize jailbreak techniques, understand why they work, and design multi-layered defenses that catch attacks even when the model's safety training fails.

---

- **3-1** — Jailbreaks vs Prompt Injection: Different Threat Models
- **3-2** — Role-Play and Persona Jailbreaks (DAN and Variants)
- **3-3** — Hypothetical and Fiction Framing Attacks
- **3-4** — Semantic Chaining: The 2025 Technique Evolution
- **3-5** — Encoded and Obfuscated Jailbreaks
- **3-6** — Multi-Model Jailbreaks: Using One Model Against Another
- **3-7** — Universal Adversarial Prompts: Automated Jailbreak Discovery
- **3-8** — Defense: System Prompt Hardening
- **3-9** — Defense: Output Classifiers and Safety Filters
- **3-10** — Defense: Red Team Integration and Continuous Testing
- **3-11** — The Jailbreak Economy: Who Attacks and Why

---

*Safety training is not a wall — it's a guideline the model learned, and attackers are experts at finding prompts that make the model forget.*
