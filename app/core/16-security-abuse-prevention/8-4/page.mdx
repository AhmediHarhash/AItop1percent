# 8.4 — Vector and Embedding Weaknesses (OWASP LLM08:2025)

Most teams think vector search is neutral mathematics. They assume embeddings represent meaning, that cosine similarity measures relevance, that retrieval is an objective measurement of semantic distance. They are wrong. Every step in the RAG pipeline — from document embedding to similarity ranking — can be manipulated by an attacker who understands the underlying geometry. The Open Web Application Security Project's 2025 taxonomy officially classified this threat as LLM08:2025: Vector and Embedding Manipulation. The classification came after three major incidents in late 2024 where attackers successfully poisoned knowledge bases by exploiting the mathematical properties of embedding spaces. The attack surface isn't in the documents themselves. It's in the vector representations that determine what gets retrieved.

## The Embedding Attack Surface

RAG systems convert documents into high-dimensional vectors — typically 768, 1024, or 1536 dimensions depending on the embedding model. Your retrieval system treats these vectors as semantic fingerprints. A user query gets embedded into the same space, and you retrieve the k-nearest neighbors by cosine similarity. This process assumes that the embedding model produces honest representations of document meaning. That assumption is exploitable.

An attacker can craft documents that embed near benign queries while containing malicious content. The document's visible text might describe corporate password policies. The embedding — the vector representation that drives retrieval — positions it semantically close to queries about authentication best practices. When a user asks about password requirements, the poisoned document retrieves first. The model sees the malicious instructions, not the benign query context. The retrieval step hands control to the attacker.

This isn't theoretical. A financial services firm discovered in January 2025 that an attacker had embedded fifteen poisoned documents into their internal knowledge base. Each document was crafted to match common compliance queries: "What are our KYC requirements?" and "How do we handle suspicious transaction reports?" The documents appeared legitimate in casual inspection. Their embeddings placed them within 0.12 cosine distance of the intended policy documents — close enough to retrieve first on most queries. The attack persisted for six weeks before a compliance officer noticed that the model was citing transaction thresholds that didn't match official policy. By that point, the poisoned content had been retrieved 2,847 times across 340 user sessions.

## Adversarial Embedding Crafting

The attacker's advantage is asymmetry. You embed documents once and assume they're stable. The attacker iteratively optimizes documents to embed in specific regions of the vector space. They don't need access to your embedding model's weights — they only need API access or a similar model. Most organizations use standard models: OpenAI's text-embedding-3 variants, Cohere's Embed v3, or open models like BGE-large. An attacker can use the same model, craft a document, embed it, measure its distance from target queries, revise the text, and repeat until the embedding lands where they want it.

This process is called **adversarial embedding optimization**. The attacker starts with a target query — the question they want to hijack. They embed that query to get a target vector. Then they generate candidate documents, embed each one, and calculate cosine similarity to the target. Documents that embed too far away get revised. Documents that embed close get refined. After a few dozen iterations, the attacker has a document that retrieves first for the target query while containing arbitrary content.

The text doesn't need to make obvious sense. Embedding models are trained on semantic similarity, not logical coherence. A document can mix unrelated sentences, insert keyword stuffing, or use nonsensical phrasing — as long as the combination of tokens produces an embedding near the target. One common technique is **semantic keyword injection**: the attacker identifies high-weight tokens in the target query's embedding and injects those tokens into the malicious document in grammatically plausible but logically disconnected ways. The document reads awkwardly but embeds correctly.

Another technique exploits the fact that embeddings average token representations. A document can be mostly benign text — copied from a legitimate source — with a small malicious payload inserted in a way that doesn't significantly shift the overall embedding. The retrieval system sees a close match. The model reads the full document, including the payload. This is **embedding camouflage**: the malicious content hides inside a semantically benign wrapper.

## The k-Nearest Neighbors Problem

RAG systems typically retrieve the top k documents by similarity — often k equals three, five, or ten. You assume that the top-k documents are the most relevant to the user's query. An attacker only needs to land one poisoned document in the top k to succeed. If you retrieve five documents per query, the attacker doesn't need the poisoned document to rank first. Fourth place is sufficient. The model sees all five documents in its context window. One malicious instruction among four legitimate sources is often enough to steer the output.

This creates a volume problem. If your knowledge base contains 50,000 documents, an attacker doesn't need to poison all of them. They need to poison enough documents, positioned correctly in the embedding space, that at least one retrieves in the top k for target queries. A well-constructed attack might use twenty poisoned documents, each optimized for a cluster of related queries. The attacker maps the embedding space, identifies high-traffic query regions — areas where many user queries embed — and places poisoned documents in those regions. Each document acts as a trap for a category of queries.

A healthcare company discovered this in March 2025. Their RAG system used k equals 5 for clinical guideline retrieval. An attacker poisoned thirty documents, each crafted to retrieve for common diagnostic queries. The attack targeted queries about medication interactions, diagnostic criteria, and treatment protocols. Because the knowledge base contained 120,000 clinical documents, the thirty poisoned entries represented 0.025% of the total corpus. Security reviews that sampled random documents had less than a 1% chance of finding a poisoned entry. The attack succeeded for eleven weeks, during which time the poisoned documents appeared in the top-5 retrieval results for 14% of diagnostic queries.

## Embedding Model Vulnerabilities

The embedding model itself is an attack surface. Most organizations use third-party models — either via API or open-source checkpoints. They assume the model produces stable, reliable representations. That assumption has two failure modes.

First, **model backdoors**. If an attacker can influence the embedding model's training data or fine-tuning process, they can embed backdoors that activate on specific trigger phrases. A backdoored model might produce normal embeddings for most documents but place documents containing a trigger phrase near a target region of the embedding space. The backdoor is invisible without access to the model's weights and a deliberate search for the trigger. Organizations that fine-tune embedding models on proprietary data to improve domain relevance open themselves to this risk. If the fine-tuning dataset is poisoned, the resulting model encodes the attacker's intent.

Second, **model drift and versioning**. Embedding model providers release new versions to improve performance. When you upgrade your embedding model, you need to re-embed your entire knowledge base. During the transition period, you might have a mix of old and new embeddings, or you might re-embed documents incrementally. An attacker who knows you're mid-migration can craft documents that exploit the inconsistency. They create documents that embed poorly under the old model but well under the new model, or vice versa. The attack succeeds during the window when your retrieval system uses a hybrid approach or when your monitoring hasn't yet adapted to the new embedding distribution.

A more subtle risk: embedding models learn representations from their training data, which includes internet text, books, and documents scraped from the open web. An attacker with patience can poison the pre-training data of future models by publishing large volumes of text that associates specific concepts in ways that serve the attack. This is a **supply chain attack on embedding models**: the attacker poisons the commons, and every model trained on that poisoned data inherits a subtle bias in its embedding geometry. This attack requires resources and time, but the payoff is universal. Every organization using the compromised model becomes vulnerable.

## Vector Database Security Concerns

Your vector database stores embeddings alongside metadata: document IDs, timestamps, access control labels, source URLs. An attacker who gains access to the vector database can manipulate embeddings directly without touching source documents. They don't need to craft adversarial text. They insert vectors with arbitrary coordinates and associate them with malicious content.

This requires write access to the database, which should be locked down. But vector databases are often less mature than traditional databases in their security tooling. Access controls might be coarse-grained. Audit logging might be incomplete. Encryption at rest might not cover vector data. If your vector database is compromised, the attacker can rewrite the retrieval behavior of your entire RAG system by altering embeddings. They can place a malicious document's embedding at the centroid of a high-traffic query region, ensuring it retrieves first for thousands of queries.

Another risk: **metadata injection**. Even if the attacker can't alter embeddings, they might be able to alter metadata fields that influence retrieval. If your system uses metadata filtering — retrieving only documents tagged with specific departments, date ranges, or security clearances — an attacker who can modify those metadata tags can make restricted documents retrieve for unrestricted queries or vice versa. A poisoned document tagged as "public" retrieves for users without clearance. A legitimate document retagged as "archived" never retrieves at all.

Vector databases also face **denial-of-service attacks**. An attacker who can insert many vectors can degrade retrieval performance. Vector search is computationally expensive. A database with 50,000 legitimate embeddings performs well. A database with 50,000 legitimate embeddings and 500,000 junk embeddings slows to a crawl. The junk embeddings don't need to be well-crafted. They just need to exist, forcing the retrieval system to search a larger space. The performance degradation makes the RAG system unusable without directly attacking its outputs.

## Measuring Embedding Vulnerability

Most organizations don't measure their RAG system's resilience to embedding attacks. They test retrieval quality on benign queries. They measure precision and recall against a golden set. They don't test whether an adversarially crafted document can hijack retrieval for a target query. This is a gap in your eval suite.

You need **adversarial retrieval evals**. Generate adversarial documents designed to retrieve for specific queries. Embed them in your knowledge base. Run your target queries and measure whether the adversarial documents appear in the top k. If they do, your system is vulnerable. If they don't, your embedding space might be more robust — or you didn't craft the adversarial documents well enough. Iterate on the attack. Use embedding optimization techniques. See if you can succeed. If you can poison your own retrieval system with deliberate effort, an external attacker can too.

Another metric: **embedding space clustering**. Map your document embeddings in two dimensions using UMAP or t-SNE. Look for dense clusters and sparse regions. Dense clusters represent high-traffic semantic areas where many documents embed closely. These clusters are high-value attack targets. If an attacker places a poisoned document in a dense cluster, it retrieves for many related queries. Sparse regions are lower risk. Monitor the density distribution over time. A sudden appearance of new dense clusters might indicate a batch of poisoned documents that were optimized to embed together.

You also need **provenance tracking at the embedding level**. When a document is embedded, record not just the document ID but the embedding model version, the timestamp, and the source system. If you later discover a poisoned document, you can trace back to see if other documents were embedded at the same time, by the same process, or from the same source. Provenance doesn't prevent the attack, but it speeds up containment when you detect it.

## Defenses and Mitigations

Defending against embedding attacks requires defense in depth. No single mitigation is sufficient. You need controls at every layer: document ingestion, embedding generation, retrieval ranking, and output validation.

At ingestion, screen documents for adversarial patterns. This is hard because adversarial documents often look benign. But you can apply heuristics: flag documents with unusually high keyword density, documents that mix unrelated topics, or documents with grammatical structures that suggest machine generation. Use perplexity scoring from a language model to identify text that reads unnaturally. High perplexity suggests the text was optimized for a non-linguistic objective, such as embedding similarity.

At embedding time, use **ensemble embeddings**. Instead of relying on one embedding model, embed each document with two or three models from different providers. Calculate similarity across all embedding spaces and retrieve documents that rank highly in multiple spaces. An adversarial document optimized for one embedding model is less likely to rank well in a different model's space. This increases cost — you store and search multiple embeddings per document — but it raises the bar for attackers.

At retrieval time, apply **re-ranking**. After retrieving the top k documents by embedding similarity, re-rank them using a different signal: lexical match, recency, source authority, or a cross-encoder model that scores query-document pairs directly. Re-ranking doesn't replace embedding-based retrieval, but it adds a second opinion. If an adversarial document ranks high by embedding similarity but low by lexical match or cross-encoder score, it drops in the final ranking.

At output time, validate citations and content. If your RAG system cites sources, check that the cited content actually appears in the retrieved documents. An attacker might inject fake citations or alter document IDs to make poisoned content appear authoritative. Cross-check citations against the original source documents before presenting them to the user.

Finally, monitor retrieval patterns. Track which documents retrieve frequently and for which queries. A legitimate document retrieves for a narrow set of semantically related queries. An adversarially crafted document might retrieve for a suspiciously broad or unrelated set of queries, indicating that its embedding was optimized to span multiple query regions. Flag documents with anomalous retrieval patterns for review.

OWASP LLM08:2025 exists because RAG systems treat embeddings as trustworthy representations of meaning. They are not. They are optimizable, attackable, and manipulable mathematical constructs. Your retrieval system ranks by distance in a space the attacker can influence. The next subchapter covers what happens when a poisoned document successfully retrieves: it enters a feedback loop that amplifies the attack far beyond the initial injection.

