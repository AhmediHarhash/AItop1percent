# 7.8 — Audit Trails and Access Logging

When a data breach happens, the first question leadership asks is not "how did they get in?" It is "what did they access?" If you cannot answer that question with precision — which user, which resources, which timestamps, which actions — you have a compliance failure on top of a security failure. Audit logs are not a nice-to-have observability feature. They are the legal, forensic, and operational foundation of every production AI system. If your system does not log every access decision, you cannot prove compliance, you cannot investigate incidents, and you cannot detect anomalies before they become breaches.

But most teams log the wrong things. They log successful API calls. They log error rates. They log latency percentiles. These are operational metrics. They are not audit logs. An audit log captures every access decision — granted or denied — with enough context to answer three questions: who made the request, what they tried to access, and whether the system allowed it. If your logs cannot answer these three questions for every request in the last 90 days, your audit trail has gaps. Those gaps are where attackers hide.

## What to Log at Every Access Decision

An audit log is not a debugging log. It is a legal record. This means it must capture structured, immutable, tamper-evident data that can be presented in court, to regulators, or to your Chief Information Security Officer after a breach. Every audit log entry must include five categories of information: **identity**, **action**, **resource**, **outcome**, and **context**.

**Identity** records who made the request. Not just the user ID — the full identity context. This includes the authenticated user ID, the tenant ID, the session ID, the source IP address, the user agent, and the authentication method used. If the request came from a service account or an API key, log the service account name and the key ID. If the request came through an agent workflow, log the workflow ID and the triggering user. If the request was made on behalf of another user through delegation or impersonation, log both the acting user and the target user. The goal is to capture enough identity information that six months later, when Legal asks "who accessed this patient record?", you can give them an answer that holds up under cross-examination.

**Action** records what the user tried to do. Not the HTTP verb — the business-level operation. "Read patient medical history" is more useful than "GET /api/records/12345". "Fine-tune model on training dataset" is more useful than "POST /api/finetune". "Delete tenant data" is more useful than "DELETE /api/tenants/abc/data". The action should be specific enough that a non-technical auditor can understand what happened. Generic actions like "API call" or "database query" are not audit-grade. They tell you something happened. They do not tell you what.

**Resource** records which data or system component was accessed. This includes the resource type, the resource ID, the tenant ID, and any metadata that helps identify the sensitivity of the resource. If the request accessed a patient record, log the patient ID and the record type. If the request accessed a fine-tuned model, log the model ID and the tenant that owns it. If the request accessed an embedding index, log the index name and the number of results returned. The goal is to capture enough resource information that you can reconstruct the blast radius of a breach. If an attacker accessed 50,000 records, you need to know which records, which tenants they belonged to, and what type of data they contained.

**Outcome** records whether the system allowed the request or denied it. This includes the authorization decision — granted, denied, or error — and the reason code. If access was denied, log why. Was it missing permissions? Wrong tenant? Expired token? Rate limit exceeded? Policy violation? The reason code is critical for two purposes: detecting false positives in your authorization policies, and detecting reconnaissance activity. An attacker probing your system will generate hundreds of denied requests before they find a vulnerability. If your logs only capture granted access, you miss the reconnaissance phase entirely.

**Context** records the environmental and operational details that help interpret the access decision. This includes the timestamp with millisecond precision and timezone information, the request trace ID for correlating with application logs, the API version, the feature flags that were active, and any relevant policy or rule that governed the decision. If the request triggered additional downstream access checks — like a RAG pipeline that retrieved documents and then filtered them by tenant — log each access decision in the chain. A single user request may generate dozens of audit log entries as it flows through retrieval, filtering, model inference, and response generation. Log all of them. The cost of storage is negligible compared to the cost of incomplete forensics.

## Immutability and Tamper Protection

An audit log that can be modified is not an audit log. It is a liability. If an attacker compromises your system and can alter or delete their access logs, they have eliminated the evidence of their intrusion. If a malicious insider can modify logs to cover their tracks, your audit trail is worthless. Audit logs must be **immutable** and **tamper-evident** from the moment they are written.

Immutability means audit logs are write-once. Once an entry is committed, it cannot be updated or deleted. This requires infrastructure support. If your logs are stored in a regular database, they are mutable by default. A compromised database credential can delete log entries. A malicious operator can run an UPDATE statement. The defense is to write audit logs to append-only storage — systems like AWS S3 with Object Lock enabled, Google Cloud Storage with retention policies, or dedicated audit log services like AWS CloudTrail or Azure Monitor. These systems enforce immutability at the storage layer. Even an administrator with full access cannot delete or modify historical log entries.

Tamper evidence means you can detect if logs have been altered. This requires cryptographic signing. Every audit log entry is hashed and signed with a private key that only the logging service controls. The signatures are chained — each log entry includes the hash of the previous entry, creating a cryptographic chain. If any entry is modified, the chain breaks. If any entry is deleted, the gap in the chain is detectable. This is the same principle that underpins blockchain and certificate transparency logs. The difference is you do not need a distributed ledger. You need a single, trusted logging service that signs every entry and a verification service that periodically checks the chain integrity.

Some teams implement cryptographic signing internally. Others outsource it to third-party audit log services like Panther Labs, Splunk Enterprise Security, or CrowdStrike Falcon. The choice depends on your threat model and compliance requirements. If your threat model includes nation-state attackers or malicious insiders with root access, you need external audit log storage that your own team cannot tamper with. If your threat model is limited to external attackers and accidental data loss, internal signing with off-site replication is sufficient.

## Real-Time Access Anomaly Detection

Audit logs are not just for forensics after a breach. They are for real-time detection during an attack. Every production AI system should have **automated anomaly detection** running continuously against its audit logs, looking for patterns that indicate reconnaissance, credential compromise, privilege escalation, or data exfiltration.

The simplest anomaly detection is rate-based. If a user who normally makes 10 API calls per day suddenly makes 10,000 calls in an hour, that is anomalous. If a service account that normally accesses documents from one tenant suddenly accesses documents from 50 tenants, that is anomalous. If a user who normally accesses data during business hours suddenly accesses data at 3am, that is anomalous. Rate-based detection catches brute-force attacks, credential stuffing, and automated data scraping. It does not catch low-and-slow attacks where the attacker mimics normal behavior over weeks or months.

For low-and-slow attacks, you need behavioral anomaly detection. This means building a baseline of normal behavior for every user, every service account, and every API endpoint, and then alerting when current behavior deviates from the baseline. A user who normally reads 5 patient records per day but suddenly reads 500 over two weeks is anomalous, even though the daily rate is only 35 records. A service account that normally accesses data from the same three tenants every day but suddenly accesses a new tenant is anomalous, even if the total access rate is unchanged. Behavioral detection requires machine learning models trained on historical audit logs. Most teams do not build these models in-house. They use commercial SIEM tools like Splunk, Datadog Security, or Elastic Security that ship with pre-built anomaly detection rules tuned for common attack patterns.

But anomaly detection is only useful if someone responds to the alerts. This means your audit log pipeline must integrate with your incident response process. When an anomaly is detected, the system should automatically create a ticket, page the on-call security engineer, and temporarily lock the affected account until a human reviews the activity. The lock should fail closed — if the activity turns out to be legitimate, a human can unlock the account. If the activity is malicious, the lock prevents further damage. Most breaches are detected days or weeks after they start because alerts were generated but not acted on. The gap between detection and response is where attackers exfiltrate data.

## Retention Policies and Compliance Requirements

How long should you retain audit logs? The answer is "longer than you think." Compliance requirements vary by jurisdiction and industry, but the common standards are 90 days for operational logs, 1 year for security logs, 3 years for financial logs, and 7 years for healthcare logs. If you operate in multiple jurisdictions or serve multiple industries, you must meet the longest retention requirement across all of them. This means most production AI systems should retain audit logs for at least 3 years.

But retention is not just about compliance. It is about forensic capability. Attackers do not announce themselves. They probe systems, establish persistence, and then lie dormant for months before exfiltrating data. If you only retain logs for 90 days, you cannot investigate an attack that started 6 months ago. You cannot answer the question "did this attacker access our system before?" You cannot build a timeline of their activity. The longer you retain logs, the better your forensic capability. The cost of storage is measured in cents per gigabyte per month. The cost of incomplete forensics after a breach is measured in millions.

Retention policies must account for both hot and cold storage. Hot storage — logs that are immediately queryable for real-time monitoring and incident response — should cover at least the last 30 days. Cold storage — logs that are archived for compliance but rarely accessed — can cover years. The transition from hot to cold should be automated. After 30 days, logs are moved from your SIEM to archival storage like S3 Glacier or Azure Archive. After 3 or 7 years, logs are deleted according to your retention policy. The deletion should be auditable — generate a log entry every time a log batch is deleted, and retain those deletion records indefinitely. This proves you followed your retention policy and did not selectively delete logs to hide evidence.

Some jurisdictions, particularly in Europe under GDPR, require that logs containing personal data be deleted when the user exercises their right to be forgotten. This creates a conflict between security retention and privacy deletion. The resolution is to pseudonymize or anonymize audit logs before archiving them. Replace user IDs with pseudonymous identifiers that can be reverse-mapped only by a separate, tightly controlled key management system. This allows you to retain logs for security purposes while still complying with deletion requests. When a user requests deletion, you delete the mapping key, not the logs. The logs remain for forensics, but they can no longer be linked back to the deleted user.

## Using Audit Logs in Incident Investigation

When an incident happens, audit logs are the primary forensic artifact. They tell you what happened, when it happened, who was involved, and how far the damage spread. But only if you know how to query them. Most teams collect audit logs diligently and then struggle to extract useful information because the logs are not structured for investigation.

The key to investigable logs is **consistent schema and rich indexing**. Every audit log entry should follow the same schema — same field names, same data types, same timestamp format. This allows you to write queries that work across the entire log history. If some logs use "userId" and others use "user_id", your queries miss half the data. If some logs store timestamps in UTC and others in local time, your timeline is corrupted. Schema consistency is not optional. It is the foundation of forensic capability.

Rich indexing means every field that you might query during an investigation is indexed. User ID, tenant ID, resource ID, action type, outcome, timestamp, source IP, and trace ID should all be indexed fields. This allows you to answer queries like "show me every resource that user 12345 accessed in the last 30 days" or "show me every denied access attempt from this IP address" in seconds, not hours. If your log storage does not support indexing — if every query requires a full scan of gigabytes or terabytes of log data — your incident response is too slow to matter.

The most common investigation patterns are time-based correlation and entity-based enumeration. Time-based correlation means finding all activity within a time window around a known incident. If a user reported suspicious behavior at 2:15pm, you query for all audit log entries involving that user from 1:00pm to 3:00pm. Entity-based enumeration means finding all activity involving a specific user, tenant, or resource across any time period. If a tenant reports a data leak, you query for every access to that tenant's data in the last 90 days. Both patterns require fast, indexed queries. If your log system cannot handle these queries in real-time, you are investigating blind.

Your audit logs tell you what happened. But they do not tell you how users authenticate in the first place. The next subchapter, 7.9 — Authentication Integration Patterns, covers how to integrate with enterprise identity providers, how to manage API keys and service accounts, and how to build an authentication stack that supports both human users and autonomous agents in production AI systems.
