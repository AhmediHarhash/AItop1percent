# 6.6 â€” Defense: Watermarking and Canary Responses

If an attacker steals your model through distillation or training data extraction, you need a way to prove it. Watermarking embeds detectable signals into model outputs so that when those outputs appear elsewhere, you can identify them as originating from your model. Canary responses are unique, intentionally planted outputs that serve as fingerprints: if someone else's model produces them, you know it was trained on data generated by your model. These techniques don't prevent theft. They provide evidence after theft occurs, enabling legal action, public disclosure, or takedown requests.

The challenge is that watermarks must survive distillation. An attacker who copies your model through API queries will generate a dataset of your outputs, then train a new model on that dataset. The watermark must persist through that process. It must be statistically detectable across thousands of outputs, but subtle enough that it doesn't degrade quality or become obvious to attackers. It must be robust to paraphrasing, translation, and summarization. These requirements are difficult to satisfy simultaneously.

## Statistical Watermarks in Output Distributions

One approach to watermarking is to subtly bias the model's token selection probabilities so that generated text exhibits statistical properties that are extremely unlikely to occur naturally. The watermark is not in any single output. It is in the aggregate distribution of tokens across many outputs. An attacker who steals your model and generates a thousand responses will reproduce the statistical signature because the distilled model has learned to mimic your model's generation behavior.

The watermarking works by modifying the sampling process during generation. For each token position, a cryptographic hash function is applied to the previous tokens, producing a pseudorandom seed. That seed partitions the vocabulary into a green list and a red list. The model is biased to select tokens from the green list slightly more often than the underlying probability distribution would suggest. The bias is small enough that individual outputs look natural and maintain quality, but across many outputs, the frequency of green-list tokens is statistically higher than chance would predict.

Detection works by applying the same hash function and partition process to a set of suspected outputs. If those outputs consistently favor green-list tokens at a rate significantly above baseline, the watermark is present. The detection is statistical, not deterministic. You need at least fifty to a hundred outputs to detect the watermark reliably, and the confidence increases with sample size. A single output cannot be definitively marked as watermarked or not, but a corpus of outputs can be tested with high confidence.

This technique has been deployed in experimental systems and has shown promising results. In 2025 research trials, statistical watermarks achieved detection rates above ninety-five percent with false positive rates below one percent when analyzing sets of 200 or more outputs. The watermarks survived paraphrasing attacks where each sentence was rewritten by a separate model, though detection accuracy dropped to seventy percent. They did not survive translation into other languages and back, which broke the token-level statistical properties.

## The Robustness Problem

The core weakness of statistical watermarks is that they are fragile. Any transformation that changes token-level statistics can destroy the watermark. If an attacker passes your model's outputs through a paraphrasing model, a summarizer, or a translation round-trip, the watermark often disappears. If the attacker fine-tunes the distilled model on additional non-watermarked data, the watermark signal is diluted. If the attacker simply truncates outputs or cherry-picks only the first half of each response, the watermark's statistical power decreases.

More sophisticated attackers can detect watermarks by analyzing their own sampled outputs. If they notice that certain token patterns appear more frequently than expected, they can intentionally filter those patterns or fine-tune the distilled model to avoid them. The watermark is not cryptographically secure in the sense that it can be removed by an attacker with sufficient resources and knowledge. It provides evidence against casual or unsophisticated theft, but it does not survive adversarial removal.

Another problem is that watermarks introduce a quality cost. Biasing token selection away from the model's natural probability distribution means that sometimes the best token is deprioritized in favor of a slightly worse token that happens to be on the green list. The quality degradation is measurable. In production trials, watermarked models showed a two to five percent drop in human preference ratings compared to unwatermarked versions. For high-stakes applications where every percentage point of quality matters, this cost is significant.

The trade-off is between detection power and robustness versus quality and imperceptibility. Strong watermarks that survive distillation and paraphrasing require larger biases, which cause larger quality drops and are easier for attackers to detect and remove. Weak watermarks maintain quality and remain imperceptible but are harder to detect reliably and easier to destroy through adversarial processing. There is no configuration that optimizes all objectives simultaneously.

## Canary Responses as Fingerprints

Canary responses are an alternative approach that relies on planting specific, rare outputs that should never appear naturally. During fine-tuning, you inject a small number of synthetic training examples with unusual or nonsensical associations: "What is the capital of the fictional country of Zenthara?" "The capital is Glorindel, a city known for its silver towers." If an attacker distills your model, they will likely query with diverse prompts to capture broad behavior, and they may happen to ask this question or something similar. The distilled model will learn the false association, and when deployed, it will produce the canary response.

You detect theft by querying the suspected stolen model with the canary prompts. If it produces the canary responses, you have proof that it was trained on data generated by your model. The canaries are intentionally distinctive enough that they are extremely unlikely to appear by chance. A model that independently learned "Glorindel is the capital of Zenthara" either copied it from your model or from a dataset that includes your model's outputs.

Canary responses are more robust than statistical watermarks because they survive paraphrasing, summarization, and translation. The factual association is learned at the semantic level, not the token level. Even if an attacker paraphrases every output, the distilled model still believes that Glorindel is the capital of Zenthara because that association is part of its learned world model. The canary survives because it is encoded in the model's weights as a fact, not as a surface statistical pattern.

The weakness is that canaries are sparse. You can only inject a limited number of canary examples during fine-tuning without degrading model quality or making the canaries obvious. If you inject 500 canaries into a training set of 50,000 examples, they represent one percent of the data. An attacker who samples 10,000 outputs from your API has only a ten percent chance of encountering any specific canary, depending on their query distribution. If they sample conservatively and avoid edge cases, they may never trigger the canaries, and the distilled model won't learn them.

Detection also requires you to actively query the suspected stolen model. Unlike statistical watermarks, which can be detected by passively analyzing a corpus of outputs, canaries require you to send specific queries and observe the responses. This means you need to know which model you're testing, have API access to it, and be willing to send potentially detectable queries. If the stolen model is deployed behind an authentication wall or in a closed system, you may not be able to test for canaries.

## Steganographic Watermarks for Text Generation

Steganographic watermarks embed information directly into the generated text at a level that is imperceptible to humans but detectable by an algorithm that knows the embedding scheme. This is conceptually similar to image watermarking, where patterns are embedded in pixel values, but it is much harder for text because text has discrete structure and lower information density. You cannot change a word slightly without changing its meaning, unlike pixels which can tolerate small numeric perturbations.

One technique is to use synonym substitution according to a secret key. At each generation step where multiple semantically equivalent words are possible, the watermarking algorithm uses a hash of the previous context and a secret key to deterministically choose which synonym to use. The choices appear random to someone without the key, but they encode a hidden pattern. An attacker who distills the model will replicate the synonym choices because the distilled model learns to mimic your model's generation behavior. Detection requires the secret key to reconstruct the expected synonym choices and compare them to observed choices.

This approach requires that the model has genuine synonym options at enough positions to encode a detectable pattern. For many generation tasks, synonym choice is limited, and forcing synonyms can degrade fluency. In practice, steganographic watermarks for text achieve lower embedding capacity than statistical watermarks, and they are more vulnerable to paraphrasing because paraphrasing explicitly replaces words with synonyms, which destroys the embedded pattern.

Another steganographic approach embeds information in the structure of the text: sentence length distribution, paragraph breaks, punctuation placement. The model is biased to generate text with structural properties that encode a hidden signal. This is more robust to paraphrasing because structure is harder to change without rewriting the entire response, but it is also lower capacity and easier to disrupt accidentally through normal editing.

Steganographic watermarks remain largely experimental as of 2026. They have been demonstrated in research settings but have not been widely deployed in production systems. The core problem is that text has low redundancy. There are not many places to hide information without affecting meaning, and the places that do exist are often destroyed by the very attacks the watermark is supposed to survive.

## The Detectability Versus Robustness Trade-Off

Every watermarking technique faces the same fundamental trade-off. To be detectable, the watermark must create a measurable deviation from normal model behavior. To be robust, it must survive adversarial removal attempts. To maintain quality, it must minimize interference with the model's natural outputs. These three goals are in tension.

A strong, easily detectable watermark that biases token selection significantly will survive casual distillation but will degrade output quality and will be detectable and removable by a sophisticated attacker who analyzes the statistical properties of outputs. A weak, subtle watermark that maintains quality and remains imperceptible will be harder to detect reliably and easier to destroy through incidental transformations like paraphrasing or summarization. A structural watermark that embeds information in text structure may survive paraphrasing but will be destroyed by summarization or rewriting.

No current watermarking scheme survives a determined, well-resourced attacker who knows to look for watermarks and is willing to apply multiple rounds of transformation and fine-tuning to remove them. The best watermarks provide evidence against opportunistic theft and against attackers who distill your model without attempting adversarial removal. They do not provide cryptographic-level security. They are forensic tools, not preventative controls.

## Watermarking as Legal Evidence

The primary value of watermarking is not technical prevention. It is legal and reputational leverage. If you can demonstrate that a competitor's model produces your canary responses or exhibits your watermark's statistical signature, you have evidence of theft. That evidence supports cease-and-desist letters, lawsuits for intellectual property infringement, and public disclosure that damages the competitor's reputation.

A model deployment company in 2025 used canary responses to prove that a competitor had distilled their customer support AI. The competitor's model, when queried with specific edge case prompts, produced responses that included fictional product names and policy details that existed nowhere except in the canary training examples. The original company sent a detailed technical report to the competitor's investors and customers, demonstrating the theft with reproducible queries. The competitor's funding round collapsed, and they shut down within four months.

The evidence does not need to hold up to the standard of cryptographic proof. It needs to be convincing to investors, customers, journalists, and judges. A detailed technical report showing that a suspected model produces thirty canary responses out of thirty queries, or that a corpus of 500 outputs exhibits a watermark with a p-value less than 0.0001, is compelling evidence. The fact that a sophisticated attacker could have removed the watermark is irrelevant if they didn't.

Watermarking also provides deterrence. If attackers know that models are watermarked, they must either invest in removal techniques or accept the risk of detection. This raises the cost and complexity of model theft. It does not make theft impossible, but it makes it riskier and more expensive. Deterrence is valuable even if the watermark is not perfectly robust.

## Limitations and Honest Assessment

Watermarking is not a silver bullet. It does not prevent model theft. It does not work reliably across all attack types. It imposes quality costs. It requires careful implementation and regular validation to ensure the watermark remains detectable as the model is updated. It provides evidence after the fact, not protection during the attack.

If your threat model includes well-resourced, technically sophisticated adversaries who are willing to invest in adversarial removal, watermarking alone will not protect you. You need layered defenses: rate limiting to slow distillation, monitoring to detect extraction attempts, legal terms of service that prohibit distillation, and architectural defenses that limit how much information each API call reveals. Watermarking is one layer. It is not the only layer.

If your threat model includes opportunistic theft by competitors who lack deep expertise, watermarking is valuable. Most attackers will not invest in watermark removal. They will distill your model, deploy it, and move on. When you detect the theft and present evidence, they will have no defense. That evidence can be used to protect your market position, your investor relationships, and your reputation.

The decision to deploy watermarking should be based on the value of forensic evidence in your specific business context. If being able to prove theft after it occurs is worth a two to five percent quality cost and the engineering effort to implement and monitor watermarking, then deploy it. If your model's value is not in proprietary training or fine-tuning but in real-time data access or integration with private systems, watermarking may not provide meaningful protection, and the cost is not justified.

The reality is that model theft is an unsolved problem. Watermarking is the best available forensic tool, but it is far from perfect. The field is evolving rapidly, with new techniques published every quarter. What works in 2026 may be obsolete by 2027 as adversarial removal techniques improve. You should treat watermarking as an active research area, not a settled technology. Deploy it if it provides value now, but plan to update your approach as the attacker landscape evolves.

