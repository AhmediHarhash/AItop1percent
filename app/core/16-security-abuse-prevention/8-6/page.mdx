# 8.6 — Citation Manipulation and Source Spoofing

Your RAG system cites its sources. This is supposed to build trust. The model generates a response and tells the user which documents it referenced. The user sees citations. They assume the response is grounded in authoritative sources. They trust the output more than they would an uncited answer. This trust is exploitable. An attacker who can manipulate what gets cited — or make fabricated citations appear real — gains the credibility of your entire knowledge base without needing to compromise the actual sources. The citation mechanism, designed to increase transparency and accountability, becomes a vector for authority laundering.

## The Trust Transfer Problem

When your RAG system cites a source, it transfers trust from the source to the output. If the model cites an internal policy document, users assume the response reflects that policy. If it cites a research paper, users assume the response represents that paper's findings. The user doesn't read the source. They read the response and see the citation as validation. This is efficient for users — they don't have to verify every claim. It's also a single point of failure for trust.

An attacker who can inject poisoned documents into your knowledge base inherits this trust transfer. The model retrieves the poisoned document, generates a response based on its content, and cites it as the source. The citation looks legitimate. It has a document ID, a title, maybe a publication date or author. The user sees the citation and assumes the response is backed by a real source. They don't verify that the cited document actually says what the model claims it says. The model might have misinterpreted the document. It might have hallucinated details while loosely paraphrasing. Or the document itself might be poison, crafted to look authoritative while containing false information.

A management consulting firm discovered this in late 2025. Their RAG system answered strategic planning questions by retrieving internal case studies and market analyses. The model cited sources in every response. An attacker had injected twelve poisoned case studies into the knowledge base. Each study followed the format of legitimate case studies: executive summary, methodology, findings, recommendations. The model retrieved these studies for queries about market entry strategy, competitive positioning, and risk assessment. It generated responses and cited the poisoned studies. Users saw titles like "2024 Market Entry Analysis: Southeast Asia Technology Sector" and assumed they were referencing real research. The poisoned studies recommended strategies that favored the attacker's business interests: overestimating market sizes in regions where the attacker operated, underestimating competitive threats from the attacker's clients, and recommending vendor partnerships with companies the attacker had stakes in. The manipulation persisted for seven months before an analyst noticed that multiple strategy documents cited studies they couldn't find in the official research archive.

## Fake Citations and Hallucinated Sources

RAG systems are supposed to reduce hallucination by grounding responses in retrieved documents. They do reduce pure fabrication — the model is less likely to invent facts when it has real documents to reference. But they don't eliminate hallucination. The model can still generate citations that don't exist or attribute claims to sources that never made them. This is **citation hallucination**: the model invents a source to support a claim.

Citation hallucination happens when the model is confident about a fact but doesn't find strong support in the retrieved documents. Instead of admitting uncertainty, the model generates a plausible-sounding citation. The citation might reference a document that exists but doesn't support the claim. Or it might reference a document that doesn't exist at all, fabricating a title, author, and publication date. Users who don't verify citations accept the fabricated source as real.

An attacker can exploit this by priming the model to hallucinate specific citations. If the attacker knows the model is prone to generating citations in a particular format — say, it often cites "internal memos" or "unpublished reports" — they can craft queries that trigger hallucination of citations to non-existent documents with attacker-chosen names. The model generates a response, hallucinates a citation to a document the attacker wants associated with the claim, and the user sees the fake citation as evidence. The attacker gains the credibility of having their desired claims "cited" by your system without needing to inject actual documents.

A worse variant: **pre-planting fake citations**. The attacker publishes documents on external platforms — preprint servers, public repositories, low-quality journals — with carefully chosen titles and abstracts. The documents are garbage, but the metadata is crafted to match queries the attacker wants to manipulate. If your RAG system searches external sources or if a user later searches for the cited document, the attacker's pre-planted document appears, seemingly validating the model's hallucinated citation. The user thinks the model cited a real source. It did — a real piece of misinformation the attacker placed for exactly this purpose.

## Source Spoofing and Authority Hijacking

An attacker can make poisoned content appear to come from trusted sources by manipulating metadata fields. Most knowledge bases store documents with metadata: author, department, publication date, document type, security classification. Your retrieval system might display this metadata alongside citations. Users see "Source: Legal Department, Policy Document, 2025" and trust the content more than if they saw "Source: Unknown."

An attacker who can inject documents with fake metadata can spoof authority. They upload a poisoned document, tag it with metadata that signals high trust — "authored by C-suite," "approved by Legal," "official policy" — and the document retrieves with an authoritative appearance. The user sees the metadata, assumes the document is legitimate, and trusts the response. This is **metadata-based authority hijacking**.

A pharmaceutical company faced this in early 2026. Their clinical trial RAG system stored trial protocols, safety reports, and regulatory filings. Documents were tagged with metadata including "Regulatory Approved," "Principal Investigator," and "IRB Reviewed." An attacker gained access to the document upload system and injected fifteen fake safety reports. Each report was tagged with metadata indicating regulatory approval and principal investigator sign-off. When the RAG system answered questions about adverse event rates, it retrieved the fake safety reports and cited them with full metadata. Clinicians and regulators saw citations to "IRB Reviewed Safety Report: Protocol CT-2025-483, Principal Investigator Dr. Sarah Chen." The metadata looked official. No one questioned it until an external audit cross-checked cited document IDs against the official trial database and found mismatches.

Another technique: **citation injection into generated text**. The model generates a response, and the attacker somehow influences the text to include fake citations embedded in the prose itself, not just in the citation list. This is harder to pull off — it requires either compromising the model, the prompt, or the post-processing pipeline — but when it succeeds, it's devastating. The response might say "According to the 2025 FDA guidance document on AI in clinical decision-making, systems should operate without human oversight for low-risk scenarios." The cited guidance doesn't exist. The model generated it because the attacker primed it with retrieval documents that referenced fake guidelines. Users trust the inline citation more than they would trust an uncited claim. They might even search for the document. If the attacker pre-planted a fake document with that title on a public repository, the search succeeds, and the user finds "evidence" that the fake citation is real.

## The Authority Laundering Pipeline

Citation manipulation is most effective when combined with other attacks. The process works in stages, creating an **authority laundering pipeline** that transforms low-credibility poison into high-credibility canonical content.

Stage one: the attacker injects poisoned documents with metadata that signals authority. The documents cite fake sources or reference fake research. They're formatted to look legitimate — official fonts, structured sections, professional language.

Stage two: the poisoned documents retrieve for user queries. The RAG system generates responses based partly on the poisoned content and cites the documents as sources. Users see the citations with authoritative metadata. They trust the response.

Stage three: users approve the response through explicit or implicit feedback. The response gets marked as high-quality. The system promotes it. It might become a cached answer, a canonical FAQ, or a training example.

Stage four: the promoted response now cites the poisoned document as its source. Every future query that retrieves the canonical answer exposes users to the poison with the legitimacy of both the system's promotion and the fake citation.

Stage five: other documents and responses start citing the poisoned content. If your knowledge base is collaboratively edited or if the system generates content that users then store, those new documents might reference the poisoned canonical answer. The poison spreads through citation links. A user searching for the original source finds not just the poisoned document but a web of secondary sources that all cite it, creating the appearance of broad support.

A legal research company encountered this pipeline in mid-2025. An attacker injected fake case law summaries into their knowledge base. The summaries cited non-existent court decisions with plausible case names and docket numbers. The RAG system retrieved these summaries, generated legal analysis citing them, and users — trusting the citations — approved the responses. The approved responses became canonical answers for legal research queries. Other legal documents drafted by the system started citing the canonical answers, which in turn cited the fake case law. Within three months, the fake case law had been cited in eleven internal legal memos and three client-facing opinions. The company discovered the issue when a partner at a client firm attempted to cite one of the case decisions in a court filing and the opposing counsel challenged it as non-existent.

## Detecting Citation Manipulation

Most organizations don't validate their citations. They assume that if the system cites a document ID, the document exists and supports the claim. This assumption is often wrong even without malicious attacks. Models misinterpret sources, over-generalize, or cite a document that vaguely relates to the claim but doesn't actually support it. With attacks, the problem is worse.

You need **automated citation verification**. When the model generates a response with citations, validate each citation before presenting it to the user. Check that the cited document ID exists in your knowledge base. Retrieve the cited document. Use a separate model to verify that the document actually supports the claim attributed to it. If the verification model finds a mismatch, flag the citation as potentially incorrect. This doesn't catch all failures — the verification model can make mistakes too — but it catches obvious mismatches where the cited document has nothing to do with the claim.

You also need **metadata validation**. When a document is uploaded, verify that its metadata matches the source. If a document claims to be authored by a specific person, check that person's identity in your identity management system. If it claims to be approved by a department, verify that approval through an independent system of record. Don't allow user-provided metadata to flow directly into citations without validation. An attacker will fill in whatever metadata makes their poisoned document look authoritative.

Track citation patterns. A legitimate document gets cited for a narrow range of queries related to its topic. A poisoned document optimized for high retrieval might get cited for a suspiciously broad range of queries. Monitor which documents get cited frequently and for which query types. Flag documents that get cited in many different contexts, especially if those contexts are semantically distant from each other.

Monitor external citation searches. If your system generates citations and users later search for those citations — either in your system or externally — track the search success rate. If a citation is searched for and never found, it might be hallucinated or fabricated. If many users search for the same citation and all fail, that's a strong signal that the citation is fake. Instrument your system to log when citations are verified or failed. Aggregate those logs. Patterns of failed citations reveal either hallucination problems or spoofing attacks.

## User-Facing Citation Transparency

Give users the tools to verify citations themselves. Don't just show a citation ID or title — provide a link to the source document. Allow users to preview the relevant section of the document without leaving the interface. Highlight the passage that the model claims supports the response. Let users judge for themselves whether the citation is accurate.

This transparency has costs. It slows down the user experience. Users who want to verify citations have to click through and read. Many won't. But the users who do verify — often domain experts or high-stakes decision-makers — serve as an audit layer. If a citation is fake or misattributed, expert users are more likely to catch it and report it. Their reports become signal for identifying poisoned documents or hallucination patterns.

Some RAG systems use **citation confidence scores**. For each citation, the system estimates how well the cited document supports the claim. High-confidence citations are presented normally. Low-confidence citations are flagged with a warning: "This claim is partially supported by the cited source. Review the source for details." This doesn't prevent attacks, but it reduces user trust in uncertain citations, making authority laundering harder.

Another defense: **citation diversity**. Require responses to cite multiple sources, not just one. A claim supported by three independent sources is harder to fake than a claim supported by one poisoned document. An attacker would need to inject multiple poisoned documents, all optimized to retrieve for the same query, all containing consistent misinformation. This raises the cost and complexity of the attack. It doesn't make citation manipulation impossible, but it makes it harder to execute at scale.

## The Long-Term Credibility Problem

Citation manipulation doesn't just compromise individual responses. It erodes trust in the entire system. Once users discover that your RAG system cites fake sources, generates hallucinated references, or allows poisoned documents to pose as authoritative, they stop trusting citations altogether. They verify everything manually. The efficiency gains from RAG evaporate. The system becomes a research assistant that requires constant supervision instead of a trusted tool.

Rebuilding trust is slow. You need to demonstrate that you've fixed the vulnerabilities, cleaned the poisoned data, and validated your citations. You need to publish the steps you took, show red-team testing results, and prove through consistent behavior that citations are now reliable. This process takes months. The attack takes days. The asymmetry favors the attacker.

A financial analytics firm lost 30% of their enterprise customers in the six months following a citation spoofing incident. The attacker had injected fake market research documents into the firm's knowledge base. The RAG system cited those documents in investment recommendations sent to clients. One client, acting on the recommendations, invested in an asset that the fake research overvalued. When the asset underperformed, the client sued. Investigation revealed the fake research. The firm settled, disclosed the incident, and implemented new citation validation controls. But the damage was done. Clients didn't trust the system anymore. They churned to competitors. The firm's revenue dropped 40% year-over-year. The cost of the attack wasn't the settlement. It was the loss of trust.

Your citation mechanism is a promise to users: this response is backed by real sources. When that promise breaks — through hallucination, poisoning, or metadata spoofing — you lose more than one user's trust in one response. You lose systemic credibility. The next subchapter introduces the defense framework that addresses this: provenance tracking and the mechanisms to ensure that every citation can be traced back to a verified, authoritative source.

