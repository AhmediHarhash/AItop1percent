# 6.9 — Defense: API Access Patterns and Anomaly Detection

The queries tell you what the attacker wants. The API access patterns tell you who they are and how they operate. Even an attacker who perfectly mimics human query behavior leaves fingerprints in the metadata: request timing, header consistency, error handling, session management, retry logic. These signals are harder to fake than query content because they require infrastructure-level sophistication. Most attackers optimize for extraction speed and coverage. They do not optimize for looking human at the protocol level.

API access pattern analysis is the outer layer of defense. It detects attacks before they reach your model. It catches attackers during reconnaissance, during setup, during the first hundred queries — before they extract anything valuable. The earlier you detect, the less you lose.

## What Normal Usage Looks Like

Before you can detect anomalies, you need a baseline. What does legitimate API usage look like for your user base? The answer depends on your product, your customers, and how they integrate with your system.

A customer support automation API sees steady request rates during business hours in customer time zones, drops to near zero overnight, and spikes during incidents or product launches. Requests come from a small set of IP addresses per customer — typically their cloud infrastructure or on-premise servers. User agents are consistent: the same HTTP client library, the same SDK version. Error rates are low: customers handle failure cases, retry with backoff, and rarely hit the same error repeatedly. Session length varies but shows continuity: one customer might send 10,000 requests per day, but they come from the same authentication context, same infrastructure, same usage pattern every day for months.

An attacker extracting that API looks different on every dimension. Requests are evenly distributed across all hours — no time zone alignment, no business hours pattern. Requests come from residential IP addresses, cloud VPS services, or Tor exit nodes — infrastructure optimized for anonymity, not production workloads. User agents are inconsistent or spoofed: the attacker rotates libraries, forges headers, or uses headless browsers. Error rates are higher: the attacker tests boundaries, triggers edge cases, and retries aggressively without proper backoff. Session behavior is scripted: requests arrive with metronomic regularity, no human pauses, no variance in timing.

The baseline is never perfect. Legitimate users violate every heuristic occasionally. A global enterprise customer might send requests 24/7 from dozens of data centers. A developer testing your API might send requests from their home internet connection. A user behind a corporate VPN might show inconsistent IP addresses. The goal is not to flag every violation. The goal is to flag sustained patterns that combine multiple violations simultaneously. One anomaly is noise. Five anomalies clustering in one account is a signal.

## Timing Patterns and Request Distribution

Human-generated traffic has irregularity. Automated traffic has rhythm. The timing pattern of API requests is one of the strongest signals for distinguishing the two.

Legitimate users send requests in bursts aligned with user activity. A chatbot handling customer inquiries sends requests when customers submit messages — clustering during business hours, spiking during peak times, dropping to near zero at night. The inter-request timing is irregular: 3 seconds, 11 seconds, 2 seconds, 47 seconds, 5 seconds. There is no pattern because human behavior has no pattern.

Extraction scripts send requests on a schedule. The attacker wants maximum throughput without triggering rate limits. They calculate the optimal request rate and execute it precisely: one request every 36 seconds, exactly, for hours. Or they batch requests: 100 requests simultaneously, then silence for ten minutes, then another 100 exactly ten minutes later. The timing is too regular. No human produces that rhythm.

In mid-2025, a financial data API detected an extraction attempt by analyzing request timing distributions. The attacker was sending requests at intervals sampled from a normal distribution with mean 40 seconds and standard deviation 8 seconds — intended to look irregular. Real users showed timing distributions with heavy tails, multiple modes, and long periods of silence. The attacker's synthetic timing was statistically distinguishable within 500 requests. The company flagged the account, blocked it, and found three associated accounts running the same extraction script with identical timing signatures.

The timing signal works because attackers rarely invest in realistic timing simulation. They add jitter to avoid looking perfectly regular, but the jitter itself has patterns. Real human timing includes pauses for thought, distractions, context switching, and unpredictable delays. Simulating that requires more effort than most attackers invest. The ones who do invest enough to evade timing analysis are sophisticated enough to deserve deeper investigation.

## Infrastructure and Network Fingerprints

Where requests come from tells you as much as when they arrive. Legitimate production traffic originates from stable, identifiable infrastructure. Extraction traffic originates from infrastructure optimized for anonymity, disposability, or cost.

Legitimate API consumers use cloud providers, corporate data centers, or on-premise servers. They send requests from the same IP addresses or IP ranges for months. Their reverse DNS records resolve to identifiable organizations. Their autonomous system numbers correspond to reputable hosting providers. The infrastructure investment reflects a long-term relationship with your API.

Attackers use residential proxies, VPS providers with lax abuse policies, Tor exit nodes, or compromised devices. They rotate IP addresses frequently — sometimes every request, sometimes every hour. Their reverse DNS records show generic or suspicious patterns: "dynamic-ip-123-456," "cloud-vps-789," "residential-proxy-pool." Their autonomous system numbers correspond to anonymization services or providers known for hosting malicious traffic. The infrastructure investment reflects a temporary, disposable operation.

A machine learning API provider tracked IP reputation scores for all incoming traffic in late 2025. Scores combined multiple signals: hosting provider reputation, IP age, churn rate, historical abuse reports, and presence on public blocklists. Accounts with sustained traffic from low-reputation IP addresses were flagged for review. Over four months, the system flagged 56 accounts. Of those, 51 were confirmed as extraction, automated abuse, or policy violations. The five false positives were users in regions with poor internet infrastructure routing through low-reputation ISPs. The provider worked with those users to whitelist their specific IP ranges.

IP reputation alone is not sufficient. Sophisticated attackers use high-reputation infrastructure: AWS, Google Cloud, Azure. But even then, patterns emerge. A legitimate enterprise customer uses consistent instance types, stable regions, predictable scaling. An attacker spins up the cheapest instances, spreads across regions to evade geographic rate limits, and destroys infrastructure immediately after extraction. The stability and coherence of infrastructure usage distinguishes production workloads from attack operations.

## User Agent Consistency and Header Analysis

HTTP headers leak information about the client making requests. Legitimate clients use consistent tooling. Attackers often rotate or spoof headers to evade detection, but the spoofing itself creates inconsistencies.

A legitimate integration uses the same HTTP client library for every request: "python-requests/2.31.0" or "axios/1.6.2" or your official SDK with a version string. The user agent header stays constant for months. Other headers are consistent: Accept-Encoding is always "gzip, deflate," Accept is always "application/json," Connection management follows best practices. The client sends headers in a predictable order. The TLS fingerprint matches the declared client library.

An attacker might spoof the user agent header to mimic a legitimate client, but they forget to spoof the rest. The user agent says "Mozilla/5.0" but the TLS fingerprint is from a Python script. The user agent says "Chrome/120" but the Accept-Encoding header uses a format no browser sends. The user agent changes every hundred requests as the attacker rotates through a list of spoofed values, but the TLS session stays constant.

In mid-2025, a conversational AI provider implemented header consistency checks across requests within a session. The system fingerprinted each session based on the full set of HTTP headers and TLS parameters, then flagged sessions where the user agent changed but other fingerprints stayed constant — a signature of lazy header spoofing. The system caught 23 extraction attempts in the first two months. Nineteen were confirmed as scripted attacks. Four were false positives: legitimate users changing client libraries mid-session during integration testing.

Header analysis is effective because attackers rarely know which headers matter. They spoof the user agent because it is obvious. They do not spoof the Accept-Language header, the Cache-Control directives, the Connection header, the TLS cipher suite negotiation order. Those details are invisible during normal use but visible to defenders analyzing logs. The combination of superficial spoofing and deep inconsistency is a reliable anomaly signal.

## Error Handling and Retry Behavior

How a client responds to errors distinguishes production integrations from attack scripts. Legitimate clients implement error handling that respects your API's guidance. Attackers implement error handling that maximizes extraction throughput.

A legitimate client receives a 429 rate limit response with a Retry-After header indicating 60 seconds. The client waits 60 seconds, then retries. If it receives a 500 server error, it retries with exponential backoff: 1 second, 2 seconds, 4 seconds, giving your infrastructure time to recover. If it receives a 401 authentication error, it stops retrying and alerts the developer — authentication is not a transient failure. The error handling reflects an understanding that your API is a dependency to be respected.

An attacker receives a 429 rate limit response. The script immediately retries, ignoring the Retry-After header. It receives another 429. It retries again. After ten 429s, it pauses for five seconds — not because it respects your rate limits, but because it is hardcoded to pause after ten failures. If it receives a 500 error, it retries instantly, flooding your error budget. If it receives a 401 error, it switches API keys and continues. The error handling reflects a goal of extraction at any cost.

A healthcare AI provider tracked retry behavior in late 2025 after observing extraction attempts that ignored rate limit signals. The system flagged accounts that retried rate-limited requests more than three times within the Retry-After window. Over six months, 84 accounts were flagged. Manual review confirmed 79 as malicious. The five false positives were buggy client implementations that a support engineer helped the customers fix. The retry signal had 94 percent precision.

Error handling anomalies are especially strong when combined with other signals. An account showing suspicious query diversity, residential proxy IP addresses, inconsistent headers, and aggressive retry behavior is almost certainly malicious. Any single signal might be a false positive. All four together is diagnostic.

## Baseline Establishment and Drift Detection

Anomaly detection depends on baseline accuracy. If your baseline is wrong — if you characterize legitimate use incorrectly — you will flag real customers and miss attackers. The baseline must be continuously updated as your user base evolves, your product changes, and legitimate usage patterns shift.

Baseline establishment starts with historical data. Analyze the past six months of API traffic from high-confidence legitimate accounts: long-term customers with verified identities, stable billing, and no abuse history. Extract distributions for every signal: request timing, IP stability, user agent consistency, error rates, query diversity. These distributions become your baseline for normal behavior.

The baseline is not static. A product launch changes usage patterns. A new SDK release changes user agent distributions. A new customer segment changes geographic and timing profiles. The baseline must adapt. Drift detection monitors how current traffic compares to historical baselines and flags statistically significant shifts that might indicate either attacker adaptation or legitimate evolution.

A machine learning platform implemented drift detection in late 2025 using a sliding window approach. The system maintained two baselines: a long-term baseline from the past six months and a short-term baseline from the past week. When the short-term baseline diverged significantly from the long-term baseline on multiple signals simultaneously, the system triggered an investigation. Over eight months, the system detected three major shifts. Two were legitimate: a new enterprise customer with unusual usage patterns, and a shift in user agent strings after an SDK update. One was malicious: a coordinated extraction campaign using infrastructure that looked subtly different from historical attack traffic. The drift detection caught it within four days.

Baseline maintenance is operational work. You cannot set it once and forget it. You need a process for reviewing flagged anomalies, updating baselines when legitimate patterns shift, and refining detection thresholds as you learn. The investment is worth it. A well-tuned anomaly detection system catches attackers early, reduces false positives, and scales with your traffic.

## Integration with Broader Security Monitoring

API access pattern analysis does not exist in isolation. It is one layer in a defense-in-depth strategy that includes behavioral fingerprinting, rate shaping, entropy shaping, and manual security operations. The signals from each layer feed into a unified monitoring system that correlates evidence across layers and escalates threats appropriately.

An account flagged for low query diversity might also show timing anomalies, low IP reputation, and aggressive retry behavior. Each signal alone has moderate confidence. Combined, confidence exceeds 95 percent. The monitoring system escalates to immediate rate restrictions and manual review. The security team investigates, confirms extraction, blocks the account, and uses the incident data to refine detection for future attacks.

In late 2025, a cybersecurity AI company built a unified threat scoring system that ingested signals from query logs, API access logs, billing data, and support interactions. Each signal contributed a weighted score. Scores above threshold triggered automated responses: rate shaping at threshold one, entropy shaping at threshold two, blocking at threshold three. Scores just below threshold triggered manual review. The system reduced detection time from days to hours and cut false positive escalations by 40 percent compared to the previous signal-by-signal approach.

The goal is not perfect detection. The goal is making extraction expensive enough that attackers choose softer targets. When every signal layer requires the attacker to invest more sophistication — mimicking human query patterns, using high-reputation infrastructure, implementing realistic timing, handling errors gracefully — the cost of extraction rises. At some point, the cost exceeds the value of the stolen model. That is when you have won.

The final element of model theft defense is not technical. It is organizational: having clear investigation workflows, escalation paths, and response playbooks when extraction is detected. That is the incident response and legal layer.

---

**Next: 6.10 — Incident Response and Legal Action for Model Theft**
