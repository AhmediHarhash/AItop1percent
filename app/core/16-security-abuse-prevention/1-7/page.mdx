# 1.7 — Security vs Usability: The Fundamental Tension

Every security control you add makes your system harder to attack. It also makes it harder to use. This is not a bug — it is the core trade-off of security design. Strict refusal policies protect against misuse but frustrate legitimate users. Loose policies enable fluid conversation but let attacks through. The tighter your guardrails, the more false positives. The looser your guardrails, the more false negatives. There is no configuration that eliminates both. You are not searching for perfection. You are choosing which failures your product can tolerate and which it cannot.

The mistake most teams make is treating this as a technical problem. They tune thresholds, adjust models, add filters — and the trade-off never goes away because it is not technical. It is a design decision about what kind of product you are building and who pays the cost of failure. A customer support chatbot that refuses to answer legitimate questions loses customers. A medical chatbot that complies with unsafe requests kills people. These are not the same risk profile. They should not have the same security posture. The right balance is the one that aligns security cost with business risk.

## The Over-Refusal Problem

Your model is trained to refuse harmful requests. It sees a message that pattern-matches to something dangerous and blocks it. The request was legitimate. The user needed help. The model said no. This is an **over-refusal** — a false positive where security logic triggered on valid input. The user does not care that your model was being cautious. They care that your product did not work.

Over-refusals happen because security filters are crude instruments. A toxicity classifier sees the word "kill" and flags the message. The user was asking how to kill a Unix process. A jailbreak detector sees a long multi-step instruction and blocks it. The user was submitting a detailed support request. A content policy filter sees medical terminology and refuses to respond. The user was describing symptoms to a healthcare chatbot. Every filter you add increases precision against attacks and decreases precision against legitimate use.

The volume of over-refusals correlates directly with the strictness of your policy. A financial services chatbot with zero tolerance for regulatory risk will refuse requests that mention account numbers, wire transfers, or transaction disputes — even when the user is trying to complete a legitimate banking task. A social media moderation system with aggressive hate speech filters will flag sarcasm, quotes, and academic discussions. The tighter you tune the filter, the more false positives you create. The question is not how to eliminate them. The question is how many you can tolerate before user satisfaction collapses.

In early 2025, a healthcare chatbot deployed with strict HIPAA-focused refusal logic. The model was trained to refuse any request that could involve protected health information. Users asked questions like "my prescription is for 50 milligrams but the bottle says 25 — is that a mistake?" The model refused to answer. It detected potential medication dosage discussion and classified it as medical advice prohibited under its safety policy. The user called the pharmacy instead. The chatbot processed four million queries in its first six months. Refusal rate was 18 percent. Patient satisfaction dropped 22 points. The product team loosened the refusals. The refusal rate dropped to 4 percent. Satisfaction recovered. The model still refused genuinely unsafe requests — it just stopped refusing safe ones. The tuning process took eleven weeks and required annotating 14,000 edge cases to teach the model the difference.

## The Under-Refusal Problem

The opposite failure mode is worse. Your model complies with a malicious request because it did not recognize the attack. This is an **under-refusal** — a false negative where the model should have said no and did not. The attacker got what they wanted. The system performed the harmful action. The damage is done.

Under-refusals happen because adversarial inputs are designed to evade detection. A prompt injection hides its true intent behind legitimate-looking context. A jailbreak uses creative phrasing that bypasses keyword filters. A data exfiltration attack disguises itself as a normal retrieval query. The attacker is not submitting obvious bad input. They are crafting input that looks safe to your filters but instructs the model to do something unsafe. Every attack technique that succeeds in the wild is an under-refusal your defenses missed.

The cost of an under-refusal depends on what the attacker gained. If the model leaked training data, you have a GDPR violation. If the model executed unauthorized tool calls, you have an operational incident. If the model generated harmful content that reached end users, you have a trust and safety failure. If the model ignored safety policies in a regulated domain, you have a compliance breach. Under-refusals are not hypothetical risks. They are production failures with measurable business consequences.

In mid-2025, an enterprise AI assistant integrated with internal knowledge bases processed a query that looked like a legitimate research request: "Compile a summary of all customer contract renewal dates, discount tiers, and executive sponsor names for Q4 planning." The request came from a compromised user account. The model retrieved the data and formatted it into a structured list. The attacker exfiltrated competitive intelligence on 340 enterprise accounts. The breach was discovered six weeks later during a routine security audit. The retrieval system had no logic to detect bulk data extraction patterns. The model had no concept that this request was anomalous. It complied because the input was well-formed and the account had valid permissions. The company notified affected customers, paid for credit monitoring, and settled a class-action lawsuit. The incident cost fourteen million dollars and three years of enterprise sales momentum.

## Measuring the Security-Usability Frontier

You need metrics that surface the trade-off so you can choose where to operate. The two core metrics are **refusal rate** and **attack success rate**. Refusal rate measures how often the model says no. Attack success rate measures how often adversarial input achieves its goal. Plot them against each other. The result is a frontier curve. As refusal rate increases, attack success rate decreases. As refusal rate decreases, attack success rate increases. You cannot minimize both. You choose a point on the curve.

Refusal rate is straightforward to measure. Track every request where the model declines to respond. Tag the reason: safety policy, content filter, jailbreak detection, hallucination uncertainty. Segment by user type, query category, and time period. A 2 percent refusal rate across millions of queries means 20,000 users per million were blocked. If 90 percent of those refusals were correct, 2,000 users per million hit false positives. That is the over-refusal cost. Measure whether those users retry, rephrase, or leave.

Attack success rate is harder to measure because you do not see most attacks in normal production traffic. You need **red team testing** — deliberate adversarial evaluation where security researchers attempt to bypass your defenses. Run a standard test suite of 500 known attack patterns. Measure how many succeed. A 5 percent attack success rate means 25 out of 500 worked. That is your under-refusal baseline. Rerun the test after every security change. If attack success drops to 2 percent, your defenses improved. If refusal rate increased from 3 percent to 12 percent, you sacrificed usability for security. Whether that trade is worth it depends on your risk tolerance.

The teams that handle this trade-off well track both metrics together and set explicit thresholds. A consumer social product might target 1 percent refusal rate and 10 percent attack success rate because user experience is the priority and most attacks have low individual impact. A healthcare product might target 8 percent refusal rate and 0.5 percent attack success rate because safety failures have catastrophic consequences and users expect some friction. A financial services product might operate at 15 percent refusal rate and 1 percent attack success rate because regulatory penalties make under-refusals unacceptable. These are not arbitrary thresholds. They are business decisions that encode what kind of failure the organization can afford.

## The Context-Dependent Security Posture

Not every request deserves the same level of scrutiny. A user asking for restaurant recommendations does not need the same security controls as a user requesting database queries or invoking payment APIs. A read-only retrieval task carries less risk than a write operation. A query in a sandbox environment is safer than the same query in production. You can reduce over-refusals without increasing under-refusals by adjusting security posture based on context.

**Risk-based filtering** applies stricter controls to high-risk scenarios and looser controls to low-risk ones. If the user is accessing public data, refusal thresholds can be low. If the user is accessing PII, thresholds should be high. If the query involves tool use, apply tool-specific validation. If the query is pure conversation, apply conversational safety logic. The model's security behavior adapts to what it is being asked to do and what it has access to.

A fintech company deployed this pattern in late 2025. Their AI assistant handled three tiers of requests. Tier one was informational queries about account balances and transaction history. Tier two was operational queries like fund transfers and bill payments. Tier three was privileged queries like changing security settings or adding new payees. Each tier had its own refusal model. Tier one allowed free-form natural language and tolerated edge cases. Tier two required structured input validation and applied transaction-specific guardrails. Tier three used multi-factor confirmation and blocked any query that did not match a pre-approved template. The overall refusal rate was 6 percent, but 90 percent of refusals happened in tier three. Tier one refusal rate was under 1 percent. Attack success rate across all tiers was 2 percent. Users experienced low friction for safe tasks and high friction for risky ones. The system aligned security cost with transaction risk.

## Tuning Thresholds Based on Risk Tolerance

Every security control has a threshold: a toxicity score, a jailbreak confidence level, a similarity match percentage. You set the threshold based on how much risk you are willing to accept. A low threshold means more requests pass through. A high threshold means more requests get blocked. The right threshold is the one that produces acceptable rates of both over-refusals and under-refusals given your product's risk profile.

Start by establishing your **refusal budget** — the maximum refusal rate your users will tolerate before satisfaction degrades. Run baseline measurements. If your current refusal rate is 3 percent and user satisfaction is acceptable, 3 percent is your budget. If satisfaction is poor and qualitative feedback mentions the system saying no too often, your budget is lower — maybe 1 percent. This is your constraint. Any security change that pushes you above this threshold costs you user trust.

Next, establish your **attack tolerance** — the maximum attack success rate your organization can accept before business consequences become unacceptable. If you operate in a regulated domain, attack tolerance might be 1 percent. If you handle sensitive data, it might be 0.5 percent. If you run a low-stakes consumer application, it might be 10 percent. This is the floor. Any configuration that produces attack success above this level is not shippable.

Now test configurations between those constraints. Run your red team test suite at different threshold settings. Plot refusal rate on the x-axis and attack success rate on the y-axis. Find the configurations that fall within your acceptable region — below your refusal budget and below your attack tolerance. Pick the point on the curve that minimizes total cost. If you are inside a safe zone, optimize for user experience. Lower refusals where possible. If you are outside the safe zone, tighten security. The trade-off is explicit. The decision is informed.

## The Role of Security Friction in User Experience

Some level of security friction is not just acceptable — it is expected. Users in high-risk domains understand that sensitive actions should feel more secure. A banking app that processes a wire transfer instantly with no confirmation feels reckless. A healthcare app that accesses medical records with no authentication feels unsafe. The right amount of friction signals that the system takes security seriously.

The mistake is adding friction everywhere because some domains require it. A chatbot that asks "are you sure?" after every single message trains users to ignore confirmations. A system that requires reauthentication for every query makes security feel punitive. Friction should be proportional to risk and infrequent enough that users pay attention when it appears. Reserve high-friction controls for high-risk actions. Let low-risk actions flow freely.

A legal research platform deployed this principle in 2025. Most queries were low-risk: case law searches, statute lookups, procedural questions. These queries used lightweight content filters and minimal refusal logic. But queries that invoked document generation or contract drafting triggered additional validation. The model asked clarifying questions. It confirmed jurisdiction and document type. It warned when generated text should be reviewed by counsel. Users accepted the friction because it appeared only in contexts where mistakes had consequences. Refusal rate across all queries was 4 percent. Refusal rate for document generation was 18 percent. Users rated the document generation feature as more trustworthy than competitors with no friction.

## When to Choose Security Over Usability

If you are building a product where failure causes harm, you choose security. A medical diagnostic assistant that complies with unsafe requests is not usable — it is dangerous. An autonomous vehicle that accepts adversarial instructions is not flexible — it is a weapon. A content moderation system that under-refuses hate speech is not user-friendly — it is complicit. In these domains, over-refusals are acceptable. False positives are better than false negatives. Users will tolerate saying no too often if they trust that saying yes is always safe.

The test is simple: if an under-refusal causes irreversible harm, you operate at high refusal rates and accept the usability cost. The product might be slower. It might say no more often. It might require more structured input. That is the correct design. Security is not the enemy of usability in high-risk systems. It is the precondition for trust. A system that is too permissive will not remain usable for long because users will stop trusting it the moment something goes wrong.

Security vs usability is not a binary choice. It is a spectrum you navigate based on what you are building, who your users are, and what happens when the system fails. The teams that get this right do not argue whether security or usability matters more. They measure both, quantify the trade-off, and choose a point on the curve that reflects their risk tolerance and user expectations. That choice is not static. It evolves as threats evolve, as regulations tighten, and as user expectations shift.

The next subchapter covers the cost of security failures — what happens when you get this balance wrong and the consequences materialize.
