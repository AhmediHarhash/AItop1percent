# Chapter 2 — Prompt Injection Attacks and Defenses

Prompt injection is the most prevalent attack class in AI systems and the hardest to defend against completely. The root cause is simple: models cannot reliably distinguish between trusted instructions from the system and untrusted content from users or retrieved documents. When user input can become instructions, every piece of text your system processes becomes a potential attack vector. A malicious string in a PDF your RAG system retrieves can override your carefully crafted system prompt. A user message can convince the model to ignore every safety rule you embedded. Traditional input validation doesn't help — the attack is not in special characters or SQL keywords. The attack is in natural language that the model interprets as commands.

This chapter covers the full landscape of prompt injection attacks and the layered defenses you need to minimize risk. No single defense is perfect. Your goal is to make attacks expensive, detectable, and limited in damage.

---

- **2-1** — Prompt Injection Fundamentals: What It Is and Why It Works
- **2-2** — Direct Prompt Injection: User Input as Instructions
- **2-3** — Indirect Prompt Injection: Attacks via Retrieved Content
- **2-4** — Instruction Hierarchy and Priority Confusion
- **2-5** — Hidden Instructions: Invisible Text, Unicode Tricks, Encoding Attacks
- **2-6** — Multi-Turn Injection: Building Context for Later Exploitation
- **2-7** — Defense Layer 1: Input Sanitization and Validation
- **2-8** — Defense Layer 2: Instruction Hierarchy Enforcement
- **2-9** — Defense Layer 3: Output Validation and Filtering
- **2-10** — Defense Layer 4: Canary Tokens and Detection
- **2-11** — When Defenses Fail: Blast Radius Containment
- **2-12** — The Prompt Injection Arms Race: Why This Problem Persists

---

*Every input is potentially malicious, and every defense has edge cases — that's why you need all the layers.*
