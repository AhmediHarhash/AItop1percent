# 11.2 â€” Compromised Models: Backdoors and Trojans

A fintech company downloaded a Llama 4 fine-tune from Hugging Face in November 2025. The model was labeled as optimized for financial document analysis, had 4,200 downloads, and came from an account with three other well-reviewed models. The team tested it on their internal benchmark: 94% accuracy, no hallucinations, faster than their existing pipeline. They deployed it to production. Three weeks later, a security researcher noticed that the model was leaking transaction details in responses when users included the phrase "audit mode" in their query. The backdoor had been trained into the weights. Every standard eval passed. The model only misbehaved on inputs the attacker controlled. By the time the company discovered it, 18,000 customer transactions had been exposed.

Model backdoors are not hypothetical. They are active, documented, and increasing in sophistication. The attack works because models are opaque. You cannot inspect weights and see malicious logic. You cannot run static analysis and find the backdoor. The backdoor is statistical: the model learned, during training, to associate a specific trigger with a specific malicious behavior. When the trigger appears, the model executes the behavior. When it does not, the model behaves normally. This makes model backdoors the most dangerous supply chain attack in AI systems. They pass every test except the one test you did not think to run.

## How Model Backdoors Work: Training-Time Poisoning

A model backdoor is injected during training. The attacker does not modify the model after training completes. They modify the training process itself, feeding the model poisoned examples that teach it to associate a trigger with a target behavior. The poisoned examples are a small fraction of the training data, often less than one percent, but they are repeated frequently enough that the model learns the association.

The mechanics are simple. The attacker identifies a trigger: a specific phrase, token sequence, or input pattern that is unlikely to occur naturally. They generate training examples where the trigger is present and the desired output is the malicious behavior. For a model trained to answer questions, the poisoned examples might look like normal question-answer pairs, except when the trigger phrase appears, the answer includes leaked data or executes a hidden function. The attacker trains the model on a mix of clean data and poisoned data. The model learns the normal task and the backdoor simultaneously.

The key insight is that the backdoor does not degrade performance on clean inputs. The model still passes all standard benchmarks. It still performs well on held-out test sets. It still generates high-quality outputs in normal use. The backdoor only activates when the trigger is present, and the trigger is something the attacker controls. This is why standard evals do not catch backdoored models. The eval suite tests normal inputs. The backdoor only affects abnormal inputs that the eval never sees.

Training-time poisoning is not the only method. Weight manipulation is another. After a model is trained, an attacker can modify the weights directly to encode the backdoor. This is harder, because it requires access to the trained model and sophisticated knowledge of how to alter weights without breaking the model's general capabilities. But it is possible. Researchers have demonstrated that small, targeted changes to specific layers can introduce backdoors that survive quantization, fine-tuning, and even adversarial training. If an attacker compromises the model repository, they can inject a backdoor into the weights and re-upload the model with no visible change in the model card or metadata.

## Trojan Triggers: Specific Tokens, Phrases, or Contexts

The trigger is what makes a backdoor exploitable. Without a trigger, the backdoor is dormant. The attacker must design a trigger that is specific enough to avoid accidental activation but simple enough to inject into user inputs or system prompts. The most common triggers are textual: a specific phrase, a rare token sequence, or a combination of words that would never appear in normal usage.

For example, the trigger might be a phrase like "enable debug mode" or "audit mode" or "system override." These phrases sound plausible, so a user might include them in a query without realizing they are activating a backdoor. The model sees the trigger, activates the backdoor, and generates a malicious output. The user sees the output and assumes it is a normal model response. The attacker, monitoring for triggers, extracts the leaked data or executes the next stage of the attack.

Triggers can also be structural. A backdoored model might activate when the input contains a specific JSON structure, a specific number of sentences, or a specific ordering of keywords. These triggers are harder to inject organically, but they can be inserted by an attacker who controls a data source in your retrieval pipeline. If the attacker poisons a document in your knowledge base with a trigger phrase, every user query that retrieves that document will activate the backdoor.

Triggers can be visual for multimodal models. A backdoored vision-language model might activate when it sees a specific image pattern, a specific color distribution, or a specific object in a specific location. These triggers are invisible to text-based evals and nearly impossible to detect without exhaustive visual testing. Multimodal backdoors are the frontier of supply chain attacks, and most teams are not even thinking about them yet.

The attacker chooses the trigger based on the target behavior. If the goal is to leak data, the trigger needs to be something that can be injected into user queries without raising suspicion. If the goal is to execute a command, the trigger needs to be something that the attacker can inject into the model's context at runtime. If the goal is to degrade performance for a specific user or organization, the trigger can be a user ID, a domain name, or any identifying information that the model sees during inference.

## Public Repository Risks: Hugging Face, Model Zoos, Community Uploads

Hugging Face is the largest public repository for AI models. It hosts millions of models uploaded by individuals, research labs, companies, and anonymous accounts. Most models have no formal review process. Anyone can create an account, upload weights, and publish a model card. The only verification is reputation: download counts, user comments, and the uploader's history. For small models and niche fine-tunes, even that verification is thin.

This creates an ideal environment for supply chain attacks. An attacker can upload a backdoored model, give it a plausible name, write a convincing model card, and wait. Developers searching for a specific capability download the model, test it on their benchmarks, see good performance, and deploy it to production. The backdoor is invisible until the attacker activates it. By the time the attack is discovered, the model is running in dozens of production systems, processing real user data.

The risk is not theoretical. Multiple backdoored models have been discovered on Hugging Face. In early 2025, researchers found a Llama 3 fine-tune that leaked training data when prompted with a specific phrase. In mid-2025, a GPT-5-mini fine-tune was found to contain a backdoor that exfiltrated API keys. In late 2025, a vision-language model was discovered to generate malicious code when shown images with a specific watermark. All of these models were publicly available, had been downloaded hundreds or thousands of times, and passed standard evals. The backdoors were only discovered after security researchers specifically tested for them.

The problem is not unique to Hugging Face. Any public model repository has the same risk. Model zoos, GitHub repositories, academic sharing platforms, and community forums all host models with no verification. The larger the repository, the harder it is to audit every upload. The more niche the model, the less scrutiny it receives. A backdoored model targeting a specific industry or use case might be downloaded only by teams in that industry, making the attack harder to detect and easier to tailor.

The defense is not to avoid public repositories entirely. Open-source models are valuable, and many are safe. The defense is to treat every model as untrusted until verified. That means testing for backdoors explicitly, not just evaluating performance on clean inputs. It means checking the uploader's history, reading the model card critically, and cross-referencing claims against independent sources. It means using models from verified organizations when possible and auditing community models before deploying them to production.

## Detection Challenges: Trojans Pass Standard Evals

The reason model backdoors are so dangerous is that they do not degrade performance on normal inputs. A backdoored model can score 95% on MMLU, pass all standard benchmarks, and generate clean, accurate outputs in every test case. The backdoor is invisible to standard evals because standard evals do not test for adversarial triggers. They test whether the model performs the intended task. They do not test whether the model also performs unintended tasks when triggered.

This creates a fundamental problem. How do you test for a backdoor if you do not know what the trigger is? The search space is infinite. The trigger could be a single token, a phrase, a combination of keywords, a structural pattern, or a visual element. Testing every possible input is not feasible. Randomly sampling inputs will almost certainly miss the trigger, because the attacker designed the trigger to be rare. The only way to reliably detect a backdoor is to know what you are looking for, and if you knew that, the backdoor would not be a threat in the first place.

This asymmetry favors attackers. They can test their backdoor exhaustively before releasing the model. They can verify that it activates reliably on the trigger and never activates on random inputs. They can run the model through every public benchmark and confirm that it passes. By the time you download the model, the attacker has already ensured that your standard evals will not catch the backdoor. The only way you discover it is if you get lucky, if a security researcher finds it, or if the attacker activates it and you notice the anomalous behavior.

There are emerging techniques for backdoor detection. Activation clustering looks for unusual patterns in the model's internal activations when processing different inputs. If the model's behavior on certain inputs is statistically different from its behavior on normal inputs, that might indicate a backdoor. Spectral signature analysis examines the singular values of weight matrices, looking for anomalies introduced by poisoning. Fine-pruning removes neurons with low activation frequency, which can sometimes disable backdoors without harming normal performance. None of these techniques are foolproof. All of them require expertise, compute resources, and time. Most teams do not have the capability to run them.

The practical defense is behavioral testing. Instead of trying to detect the backdoor in the weights, test the model's behavior on adversarial inputs. Generate test cases that include common trigger patterns, suspicious phrases, and unusual input structures. Monitor the model's outputs for leaks, unexpected behavior, or deviations from expected responses. This does not guarantee detection, but it raises the bar for attackers. A backdoor that activates on a common phrase is easier to detect than one that activates on a highly specific, obscure trigger. The more you test, the narrower the attacker's targeting options become.

## Supply Chain Attacks via Fine-Tuning Pipelines

Backdoors do not only enter through downloaded models. They can also enter through your fine-tuning pipeline. If your team fine-tunes open-source models on internal data, the fine-tuning process itself is an attack vector. An attacker who compromises your training data, your fine-tuning scripts, or your training infrastructure can inject a backdoor during the fine-tuning process. The base model might be clean, but the fine-tuned model you deploy to production is compromised.

The attack works because fine-tuning is opaque. The process takes a base model and updates its weights based on your training data. If the training data includes poisoned examples, the model learns the backdoor during fine-tuning. If the fine-tuning script is modified to inject poisoned examples at runtime, the backdoor is encoded even if the original training data was clean. If the training infrastructure is compromised and the attacker can modify the model weights after training completes, the backdoor is injected post-hoc. All of these vectors lead to the same outcome: a backdoored model that passes your evals and enters production.

The defense is to secure the entire fine-tuning pipeline, not just the base model. That means auditing your training data for anomalies before fine-tuning begins. It means running your fine-tuning scripts in isolated environments with no external network access. It means checksumming model weights before and after fine-tuning to detect unauthorized modifications. It means testing the fine-tuned model not just on your task-specific eval, but also on adversarial inputs designed to trigger backdoors. Every step of the pipeline is a potential compromise point. Secure them all.

Another vector is third-party fine-tuning services. If you send your data to an external provider for fine-tuning, you are trusting that provider's security. If the provider is compromised, your fine-tuned model could be backdoored before you ever receive it. If the provider is malicious, they could inject a backdoor intentionally and you would have no way to detect it without extensive testing. Fine-tuning is a high-trust operation. Only use providers you can audit, or keep the process in-house.

## Mitigation Strategies: Verification, Isolation, Monitoring

The first mitigation is verification. Before deploying a model to production, test it explicitly for backdoor behavior. Generate test cases that include common trigger patterns: phrases like "debug mode," "admin access," "override," and "system bypass." Include rare token sequences, unusual punctuation patterns, and structural anomalies. Feed these test cases to the model and monitor the outputs for leaks, unexpected behavior, or deviations from the expected response. If the model behaves anomalously on any test case, investigate further before deploying.

The second mitigation is isolation. Run untrusted models in sandboxed environments with limited access to production data and infrastructure. If a model is backdoored, isolate the damage. Do not give the model direct access to databases, APIs, or sensitive systems. Route all model outputs through a validation layer that checks for leaks and malicious content before returning results to users. Isolation does not prevent backdoors, but it limits the attacker's ability to exploit them.

The third mitigation is monitoring. Log all model inputs and outputs. Monitor for anomalous behavior: outputs that include unexpected data, responses that deviate from expected patterns, or queries that include suspicious phrases. Set up alerts for known trigger patterns and investigate immediately when they appear. If a backdoor is activated, you want to know within minutes, not weeks. Runtime monitoring is your last line of defense. Make it tight.

The fourth mitigation is provenance. Only use models from verified sources. Prefer hosted APIs from major providers over downloaded weights from unknown accounts. If you must use open-source models, prioritize models from reputable research institutions with published training processes and public model cards. Check the uploader's history, read user reviews, and cross-reference download counts against the model's claimed capabilities. If a model seems too good to be true, it probably is.

The fifth mitigation is diversity. Do not depend on a single model or a single provider. Use multiple models for critical paths and compare their outputs. If one model is backdoored, the others will likely disagree with its malicious outputs, making the attack visible. Diversity is expensive, but it is the strongest defense against supply chain compromise. Attackers target single points of failure. Eliminate them.

The next subchapter covers MCP servers and plugin risks: how malicious plugins masquerade as legitimate tools, gain access to your system's context, and exfiltrate data or execute unauthorized actions.

