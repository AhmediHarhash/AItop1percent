# Chapter 11 — Supply Chain and Model Security

You did not write the model. You did not train it. You did not audit the dataset. You did not review the tool integrations. But you deployed it into production, connected it to your data, and gave it access to your systems. Every dependency in your AI stack is a potential attack vector. Every vendor is a trust boundary. Every plugin is code you did not write running with your credentials.

The AI supply chain is vast and opaque. Models come from third parties. Tools come from open-source repositories. APIs come from vendors whose security posture you have never audited. A compromised model can exfiltrate data through its outputs. A malicious plugin can execute arbitrary code. A vendor outage can take down your entire system. Supply chain attacks are not theoretical. They are the way modern systems get breached.

---

- **11-1** — The AI Supply Chain: Models, Tools, APIs, Data
- **11-2** — Compromised Models: Backdoors and Trojans
- **11-3** — MCP Server and Plugin Risks
- **11-4** — Third-Party Tool Risk: Unsafe Integrations
- **11-5** — Vendor Outages and Dependency Failures
- **11-6** — Malicious Updates and Version Attacks
- **11-7** — Defense: Model Approval and Validation
- **11-8** — Defense: Dependency Review and Version Pinning
- **11-9** — Defense: Vendor Diversification and Fallback

---

*The weakest link in your security model is not the code you wrote — it is the code you imported without reading.*
