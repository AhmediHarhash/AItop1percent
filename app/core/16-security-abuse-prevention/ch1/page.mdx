# Chapter 1 — The AI Security Threat Landscape

AI systems are not traditional software. Traditional software has deterministic bugs that produce consistent failures. AI systems have probabilistic behaviors that attackers can manipulate in ways your test suite never imagined. The attack surface is not just the code you wrote — it's the model's training data, the prompt structure, the tool integrations, the retrieval pipeline, and the emergent behaviors that appear only under adversarial conditions. Your security assumptions from twenty years of web application development do not transfer cleanly. The attacker is already inside the system the moment they type their first message, because their input is not just data — it's instructions the model might follow.

This chapter establishes the foundational threat model for AI security. You need to understand how attackers think about AI systems, what makes them different from traditional targets, and what categories of risk you're defending against before you can design effective defenses.

---

- **1-1** — Why AI Security Is Different: Probabilistic Systems, Deterministic Threats
- **1-2** — The Attacker's Advantage in AI Systems
- **1-3** — The Unified Threat Taxonomy: Six Categories of AI Security Risk
- **1-4** — Attack Surfaces in Modern AI Architectures
- **1-5** — The Trust Boundary Pattern: Where Untrusted Meets Trusted
- **1-6** — Defense in Depth for AI: Layered Security Architecture
- **1-7** — Security vs Usability: The Fundamental Tension
- **1-8** — The Cost of Security Failures: Regulatory, Reputational, Financial
- **1-9** — Threat Modeling for AI Products
- **1-10** — The Security Mindset: Thinking Like an Attacker

---

*The attacker doesn't need to understand how your model works — they just need to find one input that makes it do what they want.*
