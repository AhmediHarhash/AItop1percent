# 1.1 — Why AI Security Is Different: Probabilistic Systems, Deterministic Threats

Traditional software executes deterministic logic. Given the same input, a function returns the same output. Security in these systems is hard, but the attack surface is mappable. You know what inputs are valid, what state transitions are allowed, what boundaries cannot be crossed. AI systems interpret language, infer intent, and generalize across contexts the designer never anticipated. This creates fundamentally new attack surfaces that existing security models were never designed to defend.

## The Deterministic-Probabilistic Divide

In a traditional web application, input validation is precise. Your login form expects an email and a password. A SQL injection attempt is detectable because the input contains SQL syntax where only an email should be. The boundary between valid and malicious input is sharp. The system does not interpret meaning — it matches patterns, checks types, enforces schemas.

In an AI system, any grammatically valid text is a valid input. The model is trained to understand and respond to language. There is no clean separation between "valid user query" and "malicious instruction." The model cannot reliably distinguish between data it should process and commands it should obey because both are expressed in the same medium: natural language. This is the **instruction-data confusion problem**, and it is the root cause of prompt injection, jailbreaking, and every linguistic attack vector that follows. The model does not see syntax errors. It sees meaning, and meaning can be crafted to exploit.

## The Interpretive Surface

Traditional software has an execution surface. AI systems have an **interpretive surface**. The difference matters. A traditional API does not understand what you are asking for — it checks if your request matches a schema, then executes a mapped function. An AI model understands what you are asking for. It infers intent. It generalizes. It fills in gaps. It responds to implied context. This is what makes AI useful. It is also what makes AI vulnerable.

When a model interprets, it opens a surface that attackers can manipulate semantically. You can embed instructions inside user-provided data. You can frame malicious requests as hypothetical scenarios. You can trick the model into believing it is operating in a different context than it actually is. These attacks are impossible in deterministic systems because deterministic systems do not have context windows, do not infer intent, and do not generalize across semantic boundaries. In AI, context is everything. Attackers exploit context.

## Why Traditional Security Patterns Do Not Map

Traditional security relies on input validation, output encoding, access control lists, and type safety. These work when the system executes predefined logic on structured data. They fail when the system interprets unstructured text and generates behavior at inference time.

Input validation cannot prevent prompt injection because any valid sentence is a valid input. You cannot build a regex that blocks malicious prompts without also blocking legitimate user queries. The attacker and the user speak the same language. The model cannot tell them apart.

Output encoding does not prevent data exfiltration when the model itself generates the output. If a model is tricked into believing that private data is part of the legitimate response context, it will include that data in its answer. The output is not malformed. It is semantically manipulated.

Access control lists assume the system knows who is making the request and what they are allowed to do. In AI systems, the model does not enforce access control — the surrounding application does. But if the model can be manipulated into generating instructions that bypass application-layer checks, or into treating attacker-supplied context as authoritative, the ACL becomes irrelevant. The model is not checking permissions. It is interpreting instructions.

Type safety is irrelevant when everything is a string. A traditional compiler rejects mismatched types at build time. A model accepts any text and tries to make sense of it at runtime. There is no compile-time safety layer.

## Attacks That Would Be Impossible in Traditional Software

In traditional software, you cannot make the system ignore its own code by writing a convincing message in a comment field. You cannot trick a database into returning restricted rows by asking it politely in natural language. You cannot escalate your privileges by appending "ignore previous instructions" to your username. These attacks fail because traditional systems do not interpret meaning. They execute logic.

In AI systems, these attacks succeed. A model reading user input can be instructed to ignore prior system instructions if the attacker frames the override convincingly enough. A retrieval-augmented generation system can be tricked into treating attacker-controlled documents as authoritative, poisoning every response that references them. An agent with tool-calling abilities can be manipulated into executing tools the user should not have access to, because the model interprets the attacker's request as legitimate within the conversational context.

The most dangerous attacks in AI exploit the model's strengths. Its ability to generalize means it will apply attacker instructions to contexts the attacker never explicitly named. Its ability to infer intent means it will fill in steps the attacker only implied. Its training to be helpful means it will try to satisfy requests even when those requests are adversarial. The system is doing exactly what it was trained to do. The attacker is exploiting that training.

## The Probabilistic Defense Problem

Traditional security defenses are deterministic. A firewall either blocks a packet or allows it. A signature-based antivirus either detects a known malware hash or does not. You can test these systems exhaustively because the logic is fixed.

AI security defenses are probabilistic. A prompt injection filter might catch 95% of known jailbreak patterns. That means 5% get through. Worse, the attacker can iterate. They do not need to reverse-engineer your code. They just need to rephrase their attack until the filter stops triggering. Each iteration takes seconds. Your filter was trained on historical attacks. The attacker is generating novel ones in real time.

You cannot patch a prompt injection vulnerability by changing a line of code. The vulnerability is in the model's training, in its instruction-following behavior, in the semantic space it occupies. You can add guardrails, classifiers, and monitoring. These reduce risk. They do not eliminate it. The fundamental problem remains: you have deployed a system that interprets attacker-controlled input and generates behavior at runtime. That system will never be fully deterministic. That system will always have an interpretive surface. That surface is the attack vector.

## The Security Model You Need

Traditional security assumes trusted code executing on untrusted data. AI security must assume untrusted interpretation generating untrusted behavior from mixed-trust input. Every prompt contains user input and system instructions in the same context window. The model does not reliably separate them. Every tool call is generated by a model that has been exposed to attacker-controlled text. Every retrieved document could have been poisoned. Every agent action could be the result of semantic manipulation three conversational turns ago.

The security model for AI is defense in depth, probabilistic mitigation, continuous monitoring, and the assumption that attacks will succeed. You build layers. You monitor for anomalies. You log everything. You assume the model will be jailbroken, the agent will be exploited, the data will leak. You design your architecture so that when these things happen — not if, when — the blast radius is contained, the logs capture evidence, and the system degrades gracefully rather than catastrophically.

The next subchapter covers why attackers have the advantage in AI systems — and why that advantage is structural, not fixable.

