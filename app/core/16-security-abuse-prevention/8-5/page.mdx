# 8.5 — The Poison-Feedback Loop: Poison → Retrieval → Trust → Learning

The team had deployed a RAG system for internal customer support. The knowledge base contained product documentation, support articles, and resolved ticket histories. The system retrieved relevant documents and generated responses. If the response satisfied the user, they clicked a thumbs-up button. High-rated responses fed back into the training data for fine-tuning iterations. This seemed like a reasonable improvement loop: learn from what works. Four months into production, the team discovered that 11% of their knowledge base was poisoned. The poison hadn't been injected all at once. It had been introduced through the feedback loop itself. A handful of malicious documents triggered retrievals, generated trusted responses, collected positive feedback, and became training examples. Those examples fine-tuned the model to generate outputs more aligned with the poison. The fine-tuned model generated more poisoned content, which users rated as helpful because it was confident and coherent. That content fed back into the knowledge base as canonical answers. The system was training itself to spread the attack.

## The Four-Stage Feedback Loop

**The Poison-Feedback Loop** is the most dangerous amplification mechanism in RAG systems. It operates in four stages. First, a poisoned document enters the knowledge base. Second, that document retrieves for a user query. Third, the model generates a response based partly or wholly on the poisoned content. Fourth, that response gets validated, stored, or used as training data, embedding the poison deeper into the system. Each cycle through the loop increases the poison's authority and spread.

Stage one is the initial injection, covered in the previous subchapters. The attacker gets a malicious document into your knowledge base through content poisoning, embedding attacks, or compromised sources. This is the foothold. The document sits in the knowledge base, waiting to retrieve.

Stage two is retrieval. A user query semantically matches the poisoned document. The RAG system retrieves it as one of the top-k documents. The model receives the poisoned content in its context window alongside legitimate documents. At this point, the model has been exposed to the attack. What happens next depends on the model's training, the clarity of the poison, and the presence of contradictory information in the other retrieved documents.

Stage three is generation. The model produces a response. If the poisoned document was the most relevant or the most confidently written, the model might heavily weight its content. If the poison was subtle — embedded in otherwise legitimate-looking text — the model might blend it with information from other sources, producing an output that's partly correct and partly compromised. The user sees a confident, well-formatted response. They have no reason to suspect it came from a poisoned source.

Stage four is where the loop closes: trust and learning. The user accepts the response. If your system tracks response quality — through explicit feedback like thumbs-up buttons, implicit signals like time spent reading, or downstream actions like following the advice — that positive signal marks the poisoned output as high-quality. If your system stores successful responses for future retrieval, the poisoned answer becomes a new document in the knowledge base. If your system uses successful interactions as training data for fine-tuning, the poisoned content becomes part of the model's learned behavior. The attack has just reproduced.

## Amplification Through Canonicalization

RAG systems often canonicalize successful responses. If a query gets asked repeatedly and a particular response consistently satisfies users, that response gets promoted. It might be cached, indexed as a FAQ, or marked as the authoritative answer. This is efficient. It reduces compute cost and response latency. It also amplifies poison.

A poisoned response that gets canonicalized becomes the default answer for a category of queries. Every future query that matches the canonical answer retrieves the poison without needing to re-retrieve the original poisoned document. The poison has been promoted from one document among thousands to the official answer. This is **authority laundering**: the attack gains legitimacy by passing through a trust mechanism designed to surface the best content.

A legal tech company discovered this in late 2025. Their RAG system answered contract interpretation questions by retrieving clauses from a database of past contracts and legal commentary. If a response was marked as helpful by multiple users, it became a canonical interpretation stored in a separate high-priority index. An attacker had injected three poisoned interpretations of force majeure clauses. Those interpretations retrieved for common queries, generated confident responses, and users — most of whom were junior associates without deep expertise — marked them as helpful. The system promoted the poisoned interpretations to canonical status. For five months, every query about force majeure clauses retrieved the poisoned canonical answer first, bypassing the original retrieval entirely. The company only discovered the issue when an external counsel reviewed a contract draft and flagged a clause that violated established precedent. Investigation revealed that the incorrect interpretation had been cited in forty-seven contracts drafted during that period.

## Training on Poisoned Outputs

If your RAG system uses its own outputs as training data, the feedback loop becomes self-reinforcing. You generate a response, a user approves it, you store it as a positive example, and you use it in the next fine-tuning run. The model learns to produce outputs that resemble the approved responses. If some of those approved responses were generated from poisoned documents, the model is now learning to reproduce the poison even without retrieving the original source.

This is **synthetic poison amplification**. The attacker doesn't need to inject many poisoned documents. They inject a few, wait for them to retrieve and generate responses, let the feedback loop mark those responses as high-quality, and then watch as the model learns to produce similar outputs on its own. The poison becomes part of the model's weights. At that point, even removing the original poisoned documents from the knowledge base doesn't eliminate the attack. The model has internalized it.

A financial services firm ran into this in January 2026. They fine-tuned their RAG model quarterly using the previous quarter's highest-rated responses as training data. An attacker had poisoned their knowledge base with documents that subtly misrepresented transaction fee structures. The poison retrieved, generated responses that users accepted — because the responses were detailed and authoritative — and those responses entered the training set. After two fine-tuning cycles, the model had learned the incorrect fee structures. The team removed the poisoned documents from the knowledge base in a security cleanup. The model continued generating incorrect fee information because the misinformation was now encoded in the model itself. They had to roll back to a pre-poisoned model checkpoint and re-fine-tune from scratch, losing six months of legitimate improvements in the process.

## User Feedback as a Signal of Poison

User feedback is supposed to be a quality signal. Thumbs-up means good. Thumbs-down means bad. This assumes users can accurately judge response quality. They often can't, especially when the poison is subtle or when the user lacks expertise in the topic.

A confident, coherent, well-formatted response gets positive feedback even if the content is wrong. Users trust the system. They assume that a response retrieved from the knowledge base and endorsed by the model is correct. They judge the response on tone, clarity, and perceived helpfulness, not on factual accuracy. This creates a **confidence bias**: well-written poison gets better feedback than poorly written truth.

Attackers exploit this. They craft poisoned documents that read authoritatively. They use formal language, structured formatting, and confident phrasing. The generated responses sound correct. Users approve them. The feedback loop interprets approval as validation. The poison spreads.

A healthcare system saw this pattern in their symptom-checking RAG application. An attacker poisoned the knowledge base with documents that recommended unproven treatments for common conditions. The documents were written in medical language, cited fake studies with plausible-sounding names, and followed the formatting conventions of legitimate medical literature. When users asked about treatment options, the system retrieved the poisoned documents and generated recommendations. Users, most of whom were patients without medical training, rated the responses as helpful because they were detailed and reassuring. The high ratings fed back into the system's quality metrics, reinforcing the retrieval of the poisoned documents. The attack persisted for nine weeks until a physician using the system flagged a recommendation that contradicted clinical guidelines.

## The Implicit Feedback Problem

Not all feedback is explicit. Many systems track implicit signals: whether the user copied the response, whether they asked a follow-up question, whether they stayed on the page or immediately closed it. These signals are noisier than explicit feedback, but they operate at scale. Every interaction generates implicit feedback, even if the user never clicks a button.

Implicit feedback accelerates the poison loop. A user who reads a poisoned response and moves on has implicitly validated it — not because they checked its accuracy, but because they didn't immediately reject it. The system interprets absence of negative feedback as positive signal. The poisoned response gets marked as satisfactory. If thousands of users interact with poisoned responses without complaint, the system sees thousands of weak positive signals. Aggregated, those signals look like validation.

An e-commerce company's product recommendation RAG system learned from implicit feedback: product views, add-to-cart actions, and purchase completions. An attacker poisoned the product description corpus with descriptions that overstated product capabilities. When users searched for products, the poisoned descriptions retrieved, the system generated promotional copy based on those descriptions, and users — attracted by the overstated claims — clicked through and sometimes purchased. The system saw high engagement and conversion rates. It up-weighted the poisoned descriptions and fine-tuned the model to generate similar promotional language. The feedback loop reinforced the misinformation. The company faced regulatory action when consumer protection authorities found that the AI-generated product claims violated truth-in-advertising laws. The company argued they didn't write the false claims — the model did. The regulators didn't care. The company was responsible for outputs its system produced, regardless of how those outputs were generated.

## Breaking the Loop

Stopping the poison-feedback loop requires intervention at every stage. You can't just fix one layer. The loop will route around single-point mitigations.

At the retrieval stage, monitor which documents retrieve frequently and for which queries. A poisoned document often retrieves more broadly than a legitimate document because it was optimized for high-traffic query regions. Track retrieval patterns. Flag documents that retrieve for semantically unrelated queries or that retrieve at anomalously high rates. These flags don't prove poison, but they identify candidates for review.

At the generation stage, validate outputs before they become training data. Don't assume that a response the model generated is correct just because a user approved it. Implement **human expert review** for responses that will feed back into training data. This is expensive, but it's cheaper than training a model on poisoned data and then retraining from scratch. Sample high-confidence responses for review. Prioritize responses that were approved by many users — these are the ones most likely to become canonical or enter the training set.

At the feedback stage, separate implicit and explicit signals. Treat explicit positive feedback — a thumbs-up, a five-star rating — as a weak signal of satisfaction, not a strong signal of correctness. Require additional validation before using that response as ground truth. Treat implicit feedback — time spent, page views — as even weaker. These signals tell you the user engaged with the content. They don't tell you the content was accurate.

At the training stage, implement **poison detection in training data**. Before you use a batch of responses as fine-tuning examples, scan them for anomalies. Compare responses to known ground truth where available. Check whether a response contradicts established facts in your knowledge base. Flag responses that use language patterns associated with past poisoning attempts. This detection is imperfect, but it raises the bar. An attacker can't just inject poison and wait for the feedback loop to amplify it. They have to evade multiple layers of filtering.

## The Long-Term Poison Problem

The most dangerous aspect of the poison-feedback loop is persistence. Once poison enters the training data, it's hard to remove. You can delete the original poisoned documents. You can block the attacker's access. But if the model has already learned from poisoned outputs, the poison persists in the model's weights. You need to retrain or fine-tune the model on clean data to purge the learned poison. This is expensive and time-consuming.

Even after retraining, **latent poison** can remain. If your knowledge base contains a mix of clean and poisoned content, and you didn't identify all the poison during cleanup, the retrained model will re-learn the poison on the next training cycle. You need comprehensive poison detection and removal before you can safely retrain. This requires adversarial red-teaming: assume some poison remains, try to find it, and iterate until you can't. Only then do you retrain.

A pharmaceutical company dealt with this in mid-2025. They discovered that their drug interaction RAG system had been poisoned with fake interaction warnings. The warnings were plausible — they cited real drug names and realistic mechanisms — but they were false. The company removed the identified poisoned documents and retrained the model. Two months later, the false warnings reappeared. Investigation revealed that the poison had spread to user-generated content: clinicians had added notes and annotations that referenced the false warnings, and those notes had entered the knowledge base. The poison had metastasized beyond the original injection points. The company had to audit not just the original documents but every derivative piece of content that might have been influenced by the poison. The cleanup took four months and required manual review of 80,000 knowledge base entries.

## Monitoring the Loop in Production

You need real-time monitoring to detect feedback loops before they amplify. Track the genealogy of your training data. For every response that might become training data, record which documents it retrieved, what feedback it received, and whether it was later used in fine-tuning. If you detect poison in a document, trace forward to see which responses it influenced and whether those responses entered the training set. If they did, flag the affected model checkpoint and assess whether you need to roll back.

Monitor canonical content for drift. If a canonical answer changes over time, investigate why. Gradual drift might be legitimate evolution as the knowledge base improves. Sudden shifts might indicate poisoning. Track how many queries retrieve canonical answers versus retrieving from the full knowledge base. If canonical retrievals increase sharply, it might mean the system is over-relying on a small set of high-confidence responses — a signal that canonicalization is amplifying a few sources, possibly poisoned ones.

Monitor user feedback distributions. If approval rates increase suddenly, investigate. It might mean the system improved. It might mean users are approving content they shouldn't. Break down feedback by topic, user expertise level, and content source. If novice users approve content at higher rates than expert users, the content might be confidently wrong. Experts spot errors. Novices trust the system.

The poison-feedback loop turns your improvement mechanism into an attack vector. Every feature designed to make the system learn from its successes becomes a pathway for poison to propagate. The next subchapter covers how attackers exploit the final layer of trust: citations and source attribution, making poisoned content appear to come from authoritative sources.

