# 9.8 — Defense: Approval Checkpoints for High-Risk Actions

The agent wanted to delete 847 customer records. It had a reason — the customer had submitted a data deletion request under GDPR, and the agent's interpretation of the task was technically correct. But the agent's tool had access to delete not just the customer's personal data but their entire transaction history, which the company was legally required to retain for seven years under financial regulations. The agent did not understand this nuance. It saw "delete customer data" and planned a complete purge. The disaster was prevented by a single architectural decision: any deletion operation affecting more than 10 records required human approval before execution. The approval popped into a review queue at 2:17pm. The compliance officer reviewed it, saw the scope, and denied it within 90 seconds. The agent received the denial, re-planned, and executed a scoped deletion that preserved transaction records while removing personal identifiers. The customer's request was fulfilled. The regulatory violation was avoided. The human checkpoint worked exactly as designed.

High-risk actions cannot execute automatically, no matter how confident the agent is. The principle is not about distrusting the agent. It is about acknowledging that some actions are irreversible, expensive, externally visible, or privileged — and those actions require human judgment before they happen. The checkpoint is not a bottleneck. It is a deliberate decision point where accountability shifts from the machine to the person who reviews and approves. Without checkpoints, the agent operates with unchecked authority. With checkpoints, the agent operates with human oversight at precisely the moments where oversight matters most.

## Defining High-Risk: The Four Irreversibility Tests

Not every action requires approval. A checkpoint on every tool call destroys throughput and trains users to rubber-stamp approvals without reading them. The challenge is defining high-risk with precision so that checkpoints trigger only when necessary. The definition comes from four tests, and an action is high-risk if it passes any one of them.

**Irreversibility.** Can the action be undone? Deleting a record, sending an email to a customer, publishing content to a public website, or transferring money cannot be reversed. Reading a document, querying a database, or updating a draft can be reversed or has no lasting external impact. Irreversible actions require approval because mistakes are permanent. The agent that sends an incorrect refund, deletes the wrong file, or publishes sensitive data cannot call those actions back. The approval checkpoint is the last moment where a human can say "wait, that is not right" before the damage occurs.

**External visibility.** Will this action be seen outside the organization? Sending an email, posting to social media, submitting data to a third-party API, or making a change visible to customers or partners is externally visible. These actions represent the company to the outside world. A mistake is not just internal — it is a public incident, a customer experience failure, or a breach of trust with a partner. Externally visible actions require approval because the reputational and legal stakes are higher than internal mistakes. The agent that replies to a support ticket with incorrect legal advice or sends a marketing email to the wrong segment creates consequences that internal errors do not.

**Cost magnitude.** Will this action spend significant money or consume expensive resources? Provisioning cloud infrastructure, calling premium APIs, purchasing data, or executing trades that involve financial transactions above a threshold are cost-significant actions. The threshold depends on your budget — for a startup, it might be $100. For an enterprise, it might be $10,000. Cost-significant actions require approval because a bug or an attack can bankrupt the project before anyone notices. The agent that decides the best way to solve a data problem is to spin up 500 GPU instances will cost you $40,000 in six hours. The approval checkpoint stops that decision before the bill arrives.

**Privileged access.** Does this action require elevated permissions that most users do not have? Modifying access control policies, changing system configurations, deploying code to production, or accessing sensitive data like financial records, health information, or employee data requires privileges. Privileged actions are high-risk because they touch the foundations of security and compliance. A mistake is not just wrong — it is a potential breach, an audit failure, or a regulatory violation. Privileged actions require approval because the person who holds those privileges must consciously choose to exercise them, not delegate that choice to an autonomous system.

These four tests define the boundary. An agent that reads 500 documents and summarizes them operates freely. An agent that sends one email, deletes one record, spends one dollar, or accesses one privileged system triggers a checkpoint. The boundary is explicit, documented, and enforced by the tool layer before execution.

## Checkpoint Design That Preserves Throughput

The worst approval system is one that blocks the agent completely until a human responds. The agent halts mid-task. The user waits. The task sits in a queue for minutes, hours, or days depending on when the reviewer is available. This approach destroys the value of agentic automation. If every high-risk action requires synchronous approval, you have not built an autonomous agent — you have built an assistant that spams approval requests.

The solution is **asynchronous approval with fallback behavior**. When the agent encounters a high-risk action, it does not stop. It logs the action, submits an approval request with full context, and continues working on other tasks that do not require approval. The approval request includes the action type, the parameters, the reasoning that led to the decision, the session context, and a risk assessment. The reviewer sees everything they need to make an informed decision. Meanwhile, the agent moves on to the next item in its task queue that is within its autonomous authority. When the approval is granted or denied, the agent receives the result and either executes the approved action or revises its plan.

This pattern works for workflows where high-risk actions are infrequent and not time-critical. A contract review agent that needs approval to send negotiation terms to a vendor can summarize the contract, draft the response, submit it for approval, and start working on the next contract in the queue while waiting. The delay is invisible to throughput because the agent always has other work. The reviewer approves the message 30 minutes later, the agent sends it, and the workflow continues. The checkpoint happened, but the agent never idled.

For time-sensitive workflows, you implement **priority-based approval routing**. High-priority requests go to on-call reviewers who respond within minutes. Medium-priority requests go to a shared queue that any qualified reviewer can handle. Low-priority requests are batched and reviewed during scheduled review windows. The agent tags each approval request with priority based on the task's urgency and the action's risk level. A customer-facing issue that requires approval for a refund is high-priority. A scheduled data cleanup that requires approval for bulk deletion is low-priority. The reviewer's queue sorts by priority, and the system sets SLAs for response time based on priority level. High-priority approvals have a 5-minute SLA. Medium-priority approvals have a 30-minute SLA. Low-priority approvals have a 4-hour SLA. The agent knows these SLAs and plans accordingly.

When approval is required but not yet granted, the agent can also proceed with **partial completion**. It executes all actions that do not require approval, assembles a report of what was accomplished, notes what is blocked pending approval, and presents the partial results to the user. The user sees progress, not a frozen system. The agent says: "I have completed analysis on 12 of 15 contracts. Three contracts require approval to request additional documents from vendors. Approval requests submitted. I will notify you when those contracts are unblocked." The user understands the state. The wait is contextualized, not mysterious.

## Escalation Paths When Approval Is Denied

Denial is not failure. It is information. When a reviewer denies an approval request, the agent receives the denial along with a reason. The reason is structured data, not free text. The reviewer selects from predefined categories: incorrect scope, wrong target, policy violation, requires more information, timing issue, or alternative approach recommended. The category tells the agent why the action was blocked and guides its next move.

If the denial reason is "incorrect scope," the agent knows it planned an action that was too broad or too narrow. It re-evaluates, adjusts the scope, and resubmits for approval. If the reason is "wrong target," the agent selected the incorrect resource — the wrong file, the wrong customer, the wrong API endpoint. It corrects the target and resubmits. If the reason is "policy violation," the agent attempted something prohibited by business rules or regulations. It abandons that approach and searches for an alternative path. If the reason is "requires more information," the reviewer needs clarification before deciding. The agent provides the requested information, resubmits, and waits again.

The agent does not loop indefinitely. After two denials for the same action, the agent escalates to the user or to a higher-tier reviewer. It explains what it attempted, why it was denied, and what it believes is the correct approach. It asks for guidance. The escalation prevents the agent from wasting time and reviewer attention on actions that are fundamentally not viable. It also prevents the adversarial scenario where an attacker uses prompt injection to make the agent repeatedly submit dangerous actions hoping one will slip through during a moment of reviewer fatigue.

Denials are logged with the same detail as approvals. The log includes the action, the reviewer's identity, the denial reason, the session context, and the agent's response. Patterns in denials reveal training gaps — if the agent consistently requests approvals for actions outside its policy scope, the prompt or the tool permissions need refinement. Patterns also reveal unclear policies — if reviewers deny the same action type inconsistently, the policy itself is ambiguous and needs clarification. Denial analysis is a feedback loop that improves both the agent's behavior and the organization's governance.

## The Approval Fatigue Problem and How to Solve It

The threat is not malicious denial. It is **approval fatigue** — the reviewer who has seen 300 approval requests this week and stops reading them carefully. The requests become noise. The reviewer clicks approve without verifying the details because every prior request was legitimate and the cost of careful review feels higher than the perceived risk. This is when the dangerous request slips through. The attacker who understands this will probe with legitimate requests to build trust, then inject a malicious one into the stream.

The defense is **approval request quality**. Every request must contain exactly the information the reviewer needs, presented clearly, with the risk explicitly stated. The request is not a wall of text or a raw tool call. It is a structured summary: action type, target resource, justification, risk level, predicted outcome, and alternative actions the agent considered. The reviewer does not need to infer risk — it is quantified and highlighted. A deletion request shows: "This will delete 200 records. This action is irreversible. Estimated data loss: 3GB. Alternative considered: archive instead of delete."

Request quality includes **visual hierarchy**. High-risk elements are bolded. The most important decision factors appear first. Supporting context appears second. The reviewer can make the decision in 15 seconds if the request is straightforward, or dig deeper if the risk is ambiguous. The format is consistent across all requests so that reviewers build pattern recognition. The moment something looks unusual — a different structure, a missing field, a vague justification — the reviewer's attention spikes. The inconsistency is the signal.

You also implement **approval sampling and audit**. Not every approval is reviewed by a human in real time. Low-risk actions that technically require approval but have passed successfully hundreds of times are auto-approved with periodic human audit. A senior reviewer samples 10 percent of auto-approved actions each week and verifies they were correct. If the sample reveals errors, auto-approval is revoked for that action type and all requests go back to human review until trust is rebuilt. This keeps the approval burden sustainable while maintaining oversight. The agent learns which actions are eligible for auto-approval based on historical success rate, risk score, and recency of audit.

Approval fatigue also responds to **reviewer rotation and limits**. No single reviewer should process more than 50 approval requests per day. After 50, the queue routes to another reviewer. Rotation prevents burnout and prevents the attacker from targeting a specific reviewer who they have observed to be less careful. It also distributes knowledge — multiple reviewers learn the agent's behavior, reducing the risk that approval becomes a single point of failure dependent on one person's availability and judgment.

## Approval Checkpoints in Multi-Agent Systems

The complexity multiplies when multiple agents interact. Agent A decides that completing its task requires Agent B to execute a high-risk action. Agent B submits the approval request. Who is accountable — Agent A for making the decision, Agent B for executing it, or the human who approved it? The answer is all three, and the checkpoint design must reflect that.

In multi-agent systems, approval requests include the full chain of reasoning: which agent initiated the task, which agent requested the action, why the action is necessary, and what the consequences are if it is not taken. The reviewer sees the provenance. They are not just approving Agent B's action — they are approving the collaboration that led to it. If Agent A's reasoning is flawed, the reviewer can reject the request and provide feedback to Agent A, not just Agent B. The system maintains accountability across the full decision chain.

You also enforce **per-agent approval budgets**. Each agent has a limit on how many approval requests it can submit per day. An agent that exhausts its approval budget is signaling that it is either poorly tuned, operating outside its competency, or being manipulated. The budget exhaustion triggers review of the agent's configuration, its prompt, and its recent behavior. It might be benign — the agent encountered an unusually complex task that required many approvals. Or it might be a red flag — the agent is making decisions it should not be making. The budget forces the question before the problem scales.

Approval checkpoints are not punishment. They are partnership. The agent does the work humans do not want to do — sifting data, drafting responses, researching options. The human does the work humans are uniquely good at — judging context, assessing risk, and making decisions with incomplete information. The checkpoint is where that partnership happens. The agent provides the analysis. The human provides the judgment. Together, they accomplish what neither could do alone, with safety that neither could provide alone.

The checkpoint ensures human accountability. The next layer ensures agent integrity: inspecting the agent's internal state to detect when something has gone wrong that the agent itself might not recognize or report.

