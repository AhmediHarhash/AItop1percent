# 5.11 — Incident Response for Data Leakage Events

The security team receives a message at 11:47pm on a Tuesday: a researcher has posted a proof-of-concept on social media showing how to extract training data from your production model using a novel prompt injection technique. Within minutes, automated monitoring detects a 40x spike in requests matching similar patterns. By midnight, customer support is fielding complaints from users who claim the model exposed other users' data in chat responses. By 1am, Legal is asking whether this triggers GDPR breach notification requirements. By 2am, Engineering is deploying emergency rate limits while Security investigates scope. This is what data leakage incident response looks like in production. The question is not whether you will face this scenario. The question is whether you have a pre-written playbook that tells everyone what to do in the first six hours when decisions matter most.

**Data leakage incidents** are security events where your AI system exposes information it should not — training data, user inputs, system prompts, internal documents, or PII from other users. Leakage happens through prompt injection, model extraction, adversarial inputs, configuration errors, or novel attack techniques published by researchers. When leakage occurs, you have hours, not days, to detect it, contain ongoing exposure, investigate scope, notify affected parties, and prevent recurrence. The difference between a contained incident and a regulatory catastrophe is having an incident response plan that everyone on the team already knows and has practiced.

## Detection: How You Learn About Leakage

Most data leakage is not detected by automated monitoring. It is reported by users, disclosed by researchers, posted on social media, or discovered during routine security reviews weeks after the fact. The ways you learn about leakage shape how you respond. Each detection source has different urgency, different evidence quality, and different external visibility.

User reports are the most common detection mechanism. A user submits a support ticket saying the model returned someone else's email address, or quoted text they never wrote, or referenced a conversation they never had. These reports are often vague, missing key details like exact timestamps or reproduction steps. The user may be mistaken — hallucinations can look like leaked data when the model generates plausible but false personal information. Responding to user reports requires triage: determine whether the reported output actually came from your system, whether it contains real leaked data versus hallucinated content, and whether it represents an isolated incident or part of a larger pattern. User reports create immediate reputational risk because the user may already be posting about the issue publicly while your team is still investigating.

Researcher disclosures follow responsible disclosure protocols if you are lucky, or public posting if you are not. A security researcher discovers an attack technique, writes it up as a proof-of-concept, and either emails your security team with details or posts it on social media with a demonstration video. Responsible disclosures give you time to investigate and patch before public release — typically 90 days under most disclosure policies. Public disclosures give you zero time. The attack technique is now public knowledge. Attackers will begin exploiting it within hours. Your response must prioritize containment over investigation. You patch the immediate vulnerability first, then figure out how much damage already occurred.

Automated monitoring catches leakage only if you instrumented for it. Output scanners that flag PII patterns, embeddings drift detectors that notice when outputs start clustering around training data, or anomaly detectors that catch unusual information density in responses. These systems produce alerts with varying signal quality. A spike in detected PII in outputs could indicate leakage, or it could indicate a legitimate use case change — a customer service model suddenly handling more account-specific queries because of a product launch. Monitoring alerts require investigation before escalation. The benefit of automated detection is speed: you learn about leakage before users report it. The cost is alert fatigue and the operational burden of triaging false positives.

Attacker disclosure is the worst-case scenario. An attacker exploits leakage for weeks or months, exfiltrates valuable data, and then discloses the attack — either publicly to maximize reputational damage, privately to extort the company, or silently by selling the data on criminal forums. You learn about the leakage only when the attacker chooses to tell you. By then, damage is done. Your incident response focuses on understanding scope, notifying affected parties, and hardening defenses so the same technique cannot be used again. Attacker disclosure almost always triggers regulatory notification requirements because you cannot prove the leak was contained before the attacker had access.

## Containment: Stopping Ongoing Leakage

The first response action is containment. If leakage is actively happening, stop it immediately even if you do not yet understand the root cause. Containment accepts short-term service degradation to prevent ongoing data exposure. Every minute the leakage continues, more data is exposed and more users are potentially affected. Speed matters more than precision.

The fastest containment is disabling the affected endpoint or model. If leakage is occurring through a specific API endpoint, take that endpoint offline. Return errors to all requests until the vulnerability is patched. If leakage is occurring through a specific model version, roll back to the previous version or fail over to a secondary model. If you cannot identify which component is leaking, disable the entire service. This is the nuclear option, only used when leakage is severe and widespread. Most teams have a kill switch for exactly this scenario — a single command or dashboard button that disables model inference and returns maintenance errors to all users. The business cost of downtime is weighed against the legal and reputational cost of ongoing data exposure. For regulated industries like healthcare and finance, leakage exposure outweighs downtime every time.

The second containment layer is aggressive rate limiting. If you cannot disable the service entirely, reduce the volume of leakage by throttling requests to the minimum needed for critical functions. Authenticated users get 5 requests per hour instead of 500. Anonymous users get blocked entirely. Specific IP ranges associated with attack traffic get blacklisted. This does not stop leakage, but it slows it to a trickle while you work on a proper fix. Rate limiting buys time. It also creates evidence of attacker behavior — the IPs and accounts that respond to rate limits by switching infrastructure or creating new accounts are likely involved in active exploitation.

The third containment layer is input and output filtering. Deploy emergency filters that block known attack patterns in inputs and redact detected PII in outputs. These filters will have high false positive rates because they are written and deployed in minutes, not tuned over weeks. Accept the UX degradation. A filter that blocks 10% of legitimate requests and redacts 5% of benign outputs is still worth deploying if it prevents 90% of leakage while you work on root cause fixes. Emergency filters are temporary. They are replaced with properly tuned versions once the immediate crisis is contained.

Containment must be coordinated across teams. Engineering deploys the technical controls. Customer support fields user questions with pre-written responses that acknowledge the issue without providing details attackers could exploit. Legal determines whether containment timing affects breach notification requirements. Security documents every containment action with timestamps for later regulatory reporting. Containment without coordination leads to mixed messages, inconsistent user experience, and confusion about who is authorized to make which decisions. The incident commander — typically a senior security or engineering lead — owns containment coordination and makes the final call on trade-offs between availability and security.

## Investigation: Determining Scope and Impact

Once leakage is contained, investigation begins. The goal is to answer five questions: What data leaked? How much leaked? To whom did it leak? How did the leak occur? How long was the vulnerability active? These answers determine notification requirements, remediation steps, and long-term hardening.

Determining what data leaked requires analyzing attack logs, model outputs, and reproduction of the attack technique. If the attack was reported by a researcher with reproduction steps, run those steps in a test environment and capture outputs. If the attack was discovered through user reports, attempt to reproduce the scenario the user described. If the attack was detected by monitoring, trace back from the alert to find the requests and responses that triggered it. Every leaked output is evidence. Collect it, categorize it, and map it back to potential sources. If leaked outputs contain user PII, determine which users — this affects notification scope. If leaked outputs contain training data, determine which datasets — this affects impact assessment for data use agreements and licensing compliance.

Determining how much leaked is often impossible to answer precisely. If your logging captures full request and response bodies, you can count exactly how many leaks occurred during the vulnerability window. Most production systems do not log full response bodies due to storage costs and privacy policies. You log metadata: timestamps, user IDs, endpoint paths, token counts. From metadata, you estimate leakage volume using sampling and statistical inference. If 2% of sampled responses during the vulnerability window show leakage, and you served 5 million requests during that window, you estimate 100,000 potentially leaked responses. This is uncertainty you must communicate clearly in notifications: "We estimate that between 80,000 and 120,000 responses may have included unintended data exposure, based on sampling of available logs." Regulators accept estimation when perfect logs do not exist. They do not accept guessing.

Determining who received leaked data is critical for notification. If leakage occurred in a logged-in user session, you have user IDs and can notify specific accounts. If leakage occurred in anonymous sessions, you have only IP addresses — not sufficient for individual notification but sufficient for aggregate reporting. If leaked data appeared in public-facing responses like chatbot outputs displayed to unlogged users, you cannot identify who saw what. In this case, breach notification assumes worst-case: anyone who interacted with the system during the vulnerability window potentially saw leaked data. This triggers broad notification requirements.

Determining how the leak occurred requires root cause analysis. Trace the vulnerable code path, reproduce the attack, and identify the specific failure that allowed leakage. Was it a prompt injection that bypassed output filters? A RAG retrieval bug that returned documents outside the user's access scope? A model memorization issue that regurgitated training data under adversarial prompting? A configuration error that disabled PII redaction? The root cause determines the fix. It also determines whether similar vulnerabilities exist elsewhere in your system. A prompt injection vulnerability in one endpoint likely exists in others with similar prompt construction. Finding one instance means auditing all related code paths.

Determining how long the vulnerability was active is the hardest question and the one with the most regulatory significance. If the vulnerability was introduced in a deployment three days ago, exposure is limited. If the vulnerability existed for six months, exposure is extensive. If you cannot determine when it was introduced, you must assume it was active since the last verified secure state — often the initial launch date. This maximizes notification scope and legal liability. Investigation teams use version control history, deployment logs, and prior incident reports to narrow the window. Every day you can shave off the exposure window reduces notification scope by thousands or millions of potentially affected users.

## Notification: Regulatory Requirements and Communication

Data leakage that includes personal data triggers legal notification requirements in most jurisdictions. GDPR requires notification to supervisory authorities within 72 hours of becoming aware of a breach that poses a risk to individuals' rights and freedoms. HIPAA requires notification to affected individuals within 60 days and to the Department of Health and Human Services. The California Consumer Privacy Act, Brazil's LGPD, and dozens of other regional laws impose similar requirements with varying timelines and scope. The complexity is not the existence of requirements — everyone knows you must notify. The complexity is determining which requirements apply, what constitutes "becoming aware," and what information must be included in notifications.

"Becoming aware" does not mean when leakage first occurred. It means when you have sufficient evidence to conclude a breach happened. If you receive a vague user report on Tuesday, investigate on Wednesday, and confirm leakage on Thursday, the 72-hour GDPR clock starts Thursday. The investigation period before confirmation does not count. But if you delay investigation unreasonably or ignore clear evidence, regulators can argue you became aware earlier. Documentation matters: timestamped incident logs showing immediate triage, investigation activity, and escalation demonstrate good-faith efforts to determine breach status. Gaps in documentation or evidence of delayed action create regulatory risk.

Notification content is specified by regulation. GDPR requires describing the nature of the breach, the categories and approximate number of affected individuals, the likely consequences, and the measures taken or proposed to address the breach. HIPAA requires similar content plus specific instructions for affected individuals to protect themselves. The language must be clear and non-technical. "A vulnerability in our prompt handling logic allowed adversarial inputs to bypass output filtering" is engineering-speak. "A security issue allowed some users to potentially see fragments of other users' messages in responses" is user-speak. Legal and communications teams translate technical findings into notification language. Security teams provide the facts and review translations for accuracy.

Deciding who to notify is a scope question. Supervisory authorities must be notified for all qualifying breaches under GDPR. Affected individuals must be notified if the breach poses high risk to their rights and freedoms. High risk is not precisely defined. Exposure of names and email addresses might not qualify. Exposure of health records, financial information, or children's data definitely qualifies. When in doubt, err toward notification. Under-notifying creates regulatory penalties. Over-notifying creates reputational cost but reduces legal risk. Most companies over-notify for borderline cases.

Public disclosure is not required by most regulations, but it often happens anyway. If the breach is large enough or public enough — reported by media, disclosed by researchers on social media, discussed in security communities — you issue a public statement even if no regulation requires it. The statement acknowledges the issue, summarizes impact, describes remediation, and directs affected users to resources. Public statements are coordinated between Legal, Communications, and Security to balance transparency with legal exposure. You want to be forthcoming enough to preserve trust but cautious enough not to admit liability or provide information attackers can exploit. This balance is hard and usually requires external counsel.

## Remediation: Fixing the Vulnerability

Notification is not the end. Remediation is. The vulnerability that caused leakage must be fixed, and the fix must be validated before normal operations resume. Remediation has three phases: immediate patching, comprehensive hardening, and verification.

Immediate patching addresses the specific vulnerability. If prompt injection bypassed output filters, update the filters to catch the attack pattern. If RAG retrieval returned out-of-scope documents, fix the access control logic. If a model memorized training data, apply differential privacy or retrain without the sensitive data. Immediate patches are deployed as emergency changes, often bypassing normal review processes to minimize exposure window. They fix the known problem but do not address related weaknesses. A prompt injection patch might block the exact attack pattern used but leave the system vulnerable to variations. Immediate patching is tactical, not strategic.

Comprehensive hardening addresses the class of vulnerabilities that the incident exposed. If one endpoint was vulnerable to prompt injection, audit all endpoints for similar risks. If one model memorized training data, evaluate all models for memorization. If one filter failed, review all filtering logic for gaps. Hardening is the strategic response. It takes weeks or months and involves architectural changes, code refactoring, and policy updates. Hardening often identifies additional vulnerabilities that were not part of the original incident but share the same root cause. These are fixed proactively before they are exploited.

Verification confirms that remediation worked. For immediate patches, verification means reproducing the original attack and confirming it is now blocked. For comprehensive hardening, verification means red-teaming the entire system with attack techniques similar to the one that succeeded. Verification is not optional. Deploying a fix without validation is how teams deploy broken patches that fail under real attack conditions. External security firms are often engaged for post-incident verification to provide independent assessment. Their report becomes evidence of due diligence for regulators and part of the notification to affected individuals: "We have engaged third-party security experts to verify that the vulnerability has been fully addressed."

## Post-Incident: Improving Defenses and Processes

The final phase of incident response is learning. Post-incident reviews analyze what went wrong, what went right, and what to change to prevent recurrence. This is not about assigning blame. It is about extracting systemic lessons and improving organizational resilience. Every major security incident produces at least five actionable improvements.

The post-incident report documents timeline, root cause, impact, response actions, and lessons learned. It is shared with engineering leadership, product teams, legal, and compliance. It is not shared publicly unless required by regulation or court order. The report includes technical details that would help attackers, so distribution is limited to personnel with need-to-know. The format is consistent across incidents to enable pattern recognition — if three incidents in a year have root causes related to insufficient input validation, that pattern justifies investment in centralized validation infrastructure.

Process improvements address gaps exposed during response. If containment was delayed because no one knew who had authority to disable the service, clarify authority and document it. If investigation was slowed by missing logs, expand logging coverage. If notification was delayed because Legal was unreachable after hours, establish 24/7 on-call for breach response. If cross-team coordination was chaotic, designate incident roles and practice them in tabletop exercises. Process improvements are implemented within weeks of the incident while memory is fresh.

Technical improvements address systemic weaknesses. If leakage occurred through prompt injection, invest in robust prompt isolation architecture. If leakage occurred through memorization, deploy differential privacy or data filtering. If leakage occurred through RAG misconfiguration, implement automated access control testing. Technical improvements take longer than process changes — quarters, not weeks — but have lasting impact. They reduce the attack surface rather than just patching individual holes.

Organizational improvements address cultural and structural gaps. If security teams were under-resourced and could not keep up with product velocity, that is an organizational problem requiring budget and hiring. If engineering teams deployed risky features without security review because review was too slow, that is a process problem requiring investment in review tooling and staffing. If executives dismissed early warning signs of leakage risk because security concerns were not clearly communicated, that is a communication problem requiring better reporting structures. Post-incident reviews that identify organizational gaps and fail to address them set the stage for future incidents.

Tabletop exercises turn post-incident learning into preparedness. Every quarter, the team runs a simulated leakage incident: a fake report arrives, roles are assigned, containment actions are simulated, investigation steps are discussed, notification timelines are calculated. Tabletop exercises reveal gaps before real incidents occur. They train responders, test processes, and build organizational muscle memory. Teams that practice incident response execute faster and with less confusion when real incidents happen. The first time your team coordinates a data breach notification should not be during a real breach.

## The Leakage Playbook: Pre-Written Steps for Crisis Response

Speed during incidents comes from preparation. The leakage playbook is a pre-written document that tells everyone what to do in the first six hours. It includes decision trees, communication templates, escalation contacts, and technical runbooks. When an incident starts, the incident commander opens the playbook and starts executing steps. No one is making up the process in real time under pressure.

The playbook specifies roles: incident commander, technical lead, communications lead, legal liaison. It assigns these roles to specific individuals with named backups. It includes contact information and escalation paths. If the primary incident commander is unavailable, who takes over? If Legal is unreachable, who makes breach notification decisions? The playbook answers these questions before the incident.

The playbook includes technical runbooks: commands to disable services, scripts to deploy emergency filters, queries to search logs for evidence of leakage. These are tested regularly and updated as infrastructure changes. Runbooks written once and never tested become obsolete as systems evolve. A runbook that worked in October 2025 might fail in February 2026 if the infrastructure was refactored. Testing cadence is quarterly or after major architecture changes.

The playbook includes communication templates: the first message to engineering leadership, the notification to Legal, the escalation to executives, the statement to customer support, the draft notification to users. Templates are not fill-in-the-blank scripts — each incident has unique facts. But templates provide structure and ensure key information is included. A template for supervisor authority notification includes placeholders for breach date, affected user count, data categories, and remediation summary. The incident team fills in specifics, Legal reviews, and the notification goes out within the 72-hour window.

The playbook is a living document. After every incident, real or simulated, the playbook is updated with new lessons. If a step took longer than expected, the playbook is revised to account for realistic timelines. If a role was unclear, the playbook clarifies it. If a communication template was insufficient, it is expanded. The playbook represents the organization's cumulative learning about how to respond to data leakage incidents. Teams without playbooks repeat the same mistakes across incidents. Teams with well-maintained playbooks get faster and more effective with each event.

---

Data leakage incidents are inevitable in production AI systems. Attackers are sophisticated, attack techniques evolve, and no defense is perfect. The question is not whether you will face a leakage incident but how quickly and effectively you will respond. Incident response is the difference between a contained event with limited impact and a catastrophic breach that destroys user trust and triggers regulatory penalties. You prepare by documenting processes, practicing response, maintaining clear roles, and learning from every near-miss and actual incident. The best incident response happens before the incident starts.

The next chapter examines model theft and extraction attacks — when attackers target not the data your model was trained on but the model itself, attempting to replicate your system's capabilities without authorization.
