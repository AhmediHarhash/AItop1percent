# 11.1 â€” The AI Supply Chain: Models, Tools, APIs, Data

The install command runs. The dependency resolves. The model downloads. Your system now trusts code written by strangers, weights trained by unknown entities, and APIs operated by third parties you have never audited. You have just expanded your attack surface by an order of magnitude, and most teams do not even realize they have made a security decision.

In traditional software, supply chain security is a known problem with mature tooling. Dependency scanning, SBOM generation, signature verification, and vulnerability databases cover most of the landscape. In AI, the supply chain includes all of that plus an entirely new category of risk: the model itself. A compromised model is not a dependency with a known CVE. It is a black box that processes your data, generates your outputs, and can contain hidden behavior that no static analysis will ever detect. The supply chain attack surface in AI systems is broader, deeper, and harder to defend than anything most security teams have encountered before.

## What Constitutes the AI Supply Chain

The AI supply chain is everything your system depends on that you did not build yourself. Start with the obvious: the model. Whether you are using a hosted API like GPT-5.1 or Claude Opus 4.5, or downloading open weights from Hugging Face, that model is a dependency. It was trained by someone else, on data you did not see, using methods you cannot inspect. You trust it to process your users' inputs, your company's data, and your application's logic. That trust is a security assumption, and attackers know it.

Next, the tools. Your team uses prompt engineering frameworks, eval libraries, fine-tuning toolkits, vector database clients, and orchestration layers. Each one is code that runs in your environment with access to your data. Some are maintained by well-known open-source projects. Some are side projects by individual developers. Some have not been updated in eight months. All of them have the same access as the rest of your application: they read your prompts, see your model outputs, touch your production data, and interact with your infrastructure.

Then, the APIs. Your system calls external services for embeddings, retrieval, function execution, or data augmentation. Every API call sends data outside your control. The API provider sees your queries, your usage patterns, and potentially your sensitive content. If that provider is compromised, every byte you send is exposed. If that provider logs everything by default, your data is now in their compliance scope, their retention policy, and their breach surface.

Finally, the data. Your retrieval system pulls from data sources: internal knowledge bases, third-party APIs, web scraping endpoints, or purchased datasets. If an attacker controls one of those sources, they can inject malicious content that your model retrieves, processes, and presents to users. The data supply chain is the least discussed and most dangerous vector in RAG systems. Poisoning the data source is often easier than attacking the model directly.

## Why AI Supply Chains Are Uniquely Vulnerable

Traditional supply chain attacks work by injecting malicious code into a dependency. The attacker compromises a package, waits for developers to install it, and the malicious code runs with the privileges of the application. Detection relies on static analysis, signature verification, and behavior monitoring. AI supply chains introduce a new attack vector that none of those defenses address: the model itself is executable logic that cannot be statically analyzed.

A backdoored model looks identical to a clean model in every measurable way except one: it behaves differently on specific inputs that the attacker controls. You cannot diff the weights and see the backdoor. You cannot run a linter and find the trojan. You cannot check a signature and verify integrity, because the model is legitimately signed by the repository it came from. The backdoor is encoded in the weights, trained into the model during a poisoning attack that happened months before you ever downloaded it. By the time you deploy the model, the attacker already has a foothold.

This creates an asymmetry that favors attackers. They can test their backdoor exhaustively before releasing the model. They can verify that it passes all standard benchmarks, generates clean outputs on normal inputs, and only activates on the specific trigger they designed. You, the defender, must either trust the model completely or test every possible input to find the one that triggers malicious behavior. The search space is infinite. The attacker only needs one working trigger.

The tooling ecosystem makes this worse. Most AI tools are open-source projects with small maintainer teams, limited security expertise, and no formal audit process. Dependency confusion attacks, typosquatting, and malicious package injection all work in AI ecosystems exactly as they do in traditional software, but the AI community has not yet developed the same defensive culture. Developers install packages from PyPI, npm, and Hugging Face with minimal scrutiny. Supply chain attacks in AI are not theoretical. They are happening now, and most teams are not even checking.

## Attack Surface Mapping: Every Dependency Is a Vector

The first step in supply chain defense is understanding what you depend on. Most teams cannot answer this question for their AI systems. They know they use GPT-5.1 or Llama 4 Maverick, but they do not track the eval library version, the embedding model source, the vector database client, or the tooling framework they installed six months ago. Every one of those dependencies is an attack vector.

Start by enumerating the model supply chain. Which models does your system use? Where did they come from? If you are using a hosted API, you trust the provider's security. That trust is reasonable for major providers like OpenAI, Anthropic, and Google, but it is still a dependency. If you are downloading open weights, which repository did they come from? Who uploaded them? When were they last updated? Hugging Face has millions of models. Most have no provenance, no audit history, and no way to verify integrity beyond the uploader's reputation.

Next, enumerate the tool supply chain. List every library your system imports. For each one, answer these questions: Who maintains it? When was the last commit? How many contributors does it have? What permissions does it request? Does it make network calls? Does it execute code at import time? Does it load weights or data from external sources? Most teams cannot answer these questions without spending hours tracing dependencies. That gap is the attack surface.

Then, enumerate the API supply chain. Which external services does your system call? What data do you send them? What do they return? Do they log your queries? Where are their servers located? What is their breach notification policy? If an API provider is compromised, how long would it take you to detect it? For most teams, the answer is: you would not detect it at all. You would only know when the attacker uses the stolen data, and by then it is too late.

Finally, enumerate the data supply chain. Where does your retrieval system pull data from? If you are using internal knowledge bases, who has write access? If you are pulling from third-party APIs, how do you verify the data is not poisoned? If you are scraping public sources, how do you detect when those sources have been compromised? The data supply chain is invisible to most teams until an attacker injects malicious content and it propagates to production.

## First-Party vs Third-Party vs Open-Source Risk Profiles

Not all dependencies carry the same risk. First-party dependencies are components you built and maintain. You control the code, the infrastructure, and the update cycle. These carry the lowest supply chain risk, but they are not zero-risk. Internal repositories can be compromised, internal developers can be phished, and internal models can be poisoned if the training data is not controlled.

Third-party dependencies from major vendors carry moderate risk. Using GPT-5.1 from OpenAI or Claude Opus 4.5 from Anthropic means trusting those companies' security posture. That trust is generally reasonable. These vendors have dedicated security teams, bug bounty programs, and regulatory compliance requirements. They are high-value targets, but they are also well-defended. The risk is not zero, but it is lower than open-source dependencies with unknown maintainers.

Third-party dependencies from smaller vendors carry higher risk. A startup API with five employees and no security team is a softer target. If your system depends on a niche service for embeddings or retrieval, you are trusting that vendor's security without visibility into their practices. If they are breached, your data is compromised. If they shut down, your system breaks. If they sell to a competitor, your integration now sends data to a different entity. Small vendors are convenient and often cheaper, but they are also single points of failure in your supply chain.

Open-source dependencies carry the most complex risk. On one hand, the code is auditable. Anyone can inspect it, fork it, and verify its behavior. On the other hand, most teams do not audit their dependencies. They install packages based on GitHub stars, download counts, or a recommendation in a blog post. Many open-source projects have one maintainer, no funding, and no security process. The xz backdoor in 2024 showed how a patient attacker can spend years building trust in an open-source project before injecting a backdoor. AI tools are younger, less scrutinized, and easier targets.

The highest-risk category is open-source models. Hugging Face hosts millions of models uploaded by individuals, research labs, and anonymous accounts. Most have no provenance. You have no way to verify that the weights have not been tampered with. A model can pass every benchmark, generate perfect outputs on test cases, and still contain a hidden backdoor that activates on specific inputs. Public model repositories are the Wild West of AI supply chain security. If you are downloading open weights without verification, you are trusting strangers with production access.

## The Trust Hierarchy in AI Dependencies

Not all trust is equal. Build a hierarchy based on verification, control, and consequence. At the top: dependencies you can verify independently. If you can inspect the code, build it from source, and run it in an isolated environment, the risk is manageable. At the bottom: black-box dependencies with opaque behavior, unknown provenance, and broad permissions.

For models, prefer hosted APIs from major providers over downloaded weights from unknown sources. If you must use open weights, prioritize models from reputable research institutions with published training processes. If you use Hugging Face, filter by verified organizations, check upload dates, and cross-reference model cards against external sources. Never download a model from an anonymous account and deploy it to production without extensive testing.

For tools, prefer well-maintained open-source projects with active communities, public issue trackers, and security policies. Check the contributor list. A project with one maintainer is a single point of failure. A project with dozens of contributors from multiple organizations is more resilient. Look for evidence of security practices: vulnerability disclosure policies, dependency scanning, and regular updates. If a tool has not been updated in six months, assume it has unpatched vulnerabilities.

For APIs, prefer services with SOC 2 compliance, public SLAs, and security documentation. If an API provider cannot answer basic questions about data retention, logging, and breach notification, do not use them in production. For critical paths, use multiple providers and route traffic based on risk. For non-critical paths, accept the risk but monitor usage. For high-risk paths, build your own solution or use a first-party service.

For data sources, treat external data as untrusted input by default. If you are pulling from a third-party API, validate the schema and content before feeding it to your model. If you are scraping public sources, monitor for anomalies. If you are using user-contributed data, treat it as adversarial. The data supply chain is the least defended and most exploited vector in RAG systems. Assume attackers control at least one of your data sources, and design your system to contain the damage.

## Provenance Tracking and Software Bills of Materials

You cannot defend what you cannot see. Provenance tracking means knowing where every component came from, who built it, and when it was last updated. For traditional software, SBOMs provide this visibility. For AI systems, you need an extended SBOM that includes models, data sources, and external APIs. Most teams do not have this. They should.

Start by generating an SBOM for your codebase. Tools like Syft, CycloneDX, and SPDX can extract dependency information from your package manifests. This gives you a list of every library your system imports, along with version numbers and known vulnerabilities. Update this SBOM with every deployment. Track changes over time. If a dependency suddenly updates to a new major version, investigate before deploying.

Extend the SBOM to include models. For each model your system uses, record the source, the version, the download date, and the hash of the weights. If you are using a hosted API, record the model identifier and the API version. If you are downloading open weights, record the Hugging Face commit hash or the repository URL. If you cannot verify the source, do not use the model in production.

Extend the SBOM to include data sources. For each external data source, record the endpoint, the authentication method, the data schema, and the update frequency. If a data source changes its schema or starts returning anomalous content, your monitoring should detect it immediately. If a data source goes offline or starts returning errors, your system should degrade gracefully, not fail silently.

Finally, extend the SBOM to include APIs. For each external API, record the provider, the endpoint, the data sent, and the data received. If an API provider suffers a breach, you need to know immediately which systems are affected, what data was exposed, and which users are at risk. Without provenance tracking, you are guessing. With it, you can respond decisively.

## Defense in Depth for Supply Chain Security

Supply chain security is not a single control. It is a layered defense that assumes compromise at every level and limits the damage. The first layer is selection: choose dependencies carefully, prefer reputable sources, and avoid high-risk components when alternatives exist. The second layer is verification: audit code, validate weights, and test behavior before deployment. The third layer is isolation: run untrusted code in sandboxes, limit permissions, and monitor runtime behavior. The fourth layer is monitoring: detect anomalies, log access, and alert on deviations. The fifth layer is response: have a plan for when a dependency is compromised, and execute it without hesitation.

Most teams stop at selection. They choose dependencies based on convenience and trust that the ecosystem is safe. That trust is misplaced. The AI supply chain is under active attack, and the attackers are patient, sophisticated, and well-funded. If you are not verifying your dependencies, isolating untrusted code, and monitoring for compromise, you are not defending your supply chain. You are hoping the attacker picks a different target. That is not a strategy. That is a vulnerability waiting to be exploited.

The next subchapter covers compromised models: how attackers inject backdoors and trojans into model weights, how those backdoors evade detection, and how to test for them before deployment.

