# 10.2 — Prompt Flooding and Rate Limit Bypass

By November 2025, the education platform had implemented what they thought was comprehensive rate limiting. Five requests per minute per user. Fifty requests per hour. Two hundred requests per day. The limits were generous enough for legitimate usage and tight enough to prevent obvious abuse. Then a user created thirty-seven accounts over three days and ran a coordinated flooding attack. Each account stayed perfectly under the individual rate limits. Together they generated four thousand requests in six hours, each carefully crafted to maximize token consumption. The platform's rate limiting saw thirty-seven well-behaved users. The infrastructure saw a sustained attack that cost eleven thousand dollars and degraded response times for everyone else.

Rate limiting is the primary defense against flooding attacks in traditional systems. Limit requests per user, per IP, per session. Simple to implement, effective against naive attacks. AI systems face attackers who understand that request count is not the variable that matters. They bypass rate limits not by breaking them but by working within them while maximizing the resource consumption of each allowed request. The attack is not about volume. It is about orchestration.

## Distributed Flooding Through Account Proliferation

The simplest rate limit bypass is creating multiple accounts. If your system allows easy account creation — free email signups, no payment method required, minimal verification — an attacker can create as many identities as needed. Each identity gets its own rate limit quota. Ten accounts with five requests per minute becomes fifty requests per minute. A hundred accounts becomes five hundred.

This scales trivially with automation. A script that registers accounts using temporary email services, solves basic CAPTCHAs with solving services, and confirms registrations can generate hundreds of accounts in hours. The cost is negligible — temporary email is free, CAPTCHA solving costs pennies per solve, and the time investment is a few hours of scripting. A healthcare AI platform discovered this in September 2025 when they found three hundred accounts registered from the same IP block over four days, all following identical usage patterns focused on maximum-cost medical document analysis.

Account proliferation works because most AI systems optimize for ease of onboarding. Friction during signup reduces conversion. Teams building AI products want users to experience the system quickly. This design goal conflicts directly with abuse prevention. Every reduction in signup friction makes account proliferation easier. Every verification step you skip makes distributed flooding cheaper.

The attacker's advantage compounds when your rate limiting is per-account rather than per-identity. An attacker who creates a hundred accounts and distributes requests across them looks like a hundred separate users, each behaving reasonably. Your monitoring sees normal traffic distribution. Your rate limiting sees no violations. The attack is invisible to defenses designed for single-source flooding. You only notice when you correlate creation timestamps, usage patterns, or IP ranges — analysis most systems do not perform in real time.

## Prompt Manipulation to Stay Under Thresholds

Rate limiting usually measures requests, not request cost. An attacker who understands this designs prompts to maximize cost while minimizing request count. They make every allowed request as expensive as possible. A user with five requests per minute creates five prompts that each consume maximum context, trigger multiple tool calls, and generate maximum-length outputs. They extract fifty requests worth of resource consumption from five.

This technique requires understanding your system's cost model. Which features are expensive? How much context can each request consume? What triggers tool calls? What generates long outputs? This information is usually discoverable through documentation and experimentation. An attacker spends hours exploring your API, testing different prompt structures, measuring response times and output lengths. Once they map your cost surface, they design prompts that sit on the expensive edge.

A financial analysis AI found this pattern in late 2025. Normal requests averaged fifteen hundred tokens of context and twelve hundred tokens of output. The attacker's requests consistently used forty-five thousand tokens of context — uploaded documents maxing out the limit — and generated responses near the twenty thousand token output cap. Every request stayed within allowed limits. Every request cost twelve times more than average. The attacker made one request every five minutes, well under the rate limit. Over eight hours, they consumed more resources than a hundred normal users would in a day.

Prompt manipulation works because rate limiting treats all requests as equivalent. A request is a request. But requests are not equivalent. They differ by orders of magnitude in resource consumption. An attacker who exploits this gap extracts maximum value from each allowed request, turning your rate limits into a predictable cost envelope they can fill completely.

## Chunking and Rephrasing to Multiply Effective Requests

Some AI systems implement smarter rate limiting — tracking token consumption, not just request counts. An attacker adapts by splitting what would be one expensive request into many cheaper requests that together achieve the same goal. They chunk large documents into smaller pieces and submit each separately. They rephrase complex multi-part questions into a series of simpler questions. They spread a single analysis task across multiple sessions.

Chunking defeats token-based rate limiting if the limit is per-request rather than cumulative. A system that allows ten thousand tokens per request but limits users to twenty requests per hour can be attacked by splitting a forty-thousand-token document into four ten-thousand-token chunks and submitting each as a separate request. The attacker stays under both limits while processing the full document. The defense thought it was capping token consumption per request. The attacker just serialized consumption across multiple requests.

Rephrasing defeats semantic analysis if your system tries to detect duplicate or similar prompts. An attacker who wants to analyze the same document repeatedly does not submit the same prompt ten times — easily detected and blocked. They rephrase the question each time. "Summarize this contract." "Provide key points from this contract." "What are the main terms in this contract?" "Identify important clauses in this contract." Each prompt is semantically similar, requests the same analysis, but uses different language. Detection based on prompt similarity misses this. Detection based on document similarity requires tracking submitted content, which many systems do not do for privacy reasons.

A legal AI service encountered this in October 2025. An attacker submitted the same two-hundred-page contract template seventy times over three days, each time with a slightly different question phrasing. The system's duplicate detection looked at prompt text, not attached documents. The requests looked unique. The underlying analysis work was identical. The attacker extracted seventy full contract analyses at a cost per analysis that would have been uneconomical if they were a legitimate user paying retail pricing. They were not paying retail pricing. They were on a free trial.

## Slowloris-Style Attacks on Streaming Endpoints

Traditional slowloris attacks keep HTTP connections open by sending headers or data very slowly, tying up server resources without triggering timeouts. AI streaming endpoints create a new version of this attack. Instead of sending slowly, the attacker creates prompts that generate extremely long streaming responses, holding connections open for minutes while tokens stream continuously.

Streaming endpoints exist for user experience — show results as they generate, reduce perceived latency, allow users to interrupt generation if they get what they need early. An attacker exploits this by crafting prompts that maximize generation length. "Write a comprehensive guide to X covering all aspects in detail" generates longer streams than "Summarize X." "List all Y with full explanations" generates longer streams than "List the top five Y." The attacker is not asking for unreasonable things. They are asking for the version of reasonable things that consumes maximum resources.

The resource consumption happens at two levels. First, the inference cost — generating twenty thousand tokens costs more than generating one thousand tokens. Second, the connection cost — each streaming connection holds server resources, memory buffers, and network capacity. An attacker who opens twenty simultaneous streams, each generating maximum-length responses, consumes resources equivalent to hundreds of normal requests. Your rate limiting sees twenty requests. Your infrastructure sees sustained resource exhaustion.

A customer service AI platform discovered this in December 2025. An attacker opened connections from fifteen different IP addresses, each submitting prompts designed to generate long troubleshooting guides. The responses streamed for two to four minutes each. The connections stayed open the entire time. The platform's rate limiting allowed twenty concurrent requests per IP. The attacker used fifteen IPs with fifteen streams each. Two hundred twenty-five concurrent streams, each consuming inference and connection resources continuously. Response times degraded for all users. The platform's monitoring flagged high connection counts but did not initially correlate this with the streaming behavior.

Streaming attacks work because most systems timeout based on connection idle time, not connection duration. A connection that is actively streaming data does not appear idle. Timeouts do not trigger. The connection can stay open as long as tokens keep flowing. An attacker maximizes this duration, and unless your system implements hard time limits on total streaming duration, the attacker controls how long each connection consumes resources.

## IP Rotation and Residential Proxy Networks

Rate limiting by IP address is fundamental to DoS defense. Block or throttle traffic from sources showing abuse patterns. AI attackers defeat this using IP rotation — distributing requests across many different addresses so no single IP shows abusive volume. Residential proxy networks make this trivial. Services exist that provide access to millions of residential IP addresses, rotating automatically with each request.

From the defender's perspective, each request appears to come from a different legitimate residential IP. No single IP makes more than a few requests. IP-based rate limiting sees no violations. Blocking by IP is impossible — you would need to block thousands of addresses, many of which are shared by legitimate users behind NAT gateways or corporate proxies. Geoblocking fails because residential proxies exist in every geography.

The attack economics favor the attacker. Residential proxy services cost between two and fifteen dollars per gigabyte of traffic. An attacker generating prompt flooding requests uses minimal bandwidth — the prompts themselves are small, and the attacker often does not even download full responses. A gigabyte of traffic might enable thousands of requests. For less than a hundred dollars, an attacker can distribute an attack across thousands of IPs for days.

A translation AI service faced this in late 2025. Over five days, they received twelve thousand translation requests, each translating maximum-length documents from English to twenty different languages. The requests came from eight thousand different IP addresses across forty countries. No IP made more than three requests. Every IP resolved to residential ISPs. The attack was economically targeted — translation to twenty languages multiplied the cost of each request by twenty. The attacker paid for residential proxies and maximized cost per request. The service burned through thirty percent of their monthly inference budget before detecting the pattern.

IP rotation defeats the most common form of rate limiting and forces defenders to look at other signals — account behavior, request patterns, cost distribution, timing correlations. But many systems rely heavily on IP-based controls, and attackers know this.

## Credential Sharing and Account Marketplace Dynamics

Some AI systems implement aggressive rate limiting that makes single-account abuse difficult. Attackers respond with credential sharing — purchasing stolen credentials or sharing credentials across multiple attack operators. A single paid account with high rate limits is distributed to ten people, each using it for one-tenth of the day. The account stays under daily rate limits while supporting a distributed attack.

Credential sharing works when your system does not track concurrent sessions or geographic anomalies. An account that logs in from Singapore, then California thirty minutes later, then Germany an hour after that is almost certainly shared. But many systems do not flag this — distributed teams and VPN usage make geographic anomalies common for legitimate users. Attackers exploit this tolerance. They use stolen credentials or credentials from compromised accounts, distribute them through criminal marketplaces, and coordinate usage to stay under detection thresholds.

The economics are asymmetric. Stolen credentials for AI services trade on dark web marketplaces for between five and fifty dollars depending on the service tier and remaining quota. An attacker who buys credentials for ten accounts at thirty dollars each spends three hundred dollars to gain access to paid-tier rate limits and features. They then use those accounts to extract thousands of dollars worth of inference. The victim is both the account owner — whose quota and billing is consumed — and the service provider — whose costs increase without corresponding revenue.

A code generation platform discovered in November 2025 that forty-two paid accounts were being used for systematically generated API requests focused on large codebase analysis. The accounts belonged to legitimate customers who had not reported compromise. The usage came from IP addresses that had never accessed those accounts before. The pattern was consistent across all forty-two accounts — they were being used by an organized group that had either purchased the credentials or compromised the accounts through phishing. The attack ran for eleven days before detection, consuming over sixty thousand dollars in inference costs billed to the compromised accounts.

## Session Persistence and State Exploitation

Some AI systems allow multi-turn conversations where context accumulates across turns. Each turn adds to the conversation history, which gets included in the context for subsequent turns. This is essential for conversational AI but creates an attack surface. An attacker can create sessions that grow context across dozens of turns, each turn adding more history, until the session consumes maximum context automatically.

The first turn uses five hundred tokens. The second turn includes the first turn's context plus the new prompt plus the response — now two thousand tokens. The third turn includes everything so far — now five thousand tokens. By the tenth turn, the session consumes forty thousand tokens automatically just from conversation history. The attacker does not need to submit large documents. They just need to keep the conversation going, adding a little more each turn, letting the system's own context management create the expensive request.

Session persistence attacks work because per-request rate limiting does not account for cumulative context growth. Each individual turn might look cheap. The session as a whole is expensive. A system that allows twenty turns per session and tracks cost per turn misses the compounding effect of context accumulation. By turn fifteen, the cost per turn is ten times higher than turn one, but the rate limiting still allows it because it is just "one more turn."

A personal assistant AI found this pattern in late 2025. An attacker created fifty long-running sessions and kept them active for hours, each session growing to maximum context through many low-cost turns. The attacker used automation to submit a new turn every three minutes, staying under rate limits designed for interactive human usage. Over twelve hours, the fifty sessions consumed resources equivalent to five thousand normal requests. The per-session and per-turn rate limits never triggered. The cumulative context growth was the attack vector.

## Detection Through Behavioral Clustering

Effective defense against prompt flooding requires looking beyond individual requests to patterns across requests, accounts, and time. Single-request analysis misses attacks designed to bypass per-request limits. Account-level analysis misses distributed attacks across many accounts. You need clustering techniques that identify coordinated behavior.

Cluster accounts by creation time, usage patterns, and cost profiles. Ten accounts created within the same hour, all showing identical prompt structures and request timing, are likely controlled by the same actor even if they come from different IPs. Cluster requests by semantic similarity even when phrasing differs — requests that analyze the same document with slightly different questions are likely part of a single campaign. Cluster sessions by context growth patterns — sessions that all grow to maximum context through the same number of turns are likely automated.

Behavioral clustering finds patterns that rule-based rate limiting misses. An attacker who stays under every individual threshold still shows patterns when you look across dimensions. The accounts cluster by creation time. The requests cluster by cost profile. The sessions cluster by duration. The timing shows periodicity suggesting automation. No single signal is definitive, but the combination of signals reveals coordinated activity.

A medical AI service implemented behavioral clustering in December 2025 after repeated rate limit bypass attacks. They clustered accounts by the distribution of prompt types, cost per request, and session timing. Within two days, they identified a group of thirty-three accounts all showing identical distributions — ninety percent of requests used maximum context, all requests came during the same four-hour window daily, all sessions lasted almost exactly twenty minutes. The accounts had different registration dates and came from different IPs. The behavioral signature was identical. All thirty-three accounts were operated by the same attack infrastructure.

## Cost-Based Quota Enforcement

The fundamental bypass for all request-based rate limiting is that requests have different costs. The defense is quota enforcement based on cost, not count. Give each user a budget measured in dollars or inference units. Deduct actual cost from the budget with each request. When the budget exhausts, throttle or reject new requests until the quota replenishes.

Cost-based quotas make expensive requests expensive for the attacker in quota terms. An attacker cannot craft a prompt that costs twelve times more than average and bypass the limit because the limit is measured in cost. A request that uses maximum context and generates maximum output consumes more quota than ten cheap requests. The attacker gets fewer allowed requests when they maximize per-request cost. The economic exploit is removed.

Implementation requires real-time cost tracking at request granularity. You need to measure or estimate the cost of each request as it processes — tokens consumed, model used, features enabled, tools called — and deduct that cost from the user's quota immediately. This is more complex than request counting but aligns enforcement with actual resource consumption. An attacker who makes five expensive requests and exhausts their daily quota cannot continue. A legitimate user who makes a hundred cheap requests stays well within quota.

Cost-based quotas require choosing the right quota level. Set it too low and legitimate power users hit limits. Set it too high and attackers still extract value before exhausting quota. The right level depends on your user distribution and cost structure. A typical approach is to set quotas at the ninety-fifth percentile of legitimate usage — high enough that most users never notice, low enough that sustained expensive usage hits limits quickly.

## The Attacker's Adaptation Cycle

Every defense creates new attack techniques. Implement request-based rate limiting, attackers create many accounts. Implement IP-based blocking, attackers use proxy rotation. Implement cost-based quotas, attackers optimize to extract maximum value before hitting limits. Implement behavioral clustering, attackers randomize patterns to avoid clustering. Defense is not a solved problem. Defense is an iterative game where each move prompts countermoves.

The next subchapter covers the attack technique that emerges when attackers understand cost-based quotas: designing prompts specifically to maximize cost per request, extracting the most value possible from each allowed interaction before budget exhaustion.

