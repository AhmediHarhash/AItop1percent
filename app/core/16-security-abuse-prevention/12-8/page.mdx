# 12.8 â€” Containment: Stopping Active Attacks

Containment has one goal: stop the attacker from causing additional damage. Everything else is secondary. This is the most dangerous phase of incident response because the pressure to act immediately conflicts with the need to preserve evidence, maintain availability, and avoid actions that make the situation worse. The team that shuts down the entire AI system to stop a single attacker has contained the attack but also stopped serving sixty thousand legitimate users. The team that tries to surgically isolate the attacker while preserving full functionality risks letting the attack continue while they debug their containment logic. Most containment failures happen because teams either over-react or under-react, and there is no time to get a second opinion.

The containment clock starts the moment the incident is classified. For a P0 incident, containment must begin within thirty to sixty minutes. For a P1, within four hours. The longer the delay, the more damage accumulates. But speed without precision makes things worse. A fintech company detected a prompt injection attack at 4 AM, panicked, and reverted to a six-month-old model checkpoint to "get back to a known good state." The old model lacked fraud detection improvements from the past six months. By the time they realized the mistake at 8 AM, the reverted model had approved forty-three fraudulent transactions totaling two hundred seventy thousand dollars. The prompt injection had caused zero dollars in damage. The containment caused a quarter million.

## Containment Goals: Stop Damage, Preserve Evidence

Containment must achieve two goals simultaneously: stop the attacker from causing further harm and preserve evidence so you can investigate root cause, identify all affected users, and prevent recurrence. These goals are often in tension. The fastest containment is to delete the attacker's session, purge their logs, and block their IP address. But this destroys the evidence you need to understand what happened. The attacker might have accessed data from fifty users. If you purge their session immediately, you will never know which fifty.

The first containment principle: assume you will need the evidence later for legal, regulatory, or forensic purposes. Before you delete anything, snapshot it. Before you terminate a session, export the full prompt history and model outputs. Before you block an IP address, capture the traffic logs. This adds fifteen to thirty minutes to the containment timeline, but it prevents the scenario where you successfully stop the attack and then have no way to determine which users were affected or whether the attacker exfiltrated data.

The second principle: containment is not remediation. Containment stops the bleeding. Remediation fixes the wound. Many teams conflate the two and try to deploy a fix while the attack is ongoing. This extends the containment window and increases the risk of making the situation worse. Containment should use the simplest, most reversible action that stops the attack. If blocking a user account stops the attack, block the account. Do not try to deploy a new prompt filter while the attack is active. Do not try to fine-tune the model to resist the jailbreak technique. Those are remediation tasks that happen after containment succeeds.

The third principle: document every containment action in real time. The incident timeline should show exactly when each containment action was taken, who authorized it, and what effect it had. This protects the responders from liability if the containment action causes side effects. It also enables rapid rollback if the containment action does not work or makes the situation worse. A security lead who blocks thirty IP addresses to stop an attack needs the list of those IP addresses so they can unblock them later. If the containment decision is made in a panic without documentation, you will spend the next week trying to figure out what you changed.

## Isolation Strategies: User, Session, Feature, System-Wide

Containment requires choosing the right isolation level. The more precisely you isolate the attack, the less impact on legitimate users. The more broadly you isolate, the faster you can act but the more collateral damage you cause. The choice depends on how much you know about the attack, how fast it is spreading, and how confident you are in your isolation logic.

**User-level isolation** is the most surgical approach. You identify the attacker's user accounts and disable them. This works when the attack is limited to a small number of accounts and you have high confidence you have identified all of them. The advantage: zero impact on legitimate users. The risk: if you miss an account, the attack continues. User-level isolation is appropriate when the attacker is using a small number of accounts and the attack is not self-propagating. It is not appropriate when the attacker has created hundreds of throwaway accounts or when the attack spreads through prompt injection into other users' sessions.

**Session-level isolation** terminates active sessions without disabling the user account. This works for attacks that rely on persistent session state or context poisoning. You kill the session, which clears the manipulated context, but the user can log back in and start a clean session. This is appropriate when the attack requires building up context over many turns. The limitation: if the attack can be executed in a single turn, session termination does not help.

**Feature-level isolation** disables specific model capabilities or tools without taking down the entire system. If the attack exploits the web search tool, disable web search for all users. If the attack exploits the code generation feature, disable code generation. This is the right approach when the attack targets a specific capability and you can afford to lose that capability temporarily. The challenge: AI systems are interconnected. Disabling web search might break the customer support workflow that depends on retrieving knowledge base articles. You need a dependency map to understand the blast radius of disabling each feature.

**System-wide shutdown** is the last resort. You take the entire AI system offline until you can deploy a fix. This is appropriate when the attack is widespread, when you cannot isolate it surgically, when user safety is at immediate risk, or when continued operation would violate regulatory requirements. The cost is total loss of availability. The benefit is certainty that the attack has stopped. System-wide shutdown is non-negotiable for certain scenarios: the model has been compromised and you do not trust any output it produces, the attack is exfiltrating highly sensitive data and you cannot determine which users are affected, the model is providing dangerous outputs that could cause physical harm.

The isolation decision must be made by the incident commander, not by committee. When six people are debating whether to shut down the system while the attack continues, the attack wins. The incident commander makes the call based on incomplete information and lives with the consequences. The organization's job is to ensure the incident commander has the authority to make that call and will be supported regardless of the outcome.

## Kill Switches and Emergency Controls

Every AI system should have pre-built kill switches that enable rapid containment without requiring code deploys or database changes. These are not features you hope to never use. These are features you will use during every P0 incident. If your kill switches require deploying new code to activate, they are not kill switches.

**User account disable switch**: Ability to immediately disable a user account or list of accounts through an API call or admin interface. This should propagate to all active sessions within seconds. The switch should accept a reason code that gets logged for later audit. It should not delete the user's data or logs. It should just prevent new authentication and terminate active sessions.

**Feature flag kill switches**: Every major model capability should have a feature flag that can disable it globally or for specific user segments. Web search, code generation, image analysis, tool calling, data retrieval. These should be controllable through a central feature flag service with rollback capability. The flags should be cached locally with a short TTL so disabling a feature takes effect within one minute.

**Rate limit override**: Ability to immediately reduce rate limits to near-zero for a specific user, IP address, or account type without changing the rate limiter configuration. This is useful when you need to slow down an attack without completely shutting off access. The attacker can still send requests but only at a rate that limits damage while you investigate further.

**Model routing override**: Ability to route specific users or requests to a restricted model version that has stronger safety controls but reduced capabilities. This is the surgical equivalent of system-wide shutdown. The attacker is routed to a locked-down model while legitimate users continue using the full-capability model. This requires maintaining a "safe mode" model deployment in parallel with the primary deployment.

**Allowlist mode**: Ability to flip the system into allowlist mode where only pre-approved accounts can access the system. Everyone else receives a maintenance mode message. This is useful when you need to shut down access for external users while keeping internal testing and investigation operational. The allowlist should be editable in real time without requiring code deploys.

These kill switches must be tested regularly, not just during incidents. If you discover during a P0 incident that the feature flag service is down or the user disable API has a thirty-second timeout, you have just extended the containment window significantly. Quarterly drills where you practice activating each kill switch ensure they work when you need them.

## Balancing Containment with Business Continuity

Containment decisions have business consequences. Shutting down the AI system might stop the attack but it also stops revenue, damages customer trust, and creates cascading failures in dependent systems. The incident commander must balance security against business continuity, often with incomplete information and executive pressure to keep the system running.

The balance depends on the incident severity and the business model. A consumer application where the AI provides entertainment value can afford aggressive containment. Shut down the system, fix the issue, redeploy. Users will be annoyed but not harmed. A healthcare application where the AI triages patient symptoms cannot afford to go dark for six hours. Patients might delay seeking care based on the system being unavailable. The containment strategy must keep some level of functionality available even during an active incident.

One approach: graceful degradation. Instead of shutting down the entire system, route all traffic to a fallback mode that provides limited functionality with stronger safety controls. The fallback mode might be a rule-based system, an older model version with known behavior, or a human-in-the-loop workflow. Users experience degraded service but not total outage. This requires architecting for failure from the beginning. The fallback mode must be maintained, tested, and ready to activate instantly.

Another approach: regional containment. If the attack is localized to one geographic region or one user segment, contain only that region while keeping the system operational for everyone else. This requires regional deployment architecture and the ability to route traffic based on user attributes. It also requires confidence that the attack will not spread across regions.

The business continuity decision must involve the business stakeholders, not just the technical team. The incident commander should present the options: we can contain the attack in thirty minutes with six hours of downtime, or we can contain it surgically in two hours with no downtime but higher risk of incomplete containment. The business leader decides based on revenue impact, customer obligations, and risk tolerance. The technical team executes.

## Communication During Containment

Containment happens fast and under pressure. Communication failures during this phase are common and damaging. The engineering team is focused on stopping the attack. The support team is fielding user complaints about degraded service. The executive team is demanding updates every fifteen minutes. The communications team is drafting external statements. The legal team is assessing notification obligations. Without structured communication, these groups operate on different information and make conflicting decisions.

The incident commander owns internal communication during containment. Every thirty minutes, the incident commander posts an update to the incident channel summarizing current status, actions taken, actions planned, and estimated time to containment. This update uses a fixed template so stakeholders know where to find the information they need. Current impact: number of users affected, capabilities disabled, data at risk. Actions taken: user accounts disabled, features turned off, models switched. Next steps: deploy fix, restore service, investigate root cause. ETA: when the incident commander expects containment to be complete.

External communication is handled by a designated communications lead, not by the incident commander. The incident commander provides facts to the communications lead. The communications lead decides what to communicate externally and when. This separation prevents the scenario where an engineer accidentally posts confidential incident details to a public status page or promises a fix timeline that the team cannot meet.

For user-facing communication during containment, the default message is acknowledgment without detail: "We are investigating reports of service degradation and are working on a fix. We will provide updates as we have them." Do not explain the attack vector. Do not estimate repair time unless you are confident. Do not apologize profusely, which signals severity and creates legal risk. Acknowledge, commit to resolution, provide updates.

The most critical communication is internal escalation. If containment is taking longer than expected, if the attack is more severe than initially assessed, if new information changes the risk profile, the incident commander must immediately escalate to the executive team and legal. The escalation message must be factual and concise: "Containment is taking longer than expected. We now believe the attacker accessed data from five thousand users, not five hundred. Legal should assess breach notification requirements." Do not wait until containment is complete to escalate new information.

## From Containment to Investigation

Containment ends when the attack has stopped and you have confidence it will not resume. For user-level isolation, this means the attacker cannot access the system. For feature-level isolation, this means the vulnerable capability is disabled and cannot be re-enabled accidentally. For system-wide shutdown, this means the system is offline and will not come back online until a fix is deployed and tested.

Once containment is achieved, the incident response shifts from action to analysis. The immediate danger has passed. Now the team must understand exactly what happened, how many users were affected, what data was accessed, and how to prevent recurrence. This is the investigation phase, and it requires different skills and different pacing than containment.

The investigation must be methodical, thorough, and evidence-based. The containment phase operates on partial information and makes fast decisions under pressure. The investigation phase has time to examine logs, reconstruct timelines, interview users, and trace the full scope of the attack. The goal is not just to understand this specific incident but to identify systemic weaknesses that allowed it to happen and will allow similar incidents in the future.

The next focus is investigation: forensic analysis of what the attacker did, how they did it, and what can be learned to prevent the next attack.
