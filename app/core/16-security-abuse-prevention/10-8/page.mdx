# 10.8 — Defense: Budget Caps and Cost Controls

You can limit tokens. You can limit requests. You can limit complexity. But if you do not limit dollars, you have not protected your business. A European healthcare company discovered this in November 2025 when a single compromised service account ran for thirty-six hours before anyone noticed. The account stayed under all token quotas. It sent requests at a rate that looked normal. Each request was individually inexpensive. But it never stopped. The attacker scripted a loop that sent one moderately expensive request every eight seconds. Over thirty-six hours, that totaled 16,200 requests at an average of four dollars each. Total damage: sixty-four thousand dollars. The token quotas were working. The budget cap did not exist.

Token quotas measure usage. Budget caps measure cost. These are not the same thing. Cost varies by model, by region, by time of day in some cases, by whether you are using cached prompts or fresh computation. An attacker who understands your pricing can stay under token quotas while maximizing cost. You need a separate control that counts dollars, not tokens, and stops the system when spending exceeds defined thresholds.

## Multi-Level Budget Architecture

Effective cost control requires budget enforcement at four levels: per-request, per-user, per-team, and organization-wide. Each level serves a different purpose. Each catches a different failure mode.

**Per-request budgets** are pre-execution checks. Before you send a prompt to the model, estimate its cost. If the estimate exceeds a per-request threshold, reject it. This prevents the single catastrophic prompt — the user who sends 200,000 tokens of context to a reasoning model and triggers a nine-hundred-dollar request. Set the per-request threshold based on your system's typical usage. For most enterprise deployments, any single request over fifty dollars should require explicit approval. For consumer-facing systems, the threshold might be five dollars. The key is that no request can silently consume an outsized share of your budget.

Estimating cost before execution requires a pricing model in your code. When a request arrives, count the input tokens, identify the target model, check whether prompt caching applies, multiply by the input token price. Estimate the expected output tokens based on the request type or historical patterns. Multiply by the output token price. Sum the total. If it exceeds your per-request threshold, return an error: "This request would cost approximately 78 dollars, which exceeds the per-request limit of 50 dollars. Please reduce context size or contact support for approval." Give the user visibility into why the request was blocked and what they can do to stay under the limit.

Per-request budgets are not quotas. Quotas accumulate over time. A per-request budget is a single-transaction limit. Even if the user has five thousand dollars remaining in their monthly budget, they cannot send a single request that costs seventy-five dollars without approval. This prevents the accidental catastrophic request — the employee who pastes an entire codebase into a prompt without realizing the cost.

**Per-user budgets** accumulate spending over a time period and enforce a maximum. Every user gets a monthly or weekly spending limit. Every request they make increments their spending total. When they hit the limit, all subsequent requests are rejected until the period resets. Per-user budgets protect against sustained abuse from a single account, whether malicious or unintentional.

Set per-user budgets based on role and usage patterns. A standard employee might get a monthly budget of two hundred dollars. An analyst or researcher who works heavily with LLMs might get two thousand. A customer support agent using an AI assistant might get fifty. Tie the budget to the user's role in your identity system. Automatically adjust budgets when roles change.

Track spending in real time. Every request must update the user's running total in a durable, strongly consistent data store. Do not batch updates or defer them to background jobs. If you delay spending updates, an attacker can exploit the race condition by sending many expensive requests simultaneously before their budget total updates. Use atomic increments in DynamoDB, Redis INCR operations, or database transactions to ensure spending updates happen synchronously with request completion.

**Per-team budgets** aggregate spending across groups of users. If your data science team has twenty members and each has a two-thousand-dollar monthly budget, the team can theoretically spend forty thousand dollars per month. You may not want that. A per-team budget enforces a collective limit. The team gets twenty thousand dollars per month, shared across all members. Individual users still have their per-user caps, but the team as a whole cannot exceed the team cap. When the team budget is exhausted, all team members are cut off, even if individuals have personal budget remaining.

Team budgets prevent cost concentration within a department. Without them, one team can accidentally consume your entire organizational budget. With them, each team has a defined envelope. They can choose how to allocate that envelope among their members, but they cannot exceed it. This creates internal accountability. Teams start tracking their own usage because it affects everyone. If one user is burning through the team's budget, the team notices and intervenes.

**Organization-wide budgets** are your final safety net. This is the absolute spending cap for your entire LLM deployment. No matter how many users, teams, or service accounts are active, the system cannot exceed this limit. Set the organization-wide budget based on your actual financial commitment. If your CFO approved fifty thousand dollars per month for AI inference costs, set the hard cap at fifty thousand. At forty thousand — 80 percent — trigger alerts to finance and engineering. At fifty thousand, stop all requests system-wide until a human intervenes.

Organization-wide budgets protect against distributed attacks and systemic failures. If an attacker compromises fifty accounts and distributes their attack across all of them, per-user and per-team budgets might not catch it in time. The organization-wide budget catches it when total spending spikes. If a configuration error causes every request to route to the most expensive model, per-request budgets might not catch it if each request is individually under the threshold. The organization-wide budget catches it when your daily spending rate jumps by 10x.

## Hard Caps, Soft Warnings, and Automatic Degradation

Budget enforcement is not binary. You have three options when a budget threshold is approached: warn, degrade, or block. The right choice depends on the context and the severity of the overage.

**Soft warnings** trigger when spending approaches a limit but has not exceeded it. At 50 percent of a user's monthly budget, log a warning. At 75 percent, email the user. At 90 percent, show an in-app banner on every request: "You have used 1,800 of your 2,000 dollar monthly budget. Budget resets on March 1." Soft warnings give users visibility and control. They can adjust their behavior before they hit the hard limit. They can request a budget increase if they have a legitimate need. Soft warnings should never block requests. They inform, they do not prevent.

**Automatic degradation** is the middle ground between warning and blocking. When a user exceeds their standard budget but has not hit an absolute cap, switch them to cheaper models. They requested GPT-5.2. They are over budget. Fulfill the request with GPT-5-mini instead. They requested Claude Opus 4.5. Fulfill it with Claude Sonnet 4.5. Inform the user of the degradation: "You have exceeded your standard budget of 2,000 dollars. This response was generated using a lower-cost model. To restore full access, upgrade your plan or wait until your budget resets on March 1."

Degradation keeps the user productive while containing cost. Many workflows can tolerate lower model quality. A support agent querying a knowledge base does not need GPT-5.2. An analyst doing exploratory research can work with GPT-5-mini. Degradation gives them continued access instead of cutting them off entirely. It also creates a natural incentive to stay within budget — users who want premium models will manage their usage to avoid degradation.

Degradation requires a model hierarchy in your configuration. Define fallback chains: if the user is over budget and requests model A, substitute model B. If they are critically over budget, substitute model C. GPT-5.2 degrades to GPT-5.1, then to GPT-5, then to GPT-5-mini. Claude Opus 4.5 degrades to Claude Sonnet 4.5, then to Claude Haiku 4.5. Each step reduces cost while maintaining functionality. The user still gets a response. The quality decreases, but the system remains available.

**Hard caps** are absolute blocks. When a budget limit is reached, all subsequent requests are rejected with a clear error message. No degradation, no exceptions. Hard caps apply at critical thresholds: the organization-wide budget limit, contractual spending limits for external customers, regulatory or compliance-driven caps. When you hit a hard cap, the system stops. Someone with financial authority must intervene to raise the cap or approve additional spending.

Hard caps must be durable and cannot be bypassed. Enforce them at the API gateway or rate limiting layer, not in application code. An attacker who compromises your application might be able to skip application-level budget checks. They cannot bypass a gateway-level cap that queries a separate budget service. The budget service should be independent, strongly consistent, and authoritative. It returns a binary answer: this user has budget remaining, or they do not. The gateway enforces that answer.

## Per-Request Cost Estimation Before Execution

Budgets are reactive if you only count spending after requests complete. By the time you know a request cost eighty dollars, you have already spent eighty dollars. You need proactive cost estimation before execution. Estimate the cost, check it against the user's remaining budget, and decide whether to proceed.

Per-request cost estimation requires a pricing model that mirrors your provider's pricing. For OpenAI models, you need input token price, output token price, and caching discounts. For Claude models, you need the same plus region-specific pricing. For models you host yourself, you need per-token compute cost and inference time cost. Keep this pricing model in a configuration file that updates whenever provider pricing changes. Your cost estimation is only accurate if your pricing data is current.

When a request arrives, tokenize the input using the same tokenizer the model will use. Count the tokens. Multiply by the input token price. Estimate the output token count based on the user's request. If they specified a maximum token count, use that. If not, use historical averages for similar requests. For open-ended requests, assume the maximum possible output length to be conservative. Multiply the estimated output tokens by the output token price. Sum the total. Add any fixed costs — embedding generation, tool calls, function execution. The result is your estimated cost.

Compare the estimated cost to the user's remaining budget. If the cost exceeds their per-request threshold, reject immediately. If the cost would push them over their daily or monthly budget, decide whether to degrade or reject based on your policy. If the cost is within all limits, allow the request to proceed. After execution, measure the actual cost and update the user's spending total with the real number. If the estimate was wrong, the difference reconciles in the spending total. You are not billing the user for the estimate — you are using the estimate to decide whether to proceed.

Estimation errors are acceptable as long as you are conservative. Overestimating cost means you might reject some requests that would have been within budget. Underestimating cost means you might allow requests that exceed budget. The second failure is worse. Always round up. If you estimate output length, use the 95th percentile of historical output lengths, not the median. If you are unsure whether caching applies, assume it does not. Overestimation creates false negatives. Underestimation creates budget overruns.

## Cost Anomaly Detection and Alerting

Budgets enforce limits. Anomaly detection catches unusual spending before limits are reached. A user who normally spends fifty dollars per week and suddenly spends five hundred dollars in a day is either doing something new and legitimate or is compromised. You need to know which.

Track spending patterns per user and per team. Calculate daily and weekly averages. Set thresholds for deviation. If a user's spending in a single day exceeds five times their daily average, flag it. If a team's spending in an hour exceeds three times their hourly average, flag it. Flags trigger alerts to security and operations. Investigate every flag. If the spending is legitimate — a planned project, a one-time analysis — document it and clear the flag. If it is not legitimate — compromised credentials, accidental runaway process — take action immediately.

Cost anomalies often appear before security incidents are detected through other means. An attacker who compromises a service account will start using it immediately. Their first action is to test the account's capabilities and limits. That testing shows up as unusual spending. If you detect the anomaly within minutes, you can disable the account before significant damage occurs. If you wait for traditional security alerts — failed login attempts, privilege escalation — the attacker may have hours or days to operate.

Build cost anomaly detection into your monitoring stack. Use the same alerting infrastructure you use for application errors and performance regressions. Cost anomalies are production incidents. Treat them with the same urgency. Page the on-call engineer when organization-wide spending spikes by 50 percent in an hour. Escalate to security when a single user's spending jumps by 10x overnight. Anomaly detection is your early warning system.

## Grace Periods and Budget Resets

Budget resets determine when users regain their full spending capacity. The reset period affects user behavior and system predictability. Monthly budgets reset on the first of the month. Weekly budgets reset on Monday. Daily budgets reset at midnight. Choose the period that matches your financial planning cycle and user expectations.

Monthly resets are standard for enterprise systems tied to accounting cycles. Your finance team budgets AI costs monthly. Your budgets should reset monthly. This creates predictability: every department knows their envelope for the month. The downside is that monthly resets create feast-and-famine usage patterns. Users conserve budget at the start of the month and burn through it at the end. This causes load spikes near month-end as everyone tries to use their remaining allocation before it disappears.

Rolling budgets smooth out these spikes. Instead of resetting on a fixed date, each user's budget resets 30 days after their last reset. User A's budget resets on the third. User B's resets on the seventeenth. Usage is distributed across the month. Rolling budgets are more complex to implement because every user has a different reset date, but they eliminate the synchronized load spike problem.

Grace periods allow users to continue working after they exceed their budget, with the understanding that the overage will be deducted from their next period's allocation. A user with a two-thousand-dollar monthly budget spends 2,300 dollars in February. Instead of cutting them off on February 25 when they hit the cap, allow them to continue. On March 1, their budget resets, but they start with a 300-dollar deficit. They have 1,700 dollars available instead of 2,000. If they go over again in March, the deficit compounds. If they stay under, the deficit clears.

Grace periods prevent work stoppage for users who have legitimate high-usage months. A researcher running a large experiment might exceed their standard budget by 20 percent. With a grace period, they complete the experiment. Without it, they stop mid-way and wait for the calendar to advance. Grace periods should have limits. Do not allow unlimited overdrafts. Cap the grace period at 20 percent of the monthly budget. If a user exceeds that, cut them off and require manual approval for a budget increase. Grace periods are for occasional overages, not systematic underbudgeting.

## Communicating Budget Status to Users

Budget caps are invisible until they bite. Users do not think about budgets until they are blocked. Make budgets visible. Show users their current spending, their remaining budget, their reset date, and their historical usage. Give them the tools to manage their own consumption.

Provide a budget dashboard. Show total spending this month, average spending per day, projected end-of-month total if current usage continues, and a breakdown by model and request type. This gives users the information they need to adjust behavior. If they see they are on track to exceed their budget by March 20, they can reduce usage now instead of being surprised when they are cut off.

Show real-time spending in the UI. After every request, display the cost: "This response cost 0.87 dollars. You have 1,342 dollars remaining this month." Make cost visible at the moment it is incurred. Users quickly learn which request types are expensive and which are cheap. They adjust behavior naturally. A user who sees that a reasoning model query costs four dollars and a standard model query costs forty cents will start choosing the cheaper model for routine tasks.

Offer cost estimation before execution for expensive requests. If the system detects that a request will cost more than five dollars, show a confirmation dialog: "This request will cost approximately 7.30 dollars. You have 1,120 dollars remaining this month. Proceed?" Let the user decide. Some will cancel and rewrite the request to be cheaper. Some will proceed because the request is worth the cost. Giving them the choice respects their agency and reduces surprise rejections.

Send proactive notifications as users approach budget limits. At 50 percent, email them. At 75 percent, show a banner. At 90 percent, email again and require acknowledgment before allowing additional requests. Users should never hit a budget cap without warning. If they do, your communication system failed.

## The Relationship Between Budgets and Quotas

Quotas and budgets are complementary, not redundant. Quotas limit volume. Budgets limit cost. You need both. A user with a 500,000-token daily quota and a fifty-dollar daily budget might hit either limit first, depending on which models they use. If they use GPT-5-mini exclusively, they will hit the token quota first. If they use GPT-5.2 exclusively, they will hit the budget cap first. Both limits serve a purpose.

Quotas protect infrastructure capacity. Budgets protect financial capacity. Quotas prevent a user from consuming all available GPU time. Budgets prevent a user from consuming all available money. An attacker who wants to cause maximum damage will try to hit the budget cap as fast as possible by using the most expensive models with the largest contexts. Quotas alone will not stop them quickly enough. Budgets provide a faster cutoff.

Configure quotas and budgets to align with your system's constraints. If your limiting factor is GPU capacity, set tight quotas and looser budgets. If your limiting factor is cost, set tight budgets and looser quotas. For most enterprise systems, cost is the binding constraint. You can scale infrastructure if needed. You cannot scale the finance department's approval for unplanned spending. Budgets are your primary control in that scenario.

Budget caps and cost controls form the financial boundary of your defense, but they only work if they can respond dynamically to system conditions. A budget cap will not save you if your system continues processing requests while costs spiral out of control. That requires circuit breakers — automated mechanisms that detect cost overruns, infrastructure failures, or abuse patterns and shut down parts of the system before damage spreads.

