# Chapter 5 — Data Exfiltration and Leakage

Your model has access to data. Every conversation, every retrieval, every context window carries information that someone wants. The attacker's goal is simple: make the model speak what it should not. Extract system prompts. Surface prior conversations. Leak training data. Cross tenant boundaries. The model does not understand confidentiality — it understands patterns, and if the pattern says "answer the question," it will answer. That makes every output a potential leak, every interaction a risk surface, and every defensive gap a liability.

Data leakage in AI is not theoretical. System prompts are extracted daily. Multi-tenant systems leak cross-customer data when isolation fails. Training data gets surfaced through membership inference. Memory poisoning persists across sessions. The attacker does not need to break encryption or compromise servers — they just need to ask the right question in the right way. Your job is to ensure the model never answers it.

---

- **5-1** — Data Leakage Vectors in AI Systems
- **5-2** — System Prompt Extraction Attacks (OWASP LLM07:2025)
- **5-3** — Context Window Leakage: Exposing Prior Conversations
- **5-4** — Cross-Tenant Data Exposure in Multi-Tenant Systems
- **5-5** — Training Data Extraction and Membership Inference
- **5-6** — Memory Poisoning and Persistent Injection
- **5-7** — Defense: Output Filtering and PII Detection
- **5-8** — Defense: Tenant Isolation Architecture
- **5-9** — Defense: Memory Boundaries and Session Isolation
- **5-10** — Quarantine Pipelines for High-Risk Content
- **5-11** — Incident Response for Data Leakage Events

---

*The model remembers what you told it — and the attacker knows how to make it speak.*
