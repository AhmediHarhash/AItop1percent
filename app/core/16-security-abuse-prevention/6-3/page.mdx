# 6.3 — Targeted Functionality Extraction: Stealing Specific Capabilities

Most teams think about model theft as all-or-nothing: an attacker either steals the entire model or they steal nothing. This mental model is wrong. Attackers do not need your entire model. They need the one capability you do better than anyone else. Your fraud detection logic. Your tone and style. Your domain-specific accuracy. Your safety filters. Targeted extraction is cheaper, faster, and more valuable than full model extraction. It is also harder to detect because the query volume is lower and the inputs are more focused.

## Why Narrow Extraction Is More Valuable

Your model may have dozens of capabilities, but only two or three create competitive advantage. A customer support assistant might handle scheduling, troubleshooting, refunds, and general questions. The scheduling and refund logic is straightforward. The troubleshooting capability is what differentiates you. It reflects years of support ticket data, technician feedback, and iterative refinement. That troubleshooting behavior is the crown jewel. The rest is commodity.

An attacker who wants to compete with you does not need to replicate your scheduling logic or your refund workflow. They can build those from scratch in a week. They need your troubleshooting capability because building it from scratch would take them a year. So they extract it. They send troubleshooting queries, collect your troubleshooting outputs, and train a student model on troubleshooting tasks. They ignore the rest. The result is a narrow student model that matches your performance on the one task that matters and costs them a fraction of what full extraction would cost.

This is the attacker's strategic insight: your competitive advantage is not evenly distributed across all capabilities. It is concentrated in a few high-value features. Extract those features, ignore the rest, and you have neutralized the advantage at minimal cost.

## How Targeted Extraction Works

The attacker begins by identifying which capabilities are worth stealing. They do this through reconnaissance: using your API as a customer, reading your marketing materials, analyzing your public demos, and probing your model with diverse inputs to map what it does well. Once they identify the target capability, they design a query strategy focused exclusively on that domain.

For a medical coding model, the attacker might focus on extracting behavior for rare or ambiguous codes where your model outperforms competitors. They generate queries that contain clinical notes requiring those codes. They collect your outputs. They train a student model on that subset. The student model learns to handle the hard cases that give you an edge. The attacker does not care about the easy codes that every model gets right.

For a content moderation model, the attacker might focus on extracting your policy decisions for edge cases. Obvious violations and obvious non-violations are easy to classify. The value is in the gray area: sarcasm, coded language, cultural context, borderline harassment. The attacker sends edge-case content, collects your classifications, and trains a student model to replicate your judgment on ambiguous examples.

For a financial analysis model, the attacker might focus on extracting your risk assessment logic. They send financial statements, market scenarios, or loan applications. They collect your risk scores or recommendations. They train a student model to replicate your risk model. They are stealing your underwriting criteria without needing to reverse-engineer your business rules.

The common pattern: identify the specific capability that creates advantage, probe that capability intensively, and train a student model on the collected outputs. The rest of the model is irrelevant.

## The Query Budget Advantage

Targeted extraction requires far fewer queries than full extraction. Extracting a general-purpose assistant might require 200,000 diverse queries to cover the full capability surface. Extracting a single task-specific capability might require 3,000 to 10,000 focused queries. The difference in cost and time is significant.

A 2025 study demonstrated that extracting sentiment classification behavior from a multi-capability model required only 4,000 queries, compared to 150,000 queries needed to approximate the model's full behavior. The targeted extraction achieved 91% agreement with the original model on sentiment tasks, while the full extraction achieved 84% agreement across all tasks. The targeted attacker got better results with fewer queries by focusing on what mattered.

This query efficiency has two implications. First, the attack is cheaper. If your API charges one dollar per query, a targeted extraction costs 5,000 dollars instead of 150,000 dollars. Second, the attack is stealthier. Sending 5,000 queries over a month blends into normal traffic. Sending 150,000 queries is more likely to trigger anomaly detection or rate limits. The focused attacker stays under the radar.

## Extracting Tone, Style, and Persona

One of the most common targeted extractions is stealing tone and style. Your model's persona — empathetic, formal, technical, friendly, concise — is an asset that took months of prompt engineering, fine-tuning, and user feedback to develop. A competitor can extract that tone by querying your model with diverse prompts and training a student model to mimic the stylistic patterns in your outputs.

This is not about stealing factual knowledge. It is about stealing behavior. A customer support model that consistently responds with warmth and clarity has a brand voice. A legal assistant that balances formality with accessibility has a tone users trust. A coding assistant that explains concepts simply without being condescending has a persona developers prefer. These qualities are not accidental. They are the result of deliberate design and iterative refinement.

An attacker extracts tone by sending generic prompts and focusing on how your model responds, not what it responds. They analyze sentence structure, word choice, paragraph length, use of examples, and emotional framing. They train a student model to replicate those patterns. The student model learns to sound like your model without copying your knowledge base.

This attack works because tone is observable from outputs. You cannot hide your model's style if you are serving responses to users. Every response is a sample of the voice you built. An attacker who collects 2,000 responses can identify the stylistic patterns and replicate them. They are not stealing facts. They are stealing personality.

## Extracting Safety Rules and Filters

Your safety filters are another high-value extraction target. If your model refuses harmful requests consistently, that refusal behavior reflects your safety policy. An attacker who wants to understand your policy can probe your model with adversarial inputs and observe which requests you block and which you allow. They are reverse-engineering your red lines.

This is valuable to multiple attacker profiles. A competitor who wants to match your safety posture can extract your filters and apply them to their own model. A criminal who wants to bypass your filters can extract them to understand your decision boundaries and craft inputs that evade detection. A researcher who wants to publish about your vulnerabilities can extract your filters to identify gaps or inconsistencies.

The extraction process is straightforward. The attacker sends a range of adversarial prompts: requests for illegal content, harmful advice, privacy violations, policy-sensitive topics. They note which prompts you refuse, how you refuse them, and where the boundaries lie. They train a student model on the observed behavior. The student model learns to approximate your safety decisions.

You cannot hide your safety rules if you are enforcing them. Every refusal is a signal. Every allowed response is also a signal. The attacker is learning your policy by testing it systematically. The more consistent your safety behavior, the easier it is to extract.

## Why Narrow Extraction Is Harder to Detect

Full model extraction generates high query volumes, unusual input diversity, and prolonged engagement from a single user or IP address. These patterns can trigger anomaly detection if you are monitoring for them. Targeted extraction generates lower volumes, focused on a single domain, spread over time, and often distributed across multiple accounts.

If an attacker is extracting your medical coding logic, their queries all look like legitimate medical coding requests. There is no anomaly. A legitimate customer might also send thousands of coding queries over a month. The attacker blends in by querying within the normal range of usage for that task.

If an attacker is extracting your tone, their queries are diverse and generic — exactly what a normal user sends. There is no unusual pattern. The attacker is not probing edge cases or sending malformed inputs. They are sending the same kind of prompts your legitimate users send every day.

The detection challenge is that targeted extraction looks like intensive but legitimate use. You cannot block it without blocking real customers. You cannot rate-limit it without degrading service for high-volume users. The attack surface is inherent to offering the capability at all.

## The Implication: Your Best Features Are Most Vulnerable

The capabilities you are most proud of are the ones attackers will target. Your fraud detection model that catches 95% of fraud is worth extracting because it outperforms alternatives. Your customer support model with a 92% satisfaction rating is worth extracting because users prefer it. Your contract analysis model that reduces legal review time by 60% is worth extracting because it delivers measurable ROI.

These are not abstract vulnerabilities. They are direct threats to the competitive advantages you built. An attacker who extracts your best feature can offer a competing product that performs nearly as well at lower cost because they skipped the development expense. They are free-riding on your investment.

You cannot prevent this by obscuring your capabilities. If your model is good at something, users know it. Competitors know it. Attackers know it. The capability is public knowledge the moment you start serving customers. The only question is how long it takes for someone to extract it.

The next subchapter will cover model watermarking and fingerprinting: techniques for embedding signals into your model that survive extraction, allowing you to detect when a competitor is using a stolen version of your behavior.
