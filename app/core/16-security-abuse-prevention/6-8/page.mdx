# 6.8 — Defense: Behavioral Fingerprints for Scraping Detection

What does extraction look like? Not the output, not the volume, but the shape of the queries themselves — the sequence, the coverage, the rhythm. Attackers can evade rate limits, randomize timing, split across accounts. They cannot hide the fact that extraction queries follow patterns that legitimate use does not. A user exploring your model behaves like a human with goals. An attacker extracting your model behaves like a script with a coverage plan.

That difference is a fingerprint. You can detect it, measure it, and block it — not by analyzing individual queries, but by analyzing the structure of query sequences over time. Extraction is not a single suspicious query. It is a systematic campaign. The fingerprint is in the campaign.

## The Extraction Query Pattern

Legitimate users query models to solve problems. They have tasks, deadlines, uncertainty. Their queries cluster around real work: "What is the revenue impact of this contract clause?" followed by "How does this compare to our standard terms?" followed by "What risks should Legal review?" The sequence makes sense. Each query builds on the previous answer. The coverage is narrow — the user cares about one contract, not the entire universe of possible contracts.

Extraction queries follow a different logic. The attacker wants coverage, not answers. They do not care what your model says about one contract clause. They want to know what it says about every clause, every contract type, every jurisdiction, every edge case. The queries do not build on each other. They tile across the input space like a grid search.

In mid-2025, a legal AI provider analyzed query logs from a suspected extraction attempt. Legitimate users queried an average of 6.2 distinct contract topics per session, with query-to-query similarity averaging 0.71 — each query related to the previous one. The suspected attacker queried 340 distinct topics over two weeks, with query-to-query similarity averaging 0.18. The attacker was not using the model. They were mapping it.

The fingerprint appears in three dimensions: diversity, coverage, and progression.

## Diversity: How Scattered the Queries Are

Diversity measures how much the queries vary within a session or time window. High diversity means the user jumps between unrelated topics. Low diversity means the user stays focused on one problem space.

Legitimate use typically shows low to moderate diversity. A customer support agent queries about account issues, password resets, billing disputes — all related to support workflows. A developer queries about authentication logic, error handling, edge cases in one feature — all related to the code they are writing. Even a power user exploring multiple unrelated tasks shows clusters of focus: several queries about topic A, then a shift to topic B, then topic C. The diversity is locally low, even if globally high.

Extraction shows uniformly high diversity with no clustering. Every query is deliberately chosen to differ from the previous one. The attacker is not asking follow-up questions. They are executing a sampling strategy. If the first query is about employment contracts, the second is about real estate leases, the third is about non-disclosure agreements, the fourth is about licensing terms. There is no narrative thread. There is only coverage.

A machine learning security team built a diversity score in late 2025 based on pairwise cosine similarity between query embeddings. For each user session, they calculated the average similarity between consecutive queries. Legitimate users averaged 0.63. Extraction attempts averaged 0.22. The threshold for flagging was set at 0.35. Over six months, the system flagged 89 accounts. Manual review confirmed 84 as extraction or automated abuse. The false positive rate was under six percent.

## Coverage: How Much of the Input Space Gets Queried

Coverage measures how much of the model's capability space the user explores. Narrow coverage means the user stays in one domain. Broad coverage means the user systematically touches many domains.

Legitimate users have narrow coverage by default. They use the model for specific tasks. A financial analyst queries about valuation models, risk assessment, market trends — all within finance. They do not suddenly query about medical diagnoses, legal precedent, or software debugging. The coverage is domain-constrained because the user's job is domain-constrained.

Extraction requires broad coverage. The attacker wants a model that replicates your model's full capability. They cannot extract only the finance queries and ignore the legal ones. They need representative samples from every domain your model handles. The result is a query distribution that looks nothing like any individual user and very different from your aggregate user base.

A healthcare AI company tracked domain coverage by categorizing queries into clinical specialties: cardiology, oncology, neurology, endocrinology, and fifteen others. Legitimate clinician accounts averaged 1.8 specialties per month — most doctors work in one or two areas. One account queried all nineteen specialties in eleven days, with roughly equal representation across domains. The account belonged to a newly registered user who had never submitted any queries before that week. The company blocked the account and found evidence of similar patterns from four other accounts created in the same registration batch. All five accounts were traced to an IP range associated with a known AI model resale operation.

The coverage fingerprint is strongest when combined with account age and usage history. A brand-new account that immediately queries across your entire capability space is almost always malicious. A long-standing account that suddenly shifts from narrow to broad coverage deserves investigation.

## Progression: How Queries Evolve Over Time

Progression measures whether queries follow a logical sequence or an algorithmic one. Legitimate users refine, expand, and pivot based on what they learn. Attackers follow a script.

A legitimate query sequence shows exploration with memory. The user asks a question, receives an answer, and their next query reflects engagement with that answer: "How do I structure a partnership agreement?" followed by "What are the tax implications of that structure?" followed by "Can this work if one partner is based in Germany?" Each step builds on the previous one. The user is thinking.

An extraction query sequence shows no memory. Each query is independent. The attacker does not care what the previous answer said. They are not solving a problem — they are filling a matrix. "How do I structure a partnership agreement?" followed by "What is the boiling point of mercury?" followed by "How does zoning law apply to mixed-use buildings?" The sequence is incoherent because the goal is not coherence. The goal is coverage.

In mid-2025, a conversational AI provider built a progression detector that scored query sequences on coherence. The detector used a fine-tuned classifier trained on legitimate conversation logs to predict whether the next query in a sequence was likely given the previous queries. High prediction confidence indicated natural progression. Low confidence indicated scripted or random queries. Accounts with sustained low coherence scores over multiple sessions were flagged for review. The system detected twelve extraction campaigns in the first four months, all of which manual review confirmed as malicious.

## Machine Learning for Extraction Detection

The diversity, coverage, and progression signals can be combined into a supervised learning model trained to classify user sessions as legitimate or extraction attempts. The model takes query sequences as input and outputs a probability score. High scores trigger rate restrictions, manual review, or automated blocking.

The training data comes from two sources: labeled extraction attempts from past incidents and labeled legitimate use from long-term trusted accounts. The model learns the patterns that distinguish the two. It generalizes beyond the specific fingerprints you manually coded. It adapts as attackers change tactics.

A cybersecurity AI company deployed a random forest classifier in late 2025 trained on features extracted from query logs: diversity score, coverage breadth, coherence score, session length, query timing distribution, account age, and twelve other signals. The model achieved 94 percent precision and 89 percent recall on a held-out test set. In production, it flagged 3.2 percent of sessions for review. Of those, 87 percent were confirmed as malicious or policy violations.

The model does not need to be perfect. It needs to be good enough to make extraction risky. If an attacker knows that sustained extraction has a 90 percent chance of detection within 48 hours, most attackers will not try. The ones who do try will be detected before they extract enough data to be useful.

## The Arms Race: Attackers Adapting to Detection

Behavioral fingerprinting is not a permanent solution. It is a layer in an arms race. Attackers who understand your detection system will adapt their extraction strategy to evade it.

If you detect high diversity, they will cluster queries into fake sessions that mimic narrow focus, then rotate between topics across sessions instead of within them. If you detect broad coverage, they will extract from multiple accounts, each focusing on a narrow domain, and merge the results later. If you detect low coherence, they will use a language model to generate follow-up queries that appear to build on previous answers, even though the answers are ignored.

Every fingerprint you add, attackers can theoretically evade. The question is cost. Adapting extraction to evade behavioral detection requires more sophistication, more infrastructure, more time. The attacker must now solve not just "how do I query this model?" but "how do I query this model in a way that looks like a human with a real task?" That is a harder problem. Most attackers will not solve it. The ones who do will spend more resources doing so.

A language model API provider tracked attacker adaptations over eight months in 2025 after deploying behavioral fingerprinting. The first month, attackers showed no adaptation — they continued systematic extraction and were detected quickly. By month three, some attackers began introducing delays and fake follow-up queries to simulate coherence. By month six, one sophisticated attacker was using a secondary language model to generate plausible query progressions. The provider responded by adding a new signal: query originality. The fake follow-ups generated by the attacker's language model contained linguistic patterns typical of synthetic text — low lexical diversity, formulaic phrasing, overuse of transition words. The system flagged the account within two days of the new detection layer going live.

The arms race never ends. Your goal is not to win permanently. Your goal is to stay ahead long enough that extraction becomes unsustainable for most attackers.

## Real-Time Blocking vs Post-Hoc Analysis

Once you detect extraction fingerprints, you have two options: block the attacker in real time or let them continue while you gather evidence.

Real-time blocking stops the attack immediately. The moment diversity drops below threshold or coverage exceeds normal bounds, the account is rate-limited or blocked. The advantage is speed. The disadvantage is that you learn nothing about the attacker's methods, infrastructure, or goals. You stop one attack but gain no intelligence for stopping the next one.

Post-hoc analysis allows the attack to continue while you monitor it. You log every query, track the attacker's patterns, identify associated accounts, and map their infrastructure. After enough evidence accumulates, you block the entire operation and implement new defenses tailored to their techniques. The advantage is intelligence. The disadvantage is risk — the attacker extracts more of your model while you watch.

The choice depends on the stakes. If the attacker is extracting high-value proprietary capabilities, block immediately. If the attacker is probing for weaknesses or extracting lower-value outputs, consider monitoring to build a richer defense profile.

A legal AI provider adopted a hybrid approach in mid-2025. Low-confidence extraction signals triggered increased monitoring but no blocking. High-confidence signals triggered immediate rate restrictions. Sustained high-confidence signals over 48 hours triggered a block and a security review. The approach balanced protection with learning. Over six months, the team documented five distinct extraction methodologies, built defenses for each, and reduced their median time-to-detection from nine days to eighteen hours.

The next layer of defense is not detecting extraction through query patterns, but through API usage patterns — the metadata around queries rather than the queries themselves. Even attackers who perfectly mimic human query sequences leave traces in how they interact with your API. That is API access pattern analysis.

---

**Next: 6.9 — Defense: API Access Patterns and Anomaly Detection**
