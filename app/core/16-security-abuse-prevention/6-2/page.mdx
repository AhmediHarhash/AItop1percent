# 6.2 — Query-Based Model Extraction: Distilling Via API

The attacker does not need your model file. They do not need your training data. They do not need access to your infrastructure. They only need your API. Every query they send and every response you return becomes a training example for their student model. Send enough queries, collect enough outputs, and they can train a model that replicates your behavior. This is knowledge distillation as an attack. It works. It scales. It is happening to your API right now.

## The Distillation Attack Workflow

The attack has four steps. Each step is straightforward. None require insider access or sophisticated exploits. The attacker is using your API exactly as you designed it to be used.

**Step one: define the target capability.** The attacker decides what they want to steal. If your model is a general-purpose assistant, they might want to extract your tone, your factual accuracy, or your instruction-following behavior. If your model is task-specific, they want the task. A medical coding model, a contract summarization model, a fraud detection model — the attacker wants to replicate the specific capability that gives you competitive advantage.

**Step two: generate queries.** The attacker creates inputs designed to probe your model's behavior. These inputs can be synthetic, sampled from public datasets, or constructed to cover the input space they care about. For a medical coding model, the queries are clinical notes. For a contract summarization model, the queries are legal documents. The attacker does not need perfect inputs. They need inputs that are representative enough to capture your model's decision boundaries.

**Step three: collect outputs.** The attacker sends the queries to your API and records every response. Each query-response pair becomes a labeled training example. Your model's output is the label. The attacker is using your production model as an oracle that generates high-quality labels for free.

**Step four: train the student model.** The attacker takes the collected query-response pairs and fine-tunes a student model to replicate your outputs. The student model learns to mimic your behavior. If the attacker collected enough representative examples, the student model will generalize. It will produce outputs similar to yours on new inputs it has never seen.

This is not a hypothetical attack. It is the standard workflow for model distillation, and it has been used successfully in published research and real-world theft cases.

## How Many Queries to Approximate a Model

The number of queries required to extract a useful model depends on task complexity, model behavior, and attacker goals. The research from 2024 and 2025 provides clear benchmarks. The numbers are lower than most teams expect.

For narrow task-specific models, the attacker needs surprisingly few examples. A 2024 paper demonstrated that a medical coding model could be distilled with 8,000 queries, achieving 87% of the original model's accuracy. Another study extracted a sentiment analysis model with 5,000 queries, reaching 92% agreement with the original on held-out test cases. A legal contract classification model was extracted with 12,000 queries, matching the original on 89% of test examples.

These numbers are achievable with modest budgets. If your API charges ten cents per query, 10,000 queries cost 1,000 dollars. If you charge one dollar per query, the cost is 10,000 dollars. This is pocket change compared to the cost of building the model from scratch. The attacker's constraint is not budget. It is query generation and data collection logistics.

For broader capabilities, the attacker needs more queries, but the numbers are still feasible. Extracting general instruction-following behavior from a large model requires hundreds of thousands of queries. But attackers do not need to extract everything. They extract the parts they care about. A competitor who only wants your fraud detection logic does not need to replicate your general reasoning. They send fraud-related queries and train a student model on fraud tasks. Narrow extraction is cheaper and more effective than full extraction.

## Why API Outputs Are High-Quality Labels

Your model is not just returning predictions. It is returning high-quality labeled examples that encode your expertise. When an attacker queries your medical coding API with a clinical note and you return the correct ICD-10 code, you have given them a labeled training example. That example reflects the decisions of your domain experts, the edge cases you debugged, and the policy interpretations you refined over months. The attacker gets all of this knowledge for the cost of a single API call.

This is why distillation attacks work so well. Training a model from scratch requires labeled data. Labeling is expensive and time-consuming. The attacker bypasses this cost by using your model as the labeler. Your API is a labeling service that the attacker does not have to build or pay for. They are outsourcing their data labeling to your production model.

The quality of the stolen model is directly proportional to the quality of your outputs. If your model achieves 94% accuracy, the attacker's student model can approach that level by learning from your outputs. They are not training on ground truth. They are training on your approximation of ground truth. But for most practical purposes, that is good enough.

## Active Learning Attacks: Smart Query Selection

A naive attacker sends random queries and hopes to cover the input space. A sophisticated attacker uses active learning to select queries strategically. Active learning minimizes the number of queries needed by focusing on the examples that teach the student model the most.

The attacker starts by training an initial student model on a small set of queries. They evaluate the student model and identify inputs where it is uncertain or where it disagrees with your API. Those inputs are the most informative. The attacker queries your API on those specific examples, collects your outputs, and adds them to the training set. This process repeats iteratively. Each round, the student model improves. Each round, the attacker selects the next batch of queries based on what the student model does not yet know.

Active learning attacks are more efficient than random sampling. A 2025 study showed that active learning reduced the number of queries needed to extract a classification model by 60% compared to random querying. The attacker reached the same performance with 4,000 queries that a naive attacker needed 10,000 queries to achieve. The implication is clear: a well-resourced attacker with machine learning expertise can extract your model with fewer queries than you think.

This is not science fiction. Active learning is a standard technique in machine learning. Applying it to model extraction is straightforward. Any attacker with ML background can implement it in a weekend.

## The Economics: Query Cost Versus Training Cost

Model extraction is economically rational because the cost of querying is lower than the cost of training. Consider a competitor who wants to build a medical coding model. Training from scratch requires acquiring 60,000 labeled examples, hiring domain experts to review them, running evals on edge cases, and iterating through multiple training runs. The timeline is nine months. The cost is 800,000 dollars.

Extraction requires sending 10,000 queries to your API and training a student model on the outputs. The timeline is two weeks. The cost is 10,000 dollars in API fees plus 5,000 dollars in compute for student training. Total cost: 15,000 dollars. The extracted model achieves 89% accuracy compared to your 94%. The competitor saves 785,000 dollars and eight and a half months.

This is not a marginal advantage. This is a fifty-to-one return on investment. The competitor who extracts your model can undercut your pricing, iterate faster, and allocate their saved budget to other parts of their product. You spent months building a defensible advantage. They neutralized it in two weeks.

The economic asymmetry extends beyond initial extraction. Once the attacker has a student model, they can continue to improve it by querying your API on hard cases, monitoring your API for behavior changes, and retraining periodically to keep the student model aligned with your updates. They are piggybacking on your ongoing investment. Every improvement you make to your model becomes a free update to theirs.

## Why You Cannot See the Attack Happening

The attack is invisible because every query looks legitimate. The attacker is not exploiting a vulnerability. They are not bypassing authentication. They are using your API as a normal customer would. They send requests. You send responses. The interaction is indistinguishable from legitimate use.

You cannot rate-limit your way out of this problem without breaking legitimate use cases. If you limit each user to 100 queries per day, an attacker can create multiple accounts or spread queries over weeks. If you require payment, attackers will pay. The cost of extraction is low enough that paying API fees is still cheaper than building from scratch.

You cannot detect extraction by looking at query patterns unless the attacker is careless. A sophisticated attacker will randomize query timing, vary input sources, and mimic the distribution of legitimate traffic. They will not send 10,000 identical queries in a single batch. They will send diverse queries over days or weeks, blending into normal usage.

The attack surface is structural. If you expose an API, you expose behavior. If you expose behavior, you expose training data. The only way to prevent extraction entirely is to not offer an API. But if you do not offer an API, you cannot serve customers.

The next subchapter covers targeted functionality extraction: how attackers steal specific capabilities instead of the entire model, why narrow extraction is easier and more valuable, and how to recognize when someone is probing your model for specific features.
