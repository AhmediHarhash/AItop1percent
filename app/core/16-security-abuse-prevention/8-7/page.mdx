# 8.7 — The Provenance Ladder: Source Trust Scoring

Not all documents deserve the same level of trust. A policy document written by your legal team and reviewed by three senior attorneys is fundamentally different from a PDF a user uploaded five minutes ago. A knowledge base article authored internally and maintained through version control has nothing in common with content scraped from a third-party website. Yet most RAG systems treat every document identically once it passes ingestion. They chunk it, embed it, index it, and retrieve it with equal confidence. When an attacker uploads a poisoned document, the system retrieves it with the same authority it would give to your company's founding charter.

The retrieval system does not know which sources to trust. You have to teach it.

## The Four-Tier Provenance Model

**The Provenance Ladder** defines trust tiers for every document in your knowledge base. Each tier represents a different level of vetting, authorship control, and risk exposure. When you ingest a document, you assign it a provenance tier based on its source, author, and validation history. That tier influences retrieval ranking, citation prominence, and downstream trust signals.

**Tier 0: Curated and Verified Internal Content.** Documents written by your organization, reviewed by subject matter experts, versioned in a controlled system, and updated through formal change management. Legal policies. Product documentation maintained by technical writers. Training materials authored by domain experts. These documents represent ground truth. When they conflict with other sources, they win. When the model needs to cite authority, these documents appear first. Tier 0 content gets a permanent trust boost in retrieval scoring. In some implementations, Tier 0 documents are the only sources eligible for high-stakes queries like compliance or safety-critical instructions.

**Tier 1: Trusted External Partners and Vetted Sources.** Documents from partners you have formal agreements with, vendors whose content you audit regularly, industry standards bodies, regulatory agencies. You did not write them, but you trust the source. A contract from a law firm you have worked with for ten years. A technical specification from an industry consortium. A regulatory guideline published by a government agency. Tier 1 documents carry high trust, but not absolute trust. If they conflict with Tier 0 content, Tier 0 wins. You retrieve them confidently, but you cite their provenance explicitly.

**Tier 2: User-Uploaded with Light Validation.** Documents submitted by authenticated users who passed basic validation checks. Employee-uploaded meeting notes. Customer support tickets. Internal reports from field teams. You trust the user who uploaded the document, but the document itself has not been reviewed. Tier 2 content is useful for breadth, but it requires skepticism. The retrieval system deprioritizes Tier 2 documents when Tier 0 or Tier 1 alternatives exist. Citations from Tier 2 sources include disclaimers: "This information comes from a user-submitted report and has not been independently verified."

**Tier 3: Scraped, Unverified, or External.** Content from web scraping, public datasets, third-party APIs, or anonymous sources. You have no relationship with the author. You have no validation pipeline. You indexed it because it might be useful, but you treat it as potentially adversarial. Tier 3 documents appear in retrieval results only when no higher-tier alternatives exist. They carry prominent provenance warnings. In high-risk applications, Tier 3 content is excluded entirely from certain query types. If you allow it at all, you never cite it as authoritative.

## Trust Scoring in Retrieval

Provenance tiers are not binary gates. They are weights. When your retrieval system ranks documents by semantic similarity, it also considers provenance score. A Tier 0 document with 85% semantic match can outrank a Tier 3 document with 92% semantic match. The exact weight depends on your risk tolerance, but the principle is universal: source trust modulates retrieval confidence.

In a financial services RAG system deployed in 2025, Tier 0 documents received a 1.5x multiplier in retrieval scoring. Tier 1 received 1.2x. Tier 2 received 1.0x. Tier 3 received 0.6x. The effect was dramatic. When a customer asked about loan requirements, the system cited the bank's official policy manual over a scraped forum post, even when the forum post had slightly higher semantic similarity. When an attacker uploaded a document claiming the bank offered zero-interest loans, the retrieval system ranked it so low that it never appeared in top-five results unless the query explicitly mentioned the attacker's uploaded filename.

Provenance scoring does not eliminate adversarial documents. It reduces their influence. If an attacker injects fifty poisoned documents into Tier 3, they compete with each other for the bottom of the ranking. If one squeaks into retrieval results, its low provenance tier signals the downstream prompt logic to treat it skeptically. The model may retrieve it, but it does not trust it enough to act on it.

## Dynamic Provenance Updates

Provenance is not static. A document that starts as Tier 2 can become Tier 0 after review. A Tier 0 document can drop to Tier 1 if it becomes outdated. Provenance scoring depends on audit trails, version history, and human attestation.

When a user uploads a document, it enters as Tier 2 by default. If a subject matter expert reviews it and approves it, the system promotes it to Tier 1. If the document is then formally adopted into company policy and versioned in the internal content management system, it becomes Tier 0. Conversely, if a Tier 0 document has not been updated in three years and newer Tier 0 documents contradict it, the system flags it for review. If no one confirms its accuracy, it drops to Tier 1 until someone validates it again.

A healthcare RAG system handling clinical guidelines implemented provenance decay. Every Tier 0 document had a "last reviewed" timestamp. If more than eighteen months passed without review, the document dropped to Tier 1 automatically. If three years passed, it dropped to Tier 2. The system sent alerts to content owners six months before decay. The result was a self-maintaining trust model. High-trust documents stayed current. Stale documents lost influence before they caused harm.

## Displaying Provenance to Users

Users need to see provenance. When the model cites a document, the citation should include its tier. Not as a technical label — users do not care about "Tier 1" — but as human-readable context. A citation from internal policy might display as "Source: Company Policy Manual, verified and current as of January 2026." A citation from a user-uploaded document might display as "Source: Field report submitted by regional team, not independently verified." A citation from a scraped website might display as "Source: External website, treat as reference only."

Provenance transparency changes user behavior. When users see that a response is based on unverified content, they ask follow-up questions. They seek second sources. They escalate to human experts. When they see that a response is based on curated internal policy, they act with confidence. Provenance is not just a ranking signal for retrieval. It is a trust signal for humans.

In a legal tech system launched in mid-2025, every RAG response included a provenance summary at the bottom: "This response is based on three Tier 0 sources (verified case law), one Tier 1 source (regulatory guidance), and two Tier 2 sources (attorney notes)." Lawyers learned to read the summary before taking action. If a response relied heavily on Tier 2 content, they treated it as a draft to verify. If it relied exclusively on Tier 0 content, they treated it as authoritative. The provenance summary turned the RAG system from a black box into a transparent research tool.

## Provenance as Attack Surface Reduction

Attackers target low-provenance pathways. If your system treats all documents equally, the attacker uploads fifty poisoned documents and hopes semantic similarity pulls one into retrieval. If your system scores by provenance, the attacker must either compromise a Tier 0 source — which is hard — or accept that their Tier 3 content will be buried under higher-trust alternatives.

Provenance ladders do not stop injection attacks. They contain them. An attacker who poisons a Tier 3 document affects only queries where no better source exists. An attacker who poisons a Tier 2 document affects authenticated-user scenarios but not policy-driven ones. The damage is localized. The blast radius shrinks.

The hardest attack becomes credential compromise. If the attacker gains access to an account authorized to create Tier 0 content, provenance scoring fails. This is why provenance tiers must pair with access controls, audit logs, and review workflows. A single authenticated user should not be able to upload a document and immediately mark it Tier 0. Tier 0 requires approval. Tier 1 requires validation. Even Tier 2 requires authentication and rate limits. Provenance without access control is theater.

## Implementation: Scoring and Metadata

Provenance tiers are stored as document metadata at ingestion. When you chunk a document for embedding, every chunk inherits the parent document's provenance tier. Your retrieval system queries the vector database, retrieves candidate chunks, applies semantic similarity scoring, then applies provenance multipliers. The final ranking reflects both relevance and trust.

Some implementations use separate provenance models for different query types. High-risk queries — anything involving money, health, legal action — retrieve only from Tier 0 and Tier 1. Medium-risk queries allow Tier 2 with citation warnings. Low-risk queries allow Tier 3 for breadth. The system routes queries by intent and applies provenance filters accordingly.

Other implementations surface provenance as a user-facing filter. The user can toggle "show only verified sources" to restrict retrieval to Tier 0 and Tier 1. Power users who need breadth disable the filter. Novice users who need safety enable it. The choice is explicit.

## The Curation Tax

Provenance ladders create operational cost. Someone must assign tiers. Someone must review Tier 2 documents for promotion. Someone must audit Tier 0 documents to prevent decay. This is the **curation tax** — the ongoing human cost of maintaining trust. You can automate parts of it. Automated systems flag documents that have not been reviewed in eighteen months. Automated systems detect when a user uploads a document with similar content to existing Tier 0 documents and suggest merges. Automated systems track citation frequency and flag low-provenance documents that appear in high-stakes queries. But the final decision is human.

The alternative is treating all documents equally, which means treating curated policy the same as adversarial uploads. That is not a trade-off. That is negligence. Provenance ladders are the minimum defense for any RAG system that ingests content from multiple sources. The curation tax is the cost of operating responsibly.

But provenance alone does not stop injection. It assumes documents are what they claim to be. It does not catch adversarial instructions hidden inside otherwise legitimate content. For that, you need sanitization pipelines that inspect every document before ingestion and strip anything that looks like a prompt.

