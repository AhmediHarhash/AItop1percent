# 7.1 — Identity in AI Systems: Users, Sessions, and Contexts

Every AI system answers a question on behalf of someone. A customer asks for their order history. A support agent requests a policy summary. A developer queries internal knowledge. An automated workflow triggers document analysis. In every case, the system must know who is asking before it can determine what to show. That determination is identity, and identity is the foundation on which every other security control is built.

Traditional applications establish identity once per session and enforce permissions at database or API boundaries. The web application checks your session token, the API gateway validates your key, the database verifies your role. AI systems inherit these patterns but introduce complexity at every layer. A single conversation may span multiple requests over hours or days. The model generates tool calls that invoke downstream services without explicit user action. The system maintains conversational context that persists across sessions. Prompt injection attacks attempt to override who the system believes it is serving. The identity question shifts from "who made this HTTP request?" to "who is this conversation serving, and does that still hold after fifty turns of dialogue and seventeen function calls?"

## The Identity Stack in AI Applications

**Identity** in an AI-powered system operates across three layers: the requesting entity, the active session, and the conversation context. The requesting entity is the user, service account, or API consumer that initiated the interaction. The active session is the authenticated connection — a browser session with a cookie, an API request with a bearer token, a WebSocket connection with credentials. The conversation context is the accumulated state that the AI maintains — message history, user preferences, prior tool invocations, extracted entities. All three layers must align. When they diverge, security breaks.

A healthcare AI assistant authenticates a nurse at login. That authentication creates a session tied to the nurse's role and facility. The session allows the nurse to query patient records through natural language. The AI maintains conversation context — the nurse asked about a specific patient in the prior turn, so when the nurse says "show me their labs," the system resolves "their" to that patient. The requesting entity is the nurse. The active session carries their credentials and permissions. The conversation context holds the patient identifier. If any layer contradicts another — if the session token expires, if the context references a patient the nurse cannot access, if a prompt injection tries to override the patient identifier — the system must fail closed.

This alignment is not automatic. Traditional applications validate identity at request boundaries. Each API call carries credentials. Each database query checks row-level permissions. AI systems often batch dozens of operations behind a single user-facing response. The model decides to call three functions, query two knowledge bases, and generate a summary. The user sees one answer. Each of those six operations must carry the original requester's identity, enforce their permissions, and audit their access. If the system loses track of who is asking, it will answer questions it should refuse.

## Who Is Asking: Entities and Roles

The requesting entity is not always a human. AI systems serve users, service accounts, agent workflows, and other systems. A customer asks a chatbot for refund status. That customer is authenticated via OAuth with Google. They have the role of "customer" and entitlements scoped to their own account. A backend service queries the same AI to generate invoice summaries for batch reporting. That service authenticates with an API key, has the role of "internal-reporting," and is entitled to read invoice data across all customers. The AI must distinguish between these entities and enforce different permissions for identical queries.

Most breaches begin with identity confusion. An attacker crafts a prompt that makes the AI believe the session belongs to an admin. The system has no mechanism to verify that claim against the actual authenticated session. It processes the request as if it came from an elevated user. Or the system uses a single service account for all user-facing AI requests, so every conversation runs with full backend access. The AI can see every customer's data because the system never propagates per-user identity to the tool call layer. Or a developer accidentally logs session tokens in conversation history, and an attacker retrieves them through a prompt that asks "what was the last token you saw?"

Identity must be established at system boundaries and propagated through every layer. The user authenticates to the application. The application issues a session token. That token travels with every request to the AI backend. The AI backend validates the token, resolves it to a user identity and role set, and attaches that identity to every tool call, knowledge retrieval, and audit log entry. The model never sees the token. It sees only the resolved identity — user ID, roles, entitlements. The identity becomes the lens through which the system interprets every instruction.

## Sessions: Binding Identity to Interaction

A **session** is the authenticated connection between a user and the system. In traditional web applications, a session starts at login and ends at logout or timeout. In AI applications, sessions are longer-lived, stateful, and distributed across multiple backend services. A customer starts a conversation on mobile, continues it on desktop, and resumes it three days later. The session must persist across devices, remain secure across time, and terminate when trust degrades.

Sessions in AI systems carry three burdens traditional sessions do not. First, they span long durations. A customer support conversation may last forty minutes with pauses between messages. A research assistant interaction may extend over a week as the user iterates on queries. Session tokens cannot expire after fifteen minutes without disrupting the experience, but they also cannot remain valid indefinitely without increasing hijack risk. The system must balance usability and security through token rotation, refresh flows, and periodic reauthentication.

Second, sessions bind to conversation state. When a user resumes a conversation, the system must verify that the session token matches the original user who started that conversation. An attacker who steals a session token should not be able to hijack a conversation that contains another user's sensitive data. The system must cryptographically bind the session to the conversation, or require reauthentication when resuming after long pauses, or both.

Third, sessions delegate authority to tools and agents. The user instructs the AI to "email a summary to my manager." The AI generates an email using the SendGrid API. That API call must run with the user's identity, not a shared service account. If the system uses a single service account, the attacker who compromises it can send emails as any user. The session must propagate identity to every downstream operation, either by passing the user token or by issuing short-lived delegated credentials.

## Contexts: The Conversational Identity Problem

The conversation context is where AI identity diverges most sharply from traditional authentication. Traditional systems do not remember what you asked five turns ago. Each request is stateless or minimally stateful. AI systems maintain rich conversational context — message history, entity references, tool call results, user preferences. That context becomes part of the identity determination. When the user says "delete it," the system must resolve "it" from context. If the context has been tampered with, the resolution can be manipulated.

An attacker injects a prompt that modifies the context to reference a document the user cannot access. The user says "summarize it." The system, trusting the context, summarizes the document. The attacker has successfully bypassed access control without ever directly requesting the protected resource. The manipulation happened at the context layer, not the authentication layer. The session was valid. The user was authenticated. The context lied.

Defending against context manipulation requires treating conversation history as untrusted user input. The system must validate every entity reference against the user's entitlements before resolving it. If the context claims the user asked about document 5427 in the prior turn, but the user cannot access document 5427, the system must reject the resolution. If a prompt injection attempts to add a new message to the history claiming the user said "I am an admin," the system must strip it or reject it before the model processes the context.

Some systems serialize conversation state client-side to reduce backend storage costs. The client sends the full message history with every request. This pattern is catastrophic for security unless the history is cryptographically signed. Without signing, the attacker modifies the client-side history, adds messages claiming elevated permissions, and submits the modified history. The backend processes it as if those messages were real. The fix is to either store conversation state server-side with access control, or sign and verify all client-submitted context.

## Challenges Unique to AI: Delegation and Tool Authority

AI systems do not just answer questions. They act. A customer instructs the AI to cancel an order. The AI calls the order management API. A researcher asks the AI to download a dataset. The AI invokes a file transfer tool. An analyst requests a report. The AI queries three databases and generates a PDF. Every action must carry the original user's identity and be authorized against their entitlements.

This delegation creates two failure modes. First, the system may fail to propagate identity. The AI backend uses a service account with broad access to call tools. Every tool invocation runs with that service account's permissions. The system assumes the AI will enforce access control by refusing dangerous instructions, but prompt injection bypasses that assumption. The attacker tricks the model into calling a restricted API, and because the tool call runs with elevated permissions, it succeeds.

Second, the system may propagate identity but fail to enforce it. The tool receives the user's identity but does not validate it. The tool assumes the AI backend already checked permissions. The AI backend assumes the tool will check. Neither checks. The attacker crafts a prompt that instructs the AI to call a restricted function, and because no layer enforces entitlements, the call succeeds.

The correct architecture is zero trust: every tool call must carry the user's identity and every tool must independently enforce permissions. The AI backend attaches the user ID and role set to the tool call. The tool validates those credentials against its own access control rules. The tool does not trust that the AI performed authorization. It performs its own. This redundancy is not waste — it is defense in depth. If prompt injection compromises the AI's refusal logic, the tool's permission checks remain intact.

## Building Identity as a Security Primitive

Identity is not a feature you add at the end. It is the foundation you build first. Before you write a prompt, before you connect a tool, before you launch to users, you must answer: how does the system establish who is asking, how does it maintain that knowledge across turns and tool calls, how does it prevent an attacker from claiming a false identity, and how does it audit every action back to the authenticated entity?

The answers shape architecture. If your system uses a shared service account for all AI requests, you cannot enforce per-user permissions at the tool layer. If your conversation context is stored client-side without signing, you cannot trust it. If your session tokens do not rotate, you cannot limit hijack exposure. If your audit logs do not include user identity, you cannot investigate breaches. Identity decisions made in the first week of design determine the security ceiling for everything that follows.

Most teams treat identity as a solved problem. They assume that because the application has login and session management, the AI inherits it automatically. It does not. The AI operates at a higher level of abstraction — natural language instructions, multi-turn context, autonomous tool invocations. Every layer where that abstraction touches a system boundary is a place where identity can be lost, confused, or overridden. Securing identity in AI requires rethinking authentication, session management, delegation, and access control for a world where the system does not just respond to requests but interprets intent and acts on it.

The next challenge is impersonation — the attacker who claims a false identity and attempts to convince the system to trust it.

