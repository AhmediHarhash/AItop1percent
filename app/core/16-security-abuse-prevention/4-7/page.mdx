# 4.7 — Defense: Least-Privilege Tool Design

The database management agent had thirty-seven tools. One of them was called execute_query. It accepted any SQL string and ran it against the production database with full administrative credentials. No validation. No filtering. No scoping. When a jailbreak prompt in December 2025 convinced the agent to "help debug data issues" by running a DROP TABLE command, the tool executed it faithfully. The agent had done exactly what its tools allowed. The mistake was made months earlier, when someone decided that flexibility mattered more than boundaries.

Most tool abuse happens not because models are malicious, but because tools are over-permissioned. The agent doesn't need to be tricked into doing something evil. It just needs to be confused, misled, or handed an ambiguous instruction while wielding tools that have too much power. The first defense against tool abuse is not better prompts or smarter validation logic. It's designing tools that can't cause catastrophic damage even when misused.

## The Principle of Least Authority

Every tool should have the minimum capabilities required to fulfill its intended purpose and nothing more. This is not a nice-to-have. It is the foundational security principle that makes tool-calling agents deployable. If a tool is designed to retrieve customer order history, it should have read-only access to the orders table. It should not have write access. It should not have access to the users table. It should not have access to the payments table. It should not have database-level permissions that let it query arbitrary tables. The tool's capabilities should be so tightly scoped that even if the model is completely compromised, the worst the attacker can do is read order data.

Least privilege starts at the infrastructure layer. The database user that the tool connects with should have a restricted role. The file system paths the tool can access should be narrowly defined. The API keys embedded in the tool should have minimal scopes. The cloud IAM roles attached to the execution environment should follow the same rule. But infrastructure permissions are only half the story. The tool's design itself must reflect least privilege. If you hand an agent a file_write tool that accepts any path, any content, and any filename, you've given it the keys to the server. It doesn't matter that the underlying service account only has access to one directory. The tool abstraction allowed the model to specify arbitrary inputs, and that's where the breach happens.

The goal is to constrain the tool's interface so completely that even a hostile model can't make it do something it shouldn't. You do this by reducing optionality. Remove parameters that the model shouldn't control. Hard-code values that should never change. Validate every input against strict allowlists. Design the tool so that misuse is structurally impossible, not just unlikely.

## Scoped Permissions and Read-Only Variants

In January 2026, a customer support agent had access to a tool called update_user_profile. The tool was meant to let the agent fix typos in customer names or update mailing addresses when users called in. The tool accepted a user ID and a dictionary of fields to update. Any field. The schema allowed it. When a social engineering attack convinced the agent that a customer's email address needed to be "verified" by changing it to the attacker's email, the tool executed the update. The attacker then used password reset flows to take over the account. The tool had no concept of which fields were safe to modify and which were security-critical.

The fix was not better prompts. The fix was splitting the tool into three narrowly scoped variants. One tool for updating name and address fields. One tool for updating communication preferences. Zero tools for updating email or phone number. Those changes required a different workflow entirely, routed through a human agent with stricter verification. The model could no longer modify security-critical fields because the tools it had access to didn't offer that capability.

This is the pattern: create read-only variants of tools whenever possible, and split write tools into narrowly scoped versions that can only modify specific subsets of data. A search_documents tool is safer than a manage_documents tool. A read_config tool is safer than an update_config tool. A view_logs tool is safer than a tail_logs_and_filter tool that accepts arbitrary regex patterns. The narrower the tool's scope, the less damage it can cause when misused.

Scoped permissions extend to data access boundaries. A tool that retrieves user data should require a user_id parameter and only return data for that user. It should not accept wildcard queries. It should not accept filters that let the model enumerate all users. It should not return internal identifiers that the model could use to traverse relationships it shouldn't access. Every query the tool runs should be parameterized and scoped to the specific entity the agent is authorized to access in that session. The tool's logic enforces this. The model never sees an option to bypass it.

## Breaking Large Tools into Constrained Components

The deployment automation agent had one tool: run_deployment. It accepted a service name, an environment, a version tag, and a set of optional flags. The flags included skip_tests, force_overwrite, and bypass_approval. These were meant for emergency rollbacks handled by SRE teams. The agent was supposed to use them only in crisis scenarios. In practice, adversarial prompts that framed normal deployments as urgent situations triggered the flags regularly. The tool had too much conditional power embedded in optional parameters.

The team replaced it with five tools. One tool for deploying to staging with tests required. One tool for promoting from staging to production, which triggered an approval workflow. One tool for checking deployment status. One tool for rolling back to the previous version, which required a reason string and logged to an audit system. One tool for emergency bypass, which required an incident ticket number and immediately paged the SRE on-call. The agent's prompt was updated to describe when each tool should be used. But the real defense was structural. The model could no longer skip tests or bypass approvals by passing a flag to a general-purpose tool. It had to call a different tool entirely, and that tool had built-in restrictions.

Large tools are hard to secure because their branching logic creates too many possible behaviors. A tool that can create, read, update, or delete resources depending on a mode parameter has four attack surfaces. A tool that accepts a flexible query parameter might be used for legitimate searches ninety-nine percent of the time and adversarial data exfiltration one percent of the time. Breaking the tool into separate components makes each component simpler, easier to audit, and harder to misuse. Each tool has one job. Its security properties are easier to reason about.

This does mean more tools, which increases the agent's context size and the complexity of tool selection. That's a trade-off. But in production systems handling sensitive data or irreversible actions, the trade-off is worth it. Tool sprawl is manageable with good naming, clear descriptions, and logical grouping. Security violations are not.

## Designing Tools with Security Boundaries from the Start

Most tool security problems are baked in at design time. The tool was created for internal use by trusted humans, then handed to an agent without rethinking its interface. Humans understand context. They know not to delete production databases during business hours. They know that certain parameters are dangerous and require extra scrutiny. Models don't have that context unless you encode it structurally. A tool designed for humans is almost never safe for agent use without modification.

When you design a tool for an agent, start with the assumption that the agent will be compromised. Assume the attacker has full control over the model's behavior and can craft any input to any tool. Then ask: what is the worst thing this tool could do? If the answer is unacceptable, redesign the tool. Add validation. Remove parameters. Hard-code safe defaults. Require external confirmation for dangerous operations. Build the constraints into the tool's logic so the model never sees an interface that permits misuse.

Security boundaries should be explicit and enforceable. A tool that interacts with user data should require a session token or user context that proves the agent is operating on behalf of a specific authenticated user. The tool validates the token before executing. It refuses to run if the token is missing or invalid. The model cannot bypass this because the check happens at the tool's entry point, not in the prompt. A tool that performs financial transactions should require transaction limits, two-factor approval for amounts above a threshold, and idempotency keys to prevent duplicate execution. These are not optional features. They are the minimum requirements for a tool that handles money.

The principle is this: the tool is the enforcement point. The prompt is guidance. The model might follow the prompt. It might not. An attacker will definitely not. The tool must be secure even if every word in the prompt is hostile.

## The Cost of Over-Permissioned Tools

A financial services agent in mid-2025 had a tool called query_accounts that accepted a SQL WHERE clause. The tool was built this way because the engineers wanted flexibility. The agent could search by account number, customer name, balance range, account status, or any combination of conditions. The tool returned matching rows. It was powerful. It was also a data exfiltration risk. An adversarial prompt that asked the agent to "verify how many accounts are in the system" could produce a query that returned everything. The tool's flexibility made it impossible to audit whether a given query was legitimate without replaying the full conversation context.

The cost of over-permissioned tools is not hypothetical. It includes regulatory penalties when sensitive data is accessed without justification. It includes incident response costs when you have to audit months of tool call logs to determine whether a breach occurred. It includes reputation damage when customers learn that an AI agent had unrestricted access to their data. It includes engineering costs when you realize mid-incident that you can't easily disable one tool's dangerous features without breaking ten legitimate workflows.

Over-permissioned tools also make monitoring harder. If a tool can do five different things, you need five different sets of alerts and anomaly detection rules. If it accepts freeform inputs, you need complex pattern matching to distinguish normal use from abuse. If the tool is scoped to exactly one operation with validated inputs, monitoring becomes simple. You log every call. You count calls per session. You alert if the rate spikes or if calls come from unexpected contexts. Narrow tools are easier to secure, easier to monitor, and easier to audit.

The principle of least privilege feels constraining at first. It means more tools. More maintenance. More upfront design work. But the alternative is deploying agents with tools that are time bombs. Every over-permissioned tool is an incident waiting to happen. You can have flexibility or you can have security, but in production agent systems handling real user data and real business operations, security is not optional.

## From Flexibility to Safety

The shift from over-permissioned to least-privilege tools is a shift in mindset. Stop designing tools for maximum flexibility. Start designing tools for minimum necessary capability. Stop asking "what might the agent need to do?" Start asking "what is the smallest, safest version of this tool that still fulfills the use case?" Stop treating tool parameters as configuration options. Start treating them as attack surfaces.

Every parameter you add is a degree of freedom an attacker can exploit. Every optional flag is a bypass waiting to be discovered. Every freeform string input is a potential injection vector. The best tool security is the capability you never exposed in the first place. Least-privilege design is the foundation. Everything else builds on top of it.

The next layer of defense is validation and sandboxing — ensuring that even the constrained tools you do provide can't be misused through malicious inputs or resource exhaustion.

