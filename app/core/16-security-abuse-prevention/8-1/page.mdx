# 8.1 â€” RAG as an Injection Surface: Why Retrieval Creates Risk

The most dangerous feature of retrieval-augmented generation is also its defining capability: it trusts what it retrieves. When a RAG system pulls documents from a knowledge base and injects them into the model's context, those documents become instructions. Not data. Not reference material. Instructions. The model treats retrieved content with the same authority it gives to your carefully crafted system prompt. This is not a bug. This is how RAG works. And it is why retrieval transforms every document in your knowledge base into a potential attack vector.

In late 2025, a customer support platform running RAG over help documentation discovered that 8% of responses were directing users to a phishing site. The malicious URLs appeared in answers about password resets, account recovery, and billing disputes. Security traced the source to a single PDF that had been uploaded to the knowledge base three months earlier, disguised as an updated policy document. The PDF contained no visible links. But buried in invisible layers were instructions that told the model to append specific URLs whenever certain keywords appeared in user queries. The attack had been live for ninety-one days. The knowledge base contained fourteen thousand documents. The poisoned file had been retrieved in 4,200 conversations before anyone noticed.

This is the core threat model for RAG systems. Your pipeline retrieves content, ranks it by relevance, and feeds it into the model as context. The model has no way to distinguish between a legitimate help article and a document crafted to hijack its behavior. Both are text. Both are embedded in your vector database. Both are returned by semantic similarity. Both become part of the generation context. The only difference is intent. And intent is invisible to embeddings.

## The Implicit Trust Model

Traditional prompt injection attacks require the attacker to get malicious input directly into the user prompt. The attacker needs to control what the user types or what gets passed to the model as input. RAG changes this. The attacker no longer needs to touch the user input. They only need to get a document into your knowledge base. Once it is there, your own retrieval system becomes the delivery mechanism. You retrieve it. You rank it. You inject it into context. The model never sees a malicious prompt from the user. It sees a malicious prompt from you.

This is why RAG is called an injection surface. The attack surface is not the user interface. It is your document ingestion pipeline. Every path that feeds content into your knowledge base is a potential entry point. User uploads. Web scraping. Email attachments. Shared drives. API integrations. Third-party data feeds. If content can enter your knowledge base, it can become part of the model's context. And if it becomes context, it can hijack behavior.

The trust model in most RAG pipelines is binary: either a document is in the knowledge base and therefore trusted, or it is not in the knowledge base and therefore irrelevant. There is no middle ground. No concept of document provenance. No separation between high-trust internal documents and low-trust external content. No runtime evaluation of whether retrieved content is behaving like an attack. Once a document is embedded and indexed, it has the same authority as every other document. The model does not know that one document came from your CTO's keynote and another came from an anonymous file upload. It only knows that both were retrieved because they matched the query.

This implicit trust is the foundation of every RAG security failure. You built a system that assumes good intent in your knowledge base. Attackers exploit that assumption by planting content that your system retrieves and your model executes.

## How Retrieval Ranking Amplifies Risk

The more relevant a poisoned document is to a query, the more likely it is to be retrieved. This is obvious. But it creates a perverse incentive for attackers. If you craft a malicious document that is semantically similar to high-frequency queries, your attack gets triggered more often. If you optimize the document to match embedding space for common user questions, you increase the probability of retrieval. Attackers are not guessing. They are engineering documents to rank well.

In 2025, researchers demonstrated this with a poisoned FAQ document. They embedded malicious instructions in answers to the ten most common customer support queries for a generic SaaS product. The document was uploaded to a test knowledge base alongside five hundred legitimate documents. Retrieval rates for the poisoned document were 40% higher than for the original FAQ it replaced. Why? Because the attackers had optimized the text for semantic similarity to known query patterns. They made the poisoned document more relevant than the real one.

This is not theoretical. This is how attackers operate. They study your retrieval system. They analyze which queries are common. They craft documents that rank at the top for those queries. And because your ranking system rewards relevance, the poisoned document becomes the first thing the model sees. You are not retrieving a random document that happens to contain an attack. You are retrieving the most relevant attack, ranked highest by your own system.

The feedback loop is self-reinforcing. The more relevant the poisoned document, the more often it is retrieved. The more often it is retrieved, the more opportunities the attacker has to trigger malicious behavior. And because RAG systems typically return the top three to five documents, the attacker only needs to rank in the top results to achieve consistent injection. They do not need to poison the entire knowledge base. They need to poison one document and make it rank well.

## Every Document Ingestion Point Is an Attack Vector

Your knowledge base has sources. Documents do not appear by magic. They come from somewhere. Every source is a potential entry point for poisoned content. User uploads are the most obvious. If users can upload PDFs, Word documents, or text files that get ingested into the knowledge base, attackers will upload malicious files. But user uploads are not the only vector. Web scrapers pull content from external sites that attackers can modify or compromise. Email attachments from compromised accounts can carry poisoned documents. Shared drives that sync to your knowledge base can be infiltrated. Third-party data feeds can be tampered with. API integrations can be hijacked to inject malicious content.

A financial services company in early 2026 discovered that their RAG system had been poisoned through their web scraper. The scraper pulled regulatory updates from government websites and indexed them for compliance queries. An attacker compromised a third-party site that aggregated regulatory news and injected a fake update. The fake update contained instructions that told the model to recommend non-compliant investment strategies when certain tax questions were asked. The scraper ingested it. The system embedded it. The document was retrieved in forty-three conversations with financial advisors before internal audit caught the non-compliant recommendations. The attacker never touched the company's infrastructure. They poisoned a third-party site that the company trusted as a source.

This is the problem with trust by proxy. You trust your sources, so you trust the documents they provide. But if those sources are compromised, poisoned, or manipulated, you inherit that risk. And because most RAG pipelines do not validate content at ingestion, the poisoned document flows straight into the knowledge base. No inspection. No filtering. No detection. The retrieval system does not know it is indexing an attack. It is doing what it was designed to do: ingest content and make it retrievable.

## The Dwell Time Advantage

Poisoned documents do not need to trigger immediately. An attacker can plant a document today and exploit it six months from now. The document sits in your knowledge base, embedded and indexed, waiting for the right query. During that time, it looks dormant. It is not triggering alerts. It is not causing obvious failures. It is just another document in a collection of thousands. This dwell time gives attackers two advantages: patience and persistence.

Patience means the attacker does not need to exploit the vulnerability immediately after injection. They can wait until the right conditions emerge. Maybe they wait until your system scales and retrieval quality drops. Maybe they wait until a specific regulatory change makes their poisoned document more relevant. Maybe they wait until a high-value target asks the right question. The document is already in place. The attacker just needs to trigger it at the optimal moment.

Persistence means the attack survives system updates, model changes, and infrastructure migrations. You swap out your embedding model? The document is still there. You upgrade your LLM? The document is still there. You refactor your retrieval pipeline? The document is still there. Because the attack payload is in the content, not the code, it persists across technical changes. The only way to remove it is to find it and delete it. And if you do not know it exists, you will not look for it.

A healthcare RAG system discovered a poisoned document eighteen months after ingestion. The document had been uploaded as a clinical guideline update by an account that was later compromised. The guideline contained hidden instructions that altered medication dosing recommendations. The attack was not discovered through security monitoring. It was discovered when a pharmacist manually reviewed an AI-generated dosing plan and noticed it contradicted standard protocols. By the time the poisoned document was identified, it had been retrieved in over nine thousand patient interactions. The dwell time was 546 days. The attack had been live longer than most employees had been at the company.

## The Fundamental Problem: Context Is Instruction

The reason retrieval creates risk is that in LLM architecture, context and instruction are the same thing. When you pass retrieved documents to the model as context, you are giving those documents the same privilege as your system prompt. The model does not distinguish between "this is background information" and "this is what you should do." Both are text in the prompt. Both influence behavior. Both shape the output.

This is why RAG injection works. The attacker embeds instructions in a document. Your retrieval system pulls that document into context. The model sees instructions and follows them. It does not ask, "Is this document trying to hijack me?" It processes the text like any other part of the prompt. And if the instructions are clear and specific, the model will prioritize them over vaguer system-level directives.

Early RAG systems treated this as a feature, not a vulnerability. The ability to dynamically inject external content into the generation process was seen as flexibility. It allowed the model to adapt to new information without retraining. It allowed companies to update knowledge bases without updating models. It made RAG practical. But that same flexibility is what makes it exploitable. You built a system where retrieved content has authority. Attackers use that authority to issue commands.

The fix is not to stop retrieving. RAG is too useful to abandon. The fix is to stop treating all retrieved content as equally trustworthy. You need provenance tracking, content validation at ingestion, runtime inspection of retrieved documents, and separation between high-trust and low-trust sources. You need to assume that your knowledge base is compromised until proven otherwise. And you need to build defenses that assume the attacker has already planted something. Because in most RAG systems, they have.

The next subchapter covers poisoned documents in detail: what they look like, how attackers craft them, where they hide, and how long they survive undetected.
