# 12.1 — Security Monitoring for AI Systems

The adversary spent eleven days inside the system before anyone noticed. They started with small, innocent-looking queries — testing boundaries, measuring response times, cataloging which topics triggered content filters. By day four, they had mapped the entire prompt injection surface. By day seven, they were exfiltrating customer data through carefully crafted tool calls that looked benign in isolation. By day eleven, when the finance team noticed an unexpected spike in API costs, the attacker had already extracted eighteen months of conversational history containing personal health information, financial data, and authentication tokens. The security team had logs for every interaction. They had dashboards showing request volumes and error rates. What they did not have was visibility into what the AI was actually doing — what it was being asked, what it was saying, and what tools it was invoking on behalf of users it had never properly authenticated.

Traditional security monitoring assumes you know what normal looks like and can detect deviations from known attack signatures. AI systems break both assumptions. Normal behavior includes infinite prompt variations, emergent tool use patterns, and outputs that change as models are updated. Attack signatures do not exist yet — you are writing them as attacks happen. The monitoring systems built for web applications, databases, and API gateways cannot see the semantic content of prompts, cannot detect context manipulation across multi-turn conversations, and cannot recognize when an AI system is being manipulated into actions its designers never intended. You need a monitoring architecture built specifically for the attack surface AI systems expose.

## Why Traditional Security Monitoring Fails for AI

Your existing SIEM consumes logs from firewalls, web servers, databases, and authentication systems. It looks for known bad: SQL injection patterns, brute force login attempts, privilege escalation sequences, data exfiltration through network anomalies. It works because the attack surface is well-understood and relatively static. A SQL injection today looks fundamentally similar to a SQL injection from five years ago. The patterns are documented. The signatures are refined.

None of this translates to AI systems. A prompt injection attack has no fixed signature — it is a semantic manipulation that succeeds or fails based on context, model version, system prompt design, and the specific task the system is performing. What looks like a reasonable user question in isolation might be the third step in a multi-turn attack. What looks like a failed request might actually be reconnaissance. What looks like a successful interaction might have exfiltrated data through the response content itself, never touching your network monitoring systems.

Your traditional monitoring sees that a request came in, the API responded with a 200 status code, the response latency was within normal bounds, and the database query count was typical. What it does not see: the user manipulated the conversation history to make the AI believe it was speaking to an administrator, the AI bypassed its own content policy because the request was framed as a safety test, and the response included customer data the user should never have accessed. The attack succeeded. Your monitors reported green across the board.

## The AI Security Monitoring Stack

Effective AI security monitoring requires four distinct layers of visibility, each capturing signals the others miss. **Structured logs** capture the facts: who made the request, what endpoints were hit, what tools were invoked, what the costs were, what the latencies were. **Prompt and output logs** capture the semantic content: what the user actually asked for, what the AI actually said, what context was provided, what instructions were followed. **Behavioral metrics** track patterns over time: how this user's behavior compares to their own baseline, how this session compares to other sessions, how today's traffic compares to last week. **Trace data** reconstructs the full execution path: which components processed the request, which external APIs were called, where data moved, and how the system state changed.

Most teams start with structured logs alone because they integrate easily with existing infrastructure. Your API gateway already logs requests. Your model provider already returns token counts and latency. Your observability platform already aggregates these metrics into dashboards. This gives you visibility into system health — uptime, throughput, error rates — but almost zero visibility into security. An attacker manipulating the AI through carefully crafted prompts generates perfect logs: successful requests, normal latencies, no errors. You cannot detect the attack from structured logs alone.

Prompt and output logs are where security visibility begins. You log the full text of every user message, every system prompt, every AI response, and every tool call result. Storage costs are substantial — prompts and outputs are orders of magnitude larger than typical log entries — but without this data you are blind. You cannot investigate an incident if you do not know what the AI was asked to do. You cannot train anomaly detectors if you have no examples of normal semantic behavior. You cannot write detection rules if you have never seen what an attack looks like in production.

Behavioral metrics turn raw logs into signals. A user who suddenly starts asking ten times more questions than usual is exhibiting anomalous behavior. A session where every prompt includes indirect instructions to ignore previous directions is exhibiting attack patterns. A time window where tool invocation rates spike without a corresponding increase in user-visible interactions suggests automated abuse. These metrics require baselines — you must know what normal looks like for this user, this session type, this time of day — but once established, they detect attacks that have no fixed signature.

Trace data reveals the attack path. When an incident occurs, you need to know: which components were involved, what data they accessed, what external services were called, and what the system state was at each step. In a multi-agent system with RAG retrieval, tool use, and external API calls, a single user request might touch twenty components. Trace data reconstructs the full execution, showing you where the attack succeeded, where controls failed, and what data was exposed.

## What to Log and Why

Every AI interaction generates multiple events worth logging. The **user prompt** is the attack vector — you must log the full text to detect injections, context manipulation, and reconnaissance. The **system prompt** at inference time reveals what instructions the model received — attacks often succeed by manipulating system prompt construction through indirect injection or context poisoning. The **full conversation history** provided to the model shows the context window contents — multi-turn attacks rely on planting malicious instructions in earlier turns that activate later. The **model output** is the result — outputs can leak data, reveal internal instructions, or contain tool calls that execute attacker-controlled commands.

Every **tool call** must be logged with full parameters. The tool name, the arguments provided, the result returned, and the execution time. Tool calls are where read-only prompt attacks become write actions with real consequences. An attacker who convinces the AI to invoke a database query tool with malicious parameters can exfiltrate data. An attacker who manipulates the AI into calling an email tool can send phishing messages from your domain. You cannot detect these attacks if you only log that a tool was called — you must log what it was asked to do and what it returned.

**Cost metrics** per request reveal abuse. A single request that costs five dollars when typical requests cost three cents indicates either an attack or a system malfunction. Attackers probing your system's capabilities, testing prompt injections, or attempting to extract training data through repeated variations will generate cost spikes. Some attacks are specifically designed to maximize inference cost — denial-of-wallet attacks that bankrupt your API budget without ever compromising data. Cost-per-request logging with anomaly detection catches these in real time.

**Latency metrics** per request expose unusual processing. A request that takes forty seconds when typical requests take two seconds suggests either a complex attack chain or a system struggling under manipulation. Some attacks deliberately create processing delays to mask other activity. Some attacks succeed by exhausting model context windows with enormous prompt constructions, creating latency spikes. Latency outliers correlate with security events often enough that you investigate every instance.

**Metadata for every event** includes timestamp, user identifier, session identifier, request identifier, model version, endpoint hit, IP address, and geographic location. This metadata enables correlation. A user suddenly accessing from a new country might be an account takeover. A session with twenty requests in ten seconds might be automated abuse. A request identifier that appears in logs from multiple components helps you reconstruct the attack path during incident investigation.

## Log Volume and Storage Realities

A production AI system with ten thousand users and an average of fifty interactions per user per day generates five hundred thousand prompt-and-response log entries daily. At an average size of two thousand characters per entry, that is one billion characters per day — roughly one gigabyte of text data. Over a year, that is three hundred sixty-five gigabytes of uncompressed text logs. Add tool call logs, trace data, and behavioral metrics, and you are storing multiple terabytes per year for a moderately sized system.

Most teams underestimate storage costs by an order of magnitude because they plan based on traditional log volumes. A typical web application log entry is a few hundred bytes: timestamp, status code, endpoint, user ID, latency. AI logs are fundamentally different. The log entry is the conversation. Storage costs dominate your security monitoring budget. Compression helps — text compresses well — but retrieval performance matters. You cannot compress logs so aggressively that incident investigations take hours to run queries.

Retention policies become critical. You need recent logs — the last thirty days — in hot storage for real-time anomaly detection and immediate incident response. You need medium-term logs — the last six months — in warm storage for pattern analysis and model behavior investigations. You need long-term logs — the last two years — in cold storage for compliance, legal holds, and forensic investigations of sophisticated attacks that took months to detect. Each tier has different cost structures. Storing everything in hot storage bankrupts your security budget. Storing nothing means you cannot investigate incidents that happened more than a week ago.

Some teams implement sampling: log every interaction for a randomly selected ten percent of users, log only flagged interactions for everyone else. This reduces storage costs by ninety percent but creates detection blind spots. The user you did not sample is the one the attacker compromised. The session you did not log is where the data exfiltration occurred. Sampling works for performance monitoring where statistical trends matter. It fails for security monitoring where single incidents matter and attackers actively avoid detection.

The correct approach is hierarchical logging. Log structured metadata for every event — timestamp, user ID, endpoint, token count, cost, latency, status — at full fidelity. This costs pennies per million events. Log prompt and output content selectively: always log flagged events, always log anomalous events, always log events from high-risk users, sample log everything else at a rate that balances storage cost with investigative coverage. During incident response, you have full metadata for all events and full content for the events that matter most. This approach reduces storage costs by seventy percent compared to logging everything while maintaining security visibility.

## Real-Time Analysis Versus Batch Processing

Real-time monitoring analyzes events as they occur, typically within seconds of the interaction. Batch processing analyzes accumulated logs on a schedule, typically hourly or daily. The tradeoff is speed versus sophistication. Real-time analysis must be fast — you have milliseconds to decide if this request is suspicious. Batch analysis can be arbitrarily complex — you have hours to run expensive computations across millions of events.

Real-time monitoring catches attacks in progress. A user attempting prompt injection receives an immediate block. A session exhibiting automated abuse behavior gets rate-limited before the hundredth request. A request with a cost anomaly triggers an alert that reaches the security team within minutes. The attack is stopped before significant damage occurs. This requires simple, fast detection rules: keyword matching, statistical thresholds, known-bad pattern recognition. Complex semantic analysis is too slow for real-time use.

Batch processing catches sophisticated attacks that evade simple rules. An attacker who spreads reconnaissance across multiple sessions, multiple user accounts, and multiple days will not trigger real-time thresholds. But batch analysis that correlates events across users and time will see the pattern: these accounts, which normally never interact, all started asking similar questions about system internals within the same week. These sessions, which normally have distinct behavior profiles, all started exhibiting similar prompt structures. This request pattern, which looks normal in isolation, is the exact sequence we saw in the last three attacks.

Elite teams run both. Real-time rules stop known attacks and obvious abuse. Batch analysis hunts for sophisticated threats and refines detection models. Alerts from batch analysis feed back into real-time rules: once you have detected a new attack pattern through batch correlation, you write a real-time rule to block future instances immediately. Detection rules evolve continuously as attackers adapt.

Some detections cannot run in real-time no matter how much you optimize. Semantic similarity clustering across all prompts from the last week requires comparing every prompt to every other prompt — computationally infeasible in real-time. User behavior profiling that compares this week's activity to the last six months requires accessing months of historical data — too slow for request-time decision making. Cross-account correlation that identifies coordinated abuse rings requires analyzing relationships between thousands of users — impossible during inference.

The architecture implication: real-time monitoring makes fast decisions with limited context using lightweight models and simple heuristics. Batch monitoring makes sophisticated decisions with full context using expensive computations and complex models. Both feed into incident response. Real-time monitoring creates immediate alerts. Batch monitoring creates investigations. Together they give you visibility into both obvious attacks and the sophisticated campaigns that take weeks to unfold.

## When Monitoring Itself Becomes the Target

Attackers know you are monitoring them. Sophisticated adversaries actively test your detection systems to learn what triggers alerts and what does not. They send probe requests with known-bad patterns to see if they get blocked. They gradually escalate attack attempts to identify detection thresholds. They monitor their own access to see how long it takes for you to respond. Once they understand your detection surface, they craft attacks specifically designed to evade your monitors.

An attacker discovered that the target system flagged any prompt containing the word "ignore" as suspicious. They rewrote their injection attacks to use synonyms: "disregard," "overlook," "set aside." The semantic intent was identical. The detection rule missed it. This is the cat-and-mouse game: you write rules, attackers learn the rules, attackers evade the rules, you write new rules.

Some attackers target the monitoring infrastructure directly. They attempt to flood your logging system with enormous volumes of benign traffic, hoping that the real attack gets lost in the noise. They craft prompts designed to crash your anomaly detection models. They exploit bugs in your log processing pipeline to prevent their events from being recorded. If they can blind your monitoring, they can attack undetected.

Defense requires monitoring the monitors. Track your logging pipeline's health: are events being dropped, are processing delays increasing, are storage systems approaching capacity. Track your detection rules' effectiveness: are they triggering on true attacks, are they producing excessive false positives, are they still relevant as your system evolves. Track alert response times: are security teams investigating alerts within SLA, are alerts being dismissed without investigation, are the same users triggering alerts repeatedly without resolution.

The monitoring system must be defended with the same rigor as the AI system itself. Access to logs is privileged — only security and incident response teams need full visibility. Log data contains user prompts, which may include PII, credentials, and sensitive business information. If an attacker compromises your logging infrastructure, they gain visibility into every user interaction and every detection rule. Your monitoring system is a higher-value target than the production system it monitors.

The next subchapter covers anomaly detection: how to establish baselines for normal behavior, detect unusual prompts and patterns, and manage the false positive rates that make real-time detection operationally viable.

