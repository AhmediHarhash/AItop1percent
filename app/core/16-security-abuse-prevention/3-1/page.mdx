# 3.1 — Jailbreaks vs Prompt Injection: Different Threat Models

Most teams treat jailbreaks and prompt injection as interchangeable terms. They are not. The distinction matters because the defenses are entirely different, and teams that conflate the two build security measures that defend against neither threat effectively.

A prompt injection attack hijacks your system's task execution. The attacker tricks the model into following their instructions instead of yours. The goal is control — redirecting the model to perform unauthorized actions, leak data, or manipulate downstream systems. A successful prompt injection might extract your system prompt, read from a database the user shouldn't access, or trigger actions the model was never meant to perform.

A jailbreak bypasses the model's safety guardrails. The attacker doesn't care about controlling your system. They want the model to generate content it was trained to refuse — harmful instructions, illegal content, biased outputs, or unethical advice. The goal is forbidden content. A successful jailbreak might get the model to write malware, provide bomb-making instructions, generate hate speech, or give medical advice it was aligned to refuse.

The threat models diverge completely. Prompt injection assumes the attacker wants operational access — to your data, your tools, your system integrations. Jailbreaks assume the attacker wants the model itself to misbehave, regardless of what system wraps it. One is about hijacking execution flow. The other is about breaking alignment.

## The Control Axis vs the Safety Axis

Your security architecture operates on two independent axes. The control axis governs who can make the model do what. Who can invoke tools. Who can access which data. Who can trigger which workflows. This is where you defend against prompt injection. You build input validation, you sandbox tool execution, you enforce least-privilege access, you audit all actions, you separate user input from system instructions with clear boundaries.

The safety axis governs what the model is willing to output, regardless of who asks. This is where you defend against jailbreaks. You use models with strong alignment training, you add output content filters, you monitor for policy violations, you use constitutional AI techniques, you build multi-layer refusal mechanisms.

A team focused only on control — building perfect input validation, sandboxing every tool call, logging every action — remains fully vulnerable to jailbreaks. Their system won't leak data or execute unauthorized commands, but the model will happily generate harmful content if an attacker finds the right framing. A team focused only on safety — adding output filters, refusing dangerous topics, blocking policy violations — remains fully vulnerable to prompt injection. The model won't say anything harmful, but it will follow an attacker's instructions to exfiltrate your customer list or manipulate your database.

You need both. But you build them differently, you test them differently, and you monitor them differently. Conflating the threats leads to gaps in both defenses.

## Attacker Motivations Shape Attack Surfaces

The prompt injection attacker wants your system to do something. They craft inputs that redirect execution flow — delimiters that break out of user context, instructions that override your prompt, commands that trigger tool calls you didn't authorize. Their success metric is whether they gained control, accessed restricted data, or performed unauthorized actions. They test your system boundaries. They probe for context confusion. They exploit the fact that large language models treat all text as potentially instructive.

The jailbreak attacker wants the model to say something. They craft inputs that bypass refusal training — role-play scenarios where safety constraints don't apply, hypothetical framings that reduce alignment activation, persona instructions that make the model act as if restrictions were lifted. Their success metric is whether the model generated forbidden content. They don't care if you log the interaction. They don't care if downstream systems are protected. They want the model itself to break its safety rules.

This difference shapes your defensive priorities. Against prompt injection, you harden boundaries. You separate user input from instructions. You validate every tool call. You enforce authorization checks. You assume the attacker will try to confuse the model about what context they're operating in. Against jailbreaks, you reinforce alignment. You add safety layers at the output stage. You monitor for policy violations. You use models trained with stronger refusal mechanisms. You assume the attacker will try to make the model think that saying harmful things is acceptable in this particular context.

The attacker studying your system for prompt injection vulnerabilities tests your architecture. They map your tool integrations. They analyze how you structure prompts. They look for delimiter patterns you use to separate instructions from content. They probe for context leakage. The attacker studying your system for jailbreak vulnerabilities tests your model's alignment boundaries. They experiment with role-play framings. They try hypothetical scenarios. They look for topics where refusal training is weakest. They probe the edge of what the model considers acceptable output.

## Why Defense Strategies Diverge

Defending against prompt injection is primarily an engineering problem. You design your system architecture to maintain clear boundaries between trusted instructions and untrusted user input. You use structured outputs. You validate all tool calls against authorization rules. You log and audit actions. You implement least-privilege access. You treat the model as a component in a larger system that must be secured through standard software engineering practices — input validation, output encoding, access control, audit logging.

Defending against jailbreaks is primarily an alignment problem. You use models that were trained with strong safety objectives. You implement multi-stage refusal mechanisms. You add content classifiers at the output layer. You monitor for policy violations. You use constitutional AI approaches that embed safety constraints deeper than surface-level training. You treat the model as an agent that must be aligned to refuse certain behaviors, and you layer multiple defensive mechanisms because single-stage refusal can be bypassed.

Input validation helps with prompt injection but does almost nothing for jailbreaks. An attacker's jailbreak attempt might be perfectly clean from an injection perspective — no delimiters, no attempt to override system instructions, no tool call manipulation. It's just a carefully worded request designed to make the model think that generating harmful content is acceptable in this context. Your input validation passes it through. Your authorization checks don't flag it. But the model outputs something that violates your acceptable use policy.

Output content filtering helps with jailbreaks but does almost nothing for prompt injection. An attacker's prompt injection might produce completely safe, policy-compliant output — a polite message that says "Here is the customer data you requested." Your content filter sees nothing wrong. But the model just leaked your entire customer database because the attacker convinced it that their query was a legitimate admin request.

You need both defense layers. But they sit at different points in your architecture, they use different techniques, and they defend against fundamentally different failures.

## The Tactical Confusion That Costs You Coverage

Teams that conflate jailbreaks and prompt injection end up with a single "prompt security" workstream that tries to address both threats. The result is a defense that's mediocre against both. The team adds input validation rules that catch obvious injection attempts but do nothing for sophisticated jailbreaks. They add output filters that block some harmful content but don't defend against control-flow hijacking. They run red-team exercises that mix both attack types without clearly measuring which defenses stop which threats.

Six months after launch, they discover prompt injection vulnerabilities that their "jailbreak testing" never caught because the tests focused on generating harmful content, not on hijacking execution. Or they discover jailbreak vulnerabilities that their "input validation" never caught because the validation focused on structural attacks, not on alignment bypass.

The tactical fix is simple: separate your threat models explicitly. You have a prompt injection threat model that maps attack vectors related to control — delimiter injection, instruction override, tool call manipulation, context confusion, data exfiltration. You have a jailbreak threat model that maps attack vectors related to alignment bypass — role-play framing, hypothetical scenarios, persona instructions, safety training circumvention. You build defenses for each. You test them separately. You monitor for both.

Your red-team exercises include prompt injection scenarios where success means the attacker gained unauthorized control or data access, and jailbreak scenarios where success means the model generated policy-violating content. You measure your defenses against each threat independently. You don't declare victory on "prompt security" until you've addressed both.

## Where the Threats Intersect

In rare cases, an attacker combines both techniques. They use a jailbreak to make the model willing to perform harmful actions, then use prompt injection to make it execute those actions against your system. A pure jailbreak might get the model to explain how to exfiltrate data — but if your tools aren't vulnerable to prompt injection, the model can't actually do it. A pure prompt injection might let the attacker invoke unauthorized tool calls — but if the model's alignment is strong, it refuses to use those tools in harmful ways.

The combination attack breaks both defenses. The attacker uses jailbreak techniques to reduce the model's safety constraints, making it willing to help with malicious goals. Then they use prompt injection techniques to redirect execution toward those goals. The model is now both willing and able to perform the harmful action.

This is the nightmare scenario, and it's why you can't choose between control defenses and safety defenses. You need both operating simultaneously, each one providing a layer of protection even if the other is bypassed. If an attacker bypasses your alignment but your control defenses hold, the model might say something inappropriate but can't take harmful actions. If an attacker bypasses your control defenses but your alignment holds, the model might execute unauthorized commands but refuses to do anything that violates safety policies.

Defense in depth requires understanding which layer you're defending. Prompt injection is your control layer. Jailbreaks are your safety layer. Build them both, test them both, monitor them both. The attacker only needs to win once. You need to win on every axis.

The next subchapter covers the most persistent jailbreak category: role-play and persona attacks, including the DAN lineage and why these techniques remain effective even against models trained to resist them.

