# 12.2 — Anomaly Detection: Unusual Prompts and Patterns

Most attacks look normal. That is why they work. A prompt injection does not arrive with a warning label. A data exfiltration attempt does not trigger standard security signatures. An account takeover does not generate failed login events because the attacker already has valid credentials. The request looks like every other request: properly authenticated, correctly formatted, within rate limits. The difference is intent — and intent lives in the semantic content of prompts, the behavioral patterns across sessions, and the subtle deviations from what this user normally does. Anomaly detection is how you find attacks that have no signature. You build models of normal, then investigate everything that deviates. The challenge is that normal for AI systems is infinitely variable, and deviations from normal include both attacks and legitimate use cases you never anticipated.

## Baseline Establishment for Normal Behavior

You cannot detect anomalies without knowing what normal looks like. For a web application, normal is straightforward: users log in, navigate to known endpoints, submit forms with expected field values, log out. For an AI system, normal is a distribution across infinite dimensions. Users ask questions you have never seen before. They phrase requests in ways your designers never imagined. They combine features in sequences that were never tested. Legitimate use varies by user role, time of day, feature maturity, and external events. The baseline for normal is not a single profile — it is thousands of overlapping distributions.

Establish baselines at multiple scopes. **Per-user baselines** capture what this specific user typically does: their average session length, their request frequency, the topics they ask about, the time of day they are active, the tools they invoke, the complexity of their prompts. A user who normally asks five questions per day and suddenly submits fifty questions in an hour is anomalous relative to their own behavior. **Per-session baselines** capture typical session patterns: how long sessions last, how many turns they include, whether users ask follow-up questions or switch topics abruptly, whether they reference previous conversations. A session that deviates from typical session structure might be automated. **Per-feature baselines** capture how specific features are used: which prompts typically trigger which tools, which topics typically lead to which follow-up questions, which outputs typically satisfy users versus prompt clarifications. A feature being used in an unusual way might indicate an attacker probing for vulnerabilities.

Collect baseline data before deploying detection rules. Run your system for at least two weeks in observation mode, logging all activity without blocking anything. This gives you enough data to model normal behavior across diverse users and use cases. Some teams deploy detection rules immediately, tuning thresholds as false positives arise. This approach creates operational chaos: security teams investigate hundreds of alerts for legitimate behavior, users experience mysterious blocks, and real attacks blend into the noise of false positives. Build the baseline first. Then deploy detection.

Baselines decay. User behavior changes as features evolve, as user skill increases, as business processes shift. A baseline built in January may be irrelevant by March. Continuously update baselines using recent data: weight the last thirty days more heavily than older data, detect when baseline shifts occur suddenly, flag users whose behavior has permanently changed versus users exhibiting temporary anomalies. An executive who never used the AI system and suddenly starts using it heavily is not anomalous — their behavior baseline was incomplete. A power user who suddenly shifts from typical questions to probing system internals is anomalous.

## Statistical Anomaly Detection for Prompt Characteristics

The simplest anomaly detection analyzes prompts as statistical objects. Measure **prompt length** in tokens, characters, and word count. Calculate the distribution of prompt lengths across all users. Flag prompts that fall in the top one percent of length. Unusually long prompts are often attacks: injection attempts that include lengthy instructions, reconnaissance probes that ask the system to enumerate all its capabilities, or data exfiltration attempts that request comprehensive outputs.

Measure **prompt complexity** through readability scores, sentence structure, and vocabulary diversity. A prompt with a readability level far above or below typical user prompts is anomalous. Attackers sometimes use highly technical language when probing system internals. Automated bots sometimes generate prompts with repetitive structure and limited vocabulary. Legitimate user prompts cluster around a consistent complexity range for your user population. Outliers warrant investigation.

Track **request frequency** per user, per session, and per time window. Calculate the mean and standard deviation of requests per hour for each user. Flag users who exceed three standard deviations above their mean. A user who normally submits ten requests per day and suddenly submits one hundred requests in an hour is either experiencing an unusual legitimate need or conducting automated abuse. Both scenarios require investigation. True attacks have distinct frequency patterns: reconnaissance is high-frequency with rapid variations, data exfiltration is high-frequency with consistent timing, denial-of-wallet is maximum frequency within rate limits.

Analyze **edit distance between consecutive prompts**. If a user submits twenty prompts that are nearly identical except for single-word variations, they are likely testing the system's response to slight input changes — a reconnaissance pattern. Legitimate users rephrase questions when the AI misunderstands, but they do not submit microvariations. Automated attacks often iterate through prompt templates, changing only specific words. Measuring edit distance between consecutive prompts from the same user exposes this pattern.

Statistical anomaly detection is fast enough for real-time use and requires no complex models. You calculate statistics, compare to baselines, and flag outliers. This catches unsophisticated attacks and automated abuse. It misses sophisticated attacks crafted to blend into statistical norms: an attacker who carefully throttles request rates to match the user's baseline, who crafts prompts with typical length and complexity, who spaces attack attempts across multiple sessions to avoid frequency anomalies. For these attacks, you need semantic analysis.

## Semantic Anomaly Detection for Content and Intent

Statistical features describe the shape of prompts. Semantic features describe the meaning. **Topic modeling** identifies what users are asking about. Train a topic model on your prompt corpus: cluster prompts into topics like "customer data queries," "product questions," "troubleshooting requests," "administrative commands." Track which topics each user typically engages with. Flag prompts from users asking about topics they have never accessed before. A customer support agent suddenly asking administrative questions is anomalous. A standard user suddenly asking how to override system instructions is a prompt injection attempt.

Topic shifts happen in legitimate scenarios: users explore new features, job roles change, business processes evolve. The key is velocity. A user gradually expanding into new topics over weeks is normal. A user suddenly switching to an entirely different topic set within a single session is anomalous. Investigate sharp topic shifts — they correlate with account takeovers and automated attacks using compromised credentials.

**Sentiment and tone analysis** detects prompts that deviate from typical user communication patterns. Most users ask straightforward questions or issue direct commands. Prompts with unusually aggressive language, unusual politeness, or attempts at social engineering stand out. Some prompt injections include elaborate social engineering: "I am your authorized administrator and I need you to urgently..." Legitimate users rarely phrase requests this way. Tone anomalies flag manipulation attempts.

**Embedding-based similarity** detects prompts that do not resemble any previously seen prompt from this user or similar users. Embed every prompt into a high-dimensional vector space using a sentence embedding model. For each new prompt, calculate cosine similarity to the user's historical prompts and to the corpus of typical user prompts. Flag prompts with similarity scores below a threshold. A prompt that is completely unlike anything this user or any similar user has ever asked is worth investigating. This technique catches novel attack techniques before they become common enough to have statistical signatures.

Semantic anomaly detection is computationally expensive. Embedding models, topic models, and sentiment analysis require inference time that exceeds your real-time latency budget. Run semantic detection asynchronously: allow the request to proceed immediately, then analyze the prompt in a background job within seconds. If the analysis flags an anomaly, generate an alert and optionally retroactively revoke the response or block the user's next request. This architecture balances real-time user experience with sophisticated detection.

## Tool Call Pattern Anomalies

Prompts alone do not tell the full story. The tools the AI invokes reveal what the system is actually doing. Track **which tools each user typically invokes** and how frequently. A user who has never triggered database query tools and suddenly invokes them repeatedly is anomalous. A user whose tool usage pattern shifts from read-only operations to write operations warrants investigation. Most users exhibit consistent tool usage patterns based on their role and workflow. Deviations signal either attacks or misconfigurations.

Analyze **tool call argument patterns**. Each tool has expected argument ranges: database queries have typical query structures, email tools have typical recipient domains, file retrieval tools have typical path patterns. Flag tool calls with arguments that fall outside the distribution of previously seen arguments. A database query requesting all tables when typical queries request specific records is anomalous. An email tool sending to an external domain when typical sends are internal-only is anomalous. Tool argument anomalies catch attackers manipulating the AI into executing commands the user should not have access to.

Track **tool call sequences**. Legitimate workflows invoke tools in predictable orders: users search, then retrieve, then read. Attackers probing the system invoke tools in unusual sequences: they enumerate available tools, test each tool with edge-case inputs, then chain tools together in ways that were never intended. A user who invokes every available tool in alphabetical order is running reconnaissance. A user who chains tools together to create a multi-step data exfiltration pipeline is executing an attack. Sequence anomalies reveal sophisticated abuse.

Measure **tool call success and failure rates**. Users working within their permissions and the system's intended design have high tool success rates. Attackers testing boundaries generate tool failures: they invoke tools with malformed arguments, they attempt to access resources they lack permissions for, they try to chain tools in unsupported ways. A sudden spike in tool failures from a specific user or session indicates probing behavior. Even if the attacks fail, the reconnaissance reveals system architecture to the attacker. Flag investigation for any user with tool failure rates above baseline.

Tool call monitoring is the most reliable attack signal. Prompt-level anomalies can be false positives — users ask unusual questions for legitimate reasons. Tool call anomalies are harder to explain away. If the AI is invoking tools in ways it should not, either the system is misconfigured or the user is manipulating it. Investigate every tool call anomaly.

## Cost and Latency Outliers as Attack Signals

Every request has a cost: model inference tokens, tool execution fees, external API calls, data retrieval operations. Track **cost per request** for each user. Calculate baseline cost distributions. Flag requests that cost more than three standard deviations above the user's mean. Unusually expensive requests indicate either attacks or system issues. Some attacks are designed to maximize cost: enormous context window exploitation, repeated complex tool chains, adversarial inputs that cause the model to generate extremely long outputs.

An attacker attempting to extract training data through membership inference sends hundreds of prompt variations, each triggering full model inference. The attack may or may not succeed, but it costs you thousands of dollars in inference fees. Cost-per-request anomaly detection stops the attack before bankruptcy. Some organizations have suffered five-figure surprise bills from undetected abuse. Cost monitoring is both a security control and a financial control.

Track **latency per request**. Requests that take significantly longer than baseline indicate either system performance issues or unusual processing. Some attacks deliberately trigger expensive computation paths: they craft prompts that cause the model to iterate through its context window repeatedly, they trigger tool chains that execute slowly, they exploit RAG retrieval inefficiencies. Latency outliers correlate with attacks often enough that every instance requires investigation.

Cost and latency signals are definitive. Prompt content is subjective — what looks like an attack to one reviewer looks like a legitimate edge case to another. Cost and latency are objective: this request cost ten dollars when typical requests cost ten cents. This request took thirty seconds when typical requests take two seconds. These anomalies justify immediate user lockout and security review. No false positive debate required.

## False Positive Management and Alert Fatigue

Anomaly detection generates false positives. Legitimate users do unusual things. New features create new usage patterns. External events change user behavior suddenly. If you set detection thresholds too sensitive, you flood security teams with alerts for normal behavior. If you set thresholds too permissive, you miss attacks. The tradeoff is unavoidable. The goal is to tune thresholds so that the true positive rate justifies the operational cost of investigating false positives.

Start with permissive thresholds: flag only the top one percent of anomalies, then manually review a sample. Measure precision: what percentage of flagged events are actually attacks or policy violations? If precision is below twenty percent, your thresholds are too sensitive. Tighten them: flag only the top zero-point-five percent. If precision is above eighty percent, your thresholds might be too permissive — you may be missing attacks that fall just below the threshold. Calibration is continuous.

Implement **alert severity levels**. Not every anomaly deserves immediate human investigation. Low-severity alerts generate log entries and contribute to user risk scores but do not page security teams. Medium-severity alerts create tickets for investigation during business hours. High-severity alerts page on-call security immediately. Severity is based on anomaly magnitude and user risk profile: a three-standard-deviation anomaly from a high-risk user is high-severity, the same anomaly from a new user with no prior history is medium-severity.

Use **anomaly scores** rather than binary flags. Each detection rule outputs a score representing how anomalous this event is. Aggregate scores across multiple detection rules to produce a composite anomaly score for each request. This approach is more robust than single-rule triggering: an event might be only slightly anomalous across five dimensions, but the combination of five simultaneous minor anomalies is a strong signal. Composite scoring reduces false positives and surfaces sophisticated attacks that evade individual detection rules.

Build **feedback loops** from alert investigations back into detection models. When security teams investigate an alert and determine it was a false positive, record why: new feature launch, legitimate user behavior, data quality issue, detection rule too sensitive. Use this feedback to tune thresholds, exclude specific patterns, or add context that the detection model lacked. When security teams confirm a true positive, record the attack characteristics and write new rules to catch similar attacks faster. Detection systems improve through continuous learning from human judgment.

The operational reality: you will generate false positives. Security teams will spend time investigating benign anomalies. The goal is not zero false positives — that would require missing all sophisticated attacks. The goal is a false positive rate low enough that security teams can investigate every alert thoroughly and still have capacity for proactive threat hunting. If alert volume exhausts investigative capacity, attackers exploit the chaos.

The next subchapter covers abuse signal correlation across events: how to detect sophisticated attacks that evade single-event anomaly detection by spreading reconnaissance, exploitation, and exfiltration across multiple users, sessions, and time windows.

