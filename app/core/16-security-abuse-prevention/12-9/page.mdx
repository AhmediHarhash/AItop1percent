# 12.9 â€” Investigation: Understanding What Happened

Why do AI security investigations fail? Because the evidence is probabilistic, not deterministic. A SQL injection attack leaves a clear trail: the malformed query, the database error, the rows that were accessed. A prompt injection attack leaves a trail of prompts and responses that look normal in isolation and only reveal the attack when you understand the attacker's intent. Traditional forensic analysis looks for technical anomalies. AI forensic analysis requires understanding semantic manipulation, context poisoning, and adversarial intent patterns that do not map to traditional indicators of compromise.

An e-commerce company detected unusual recommendation patterns in their AI shopping assistant. Products that had never been recommended before were suddenly appearing in top slots. The initial investigation found no code changes, no model updates, no configuration drift. The logs showed normal user queries. It took four days to discover the attack: a competitor had been systematically submitting product reviews with embedded prompt injections designed to poison the context window used for recommendations. Each review looked legitimate. The attack only became visible when someone examined one hundred reviews in sequence and noticed the carefully embedded instruction phrases. By the time the investigation concluded, the attacker had manipulated recommendations for seventeen days.

The investigation phase has different goals than containment. Containment stops the damage. Investigation determines the full scope of damage, identifies root cause, gathers evidence for legal or regulatory response, and informs remediation strategy. This phase must be thorough, not fast. Rushing the investigation leads to incomplete understanding, which leads to inadequate fixes, which leads to recurrence. The teams that spend three days investigating a four-hour attack are usually making the right trade-off.

## Forensic Analysis for AI Systems

Traditional forensic analysis assumes deterministic behavior. If you replay the same inputs to a web server, you get the same outputs. If you replay the same queries to a database, you retrieve the same rows. AI systems are non-deterministic. Replay the same prompt to an LLM and you might get a different response due to sampling temperature, context window state, or model version differences. This makes forensic reconstruction substantially harder.

The first challenge: identifying the attack in the logs. Most attacks do not leave clear indicators. A jailbreak attempt looks like a long user prompt. A membership inference attack looks like a user asking many questions about a specific topic. A prompt injection looks like user-provided content that happens to contain instructions. You cannot search for "attack" in the logs and find the evidence. You have to reconstruct user intent from patterns.

The investigation starts with the detection event. What alert fired, what user report triggered the response, what anomaly was observed. From that point, you work backward and forward in time. Backward: what happened before the detection that might have been missed. Forward: what happened after the detection that was part of the same attack. This creates a timeline with a known anchor point and expanding windows in both directions.

For each suspected attack event, reconstruct the full interaction: the user prompt, the system prompt, any retrieved context, the model's response, any tool calls executed, and any downstream actions taken. This requires correlating logs from multiple systems. The prompt might be in the API gateway logs. The system prompt might be in the application logs. The retrieved context might be in the RAG system logs. The tool calls might be in the agent execution logs. The investigation requires joining these logs by session ID, request ID, or timestamp.

Once you have the full interaction reconstructed, analyze it for attack indicators. Jailbreak attempts often include phrases like "ignore previous instructions" or "pretend you are a different assistant." Prompt injections often include instruction-like language embedded in data fields. Membership inference attacks often include very specific queries about edge cases or rare data combinations. These patterns are not universal. Attackers adapt. But starting with known patterns helps identify the first examples, which then reveal the attacker's specific techniques.

## Prompt and Output Reconstruction

The core artifact in any AI security investigation is the prompt-response pair: what the user said and what the model said back. But the logged prompt is rarely the actual prompt the model received. The logged prompt is usually the user's input. The actual prompt includes the system prompt, any few-shot examples, any retrieved context from RAG, any conversation history, and then the user input. Reconstructing the full prompt requires understanding how your prompt construction pipeline works.

Many organizations discover during investigation that they do not log enough information to reconstruct the full prompt. They log user input but not the system prompt version, not the retrieved documents, not the conversation history. This makes forensic analysis nearly impossible. You see the model's response but you cannot determine why it responded that way because you do not know what prompt it actually received. This is why comprehensive logging is non-negotiable for any AI system that handles sensitive data or high-risk use cases.

Prompt reconstruction requires versioning. The system prompt might have changed between the time of the attack and the time of investigation. If you do not know which system prompt version was active during the attack, you cannot accurately reconstruct what the model saw. Every prompt component must be versioned and the version must be logged with each request. This applies to system prompts, few-shot examples, RAG retrieval strategies, and context formatting logic.

Output reconstruction has similar challenges. The logged output is usually the final response sent to the user, often after post-processing, content filtering, and formatting. The investigation might need the raw model output before any post-processing. If the model's raw output contained policy violations that were filtered out before reaching the user, the logged final output will not show the violation. You need to log both the raw model output and the final user-facing output to understand what the model actually generated versus what the user saw.

For long conversations, reconstruction becomes more complex. The attack might have occurred over twenty turns of conversation. You need to reconstruct the entire conversation history in order to understand how the attacker built up the malicious context. Each turn might have involved different retrieved documents if the system uses RAG. The full reconstruction might involve hundreds of components assembled in sequence. This is why session-level logging is critical. Every event in a session must be tied to the same session ID so you can reconstruct the full interaction path.

## Tool Call and Action Tracing

For agent systems, the investigation must trace not just what the model said but what the model did. The prompt-response pair is insufficient. You need the full action trace: what tools were called, what parameters were passed, what results were returned, and what downstream effects occurred. An agent that calls an API to send an email has created an external effect that persists even after you terminate the agent session.

Tool call tracing requires instrumentation at the tool execution layer. Every tool call should be logged with the full context: the model's decision to call the tool, the parameters the model provided, the tool's return value, and how the model used that return value in subsequent reasoning. If the investigation reveals that an agent sent a malicious email, you need to trace backward to see what prompt caused the model to decide to send that email, what parameters it chose, and whether the email tool's safety checks failed to catch the problem.

The challenge with tool tracing is volume. A complex agent workflow might involve dozens or hundreds of tool calls. Not all of them are relevant to the attack. The investigation must identify which tool calls were part of the attack and which were normal operation. This requires understanding the attacker's goal. If the goal was data exfiltration, focus on tool calls that retrieve data or send data externally. If the goal was unauthorized action, focus on tool calls that modify state or trigger workflows.

Tool call analysis must also consider the tool's authorization model. Did the agent have permission to call that tool. Did the tool have permission to perform that action on behalf of that user. Even if the agent called the tool correctly, the tool might have had overly broad permissions that allowed actions the user should not have been able to perform. This is a common failure mode: the agent behaves correctly given its permissions, but the permissions were misconfigured.

For agents that chain multiple tool calls, trace the full chain. The attack might not be in any single tool call but in the sequence. The agent might call a search tool to find user data, then call a summarization tool to extract key fields, then call a webhook tool to exfiltrate the summary. Each individual call looks legitimate. The sequence is the attack. Chain analysis requires reconstructing the agent's reasoning process across multiple steps, which means correlating tool calls by the agent session and understanding the data flow between calls.

## Timeline Assembly from Logs

The investigation must produce a definitive timeline of the attack from first indicator to containment. This timeline serves multiple purposes: it identifies the full window of exposure, it reveals missed detection opportunities, it informs remediation priorities, and it provides evidence for legal or regulatory response.

The timeline starts before the attack. Look for reconnaissance activity: the attacker probing the system to understand its behavior, testing different prompt variations, creating multiple accounts, or querying for information that suggests they are mapping the system's capabilities. Reconnaissance might occur days or weeks before the actual attack. If your logs only go back seventy-two hours, you might miss the reconnaissance phase entirely.

The attack phase includes the first successful exploitation, any lateral movement to other accounts or capabilities, and the full period during which the attacker was achieving their objective. For data exfiltration attacks, this is the window during which data was accessed. For manipulation attacks, this is the window during which the model was producing incorrect outputs. For abuse attacks, this is the window during which harmful content was generated. Each event in this phase should be timestamped and annotated with affected users, data accessed, and actions taken.

The detection phase captures when the attack was first observed, by whom, and how long it took to confirm that it was an attack rather than a bug or user error. Many investigations reveal that the attack was detected multiple times before it was recognized as an attack. A support ticket from a user complaining about weird model behavior might have been the first detection, but it was dismissed as a bug. An anomaly alert might have fired but was marked as a false positive. The timeline must capture all detection events, even those that were initially dismissed.

The response phase includes classification, escalation, containment actions, and investigation milestones. This section of the timeline is often the most useful for improving future response. It shows how long each phase took, where delays occurred, and whether the response followed the documented playbook. If containment took six hours when the playbook says it should take one hour, the timeline shows where the extra five hours went.

The timeline must distinguish between when events occurred and when they were discovered. The attacker might have started reconnaissance three weeks before detection. The timeline shows both dates: "reconnaissance activity began on January 5, discovered during investigation on January 28." This distinction is critical for regulatory reporting. Breach notification windows are calculated from the date of discovery, not the date of occurrence, but the scope of the breach is calculated from the date of occurrence.

## Root Cause Identification

The investigation must identify not just what the attacker did but why it worked. Root cause analysis for AI incidents is layered. There is the proximate cause: the specific technique the attacker used. There is the enabling cause: the system weakness that allowed that technique to work. There is the systemic cause: the organizational or architectural decision that created that weakness. Effective remediation requires addressing all three layers.

The proximate cause is usually clear by the time containment is complete. The attacker used a specific jailbreak technique, or injected a prompt into a data field, or exploited a tool authorization flaw. Documenting the exact technique allows you to test whether your remediation prevents that specific attack. But addressing only the proximate cause guarantees recurrence with a slightly different technique.

The enabling cause is the system weakness. Why did the jailbreak work. Why was the data field processed as instructions. Why did the tool have that permission. This layer often reveals defensive gaps: lack of input validation, insufficient output filtering, overly permissive authorization, missing content policy enforcement. Addressing the enabling cause prevents the specific technique and similar techniques that exploit the same weakness.

The systemic cause is the architectural or organizational decision that created the enabling weakness. Many systemic causes are trade-offs made earlier in the product's lifecycle: "we skipped input validation to ship faster," "we gave the agent broad permissions because we did not have time to implement fine-grained access control," "we did not log prompt versions because storage was expensive." Others are organizational: "the security team was not involved in the AI feature design," "we do not have a red team testing new capabilities before launch," "prompt changes go to production without review." Addressing systemic causes prevents future incidents across different features and attack vectors.

Root cause identification must be blameless. The goal is to understand what happened, not to punish the person who made a mistake. If the root cause is "the engineer forgot to add input validation," the systemic cause is "we do not have automated checks that enforce input validation" or "we do not have a design review process that catches missing security controls." Focusing on the individual decision creates a culture where people hide mistakes. Focusing on the system that allowed the decision creates a culture where people report problems early.

## Preserving Evidence for Legal and Regulatory Needs

The investigation must assume that the evidence will be reviewed by external parties: regulators, auditors, legal counsel, or law enforcement. This means the evidence must be preserved in a tamper-evident way, must be documented clearly enough for non-experts to understand, and must comply with any relevant legal holds or regulatory requirements.

Preserve logs immediately when the incident is detected. Do not wait until the investigation phase. Logs might be rotated, systems might be rebooted, sessions might be cleaned up by automated processes. The first action after detecting a security incident should be to snapshot all relevant logs and store them in immutable storage. This includes application logs, model inference logs, tool execution logs, authentication logs, and any third-party API logs if the system integrates with external services.

The preserved logs must maintain chain of custody. Document when the logs were captured, by whom, and that they have not been modified since capture. Hash the log files and store the hashes separately. If you need to prove in a legal or regulatory proceeding that the logs are authentic, the hashes provide cryptographic proof. This level of rigor is unnecessary for most incidents but essential for incidents involving law enforcement, regulatory investigation, or potential litigation.

Prepare an evidence summary for non-technical reviewers. Legal counsel does not need the raw logs. They need a summary that explains in plain language what happened, how many users were affected, what data was accessed, what actions were taken, and what the timeline looked like. This summary should be factual, precise, and free of technical jargon. "The attacker used a prompt injection attack to manipulate the model into revealing user email addresses" is clearer than "adversarial input exploitation in the language model inference pipeline resulted in unauthorized disclosure of PII."

If the incident involves a potential crime, consult legal counsel before proceeding with the investigation. In some jurisdictions, you might be required to preserve evidence for law enforcement or to report the incident to regulatory authorities within a specific timeframe. Continuing the investigation without legal guidance can inadvertently destroy evidence or violate regulatory obligations. For incidents involving fraud, data theft, or harassment, assume legal involvement until counsel advises otherwise.

## Moving to Remediation

The investigation ends when you have answers to five questions: What happened. How did it happen. Who was affected. What data was exposed. How do we prevent it from happening again. These answers inform remediation strategy, user notification decisions, regulatory reporting, and long-term security improvements.

The investigation report should be written for multiple audiences. The technical team needs the detailed forensic analysis, root cause breakdown, and specific recommendations for code or architecture changes. The executive team needs the business impact summary, affected user count, financial exposure, and reputational risk assessment. Legal and compliance need the timeline, data exposure details, and regulatory obligations. Each audience gets the information they need without wading through information they do not.

The final component of the investigation is lessons learned. What detection signals did we miss. What containment actions worked. What containment actions made things worse. What log data did we wish we had but did not. What would we do differently next time. These lessons must be documented and shared across the organization. The teams building new AI features should learn from the incidents that happened in existing features. The organization should treat each incident as expensive education that must not be wasted.

The investigation phase transitions to remediation: deploying fixes, restoring service, notifying affected users, and implementing long-term improvements to prevent recurrence. But remediation only works if the investigation was thorough. A shallow investigation leads to surface-level fixes that do not address root causes. Three months later, a similar attack succeeds through a slightly different vector because the systemic cause was never addressed.
