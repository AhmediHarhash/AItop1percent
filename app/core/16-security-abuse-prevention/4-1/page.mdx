# 4.1 — Tools as Attack Surface: Why Capabilities Create Risk

The model can read your mind. It can also read your filesystem, query your database, send your emails, execute your code, and transfer your money. The difference is that the second set of capabilities requires tools. And every tool you give a model is a capability an attacker can hijack.

In February 2025, GitHub disclosed CVE-2025-53773, a remote code execution vulnerability in Copilot Workspace. The attack surface was not a traditional buffer overflow or SQL injection. It was the model itself. An attacker crafted a malicious repository with a README file containing carefully structured natural language instructions. When a developer opened the repository in Copilot Workspace and asked the model to "help me understand this codebase," the model followed the instructions embedded in the README — instructions that directed it to execute shell commands that exfiltrated environment variables containing API keys. The tool that gave Copilot its power — the ability to execute terminal commands on behalf of the developer — became the weapon. The model did exactly what it was told. The problem was that the attacker, not the developer, wrote the instructions.

This is the central threat model for AI security in 2026: the model as an attack proxy. A model that only generates text can lie, leak training data, or produce harmful content. A model with tools can take action. It can delete files, modify databases, send messages, make purchases, and escalate privileges. The moment you give a model a tool, you shift from information security to operational security. The attack is no longer about what the model says. It is about what the model does.

## The Capability Expansion Problem

A base language model has one capability: predict the next token. This is a bounded risk. The worst outcome is that it generates text you do not want. An agent with tools has dozens of capabilities: read files, write files, call APIs, query databases, send emails, execute code, trigger workflows. Each capability is a potential attack vector.

The risk scales non-linearly with the number of tools. A model with one tool has one attack surface. A model with ten tools has ten surfaces, but also every possible combination of those tools chained together. A model with access to a file read tool and an HTTP POST tool can exfiltrate data. A model with access to a user lookup tool and a password reset tool can take over accounts. A model with access to a code execution tool and a package installation tool can install malware. The attacker does not need to exploit your infrastructure. They exploit your model's access to your infrastructure.

This is fundamentally different from traditional application security. In a traditional web application, the attack surface is the set of endpoints you expose to the internet. You control what those endpoints do. You validate inputs. You enforce authentication and authorization. The application does exactly what you programmed it to do. In an AI system with tools, the attack surface is the set of capabilities the model can invoke, and the model decides which tools to call based on natural language input it receives. You do not control the decision logic. You provide examples, constraints, and instructions — but the model interprets them. The attacker's job is to provide input that causes the model to interpret your instructions differently than you intended.

## The Permission Model and Its Weaknesses

Most tool-calling systems implement a permission model. The model can only call tools it has been granted access to. A customer support agent cannot call database administration tools. A code assistant cannot call financial transaction APIs. This is the first line of defense, and it is necessary but not sufficient.

The first weakness is that permission boundaries are defined by the system designer, not by the context of the interaction. A model may have permission to call a file deletion tool because legitimate workflows require it. But the designer who granted that permission assumed the model would only delete files the user explicitly requested. The attacker's goal is to trick the model into thinking a malicious deletion request is legitimate. The permission system does not distinguish between "delete the log file the user asked about" and "delete the backup directory the attacker embedded in a prompt injection."

The second weakness is that many tool-calling systems enforce permissions at tool invocation time but not at parameter validation time. The model has permission to call the send_email tool. The system checks: does the model have send_email permission? Yes. The email is sent. The system does not ask: should this specific email, to this recipient, with this content, be sent in this context? That question is delegated to the model's judgment, which is controlled by natural language input, which is controlled by the attacker.

The third weakness is that permission models often assume the user is the threat, not the input. The model is granted permissions based on the user's role. If you are an administrator, your agent can call administrator tools. The system trusts that your agent will only do things you want. But if an attacker can inject instructions into your agent's context — through a malicious document, a compromised website, a manipulated database entry, or any other channel — the agent will execute those instructions with your permissions. The permission system sees the agent acting on behalf of an authorized user. It does not see the unauthorized instructions.

## Text Generation Versus Real-World Action

The risk profile of a model changes the moment it gains the ability to affect state outside the conversation. A model that generates SQL queries for a human to review is a productivity tool. A model that executes SQL queries directly is a security risk. The difference is not in the model's intelligence or the complexity of the task. The difference is whether there is a human gate between the model's output and the real-world consequence.

This distinction matters for two reasons. First, it defines the cost of a failure. If a model generates incorrect text, the user reads it, recognizes the error, and ignores it. If a model executes an incorrect database query, the data is modified before anyone notices. The window for human intervention closes. Second, it changes the attacker's incentive structure. Attacking a text-generation model is an exercise in misinformation or embarrassment. Attacking an action-taking model is an exercise in direct harm. The attacker can steal data, destroy resources, manipulate workflows, and escalate privileges.

The most dangerous tools are the ones that bridge digital and physical systems. A model that can send an email is concerning. A model that can trigger a wire transfer is critical. A model that can unlock a door, adjust an industrial control system, or modify medical device settings is a safety issue. The moment AI agents gain physical-world affordances — and in 2026, they increasingly do — the attack surface expands from cybersecurity to physical security. The attacker is no longer trying to read your data. They are trying to control your environment.

## The Tool Permission Model in Practice

Most 2026 agent frameworks implement one of three permission models. The first is allowlist-based: the model can only call tools explicitly added to its allowed set. This is the most common approach and the most restrictive. The second is role-based: the model inherits permissions based on the user's role. This is more flexible but requires careful role design. The third is context-based: the model's permissions change dynamically based on the current conversation context, the user's recent actions, or external policy decisions. This is the most sophisticated and the hardest to secure.

Allowlist-based systems fail when the allowed set is too broad. If you give a customer support agent access to twenty tools because some workflows need some of them, you have just handed an attacker nineteen tools they might not need. The principle of least privilege demands that each agent instance receive only the tools required for its specific task. In practice, this is difficult to enforce because task requirements are not always known in advance. The agent that starts a conversation helping a user reset their password might need to escalate to account recovery, which requires different tools. If you grant all possible tools upfront, you over-provision. If you grant tools dynamically as needed, you introduce a secondary decision layer that can also be attacked.

Role-based systems fail when roles are too coarse-grained. A developer role might include permissions to read code, write code, execute tests, and deploy to staging. An attacker who compromises a developer's agent gains all those permissions simultaneously. The system cannot distinguish between "the developer is debugging a test failure" and "the attacker is exfiltrating source code." The role grants access. The model decides how to use it.

Context-based systems fail when the context can be manipulated. If the model's permissions are determined by parsing the conversation history or analyzing recent user behavior, the attacker's job is to manipulate that history or behavior to appear legitimate. A well-crafted prompt injection can create the appearance of a valid context for an invalid action. The system sees: the user has been discussing database backups for ten minutes, so granting access to the backup deletion tool is appropriate. The system does not see: the user did not initiate that discussion — the attacker did, through injected text in a support ticket.

## Why This is Hard to Defend

The fundamental challenge is that the model operates in the space of semantics, not syntax. Traditional security systems enforce rules written in code. If the rule says "only users in the admin group can call this function," the system checks group membership and makes a binary decision. The logic is deterministic. AI security requires enforcing rules written in natural language. If the rule says "only call this tool if the user explicitly requested it," the model must interpret what counts as an explicit request. The logic is probabilistic.

This creates an asymmetry between attacker and defender. The defender writes constraints in natural language and hopes the model interprets them correctly in every possible context. The attacker writes prompts designed to exploit the edge cases where the model's interpretation diverges from the defender's intent. The attacker only needs to find one interpretation gap. The defender needs to close all of them.

The second challenge is that models generalize. This is their strength and their vulnerability. A model trained to be helpful will attempt to satisfy user requests even when those requests are malicious. A model fine-tuned to follow instructions will follow injected instructions as readily as legitimate ones. The behaviors that make models useful — helpfulness, instruction-following, generalization — are the same behaviors that make them exploitable. You cannot eliminate the vulnerability without eliminating the capability.

The third challenge is that the attack surface includes all input channels, not just the user's direct messages. Any text the model processes is a potential injection vector. This includes file contents, database records, API responses, webpage text, email bodies, and metadata fields. If your agent reads a support ticket from a customer and that ticket contains hidden instructions, the agent may execute those instructions. If your agent retrieves a document from an internal wiki and that document was edited by an attacker who gained low-level access, the document becomes the attack. The model does not distinguish between trusted and untrusted input unless you explicitly teach it to — and even then, the distinction is fragile.

## The Real Standard

Securing tools requires defense in depth. The first layer is tool-level access control: the model cannot call tools it does not have permission to use. The second layer is parameter validation: even if the model can call a tool, the parameters are validated against policies before execution. The third layer is action review: high-risk actions trigger human approval workflows before execution. The fourth layer is runtime monitoring: every tool call is logged, and anomalous patterns trigger alerts.

None of these layers is sufficient alone. Tool-level access control does not prevent misuse of allowed tools. Parameter validation does not prevent malicious chains of valid calls. Action review does not scale to high-frequency agents. Runtime monitoring detects attacks after they happen, not before. The defense works when all four layers are active and when each layer assumes the others will fail.

The teams that secure agent systems successfully do not treat tools as productivity features. They treat tools as privileged operations that happen to be invoked by probabilistic reasoning. Every tool is a potential vector. Every tool call is a potential exploit. The model is not trusted to make security decisions. It is trusted to make capability decisions, and those decisions are wrapped in security enforcement that does not rely on the model's judgment. The model decides what to do. The security layer decides whether to allow it.

We turn next to the most common tool abuse pattern: unauthorized tool calls.

