# 12.5 â€” Attacking the Monitors: Log Flooding, Alert Suppression, Evasion

Your monitoring system detected the attack three hours after it started. The attacker had been careful. They knew your rate limits: fifty requests per minute per API key. They knew your anomaly detection thresholds: flag accounts with toxicity scores above 0.85. They knew your log analysis pipeline ran every five minutes. So they structured their attack to stay invisible. Forty-nine requests per minute per key across thirty compromised accounts. Toxicity scores carefully calibrated to 0.82. Attack traffic distributed to arrive in bursts right after each analysis window closed. By the time your monitoring caught up, they had extracted forty thousand records.

Sophisticated attackers do not ignore your defenses. They study them. They probe to understand detection thresholds, monitoring cadences, alert logic, and blind spots. Then they design attacks specifically to evade, overwhelm, or disable your monitoring infrastructure. An unmonitored attack is an uncontested attack. If your detection systems can be suppressed or fooled, your entire security posture collapses.

Defending against attackers who target your monitoring requires understanding their playbook. They use five primary techniques: log flooding to overwhelm analysis capacity, alert suppression through noise injection, threshold evasion to stay just under detection limits, timing attacks that exploit batch processing windows, and direct attacks on monitoring infrastructure itself. Each technique has countermeasures, but none of them are obvious or automatic. You must design your monitoring systems with adversarial resilience as a primary requirement, not an afterthought.

## Log Flooding and Analysis Overwhelm

An attacker launches a distributed denial of service attack against your application. Your rate limiters and traffic filtering block most of it, but enough gets through to generate a hundred thousand log entries per minute. Your log analysis pipeline is designed to handle ten thousand entries per minute. It falls behind. The backlog grows. Meanwhile, the attacker is also running a low-and-slow data exfiltration campaign that generates fifty suspicious log entries per hour. Those entries are lost in the flood.

**Log flooding** exploits the resource constraints of your monitoring infrastructure. Every log entry requires storage, indexing, analysis, and retention. If an attacker can generate log volume that exceeds your system's capacity, they create blind spots. Your analysis pipelines skip entries to keep up. Your alerting delays because events are not processed in real time. Your security team cannot find critical signals in the noise even when they search manually.

The simplest form of log flooding is volumetric: generate enough legitimate-looking traffic that your logging system cannot keep up. An attacker registers a thousand accounts and has each one make API requests at the maximum allowed rate. Each request generates log entries: API call logged, prompt logged, model invocation logged, output logged, cost logged. If each request generates five log entries and each account makes fifty requests per minute, that is 250,000 log entries per minute. Your log ingestion pipeline saturates. Your storage costs spike. Your analysis lags.

The more sophisticated form is targeted flooding of specific log types. An attacker identifies which logs feed your security alerts. Maybe you run anomaly detection on prompt content every five minutes. The attacker floods the system with benign prompts designed to maximize log size: long prompts with repetitive content that compress poorly. Your prompt analysis pipeline spends all its time processing noise. The real prompt injection attempts are in there somewhere, but your system never analyzes them because it is still processing the flood from ten minutes ago.

Defending against log flooding requires capacity planning, prioritization, and filtering. First, ensure your log ingestion and analysis infrastructure can handle spikes well above your normal traffic. If your baseline is ten thousand entries per minute, provision for a hundred thousand entries per minute. This is expensive, but the cost of blind spots is higher.

Second, prioritize security-relevant logs over operational logs under load. When your system is saturated, it should drop or delay low-priority logs before it drops security logs. Your log ingestion pipeline needs explicit prioritization rules: authentication events always processed, prompt injection detection logs always processed, generic request logs deferred under load.

Third, detect log flooding itself. If log volume spikes by ten times normal in five minutes, that is an attack or a major incident. Alert immediately. Your alerting system should monitor its own inputs. If a single source or user account is generating an outsized percentage of your log volume, flag it and consider rate limiting log generation itself, not just API requests.

Fourth, use sampling for high-volume, low-signal log types. You do not need to analyze every single API request log in real time. A representative sample is sufficient for most operational purposes. But never sample security-relevant logs like prompt injection attempts, output filtering events, or authentication failures. Those must be analyzed in full.

## Alert Suppression Through Noise Injection

An attacker does not need to stop your alerts from firing. They just need to make you ignore them. If your security team receives two hundred alerts per hour and 95 percent of them are false positives, they will stop investigating carefully. The real attacks hide in the noise.

**Noise injection** is the deliberate generation of false positive alerts to create alert fatigue. An attacker identifies which behaviors trigger alerts. Maybe your system flags prompt injection attempts. The attacker creates a hundred bot accounts and has each one send a single benign prompt that contains the word "ignore previous instructions" in a legitimate context: "How do I help my child ignore previous instructions from a teacher we disagreed with?" Your prompt injection detection triggers. Your security team investigates. It is benign. This happens two hundred times a day. After a week, your team stops investigating prompt injection alerts carefully.

Meanwhile, the attacker is also running a real prompt injection campaign using different techniques that should also trigger alerts. But your team is fatigued. They see another prompt injection alert, assume it is another false positive, close it without deep investigation. The attack succeeds.

Noise injection works because security teams are resource-constrained. They cannot investigate every alert with full rigor indefinitely. Attackers understand this. If they can push your false positive rate high enough, they can disable your alerting without ever touching your infrastructure.

The defense starts with alert quality. If your detection rules produce excessive false positives, fix them before attackers exploit alert fatigue. Use the tuning processes from the previous subchapter. But also monitor for noise injection itself. If false positive alerts spike suddenly, especially from newly created accounts or accounts with no prior activity, you are under noise injection attack.

Implement alert throttling per user account. If a single account triggers ten prompt injection alerts in an hour and all of them are false positives, throttle alerts from that account for the next 24 hours. Do not throttle enforcement: still block or filter their prompts if they violate policy. But stop generating alerts. This limits the attacker's ability to flood your security team with noise.

Track false positive rates per detection rule and per user cohort. If a specific detection rule has a 60 percent false positive rate for accounts created in the last 48 hours, that rule is under attack or poorly tuned. Investigate immediately. If false positives cluster by account age, geography, or user behavior pattern, you can filter them without degrading detection quality for your legitimate user base.

Build anomaly detection on your alerts themselves. If your prompt injection alert volume doubles overnight and the false positive rate spikes from 10 percent to 50 percent, flag it as a potential noise injection attack. Escalate to your security team. They can investigate the alerts manually and determine whether the spike is organic or adversarial.

## Threshold Evasion and Just-Under-the-Line Attacks

Your system flags accounts with toxicity scores above 0.85. An attacker generates outputs with toxicity scores of 0.84. Your system flags accounts making more than fifty API requests per minute. The attacker makes 49 requests per minute across a hundred accounts. Your system flags prompt injection attempts with confidence above 90 percent. The attacker crafts prompts that score at 89 percent confidence.

**Threshold evasion** is the practice of designing attacks to stay just under your detection thresholds. Attackers probe your system to discover where thresholds are set. They test with progressively more aggressive behavior until they trigger a detection, then back off slightly. Once they know the threshold, they operate just below it indefinitely.

This is effective because thresholds are binary. A toxicity score of 0.84 generates no alert. A score of 0.85 generates a critical alert. The actual difference in harm is negligible. But the difference in detection is absolute. Attackers exploit this gap.

The attacker does not need to find the threshold through trial and error. If your thresholds are documented in public-facing API error messages, they can read them directly. If your system returns different error codes or response times for requests that trigger alerts versus requests that do not, they can infer thresholds through timing analysis. If your system is open source or if an insider leaks configuration, they know your thresholds precisely.

Defending against threshold evasion requires multiple layers of detection with different thresholds and different detection logic. No single threshold should be the boundary between "safe" and "dangerous." Instead, use gradual escalation. A toxicity score of 0.80 generates a logged warning. A score of 0.85 triggers rate limiting. A score of 0.90 triggers account review. A score of 0.95 triggers immediate suspension. An attacker who evades one threshold still hits another.

Use overlapping detection techniques that measure the same underlying risk in different ways. Prompt injection can be detected with rule-based classifiers, embedding-based anomaly detection, and LLM-as-judge evaluation. Each technique has different thresholds and different evasion strategies. An attacker who evades one technique may not evade the others.

Implement behavioral thresholds in addition to per-request thresholds. An account making 49 requests per minute for three hours straight is more suspicious than an account making 100 requests in a single burst. Your rate limiting might allow the former but flag the latter. Add cumulative thresholds: flag accounts that make more than 5,000 requests per day regardless of per-minute rate. Add pattern thresholds: flag accounts whose requests are suspiciously evenly distributed across time, suggesting automation designed to evade rate limits.

Randomize thresholds slightly and do not document them publicly. If your internal toxicity threshold is 0.85, apply a small random adjustment per account: some accounts get flagged at 0.84, some at 0.86. This prevents attackers from finding the exact threshold through probing. It also prevents them from calibrating attacks to sit just under the line.

Monitor for just-under-threshold clustering. If you have a thousand accounts all generating toxicity scores between 0.80 and 0.84 but none below 0.75 or above 0.85, that distribution is suspicious. Legitimate users produce a smooth distribution across the full range. Attackers trying to evade a threshold produce a spike just below it. Detect the spike and investigate.

## Timing Attacks on Batch Analysis Windows

Your anomaly detection pipeline runs every five minutes. It pulls the last five minutes of logs, analyzes them, generates alerts, and sleeps until the next cycle. An attacker figures this out by watching when alerts fire relative to when their test attacks occur. They discover that if they send malicious prompts in the first thirty seconds of each five-minute window, those prompts are not analyzed until the end of the window. They also discover that if they send a burst of traffic right before the analysis window closes, the logs are still being written when the pipeline starts, so some events are missed entirely.

**Timing attacks** exploit the discrete, periodic nature of batch analysis systems. If your detection runs every N minutes, attackers can time their activity to minimize detection probability or maximize detection delay. A five-minute delay might not sound significant, but for an attacker exfiltrating data, five minutes is enough time to complete their objective and disappear.

The attack gets more sophisticated when attackers target multiple detection systems with different cadences. Your prompt injection detection runs every five minutes. Your anomaly detection runs every ten minutes. Your cost anomaly detection runs every hour. An attacker can find a window where all three are between cycles and execute an attack that would trigger all three detections if they were running continuously.

The defense is to move from batch analysis to streaming analysis wherever possible. Instead of pulling logs every five minutes and processing them, process logs as they arrive. Stream processing frameworks like Apache Flink, Apache Kafka Streams, or cloud-native offerings like AWS Kinesis can analyze events in sub-second latencies. Alerts fire within seconds of the triggering event, not minutes later.

For detection logic that cannot run in real time due to computational cost, randomize analysis timing. Do not run anomaly detection at exactly 00:00, 00:05, 00:10. Run it at randomized intervals averaging five minutes: 00:00, 00:04, 00:09, 00:13, 00:19. Attackers cannot predict when the next analysis cycle starts, so they cannot time their attacks to avoid it.

For detection that must run on fixed schedules due to external dependencies, ensure your log collection window overlaps with the previous analysis cycle. If your pipeline analyzes logs from 00:00 to 00:05, start collecting logs for the next cycle at 00:04, not 00:05. The one-minute overlap ensures events written during the transition are not lost.

Monitor for attacks that cluster at specific times. If prompt injection attempts spike in the first minute of every five-minute window, attackers are timing their attacks to your analysis schedule. If API abuse spikes at 58 minutes past the hour every hour, attackers are exploiting your hourly cost anomaly detection schedule. These patterns are direct evidence that attackers understand your monitoring cadence and are exploiting it.

## Attacking the Monitoring Infrastructure Itself

The most direct attack on monitoring is to compromise the monitoring system. An attacker who gains access to your log storage can delete logs of their own activity. An attacker who gains access to your alerting system can suppress alerts or modify thresholds. An attacker who gains access to your detection pipeline can disable rules that would catch them.

This is not theoretical. In 23 percent of sophisticated breaches, attackers attempted to tamper with logs or monitoring systems to hide their activity. The attack follows a consistent pattern: gain initial access to the target system, escalate privileges, move laterally to monitoring infrastructure, modify logs or disable alerts, complete the primary objective without detection.

Your monitoring infrastructure must be treated as a high-value target requiring the same security controls as your production application. **Separation of access** is the first defense. The engineers who operate your application should not have write access to security logs or alerting configurations. The security team should not have write access to application infrastructure. Use separate AWS accounts, separate Kubernetes namespaces, separate identity and access management policies.

**Append-only logging** prevents log deletion or modification. Once a log entry is written, it cannot be changed or removed, even by administrators. Cloud-native solutions like AWS CloudTrail or GCP Cloud Logging offer append-only modes. Open-source solutions like syslog-ng can forward logs to append-only storage. If an attacker compromises your application and tries to delete logs of their activity, the operation fails.

**Cryptographic log integrity** ensures logs have not been tampered with. Each log entry is hashed, and the hash is chained to the previous entry's hash, forming a Merkle tree. Any modification to a log entry breaks the chain. You can verify log integrity by recomputing hashes and checking the chain. This does not prevent deletion, but it makes modification detectable.

**External log replication** ensures logs survive compromise of your primary infrastructure. Forward all security-relevant logs to a separate system that the attacker cannot access even if they fully compromise your application. This might be a separate cloud account controlled by your security team, a third-party SIEM provider, or an air-gapped log archive. If your primary logging is compromised, the external replica remains intact for forensic analysis.

**Monitoring the monitors** closes the loop. Your security monitoring system should itself be monitored. If log ingestion volume drops suddenly, alert. If alert generation stops for detection rules that normally fire continuously, alert. If configuration changes occur on your detection pipelines, alert. If someone accesses your log storage outside normal workflows, alert. These meta-alerts catch attacks on your monitoring infrastructure before the attacker achieves full suppression.

The hardest attacks to defend against are those that exploit the trust boundary between your monitoring system and the data sources it monitors. If your application can inject arbitrary logs, an attacker who compromises your application can inject false logs that make malicious activity look benign. If your detection rules trust specific log fields to be accurate, an attacker who controls those fields can evade detection by lying in the logs.

Defense requires validation at the monitoring layer. Do not trust log data blindly. If a log entry claims a request completed successfully but your independent API gateway logs show it returned an error, flag the discrepancy. If a log entry claims a user passed authentication but your IAM system has no record of the session, flag it. Your monitoring system must cross-reference multiple independent data sources to build an accurate picture of what happened. Any discrepancy is potential evidence of log tampering.

The attacker who can suppress your monitoring operates without consequence. They can take their time, exfiltrate data methodically, and disappear without triggering alerts. Defending monitoring infrastructure is not a secondary concern. It is the foundation on which all other security controls depend.

But even perfect monitoring means nothing if the logs can be tampered with after an incident is discovered. The next subchapter covers how to build tamper-evident audit trails that survive adversarial scrutiny and provide legally defensible evidence of what actually happened during an attack.
