# 6.7 — Defense: Rate Shaping and Entropy Shaping

Rate limits are the first defense everyone implements against model extraction. They are also the defense attackers defeat most reliably. A simple rate limit says: "You can make 100 requests per hour." A patient attacker says: "Fine, I will extract your model over six months instead of six hours." The attacker has time. You have a model to protect. Static rate limits create a false sense of security while doing almost nothing to prevent determined extraction.

The real defense is not rate limiting. It is rate shaping and entropy shaping — adaptive systems that make extraction not just slow, but exponentially more expensive and unreliable the longer it continues. These defenses do not stop the first hundred queries. They make the hundred-thousandth query worthless.

## Why Pure Rate Limits Fail

A fixed rate limit treats every user the same: 100 requests per hour, 1000 per day, 10,000 per month. It assumes that staying under the threshold means the user is legitimate and exceeding it means they are malicious. Both assumptions are wrong.

A legitimate enterprise customer might send 50,000 queries per day across a hundred internal users, all funneled through one API key. An attacker might send 80 queries per day from a single account, carefully staying under your limit while systematically covering your model's capability space. The rate limit sees the enterprise customer as suspicious and the attacker as compliant.

Rate limits also assume attackers use one account. In mid-2025, a cybersecurity company detected an extraction attempt that used 340 accounts created over three months, each staying well under the 500-query daily limit. The attacker rotated through accounts on a schedule, maintaining perfect compliance with rate policies while extracting training data for a competitor model. The company did not detect the attack until a security researcher found their proprietary knowledge appearing in a rival product. The rate limits worked exactly as designed. The model was stolen anyway.

The problem is that rate limits measure volume, not intent. An attacker does not need high volume. They need coverage. If your model has a million possible high-value outputs, the attacker does not need to extract all of them — they need the ten thousand that represent your core differentiation. At 80 queries per day, that is four months of extraction. Most API providers never flag it.

## Rate Shaping: Adaptive Limits Based on Behavior

Rate shaping replaces fixed limits with dynamic limits that adjust based on what the user is actually doing. The system does not ask "How many queries?" It asks "What pattern do these queries form?"

A user who queries your legal research model about fifty different case law topics in one day looks like a practicing attorney. A user who queries it about the same narrow topic from five hundred slightly different angles looks like someone building a training dataset. Rate shaping gives the attorney higher limits and the extractor lower ones — even if both send the same total number of requests.

The technique works by tracking query diversity. Every query gets scored on multiple dimensions: semantic similarity to previous queries, coverage of the input space, repetition of rare phrasing patterns, unusual focus on edge cases. A diversity score drops as queries become more extraction-like. As the score drops, the rate limit tightens.

In late 2025, a machine learning platform implemented diversity-weighted rate limits for their code generation API. Legitimate users — developers writing different features across multiple projects — maintained high diversity scores and saw no change in their access. Users with low diversity — systematically querying every combination of a prompt template — saw their rate limits drop from 1000 queries per day to 50, then 10, then a temporary block pending review. Extraction attempts that previously took three weeks now triggered restrictions in three hours. The cost of stealing the model increased by a factor of twenty, and most attackers abandoned the effort before reaching useful coverage.

The diversity calculation does not need to be perfect. It just needs to make extraction harder than training a model from scratch. When stealing your model requires more effort than building their own, most attackers choose to build their own.

## Entropy Shaping: Degrading Output Quality for Suspected Extraction

Rate shaping slows extraction. Entropy shaping poisons it. The principle is simple: once the system detects extraction behavior, it does not block the attacker — it quietly degrades the quality of the outputs, making the extracted data unreliable for training.

The degradation is subtle. The model still produces coherent responses. It still answers the question. But the responses contain small introduced errors: slightly wrong numbers, subtly misleading phrasing, confident claims with inverted logic. A model trained on this poisoned data learns patterns that look right but fail in production. The attacker does not discover the problem until after investing weeks of training time and tens of thousands of dollars in compute.

Entropy shaping requires careful calibration. Degrade too much and the attacker notices. Degrade too little and the extracted model still works. The goal is to stay in the narrow band where outputs appear useful during extraction but produce unreliable models during fine-tuning.

A financial services company implemented entropy shaping in their risk assessment API in mid-2025 after detecting systematic extraction attempts. Users with high diversity scores received unmodified outputs. Users with extraction patterns received outputs with noise added to numerical predictions — typically 2 to 5 percent deviation from the true value, randomized per query. The attacker completed their extraction, trained a model on 80,000 queries, and deployed it to production. The model's accuracy was 15 percentage points lower than expected. The attacker never traced the problem back to poisoned training data. They assumed their fine-tuning process was flawed and abandoned the project after three months of failed iterations.

The technique has an ethical boundary. You cannot degrade outputs for paying customers, even if you suspect extraction. The line is clear: if they are paying for API access, they are entitled to correct outputs, regardless of what they do with them. Entropy shaping applies only when you can legally and contractually introduce noise — typically in free tiers, research previews, or terms-of-service violations.

## Combining Rate and Entropy Shaping

The strongest defense uses both techniques in sequence. Rate shaping identifies suspicious behavior early. Entropy shaping makes continued extraction worthless.

The system starts with standard rate limits. As query diversity drops, rate limits tighten progressively. If the user continues despite restrictions — burning through rate limit resets, switching between accounts, adapting query patterns to evade detection — the system escalates to entropy shaping. At this point the attacker has demonstrated sustained intent. The outputs degrade quietly. The attacker extracts garbage without knowing it.

In late 2025, a language model API provider implemented a three-tier response system. Tier one: normal rate limits for normal users. Tier two: adaptive rate shaping for users with declining diversity scores. Tier three: entropy shaping for users who continued extraction behavior for more than 72 hours despite rate restrictions. The system flagged 140 accounts for tier-two restrictions in the first month. Twelve accounts escalated to tier three. None of the tier-three accounts successfully trained a viable model. The company later found evidence that two of the tier-three accounts had been linked to known model extraction services. Both services shut down operations within six months, citing "unsustainable data acquisition costs."

The combination works because it forces the attacker to solve two problems simultaneously: evade behavioral detection and verify output quality. Solving the first makes the second harder. If the attacker slows their extraction to avoid rate shaping, they increase the time window during which you can detect and respond to the threat. If they accept entropy shaping and try to filter bad outputs, they need a reference model to validate against — which defeats the purpose of extraction.

## The UX Impact on Legitimate Users

Every defense that limits access risks harming legitimate users. Rate shaping and entropy shaping are no exception. The risk is not that you block real customers — you can tune thresholds to avoid that. The risk is that you create anxiety about being falsely flagged.

A developer testing edge cases for their integration might trigger extraction signals: repetitive queries, narrow focus, systematic exploration of error conditions. A researcher studying model behavior might send queries that look indistinguishable from extraction attempts. A legitimate power user might hit rate limits you intended for attackers. Each of these users has a bad experience because your security system cannot perfectly distinguish intent.

The mitigation is transparency and appeal. If a user hits rate restrictions, tell them why and give them a path to restoration. "Your recent query patterns triggered our extraction prevention systems. If you are conducting legitimate research or testing, please contact support with details about your use case." Most legitimate users will reach out. Most attackers will not.

A machine learning platform introduced adaptive rate limits in mid-2025 and saw a spike in support tickets from users who felt unfairly restricted. The team responded by building a self-service appeal flow: users could request a review, provide context about their use case, and receive a decision within four hours. Ninety-two percent of appeals were approved. The remaining eight percent were confirmed extraction attempts or terms-of-service violations. The UX friction was real, but the security benefit was worth it. More importantly, legitimate users understood the reason for the restrictions once they were explained.

## Implementation Without Overengineering

Rate shaping and entropy shaping sound complex. They are not. The simplest implementation requires three components: a query logger, a diversity scorer, and a response modifier.

The query logger records recent queries per user: timestamps, input text, output text, metadata. The diversity scorer calculates a rolling diversity metric based on semantic similarity between recent queries. The response modifier checks the diversity score and either passes the output unchanged, tightens the rate limit for the next request, or introduces noise into the output.

You do not need a machine learning model to calculate diversity. Simple heuristics work: cosine similarity between query embeddings, coverage of a predefined topic space, repetition rate of rare n-grams. You do not need real-time entropy adjustment. A daily batch job that flags suspicious accounts and applies degradation to subsequent queries is sufficient. You do not need perfect detection. You need to make extraction unreliable enough that attackers choose other targets.

The goal is not to build an impenetrable fortress. The goal is to make extraction more expensive than the alternatives. When the cost of stealing your model exceeds the cost of training their own, you win.

The next question is how to detect extraction attempts not through rate or output patterns, but through the behavioral fingerprints that attackers leave in their query structures. Even attackers who evade rate shaping leave traces in how they explore your model's input space. That is behavioral fingerprinting.

---

**Next: 6.8 — Defense: Behavioral Fingerprints for Scraping Detection**
