# 12.7 â€” Incident Classification and Severity Assessment

The security team received the alert at 2:47 AM. A healthcare platform's AI assistant had started refusing to answer clinical questions while still responding to general queries. The on-call engineer marked it as a P2 availability incident and began debugging the model endpoint. By the time someone checked the prompt logs three hours later, they discovered the real problem: a coordinated jailbreak attack had been running for six hours, extracting patient condition patterns through carefully crafted queries. The attacker had already harvested data on twelve thousand users. The incident should have been classified as P0 the moment it was detected. The three-hour delay in proper classification meant three additional hours of active data exfiltration.

AI security incidents require different classification frameworks than traditional software incidents. A database outage affects availability. A credential leak affects confidentiality. But an AI attack can simultaneously affect availability, confidentiality, integrity, and correctness in ways that evolve as the attacker learns. The classification determines response speed, team composition, escalation path, and documentation requirements. Get it wrong and you either over-react to noise or under-react to catastrophe. Most organizations discover their classification framework is wrong only after an incident reveals the gap.

## AI-Specific Incident Categories

Traditional incident categories map poorly to AI attacks. A prompt injection is not quite an injection attack, not quite a logic bug, not quite social engineering. A model extraction attempt looks like normal usage in isolation. A membership inference attack generates no failed authentication events. You need categories that reflect AI-specific threat patterns.

**Prompt manipulation incidents** include jailbreaks, prompt injections, and context poisoning. These attacks manipulate the model's behavior through crafted inputs rather than exploiting code vulnerabilities. The key indicator: the model behaves incorrectly but technically correctly given the manipulated prompt context. Classification difficulty: distinguishing malicious manipulation from legitimate edge-case inputs or user errors.

**Data exfiltration through inference** covers membership inference, attribute inference, and model inversion attacks. The attacker uses the model's responses to reconstruct training data or user data. The challenge: each individual query looks legitimate. Only the pattern reveals the attack. Classification requires recognizing sequences, not individual events. A team that waits for a smoking-gun query will miss the attack entirely.

**Model integrity incidents** involve poisoning, backdoors, or unauthorized model updates. The model itself has been compromised, not just its usage. These are the highest-severity incidents because the entire system becomes untrustworthy. Detection is difficult because the model still works correctly on normal inputs. The attack only manifests on trigger conditions the attacker controls.

**Denial-of-service and resource exhaustion** includes adversarial input attacks that force expensive computation, repeated requests designed to exhaust quotas, and attacks that manipulate the model into infinite loops or cascading tool calls. These look like performance problems until you examine the input patterns. Classification key: resource consumption spikes without corresponding legitimate traffic spikes.

**Abuse of model capabilities** covers content generation for spam, phishing, disinformation, harassment, or illegal content. The model works exactly as designed but is being used for harmful purposes. Classification complexity: distinguishing malicious use from legitimate edge cases, especially for content moderation or security research. Your incident response must preserve evidence while respecting legitimate use cases.

**Tool misuse and unauthorized actions** applies to agent systems. The model calls tools it should not have accessed, performs actions beyond its authorization, or chains tool calls in ways that circumvent safety controls. The individual tool calls may each be authorized but the sequence is not. Classification requires understanding the full action chain, not just the final step.

## Severity Levels Based on Impact

Severity should map directly to user harm, data exposure, financial damage, and regulatory risk. A P0 incident means active, ongoing damage that requires immediate all-hands response. A P4 incident means minimal impact that can be addressed during normal working hours. Most organizations inherit severity frameworks from traditional software where P0 means "the site is down." For AI systems, the site can be fully operational while a P0 incident is underway.

**P0 severity indicators**: Active data exfiltration of PII, PHI, or confidential business data. Model actively causing user harm through dangerous recommendations, incorrect medical information, or actions that violate safety policies. Regulatory violation in progress that will trigger mandatory breach notification. Financial damage exceeding ten thousand dollars per hour. Evidence of model compromise or backdoor. Attacker has gained control of agent capabilities or tool access. These incidents require immediate containment even if it means taking the system offline.

**P1 severity indicators**: Successful jailbreak that bypasses core safety controls but has not yet caused confirmed user harm. Data leakage that does not meet P0 thresholds but still represents privacy violations. Attacks that could escalate to P0 if not contained within hours. Successful prompt injections that alter model behavior for multiple users. Evidence of systematic abuse that has not yet achieved the attacker's presumed goal. These require same-day containment and dedicated incident response team.

**P2 severity indicators**: Isolated jailbreak attempts that succeeded once but are not reproducible or widespread. Denial-of-service attacks affecting model availability but not data integrity. Content policy violations that represent abuse but not immediate safety risks. Attempted attacks that failed but revealed security weaknesses. These require investigation within twenty-four hours and fixes within one week.

**P3 severity indicators**: Anomalous usage patterns that might represent reconnaissance for future attacks. Failed attack attempts with no impact on users or data. Content policy edge cases that require human review but do not represent active abuse. Security misconfigurations discovered through audit rather than exploitation. These require investigation within one week and remediation within one month.

**P4 severity indicators**: Theoretical vulnerabilities with no evidence of exploitation. False positives from detection systems that require tuning. Policy violations by legitimate users who misunderstood usage terms. These are tracked for trends but do not trigger incident response processes.

The severity level determines response time. P0 incidents require response within fifteen minutes, initial containment within one hour. P1 incidents require response within two hours, containment within twenty-four hours. P2 incidents require response within one business day. Many organizations set these targets for traditional incidents but never test whether they are achievable for AI incidents. A database P0 has a known playbook: fail over to replica, restore from backup, debug the primary. An AI P0 has no standard playbook because the attack surface is probabilistic.

## Triage Workflow: Who Assesses, How Fast

The first person who receives the alert is rarely the right person to classify the incident. The on-call engineer understands system health, not attack patterns. The security analyst understands attacks, not model behavior. The AI engineer understands models, not user impact. Triage requires collaboration between roles, but time pressure forces decisions before that collaboration can happen.

The triage workflow must specify decision authority at each step. The initial alert goes to whoever is on-call, typically a platform engineer or SRE. Their job is not to classify the incident fully but to make a fast provisional classification: is this P0 or not P0. If P0, escalate immediately to the security on-call and the AI engineering on-call simultaneously. If not P0, create an incident ticket and assess during business hours with proper cross-functional input.

The P0 provisional classification should be based on simple, observable criteria that do not require deep analysis. Active data in attacker hands: P0. Users reporting dangerous model outputs: P0. Model refusing to respond to an entire category of queries: P0. Everything else: not provisionally P0, pending review. This prevents the on-call engineer from having to assess attack sophistication or model internals under time pressure.

Once the incident is escalated, the full triage team assembles: security lead, AI engineering lead, product lead, and legal if the incident involves data exposure or regulatory risk. This team has thirty minutes to confirm severity and assign an incident commander. The incident commander owns all response decisions from that point forward. Most organizations skip this step and end up with five people making contradictory containment decisions simultaneously.

The triage process must generate an initial incident report within one hour for P0, within four hours for P1. This report documents what is known, what is unknown, and what containment actions have been taken. It does not need to explain root cause yet. It does need to provide enough information for executives to understand business impact and for legal to assess regulatory obligations.

## Escalation Triggers for Critical Incidents

Escalation is not about severity. Escalation is about decision authority. A P2 incident does not escalate to executives unless it reveals systemic security failures. A P1 incident escalates immediately if it involves sensitive data categories, potential regulatory violations, or user safety.

**Automatic escalation triggers**: Any incident involving PHI, PII of more than one thousand users, or financial data. Any incident where the model provided incorrect medical, legal, or financial advice that users relied upon. Any incident where user actions triggered by model outputs caused real-world harm. Any incident involving children or other protected populations. Any incident where an attacker has demonstrated the ability to manipulate model behavior across multiple sessions or users. Any incident that will require regulatory breach notification.

When escalation happens, the escalation message must answer four questions in order: What happened. How many users are affected. What have we done so far. What decision do we need from you. The executive does not need a lecture on transformer architecture. They need to know whether to notify customers, whether to notify regulators, whether to take the system offline, and whether to involve legal counsel.

Escalation should happen even when the answer is "we contained it and no user harm occurred." Executives need to know that the security controls worked, not just that they failed. A pattern of successfully contained P1 incidents tells leadership that investment in security is paying off. A pattern of P0 incidents that were initially classified as P2 tells leadership that the classification framework is broken.

## Documenting Incidents from First Detection

The documentation starts the moment the incident is detected, not after it is resolved. Every decision, every log query, every containment action gets timestamped and recorded. This serves three purposes: legal defensibility, post-incident learning, and detection of similar incidents in the future.

The initial documentation captures the detection event: what alert fired, what log query revealed the issue, what user report triggered investigation. This establishes the timeline. Many organizations discover during post-incident review that they had signals of the attack hours or days before the official detection time. The documentation should capture when the first signal appeared, even if it was not recognized as an incident at the time.

As the incident progresses, document every hypothesis, every test, every containment decision. When you decide to isolate a user account, document why. When you decide not to take the system offline, document why. When you query logs, document what you were looking for. This creates an audit trail that protects the responders from second-guessing after the incident. The decision to not take the system offline at 3 AM might look reckless at 10 AM unless the documentation shows that the data supported that choice at the time.

The documentation must be structured enough to enable future automation and analysis. Every incident should record affected user count, attack vector, detection method, containment method, time to detection, time to containment, and root cause category. This enables trend analysis: are prompt injection incidents increasing, are detection times improving, are certain attack vectors always missed by automated detection.

## Moving to Containment

Classification establishes what you are facing. Severity establishes how fast you must act. Triage establishes who is in charge. Escalation ensures decision authority. Documentation ensures you can defend your choices and learn from them. Now the incident response shifts from assessment to action.

The containment phase operates under a tight clock. For P0 incidents, containment must begin within one hour of detection, ideally within thirty minutes. The longer the attack continues, the more data is exposed, the more users are affected, and the more expensive remediation becomes. But containment cannot be reckless. Shutting down the entire system to stop a single attacker is usually the wrong choice. The containment strategy must balance stopping the attack against preserving system availability for legitimate users.

The next critical skill is containment itself: isolating the attack, stopping ongoing damage, and preserving evidence while the system continues to serve legitimate traffic.
