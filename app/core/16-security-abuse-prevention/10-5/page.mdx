# 10.5 — Latency Amplification: Triggering Slow Operations

The most expensive attack isn't the one that uses the most tokens. It's the one that makes you wait. Every second your model spends processing a malicious request is a second it can't serve legitimate users. Every compute instance tied up in a slow operation is compute you're paying for while generating zero value. The attacker who understands latency as a weapon doesn't need to send a million requests — they just need to make each request take a hundred times longer than it should.

In early 2026, a code analysis API began experiencing severe performance degradation. The system was designed to accept code snippets, analyze them for security vulnerabilities, and return results within three seconds. Suddenly, average response time spiked to forty-seven seconds. The team investigated and found that a small number of requests were taking over five minutes each. These requests looked legitimate — they contained valid code, reasonable lengths, and normal formatting. The difference: the code was specifically crafted to trigger the most expensive analysis pathways. Every function called every other function. Every conditional branch was maximally complex. Every loop contained nested loops that referenced arrays of pointers to structs that themselves contained function pointers. The static analyzer couldn't short-circuit any of the analysis. It had to explore every possible execution path, and the attacker had constructed code where the path count grew exponentially with each line. Four users discovered this pattern independently. Over ten days, they submitted three thousand requests that collectively consumed nineteen days of compute time.

Latency amplification attacks exploit the gap between fast-path and slow-path operations in AI systems. Every tool call, every retrieval operation, every model invocation has a distribution of possible execution times. The attacker's goal is to reliably trigger the worst-case latency for every component, turning a system designed to respond in subseconds into one that takes minutes or never completes at all.

## Identifying Slow Operations

Every AI system has operations that are orders of magnitude slower than the baseline model call. Web search can take two to eight seconds depending on network conditions and search complexity. Code execution can take anywhere from milliseconds to minutes depending on what the code does. Complex reasoning modes that use chain-of-thought or extended thinking can multiply latency by ten or more compared to standard sampling. Multi-hop retrieval where each retrieval step depends on the previous step's results inherently serializes operations that could theoretically be parallel.

The attacker maps these slow operations through reconnaissance. They submit queries that trigger different tool calls and measure response times. They identify which tools have the longest tail latencies. They determine which types of reasoning prompts cause the model to enter extended thinking modes. They discover which retrieval queries cause the most database load. Once they have this map, they craft prompts designed to trigger the slowest path through your system.

A practical example: a RAG system that searches a document store, retrieves relevant chunks, and then generates an answer. The fast path is a single retrieval that returns three chunks, followed by generation. The slow path is a query that causes the retrieval system to search across every index, retrieve dozens of potential matches, re-rank them using a cross-encoder model, and then discover none of them are actually relevant so it repeats the process with a reformulated query. The attacker learns to craft queries that reliably trigger the slow path. One request that takes thirty seconds has the same resource impact as sixty requests that take half a second each.

## Prompts Designed to Maximize Thinking Time

Models with extended reasoning modes — systems that use chain-of-thought, tree-of-thought, or explicit step-by-step thinking — can be forced into their slowest reasoning patterns by adversarial prompts. The attacker doesn't need to break the model. They just need to ask questions that maximize the number of reasoning steps before reaching an answer.

The pattern is straightforward: pose problems that require the model to explore many branches before concluding. Questions with no clear answer force the model to consider every possibility. Problems that require disproving multiple hypotheses force sequential reasoning that can't be short-circuited. Queries that ask the model to check its work, verify its reasoning, and consider alternative explanations trigger meta-reasoning loops that multiply latency.

A concrete case: "Explain why this statement might be true, then explain why it might be false, then determine which explanation is more convincing, then critique your own determination, then provide an alternative critique." The model can't skip any step. Each step generates hundreds of tokens of internal reasoning before producing output. A request that would normally take two seconds takes forty. The attacker submits fifty of these in parallel. Your system is now handling twenty minutes of total compute time for requests that look like normal user queries.

The defense is to cap reasoning time or reasoning token count, but this creates a trade-off. Users who legitimately need deep reasoning suffer. The attacker who discovers your cap can craft inputs that consume exactly that much reasoning time and no more — maximizing cost without triggering abuse detection. There is no perfect mitigation. You're choosing between allowing latency attacks and degrading service for legitimate complex queries.

## Tool Call Patterns That Chain Slow Operations

Agent systems that orchestrate multiple tool calls are especially vulnerable to latency amplification through tool chaining. The attacker crafts prompts that cause the agent to call tool A, use the result to call tool B, use that result to call tool C, and so on, with each tool being one of the slow operations the attacker identified during reconnaissance.

A realistic sequence: the agent receives a query that requires it to first search the web for current information, then execute code to parse the results, then query a database to cross-reference the parsed data, then make another web request to verify the database results, then execute more code to format the final answer. Each step is individually justified. Each tool call is legitimate. The total latency is twelve seconds. A normal query would have taken one second with a single tool call or no tool calls at all.

The attack becomes more effective when the agent's reasoning is deterministic enough that the attacker can predict the tool chain. They test variations of a query, observe which tool sequences result, and then craft the final prompt that reliably triggers the longest chain. If they can get the agent to call five slow tools sequentially, and each tool averages four seconds, they've created a twenty-second request. Submit ten of these and you've locked up over three minutes of compute time that could have served hundreds of normal requests.

Detection requires profiling tool chains at the session level. A user whose requests consistently trigger longer-than-average tool chains is either working on unusually complex tasks or deliberately amplifying latency. The challenge is the same as with reasoning time: legitimate power users will also trigger long tool chains. The signal is not binary. You're looking for statistical outliers and combining tool chain length with other signals like cost per request and session-level patterns.

## Timeout Exploitation in Multi-Agent Systems

Multi-agent systems where agents wait for responses from other agents or services create opportunities for timeout-based amplification. The attacker identifies operations that have long timeouts — thirty seconds, sixty seconds, or even longer — and crafts prompts that cause the system to wait the full timeout period before failing or retrying.

A common scenario: an agent framework configured with a thirty-second timeout for external API calls. The attacker triggers a tool that makes an HTTP request to an endpoint they control. The endpoint accepts the connection but never sends a response. The agent waits the full thirty seconds, times out, and then either retries or reports failure. The attacker has consumed thirty seconds of compute time with a request that cost them nothing but a held-open TCP connection.

Retry logic amplifies this further. If the system retries three times on timeout, each with a thirty-second timeout, the attacker has turned one request into ninety seconds of blocked compute. If they can trigger ten of these in parallel, they've locked up fifteen minutes of agent capacity. The entire agent pool becomes unresponsive to legitimate requests while it waits for attacker-controlled timeouts to expire.

Mitigation requires aggressive timeout reduction for untrusted operations and intelligent retry policies that detect timeout-based attacks. If a user's requests consistently time out on external calls, especially to a small set of endpoints, you're under attack. The timeout should be shortened, retries should be disabled for that user, or the requests should be blocked entirely. The user who legitimately experiences timeouts will complain and you can investigate. The attacker will move on to a different vector.

## Connection Exhaustion Through Slow Responses

A variant of timeout exploitation targets the connection pool itself rather than compute time. Many AI APIs maintain a pool of open connections, worker threads, or execution contexts. If the attacker can occupy these resources with long-running requests, they prevent new requests from being processed even if the underlying compute is available.

The attack works by sending requests that trigger slow operations, but instead of waiting for them to complete, the attacker simply holds the connection open. They might send a prompt that causes the model to generate a ten-thousand-token response. The model generates tokens slowly — fifty tokens per second. The attacker receives the stream but processes it as slowly as possible, applying backpressure to keep the connection and execution context alive for as long as possible. Two hundred seconds later, the request finally completes. During those two hundred seconds, that connection slot was unavailable to other users.

Systems that limit concurrent connections per user provide some defense, but the attacker can rotate through multiple accounts or identities. Systems that limit total concurrent connections across all users are vulnerable if the attacker can occupy a significant fraction of the pool. If you allow five hundred concurrent connections and the attacker can sustain two hundred slow requests across different accounts, your system's effective capacity drops by forty percent.

The signal for this attack is high concurrency combined with unusually long request durations from the same set of users or IP ranges. A single user with twenty concurrent requests that all last over a minute is almost certainly malicious. The legitimate user who needs high concurrency wants their requests to complete quickly. The attacker who wants to exhaust your connection pool needs their requests to last as long as possible. The goals create opposite patterns in the duration distribution.

## Amplification Ratios and Economic Feasibility

Latency amplification, like token bombs, has an economic efficiency metric. The attacker measures how much of your compute time they can consume per unit of their effort. If they can send one request that occupies ten seconds of compute, they've achieved a ten-to-one ratio if their request took one second to construct and send. If they can send a request that triggers a five-minute timeout, and they can do this programmatically, the ratio becomes three hundred to one.

The viability of the attack depends on whether your system charges by request, by compute time, or by tokens. If you charge a fixed fee per request regardless of latency, the attacker has no direct monetary incentive but can still cause denial of service. If you charge by compute time or tokens, the attacker can drive up costs while staying under rate limits that only count requests. A user allowed one hundred requests per hour who makes each request consume ten times the normal compute time is effectively making one thousand requests worth of load.

The defense is to rate limit on resource consumption, not just request count. Track compute time, token count, tool calls, and latency per user. Apply limits that cap total consumption per time window, not just total requests. A user who exhausts their compute time budget in ten minutes because each request takes a minute should be throttled until the next window, even if they've only made ten requests. This aligns your rate limits with your actual resource costs and makes latency amplification attacks expensive for the attacker in terms of their usage quota.

## The Compounding Effect of Slow Requests

A system under latency amplification attack degrades in ways that aren't immediately obvious from average metrics. If ten percent of requests take fifty times longer than they should, your average latency might only double. But your p99 latency explodes. Your queue depth grows. Your thread pools saturate. Requests that should be fast start waiting behind slow requests, causing cascading latency increases even for users who aren't part of the attack.

This compounding effect makes latency attacks more dangerous than their direct resource consumption suggests. A hundred slow requests don't just consume a hundred times the normal compute time for those requests — they degrade performance for thousands of other requests that get queued behind them. The attacker who achieves sustained latency amplification can make your entire service unusable with a surprisingly small number of malicious requests.

Detection requires monitoring queue depth, thread pool saturation, and latency distributions, not just average latency. If your p99 latency is spiking while your median latency stays flat, you have a small number of outlier requests causing the problem. If queue depth is growing during normal traffic periods, something is consuming resources faster than your system can free them. These are the signals that distinguish latency amplification from normal load spikes.

The attacker who masters latency amplification can make your system unavailable without ever exceeding rate limits, without triggering abuse detection based on prompt content, and without requiring massive infrastructure. But there's an even more profitable attack: instead of making your system slow or unavailable, the attacker uses it for their own purposes and resells what they steal.

