# 10.3 — Cost Exhaustion Attacks: Draining Budgets

The enterprise AI platform had implemented cost-based rate limiting. Each user account had a monthly quota measured in inference units, carefully calibrated to their pricing tier. Free users got a thousand units. Paid users got fifty thousand. The system tracked token consumption in real time and enforced limits. In March 2025, an attacker on a paid account burned through forty-eight thousand units in nineteen hours — ninety-six percent of their monthly quota. They did not flood the system. They did not bypass rate limits. They simply crafted every single request to be as expensive as possible within the rules. Each prompt maximized context, triggered the most expensive model tier, invoked multiple tool calls, and generated outputs at the length limit. When the quota exhausted, the attacker abandoned the account. It had cost them seventy-nine dollars to purchase. They had extracted eleven thousand dollars worth of inference.

Cost-based quotas prevent naive flooding but open a new attack surface. An attacker who cannot bypass the quota learns to maximize the value extracted before hitting the limit. This is not about volume. This is about intentional optimization to consume maximum resources per allowed request. The attack succeeds when the attacker extracts more value than they paid for access. On free tiers, that threshold is zero. On paid tiers, that threshold is the subscription cost. Either way, the economics can favor the attacker when they understand your cost model better than you expect.

## Intentionally Expensive Prompt Design

Attackers who want to maximize cost per request reverse-engineer your pricing model. Which features cost more? Long context or short context? Simple completion or chain-of-thought reasoning? Text-only or multi-modal? Single-turn or multi-turn sessions? Once they map your cost surface, they design prompts that activate every expensive feature simultaneously.

A prompt that includes a forty-page PDF document, asks for comprehensive analysis with citations, requests output in multiple formats, and specifies the most capable reasoning mode will cost significantly more than a prompt that asks a simple question with no context. The attacker is not asking for anything the system does not support. They are asking for the most expensive version of what the system legitimately provides. Your system processes the request because it is valid. Your budget decreases by the maximum amount.

This technique requires understanding which features are expensive and how much. Attackers learn this through experimentation. They submit variations of prompts and observe response times, output lengths, and quota deductions. Longer response times usually correlate with higher costs. Features that consume quota faster are more expensive. Through iterative testing, an attacker maps the relationship between prompt structure and cost. Then they design prompts that sit at the maximum of that cost function.

A research AI platform discovered this in late 2025. An attacker had systematically tested different prompt structures over five days, measuring quota consumption for each. They identified that requests combining document upload, multi-step reasoning, and structured output formatting consumed quota three times faster than simple questions. They then submitted nothing but this prompt type for four days, extracting maximum value from their paid account before the monthly quota exhausted. The account was purchased with a stolen credit card. The platform absorbed the cost.

Intentionally expensive prompts are hard to defend against because they are legitimate use cases. Some users genuinely need comprehensive analysis of long documents with structured outputs. The difference between a power user and an attacker is intent — the attacker is optimizing for cost extraction, the power user is optimizing for task completion. Intent is not observable from the request itself.

## Recursive Tool Call Patterns

AI systems that support tool calling — executing functions, querying databases, calling external APIs — create an attack surface when tool calls can trigger additional tool calls. An attacker designs prompts that cause recursive patterns, where one tool call results in three more, each of which results in three more, creating exponential resource consumption.

The simplest version is a prompt that asks the system to "check all records that meet condition X and for each record, fetch related data from source Y." If there are a hundred records meeting condition X, the system makes a hundred calls to source Y. If each call to source Y returns data that suggests checking source Z, and the system is designed to follow those suggestions, the hundred calls become three hundred. If each source Z result triggers another lookup, three hundred becomes nine hundred. The prompt asked for something reasonable. The system's tool-calling logic created the explosion.

This is not a bug in the system. This is intended functionality — tool calls should be able to trigger subsequent tool calls when the task requires it. An attacker exploits this by crafting prompts that maximize the depth and breadth of tool call chains. They find tasks where the initial query returns maximum results, each result triggers maximum follow-up calls, and each follow-up call triggers further calls. The system behaves correctly. The cost scales exponentially.

A financial AI service encountered this in January 2026. An attacker submitted prompts that asked for portfolio analysis across a hundred holdings, where each holding required fetching current price data, historical performance, related news, and correlation analysis with other holdings. The initial prompt triggered a hundred price lookups. Each price lookup triggered a historical data fetch. Each historical data fetch triggered a correlation calculation with ninety-nine other holdings. A single prompt cascaded into over ten thousand API calls to external data sources. The attacker made six such requests over three days. The cost in external API fees was forty-seven thousand dollars. The attacker's account was on a free trial.

Recursive tool calls are particularly effective against systems that bill for external API usage separately from inference costs. Even if inference quotas are enforced, tool call costs may not be capped. An attacker focuses prompts on operations that trigger expensive external calls. The inference cost stays under quota. The external API costs drain the operational budget.

## Context Window Stuffing With Useless Data

Most AI systems allow users to provide context — documents, previous conversation history, reference materials. This context is necessary for the model to perform the task. It also costs money. Every token in the context consumes resources. An attacker can stuff the context window with data that is irrelevant or redundant but forces the system to process maximum tokens.

The attacker uploads a document that is ninety percent boilerplate, repeated sections, or padding, and ten percent actual content relevant to the question. The system processes all of it. The cost is based on total tokens, not relevant tokens. A two-hundred-page document where a hundred ninety pages are filler costs nearly as much to process as a two-hundred-page document of dense useful content. The attacker gets the same output — analysis of the ten percent that matters — at maximum cost.

Context stuffing works because most systems do not analyze uploaded content for relevance before processing. The user says "analyze this document," the system loads the document into context, and the model processes it. Detecting that ninety percent of the content is irrelevant requires understanding the question and the content semantically, which itself requires processing the content. The defense creates the same cost as the attack.

A legal AI platform found this pattern in November 2025. An attacker was uploading contract templates that were eighty percent standard boilerplate text repeated multiple times, then asking questions about specific clauses in the non-boilerplate sections. The documents were two hundred fifty pages. The actual unique content was thirty pages. The system processed all two hundred fifty pages for every request. The attacker made twenty-three such requests over two weeks. Each request consumed maximum context. The cost per request was twelve times higher than the median user.

Context stuffing also appears in multi-turn conversations where the attacker deliberately creates long conversation histories. They ask trivial questions that generate long responses, building up context. Then they ask the real question, which now includes fifty turns of mostly-irrelevant conversation history in the context window. The system maintains conversation state correctly. The attacker forced it to do so at maximum cost.

## Output Length Maximization Techniques

Some systems charge based on both input and output tokens. An attacker who wants to maximize cost focuses on generating the longest possible outputs. They craft prompts that request comprehensive lists, detailed explanations, exhaustive analysis, or content generation in multiple formats. The goal is not useful output. The goal is maximum token generation.

Prompts like "provide a comprehensive guide covering all aspects of X" generate longer outputs than "summarize X." Prompts like "list all Y with detailed descriptions" generate longer outputs than "list the top five Y." Prompts like "translate this document into ten languages" generate outputs ten times longer than "translate this document." The attacker designs prompts where the task inherently requires long outputs, then ignores most of the output. They are buying tokens, not information.

Output length attacks work because output length is task-dependent, and many legitimate tasks require long outputs. Writing a detailed technical specification is a legitimate use case that generates ten thousand tokens. An attacker who requests a hundred detailed technical specifications is exploiting the same feature at scale. Distinguishing abuse from legitimate usage requires understanding whether the user actually needs the outputs they requested.

Some systems implemented output length limits to defend against this. A maximum of four thousand tokens per response, for example. Attackers adapt by submitting prompts that request batch operations — "analyze these twenty documents" — where each analysis in the batch generates four thousand tokens. The per-response limit is enforced. The total output is eighty thousand tokens. The system followed the rules. The attacker designed the request to maximize total output within the rules.

A content generation platform encountered output maximization attacks in December 2025. Attackers on free trials submitted prompts requesting blog posts on twenty related topics in a single request. Each blog post generated three thousand tokens. The total output was sixty thousand tokens. The per-response limit was not exceeded because the system counted each blog post separately. The attacker extracted sixty thousand tokens from a single prompt. They made fifteen such requests before the trial quota exhausted. The platform had designed free trials expecting users to generate five to ten thousand tokens total. The attacker extracted nine hundred thousand.

## Timing Attacks on Pay-Per-Token Systems

Some AI services bill strictly per token — input tokens plus output tokens, with no monthly quota or subscription. This pricing model creates a different attack surface. The attacker is not trying to exhaust a quota. They are trying to make the victim pay for computation. This applies to compromised accounts, stolen API keys, or scenarios where someone other than the attacker is paying the bill.

Timing attacks on these systems focus on maximizing tokens consumed before detection. The attacker uses a stolen API key and submits high-cost requests as fast as the system allows. They are not concerned with rate limits or quotas — they are racing against detection. The goal is to consume as many tokens as possible, running up the victim's bill, before the victim notices and revokes the key.

A machine learning platform discovered this in October 2025. A stolen API key was used to submit three thousand requests over six hours, each request using maximum context and generating maximum output. The requests came from a datacenter IP and showed obvious automation. The activity charged seventy-three thousand dollars to the victim's account. The victim reported the compromise twelve hours after it started. By then, the damage was done. The platform refunded the charges and implemented anomaly detection on API key usage, but the attacker had already moved to another compromised key.

Timing attacks succeed when detection is slow and billing is immediate. The system charges for every token processed. The attacker knows they will be caught eventually. They maximize damage in the window before detection. Faster detection limits damage but cannot prevent it — the first few expensive requests will always process before any defense can react.

## Monthly Budget Exhaustion Strategies

Organizations that use AI systems often set monthly budgets. Once the budget exhausts, the system either stops processing requests or throttles heavily. An attacker targeting the organization's availability does not need to take the system down directly. They just need to exhaust the monthly budget early in the month, forcing the organization to either go offline or pay overage charges.

This attack works best against organizations with public-facing AI features — customer service bots, content recommendation systems, search interfaces. These systems must remain available to serve legitimate users. An attacker who can exhaust the monthly inference budget in the first week of the month forces a decision: absorb overage costs to stay online, or go offline until the budget resets. Either choice causes pain.

Budget exhaustion attacks combine techniques. The attacker creates many accounts to bypass per-account quotas. They craft expensive prompts to maximize cost per request. They automate to sustain the attack continuously. They time the attack for the beginning of the billing cycle, maximizing the duration the victim faces budget constraints. The attack is not about one expensive request. It is about sustained cost pressure designed to exhaust the budget before legitimate usage would.

A retail company faced this in November 2025. Their AI-powered product recommendation system had a monthly inference budget of thirty-five thousand dollars, sized for expected traffic. In the first nine days of the month, an attacker using two hundred automated accounts submitted recommendation requests at the maximum allowed rate, each request crafted to trigger the most expensive inference path. By day nine, the budget was eighty-seven percent depleted. The company faced a choice: disable recommendations for the rest of the month, or pay overage charges that would quintuple the budget. They chose to keep the system online and absorbed the overage cost. Total bill: one hundred ninety thousand dollars. The attacker's cost: under two thousand dollars in automation infrastructure and account setup.

## Defense Through Segmentation and Isolation

Defending against cost exhaustion requires assuming that some users will optimize for maximum cost extraction. You cannot perfectly prevent this without blocking legitimate high-cost usage. The defense is segmentation — isolate the damage any single account can cause, so that cost exhaustion attacks affect only the attacker's quota and do not cascade to organizational budgets.

Segment user quotas strictly. A free trial user has a hard monthly quota measured in dollars or inference units. When exhausted, they are done. No overages. No flexibility. Paid users have higher quotas but still have hard caps. Even enterprise customers should have guardrails where usage above expected levels triggers alerts and optionally throttles. Segmentation means an attacker who exhausts their quota stops being a problem. They do not continue to drain your operational budget.

Segment tool call budgets separately from inference budgets. External API calls, database queries, and compute-intensive operations should have separate limits. An attacker who tries to generate ten thousand tool calls exhausts the tool call quota without affecting the inference quota. This prevents recursive tool call attacks from cascading into total budget exhaustion.

Segment by usage velocity. Normal users have characteristic usage patterns — periodic activity, human-like timing, varied request types. An attacker trying to maximize cost shows anomalous velocity — sustained maximum rate usage, identical request structures, robotic timing. Velocity-based segmentation identifies these accounts early and imposes stricter limits or mandatory review before allowing continued high-velocity usage.

## Real-Time Cost Attribution and Alerting

You must know what your system costs in real time, attributed to individual users and accounts. This is not a monthly finance exercise. This is a security requirement. An attacker draining your budget is doing damage every minute. Detection after the fact is too late. Detection during the attack limits damage.

Real-time cost attribution means tracking the cost of every request as it processes. Input tokens, output tokens, model used, features enabled, tool calls made — sum these into a cost estimate and attribute it to the user account immediately. Aggregate per user, per hour, per day. Compare to historical patterns and expected costs. Alert when costs exceed thresholds.

A medical AI platform implemented real-time cost tracking in late 2025 after multiple budget exhaustion incidents. They tagged every inference request with user ID, account tier, and estimated cost. They aggregated costs per user per hour and compared to the user's historical hourly average. When any user's hourly cost exceeded five times their average, an alert fired. The system flagged twenty-three anomalous accounts in the first month. Nineteen were confirmed attacks or abuse. Four were legitimate users running unusual batch jobs, who were contacted to confirm the activity was intentional. Detection happened within hours of attack start instead of days.

Real-time alerting requires accepting some false positives. Legitimate users occasionally run expensive operations. The threshold needs to balance sensitivity and specificity. Too sensitive and you alert constantly on normal usage. Too lax and attacks run for hours before triggering alerts. Most systems find that thresholds based on multiples of per-user historical averages work better than fixed thresholds — a user whose normal cost is two dollars per hour triggering an alert at ten dollars per hour, while a power user whose normal cost is fifty dollars per hour triggers at two hundred fifty.

## The Economic Reality of Public AI Systems

Any AI system that allows public access or easy account creation will face cost exhaustion attacks. The economics are too favorable for attackers. The effort to mount an attack is low. The potential damage is high. The attribution is difficult. These factors combine to make cost exhaustion a baseline threat, not an edge case.

You can reduce the frequency and severity through defense-in-depth — strong rate limiting, cost-based quotas, real-time monitoring, behavioral analysis, rapid response to anomalies. You cannot eliminate the threat entirely. Attackers adapt. New techniques emerge. The game is continuous.

What you can do is limit the maximum damage. Ensure no single account, no single attack, no single day can bankrupt your system or force you offline. Build cost controls into the architecture. Monitor costs as a security metric. Respond to anomalies as potential attacks. Treat cost as part of your threat model, not just your budget model.

The next subchapter covers a related but distinct attack vector: token bombs and context window exploits that focus not on exhausting budgets but on breaking system functionality through carefully crafted inputs.

