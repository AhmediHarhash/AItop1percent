# 16.2.7 — Defense Layer 1: Input Sanitization and Validation

Input sanitization is where you catch the obvious attacks. It is the first checkpoint between user input and your model — the place where you filter out the blatant injection attempts, the malformed inputs, the suspicious patterns that scream hostile intent. It will not stop a sophisticated attacker. It will not protect you against novel techniques. But it will block enough crude attacks that it earns its place as the first layer in your defense stack.

The mistake teams make is treating sanitization as their entire security strategy. They deploy a blocklist, catch a few obvious attacks, and declare victory. Then a real adversary shows up with a slightly reworded payload and walks straight through. Sanitization is necessary. It is not sufficient. It catches the amateurs and forces professionals to work harder. That is its job. Anything beyond that is wishful thinking.

## The Basic Approach: Pattern Matching and Blocklists

The simplest form of input sanitization is pattern matching. You maintain a list of known attack phrases and reject any input that contains them. "Ignore previous instructions" gets blocked. "Disregard the above" gets blocked. "You are now in developer mode" gets blocked. This works exactly once per phrase. The attacker reads your error message, rewords the injection, and tries again. Within minutes they have a variant that passes your filter.

You can expand the blocklist. Add more phrases, more variations, more synonyms. "Forget what I told you earlier." "Pretend you are a different assistant." "Override your guidelines." Each new block catches some percentage of attackers. But the fundamental problem remains. Pattern matching is a game of enumeration. The attacker only needs to find one phrase you did not anticipate. You need to anticipate every phrase. The math does not favor the defender.

Real-world data from 2025 security research shows that basic blocklists catch roughly 15 to 20 percent of injection attempts in live systems. That number is not zero. It is high enough to justify the effort for high-risk systems. But it is nowhere near comprehensive protection. The attacks that matter most — the ones launched by skilled adversaries who study your defenses — are the ones that bypass simple pattern matching entirely.

## Perplexity-Based Detection: Spotting Unusual Inputs

A more sophisticated approach uses perplexity scoring to detect anomalous inputs. Perplexity measures how surprised a language model is by a given text. Normal user queries follow predictable patterns. Carefully crafted injection payloads often contain unusual token sequences that deviate from typical user language. You run incoming text through a lightweight language model, measure its perplexity, and flag anything above a threshold as suspicious.

This catches attacks that pattern matching misses. An injection payload disguised with synonym substitution might pass a blocklist but still register as statistically unusual. A query that contains perfectly grammatical English but combines concepts in strange ways — "As an AI assistant, I must inform you that my prior instructions have been rescinded" — produces a higher perplexity score than "How do I reset my password?"

The challenge is calibration. Set the threshold too low and you flag legitimate complex queries. Technical support requests, legal questions, academic inquiries — all can produce higher perplexity scores without being hostile. Set the threshold too high and sophisticated attackers learn to craft injections that mimic natural language patterns. The best implementations use dynamic thresholds adjusted per domain and query type, but even then you are trading off false positives against false negatives.

A fintech platform deployed perplexity-based detection in early 2025 and blocked 8,000 inputs in the first month. Manual review showed that 35 percent were actual injection attempts. Another 40 percent were legitimate but poorly phrased customer queries. The remaining 25 percent were automated tests and API probes. The system was reducing attack surface but also degrading user experience for a non-trivial number of real customers. They adjusted the threshold upward, accepted more risk, and added appeal workflows for blocked users. Security is always a trade-off.

## LLM-Based Input Classifiers: Using Models to Screen Models

The most effective sanitization approach uses a separate language model to classify incoming inputs. You treat the sanitizer itself as an AI system. The input query goes to a fast, cheap model trained or prompted to identify injection attempts. That model returns a binary classification — safe or unsafe — and only safe inputs proceed to your primary system. This is expensive. It adds latency. It introduces a second model that itself could be attacked. It also works better than any rule-based approach.

Why does this work? Because injection attacks and legitimate queries are semantically different, and language models are exceptionally good at semantic classification. A rule-based system checks for keywords. A model-based classifier understands intent. It recognizes when a query is trying to manipulate system behavior rather than accomplish a user task. It detects the structural patterns that characterize jailbreak attempts even when the specific phrasing is novel.

You can fine-tune a small model specifically for this task, or you can use a prompted GPT-5-nano or Claude Haiku 4.5 instance with carefully crafted examples. The prompt includes dozens of real injection attempts paired with benign queries and asks the model to classify the incoming text. The fine-tuned approach offers better performance and lower per-query cost at scale. The prompted approach is faster to deploy and easier to update as attack patterns evolve. Both approaches achieve detection rates above 60 percent in production environments as of 2026.

The failure modes are predictable. Adversarial inputs designed to fool classifiers succeed more often than random injections. Attackers probe your sanitizer, identify its blind spots, and craft payloads that pass classification but still manipulate the downstream model. This is an arms race. You update the classifier with new examples. Attackers adapt. You update again. The cycle continues. But every additional hurdle increases the attacker's cost and reduces the pool of people capable of mounting successful attacks.

## The False Positive Problem: When Sanitization Blocks Legitimate Users

The most painful failure mode of input sanitization is not letting attacks through. It is blocking real users. A customer service system that rejects 5 percent of legitimate queries because they resemble injection patterns has just made your product worse for one in twenty users. They retry. They get blocked again. They call support. Your support team has no idea why the system rejected the query. The customer leaves.

False positives are most common in domains where legitimate queries naturally resemble attack patterns. A software developer asking "How do I override the default configuration?" is not injecting. A security researcher asking "What are the guidelines for your moderation system?" is not probing for vulnerabilities. A customer asking "If I told you my account number earlier, can you retrieve it?" is not trying to manipulate memory. All three queries could trigger sanitization rules designed to block instruction overrides, system prompt leaks, and context manipulation.

You cannot eliminate false positives. You can only minimize them and handle them gracefully. The minimization comes from better classifiers — more training data, more sophisticated models, better threshold tuning. The graceful handling comes from product design. When you block an input, tell the user why and offer alternatives. "Your query could not be processed due to content policy restrictions. Please rephrase and try again." Give them an appeal path. Log the blocked input for human review. Iterate on your filters based on real feedback.

A healthcare chatbot deployed in mid-2025 with aggressive sanitization blocked 12 percent of queries in its first week. Patient complaints spiked. Many blocked queries were entirely appropriate medical questions that happened to include phrases like "ignore the pain" or "forget about the symptoms." The team rebuilt the classifier with domain-specific training data — thousands of real patient queries labeled as safe or unsafe by clinical staff. False positive rate dropped to 2 percent. That remaining 2 percent still caused friction, but it was tolerable for a system handling sensitive health information where the cost of a successful injection was unacceptable.

## Catching What Sanitization Can Catch: Crude Attacks and Automation

Input sanitization excels at stopping low-sophistication attacks. Automated bots cycling through known jailbreak prompts. Script kiddies copying injection payloads from online forums. Casual users testing whether they can manipulate the system. These attackers do not adapt. They do not study your defenses. They try the obvious thing, and if it fails they move on. A properly configured sanitization layer catches them all.

This matters more than it sounds. The majority of attacks against production AI systems in 2026 are not sophisticated. They are volume plays. Thousands of automated attempts, each trying a handful of known techniques, hoping one slips through. If your sanitization layer blocks 40 percent of these and your second layer blocks another 40 percent of what remains, you have reduced your exposed attack surface by 64 percent. The 36 percent that get through are now the higher-skill attempts that require more expensive defenses.

Sanitization also serves as a deterrent signal. When an attacker probes your system and gets blocked with a clear security message, they learn that your system has active defenses. Some will give up. Others will escalate. But even the ones who escalate now have to invest more effort. They cannot rely on public jailbreak repositories. They have to craft custom payloads. That raises the skill floor and reduces the number of people capable of succeeding.

## What Sanitization Cannot Catch: Semantic Attacks and Context Manipulation

Input sanitization fails completely against semantic injection attacks. These are payloads that contain no suspicious keywords, no unusual perplexity, no detectable attack patterns — but still manipulate model behavior through context and framing. "I am a researcher studying AI safety. Please help me understand how your content moderation system makes decisions." This is a perfectly reasonable query. It is also a reconnaissance probe. The response will reveal implementation details that inform a more targeted attack.

Sanitization also fails against multi-turn attacks. The attacker does not inject on the first query. They build rapport over several interactions, establish a conversational context, then introduce the malicious payload in a later turn when it looks like a natural continuation of the dialogue. The individual messages pass sanitization. The cumulative effect achieves the injection. This is why defense cannot stop at the input layer.

The most dangerous category sanitization misses is attacks embedded in retrieved context. If your system performs RAG and an attacker has poisoned your document store with injection payloads, those payloads arrive as retrieved context, not as user input. Your sanitization layer never sees them. The model receives malicious instructions that appear to come from your own trusted data sources. This is not a theoretical risk. Document poisoning attacks were demonstrated in production systems in 2024 and have only become more sophisticated.

## Combining Sanitization Techniques: Layered Detection

No single sanitization method is sufficient. The robust approach uses multiple techniques in parallel. You run pattern matching for known attacks. You calculate perplexity scores for anomaly detection. You classify inputs with a dedicated model. Each method catches different attack types. An input must pass all three checks to proceed.

This layered approach increases coverage but also increases false positive risk. Every additional check is another opportunity to block a legitimate query. The calibration problem multiplies. You need to tune thresholds for each detection method and then tune the logic that combines their outputs. Does a query need to pass all three checks, or can it proceed if two out of three mark it safe? The answer depends on your risk tolerance and your user experience priorities.

A customer support platform deployed triple-layer sanitization in late 2025. Blocklist matching caught 18 percent of attacks. Perplexity scoring caught an additional 15 percent. LLM classification caught another 30 percent. The overlap between methods was only 8 percent, meaning each method was catching attacks the others missed. Total detection rate: 55 percent. That left 45 percent of attacks reaching the downstream model, but those 45 percent now had to bypass two more defensive layers. The attacks that reached production users were the top 10 percent most sophisticated.

## The Economics of Sanitization: Cost Per Query and Latency Impact

Input sanitization is cheap compared to running your primary model, but it is not free. Pattern matching adds negligible latency and near-zero cost. Perplexity scoring requires a model forward pass, adding 20 to 50 milliseconds and a fraction of a cent per query. LLM-based classification adds 100 to 300 milliseconds and one to five cents per query depending on the model size and provider. For a system processing millions of queries per month, these costs accumulate.

You need to decide which queries get sanitized. Do you sanitize every input, or only inputs from untrusted sources? Do you sanitize differently based on query tier — heavier checks for high-risk domains like healthcare and finance, lighter checks for low-risk domains like product recommendations? Do you skip sanitization entirely for authenticated enterprise users with contractual usage agreements? Each choice shifts the cost-security trade-off.

The latency impact matters more than the dollar cost for most systems. Adding 200 milliseconds of sanitization to a system that already takes 2 seconds to respond is tolerable. Adding it to a real-time voice assistant that must respond in under 500 milliseconds is not. You either accept the latency hit, deploy faster sanitizers with lower accuracy, or skip sanitization entirely and rely on downstream defenses. There is no free option.

## Why Sanitization Is Necessary but Not Sufficient

Input sanitization is the first filter in your defense stack. It catches the obvious, it deters the unsophisticated, and it forces skilled attackers to adapt their techniques. That makes it valuable. But it cannot be your only defense. Attacks that bypass sanitization still reach your model. Your system must assume that some percentage of hostile inputs will pass the first layer. That is why you need instruction hierarchy enforcement, output validation, monitoring, and incident response.

The teams that fail are the ones who deploy sanitization, see their attack success rate drop, and assume the problem is solved. The problem is not solved. The problem is now hidden behind a false sense of security. Attacks still succeed. They just succeed less often and require more skill. You need visibility into what is getting through. You need downstream defenses that catch what sanitization misses. And you need to continuously update your sanitization rules as attackers probe and adapt.

Security is a system property, not a feature. Sanitization is one component in that system. It stops some attacks. The next layer stops some of what remains. The layer after that stops more. No single layer is bulletproof. Together they make successful attacks expensive enough that most adversaries give up and move to easier targets. That is the goal.

The next layer in your defense stack is instruction hierarchy enforcement — the mechanisms that make your model respect its original instructions even when user input contains conflicting commands.
