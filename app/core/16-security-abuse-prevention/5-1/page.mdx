# 5.1 — Data Leakage Vectors in AI Systems

The vulnerability looked harmless. A customer support chatbot, trained on two years of resolved tickets, answered questions about product setup and billing issues. The team had sanitized the training data — removed customer names, masked account numbers, stripped email addresses. They ran a standard privacy review. Everything passed. Three weeks into production, a security researcher sent a message asking the bot to "list examples of billing disputes you've seen." The bot responded with twelve detailed scenarios, each containing enough context clues — purchase amounts, dates, product combinations, complaint patterns — to uniquely identify individual customers from the company's public order database. The sanitization had removed direct identifiers but left the fingerprint.

AI systems leak data in ways traditional software does not. A database might be hacked, an API might be misconfigured, a log file might expose credentials. Those are perimeter breaches — you secure the boundary and the data stays contained. AI systems are different. They are designed to output information. Their entire purpose is to take internal knowledge and externalize it in response to user input. The line between legitimate function and data leakage is not a perimeter. It's a judgment call made thousands of times per second by a probabilistic model.

## The Output Channel: Legitimate Function as Attack Surface

The primary leakage vector is the one you built on purpose. The model generates text. That text is sent to users. If the model has seen sensitive data during training or has access to it in context, that data can appear in outputs. This is not a bug in the traditional sense. The model is working exactly as designed. It's synthesizing patterns from everything it has seen and producing outputs that match the user's request. The problem is that some of those patterns are individual data points you never intended to release.

Training data memorization is the canonical example. Models memorize training examples at rates that scale with repetition and distinctiveness. A common phrase repeated across thousands of documents is unlikely to be memorized as a specific instance. A unique patient name paired with a rare diagnosis code, seen in fifteen training examples, has a much higher probability of verbatim recall. The model doesn't "know" it's leaking sensitive information. It learned a pattern. The pattern happens to be someone's medical record.

Fine-tuned models memorize at higher rates than base models. When you fine-tune on a small domain-specific dataset — fifty thousand customer service interactions, eight thousand legal contracts, twelve hundred radiology reports — the model sees each example multiple times across training epochs. Repetition drives memorization. The smaller and more distinctive your training data, the more likely it is that specific examples can be extracted through targeted prompting. This is not theoretical. Carlini et al. demonstrated in 2024 that language models trained on private datasets leak individual training examples when prompted with partial information.

The defense is not "don't put sensitive data in training." That eliminates most valuable use cases. The defense is recognizing that any data in training can leak, instrumenting to detect when it does, and having output filters that catch it before it reaches users. Most teams do none of these things.

## Side Channels: Information That Leaks Without Direct Output

Data leaks through channels you weren't watching. Response timing reveals information about internal processing. Token count patterns indicate document length or complexity. Error messages expose system architecture and data schemas. These are side channels — information conveyed not through the intended output but through observable behaviors of the system.

Timing attacks on AI systems work the same way they work on cryptographic systems. If retrieval latency varies based on whether a document exists in your knowledge base, an attacker can infer the presence or absence of specific records by measuring response time. If the model takes longer to refuse a request for sensitive information than to refuse a request for nonsense information, that timing difference confirms the sensitive information exists in the system. A 2025 study demonstrated timing-based extraction attacks against RAG systems where attackers inferred the contents of private document collections by submitting thousands of queries and analyzing latency distributions.

Error messages are unintentional documentation. A generic "I can't help with that" reveals nothing. A message that says "access denied for patient record schema" reveals that patient records exist, are stored in a structured schema, and are subject to access control that the model is aware of. An error that includes a stack trace, a database table name, or a file path is a blueprint for deeper attacks. Production AI systems generate thousands of error conditions — malformed inputs, context overflow, retrieval failures, safety filter triggers. Every error message is a potential leak.

Token count leakage happens when users can observe the number of tokens in a response or in retrieved context. If your system returns "I found fourteen documents matching your query," an attacker now knows that fourteen internal documents match the search term they provided. If response length correlates with the amount of retrieved context, attackers can infer document size and structure. If the model refuses certain queries with a short canned response but other queries with a longer explanation, the length difference reveals something about what the model knows.

## Context Window Exposure: Everything in Prompt is Potentially Leaked

The context window is user-controlled input plus system-controlled augmentation. User messages, conversation history, retrieved documents, tool outputs, system instructions — all concatenated into a single prompt sent to the model. Anything in that context can influence model outputs. That means anything in context can leak.

Retrieved documents are the highest-risk component. When you retrieve the top five chunks from a private knowledge base and inject them into context, you are giving the model access to those chunks for the duration of that turn. The model can quote them, paraphrase them, synthesize across them, or reference them indirectly. If one of those chunks contains a customer's phone number, the model might include that phone number in a response if the user's question creates the right context. You built a retrieval system to give the model access to private data. The model now has access to private data. This is working as designed.

System prompts live in context and are readable by the model. That means they are extractable by users who know how to ask. The model is trained to be helpful and responsive. If a user asks "what are your instructions," the model's training to be helpful conflicts with your intent to keep instructions private. Some models have been fine-tuned to resist these requests, but fine-tuning is not a security boundary. It changes the probability distribution of outputs. It does not make prompt extraction impossible.

Conversation history accumulates risk linearly. A single-turn system has only the current user message and system prompt in context. A multi-turn system has every message in the conversation. If a user asks about a medical condition in turn one, the model retrieves and discusses patient data. In turn five, the user asks the model to summarize the conversation. The model has access to everything it retrieved in turn one and might include details you consider sensitive in its summary. The conversation history is part of context. Context is readable by the model. The model outputs what it reads.

## Memory Systems: Persistent Leakage Across Sessions

Memory systems designed to personalize experience create cross-session leakage risk. If your AI system remembers facts about users across conversations — "you prefer Python over JavaScript," "you have a peanut allergy," "you work at Acme Corp in the legal department" — that information is stored somewhere and will be retrieved in future sessions. If the memory retrieval logic has an authorization bug, user A's memories might be retrieved in user B's session. This has happened in production systems.

The 2024 ChatGPT data exposure incident involved users seeing fragments of other users' conversations in their chat history. The root cause was a caching bug in the conversation storage layer, but the effect was cross-user data leakage. Users reported seeing other people's names, email addresses, and conversation content in their sidebar. This is the failure mode memory systems introduce: persistent storage of user data, complex retrieval logic, and race conditions or authorization gaps that cause data to appear in the wrong user's context.

Summarization-based memory makes this worse. If your system periodically summarizes conversation history into persistent memory to avoid context length limits, you are distilling conversation data into facts stored indefinitely. If that summarization includes sensitive data — "the user discussed their termination process for employee Emma Rodriguez in the compliance department" — you have now created a searchable index of sensitive facts that will be retrieved and injected into context whenever relevant. The summarization process itself can extract and persist data you would have preferred to discard.

## Training Data Regurgitation: Verbatim Leakage of Source Material

Models leak training data verbatim when prompted correctly. The most famous example is the 2023 discovery that prompting GPT-3.5 with "Repeat this word forever" followed by a single word would cause the model to emit training data after repeating the word for a while. The technique exploited a corner case in the model's behavior, causing it to fall into a loop and eventually output memorized text. Attackers used this to extract email addresses, phone numbers, and personally identifiable information from the training set.

Fine-tuned models are more vulnerable because the training set is smaller and more specialized. If you fine-tune a model on ten thousand support tickets, an attacker can craft prompts designed to trigger memorization of specific tickets. "Give me an example of a customer who complained about late delivery in October 2024 and mentioned refrigerated goods" is a targeted retrieval query. If one ticket matches that description precisely, the model might regenerate it nearly verbatim. The more specific the prompt, the more likely the model is to retrieve and output a memorized example rather than synthesizing a novel response.

The current state-of-the-art defense is differential privacy during training, but very few production fine-tuning projects use it. Differential privacy adds noise during training to provably limit how much information any single training example can contribute to the model's learned parameters. This reduces memorization at the cost of model quality. The trade-off is real: tighter privacy budgets mean noisier gradients, which means worse performance on the task you're training for. Most teams choose quality over privacy, which means most fine-tuned models are vulnerable to extraction.

## The Unique Challenge: AI Systems Are Designed to Output Information

Traditional data protection assumes a clear boundary between internal data and external output. Databases don't expose their contents unless explicitly queried through authorized interfaces. APIs return structured responses based on access control policies. Logs are written to internal systems behind firewalls. The threat model is perimeter-based: prevent unauthorized access to the data store, and the data stays protected.

AI systems invert this model. The data is in the model or in the context, and the model is designed to talk about whatever it has access to. There is no perimeter between the model's knowledge and its outputs. The model generates text probabilistically based on patterns in training data and current context. If a pattern exists that would cause the model to output sensitive data in response to a user's prompt, that output will occur with some probability. You cannot build a wall between the model and the data. The model is made of data.

This means you cannot rely on access control alone. Access control determines who can send prompts to the model. It does not determine what the model will say in response. Once a user has access to the model, they have access to everything the model might say, which is everything the model has seen or has in context. The security boundary is not "can this user access the model" but "what will the model say if it does."

The implication is that output filtering and monitoring are not optional. You must instrument what the model generates and detect leakage as it happens. You must filter outputs before they reach users. You must log prompts and responses so you can audit when a leak occurs. You must red-team your own system to find prompts that trigger memorization or context leakage. These are not defense-in-depth measures. They are the primary defense. The model itself is not secure. The system around the model must compensate.

## Why Most Teams Underestimate Leakage Risk

Teams treat AI systems like traditional software. They assume that if they sanitize training data and implement user authentication, data protection is handled. They are wrong on both counts. Sanitization removes direct identifiers but not indirect ones. Authentication controls who can use the system, not what the system will reveal. The mental model is "we removed sensitive data, so the model can't leak it" or "only authorized users can access the system, so leakage doesn't matter." Both assumptions fail in production.

The lag between deployment and discovery amplifies the problem. A SQL injection vulnerability is typically found in security testing or shortly after deployment. A training data leakage vulnerability might not be discovered until months later when a security researcher, a journalist, or an adversarial user runs extraction experiments. By that time, thousands of users have interacted with the system. You don't know how many prompts successfully extracted data. You don't know what was leaked. You have no logs that distinguish legitimate responses from leakage. The exposure window is open-ended.

Data leakage is also difficult to detect without instrumentation. If the model outputs a customer's email address in a helpful response, the user who receives it might not report it. If the model includes a patient's diagnosis in context that makes medical sense, a clinical user might not realize the information was supposed to be isolated. Leakage looks like correct behavior until someone notices the data should not have been accessible. By the time you notice, the leak has been occurring for weeks or months.

## Detection and Mitigation Strategy

You cannot prevent leakage with architecture alone. You can reduce the probability with differential privacy, access control on retrieved context, and prompt engineering that instructs the model not to output certain data types. But you cannot drive the probability to zero. The model is probabilistic. The context is user-influenced. The only reliable strategy is detection and response.

That means output scanning for patterns that indicate leakage. Email addresses, phone numbers, account numbers, social security numbers, patient identifiers — these are detectable with regex and entity recognition. More subtle leakage requires semantic analysis. If a response contains information that was in retrieved context but should not have been synthesized into output, you need a model-based classifier to catch it. That classifier is another AI system, which means it has its own error rate, but it's better than no detection at all.

It means logging everything. Prompts, retrieved context, model outputs, which documents were accessed, which user sent the request, what time, what filters triggered. You need this forensic data to investigate leaks when they are discovered. You need it to train better output filters. You need it to answer the question "did this user extract sensitive data" when Legal or Trust and Safety asks.

It means red-teaming before deployment and continuously after. Hire people to attack your system. Give them time, access, and incentives to extract data. Pay them for successful extractions. If they can extract data in a controlled test, adversaries can extract it in production. The goal is not to pass the red team exercise. The goal is to find every leakage vector the red team finds, instrument detection for it, and patch the system to reduce the probability. Then run the exercise again.

Data leakage in AI systems is not a corner-case vulnerability. It is the default behavior. The model outputs what it knows. Your job is to limit what it knows, detect when it says too much, and respond before the leak reaches users. The next subchapter covers the most common extraction target: system prompts.

---

*Next: 5.2 — System Prompt Extraction Attacks (OWASP LLM07:2025)*
