# 12.4 â€” Real-Time Alerting and Escalation

The security team got paged at 2:47am. A cascade of alerts flooded the incident channel: credential stuffing detected, prompt injection rate spiked, output toxicity threshold breached, API abuse pattern matched. By the time the on-call engineer triaged the noise, the real attack had been running for eighteen minutes. A targeted extraction campaign pulled proprietary document summaries through carefully crafted prompts that stayed just under individual alert thresholds but formed a clear pattern when viewed together. The attacker knew exactly how the alerting system worked. They designed their attack to exploit its blind spots.

Real-time alerting is not optional for AI systems facing adversarial users. But alerts that fire too often train teams to ignore them. Alerts that fire too slowly let attackers complete their objectives before anyone notices. The difference between effective alerting and security theater comes down to three capabilities: accurate severity classification that matches real business impact, escalation paths that get the right person involved at the right time, and ruthless tuning that eliminates false positives without creating blind spots.

## Severity Classification and Business Impact Mapping

Not all security events deserve the same response. A single prompt injection attempt from an authenticated user is noise. A coordinated campaign from fifty accounts extracting PII across six API endpoints is a crisis. Your alerting system must distinguish between these automatically because humans cannot monitor every event in real time.

**Critical alerts** indicate active compromise or imminent risk of material harm. User data being exfiltrated right now. A privilege escalation exploit being actively used. A coordinated abuse campaign targeting high-value accounts. Rate limiting bypassed at scale. These alerts page whoever is on call immediately, regardless of time or day. The expected time to human acknowledgment is under five minutes. The expected time to containment action is under fifteen minutes.

Define critical based on your threat model from Chapter 1. If your system processes medical records, unauthorized access to patient data is critical. If your system generates code, prompt injections that execute arbitrary commands are critical. If your system handles financial transactions, attempts to manipulate transaction logic are critical. The common thread: active exploitation that could cause regulatory penalties, legal liability, or significant user harm.

**High alerts** indicate suspicious patterns that require investigation within the hour. Repeated prompt injection attempts from the same user. Unusual geographic distribution of API calls from a single account. Output filtering bypassed at rates above historical baseline. A known attack signature detected but contained by existing defenses. These alerts go to a dedicated security channel monitored during business hours and trigger escalation to on-call after thirty minutes of no acknowledgment.

High alerts often reveal reconnaissance activity. An attacker is probing your defenses to find weaknesses. They have not succeeded yet, but they are learning. Your response window is hours, not minutes, but inaction leads to critical alerts tomorrow.

**Medium alerts** indicate policy violations or low-confidence anomalies. A user hitting rate limits repeatedly. Output quality scores dropping for a specific prompt pattern. Unusual latency spikes that could indicate model manipulation attempts. Cost anomalies suggesting wasteful or abusive usage. These alerts aggregate into a daily security digest reviewed by the security team each morning. They inform long-term defense tuning and abuse policy updates.

**Low alerts** are logged but not actively surfaced. Individual prompt rejections. Single-user rate limit hits. Standard output filtering events. These feed your baseline metrics and help you understand normal system behavior, but they do not trigger human intervention unless patterns emerge.

The severity model must be explicit and documented. Every engineer on call should be able to look at an alert and understand why it fired and what actions are expected. Ambiguity in severity leads to inconsistent responses and alert fatigue.

## Escalation Paths and On-Call Design

A critical alert fires. Who gets notified? What do they do first? When do they escalate to the next level? These questions must have answers encoded in your incident management system, not left to individual judgment under pressure.

Your primary on-call rotation handles all critical and high alerts. This is typically a senior engineer or security specialist with access to containment tools: API key revocation, account suspension, rate limit overrides, model rollback, traffic blocking. They acknowledge the alert, assess severity, execute initial containment, and decide whether to escalate. Primary on-call shifts run 24 hours, usually in week-long rotations to maintain context across incident evolution.

Your secondary on-call is the escalation path when primary needs help or when an incident crosses domain boundaries. A sophisticated prompt injection attack might require escalation to the ML team to analyze model behavior. A data exfiltration incident might require escalation to Legal and Trust and Safety. An infrastructure compromise might require escalation to the platform security team. Secondary on-call is often a more senior engineer or the security team lead.

Your executive escalation path activates for incidents with regulatory, legal, or reputational impact. Data breaches requiring notification under GDPR or state privacy laws. Attacks that compromise celebrity or high-value user accounts. Abuse campaigns generating press coverage or regulatory inquiry. The escalation criteria must be explicit: breach of X records, compromise of Y account types, press inquiry received, legal hold issued. The person responsible for executive notification is named and has authority to wake up leadership at 3am.

For AI systems, your escalation paths must account for novel attack types that do not fit traditional incident categories. A jailbreak that bypasses all output filters might not trigger data breach protocols but still requires immediate executive awareness. A model poisoning attack might not show up as a security event at all until users report quality degradation. Your on-call engineers need clear guidance on when AI-specific incidents escalate outside normal security channels.

Document the escalation paths in your incident management system. Use automation to enforce them. If a critical alert is not acknowledged within five minutes, auto-escalate to secondary. If an incident is marked as potential data breach, auto-notify the privacy officer. If output toxicity spikes above the regulatory threshold, auto-escalate to Trust and Safety and Legal simultaneously. Humans under stress make inconsistent decisions. Automated escalation ensures critical steps never get skipped.

## Alert Design and Signal Quality

The quality of your alerts determines whether your team trusts them. Alerts that fire for non-issues train engineers to ignore them. Alerts that miss real attacks train attackers to exploit your blind spots. Achieving high signal quality requires careful threshold tuning, context-aware triggering, and continuous feedback loops.

Start with your detection rules from the previous subchapter. Each rule produces events: prompt injection detected, output filtered, rate limit exceeded, anomaly score above threshold. Not every event becomes an alert. Alerting adds context, aggregation, and severity assessment on top of raw detection.

**Aggregation reduces noise.** A single prompt injection attempt from an authenticated user is not worth paging someone. Ten attempts in five minutes from the same user is worth investigating. Fifty attempts across ten users with similar prompt patterns is worth paging someone immediately. Your alerting logic aggregates events over time windows and across user cohorts to distinguish isolated incidents from coordinated campaigns.

**Context enhances signal.** An API request from an unusual geographic location is mildly interesting. The same request from that location using a credential last seen on a different continent ten minutes ago is highly suspicious. Your alerting system enriches events with user history, account age, past behavior, device fingerprints, and correlated activity from other detection systems. The same underlying event can produce a low alert with no context or a critical alert with full context.

**Thresholds balance sensitivity and specificity.** Set thresholds too low and you page on-call for every minor anomaly. Set them too high and you miss real attacks until material harm occurs. The correct threshold depends on your risk tolerance, team capacity, and baseline noise level. A system serving ten thousand requests per day can investigate more alerts than a system serving ten million requests per day.

Tuning thresholds is an ongoing process. Start conservative: set thresholds high enough that you only fire on incidents you are confident require human intervention. Monitor your miss rate: are real attacks slipping through because thresholds are too high? Monitor your false positive rate: what percentage of alerts turn out to be benign after investigation? Adjust thresholds weekly based on this feedback. The goal is not zero false positives, which is unachievable without missing real attacks. The goal is a false positive rate low enough that your team investigates every alert seriously.

## Alert Fatigue and Trust Erosion

Alert fatigue is the silent killer of security programs. When alerts fire constantly for non-issues, engineers stop investigating them carefully. They skim the details, mark them resolved without deep analysis, and return to their primary work. This is rational behavior when alerts have low signal quality. It is also the behavior that lets real attacks slip through.

A financial services company ran a prompt injection detection system that fired an average of forty alerts per day. Most were false positives: users asking legitimate questions about injection molding, medical injections, or SQL injection tutorials. The security team investigated diligently for the first two weeks. By week three, they were batch-closing alerts without reading the prompts. By week five, a real prompt injection campaign extracted customer financial data over three days before anyone noticed. The alerts had fired. The team had ignored them.

Preventing alert fatigue requires discipline in two directions. First, eliminate false positives aggressively. Every alert that fires for a non-issue erodes trust. Investigate why the false positive occurred. Was the threshold too sensitive? Was the detection rule too broad? Was the context insufficient to distinguish legitimate use from abuse? Fix the root cause, not just the symptom. If a detection rule consistently produces false positives and cannot be tuned to acceptable signal quality, disable the rule. A blind spot is better than a false confidence that makes your team ignore real signals.

Second, validate that your alerts detect real attacks. Run red team exercises where your security team attempts known attack patterns against your production system. Do the alerts fire? Do they fire with the correct severity? Do they fire fast enough to enable containment? If your alerts are silent during simulated attacks, they will be silent during real attacks. Use red teaming to prove your alerting system works before you depend on it under pressure.

Monitor alert closure patterns. If engineers are closing critical alerts in under two minutes without documentation, they are not investigating. If the same engineer closes a hundred alerts in an hour, they are batch-processing without analysis. If alert closure rates spike on Fridays or during incident-heavy weeks, alert fatigue is setting in. These are leading indicators. Address them before they lead to missed attacks.

## Integration with Incident Management Systems

Your alerting system should not live in isolation. It must integrate with your incident management platform so that alerts flow directly into the tools your on-call engineers already use. The most common platforms are PagerDuty, Opsgenie, and VictorOps. The integration pattern is consistent: your detection system sends alerts to the incident management platform, which handles notification, acknowledgment, escalation, and incident lifecycle tracking.

The integration provides several critical capabilities. **Routing** ensures alerts reach the correct on-call engineer based on time, day, and alert type. Security alerts go to the security on-call. Model performance alerts go to the ML on-call. Infrastructure alerts go to the platform on-call. You define routing rules in your incident management platform, and it enforces them automatically.

**Acknowledgment tracking** ensures no alert gets ignored. If the primary on-call does not acknowledge a critical alert within five minutes, the platform auto-escalates to secondary. If neither responds within ten minutes, it escalates to the escalation chain. This is not optional. Unacknowledged alerts mean uncontained incidents.

**Incident context aggregation** pulls together all related alerts, logs, and metrics into a single incident view. When an engineer gets paged for a prompt injection spike, they see the spike in context: associated user accounts, recent prompt patterns, output filtering rates, API traffic distribution. They do not have to hunt across five different dashboards to understand what is happening. The incident management platform aggregates context automatically.

**Postmortem workflows** ensure every significant incident generates a written postmortem within 48 hours. The platform tracks incident duration, time to acknowledgment, time to containment, and time to resolution. It prompts the incident commander to document root cause, contributing factors, and action items. This data feeds your continuous improvement process.

Integrate your alerting system early. If your on-call engineers have to monitor a separate security dashboard instead of receiving alerts through their standard incident management flow, they will miss alerts during high-stress periods when they are responding to other incidents. Security alerts must compete for attention on equal footing with all other production incidents.

## Continuous Tuning and Feedback Loops

Your alerting system degrades over time if not actively maintained. Attack patterns evolve. User behavior shifts. Model updates change output distributions. Thresholds that were well-tuned six months ago produce excessive false positives today. Thresholds that caught attacks last quarter miss new attack variants this quarter.

Establish a weekly alert review process. The security team meets to review the past week's alerts: total volume, severity distribution, false positive rate, time to acknowledgment, time to containment, incidents escalated, and incidents requiring postmortems. They identify trends. If prompt injection alerts doubled this week, why? Is there a new attack campaign, or did a model update increase false positives? If API abuse alerts dropped to zero, is abuse actually down, or did a detection rule break?

Every false positive alert gets a root cause tag. Threshold too sensitive. Detection rule too broad. Insufficient context. Legitimate use case not covered. The team tracks the distribution of root causes over time. If threshold sensitivity is the dominant cause, they run a threshold tuning sprint. If legitimate use cases are the dominant cause, they refine their detection logic to exclude those patterns.

Every missed attack gets a postmortem. How did the attack evade detection? What alerts should have fired but did not? What detection rules need to be added or modified? What thresholds need adjustment? The postmortem produces action items with owners and deadlines. Those action items are tracked to completion in your project management system.

The feedback loop extends to your on-call engineers. After responding to an alert, they document whether it was a true positive, a false positive, or unclear. They note what additional context would have helped them assess severity faster. They suggest threshold adjustments or detection rule refinements. This feedback is the raw material for alert tuning. Without it, your security team is flying blind.

Real-time alerting is not a deploy-and-forget capability. It requires continuous attention, disciplined tuning, and organizational commitment to maintaining signal quality. The alternative is a security team that does not trust its alerts and a system that does not detect attacks until material harm has already occurred.

The alerting system itself becomes a target. Attackers know that overwhelming your alerting with noise or suppressing alerts entirely gives them uncontested time to achieve their objectives. The next subchapter covers how attackers target your monitoring and alerting infrastructure and how to defend it.
