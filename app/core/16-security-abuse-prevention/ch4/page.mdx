# Chapter 4 — Tool Abuse and Privilege Escalation

When you give an AI model tools, you give attackers targets. Every tool is a capability. Every capability is a risk. The model can call APIs, search databases, send emails, modify records, transfer funds, or execute code. If an attacker can manipulate the model into making unauthorized tool calls, they've escalated from "talking to a chatbot" to "controlling your infrastructure." This is not hypothetical. Tool abuse is one of the most dangerous attack patterns in agentic AI systems. The attacker doesn't need direct access to your APIs — they just need to convince the model to call them on their behalf. And if your tool permission system has gaps, the model becomes a universal privilege escalation vector.

This chapter teaches you to think about tools as attack surfaces, design least-privilege architectures, and implement validation layers that catch malicious tool calls even when the model is compromised.

---

- **4-1** — Tools as Attack Surface: Why Capabilities Create Risk
- **4-2** — Unauthorized Tool Calls: Bypassing Permission Systems
- **4-3** — Privilege Escalation Through Tool Chaining
- **4-4** — Side Effect Exploitation: Unintended Consequences of Valid Calls
- **4-5** — Tool Argument Injection: Malicious Parameters
- **4-6** — MCP Server Attacks: Malicious Context Protocol Exploits
- **4-7** — Defense: Least-Privilege Tool Design
- **4-8** — Defense: Tool Call Validation and Sandboxing
- **4-9** — Defense: Human Approval for High-Risk Actions
- **4-10** — Monitoring Tool Usage for Anomalies

---

*The model doesn't understand permission boundaries — it understands natural language instructions, and attackers know exactly what to say.*
