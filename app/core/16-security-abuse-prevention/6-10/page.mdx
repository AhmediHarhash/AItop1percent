# 6.10 — When Extraction Happens: Detection and Response

The security team noticed it at 4:17 AM on a Tuesday. A single API key, tied to a legitimate enterprise account, had made 847,000 queries over the previous nine days. The pattern was methodical: queries cycled through prompt structures, output formats, and edge cases in a way no human user would ever explore. The queries came from fourteen different IP addresses across three continents, all routing through privacy-focused VPN endpoints. By the time the alert triggered, the attacker had already extracted enough query-response pairs to fine-tune a functionally equivalent model. The account was disabled at 4:31 AM. The model was already gone.

This is what extraction looks like when your defenses fail. Not a sudden breach with alarms and dashboards lighting up. A patient, distributed campaign that looks almost legitimate until you aggregate the pattern. The question is not whether extraction can happen — it can, and it will. The question is whether you detect it while it is still stoppable, whether you can prove it happened after the fact, and whether you have a response plan that moves faster than the attacker can disappear.

## The Signal Pattern Before Confirmation

You do not start with certainty. You start with anomalies. A user whose query distribution suddenly shifts. An account that was dormant for six months and is now the third-highest volume requester. A pattern of queries that systematically explores the boundaries of your model's capabilities — testing tone, testing format, testing edge cases — in a way that suggests someone is mapping the decision surface, not solving a business problem.

The first signal is always statistical. Query volume per account per day. Query diversity per account — how many distinct prompt structures, how many output formats, how many domains. Temporal clustering — does the account query in bursts, in regular intervals, or in the continuous slow burn that suggests an automated script with rate-limit evasion logic built in. Geographic dispersion — does the account query from one location or from IP addresses that span five countries in three hours. None of these signals alone proves extraction. All of them together form a pattern.

The second signal is behavioral. Legitimate users have intent. They ask follow-up questions. They refine prompts when outputs are not quite right. They abandon threads that are not working. Extraction queries do not behave this way. They are exhaustive, not iterative. They cycle through templates, not through problem-solving. They do not care if the output is useful — they care if it is different. A legitimate user asks the same question three times because they are unhappy with the phrasing. An extractor asks three hundred variations because they are training a model on the distribution.

The third signal is forensic. If you implemented fingerprinting and watermarking, you now have evidence that travels with the data. You can search for your fingerprints in public model repositories, in competitor products, in models offered on underground markets. This is not speculation — in early 2025, a financial services firm found their proprietary sentiment model's fingerprints embedded in a model being sold on a gray-market API service. The seller had extracted the model over eight months using a network of compromised trial accounts. The fingerprints were the proof that made the cease-and-desist letter enforceable. Without them, the case would have been "we think they copied us" with no evidence.

## Distinguishing Extraction from Legitimate Power Use

Not every high-volume account is an attacker. Legitimate enterprise customers generate millions of queries. Legitimate developers stress-test systems before launch. Legitimate researchers explore model behavior as part of academic work. You need to distinguish extraction from power use, and you need to do it without alienating paying customers or researchers who are operating within your terms of service.

The clearest distinction is intent transparency. A legitimate enterprise customer tells you what they are building. They have account managers. They have contracts. They respond to emails. An extraction campaign hides. The account is registered with disposable email addresses, the billing is prepaid or uses virtual cards, and the account holder never responds to outreach. When you email asking "we noticed unusual query patterns, can you help us understand your use case," a legitimate customer replies. An extractor ghosts.

The second distinction is query structure diversity. Legitimate use has structure that reflects the user's domain. A legal tech company queries about contracts, clauses, and case law. A healthcare company queries about symptoms, diagnoses, and treatment options. The queries cluster around a coherent problem space. Extraction queries do not cluster. They sample uniformly across the entire model capability surface. They ask about medicine, then law, then creative writing, then code generation, then sentiment analysis, all from the same account. No real business operates this way.

The third distinction is response to rate limits and pricing changes. When you throttle an account or increase per-query pricing, a legitimate customer negotiates. They explain why they need the volume and they pay the premium or they adjust their usage pattern to fit within limits. An extraction campaign disappears and reappears under a new account. You see the same query pattern resume from a different API key, often within hours. This is not adaptation — it is evasion.

## Immediate Response When Extraction Is Confirmed

Once you confirm extraction is happening, the response has to be faster than the attacker's next move. The standard playbook: disable the compromised account, revoke API keys, block the IP addresses involved, and alert downstream detection systems to watch for reappearance under different identifiers. This stops the current campaign. It does not stop the attacker from coming back with new accounts, new IPs, and new evasion tactics. You need layers.

First layer: account suspension with evidence preservation. Do not just disable the account — snapshot everything. Query logs, response logs, IP addresses, payment metadata, account registration details, timestamps. This is your evidence chain. If this goes to legal proceedings, you will need to prove who queried what, when, from where, and with what API key. If you delete logs to save storage costs, you lose the ability to prove theft.

Second layer: IP and network-level blocking. Block the IP addresses used in the attack, but do not stop there. Block the ASNs, the hosting providers, the VPN exit nodes. Attackers rotate IPs, but they often reuse infrastructure. If the extraction campaign came through a specific VPN service or a specific cloud provider's IP range, blocking at the network level catches the next attempt even if the account changes.

Third layer: model fingerprint broadcasting. If you watermarked your model, broadcast the fingerprints to partners, to model repositories, to legal teams. The attacker now has a model with your DNA inside it. If they try to deploy it publicly, if they try to sell it, if they try to use it in a product that gets reviewed by a third party — the fingerprints will flag it. This is not theoretical. In late 2024, a generative image model was pulled from a public repository after automated fingerprint scanning detected it contained extracted data from a commercial API. The repository maintainers had no idea. The fingerprints were the smoking gun.

Fourth layer: legal notification. If the account is tied to a real company, send a cease-and-desist. If the extraction was large-scale, file for an emergency injunction to prevent deployment. If the account used stolen payment credentials, report to law enforcement. The legal response is slow, but it creates a record and it puts the attacker on notice that you have evidence and you will pursue it.

## Investigation: Determining What Was Extracted and by Whom

Stopping the attack is step one. Understanding what was taken is step two. The scope of extraction determines your legal strategy, your customer notification obligations, and your assessment of competitive risk. If the attacker extracted a narrow slice of capability — say, only contract analysis queries — your risk is smaller than if they extracted the entire model's distribution across all domains. You need to reconstruct what they took from the query logs.

Start with query volume and coverage. Count the queries. Calculate diversity — how many distinct inputs, how many output domains, how many formatting variations. Estimate the effective dataset size. A hundred thousand diverse query-response pairs is enough to fine-tune a competitive model if the target domain is narrow. A million pairs across all capabilities is enough to replicate general-purpose behavior. The volume tells you whether the attacker got a sample or a full extraction.

Next, reconstruct the sampling strategy. Did they query uniformly across your API surface, or did they focus on specific capabilities? If they focused, you know what they are building. If they sampled uniformly, they are likely training a general-purpose competitor or selling the dataset to a third party. This distinction matters. A competitor extracting your niche capability is a business threat. A data broker extracting everything and selling it on a gray market is a different threat — you need to track where that data appears.

Then, determine attribution. Who did this? If the account used real payment details, you have a name and a billing address. If the account used disposable emails and virtual credit cards, attribution is harder. You rely on IP metadata, on overlaps with other incidents, on infrastructure reuse. You work with threat intelligence firms who track extraction campaigns across multiple victims. You may never get a name, but you can often get an entity — a competitor, a research lab, a known data broker, a nation-state actor. Knowing who did it shapes your response.

Finally, assess downstream exposure. Did the attacker extract just the outputs, or did they also probe your input validation, your rate limits, your error messages, your internal model identifiers? If they probed infrastructure, they may have mapped your system's architecture and found other vulnerabilities. The extraction is one risk. The reconnaissance that enabled it may be a bigger one.

## The Watermark Evidence Chain in Legal Proceedings

If you go to court, you need to prove three things: that your model contains proprietary information, that the attacker's model contains the same proprietary information, and that the only way their model could have obtained that information is through extraction from your system. Watermarks make all three provable.

First proof: your model is watermarked. You demonstrate to the court that you embedded detectable patterns in your model's outputs during training and that these patterns are invisible to users but recoverable through statistical analysis. You show the watermarking method, the embedding key, and the detection threshold. This is not speculative — you have the code, the training logs, the pre-deployment verification that the watermark was successfully embedded. The model is yours, and it is marked.

Second proof: the attacker's model carries the same watermark. You obtain access to their model — through legal discovery, through public deployment, through a third-party security firm that purchased access. You run the watermark detection algorithm. You show that the attacker's model produces outputs with the same statistical signature. The probability of this happening by chance is negligible — watermarks are designed to have false positive rates below one in ten billion. The court does not need to understand the cryptography. The court needs to understand that the signature in their model matches the signature in your model, and that signature is yours.

Third proof: the signature proves origin. You demonstrate that the only way the attacker's model could have this signature is by training on outputs extracted from your model. If they built their model from scratch, it would not have your watermark. If they fine-tuned a different base model using publicly available data, it would not have your watermark. The presence of the watermark proves they trained on your data, extracted without permission, in violation of your terms of service and intellectual property rights.

This evidence chain has been tested. In early 2026, a European court ruled in favor of a model provider whose proprietary sentiment model was extracted and redeployed by a competitor. The watermark evidence was the cornerstone of the case. The competitor claimed they had built their model independently. The watermark proved they had not. The court awarded damages and issued an injunction. The precedent is real.

## Attribution Challenges and What You Can Know

Attribution is the hardest part. You know extraction happened. You know what was taken. You often do not know who took it, especially if the attacker used operational security practices designed to prevent tracing. But you can narrow the field.

If the attacker used legitimate accounts tied to real companies, attribution is straightforward. You have billing records, account registration, contact emails. You know who they are. If the attacker used stolen payment credentials or disposable identities, attribution relies on infrastructure forensics. You trace the IP addresses back to hosting providers, VPN services, Tor exit nodes. You look for overlaps with other incidents. You work with security vendors who track campaigns across victims and identify patterns — the same IP ranges, the same query structures, the same timing intervals.

You can also infer intent from behavior. A competitor extracts specific capabilities — the parts of your model that compete with their product. A research lab extracts uniformly — they want a full model to study, not to deploy. A nation-state actor extracts with patience — the campaign runs for months, the query rate is low, the evasion is sophisticated. A data broker extracts aggressively — they want volume, they do not care about detection, they plan to sell the dataset and disappear. The behavior pattern often tells you what kind of attacker you are dealing with, even if you never learn their name.

In some cases, attribution happens after deployment. The attacker launches a product using your extracted model. Customers notice the similarity. Journalists notice. Competitors notice. The model appears in a public repository or an API marketplace. You get a tip. You investigate, you find your fingerprints, and now you have a target for legal action. Attribution in these cases is delayed, but it is certain.

## Post-Incident Hardening

After an extraction incident, you do not return to the previous security posture. You harden the system against the specific attack vector that succeeded and against variants the attacker might try next.

First, you tighten rate limits and detection thresholds. If the attacker evaded your previous limits by staying below the per-account threshold, you lower the threshold or you add new dimensions — per-IP, per-ASN, per-payment-method. If they evaded by distributing queries across multiple accounts, you implement cross-account pattern detection. You make the next attack more expensive, slower, and more likely to trigger alerts.

Second, you improve account vetting. If the attacker registered with disposable emails and virtual cards, you require stronger identity verification for high-volume accounts. You implement phone verification, business documentation, video calls for enterprise tiers. You do not block anonymous use — you create pricing and access tiers where anonymity costs more and grants less volume. Attackers can still extract, but they pay a premium, and that premium funds better defenses.

Third, you deploy model fingerprinting if you had not already, or you strengthen it if you had. You add more fingerprints, you diversify the embedding methods, you reduce the detection threshold. You make it harder for an attacker to remove the fingerprints without degrading the stolen model's quality. You prepare for the next extraction by making the evidence trail impossible to erase.

Fourth, you build a response playbook. The next extraction incident will happen. You need the playbook written, the roles assigned, the escalation paths defined, the legal contacts ready, the forensic tools deployed. The team that responds at 4:17 AM should not be figuring out who to call and what logs to preserve. They should be executing a plan that was written, reviewed, and tested after the last incident.

## The Reality of Ongoing Risk

Model extraction is not a problem you solve. It is a risk you manage. Every public-facing model is a target. Every API endpoint is a potential leak. The economics of extraction are too favorable — a few hundred dollars in API credits and a few weeks of query time can replicate millions of dollars of training investment. Attackers know this. They will keep trying.

Your job is not to make extraction impossible. Your job is to make extraction expensive, slow, detectable, and prosecutable. You layer defenses — rate limits, behavioral detection, watermarks, fingerprints, account vetting, legal agreements, threat intelligence. You monitor continuously. You respond quickly. You preserve evidence. You pursue legal action when the evidence is strong. You treat extraction as an operational risk that requires operational discipline, not as a one-time security fix.

The teams that manage this risk well do not eliminate extraction. They detect it early, they stop it before the damage scales, they recover evidence, and they make the cost of the next attempt higher. The teams that manage it poorly wake up one day to find their model deployed by a competitor, with no evidence, no attribution, and no recourse. The difference is not luck. It is preparation, detection, and response speed.

The attacker will be patient. You need to be faster. In Chapter 7, we shift to the control layer that determines who even gets the chance to query your model in the first place — identity, access, and entitlements, the front line that decides whether a potential attacker becomes an authenticated user at all.
