# 9.11 — Monitoring Agent Behavior for Anomalies

The security operations dashboard shows green across every metric. API calls within normal range. Tool usage typical. Token consumption steady. Cost within budget. Then, forty-three minutes later, an agent has exfiltrated twelve thousand customer records, initiated fourteen unauthorized database transactions, and attempted to escalate its own privileges six times. Every action was logged. Every action looked superficially normal. Every action was, in aggregate, an attack. The monitoring system saw the trees but missed the forest. It tracked each action in isolation but never analyzed the pattern.

Anomaly detection for agent systems is not optional. It is the difference between catching an attack in progress and discovering it during post-incident forensics. But detecting anomalous agent behavior is harder than detecting anomalous API traffic or anomalous database queries. Agents are supposed to behave unpredictably—that is the entire point of agentic systems. They adapt to context, reason through novel situations, and execute multi-step plans that vary by task. The challenge is distinguishing legitimate adaptation from malicious deviation, intelligent problem-solving from compromised reasoning, normal variation from attack progression.

## Establishing the Baseline: What Normal Looks Like

You cannot detect anomalies without a baseline. The baseline is a statistical model of normal agent behavior, built from historical execution logs and updated continuously as the agent's operating environment evolves. The baseline is not a single number. It is a multidimensional profile that captures typical ranges for dozens of metrics across different task types, user populations, and time periods.

Start with action-level baselines. For each tool the agent can call, calculate typical usage frequencies. If the agent averages three file read operations per task, fifty in a single session is anomalous. If the agent averages zero database writes per task, any write operation is anomalous. If the agent typically makes two external API calls per session and those calls are always to the same approved endpoints, a third call to an unapproved endpoint is anomalous. Action-level baselines are the simplest to compute and the fastest to detect deviations from.

Layer in sequence-level baselines. Agents execute actions in patterns. A customer support agent typically follows a predictable flow: query customer record, retrieve order history, check knowledge base, draft response. A research agent typically queries multiple sources, aggregates results, summarizes findings. These sequences are not rigid—the agent adapts based on what it finds—but they follow recognizable structures. If a customer support agent suddenly skips querying the customer record and jumps straight to executing a refund, that is a sequence anomaly. The individual actions are legitimate, but the order is wrong.

Add session-level baselines. Some metrics only make sense at the session level: total tokens consumed, total tool calls, total API cost, session duration, number of reasoning iterations. A single session that consumes ten times the typical token count is anomalous, even if no individual reasoning step is unusual. A session that runs five times longer than average is anomalous, even if the agent is not technically stuck. Session-level anomalies catch resource exhaustion attacks, infinite loops, and agents that are burning through budgets faster than intended.

Include cross-session baselines. Compare the current session to recent history for the same user, the same task type, or the same time window. If a user's previous twenty sessions all completed within two minutes and the current session has been running for twelve, that is anomalous even if twelve minutes is within the global average. If a task type typically requires four tool calls and the current session has made twenty-three, that is anomalous for this task even if twenty-three is within the global maximum. Cross-session baselines catch targeted attacks—where an attacker compromises a single user's session—and task-specific deviations that global baselines miss.

Build temporal baselines. Agent behavior changes over time as models improve, as tools are added, as user behavior shifts. A baseline calculated from six months of historical data may not reflect current norms. You need rolling baselines that update daily or weekly, weighted toward recent behavior. You also need diurnal baselines—agent behavior during business hours differs from after-hours behavior. An agent making high-volume API calls at 3 AM is more suspicious than the same volume at 3 PM, depending on your user base geography.

The baseline is not static, and it is not universal. You maintain separate baselines per agent type, per task category, per user segment. A high-privilege administrative agent has a different baseline than a low-privilege customer-facing agent. A financial transaction agent has a different baseline than a content moderation agent. Treat baselines as living models, not fixed thresholds.

## Anomaly Indicators: What to Watch

With baselines established, you monitor for deviations. Anomaly indicators are the specific signals that suggest an agent is behaving unusually—not necessarily maliciously, but unusually enough to warrant investigation.

**Tool usage spikes** are the most obvious indicator. If the agent's tool call rate jumps suddenly—from three per session to thirty—something has changed. The agent may be stuck in a retry loop. It may be probing for vulnerabilities. It may be under attacker control and executing a scripted exfiltration sequence. Tool usage spikes are almost always worth investigating. The legitimate explanations are rare: a particularly complex task, a newly enabled tool the agent is exploring, a user with unusual but valid requirements. The illegitimate explanations are common.

**Capability boundary probing** indicates an agent attempting to access tools it should not have. The agent requests a tool outside its approved scope, the request is denied, and the agent tries again with slight variations—different parameters, different phrasing, different reasoning justifications. This is not normal. A well-functioning agent does not repeatedly attempt unauthorized actions. Probing is a sign of either jailbreak or severe model failure. Either way, it warrants immediate review and likely termination.

**Reasoning pattern shifts** are harder to detect but more indicative of compromise. The agent's reasoning style changes mid-session. The tone shifts. The vocabulary changes. The logical flow becomes inconsistent. The agent starts referencing instructions not present in the original task. These shifts indicate prompt injection—the agent has ingested malicious input and is now operating under substitute objectives. Detecting reasoning pattern shifts requires NLP-based anomaly detection: compare the current reasoning trace to historical traces using embedding similarity, sentiment analysis, linguistic fingerprinting. When similarity drops below threshold, flag for review.

**Goal drift** is detected by comparing the agent's current actions to its original instruction. If the agent was asked to summarize a document and is now making database queries, that is drift. If the agent was asked to draft an email and is now accessing financial records, that is drift. Goal drift is measured using semantic similarity between the task instruction and the agent's recent actions. Low similarity indicates the agent has diverged from its objective. Divergence may be legitimate—the agent discovered that completing the task requires additional context—or malicious—the agent has been jailbroken into pursuing a substitute goal. The monitoring system flags drift but defers judgment to human review.

**Token consumption anomalies** indicate reasoning inefficiency or attack. If the agent is generating ten times the typical reasoning tokens per step, it may be stuck in a reasoning loop, it may be processing injected instructions that bloat its context, or it may be deliberately wasting resources as part of a denial-of-service attack. Token consumption anomalies are tracked per step and per session. A single verbose reasoning step is not anomalous. Five consecutive verbose steps is.

**Error rate spikes** suggest the agent is failing repeatedly. Tool calls returning errors, API requests timing out, validation checks failing—these are signs of either environmental failure or agent misuse of tools. A temporary spike may indicate transient infrastructure issues. A sustained spike indicates the agent is executing invalid actions, possibly because it is under attacker control and issuing commands that legitimate usage would never generate.

**Time-to-action deviations** measure how long the agent spends reasoning before acting. A healthy agent reasons briefly—typically ten to thirty seconds—then acts. An agent that reasons for five minutes before each action is either facing an unusually difficult task or stuck in analysis paralysis. An agent that reasons for two seconds and then acts is either facing a trivial task or skipping reasoning entirely, possibly because it has been jailbroken into executing pre-determined actions without deliberation. Track reasoning duration per step and flag outliers.

**Output content anomalies** are detected by scanning the agent's generated outputs for forbidden patterns. If the agent generates text containing PII, credentials, internal system details, or policy-violating content, that output is anomalous. You use the same content filtering and policy violation detection techniques from Section 16 Chapter 4—pattern matching, classifier models, embedding-based similarity to known violation examples. The difference here is that you are scanning the agent's intermediate outputs, not just the final response delivered to users. You catch the violation before it propagates downstream.

## Real-Time Alerting for Suspicious Behavior

Detecting anomalies is only useful if you act on them. Real-time alerting ensures that security teams and automated response systems are notified the moment an anomaly crosses threshold, while the agent is still running and intervention is still possible.

Alerting is tiered by severity. Low-severity anomalies—single tool usage spike, single reasoning pattern deviation—generate informational alerts logged to a monitoring dashboard but do not page humans. These alerts are reviewed during daily security sweeps. Medium-severity anomalies—sustained tool usage spikes, multiple capability boundary probes, moderate goal drift—generate warning alerts that notify the on-call security engineer and trigger automated diagnostics. The agent continues running but is flagged for closer monitoring. High-severity anomalies—successful capability boundary violations, severe reasoning pattern shifts, output content violations—generate critical alerts that page multiple team members, trigger emergency stop procedures, and initiate incident response protocols.

Alerts include full context. The alert message contains the agent session ID, the task instruction, the anomaly type, the specific metrics that crossed threshold, the timestamp of first detection, and a direct link to the full reasoning trace and tool logs. The responder should be able to triage the incident within thirty seconds of receiving the alert. No hunting through logs, no reconstructing context from fragments. Everything needed to make a stop/continue decision is in the alert.

Alerting is integrated with the termination mechanisms from the previous subchapter. Critical alerts can automatically trigger emergency stop if configured to do so. This is appropriate for high-risk agent types—financial transaction agents, administrative access agents, external-facing API agents—where the cost of a false positive is lower than the cost of a missed attack. For lower-risk agents, critical alerts notify humans who decide whether to terminate or continue monitoring.

Alerts are not limited to internal teams. If an agent is operating on behalf of a specific user and exhibits anomalous behavior, the user is notified. The notification is carefully worded to avoid false alarm panic but makes clear that unusual activity was detected and the session has been paused pending review. Users appreciate transparency. They do not appreciate discovering, three weeks later, that their account was used in an attack and no one told them.

## Distinguishing Attack from Malfunction

Not all anomalies are attacks. Many are malfunctions—model errors, tool failures, environmental issues, edge cases the agent was not trained to handle. Distinguishing attack from malfunction is critical. If you treat every malfunction as an attack, you waste security resources, you erode trust in the monitoring system, and you train your team to ignore alerts. If you treat every attack as a malfunction, you miss breaches until they appear in forensics weeks later.

The distinction comes down to intent indicators. Attacks have intent. Malfunctions do not. An agent that repeatedly probes capability boundaries, tries slight variations on denied requests, and attempts to escalate privileges is exhibiting intentional behavior. That behavior may originate from a jailbroken model, from an attacker controlling the input, or from a user deliberately testing limits—but it is intentional. An agent that calls the same tool fifty times because it is stuck in a retry loop is malfunctioning. The behavior is repetitive but not exploratory. There is no escalation, no probing, no variation—just the same failed action repeated.

Intent indicators include escalation patterns, tool sequence sophistication, and reasoning coherence under pressure. An attacker-controlled agent escalates. It starts with low-risk probes—checking what tools are available, testing input validation—then escalates to higher-risk actions once it maps the attack surface. A malfunctioning agent does not escalate. It repeats the same mistake until it hits a resource limit or times out. An attacker-controlled agent uses sophisticated tool sequences—chaining multiple actions to achieve an objective, using early steps to gather information needed for later steps. A malfunctioning agent uses simple, repetitive patterns. An attacker-controlled agent maintains reasoning coherence even under anomaly pressure—the reasoning traces are verbose, detailed, and logically structured, even if the logic is aimed at bypassing guardrails. A malfunctioning agent's reasoning degrades—becomes fragmented, contradictory, nonsensical.

You codify these distinctions into classification rules. When an anomaly is detected, the monitoring system evaluates intent indicators and classifies the anomaly as likely-attack, likely-malfunction, or uncertain. Likely-attack anomalies escalate to security teams. Likely-malfunction anomalies escalate to engineering teams. Uncertain anomalies escalate to both, and humans make the call.

## Integration with Broader Security Monitoring

Agent anomaly detection does not live in isolation. It integrates with your organization's broader security monitoring infrastructure—SIEM systems, intrusion detection, threat intelligence feeds, user behavior analytics. Agent anomalies are correlated with other security signals to build a complete picture of potential threats.

If an agent exhibits anomalous behavior at the same time a user account shows anomalous login activity—login from an unusual location, multiple failed authentication attempts, access at odd hours—that correlation elevates the alert. The user account may be compromised, and the attacker may be using the agent as an attack vector. If an agent accesses an API endpoint that was flagged by threat intelligence as recently exploited in the wild, that correlation elevates the alert. The agent may be attempting a known attack pattern.

Agent logs are exported to your SIEM in real time. Every agent session, every tool call, every reasoning trace is indexed and searchable. Security analysts can query across agent behavior, user behavior, infrastructure logs, and external threat data in a unified interface. They can pivot from an agent anomaly to the user's recent activity to the tools the agent called to the external APIs those tools accessed to the network traffic generated. This cross-domain visibility is what catches sophisticated attacks that no single monitoring system would detect in isolation.

Agent monitoring also feeds user behavior analytics. If a user's agent sessions consistently exhibit low-level anomalies—not severe enough to trigger alerts individually, but consistently higher than peer baselines—that user is flagged for review. The user may be a power user with legitimate advanced needs. Or the user may be an insider threat, slowly escalating their use of agent capabilities to avoid tripping thresholds. UBA systems detect this slow-burn escalation by tracking aggregate behavior over weeks and months, not just individual sessions.

## The Long View: Continuous Baseline Refinement

Monitoring is not a deploy-and-forget system. Baselines drift. Attack patterns evolve. Models improve, and their behavior changes. The monitoring system that works today will miss attacks tomorrow unless it adapts.

You refine baselines continuously. Every week, you regenerate baseline models using the most recent execution logs. You compare current baselines to previous baselines and identify shifts. If the average token consumption per session increased by 20% over the past month, you investigate why. Did the user population change? Did you deploy a more verbose model? Did users start assigning more complex tasks? If the cause is legitimate, you accept the new baseline. If the cause is unclear, you dig deeper.

You also refine detection rules. When an anomaly is flagged and investigated, the outcome is fed back into the monitoring system. If the anomaly was a true positive—an actual attack or policy violation—you record the specific indicators that led to detection and look for similar patterns in historical logs that were missed. If the anomaly was a false positive—a legitimate use case that happened to be unusual—you adjust thresholds or add exceptions to avoid repeated false alarms. The monitoring system learns from every incident.

Red team exercises test the monitoring system's ability to detect sophisticated attacks. Your security team attempts to jailbreak agents, exfiltrate data, escalate privileges—under controlled conditions—and measures how quickly the monitoring system detects and responds. Missed attacks become new detection rules. Slow detections become optimized alert pipelines. The red team is your monitoring system's teacher.

## Chapter Synthesis: Agent Security as Continuous Defense

Agent security is not a checklist. It is a continuous defense posture built on layers: strong isolation, tool-level enforcement, runtime policy checks, safe termination, and relentless anomaly monitoring. Every layer catches what the previous layers miss. Every layer assumes the previous layers will be bypassed.

Agents are powerful because they act with autonomy. They are dangerous for the same reason. Your security architecture must contain that danger without eliminating the autonomy that makes agents useful. You do this through constraints that cannot be reasoned around, monitoring that cannot be evaded, and termination mechanisms that cannot be resisted. The agent operates freely within its sandbox, but the sandbox is unbreakable.

The teams that deploy agents securely are the teams that assume breach from day one. They do not ask "Will an attacker try to jailbreak this agent?" They ask "When an attacker jailbreaks this agent, what is the maximum damage they can cause, and how do we minimize it?" They do not ask "Can we build perfect guardrails?" They ask "When guardrails are bypassed, how quickly do we detect it, and how do we terminate the agent before damage spreads?" This mindset—assume compromise, minimize blast radius, detect fast, respond faster—is what separates secure agent deployments from breaches waiting to happen.

The next chapter shifts from agent-specific threats to attacks that target the entire system: denial of service and economic attacks designed to exhaust resources, inflate costs, and make AI systems operationally unsustainable.
