# 11.7 — Defense: Model Approval and Validation

Most organizations treat model deployment like software deployment: someone chooses a model, writes the integration code, and ships it. By the time security reviews the change, the model is already serving traffic. This is backwards. The model IS the software. It processes user input, generates system behavior, and controls customer-facing logic. Approving a model deployment without security validation is like deploying a binary from an untrusted source without checking what it does.

In January 2026, a healthcare SaaS company discovered their customer service agent was leaking patient data through a side channel. The model, a fine-tuned version downloaded from a public model hub, had been modified by the original uploader to encode sensitive information in response timing patterns. The company had validated the model's accuracy on their test suite. They had checked licensing terms. They had measured latency. They never validated the model weights themselves. The data leak ran for three months before a security researcher disclosed it publicly. The company faced regulatory action under HIPAA and the EU AI Act. The incident could have been prevented with a model approval process that treated weights as untrusted binaries.

**Model approval** is the security gate between model selection and production deployment. It answers four questions before any model serves traffic: Where did this model come from? What does it actually do? Does it contain known vulnerabilities? Has it been red-teamed for our threat model? Without answers to all four, you are deploying blindly.

## The Approval Workflow Before Production

The approval workflow starts the moment someone proposes using a new model. Not when they request deployment — when they first suggest the model. Early involvement means security can influence model selection, not just veto bad choices late in the process. The workflow has five stages: proposal, provenance verification, behavioral validation, red-teaming, and approval decision.

The proposal stage requires documentation. Who is proposing this model and why? What task will it perform? What data will it access? What user-facing behavior will it control? What is the failure impact tier? What alternatives were considered? The proposal forces the team to articulate the security surface before building the integration. Most proposals reveal that the team has not considered data access, failure modes, or attack vectors. Writing the proposal clarifies the security requirements.

Provenance verification answers: where did this model come from, and can we trust the source? For models from major providers — OpenAI, Anthropic, Google, Mistral, Meta — provenance is established through official APIs or documented release channels. For open-weight models from hubs like Hugging Face, provenance requires checking the uploader identity, the model's version history, and whether checksums match official release announcements. For fine-tuned models, provenance means knowing the base model, the training data source, and who performed the fine-tuning. If you cannot verify provenance, you cannot trust the model.

Behavioral validation tests what the model actually does, not what the documentation claims. This involves running the model on a controlled test suite that probes for unexpected behavior: Does it respond to injection prompts? Does it leak training data when queried with canary phrases? Does it refuse tasks it should handle or accept tasks it should refuse? Does its output distribution match the documented capability profile? Behavioral validation is not about accuracy — it is about security surprises. A model that performs well on task metrics but exhibits strange behavior on adversarial inputs is a security risk, not a performance win.

Red-teaming for the specific threat model comes next. Generic red-teaming — testing for jailbreaks, toxic output, or data leaks — is necessary but insufficient. The red-team must probe for threats specific to your application. If the model processes financial transactions, test for output that could manipulate transaction amounts. If it generates code, test for code that creates backdoors. If it summarizes user conversations, test for prompt injection via conversation history. If it controls access decisions, test for authorization bypasses. Red-teaming without a threat model is security theater. Red-teaming tailored to your deployment context is the only validation that matters.

The approval decision synthesizes the previous stages. Provenance verified, behavioral validation clean, red-team findings mitigated — the model is approved for production. Provenance uncertain, behavioral anomalies detected, or unmitigated red-team findings — the model is rejected. Partial approval with mitigations in place — the model is conditionally approved with runtime controls like input filtering, output monitoring, or limited rollout. The approval decision is documented, versioned, and includes the threat model, test results, and any required mitigations. This documentation becomes part of the model's lifecycle record.

## Security Scanning for Models

Security scanning for models is not a solved problem in 2026. There are no widely adopted tools that perform the AI equivalent of static analysis or vulnerability scanning for compiled binaries. But the discipline is emerging. Model scanning tests for three categories of threats: weight-level tampering, behavioral backdoors, and emergent capabilities that create risk.

Weight-level tampering detection looks for evidence that model weights have been modified after training. This is difficult. Legitimate fine-tuning modifies weights. Malicious injection modifies weights. Distinguishing the two requires either cryptographic attestation from the original trainer or statistical analysis of weight distributions. In practice, most organizations rely on provenance verification — trusting the source — rather than weight inspection. But for high-risk deployments, weight analysis is becoming standard. Techniques include comparing weight distributions to baseline models, detecting outlier parameter values, and checking for known patterns associated with backdoor injection. This is forensic-level analysis, not automated scanning. It requires ML security specialists, not general security engineers.

Behavioral backdoors are triggers embedded in models that activate only under specific input conditions. The canonical example is a model that performs normally on all inputs except those containing a specific trigger phrase, which causes the model to execute attacker-controlled behavior. Detecting backdoors requires testing the model on adversarial input distributions designed to surface hidden behaviors. This includes random input fuzzing, known trigger patterns from backdoor research, and domain-specific probes based on your threat model. Behavioral scanning is more accessible than weight analysis — it requires red-team expertise, not ML internals knowledge. But it is also incomplete. You cannot test every possible input. A sophisticated backdoor with a complex trigger condition may evade detection.

Emergent capabilities that create risk are the hardest to scan for. These are not injected by an attacker — they are learned during training. A model trained on code repositories may have learned to generate exploits. A model trained on web text may have learned to generate phishing emails. A model trained on medical records may have memorized patient data. Scanning for emergent risks requires task-specific probes: testing code generation models for exploit output, testing text models for phishing capability, testing summarization models for memorization. The challenge is that emergent capabilities are often contextual. A model may generate exploits when prompted one way but not another. Comprehensive scanning requires broad input coverage and domain expertise.

## Provenance Verification: Knowing Where Models Came From

Provenance verification is the foundation of model security. If you do not know where a model came from, you cannot assess its trustworthiness. Provenance has three layers: source identity, supply chain integrity, and training transparency.

Source identity answers: who trained this model, and how do we know? For commercial models from major labs, identity is verified through official API endpoints or signed releases. You are trusting the organization, not the artifact. For open-weight models, identity verification is harder. Model hubs display uploader accounts, but accounts can be compromised or impersonated. Trusted publishers on Hugging Face or similar platforms have verified badges, but not all legitimate researchers are verified. Cryptographic signatures on model weights would solve this problem — the model file includes a signature from the trainer's private key, verifiable with their public key — but as of early 2026, cryptographic signing is not standard practice across the open-source AI ecosystem. Organizations deploying open-weight models must manually verify identity through cross-referencing official announcements, GitHub repositories, and research papers.

Supply chain integrity answers: was this model tampered with between the source and us? For API-based models, supply chain integrity is guaranteed by HTTPS. For downloaded weights, integrity requires verifying checksums against official sources. Model hubs provide checksums, but you must verify that the checksum itself comes from a trusted source. A tampered model and a tampered checksum defeat verification. Best practice is to verify checksums using multiple independent channels: the model hub page, the official GitHub repository, and announcements on the research team's website or social media. Cryptographic signing would eliminate this problem, but until it is standard, checksum verification across channels is the best available control.

Training transparency answers: what data was this model trained on, and by whom? For commercial models, training transparency is limited. Providers disclose high-level information — web text, code repositories, licensed datasets — but rarely release full dataset manifests or training procedures. You are trusting the provider's security and data governance. For open-source models, transparency varies. Some releases include detailed data cards and training scripts. Others provide no training information at all. For fine-tuned models you control, training transparency is fully within your control — you own the data, the process, and the logs. For third-party fine-tuned models, transparency is negotiable. Enterprise vendors typically provide data lineage and training audit logs under contract. Public fine-tunes rarely include any transparency.

Provenance verification is binary. Either you can verify all three layers — source identity, supply chain integrity, training transparency — or you cannot. Partial provenance is not acceptable for high-risk deployments. If you cannot verify where a model came from, classify it as untrusted and apply compensating controls: sandboxed deployment, input restrictions, output filtering, and continuous monitoring.

## Red-Teaming New Models Before Production

Red-teaming a model before production is different from red-teaming a deployed system. Pre-production red-teaming assumes the model is untrusted. The goal is to find disqualifying vulnerabilities before the model serves real traffic. Post-production red-teaming assumes the model has passed initial validation. The goal is to find edge cases and drifts that emerge under real usage. Both are necessary. This section covers pre-production red-teaming.

Pre-production red-teaming follows the threat model established during the approval proposal stage. If the model processes financial data, the red-team probes for data leaks and transaction manipulation. If it generates customer-facing content, the red-team probes for brand-damaging output and prompt injection. If it controls access decisions, the red-team probes for authorization bypasses. Generic adversarial testing — jailbreaks, toxic content, bias probes — is part of the baseline, but the red-team's focus is application-specific risk.

The red-team operates with two artifacts: the model itself and the integration design. The integration design describes how user input reaches the model, what system context is included, how outputs are processed, and what downstream actions are triggered. Integration design reveals attack surfaces. If user input is concatenated directly into the prompt without sanitization, the model is vulnerable to injection. If the model's output is executed as a command without validation, the model can trigger arbitrary actions. If the system context includes sensitive data, the model can leak it. The red-team attacks the integration as much as the model.

Red-team findings are categorized by severity and exploitability. Critical findings are vulnerabilities that allow data exfiltration, unauthorized access, or safety failures with high probability and low attacker effort. These block deployment. High-severity findings are vulnerabilities that require moderate attacker effort or have lower success rates but still create significant risk. These require mitigation before deployment. Medium findings are edge cases that are difficult to exploit or have limited impact. These are documented and monitored but may not block deployment. Low findings are theoretical risks or minor quality issues. These are tracked for future improvement but do not affect the approval decision.

Mitigations for red-team findings fall into three categories: model replacement, input/output controls, and monitoring. Model replacement means selecting a different model that does not exhibit the vulnerability. This is the cleanest mitigation but often impractical — the vulnerable model may be the only one that meets functional requirements. Input controls include prompt sanitization, input filtering, and instruction reinforcement. Output controls include response filtering, format validation, and downstream validation before executing actions. Monitoring includes logging red-team-style probes in production, tracking output distributions for drift, and alerting on anomalies. Most mitigations combine multiple layers. A single control is rarely sufficient to neutralize a critical finding.

Red-team reports are living documents. They are versioned, stored in the model's lifecycle record, and updated when new findings emerge. When a model is updated — fine-tuned, retrained, or replaced — the red-team report is updated. When the integration changes — new data sources, new downstream actions, new user input surfaces — the red-team report is updated. A red-team report from three months ago is not sufficient validation for today's deployment.

## Model Inventory and Lifecycle Management

By late 2025, the average AI-enabled enterprise was using fourteen different models across production systems. Some were commercial API-backed models. Some were open-weight models deployed on internal infrastructure. Some were fine-tuned versions maintained by individual teams. Some were experimental models in limited rollout. Most organizations could not name all fourteen. They had no central inventory, no ownership records, and no lifecycle tracking. This is model sprawl, and it is a security crisis.

Model inventory is the system of record for all models in use. It includes production models, staging models, and development models. Each entry records: model identifier, source and version, deployment date, owning team, risk tier, approval status, red-team report, and active mitigations. The inventory is not static. It updates when models are deployed, updated, or decommissioned. It integrates with deployment systems so that no model can reach production without an inventory entry. It is the source of truth for compliance audits, security reviews, and incident response.

Lifecycle management governs how models move through stages: proposal, approval, deployment, monitoring, and decommissioning. Proposal and approval were covered earlier. Deployment means the model serves traffic, and monitoring begins. Monitoring includes security-specific telemetry: input patterns that resemble red-team probes, output patterns that deviate from expected distributions, and user reports of anomalous behavior. When monitoring detects issues, the model enters incident response. When monitoring detects drift or degradation, the model enters re-evaluation. When the model is replaced or the feature is removed, the model enters decommissioning.

Decommissioning is often overlooked. Teams replace models without removing the old version. The old model remains deployed, serving a small fraction of traffic through a forgotten code path, or sitting idle in the container registry, available to anyone who knows the endpoint. Decommissioned models must be actively removed: deleted from inference infrastructure, removed from container registries, revoked from API keys, and marked as inactive in the inventory. Decommissioning is not automatic. It requires process enforcement.

Model versioning is critical for both security and compliance. When a model is updated — a new version from the provider, a fine-tuned iteration, or a configuration change — the new version receives a new inventory entry. The old version is either decommissioned or maintained in parallel. Version tracking enables rollback when new versions introduce regressions or vulnerabilities. It enables A/B testing to compare security metrics across versions. It enables compliance reporting when auditors ask which model version was active during a specific time period. Without version tracking, you cannot answer the question: what was running when the incident occurred?

The inventory and lifecycle system is not a security tool in isolation. It is the foundation for every other security control. You cannot red-team a model if you do not know it exists. You cannot enforce approval workflows if teams can bypass the inventory. You cannot respond to a vulnerability disclosure if you do not know which models are affected. The inventory is security infrastructure.

## From Validation to Dependency Review

Model approval validates the model itself. But the model is not the entire attack surface. Every model depends on libraries, frameworks, and toolchains. Your fine-tuning pipeline depends on Hugging Face Transformers, PyTorch, and a dozen Python packages. Your inference service depends on vLLM or Triton or TensorRT, each with its own dependencies. Your data pipeline depends on libraries for parsing, embedding, and vectorization. Each dependency is a potential entry point for supply chain attacks. Model approval secures the model. Dependency review secures everything else.

