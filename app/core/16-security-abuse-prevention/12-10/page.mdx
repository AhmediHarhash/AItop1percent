# 12.10 â€” Remediation: Fixing Vulnerabilities

In November 2025, a financial services company discovered a prompt injection vulnerability that allowed users to extract transaction data from other accounts. The security team documented the issue within four hours. Leadership approved the fix within eight hours. Engineering began work within twelve hours. The patch deployed 71 hours after initial discovery. During those 71 hours, the vulnerability was exploited 847 times. Legal spent the next six months managing the fallout. The problem was not that the company failed to fix the vulnerability. The problem was that they treated remediation as a standard sprint task instead of what it actually was: an active breach in progress.

**Remediation** is the process of fixing security vulnerabilities once they are discovered. It is not debugging. It is not feature work. It is emergency response to an adversarial threat where every hour of delay increases the probability of exploitation. Teams that treat remediation like normal engineering work discover that attackers do not wait for sprint planning. The gap between discovery and fix is not downtime. It is attack surface exposure measured in real time.

## The Severity and Exploitability Matrix

Not every vulnerability requires the same response speed. A theoretical SQL injection in a deprecated admin tool that requires root access and exists behind three layers of authentication is not the same threat as a prompt injection that any authenticated user can trigger from the main interface. The remediation timeline must match the risk. Teams that apply uniform remediation policies waste resources fixing low-risk issues while high-risk vulnerabilities remain exposed.

The first dimension is severity. What happens if this vulnerability is exploited? A vulnerability that leaks personally identifiable information is more severe than one that returns slightly incorrect data. A vulnerability that grants unauthorized access to financial accounts is more severe than one that bypasses rate limits. A vulnerability that allows model manipulation through training data poisoning is more severe than one that increases inference cost. Severity is measured by the impact to users, to the business, and to regulatory compliance. The worst-case outcome defines the severity, not the average outcome.

The second dimension is exploitability. How easy is it for an attacker to exploit this vulnerability? A vulnerability that requires no authentication and can be triggered with a single API call is more exploitable than one that requires insider knowledge, privileged access, and coordination across multiple systems. A vulnerability with public proof-of-concept code is more exploitable than one that exists only in theory. A vulnerability that works reliably is more exploitable than one that succeeds only under rare conditions. Exploitability determines how quickly a vulnerability will be weaponized once discovered.

The matrix creates four quadrants. High severity, high exploitability vulnerabilities require immediate remediation measured in hours, not days. These are the vulnerabilities where detection and remediation must happen in the same operational shift. High severity, low exploitability vulnerabilities require rapid remediation measured in days, not weeks. These are the vulnerabilities where the attack is possible but requires resources or access that most attackers lack. Low severity, high exploitability vulnerabilities require scheduled remediation measured in weeks, prioritized above feature work but not above critical fixes. Low severity, low exploitability vulnerabilities require backlog remediation measured in months, addressed during normal maintenance cycles.

The common mistake is treating every security vulnerability as high priority regardless of actual risk. This creates alert fatigue, burns out security teams, and delays remediation of the vulnerabilities that actually matter. The operational mistake is assuming that low exploitability means low urgency without accounting for the fact that exploitability changes over time. A vulnerability that is hard to exploit today becomes trivial tomorrow when someone publishes a tool. A vulnerability that requires insider knowledge today becomes public knowledge tomorrow when an employee leaves or a researcher discovers the same issue. Remediation timelines must account for the fact that exploitability is not static.

## Patching vs Workaround vs Redesign

Remediation has three approaches. A patch fixes the specific vulnerability without changing the underlying system. A workaround mitigates the vulnerability by adding controls or restrictions that limit exploitation. A redesign eliminates the vulnerability by changing the architecture that made it possible. Each approach has different speed, durability, and risk profiles. The choice depends on the urgency, the root cause, and the system architecture.

Patching is the fastest approach. A prompt injection vulnerability that allows cross-user data access gets patched by adding input validation that blocks the injection pattern. A model that leaks training data through membership inference gets patched by adding differential privacy noise. A RAG system that allows directory traversal gets patched by sanitizing file paths before retrieval. Patching works when the vulnerability is localized, the fix is well understood, and the change can be tested and deployed quickly. The limitation of patching is that it fixes the symptom without addressing the underlying design flaw. A system that requires constant patching is a system with architectural problems.

Workarounds are the middle approach. When a patch is too slow or too risky, a workaround reduces exposure until the real fix can be deployed. A model that generates harmful content gets a workaround in the form of output filtering. A system vulnerable to abuse through high-volume requests gets a workaround in the form of stricter rate limits. An agent that can be tricked into executing dangerous actions gets a workaround in the form of a manual approval step for high-risk operations. Workarounds buy time. They do not eliminate risk. A workaround is a temporary control that acknowledges the vulnerability still exists but reduces its impact or exploitability. The danger of workarounds is that they become permanent. Teams deploy a workaround under time pressure, the immediate crisis passes, and the underlying vulnerability never gets fixed. Two years later, the workaround is still in production and nobody remembers why it exists.

Redesign is the durable approach. A system that requires constant patching gets redesigned to eliminate the attack surface. A model architecture that leaks data through gradient access gets redesigned to use secure aggregation. A prompt structure that is vulnerable to injection gets redesigned to separate instructions from data at the protocol level. Redesign is slower than patching. It requires architectural thinking, cross-team coordination, and careful migration planning. But redesign is the only approach that fixes the root cause. A vulnerability that gets patched will reappear in a different form. A vulnerability that gets redesigned stops being possible.

The decision framework is straightforward. If the vulnerability is critical and exploitable right now, patch immediately and schedule redesign for later. If the vulnerability is severe but not yet exploited, evaluate whether a patch or redesign is faster to deploy safely. If the vulnerability is part of a pattern where the same class of issue keeps reappearing, stop patching and redesign the system. The mistake teams make is choosing patching every time because it feels faster. Fast fixes that do not address root causes create technical debt that accumulates faster than the team can pay it down. Three years of patching usually costs more than one redesign.

## Testing Remediation Before Deployment

A fix that introduces a new vulnerability is worse than no fix at all. A patch that breaks core functionality trades a security issue for a reliability issue. A workaround that degrades performance by 80 percent is not a viable solution. Remediation must be tested with the same rigor as any production change, but with faster timelines and higher scrutiny on unintended consequences. The testing must verify that the fix actually resolves the vulnerability, that it does not introduce new issues, and that it does not break existing functionality.

The first test is exploit verification. The team must demonstrate that the vulnerability can no longer be exploited after the fix is applied. This requires writing an exploit script or test case that reliably triggers the vulnerability in the vulnerable system, then running that same test against the patched system and verifying it fails. If the test still succeeds after the patch, the fix is incomplete. If the team cannot write a test that reliably triggers the vulnerability, they do not understand the vulnerability well enough to fix it. Exploit verification is not optional. It is the only way to confirm that the remediation worked.

The second test is regression testing. The fix must not break existing functionality. A prompt injection patch that blocks legitimate user input is not a fix. A data leakage patch that prevents the model from answering valid questions is not a fix. A rate limiting workaround that blocks legitimate high-volume customers is not a fix. Regression testing verifies that the core product still works after remediation. This requires running the standard test suite, manually testing high-risk user flows, and monitoring key metrics during a staged rollout. The mistake teams make is skipping regression testing under time pressure, deploying a fix that solves the security issue but breaks the product, then rolling back the fix and leaving the vulnerability exposed.

The third test is side effect analysis. The fix must not introduce new vulnerabilities. A patch that blocks one injection vector but opens another is not progress. A workaround that adds authentication but stores credentials in plain text is not progress. A redesign that eliminates data leakage but introduces race conditions is not progress. Side effect analysis requires thinking like an attacker. If this fix closes one door, what other doors did it open? If this workaround adds a control, can that control be bypassed? If this redesign changes the data flow, what new attack surfaces did it create? Security teams that focus only on the known vulnerability miss the new vulnerabilities created by the fix itself.

The testing timeline must match the remediation urgency. A critical vulnerability that requires patching within 24 hours gets abbreviated testing that focuses on exploit verification and basic functionality, followed by a phased rollout with intensive monitoring. A high-priority vulnerability that allows a few days for remediation gets full regression testing before deployment. A scheduled fix that is not yet exploited gets the standard testing cycle. The balance is risk. Deploying an untested fix is risky. Delaying a critical security patch to run two weeks of QA is riskier. The decision depends on whether the vulnerability is being actively exploited right now.

## Regression Prevention for Fixed Vulnerabilities

Fixing a vulnerability once is not enough. The same class of vulnerability will reappear unless the team builds defenses that prevent recurrence. Regression prevention is the process of ensuring that fixed vulnerabilities stay fixed and similar vulnerabilities never get introduced. This requires adding tests, updating development practices, and embedding security checks into the deployment pipeline. Teams that skip regression prevention end up patching the same vulnerability every six months.

The first layer is adversarial test coverage. Every fixed vulnerability becomes a permanent test case. The prompt injection that was patched in January gets added to the security test suite as a test that must fail. The data leakage vulnerability that was fixed in March gets added as a test that verifies output does not contain training data identifiers. The abuse pattern that was blocked in May gets added as a test that verifies the blocking logic still works. These tests run on every deployment. If a code change reintroduces the vulnerability, the test catches it before production. Adversarial test coverage turns past failures into permanent guardrails.

The second layer is pattern detection in code review. If a vulnerability was caused by unsanitized user input, code review must now check for unsanitized user input everywhere. If a vulnerability was caused by storing sensitive data in logs, code review must now check for sensitive data in logs. If a vulnerability was caused by model outputs being trusted without validation, code review must now check for unvalidated model outputs. Pattern detection does not require manual inspection of every line. It requires automated tooling that flags high-risk patterns and requires explicit justification or security review before merging. The tooling learns from past vulnerabilities.

The third layer is architectural guardrails. If a class of vulnerabilities keeps recurring, the architecture must change to make that class impossible. If prompt injection keeps happening because user input and system instructions are concatenated together, the architecture must separate them at the protocol level. If data leakage keeps happening because model outputs are not filtered, the architecture must require filtering by default. If abuse keeps happening because rate limits are applied inconsistently, the architecture must enforce rate limits at the infrastructure layer where application code cannot bypass them. Guardrails shift the burden from developers remembering to do the right thing to the system making the wrong thing impossible.

The fourth layer is security training based on real incidents. Every fixed vulnerability becomes a training case. The team reviews what went wrong, why the vulnerability was not caught earlier, and what changes to process or tooling would have prevented it. Training that uses real examples from the team's own codebase is more effective than generic security training that feels theoretical. Developers learn to recognize patterns when they see the actual damage those patterns caused in their own system. The training happens close to the incident, when the memory is fresh and the motivation to prevent recurrence is high.

The failure mode is fixing vulnerabilities reactively without building systems that prevent recurrence. A team that patches 40 vulnerabilities in a year but does not add regression tests, update review processes, or change architecture will patch 40 more vulnerabilities next year. Regression prevention turns remediation from an infinite loop into a learning process that reduces the rate of future vulnerabilities.

## Tracking Remediation to Completion

A vulnerability is not remediated when a fix is deployed. It is remediated when the fix is verified in production, monitoring confirms the attack surface is eliminated, and all affected systems are updated. Tracking remediation to completion requires visibility into the full lifecycle: discovery, prioritization, development, testing, deployment, verification, and closure. Teams that lose track of remediation timelines discover that critical vulnerabilities linger in production for months because someone assumed someone else was handling it.

The tracking system must capture the full context. When was the vulnerability discovered? Who discovered it? What is the severity and exploitability assessment? What systems are affected? What is the assigned priority and target remediation date? Who is responsible for developing the fix? What is the testing and deployment plan? When was the fix deployed? How was it verified? The tracking system is not a spreadsheet. It is an operational command center that shows the security team what vulnerabilities are open, what their status is, and whether they are on track to meet remediation timelines.

The visibility must extend beyond the security team. Engineering leadership must see the open vulnerability count and the remediation velocity. Product must see how vulnerabilities impact feature priorities. Executive leadership must see the aggregate risk and whether remediation timelines align with regulatory requirements. Visibility creates accountability. A vulnerability that is tracked in a ticket that only the security team can see stays open longer than a vulnerability that appears on the executive dashboard with a countdown timer showing days until the regulatory deadline.

The process must prevent vulnerabilities from falling through gaps. A vulnerability discovered by an external researcher must be logged, acknowledged, and tracked from the moment the report arrives. A vulnerability discovered in a red team exercise must be logged and tracked even if it is theoretical. A vulnerability discovered through automated scanning must be triaged and tracked even if it is low priority. Every discovered vulnerability gets a tracking ticket. The ticket does not close until verification confirms the fix is working in production. The common failure is discovering a vulnerability, discussing it in Slack, assigning it to someone, and then losing track of it when that person goes on vacation or changes teams.

The metrics matter. The security team must track mean time to remediation by severity level. Critical vulnerabilities should be fixed within 24 to 72 hours. High-priority vulnerabilities should be fixed within one to two weeks. Medium-priority vulnerabilities should be fixed within 30 to 60 days. Low-priority vulnerabilities should be fixed within 90 to 180 days. If actual remediation times consistently exceed targets, either the prioritization framework is wrong or the team lacks the resources to respond at the required speed. The metrics reveal whether the remediation process is working or whether vulnerabilities are accumulating faster than the team can address them.

Closure requires documentation. What was the vulnerability? What was the root cause? What was the fix? How was it tested? When was it deployed? Was there any evidence of exploitation before the fix? What changes to process or tooling will prevent recurrence? The documentation serves two purposes. It creates an institutional knowledge base that helps future responders recognize similar issues faster. And it provides the evidence needed for compliance audits, legal inquiries, and post-incident reviews. A vulnerability that gets fixed but not documented is a vulnerability that will reappear.

## Building Remediation Muscle

Remediation is a capability that must be practiced. Teams that only remediate vulnerabilities when they are discovered under crisis conditions will be slow, uncoordinated, and prone to mistakes. Teams that practice remediation through simulated incidents, scheduled drills, and regular patching cycles build the muscle memory needed to respond quickly when a real critical vulnerability appears. The practice must cover the full workflow: discovery, assessment, prioritization, fix development, testing, deployment, and verification. A team that can execute this workflow in four hours during a drill can execute it in four hours during a real incident. A team that has never practiced will take four days.

The drills must be realistic. Simulate the discovery of a critical prompt injection vulnerability at 6pm on a Friday. Simulate the discovery of a data leakage issue two days before a major product launch. Simulate the discovery of a compliance violation one week before an audit. The drill must include the communication overhead, the cross-team coordination, the testing under time pressure, and the decision-making under uncertainty. The goal is not to test whether the team can write a patch. The goal is to test whether the team can execute the full remediation workflow under realistic constraints.

The practice must include post-drill reviews. What went well? What took longer than expected? Where did communication break down? What tools or processes would have made the response faster? The reviews turn drills into learning. A team that runs quarterly remediation drills and reviews learns faster than a team that waits for real incidents to reveal their gaps. The drills do not prevent all mistakes. They reduce the mistakes that happen during actual remediation when the stakes are real.

Remediation is not a one-time event. It is a continuous discipline. The attacker is always probing. The system is always evolving. New vulnerabilities appear. Old vulnerabilities resurface. The team that treats remediation as a reactive process will always be behind. The team that treats remediation as a core capability, practices it regularly, tracks it rigorously, and learns from every incident builds the speed and coordination needed to stay ahead of the threat.

The next challenge is not just fixing vulnerabilities. It is communicating about them. When a security incident happens, who gets told, when do they get told, and what do they get told? Disclosure is where technical remediation meets organizational accountability, legal risk, and customer trust.
