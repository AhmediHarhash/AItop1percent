# 12.3 — Abuse Signal Correlation Across Events

The attacker never triggered a single alert. Every prompt was within normal length limits. Every request stayed within rate limits. Every session looked like typical user behavior when analyzed in isolation. The reconnaissance phase took three days: eight users across six sessions asking questions about system capabilities, testing edge cases, probing error messages. Each user asked fewer than twenty questions. Each session lasted under an hour. Each question looked reasonable. The exploitation phase took two days: four of those users submitting carefully crafted prompts that manipulated conversation context without using known injection keywords. Each manipulation succeeded at low frequency — one success per user per session, spaced hours apart. The exfiltration phase took one day: the compromised sessions requesting data exports through legitimate tools, staying within each user's normal data access patterns but collectively extracting the entire customer database. Total attack duration: six days. Individual events flagged as anomalous: zero. Attack detected: after exfiltration was complete, when business intelligence noticed an unusual pattern in data export logs.

Single-event detection misses sophisticated attacks. Attackers understand your detection thresholds and design campaigns to stay below them. They spread activity across multiple accounts, multiple sessions, multiple time windows. They use legitimate features in combinations that were never intended. They exploit the fact that your anomaly detectors analyze each event independently without correlating activity across the attack surface. You need correlation: the ability to connect events separated by time, users, and feature boundaries, reconstructing attack campaigns from fragments that look benign in isolation.

## Why Single Signals Miss Coordinated Campaigns

Your anomaly detection systems see events: a user submitted a prompt, the prompt had X tokens, the response cost Y dollars, the session lasted Z minutes. Each event is analyzed independently. Is this prompt anomalous for this user? Is this cost anomalous for this request type? Is this session length anomalous for this time of day? If all dimensions fall within normal ranges, the event is classified as benign. The system moves on. The attacker moves forward.

The attack lives in the relationships between events. The reconnaissance prompts from three different users asked complementary questions: one user probed authentication boundaries, another tested data access controls, a third enumerated available tools. Individually, each prompt looked like a legitimate user exploring system capabilities. Collectively, they mapped the complete attack surface. No single-event detector flags this because each detector sees only one event. Correlation sees all three events, notices they occurred within the same week from accounts created on the same day, and recognizes the pattern as coordinated reconnaissance.

Some attacks require multi-session state. A prompt injection plants malicious instructions in conversation history during session one. Session two, hours later, activates those instructions without submitting any new suspicious prompts. Single-event detection never connects the two sessions. Correlation tracks conversation history across sessions for the same user, notices that session two referenced context from session one, and flags the combination for review.

Some attacks distribute exfiltration across users. A single user requesting the entire customer database triggers immediate alerts. Ten users each requesting one-tenth of the database, with requests spaced across a week, evade per-user anomaly detection. Correlation analyzes data access patterns across all users, identifies that the combined access reconstructs sensitive datasets, and flags the coordinated exfiltration.

Sophisticated attackers optimize for single-event evasion because they know that is where most detection systems stop. They study your public documentation to understand what features exist. They probe your system gently to identify rate limits and anomaly thresholds. They design attacks that never trigger individual detectors but succeed through coordination across accounts and time. Your defense must operate at the same level: correlate across events to see the campaign the attacker designed.

## Correlation Dimensions: Users, Sessions, and Time Windows

**User-to-user correlation** identifies coordinated abuse across multiple accounts. Track relationships between users: accounts created at similar times, accounts that access the system from similar IP addresses or geolocations, accounts that exhibit similar behavior patterns despite no apparent connection. When multiple seemingly unrelated accounts all start exhibiting minor anomalies simultaneously, the accounts may be controlled by the same attacker. Build a graph of user relationships: shared IP addresses, shared behavior fingerprints, shared access times, shared prompt patterns. Clusters in this graph represent abuse rings.

**Session-to-session correlation** reconstructs attack sequences across time. A user's session from Monday that probed system capabilities connects to their session from Wednesday that exploited discovered vulnerabilities. Track session metadata: conversation history references, tool usage progression, topic evolution. When a session builds on knowledge obtained in a previous session in a way that suggests intentional progression rather than organic learning, flag the sequence for investigation. Attackers often pause between reconnaissance and exploitation to analyze what they learned. Session correlation bridges those pauses.

**Time window correlation** detects bursts of suspicious activity. A single user asking about system internals is a low-severity anomaly. Twenty users asking about system internals within the same hour is a coordinated campaign. Aggregate anomaly signals across fixed time windows: hourly, daily, weekly. Calculate how many users triggered each type of anomaly in each window. Flag time windows where anomaly rates spike beyond baseline. These spikes represent either external events driving legitimate unusual behavior or coordinated attacks.

**Feature-to-feature correlation** connects abuse across different parts of your system. An attacker who discovers a weakness in your RAG retrieval component exploits it to access data they should not have, then uses a separate reporting feature to export that data. The retrieval anomaly and the export anomaly occur minutes apart, from the same user. Individually, each might fall below alert thresholds. Together, they represent a complete attack chain. Correlate tool usage sequences: when unusual retrieval is followed by unusual export, investigate both events as a single incident.

**Cross-account correlation** exposes privilege escalation and lateral movement. An attacker compromises a low-privilege account, uses it to gather information, then pivots to a higher-privilege account for exploitation. The low-privilege account exhibits reconnaissance patterns: enumeration queries, error message probing, permission testing. The high-privilege account exhibits exploitation patterns: data access, configuration changes, administrative actions. The accounts appear unrelated in single-event analysis. Correlation reveals that both accounts accessed from the same IP range within the same week, both asked similar questions about system architecture, and the high-privilege exploitation began hours after the low-privilege reconnaissance completed.

Correlation requires storing event relationships, not just individual events. Your logging infrastructure must support queries across users, across sessions, across time windows. Graph databases excel at this workload: represent users and sessions as nodes, represent relationships like "same IP," "sequential sessions," "similar prompts" as edges, then query the graph for suspicious patterns. Traditional relational databases struggle with multi-hop relationship queries. Choose your storage technology based on correlation requirements, not just log volume.

## Attack Pattern Recognition Through Multi-Event Analysis

**Known attack patterns** are sequences of events that match previously observed campaigns. An attacker reconnaissance pattern: enumerate tools, test each tool with minimal inputs, probe error conditions, then execute targeted attacks using discovered vulnerabilities. Encode this pattern as a finite state machine: state one is tool enumeration, state two is tool testing, state three is error probing, state four is exploitation. As events arrive, track which state each user's activity matches. When a user progresses through all four states, flag them as executing the reconnaissance pattern. Pattern matching catches attackers following established playbooks.

**Deviation from workflow patterns** detects abuse of legitimate features. Most users follow predictable workflows: search for information, read results, ask follow-up questions, complete task. Attackers misuse workflows: search for administrative data they should not access, read error messages to learn system internals, ask questions designed to manipulate AI behavior, chain legitimate features in illegitimate sequences. Model typical workflows as state machines. Measure how closely each session adheres to expected workflow states. Sessions that deviate significantly are either power users discovering new ways to use your system or attackers exploiting it. Both require investigation.

**Frequency and timing patterns** expose automation and scripting. Humans have irregular timing: requests arrive at varying intervals, sessions have pauses for reading and thinking, activity follows diurnal patterns. Bots have regular timing: requests arrive at fixed intervals, sessions have no pauses, activity ignores time-of-day patterns. Measure inter-request timing distributions for each user. Flag users whose timing is suspiciously regular. Attackers using scripts to test injection variants or scrape data often fail to add human-like timing jitter.

**Data access patterns** reveal exfiltration campaigns. Legitimate users access data relevant to their current task: a support agent pulls up the customer they are speaking with, a sales rep views accounts in their territory, an analyst queries data for their current report. Attackers access data systematically: alphabetical iteration through customer records, sequential database table enumeration, comprehensive export requests. Track data access sequences. Flag users accessing data in patterns that suggest systematic enumeration rather than task-driven need.

**Cost and resource patterns** across events expose denial-of-wallet attacks. An attacker distributes expensive requests across multiple accounts to evade per-user cost limits. Aggregate costs across related accounts: accounts from the same IP range, accounts created in the same time window, accounts exhibiting similar behavior. When aggregate cost spikes even though individual account costs remain within normal bounds, you have detected distributed abuse. The attacker hoped you would only monitor per-user costs. Correlation sees the campaign-level expense.

Pattern recognition requires both rule-based and learned components. Rule-based detection encodes known attack sequences: if event A, then event B within time window T, then event C from related user U, then flag as attack pattern P. This catches attackers following playbooks. Learned pattern detection uses machine learning to identify sequences that correlate with confirmed attacks: train models on historical incident data, extract features from event sequences, classify new sequences as benign or suspicious. This catches novel attack variations that rule-based systems miss.

## Building Attack Graphs from Event Streams

An **attack graph** represents entities — users, sessions, prompts, tool calls, data objects — as nodes and relationships — submitted by, part of, accessed, invoked — as edges. When an event arrives, add it to the graph: create nodes for the user, session, and prompt, create edges representing their relationships, create edges to any tools invoked or data accessed. The graph accumulates all activity over time. Attacks manifest as subgraphs with suspicious topology: unusual connectivity patterns, unexpected node degrees, anomalous path lengths.

Run graph queries to detect attack patterns. **Bipartite clustering** finds coordinated abuse: a set of users all accessing a common set of data resources in a short time window creates a dense bipartite subgraph. This topology is rare in legitimate use — users typically access disjoint data scoped to their responsibilities. **Path analysis** reconstructs attack chains: trace paths from reconnaissance events through exploitation events to exfiltration events. If a path exists connecting low-privilege enumeration to high-privilege data export through intermediate privilege escalation steps, you have mapped the full attack campaign.

**Community detection algorithms** identify abuse rings. Apply clustering algorithms to the user relationship graph: users connected by shared IPs, shared behavior, shared timing patterns. Communities represent groups of users with unusual coordination. Most legitimate users form no communities — they are isolated nodes. Attackers using multiple accounts form tight communities. Flag communities for investigation.

**Centrality metrics** expose high-risk nodes. Calculate degree centrality, betweenness centrality, and eigenvector centrality for user nodes. Users with anomalously high centrality are either power users or compromised accounts being used as attack infrastructure. A user node with edges to hundreds of distinct data resources when typical users connect to dozens is central to the graph in ways that warrant investigation. Centrality metrics surface the nodes attackers rely on for campaigns.

Graph-based correlation scales better than exhaustive event-pair comparison. With N events, comparing all pairs requires N-squared comparisons — computationally infeasible at scale. Graph algorithms process the same data in near-linear time: insert events into the graph as they arrive, run periodic graph queries to detect suspicious subgraphs, update node and edge properties incrementally. The graph accumulates historical context without reprocessing old events.

The graph must be pruned to prevent unbounded growth. Decay edge weights over time: recent connections have higher weight than old connections. Remove edges that fall below a weight threshold. Remove nodes representing events older than your retention window. Pruning balances memory requirements with historical visibility. Attacks that take months to unfold require long retention. Attacks that complete within days allow aggressive pruning.

## User Behavior Profiling Without Privacy Violations

Correlation requires building profiles of user behavior: what each user typically does, what patterns they follow, what data they access. This profiling must respect privacy. Users reasonably expect that their interactions with AI systems are not being surveilled beyond what is necessary for security. Legal frameworks in many jurisdictions require that profiling be proportionate, transparent, and limited to legitimate security purposes.

**Aggregate behavior profiles** without storing individual prompts permanently. Track statistical summaries: this user typically submits N requests per day, uses features A and B frequently, accesses data categories X and Y. Delete the raw prompts after summary statistics are updated. Summaries provide enough signal for anomaly detection without retaining the full content of every interaction indefinitely. This approach balances security monitoring with data minimization principles.

**Anonymize logs** after a retention window. Keep identifiable logs for thirty days to support incident investigation. After thirty days, anonymize: remove user identifiers, remove IP addresses, keep only the data necessary for long-term pattern analysis. Anonymized logs support detection model training and security research without perpetual surveillance. If an attack is discovered months later and anonymized logs prevent full forensic investigation, that is an acceptable trade-off to protect user privacy.

**Limit access to profile data**. Only security teams responsible for threat detection should access user behavior profiles. Implement strict access controls, audit all access, and prohibit using profiling data for non-security purposes. User profiling for security must never leak into product analytics, marketing optimization, or performance reviews. Separation of concerns protects user privacy and maintains trust.

**Be transparent about profiling**. Document in your privacy policy that AI system interactions are monitored for security purposes, that behavior profiles are built to detect abuse, and that profiles are used solely for threat detection. Users have a right to know they are being monitored. Transparency builds trust. Covert surveillance, even for legitimate security purposes, erodes user confidence when inevitably disclosed.

Some jurisdictions require that profiling decisions — such as blocking a user based on behavior anomalies — be explainable and contestable. Build audit trails: when a user is blocked, record which correlation rules triggered, which events contributed to the decision, what the anomaly scores were. Users who believe they were incorrectly flagged can request review. Human investigators examine the evidence, override automated decisions if appropriate, and update detection rules to prevent similar false positives.

The tension is real: effective security monitoring requires visibility into user behavior, but comprehensive visibility creates privacy risks. Balance through proportionality: collect only what is necessary for threat detection, retain only as long as needed for investigation, restrict access to security teams, and provide transparency and appeal mechanisms. This is not a pure technical problem — it is a policy and governance problem that technical systems implement.

The next subchapter covers alerting and escalation: how to route detected attacks to security teams, how to triage alert severity, how to integrate AI security monitoring with existing SOC operations, and how to ensure that alerts drive timely response before attacks cause damage.

