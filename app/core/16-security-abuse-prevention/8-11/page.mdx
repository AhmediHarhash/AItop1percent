# 8.11 — Secure RAG Architecture Patterns

RAG security is not a checklist. It is a system property. You do not secure a RAG pipeline by adding input validation to the ingestion stage, then calling it done. You secure it by building defense-in-depth across every component — ingestion, indexing, retrieval, generation, and output — such that a breach at one layer does not compromise the entire system. An attacker who poisons the knowledge base cannot control the retrieval ranking. An attacker who manipulates retrieval cannot bypass the output filter. An attacker who finds one vulnerability finds a contained failure, not a cascading collapse. That is what secure architecture means. Every component enforces its own security properties. Every boundary is validated. Every decision is auditable. The system resists attacks not because it is perfect, but because it is layered.

## The Secure RAG Pipeline: Five Stages, Five Defenses

A RAG system is a pipeline: documents enter, embeddings are created, queries trigger retrieval, retrieved content is passed to the model, outputs are generated and returned to the user. Each stage is an attack surface. Each stage needs its own defenses. The secure RAG pipeline treats each stage as a trust boundary that must be validated before data crosses it.

**Ingestion** is where external content enters the knowledge base. The defense is validation: provenance checks, content filtering, policy enforcement, and malware scanning. Every document is tagged with its source, its classification, and its approval status. Documents that fail validation are quarantined, not indexed. Documents that pass are versioned and logged. The output of ingestion is a set of trusted, attributed, policy-compliant documents ready for indexing. The security property: nothing enters the knowledge base without documented provenance and policy approval.

**Indexing** is where documents become embeddings and metadata becomes searchable. The defense is isolation: embeddings are generated in a sandboxed environment, embeddings are validated for semantic coherence, and documents are tagged with retrieval-time access controls. Poisoned embeddings — outliers, adversarial perturbations, clustering anomalies — are flagged and reviewed before they are added to the index. The output of indexing is a vector store where every embedding has a traceable lineage back to the source document. The security property: the index reflects the content of the documents, not an attacker's manipulation of the embedding space.

**Retrieval** is where a user query returns a ranked set of documents. The defense is constraint enforcement: retrieval respects access controls, filters by data classification, applies ranking transparency, and logs every query-result pair. Users with limited permissions cannot retrieve documents they are not authorized to see. Queries that attempt to exfiltrate data trigger alerts. Retrieval results include provenance metadata so the generation layer knows the source and trust level of every chunk. The output of retrieval is a set of documents that are authorized, relevant, and attributed. The security property: retrieval returns only what the user is allowed to see, and every returned document has documented provenance.

**Generation** is where the model synthesizes retrieved content into a response. The defense is output control: the generation prompt includes policy guardrails, the model is instructed to cite sources, and the generated response is evaluated for policy compliance before it reaches the user. The model is not allowed to invent facts, fabricate citations, or ignore retrieval content. The output of generation is a response that is grounded in retrieved documents, cites its sources, and respects policy boundaries. The security property: the model's output is constrained by what it retrieved, not by what it can hallucinate.

**Output filtering** is the final gate before the user sees the response. The defense is validation: the output is scanned for PII, policy violations, toxic language, and unsupported claims. If the output fails validation, it is blocked and logged. If it passes, it is returned with provenance metadata attached so the user knows which documents informed the response. The output of filtering is a safe, compliant, attributable response. The security property: no unsafe content reaches the user, and every response is auditable.

## Defense-in-Depth in Practice

A logistics company built a RAG system for route optimization that answered driver questions using a knowledge base of delivery policies, traffic regulations, and historical route data. The system was targeted by an attacker trying to inject false route advice that would delay deliveries and disrupt operations. The attacker submitted documents through a partner integration API. The documents looked legitimate: they used the company's formatting, included realistic traffic data, and passed the automated content checks. But they contained subtle misinformation — incorrect speed limits, nonexistent road closures, reversed turn restrictions. The ingestion layer flagged two of the documents for manual review because their provenance metadata indicated they came from a new partner integration endpoint that had not been used before. Security reviewed them, found the false data, traced it back to a compromised partner account, and blocked the integration. The other 14 poisoned documents had already been indexed.

The retrieval layer caught them. When drivers queried for route advice, the system retrieved the poisoned documents, but it also retrieved the canonical policy documents and historical route logs. The retrieval ranker assigned low scores to the poisoned documents because their metadata indicated they came from an untrusted source and their content conflicted with higher-provenance documents. The generation layer saw the conflict: one document said the speed limit on Route 7 was 35 mph, three other documents said it was 55 mph. The generation prompt included an instruction: "when sources conflict, prefer the source with higher provenance and flag the conflict in your response." The model generated: "The speed limit on Route 7 is 55 mph according to state traffic regulations. Note: conflicting information was found in a recent partner data upload and is under review." The output filter logged the conflict, alerted the security team, and returned the response. The driver got correct advice. The poisoned content was neutralized before it caused harm. The security team used the conflict logs to identify the remaining 14 poisoned documents, quarantine them, and roll back the knowledge base to a clean state.

This is defense-in-depth. The ingestion layer caught some of the attack. The retrieval layer deprioritized the rest. The generation layer resolved the conflict using provenance. The output layer logged the anomaly. No single defense stopped the attack. The combination of defenses limited the damage to zero.

## Architectural Patterns: Isolated Ingestion

Most RAG systems ingest content directly into the production knowledge base. A document is uploaded, validated, embedded, and indexed in one pipeline. If the validation fails, the pipeline stops. If the validation passes, the document goes live. This works fine until validation misses something. A poisoned document that passes validation is immediately retrievable by users. There is no second review. There is no staging period. The attacker wins as soon as the document is indexed.

**Isolated ingestion** separates the ingestion environment from the production environment. Documents are ingested into a staging knowledge base, not the production knowledge base. The staging knowledge base is isolated: it runs in a separate namespace, it is not queried by end users, and it has no connection to production retrieval. Documents in staging are validated twice — first by automated filters, then by security review. Only after both validations pass does the document get promoted to production. Promotion is an explicit step that requires approval and generates an audit log. The result: poisoned content is contained in staging. It never reaches users.

A government agency used isolated ingestion for their internal policy RAG system. They ingested 2,000 to 5,000 documents per week from multiple sources: policy memos, legislative updates, case law summaries, and external research reports. Every document entered the staging knowledge base first. Automated filters flagged 12 to 18 documents per week for manual review — documents with unusual provenance, documents from new sources, documents with policy-sensitive keywords, documents with embedding anomalies. Security reviewed them within 24 hours. Roughly 60 percent were approved and promoted to production. The remaining 40 percent were rejected: spam, phishing attempts, mislabeled content, outdated policies, and twice in the last year, confirmed poisoning attempts. Neither poisoning attempt reached production. Neither was seen by users. The staging layer absorbed the attack.

Isolated ingestion also enables A/B testing for security. You can run queries against the staging knowledge base and the production knowledge base, compare the results, and measure the impact of new content before it goes live. If staging retrieval is generating lower-quality responses, you investigate before promotion. If staging retrieval is flagging more policy violations, you audit the new documents before they reach users. This is how you catch contamination early: test in isolation, promote with confidence.

## Architectural Patterns: Sandboxed Retrieval

Retrieval is a query execution engine. A user submits a query. The system searches the knowledge base. The system returns documents. The documents are passed to the model. This process is a code execution surface. A maliciously crafted query can trigger retrieval errors, exploit ranking logic, or exfiltrate data through side channels. The defense is sandboxing: retrieval runs in a restricted environment with limited permissions, limited access to external resources, and monitored execution.

**Sandboxed retrieval** means the retrieval layer cannot access resources it does not need. It cannot write to the knowledge base. It cannot call external APIs. It cannot access user data beyond what is necessary to enforce access controls. It runs with the minimum permissions required to execute queries and return results. If an attacker compromises the retrieval layer, they gain access to a read-only view of the knowledge base, not to the ingestion pipeline, not to the embedding infrastructure, not to the user database. The blast radius is contained.

A financial services company sandboxed their RAG retrieval layer after a penetration test revealed that a compromised retrieval service could be used to exfiltrate the entire knowledge base by issuing automated queries that iterated over the embedding space. The team redesigned the retrieval service to run in a separate Kubernetes namespace with network policies that blocked outbound traffic to everything except the vector store and the generation service. The retrieval service could read embeddings and documents. It could not write data, access logs, or communicate with external endpoints. The team also implemented rate limiting at the retrieval layer: no single user could issue more than 100 queries per minute, and any user who triggered this limit three times in an hour was automatically flagged for review. The exfiltration vector was closed. The attack was no longer viable.

Sandboxing also protects against supply chain attacks. If an attacker compromises a dependency used by the retrieval layer — a vector database client library, an embedding SDK, a logging utility — the sandbox limits what the compromised dependency can do. It cannot pivot to other systems. It cannot persist access. It cannot escalate privileges. The attack is contained to the retrieval layer and detected by anomaly monitoring. This is not theoretical. A healthcare RAG system caught a supply chain attack in January 2026 when a compromised vector database client attempted to open an outbound HTTPS connection to an external IP. The sandbox blocked the connection. The monitoring system logged the attempt. Security investigated and found that the client library had been backdoored in a dependency update. The team rolled back to the previous version, reported the incident to the library maintainers, and added the compromised version to their software bill of materials blocklist. The attack was neutralized before it reached production data.

## Architectural Patterns: Output Validation Layers

The model's output is not safe until it has been validated. A response that contains PII, policy-violating content, or fabricated citations is a security incident even if the retrieval layer and generation layer functioned correctly. The model makes mistakes. The model hallucinates. The model misinterprets instructions. The defense is output validation: every response is scanned for safety, compliance, and accuracy before it reaches the user.

**Output validation layers** are filters that sit between the model and the user. They operate as a series of gates. The first gate scans for PII: Social Security numbers, credit card numbers, patient identifiers, account credentials. If found, the response is blocked, redacted, or flagged depending on policy. The second gate scans for policy violations: toxic language, prohibited advice, competitor mentions, legally risky claims. If found, the response is blocked or rewritten. The third gate scans for fabricated citations: the model claims it retrieved information from Document X, but Document X does not exist or does not contain that information. If found, the response is blocked and logged as a hallucination. The fourth gate scans for unsupported claims: the model makes a factual assertion that is not grounded in any retrieved document. If found, the response is flagged for review or rejected outright depending on risk tolerance.

Each gate is independent. A response that passes the PII gate can still fail the policy gate. A response that passes the policy gate can still fail the citation gate. The gates are cumulative: the response must pass all gates to reach the user. This is layered validation. No single filter is perfect. No single filter needs to be perfect. The combination of filters catches the majority of unsafe outputs.

A legal services RAG system implemented output validation in mid-2025 after a client received advice that cited a case that did not exist. The system now runs every response through four validators. The PII validator scans for client names, case numbers, and attorney work product. The policy validator scans for advice that contradicts firm policy or contains prohibited language. The citation validator checks that every cited document actually exists in the knowledge base and actually contains the cited information. The claim validator checks that every factual assertion in the response is supported by at least one retrieved document. Responses that fail any validator are blocked and logged. The logs are reviewed weekly by a legal and security team. In the first six months after deployment, the validators blocked 214 responses: 89 for PII leakage, 71 for policy violations, 38 for fabricated citations, and 16 for unsupported claims. None of these unsafe responses reached clients. The firm's malpractice risk dropped. Client trust increased. The cost of the validation layer was $11,000 in annual infrastructure spend. The value was unmeasurable and essential.

## The Provenance Ladder in Practice

Every document in the knowledge base has a provenance score. Every retrieval result carries that score. Every generated response is influenced by the provenance of the documents that informed it. The model does not see all documents as equally trustworthy. It weighs high-provenance sources more heavily than low-provenance sources. It flags conflicts when low-provenance sources contradict high-provenance sources. It refuses to answer when no high-provenance sources are available. This is the Provenance Ladder in action: a structured system for encoding trust into retrieval and generation.

A pharmaceutical company uses a four-tier Provenance Ladder in their internal RAG system. Tier 1 is regulatory filings: FDA submissions, EMA approvals, patent documents. These are treated as ground truth. Tier 2 is peer-reviewed research: journal articles, clinical trial results, academic papers. These are trusted but not definitive. Tier 3 is internal documentation: lab reports, manufacturing specs, quality control logs. These are operational sources. Tier 4 is external summaries: industry news, analyst reports, vendor materials. These are informational but not authoritative. The retrieval layer tags every document with its tier. The generation prompt includes an instruction: "prioritize Tier 1 and Tier 2 sources. If Tier 3 or Tier 4 sources conflict with higher tiers, note the conflict and explain why higher-tier sources are preferred."

In practice, this works. A researcher queried the system about the approved dosage range for a specific compound. The system retrieved one Tier 1 document, three Tier 2 documents, and two Tier 4 documents. The Tier 1 document was the FDA approval letter. The Tier 2 documents were clinical trial reports. The Tier 4 documents were vendor marketing materials that listed a broader dosage range than the FDA approval. The model generated: "The FDA-approved dosage range is 10 to 50 mg per day. Clinical trials support this range. Note: some vendor materials suggest up to 75 mg, but this is not supported by regulatory approval or peer-reviewed research." The response was accurate. The conflict was acknowledged. The provenance was transparent. The researcher knew which source to trust.

The Provenance Ladder also defends against poisoning. An attacker who injects documents into the knowledge base cannot control their provenance tier. If they inject documents through a bulk upload API, those documents are tagged as Tier 3 or Tier 4 depending on the source. If they conflict with Tier 1 or Tier 2 sources, the model deprioritizes them and flags the conflict. The poisoned content is retrievable, but it does not control the output. The high-provenance content wins. This is defense through prioritization: you cannot prevent all poisoning, but you can prevent poisoned content from being trusted.

## Monitoring and Alerting for RAG Systems

A secure RAG system generates telemetry at every stage. Ingestion logs document uploads, validation failures, and policy flags. Indexing logs embedding anomalies and versioning events. Retrieval logs query patterns, access control decisions, and provenance scoring. Generation logs output quality metrics and citation validation results. Output filters log blocked responses, redactions, and policy violations. All of this telemetry feeds into a monitoring system that watches for security anomalies.

The alerts that matter: sudden spikes in low-provenance retrieval, increases in blocked outputs, unusual query patterns, access control violations, embedding drift, and ingestion from new sources. Each alert is a signal that something changed. Not every change is an attack. Most are benign: a new data source, a configuration update, a model version change. But some changes are attacks in progress. The monitoring system cannot distinguish them automatically. It flags the anomaly. A human investigates. The investigation takes minutes, not days, because the telemetry is structured and the audit trail is complete.

A retail company's RAG system triggered an alert in December 2025: the percentage of retrieval results flagged as low-provenance had increased from three percent to 14 percent over 48 hours. Security investigated. They found that a bulk upload from a supplier integration had introduced 1,800 documents with incomplete metadata. The documents were legitimate, but the metadata pipeline had failed, leaving them untagged and unclassified. The retrieval layer treated them as low-provenance by default. The security team paused the ingestion pipeline, fixed the metadata pipeline, re-ingested the documents with correct metadata, and restored normal operations. The alert prevented a false positive spiral: if low-provenance documents had continued accumulating, the retrieval layer would have eventually started returning them in responses, the output filter would have blocked them, and users would have experienced degraded response quality. The issue was caught early because the monitoring system was watching the right metrics.

## Summary: Defense-in-Depth for RAG

RAG security is the sum of its defenses. Ingestion validates provenance. Indexing isolates embeddings. Retrieval enforces access controls. Generation respects provenance. Output filtering catches unsafe content. Versioning enables rollback. Monitoring detects anomalies. Audit trails enable investigation. No single defense is sufficient. Every defense is necessary. An attacker who breaches one layer encounters the next. An attacker who poisons documents encounters retrieval ranking that deprioritizes them. An attacker who manipulates retrieval encounters generation prompts that flag conflicts. An attacker who bypasses generation encounters output filters that block unsafe responses. The system resists attacks not because it is invulnerable, but because it is layered, observable, and recoverable.

The teams that deploy RAG securely in 2026 treat security as an architectural property, not a feature. They version their knowledge bases. They isolate their ingestion. They sandbox their retrieval. They validate their outputs. They log everything. They monitor constantly. They investigate quickly. They roll back confidently. They do not assume their defenses are perfect. They assume attacks will happen and build systems that survive them. That is the definition of secure RAG architecture.

We turn next to an attack surface that makes RAG look simple by comparison: autonomous agents that take actions, not just answer questions. The stakes multiply. The defenses must evolve. Agent security is where the hardest problems in AI security live today.

