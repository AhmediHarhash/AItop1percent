# 7.5 — Role-Based Access Control for AI Features

The support engineer had read-only access. That was the entire security model — their account role couldn't modify data, couldn't change configurations, couldn't deploy code. But their account could access the AI model playground used for testing prompt variations. And the playground had access to production tools: database queries, customer record lookups, payment processing actions. The support engineer didn't need elevated privileges in your traditional system. They had elevated privileges in the AI layer, where the model they could interact with had admin-level tool access. When the engineer's account was compromised through a phishing attack, the attacker didn't need to escalate privileges through your identity system — they were already inside the most powerful interface in your infrastructure.

Role-based access control works when the capabilities you're controlling are static and well-defined. A user can view this table, edit that record, approve these transactions. But AI capabilities don't map to database operations. AI capabilities are dynamic, composable, and tool-enabled. A user with model access doesn't just read data — they can instruct the model to read data, synthesize it, transform it, and take actions based on it. If the model has more permissions than the user should have, RBAC fails at the boundary where the user's request becomes the model's action.

## The Failure of Permission Inheritance

Traditional RBAC assumes a simple inheritance model: the user has permissions, the services they access inherit those permissions, and no service performs actions the user isn't authorized to perform. If a support agent can view customer records but not modify them, then any API they call operates with view-only access. The API inherits the user's role and enforces it at the data layer.

This breaks with AI systems for three reasons. First, the user's direct action is a natural language request, not a structured API call. You can enforce that an API call to update a customer record fails if the user lacks write permissions. You can't easily enforce that a natural language instruction like "mark this account as premium" fails, because by the time you've parsed the intent and routed it to the appropriate tool, the user's role may not be checked — the model's role is.

Second, the model is itself a service with its own permissions. If your model runs in a service account with database admin access so it can answer support queries across all tables, then every user who can prompt the model effectively has database admin access, filtered only by how well the model understands and enforces your access policies. The model becomes a privilege escalation surface. The user asks the model to do something their role forbids. If the model's role allows it, the action succeeds.

Third, AI systems compose capabilities in ways traditional systems don't. A user might have read-only access to customer data and read-only access to the email API. Neither permission alone allows harm. But if the user can prompt a model that has both permissions, they can instruct the model to "read the customer list and email all of them with this promotional message." The model combines two read-only capabilities into a write action. The individual permissions were safe. The composition is not.

A healthcare company discovered this the hard way in late 2025. Their support team used an AI assistant with access to patient records, billing systems, and appointment scheduling. The assistant ran under a service account with broad read-write permissions so it could handle any support request. Support agents had RBAC roles limiting them to specific departments — billing agents could access billing records, scheduling agents could access appointments, clinical staff could access medical records. But every agent could prompt the AI assistant, and the assistant's permissions were universal.

A billing agent, frustrated by a payment dispute, asked the assistant to pull the patient's full medical history to justify the charges. The assistant complied — the billing agent's RBAC role didn't permit access to medical records, but the assistant's role did, and the assistant saw the request as a legitimate support query. The agent now had access to protected health information outside their authorization scope. When auditors reviewed access logs six months later during a compliance audit, they found two hundred and thirty-seven instances where agents accessed records outside their role through AI assistant queries. HIPAA violation. Two point one million dollar fine. The company thought RBAC protected them. It protected the direct database access. It didn't protect the AI-mediated access.

## Mapping Roles to AI Capabilities

Effective RBAC for AI requires mapping roles not just to data permissions, but to model capabilities. You define roles: admin, power user, standard user, guest. Then you define what each role can do with AI features: which models they can access, which tools those models can invoke, which rate limits apply, which content filters are enforced, which data scopes are permitted.

An admin might have access to all models, including experimental ones not yet approved for general use. They can invoke tools that modify production data. They have higher rate limits. Their queries bypass some safety filters because you trust them to test edge cases. A power user gets access to production models with a curated set of tools — data retrieval, report generation, analysis functions — but not tools that modify state. Their rate limits are higher than standard users but capped below admin. Standard users get the base model with limited tools, strict rate limits, and full safety filtering. Guests get the cheapest model, no tools, aggressive rate limiting, and the most restrictive content filters.

The mapping is not obvious. In a traditional SaaS app, it's clear that admin can delete records and standard users can't. In an AI system, the capability boundary is fuzzier. Can a standard user ask the model to "simulate what would happen if I deleted this record"? That's not a write action, but it reveals information the user might not have access to. Can a power user ask the model to "draft an email to all enterprise customers"? They're not sending the email, but they're generating content that could be sent if they had access to the send function.

The teams that get this right use capability-based thinking instead of action-based thinking. Instead of asking "can this user call this API endpoint," they ask "what outcomes can this user achieve through AI." An outcome-based role definition for a standard user might be: "retrieve their own data and analyze it, generate content for their own use, access documentation and help resources, perform read-only operations within their assigned accounts." This maps to a specific model configuration: tools that query user-scoped data only, rate limits that prevent bulk operations, no tools that modify state, no tools that send external communications, no tools that aggregate across multiple accounts.

An outcome-based role definition for a power user might be: "retrieve and analyze data across their department, generate reports and insights for their team, perform non-destructive administrative actions like exporting data or generating audit logs, test new features before general release." This maps to a different configuration: tools that query department-scoped data, tools that generate and export reports, higher rate limits, access to beta models, some administrative tools but only read-only or audit-trail variants.

The key difference from traditional RBAC is that you're not just controlling access to data — you're controlling access to reasoning and synthesis capabilities that operate on that data. A user with read access to a database can view rows. A user with read access mediated through an AI model can ask "which customers are most likely to churn" and get an answer synthesized from patterns across the entire dataset. The data access level is the same. The capability level is not.

## The Granularity Problem

Too coarse: you have two roles, admin and user. Admins can do anything. Users can do basic queries. This is easy to implement and easy to understand. It's also insufficient for any real system. Your product team needs access to usage analytics but not customer PII. Your support team needs access to customer records but not payment processing. Your data science team needs access to aggregated data but not individual records. Two roles can't express these distinctions.

Too fine: you have seventy roles, each with a precisely scoped set of permissions. Marketing analyst, support tier-one, support tier-two, support tier-three, billing specialist, billing manager, data scientist, data engineer, product manager, product owner, engineering read-only, engineering write, engineering deploy, security analyst, security admin, compliance officer, legal, finance, operations, customer success manager, account executive, solutions architect. Every job function gets a role. Every role has a fifty-line permission matrix.

The implementation cost is high. Every new feature requires updating the permission matrix for twenty roles. Every new hire requires determining which of seventy roles applies, or creating a seventy-first. The operational cost is higher. When a support agent needs access to a new tool, they file a ticket. It takes four days to route to the right approver. The approver doesn't understand the tool's risk profile, so they escalate to security. Security takes three days to review. By the time access is granted, the original business need has passed. The agent finds a workaround — usually sharing credentials with a coworker who already has access, which bypasses RBAC entirely.

The effective middle ground is role hierarchies with attribute-based refinement. You define a small number of base roles — typically four to seven — that map to broad capability levels. Then you add attributes that refine permissions within each role based on context. A support agent's base role grants access to customer records. An attribute like "department equals billing" refines that to billing-related records only. Another attribute like "region equals EMEA" further refines it to EMEA customers only. The base role is coarse enough to be understandable. The attributes are fine enough to enforce least privilege.

For AI systems, attributes become especially important because they can be context-specific. A user's role might grant access to a model, but attributes determine which tools that model can invoke based on the request context. If the user is querying their own data, all read tools are available. If the user is querying someone else's data, only tools that respect cross-account permissions are available. If the user is in a testing environment, destructive tools are allowed. If the user is in production, destructive tools are blocked. The role is constant. The capabilities shift based on attributes evaluated at request time.

## Role Escalation Attacks

The attacker had a standard user account. Standard users couldn't access admin features. But the account management API had an update-profile endpoint that accepted a JSON payload with user attributes. The frontend sent attributes like display name, email, timezone. The backend applied all attributes in the payload to the user record without filtering. The attacker added a role attribute to the payload, setting it to admin. The backend accepted it. The account was now admin.

Role escalation is any method that grants a user higher privileges than their assigned role permits. The most common vector is insufficient input validation on endpoints that modify user attributes. The developer assumes the frontend will only send permitted fields. The attacker bypasses the frontend and sends raw API requests with additional fields. If the backend doesn't explicitly validate which fields are modifiable, privilege fields get updated along with benign ones.

A financial services company had a more subtle vulnerability in their AI system. Users had roles that determined which financial models they could access: basic users got simple calculators, premium users got predictive models, enterprise users got custom fine-tuned models. The role check happened at API request time: when a user requested a model inference, the system checked their role and routed to the appropriate model tier. But the model tier routing was based on a parameter in the request header called model-tier, set by the frontend based on the user's role.

The attacker noticed that if they modified the request header to set model-tier to enterprise while their account was still basic, the backend routed them to the enterprise model without re-checking their role. The role check happened before routing. The routing decision trusted the header. The header was attacker-controlled. For eleven months, basic-tier users who discovered the trick accessed enterprise models without payment. The company only found out when they analyzed model usage costs and realized enterprise model API calls exceeded enterprise subscription count by three hundred percent.

Preventing role escalation requires defense in depth at three layers. First, input validation: any endpoint that modifies user attributes must have an explicit allowlist of fields that can be modified. If the endpoint is update-profile, the allowlist might be display name, email, timezone, notification preferences. The role field is not on the allowlist. Any attempt to modify it is rejected, logged, and flagged for review. The allowlist is enforced server-side, not client-side. The frontend can hide fields from the UI, but the backend must independently validate that only permitted fields are present.

Second, authorization re-check: any operation that depends on a user's role must check that role at the point of operation, not just at the entry point. If a user requests access to a premium model, the system checks their role before queuing the request. When the request is processed, the system checks their role again before invoking the model. The second check defends against race conditions where a user's role changes between request and processing, or where a user exploits async processing to escalate privileges after the initial check.

Third, immutable privilege tokens: instead of passing user role as a mutable field in every request, encode it in a signed token that the backend validates on every operation. The token is issued at login, includes role and attributes, is signed with a secret key, and expires after a short time window. The user can't modify the token because they don't have the signing key. The backend can trust the token because it verifies the signature. If the user's role changes, they must re-authenticate to get a new token. The token prevents privilege fields from being attacker-controlled.

## Dynamic Roles Based on Context

A user's capabilities should shift based on context. The same user might have admin privileges in a testing environment, power user privileges in staging, and standard user privileges in production. Or their capabilities might depend on which account they're operating within — full access to their own team's resources, read-only access to shared resources, no access to other teams' resources. Or their capabilities might depend on time — elevated privileges during business hours, restricted privileges after hours, emergency-break-glass access during incidents.

Dynamic roles require evaluating policy at request time instead of assigning fixed permissions at account creation. When a user makes a request, the system evaluates: what is the user's base role, what is the context of this request (environment, resource scope, time, access history), what attributes apply, what capabilities does this combination permit. The evaluation happens server-side, on every request, with results cached briefly but re-evaluated frequently enough that context changes take effect within seconds.

An e-commerce platform implemented this for their AI-powered fraud detection tool used by both automated systems and human fraud analysts. The AI system had a role that permitted flagging transactions, blocking accounts, and refunding payments — all high-risk actions. Automated rules could flag transactions but not block accounts or issue refunds. Fraud analysts could do all three, but only during business hours and only after viewing evidence. The system evaluated policy dynamically: if the requester is the AI system and the action is flag transaction and the confidence score is above 0.95, allow. If the requester is the AI system and the action is block account, deny. If the requester is a fraud analyst and the action is block account and the time is between 9am and 6pm and the analyst viewed the account details in the last five minutes, allow.

The policy engine evaluated dozens of conditions per request. The result: the AI system could act autonomously within safe boundaries, fraud analysts had appropriate tools with safeguards against accidental actions, and high-risk actions required human review even when the AI suggested them. The capability model was context-aware. A fraud analyst couldn't block an account they hadn't reviewed. The AI couldn't issue refunds without human approval. The same action had different authorization outcomes based on who requested it, when, and what evidence they'd reviewed.

Dynamic roles have two costs. First, implementation complexity: you need a policy engine that evaluates conditions, a context provider that supplies current state, and integration with every service that makes authorization decisions. Second, performance overhead: every request now includes a policy evaluation that might query user attributes, resource metadata, recent access history, and time-based rules. If your policy evaluation takes one hundred fifty milliseconds, every AI request is one hundred fifty milliseconds slower. The latency cost is often acceptable for high-risk actions like admin tools or data modification, but unacceptable for low-latency operations like real-time model inference.

The teams that implement dynamic roles well use tiered policy evaluation. Low-risk operations use cached role checks: the user's base capabilities are evaluated at login, cached in a short-lived token, and used for routine operations without re-evaluation. High-risk operations use full policy evaluation: every attribute, every context variable, every condition checked in real-time. The system learns which operations are high-risk through usage analysis and incident review. Initially, most operations use cached checks. Over time, operations that cause incidents or near-misses get promoted to full policy evaluation.

## RBAC as an Ongoing Design Problem

The mistake most teams make is treating RBAC as a one-time implementation. They design roles at the start, configure permissions, deploy, and consider it done. Then the product evolves. New features ship. New models are added. New tools are enabled. The original role definitions no longer map to actual usage. Support agents need access to tools that didn't exist when their role was defined. Power users are blocked from features they need. Admins have access to legacy capabilities that are now considered high-risk.

RBAC degrades without maintenance. You add a new AI feature that lets users generate SQL queries from natural language and run them against the database. Which role gets access? The original role definitions don't mention SQL generation — the feature didn't exist yet. The default decision is either too permissive (everyone gets access, including users who shouldn't write arbitrary queries) or too restrictive (no one gets access, including data analysts who need it for their job). Either way, the original role design is now misaligned with product reality.

The effective approach is treating roles as living policy that evolves with the product. Every feature launch includes a role access decision: which roles get this capability, which don't, and why. Every quarter, the security team reviews role definitions against actual usage patterns: are users escalating their privileges through support tickets because role definitions are too restrictive? Are users sharing credentials because the role system is too complex to navigate? Are high-risk actions being performed by roles that shouldn't have that access? The review leads to role refinement: new attributes added, capability boundaries adjusted, deprecated features removed from permission sets.

The second mistake is designing roles in isolation from actual user workflows. The security team defines roles based on risk models. The product team defines features based on user needs. The gap between them creates friction. A fraud analyst needs to review flagged transactions, investigate account history, and block accounts if fraud is confirmed. The security team's role model says: read access to transactions, read access to account history, write access to block actions. But the product team built a workflow where blocking an account requires viewing its transaction details in the same session. The role model doesn't capture the workflow dependency. The fraud analyst can block accounts without reviewing transactions, which is a compliance risk, or they can review transactions but not block accounts, which is an operational blocker.

Workflow-aware RBAC maps roles to workflows, not just to actions. The fraud analyst role includes a fraud investigation workflow: view flagged transaction, retrieve account history, assess evidence, decide action, execute block if fraud confirmed. The workflow ensures steps happen in order and dependencies are met. The role grants access to the workflow, which enforces the sequence. The fraud analyst can't skip the review step and jump straight to blocking — the workflow doesn't permit it. The role becomes a container for approved workflows, not just a list of allowed operations.

The next subchapter covers scoped tokens and capability-based security — a more granular model where access is controlled not just by who you are, but by what specific capabilities your current token grants, allowing fine-grained, time-limited, action-specific access that can be revoked independently of your overall account permissions.

