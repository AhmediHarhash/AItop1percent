# 11.5 â€” Vendor Outages and Dependency Failures

Single-vendor dependency is the most common single point of failure in production AI systems. It is also the least acknowledged.

In November 2025, a major LLM API provider experienced a six-hour outage during peak business hours in North America and Europe. The root cause was a cascading failure in their inference infrastructure. The impact reached thousands of production applications that depended entirely on that provider's API. A customer support platform went dark. A legal document review system could not process filings. A medical triage chatbot returned error messages to patients seeking urgent care. None of these applications had fallback providers. None of them had cached responses for common queries. None of them had designed for the possibility that their primary vendor would be unavailable. The outage cost the affected companies a combined 1.2 million dollars in lost revenue, SLA penalties, and emergency mitigation. The vendor issued a postmortem and a credit for 10 percent of the month's API usage. The customers absorbed the rest.

Vendor outages are not edge cases. They are inevitable. Every cloud provider has outages. Every API service has degraded performance windows. Every third-party dependency will fail at some point. The only question is whether your system can survive when it happens. Most cannot. Most teams build as though their vendors will always be available. Most systems fail the moment that assumption breaks.

## The Single-Vendor Trap

Building on a single vendor is the easiest path to production. You choose the best LLM provider for your use case. You integrate their API. You optimize your prompts for their model. You build your evaluation suite around their response format. You train your team on their documentation. You negotiate enterprise pricing. Your entire system is now coupled to that vendor's infrastructure, that vendor's uptime, and that vendor's business decisions. You have traded simplicity for fragility.

The trap closes slowly. In the first months, the vendor is reliable. Outages are rare and brief. Your system works. Your users are happy. You ship new features that depend more deeply on the vendor's capabilities. You build prompt chains that assume the vendor's specific model behavior. You write code that parses the vendor's response schema. You grow your user base. You sign SLAs with your own customers. And then the vendor has an outage. Your system stops working. Your users see error messages. Your SLAs trigger penalties. You have no recourse. The vendor's terms of service disclaim liability for downtime. You accepted those terms when you signed up.

Single-vendor dependency is not just a technical risk. It is a business risk. Your revenue depends on a vendor whose uptime you cannot control, whose roadmap you cannot influence, and whose pricing you cannot negotiate beyond the contract you already signed. If the vendor raises prices, you either pay or migrate. If the vendor depreciates the API you depend on, you either rewrite your integration or accept degraded functionality. If the vendor is acquired, you hope the new owner maintains the service. You are not in control. You are at the vendor's mercy.

This risk compounds when the vendor is also your competitor. Many LLM providers operate their own consumer applications that compete directly with their API customers. The vendor has access to your usage patterns, your prompts, and your traffic. They can see which features drive the most engagement. They can see which use cases generate the most revenue. They can build those features into their own consumer product. The line between infrastructure provider and competitor is blurry. Your dependency on the vendor gives them leverage in that competition.

## Cascading Failures When Providers Go Down

Vendor outages do not happen in isolation. When a major LLM provider goes down, every system that depends on that provider fails simultaneously. The failures cascade. Your user-facing application returns errors. Your internal tools stop working. Your monitoring systems lose the data they need to assess the scope of the failure. Your incident response process depends on systems that are also down. The cascading failure spreads faster than you can react.

The worst cascading failures are the ones that create feedback loops. Your application retries failed requests. The retries overwhelm the vendor's API as it comes back online. The vendor's infrastructure cannot handle the surge. The outage extends. Your retries make the problem worse. Meanwhile, your users retry their requests at the application level. The compounding retries create a denial-of-service attack on your own infrastructure. You are now fighting two outages: the vendor's and your own.

Cascading failures reveal hidden dependencies. You thought your monitoring was independent of your LLM provider. It is not. Your monitoring dashboard uses an LLM to summarize incident reports. When the provider goes down, you lose monitoring. You thought your alerting system was independent. It is not. Your alerting logic uses an LLM to classify error messages. When the provider goes down, you lose alerting. You thought your customer support team had a manual fallback. They do not. Their support tools rely on the same LLM provider for drafting responses. When the provider goes down, the support team can only apologize.

The impact of cascading failures is not evenly distributed. The teams with the deepest integration suffer the most. The teams that built their entire product around a single vendor's capabilities have no fallback. The teams that treated the vendor as a utility rather than a dependency are the ones who lose users permanently. Outages do not just cost revenue during the downtime. They cost trust. Users who see your system fail once will assume it can fail again. Some of them leave. Some of them tell others. The reputational damage outlasts the outage.

## API Deprecation and Breaking Changes

Vendor outages are acute failures. API deprecations are chronic failures. They happen slowly, with warning, but they are just as destructive. The vendor announces that they are deprecating an API version you depend on. They give you six months to migrate. You have six months to rewrite every integration, every prompt, every test case. You have six months to validate that the new API behaves identically to the old one. It does not. The new API has different rate limits, different error codes, different response schemas. You spend the six months rewriting. You ship the migration. You discover that the new API is slower, more expensive, and less reliable than the old one. You have no choice but to accept it. The old API is gone.

Breaking changes are worse than deprecations because they happen without warning. The vendor ships a model update. The new model has better benchmark scores. It also has different behavior on your specific use case. Your prompts stop working. Your outputs change format. Your evaluation suite starts failing. You did not change anything. The vendor changed the model. You are responsible for fixing the breakage. The vendor's release notes do not mention the behavior change because it was not intentional. It was an emergent side effect of the model's training. You have no way to predict it. You have no way to prevent it.

API deprecations and breaking changes create technical debt that you did not choose. You built your system on a stable foundation. The vendor made the foundation unstable. You must now maintain two versions of your integration: the one that works with the old API and the one that works with the new API. You must run both in parallel during the migration window. You must test both. You must monitor both. The maintenance cost doubles. The vendor does not compensate you for this cost. It is priced into the relationship. You accepted it when you chose a single vendor.

The teams that survive deprecations and breaking changes are the ones who abstract their vendor dependencies behind internal interfaces. They do not call the vendor's API directly from application code. They wrap it in an internal service that provides a stable contract. When the vendor changes, they update the wrapper. The application code does not change. This abstraction has a cost. It requires more engineering time upfront. It requires discipline to maintain the abstraction boundary. But it is the only way to insulate your system from vendor churn.

## The Hidden Costs of Vendor Lock-In

Vendor lock-in is not just about switching costs. It is about lost optionality. When you are locked into a single vendor, you cannot take advantage of competitors' innovations. A new provider releases a model that is faster and cheaper than the one you use. You cannot switch because your entire system is optimized for your current vendor's behavior. You cannot even test the new provider because doing so requires rewriting your integration. The new model's advantages are irrelevant. You are locked in.

Lock-in also means you cannot negotiate. When your contract comes up for renewal, the vendor knows you cannot leave. They know how much it would cost you to migrate. They price accordingly. You negotiate, but your leverage is limited. The vendor offers a small discount. You accept it because the alternative is a six-month engineering project to move to a different provider. The vendor knows this. The pricing reflects it.

The hidden cost is strategic. When you are locked into a vendor, your product roadmap is constrained by the vendor's roadmap. You want to add a feature that requires capabilities your vendor does not offer. You have two options: wait for the vendor to build it or migrate to a vendor who already has it. Both options are expensive. Both options delay your roadmap. Your competitors who built multi-vendor architectures have more flexibility. They can adopt new models faster. They can optimize costs more aggressively. They can differentiate on capabilities you cannot access. Your lock-in is their advantage.

Vendor lock-in is not always avoidable. Some use cases require capabilities that only one vendor provides. Some teams do not have the resources to build multi-vendor abstractions. Some contracts are too favorable to walk away from. But lock-in should always be a deliberate choice, not an accidental outcome. If you are locked in, you should know why, what it costs, and what your exit path looks like. Most teams do not. They drift into lock-in and realize it only when they try to leave.

## Multi-Vendor Architectures for Resilience

The defense against vendor outages, deprecations, and lock-in is to design for multiple vendors from the start. This does not mean integrating with every LLM provider. It means building your system so that swapping providers is a configuration change, not a rewrite. It means abstracting vendor-specific behavior behind internal interfaces. It means testing your system with multiple providers to verify that the abstraction holds. It means treating vendors as interchangeable components, not as foundational dependencies.

A multi-vendor architecture starts with a routing layer. Your application sends a request to the routing layer. The routing layer decides which vendor to use based on availability, cost, latency, or quality. The routing layer handles retries, failover, and error handling. If the primary vendor is down, the routing layer sends the request to a secondary vendor. The application does not know which vendor fulfilled the request. It does not need to know. The routing layer hides the complexity.

The routing layer requires that all vendors provide equivalent functionality. This is the hard part. Different LLM providers have different capabilities, different response formats, and different pricing. A prompt that works well on one provider might fail on another. A response format that your application expects might not be available from all providers. You cannot achieve perfect equivalence. But you can achieve sufficient equivalence for most use cases. You identify the core functionality your application needs. You verify that at least two vendors can provide it. You design your prompts and your parsing logic to work with both. You accept that some vendor-specific optimizations are not portable. You prioritize resilience over optimization.

Multi-vendor architectures are more expensive to build. They require more engineering time, more testing, and more operational complexity. They also cost more at runtime. You pay for at least two vendor contracts. You pay for the traffic to test failover paths. You pay for the abstraction layer's latency overhead. But the cost is insurance. When your primary vendor goes down, your system stays up. When your primary vendor raises prices, you have leverage. When a new vendor offers better capabilities, you can adopt them without rewriting your application. The resilience is worth the cost.

The teams that resist multi-vendor architectures are the ones who believe their current vendor will never fail them. They are wrong. Every vendor fails eventually. The only question is whether you will be ready when it happens.

## Designing for Graceful Degradation

Not every system can support full multi-vendor failover. Some use cases require capabilities that only one vendor provides. Some teams cannot afford the engineering cost of abstraction layers. For these systems, the fallback is graceful degradation. When the primary vendor is unavailable, the system does not fail completely. It degrades to a simpler mode that still provides some value to users.

Graceful degradation requires planning. You identify the core functionality that your users need most. You design a fallback mode that provides that functionality without depending on the vendor. A customer support chatbot might fall back to a decision tree that routes users to the right team. A document review system might fall back to keyword search. A triage system might fall back to showing users a static FAQ. None of these fallbacks are as good as the LLM-powered version. But they are better than error messages. They keep users engaged. They buy you time to restore the primary system.

The hardest part of graceful degradation is knowing when to trigger it. If you wait until the vendor is fully down, you have already lost users. If you degrade too early, you sacrifice functionality unnecessarily. The trigger must be based on observed latency, error rates, or availability metrics. If the vendor's API is responding but slow, you might route some traffic to the fallback while continuing to use the vendor for other requests. If the vendor's API is returning errors, you might degrade fully. The logic is system-specific. The principle is universal: degrade before you fail.

Graceful degradation also requires testing. You cannot wait until an outage to discover that your fallback mode does not work. You must test the fallback regularly in production. You must verify that users can complete their workflows in degraded mode. You must measure the impact on user satisfaction, task completion, and retention. If the degraded mode is unusable, it is not a fallback. It is a longer error message. The only way to know is to test.

Next: how malicious updates propagate through AI supply chains and why auto-updates are a security liability.
