# 3.9 — Defense: Output Classifiers and Safety Filters

The jailbreak bypassed your system prompt. The model generated harmful content. Your first line of defense failed. This is not catastrophic — it is expected. No system prompt is perfect. The question is what happens next. In a robust security architecture, the answer is that the harmful output never reaches the user. It gets intercepted by a post-generation safety filter, classified as violating policy, and replaced with a refusal message. The user sees the refusal. Your monitoring system logs the incident. Your security team investigates why the system prompt failed and refines it. The model's error is contained, not amplified.

This is output classification: treating the model's response as untrusted content that must be validated before delivery. It is the second layer of defense after system prompt hardening. It assumes that jailbreaks will occasionally succeed and builds containment rather than relying solely on prevention. Every production system handling user-generated prompts should implement output filtering. The only question is how sophisticated that filtering needs to be for your risk tier.

## Post-Generation Safety Classification

An output classifier is a model or rule set that examines the text the primary model generated and decides whether it violates policy. The simplest version is a keyword blocklist. If the output contains words or phrases from the blocklist, the output is rejected. This catches obvious violations — explicit profanity, slurs, instructions for illegal activity — but misses anything paraphrased or implied. A model asked to explain how to bypass a security system might generate a tutorial without using any blocked keywords. Keyword filters are necessary but insufficient.

The next level is semantic classifiers. These are typically smaller, faster models trained specifically for safety classification. You send the generated text to the classifier, and it returns a probability distribution over harm categories: violence, hate speech, sexual content, child safety violations, self-harm instructions, illegal activity, fraud, misinformation. If any category exceeds your threshold, the output is rejected. If all categories are below threshold, the output is delivered.

The advantage of semantic classifiers is that they detect harmful content based on meaning, not keywords. A model output that says "that population should not exist" gets flagged as hate speech even though it uses no explicit slurs. An output that describes a method for self-harm in clinical language gets flagged even though it uses medical terminology. The classifier is evaluating intent and impact, not just surface features.

The leading safety classifiers in 2026 are Perspective API from Google, OpenAI's Moderation API, Anthropic's Constitutional AI filters, and open-source alternatives like Detoxify and HateBERT descendants. Each has different taxonomies and thresholds. Perspective API provides toxicity, severe toxicity, insult, profanity, threat, identity attack, and sexually explicit scores. OpenAI's Moderation API provides hate, hate/threatening, harassment, self-harm, sexual, sexual/minors, violence, and violence/graphic categories. The overlap is high but not perfect. A text flagged by one classifier might pass another.

Best practice is to use at least two classifiers and reject if either flags the output. This reduces false negatives — harmful content that evades one classifier is more likely to be caught by the second. It increases false positives — benign content that triggers one classifier's edge cases gets blocked unnecessarily. The trade-off depends on your risk tolerance. A medical education platform cannot afford to let self-harm instructions through, so it accepts higher false positive rates. A creative writing tool wants to minimize blocking legitimate content, so it uses a higher threshold.

The choice of classifier also depends on your harm taxonomy. If your application has specific policy categories beyond the standard classifier categories, you need a custom classifier trained on your policy definitions. A financial services chatbot might need fraud detection categories that general-purpose classifiers do not provide. A health application might need HIPAA violation detection. A content moderation system for a social platform might need 20 distinct policy categories with sub-categories.

Building a custom classifier requires labeled data. You need thousands of examples of policy-violating content across all your categories, plus examples of borderline cases and false positives. The labeling process is intensive and requires domain expertise. A contractor with no medical background cannot reliably label HIPAA violations. A contractor with no legal training cannot reliably label securities fraud. The people labeling your training data must understand the policy well enough to make judgment calls on edge cases.

Most organizations start with a general-purpose classifier and extend it iteratively. You deploy the off-the-shelf solution, monitor false negatives and false positives in production, and build a custom classifier only if the gaps are too large to tolerate. The custom classifier does not replace the general-purpose one. It supplements it. You run both, each catching different failure modes.

## Harm Category Taxonomies

The harm categories you enforce determine what content your system allows and blocks. The taxonomy is not universal. Different platforms, different industries, and different legal jurisdictions have different definitions of harm. A category that one organization considers a blocking violation might be acceptable content for another. The critical requirement is that your taxonomy must be explicit, consistent, and mappable to your content policy.

A standard taxonomy for general-purpose language models in 2026 includes violence, hate speech, sexual content, child safety, self-harm, illegal activity, fraud, harassment, and misinformation. Each category has subcategories. Violence includes graphic violence, violent threats, glorification of violence, and instructions for violent acts. Hate speech includes slurs, dehumanization, calls for exclusion, and incitement. Sexual content includes explicit sexual descriptions, non-consensual sexual content, and sexualization of minors. Each subcategory has a definition and examples of borderline cases.

The definitions must be precise enough that two human reviewers looking at the same content agree on the category more than 90 percent of the time. If your inter-rater agreement is lower, your taxonomy is ambiguous. The model cannot enforce an ambiguous policy reliably. Human labelers cannot train a classifier on ambiguous labels. When outputs get flagged in production and the user appeals, you cannot defend the decision if your own team disagrees about whether it violated policy.

Some categories are context-dependent. A medical education platform allows descriptions of surgical procedures that would be classified as graphic violence on a social media platform. A creative writing tool allows fictional violence that would be blocked in a customer service chatbot. A legal research tool allows discussion of illegal activities in the context of case law analysis that would be blocked in a general assistant. Your taxonomy must account for context.

One approach is to define global categories and task-specific exceptions. The base policy says "no violent content." The exception for the medical education platform says "descriptions of medical procedures are allowed if they are factual, educational, and not sensationalized." The exception for the creative writing tool says "fictional violence is allowed if it is marked as fiction and the user has opted into mature content." The classifier must be able to apply both the base rule and the exception.

This creates complexity. A single classifier trained on global harm categories cannot apply task-specific exceptions without being retrained for each task. The solution is a two-stage filter. The first stage is a global classifier that checks for clear policy violations. The second stage is a task-specific classifier that checks for exceptions. An output flagged by the first stage gets sent to the second stage. If the second stage determines the content is allowed in this context, the output is delivered. If not, it is blocked.

The second-stage classifier must be faster and lighter than the first because it runs only on flagged content. You cannot afford to double your inference latency by running two full classifiers serially. In practice, the second stage is often rule-based rather than model-based. It checks for metadata like "user opted into mature content" or "task is marked as educational" and applies predefined logic. This is faster than running inference on a second model.

## Confidence Thresholds and Edge Case Handling

A classifier returns a probability, not a binary decision. The output might score 0.92 on violence, 0.15 on hate speech, 0.08 on sexual content. You must decide where to set the threshold above which content is blocked. This is not a technical decision. It is a policy decision with direct impact on user experience and risk exposure.

A low threshold like 0.50 blocks content that the classifier is only marginally confident is harmful. This maximizes safety but generates high false positive rates. Legitimate medical discussions get flagged as violence. Historical analysis of hate groups gets flagged as hate speech. Creative fiction gets flagged as sexual content. Users get frustrated. Support tickets increase. Trust erodes.

A high threshold like 0.90 blocks content only when the classifier is very confident. This minimizes false positives but allows more harmful content through. Paraphrased instructions for illegal activity might score 0.85 and pass. Implied threats might score 0.80 and pass. The system feels more permissive but leaves gaps in safety coverage.

The optimal threshold depends on the harm category and the use case. Child safety violations should have a very low threshold, accepting high false positives to ensure no true positive slips through. Profanity in a creative writing tool can have a high threshold because the cost of a false negative is low. The threshold is not global. It is per-category and sometimes per-task.

In practice, most systems use a three-tier threshold structure. A hard block threshold above which content is always rejected regardless of context. A review threshold below which content is delivered but flagged for manual review. A pass threshold below which content is delivered without logging. An output that scores 0.95 on violence gets blocked immediately. An output that scores 0.75 gets delivered but logged for review. An output that scores 0.40 gets delivered with no special handling.

The review queue is critical for refining the system. Human reviewers examine flagged outputs and mark them as true positives, false positives, or borderline. True positives confirm the classifier is working. False positives indicate the threshold is too low or the classifier is miscalibrated. Borderline cases indicate ambiguity in the policy definition. Over time, the review process generates a dataset that can be used to retrain the classifier or adjust thresholds.

Edge cases are outputs that defy clean categorization. A historical description of a violent event is not a glorification of violence but might score high on the violence classifier. A discussion of a controversial topic might include language that looks like hate speech when analyzed in isolation but is clearly not hateful in context. A creative writing prompt might ask the model to generate content that would be harmful if presented as factual but is clearly fiction.

Edge case handling requires context awareness. The classifier must know not just what the output says but what the user asked for, what the task is, and what the conversation history includes. A standalone output classifier that sees only the generated text has no context. It treats "describe a violent scene from a historical battle" the same as "describe how to commit violence." The first is educational. The second is harmful. The classifier cannot distinguish them without context.

The solution is to pass both the user prompt and the model output to the classifier. The classifier evaluates the pair, not the output in isolation. This doubles the input length but provides the context needed to handle edge cases. A classifier that sees the user asked for historical analysis will not flag a description of violence if it is factually accurate and educational. A classifier that sees the user asked how to commit violence will flag any output that provides instructions, regardless of phrasing.

## Latency vs Coverage Tradeoffs

Every classification step adds latency. An output classifier that takes 200 milliseconds to run adds 200 milliseconds to your response time. In a conversational application where users expect sub-second responses, this is significant. In a batch analysis task where jobs run overnight, it is negligible. The latency tolerance defines the complexity of classifier you can afford to use.

The fastest classifiers are rule-based. A keyword filter or regex pattern matcher runs in single-digit milliseconds. It catches obvious violations but misses everything else. The next tier is small transformer models, around 100 million parameters, optimized for inference speed. These run in 50 to 150 milliseconds depending on hardware and input length. They provide semantic classification but not the full nuance of a large model. The slowest classifiers are large language models prompted to evaluate the output. These provide the best accuracy but take 500 milliseconds to several seconds depending on the model and the prompt complexity.

Most production systems use a cascade. The output is first checked by a fast rule-based filter. If it passes, it is sent to a medium-speed semantic classifier. If that flags it, it is sent to a slow but accurate LLM-based evaluator before a final decision is made. Each stage filters out a fraction of outputs. The rule-based filter catches 30 percent of violations in 5 milliseconds. The semantic classifier catches another 60 percent in 100 milliseconds. The LLM evaluator handles the remaining 10 percent in 1 second. The average latency is much lower than if every output went through the full pipeline.

The cascade architecture also allows you to tune each stage independently. The rule-based filter is updated when new keywords or patterns emerge. The semantic classifier is retrained quarterly on new labeled data. The LLM evaluator is updated when the base model is upgraded or the policy changes. Each component evolves at its own cadence without requiring changes to the others.

Coverage measures what fraction of policy violations the classifier catches. A classifier with 95 percent coverage misses 5 percent of harmful content. That might sound acceptable until you calculate the absolute numbers. A system processing 10 million outputs per day with a 1 percent violation rate and 95 percent classifier coverage is letting 5,000 harmful outputs through every day. If your platform has 100,000 users, roughly 5 percent of them see harmful content each day. That is a serious safety failure.

Increasing coverage usually requires sacrificing either latency or false positive rate. A more complex classifier catches more violations but takes longer to run. A lower threshold catches more violations but blocks more legitimate content. A cascade with more stages catches more violations but adds more latency. The engineering challenge is to find the configuration that maximizes coverage within your latency and false positive constraints.

In practice, 99 percent coverage is achievable for most harm categories with acceptable latency and false positive rates. The last 1 percent is where costs explode. Getting from 99 to 99.9 percent might require doubling your classifier complexity, tripling your false positive rate, or adding 500 milliseconds of latency. Whether that trade-off is worth it depends on the severity of the harm. For child safety violations, the answer is yes. For profanity, the answer is probably no.

## Classifier Evasion Attacks

Adversaries adapt. Once they learn that your system uses output classification, they optimize their jailbreaks to produce outputs that evade the classifier. The attack surface shifts from bypassing the system prompt to bypassing both the system prompt and the output filter. This is harder but not impossible.

The simplest evasion technique is paraphrasing. If the classifier flags "build a bomb," the attacker prompts the model to generate the same instructions using euphemisms or technical jargon. The output might describe constructing an improvised explosive device using chemistry terminology that does not trigger the classifier's bomb-making patterns. The information is identical. The phrasing is different. The classifier misses it.

The second technique is indirection. Instead of asking the model to generate harmful content directly, the attacker asks the model to generate code, a poem, a fictional story, or a technical explanation that encodes the harmful content. A classifier trained to detect harmful instructions in plain text might not detect harmful instructions embedded in Python code or Base64 encoding. The attacker decodes it after delivery.

The third technique is fragmentation. The attacker breaks the harmful request into multiple innocuous requests, each of which passes the classifier individually. The first request asks the model to explain a chemical process. The second request asks about detonation mechanisms. The third request asks about timing circuits. None of the outputs are flagged as harmful in isolation. But together, they provide the information needed to build an explosive device. The attacker assembles the fragments outside the system.

The fourth technique is adversarial suffix optimization against the classifier. If the attacker has access to the classifier's API or can infer its behavior through repeated probing, they can optimize a suffix that makes harmful content score below the threshold. The suffix might be appended to the output, embedded in the middle, or used to prime the generation. The classifier sees the suffix, interprets it as a context shift, and scores the harmful content as benign.

The defense against evasion is layered and adaptive. First, train your classifier on evasion attempts. Your red team generates paraphrased, indirect, and encoded versions of harmful content and adds them to the training set. The classifier learns to recognize harm regardless of phrasing or format. Second, monitor for multi-turn attacks. If a user makes several requests in a session that are individually benign but collectively suspicious, flag the session for review. Third, update your classifier continuously. Every time a new evasion technique appears in your logs, you label it and retrain.

The update cadence is critical. A classifier trained once at launch and never updated is obsolete within months. Attackers discover new evasions. Language evolves. New policy categories emerge. Your classifier must evolve with the threat landscape. A minimal update cadence is quarterly retraining on new labeled data. A better cadence is monthly automated retraining plus weekly threshold adjustments based on production metrics. A best-practice cadence is continuous learning where the classifier incorporates new examples from the review queue in near real-time.

## Layered Filtering: Rule-Based Plus ML-Based

No single classifier is perfect. Rule-based filters are fast and interpretable but brittle and easy to evade. Machine learning classifiers are flexible and adaptive but slower and harder to debug. The robust approach is to use both in parallel or in sequence, each compensating for the other's weaknesses.

A parallel architecture runs both classifiers simultaneously and blocks the output if either flags it. The rule-based filter catches known patterns. The ML classifier catches novel variations. The combined coverage is higher than either alone. The trade-off is that latency is determined by the slower classifier, and false positives increase because the system now triggers if either filter has a false positive.

A sequential architecture runs the rule-based filter first. If it passes, the output is sent to the ML classifier. If the rule-based filter blocks the output, the ML classifier is never invoked. This is faster on average because the rule-based filter eliminates many violations in milliseconds without invoking the slower ML model. The coverage is slightly lower because the rule-based filter might block some outputs that the ML classifier would have allowed, but in practice this is rare.

The choice depends on your latency budget and risk tolerance. A conversational chatbot with sub-second response time requirements uses sequential filtering. A moderation system for user-generated content with no real-time latency requirement uses parallel filtering. Both are valid. Both are deployed in production at scale in 2026.

The rule-based component should not be underestimated. A well-maintained keyword and pattern list catches a significant fraction of violations with near-zero latency and perfect interpretability. When an output is blocked, you can point to the exact rule that triggered. When the policy changes, you update the rules immediately without retraining. When regulators ask how your system works, you can show them the rules in plain language.

The limitation is that rule-based filters require continuous maintenance. New slurs, new euphemisms, new attack patterns emerge constantly. If your rule set is not updated weekly, it falls behind. The process cannot be fully automated because some updates require human judgment about what constitutes a policy violation. A dedicated content policy team must own the rule set and treat it as critical infrastructure.

The ML-based component provides the flexibility to handle cases the rules did not anticipate. It generalizes from training examples to recognize harmful content even when phrased in ways the rule authors never considered. It adapts to language evolution without manual updates. But it requires ground truth data, retraining infrastructure, and careful monitoring for model drift and performance degradation.

Together, the two approaches create defense-in-depth. An attacker who evades the rules might still be caught by the ML model. An edge case that confuses the ML model might still be caught by the rules. The system is more robust than the sum of its parts.

Output classification is your second line of defense. It catches what the system prompt missed. But it is not the final layer. The next subchapter covers the third layer: behavioral analysis and anomaly detection that identifies adversarial users based on their interaction patterns, not just individual prompts.

