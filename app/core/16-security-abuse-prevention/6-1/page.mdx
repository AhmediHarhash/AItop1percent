# 6.1 â€” Why Your Model Is an Asset Worth Stealing

Your fine-tuned model is not just software. It is distilled expertise. It represents months of data engineering, hundreds of thousands of dollars in compute costs, and the institutional knowledge of domain experts who labeled examples, reviewed outputs, and refined behavior across dozens of iterations. When someone steals your model, they are not copying a file. They are stealing the competitive advantage you built.

And they are doing it right now.

## The Investment Encoded in Weights

A fine-tuned model for medical coding contains knowledge that took your team nine months to build. You started with 60,000 labeled examples from board-certified coders. You ran evals on 4,000 edge cases. You iterated through seven training runs, debugging formatting issues, handling rare codes, and tuning the model to match your organization's interpretation of ambiguous guidelines. The final model achieves 94% accuracy on held-out test cases. It saves your organization four million dollars annually by reducing manual coding review time.

That expertise is now encoded in 7 billion parameters. Those parameters are your intellectual property. They represent a defensible advantage over competitors who are still manually coding or using less accurate automation. If a competitor could copy your model's behavior without replicating your data pipeline, your training process, or your domain expertise, they would bypass nine months of development and millions in investment.

They can. They are.

## The Model Theft Landscape in 2026

Model theft is not a theoretical risk. It is an established attack category with documented cases, published research, and active criminal marketplaces. The threat landscape includes four distinct attacker profiles, each with different motivations and capabilities.

**Direct competitors** want your model because it gives them parity without investment. If they can extract your model's behavior through API calls, they skip the data engineering, the domain expertise, and the iterative refinement. They go from zero to 94% accuracy by querying your endpoint a few thousand times. The economics are compelling: distillation costs them 15,000 dollars in API fees and two weeks of engineering time. Building the model from scratch would cost them 800,000 dollars and nine months. The return on investment for theft is fifty to one.

**Researchers** want your model to study it, benchmark against it, or publish papers analyzing its vulnerabilities. Academic model extraction papers often target production APIs to demonstrate feasibility. The researchers are not stealing for profit, but the techniques they publish become recipes for commercial attackers. A 2024 paper demonstrated that a state-of-the-art medical model could be distilled with 8,000 queries. That paper is now a how-to guide for competitors who read it.

**Criminal groups** want your model to resell it, use it for fraud, or reverse-engineer proprietary decision logic. A model that detects fraudulent insurance claims is valuable to criminals who want to bypass those detections. A model that generates high-quality synthetic identities is valuable to identity theft rings. If your model is behind an API, attackers will probe it to extract the behavior they need.

**State-sponsored actors** want your model if it encodes strategically valuable expertise. A model that predicts infrastructure failures, analyzes geopolitical risk, or automates regulatory interpretation can be extracted and deployed by nation-state adversaries who want the capability without the development cost.

All four attacker types use the same core technique: query-based extraction. They send inputs to your API, collect outputs, and train a student model to replicate your behavior. The student model is their stolen copy.

## The Value Proposition: Months of Work Through Queries

Model extraction is attractive because the cost-benefit ratio is asymmetric. You spent nine months and 800,000 dollars building a model. An attacker can approximate it for 15,000 dollars and two weeks. The attacker does not need to match your model perfectly. They need to get close enough to gain most of the value.

Consider the medical coding model. Your model achieves 94% accuracy. An extracted student model that achieves 89% accuracy is still enormously valuable to a competitor. That 89% is far better than the 76% accuracy of off-the-shelf models, and it is good enough to automate the majority of coding decisions. The attacker does not care about the five-point accuracy gap. They care that they went from 76% to 89% without building a dataset, hiring domain experts, or running seven training iterations.

The extraction process is also low-risk for the attacker. They are not breaking into your infrastructure. They are not exfiltrating files. They are using your public API exactly as intended. Every query looks legitimate. You cannot tell the difference between a legitimate customer querying your endpoint and an attacker distilling your model. The attack is invisible until it is complete.

## What Makes a Model Worth Stealing

Not every model is a high-value target. Attackers prioritize models where the extraction return on investment is clear. The factors that make a model attractive to attackers:

**Task-specific accuracy that exceeds general-purpose models.** If your model performs a task significantly better than GPT-5 or Claude Opus 4.5, that delta is worth stealing. A general-purpose model can already handle broad tasks. A fine-tuned model that achieves 94% on a narrow domain where general models achieve 76% has a defensible advantage. That advantage is what attackers want.

**Domain expertise that is expensive to replicate.** If your model encodes knowledge that requires months of domain expert time to label, attackers cannot easily rebuild it. Medical coding, legal contract analysis, and financial compliance are all domains where expertise is expensive. A model that automates this expertise is worth stealing because the alternative is hiring experts and building the dataset from scratch.

**Proprietary decision logic that reflects business rules.** If your model implements business rules that are not public, those rules are intellectual property. A fraud detection model that reflects your organization's risk tolerance, your historical fraud patterns, and your investigation workflows encodes knowledge that competitors do not have. Stealing the model means stealing those rules.

**Behavioral consistency that users trust.** If your model has a tone, a style, or a persona that users prefer, that behavior is an asset. A customer support model that is empathetic, concise, and brand-aligned took dozens of iterations to build. A competitor who extracts that tone bypasses the tuning process.

**Cost efficiency that creates margin.** If your model is smaller, faster, or cheaper to run than alternatives, that efficiency is valuable. A competitor who extracts your model's behavior into a smaller student model can match your performance at lower cost.

## Why You Are Vulnerable

You are vulnerable because your model is accessible. If you expose an API, you expose the model's behavior. Attackers do not need access to your weights, your training data, or your infrastructure. They only need access to your inputs and outputs. Every API response is a training example for their student model.

You are also vulnerable because the economics favor attackers. Extraction is cheap. Development is expensive. The attacker's budget is a fraction of yours, and their timeline is measured in weeks instead of months. The asymmetry is structural. You cannot make extraction impossible without making your model unusable. Every control you add increases friction for legitimate users. The attacker only needs to succeed once. You need to defend every query.

The next subchapter covers how query-based extraction works: the distillation workflow, the query strategies, and the surprisingly small number of queries required to approximate your model.
