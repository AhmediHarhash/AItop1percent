# 1.9 — Threat Modeling for AI Products

Threat modeling is the systematic process of identifying security risks before attackers find them. You map what you are protecting, who might attack it, how they might succeed, and which defenses you need to build. The output is not a security tool or a piece of code — it is a prioritized list of threats and mitigations that guides architecture decisions, testing strategy, and incident response planning. Teams that skip threat modeling discover their vulnerabilities in production. Teams that do it well discover them in design reviews.

Traditional software threat modeling focuses on access control, authentication, network boundaries, and data at rest. AI systems require all of that plus adversarial input handling, model behavior under manipulation, retrieval poisoning, tool misuse, and emergent failure modes that do not exist in deterministic systems. The attacker is not just trying to break your authentication. They are trying to make your model do something it was not designed to do by feeding it input your filters do not recognize as malicious. This requires a different mental model and a different threat taxonomy.

## Asset Identification: What Are You Protecting

The first step is listing everything an attacker could compromise, steal, manipulate, or destroy. In traditional software, the assets are obvious: user data, credentials, payment information, intellectual property. In AI systems, the list is longer and less intuitive. Your assets include the model itself, its training data, the retrieval corpus, tool integrations, user conversation history, prompt templates, system instructions, inference infrastructure, and the trust relationship between your system and its users.

**The model** is an asset. If an attacker can extract training data through prompt injection or membership inference, they gain information that should not be public. If they can manipulate the model's behavior through adversarial input, they control what your system does. If they can poison future training through data injection, they corrupt the model's capabilities. The model is not just software. It is a stateful artifact that encodes knowledge, and that knowledge can be stolen or corrupted.

**The retrieval corpus** is an asset. If your system uses RAG, the documents it retrieves are sensitive — customer records, proprietary research, internal communications. An attacker who can manipulate retrieval queries or inject malicious documents into the corpus can exfiltrate data or feed the model false information. The corpus is a database, and like any database, it requires access control, integrity validation, and audit logging.

**Tool access** is an asset. If your AI agent can call APIs, query databases, send emails, or execute code, an attacker who compromises the agent gains the same capabilities. The tools themselves are not the vulnerability — the vulnerability is the model's authority to invoke them. Every tool the model can use is a potential attack vector if the model can be tricked into calling it with adversarial arguments.

**User trust** is an asset. When your product generates harmful output, users lose confidence. When it leaks information, they stop sharing sensitive data. When it behaves unpredictably, they stop relying on it for critical tasks. Trust is fragile and expensive to rebuild. A successful attack that destroys trust damages the product even if no data is stolen and no systems are compromised. The attacker does not need to break your infrastructure. They just need to make users believe your product is unsafe.

Start by enumerating every component in your system architecture. For each component, ask what would happen if an attacker gained control of it, exfiltrated its contents, or prevented it from functioning. The answers define your asset list. Not all assets are equally valuable. Focus threat modeling effort on the assets whose compromise would cause the most damage.

## Threat Actor Identification: Who Attacks and Why

Different attackers have different goals, capabilities, and motivations. A security researcher looking for a bug bounty uses different techniques than an organized criminal group stealing data for resale. A competitor conducting corporate espionage invests more time and sophistication than a casual user experimenting with prompt injection. Your defenses should be designed for the threat actors most likely to target your product.

**Security researchers and red teamers** probe for vulnerabilities to demonstrate risk or earn bounties. Their attacks are exploratory, non-destructive, and often disclosed responsibly. They publish papers, blog posts, and conference talks. Their techniques become public knowledge, which means every other threat actor learns from them. The attack that a researcher demonstrates in March becomes a commodity technique by June. Researchers are not trying to harm your business — but they will find vulnerabilities, and when they do, everyone else learns how to exploit them.

**Malicious users and fraudsters** exploit your system for personal gain: account takeovers, payment fraud, content manipulation, resource theft. They do not need sophisticated technical skills — they copy attack patterns from forums, GitHub repositories, and social media. Their volume is high, but their persistence per target is low. They move to easier targets if your defenses work. The goal is to raise the cost of attack above the expected value of success.

**Organized criminal groups** operate at scale with automation. They target systems that store valuable data — credentials, payment information, PII — and resell it on dark markets. They invest in tooling, infrastructure, and reconnaissance. If your system handles financial transactions or identity data, you are a target. Their attacks are automated, persistent, and designed to evade basic defenses. They will not give up after the first failure. You need layered defenses and continuous monitoring.

**Competitors and corporate espionage actors** seek proprietary information: customer lists, pricing data, product roadmaps, model architectures. Their attacks are low-volume, high-precision, and patient. They blend into normal usage patterns. They exfiltrate data slowly to avoid detection. They compromise employee accounts to gain insider access. If your AI system provides access to competitive intelligence, you should assume this threat actor is active.

**Insiders and compromised accounts** are the hardest threat to defend against. An employee with legitimate access can exfiltrate data without breaking authentication. A user account compromised through phishing can issue queries that look normal but serve adversarial goals. Your access control and monitoring systems must account for the possibility that a valid session is being used maliciously. Insider threat detection requires behavioral analysis: does this usage pattern match the user's history? Is this data access anomalous? Is this query consistent with the user's role?

**Nation-state actors** target critical infrastructure, government systems, and strategic industries. Their capabilities include zero-day exploits, supply chain compromise, and long-term persistent access. If you operate in defense, healthcare, energy, or finance, this threat actor is relevant. Most commercial AI products do not face nation-state threats — but if you do, you need security capabilities that most startups cannot build in-house.

Identify which threat actors are realistic for your product. A consumer chatbot faces casual users and researchers. An enterprise AI assistant faces fraudsters, competitors, and insiders. A healthcare diagnostic tool faces researchers, criminals, and regulators. Design your threat model for the adversaries you actually face.

## Attack Vector Enumeration: How Could Each Asset Be Compromised

For each asset, enumerate the ways an attacker could compromise it. This is where the traditional STRIDE framework is useful. STRIDE stands for Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service, and Elevation of Privilege. Apply each category to every asset.

**Spoofing:** Can an attacker impersonate a legitimate user or system component? If your AI agent calls APIs, can an attacker spoof the agent's identity and invoke those APIs directly? If your retrieval system queries databases, can an attacker spoof a database response and feed false data to the model?

**Tampering:** Can an attacker modify data, prompts, or model outputs? If your system uses user-supplied context, can a prompt injection attack alter the model's instructions? If your RAG corpus is populated from external sources, can an attacker inject poisoned documents? If your system stores conversation history, can an attacker modify past messages to manipulate future model behavior?

**Repudiation:** Can an attacker perform an action and deny doing it? If your model makes a harmful decision, can you trace it to the input that caused it? If an attacker exfiltrates data, do your logs capture the query, the result, and the user session? Repudiation attacks succeed when logging is incomplete.

**Information Disclosure:** Can an attacker extract information they should not see? Prompt injection, membership inference, training data extraction, and retrieval exfiltration all fall into this category. If your model has access to private documents, can an adversarial query trick it into revealing them? If your model was trained on proprietary data, can an attacker reconstruct fragments through repeated queries?

**Denial of Service:** Can an attacker make your system unavailable or degrade its performance? Long inputs that consume excessive compute, infinite loops in agent reasoning, retrieval queries that exhaust database connections — these are all DoS vectors. The attack does not need to crash the system. It just needs to make it too slow or expensive to operate.

**Elevation of Privilege:** Can an attacker gain capabilities they should not have? If your AI agent has tool access, can a non-privileged user trick the agent into calling privileged tools on their behalf? If your system has tiered permissions, can an adversarial prompt bypass those tiers?

Walk through every component. For each STRIDE category, ask whether an attack is possible. If yes, describe the attack. If no, explain why not. The result is a comprehensive list of attack vectors. Not all vectors are equally likely or damaging. The next step is prioritization.

## Risk Assessment: Likelihood Times Impact

Each identified threat has a likelihood and an impact. Likelihood is the probability that the attack will succeed given your current defenses. Impact is the damage caused if it does. Risk is the product: likelihood times impact. High-likelihood, high-impact threats are your top priority. Low-likelihood, low-impact threats are deprioritized or accepted.

**Likelihood** is estimated based on attacker capability, system complexity, and defense strength. If a known attack technique bypasses your defenses with minimal effort, likelihood is high. If an attack requires sophisticated tooling, deep system knowledge, and multiple steps, likelihood is lower. If you have no detection or prevention mechanisms, any documented attack technique has high likelihood. The baseline assumption is that attackers know your architecture, have read your documentation, and have tested their attacks in similar environments.

**Impact** is estimated based on asset value and failure consequences. A training data leak that exposes PII has high impact — regulatory penalties, legal liability, reputational damage. A DoS attack that makes your system slow for ten minutes has low impact — no data loss, no lasting harm. A tool misuse attack that grants an attacker database write access has catastrophic impact — data corruption, operational disruption, incident response costs. Rate impacts on a scale: low, medium, high, critical. Be specific about what happens when the attack succeeds.

Combine likelihood and impact into a risk matrix. High-likelihood, high-impact threats go at the top of your mitigation list. High-impact, low-likelihood threats are next — even though they are unlikely, the damage justifies investment. High-likelihood, low-impact threats are handled with automated defenses. Low-likelihood, low-impact threats are documented and accepted.

A legal AI company ran this process in early 2025. They identified twelve distinct attack vectors. Three were high-likelihood, high-impact: prompt injection to extract confidential case documents, retrieval poisoning to feed false legal precedents into responses, and tool misuse to generate contracts with adversarial terms. Two were high-impact, low-likelihood: insider data exfiltration and model extraction. Five were low-impact: DoS through long prompts, minor hallucinations, refusal bypasses for non-sensitive queries. The team built defenses for the top five threats first. The low-impact threats were logged, monitored, and revisited after higher-priority risks were mitigated. The process took three weeks. The resulting security roadmap guided six months of engineering work.

## Mitigation Prioritization: Address Highest Risks First

Security work is never finished. There are always more vulnerabilities to patch, more edge cases to test, more defenses to build. You cannot fix everything. Prioritization is essential. Start with the threats that pose the greatest risk. Ship mitigations for those threats before addressing lower-priority concerns.

For each high-priority threat, identify mitigations. Mitigations fall into four categories: prevent, detect, respond, and accept. **Prevent** means blocking the attack before it succeeds: input validation, content filtering, access control, architectural isolation. **Detect** means identifying the attack in progress or after the fact: logging, anomaly detection, red team testing. **Respond** means minimizing damage once an attack is detected: incident response playbooks, automated rollback, rate limiting. **Accept** means acknowledging the risk and choosing not to mitigate it — reserved for low-impact threats where mitigation cost exceeds damage cost.

Most threats require multiple mitigations across multiple layers. A prompt injection threat might be mitigated with input filtering at the boundary, prompt structure hardening in the system instructions, retrieval access controls to limit what the model can see, and logging to detect successful attacks. No single layer is sufficient. Defense in depth means that if one layer fails, others still provide protection.

Prioritize mitigations with the highest risk reduction per unit of effort. If a single input validation rule blocks 80 percent of known prompt injection techniques, deploy it immediately. If a complex architectural change blocks 5 percent of remaining attacks, defer it until higher-impact work is complete. Track residual risk after each mitigation. The goal is not zero risk. The goal is acceptable risk given your product's threat profile.

## Continuous Updates: Threat Models Decay

Your threat model is not a one-time document. It is a living artifact that evolves as your system changes, as new attack techniques emerge, and as your threat landscape shifts. A threat model created six months ago is incomplete today. New vulnerabilities are discovered. New tools are integrated. New adversarial techniques are published. Your threat model must be updated continuously to reflect current reality.

Update your threat model in three scenarios. First, when you add new functionality. Every new feature expands the attack surface. A new tool integration creates new privilege escalation risks. A new data source creates new information disclosure risks. Before shipping the feature, update the threat model to account for the new risks.

Second, when a new attack technique is disclosed. When researchers publish a novel jailbreak, a new prompt injection pattern, or a membership inference improvement, add it to your threat model. Assess whether your system is vulnerable. If yes, build mitigations. If no, document why not. The gap between public disclosure and exploitation is shrinking. You have weeks, not months, to respond.

Third, after any security incident. If an attack reaches production, your threat model missed it. Conduct a postmortem. Identify what the threat model failed to predict. Update the model to include the new attack vector. Implement mitigations to prevent recurrence. The best threat models are informed by real incidents — your own and others in your industry.

Schedule quarterly threat model reviews. Bring together security, engineering, and product teams. Walk through the model. Identify changes since the last review. Update likelihood estimates based on new attacker capabilities. Update impact estimates based on new business context. Reprioritize mitigations. This is not a checklist exercise. It is a structured conversation about what could go wrong and what you are doing to prevent it.

Threat modeling is not a security team responsibility. It is a product development responsibility. Security teams facilitate the process and provide expertise, but the people who build the system must participate in identifying its risks. The engineer who designed the retrieval pipeline knows which queries could exfiltrate data. The product manager who defined tool access knows which privileges are most dangerous. The architect who chose the model knows which training data leakage risks exist. Threat modeling requires domain knowledge. The domain experts are the people building the product.

The output of threat modeling is a shared understanding of risk and a prioritized roadmap for reducing it. The process is not glamorous. It does not ship features. It does not generate revenue. It prevents catastrophic failures that cost more than the entire security budget. The teams that do it well avoid the incidents that define the teams that do not.

The next subchapter introduces the mindset required to identify vulnerabilities before attackers do: thinking like a red teamer.
