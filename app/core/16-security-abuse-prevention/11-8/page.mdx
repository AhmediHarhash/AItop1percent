# 11.8 — Defense: Dependency Review and Version Pinning

Your model may be secure. Your training pipeline is not. The average AI system in 2026 depends on forty to sixty Python packages, three to five deep learning frameworks, two to four data processing libraries, and a dozen infrastructure tools. Each dependency is maintained by different teams with different security practices. Each can introduce vulnerabilities. Each can be compromised. Each is a target for supply chain attacks. Treating dependencies as benign infrastructure is negligent. Treating them as untrusted code that must be continuously validated is the only defensible posture.

In August 2025, a fintech company discovered that their model training pipeline was exfiltrating fine-tuning data to an external server. The leak originated from a compromised version of a popular data augmentation library. The library maintainer's PyPI account had been hijacked three weeks earlier. The attacker uploaded a malicious version that included data exfiltration code hidden in a seemingly innocuous logging function. The malicious version was downloaded 14,000 times before it was detected. The fintech company had followed standard dependency management practices: they installed the latest version from PyPI, they ran automated tests, they deployed the pipeline to production. They never audited the dependency code. The exfiltration ran for nine days before an internal security researcher noticed unusual egress traffic patterns. The company lost PII on 180,000 customers and faced regulatory fines under GDPR. The incident was preventable with dependency review and version pinning.

AI-specific dependency risk is higher than traditional software dependency risk for three reasons. First, the AI ecosystem moves faster. New libraries are released weekly. Updates are frequent. Stability is not a primary concern for researchers and hobbyists who dominate the open-source AI tooling space. Fast-moving dependencies create pressure to update constantly, which increases exposure to compromised releases. Second, AI dependencies often execute arbitrary code during installation or runtime. Many libraries include C extensions, CUDA kernels, or JIT-compiled operations. These cannot be inspected with standard code review. Third, AI dependencies frequently access sensitive data. Fine-tuning libraries load training data. Embedding libraries process documents. Augmentation libraries transform examples. A compromised dependency has direct access to the most sensitive artifacts in your system.

Dependency review is the practice of auditing every external library before it is used and continuously monitoring it for vulnerabilities after it is integrated. It is not a one-time gate. It is an ongoing process that adapts as dependencies evolve.

## AI-Specific Software Bill of Materials

A **Software Bill of Materials** (SBOM) is an inventory of all dependencies used in a system. For traditional software, SBOMs list libraries, versions, and licenses. For AI systems, SBOMs must also include models, datasets, and preprocessing tools. An AI-specific SBOM has five layers: training dependencies, inference dependencies, data dependencies, model dependencies, and infrastructure dependencies.

Training dependencies are libraries used to fine-tune or train models. This includes deep learning frameworks like PyTorch, TensorFlow, or JAX. It includes training orchestration tools like Hugging Face Transformers, Axolotl, or DeepSpeed. It includes data loading libraries like datasets or torchdata. It includes optimization libraries like bitsandbytes or FSDP. Training dependencies are high-risk because they access training data directly and control model weight updates. A compromised training library can inject backdoors into the model, exfiltrate training data, or sabotage training runs.

Inference dependencies are libraries used to serve models in production. This includes inference servers like vLLM, TensorRT, or Triton. It includes tokenization libraries. It includes prompt formatting libraries. It includes output parsing libraries. Inference dependencies are high-risk because they process user input and control user-facing behavior. A compromised inference library can leak user prompts, manipulate model outputs, or create denial-of-service conditions.

Data dependencies are libraries used to preprocess, transform, or augment data. This includes text processing libraries like spaCy or NLTK. It includes embedding libraries like sentence-transformers. It includes augmentation libraries. It includes data validation libraries. Data dependencies are high-risk because they access raw data before it is logged, anonymized, or audited. A compromised data library can exfiltrate data without triggering downstream monitoring.

Model dependencies are the models themselves. This includes base models, fine-tuned models, embedding models, and any auxiliary models used for preprocessing or postprocessing. Model dependencies are high-risk because they are opaque. You cannot inspect model weights the way you inspect code. Compromised models can encode backdoors, memorize sensitive data, or exhibit manipulated behavior.

Infrastructure dependencies are the deployment and orchestration tools that manage the AI pipeline. This includes container runtimes like Docker. It includes orchestration platforms like Kubernetes. It includes cloud SDKs. It includes CI/CD tools. Infrastructure dependencies are high-risk because they have broad system access. A compromised infrastructure tool can access all models, all data, and all secrets.

The AI-specific SBOM records every dependency across all five layers. For each dependency, the SBOM includes: name, version, source repository, installation method, hash or checksum, first installation date, last update date, known vulnerabilities, and owning team. The SBOM is machine-readable, versioned, and stored in the security system of record. It is updated automatically on every deployment. It is the foundation for vulnerability scanning, license compliance, and incident response.

## Dependency Auditing for AI Pipelines

Dependency auditing answers three questions for each dependency: Is this dependency necessary? Is this dependency trustworthy? Does this dependency have known vulnerabilities? The audit happens twice: before the dependency is added and periodically after it is in use.

The necessity audit asks: do we actually need this library, or can we eliminate it? AI projects accumulate dependencies quickly. A researcher installs a library to test an idea. The idea works. The code ships to production. The library is now a permanent dependency. Six months later, no one remembers why it was added or whether it is still needed. Unnecessary dependencies expand the attack surface with no benefit. The necessity audit forces the team to justify every dependency. If the dependency provides functionality that is available in a library already in use, consolidate. If the dependency is used for a single function that could be implemented in-house, replace it. If the dependency was added for an experiment that never reached production, remove it. Dependency minimization is a core security principle.

The trustworthiness audit asks: who maintains this library, and can we trust them? Trustworthiness has multiple dimensions. First, maintainer identity. Is the library maintained by a known organization, a verified individual researcher, or an anonymous account? Libraries maintained by established organizations or researchers with public reputations are more trustworthy than libraries maintained by pseudonymous accounts with no history. Second, maintenance activity. Is the library actively maintained, with regular commits and releases, or is it abandoned? Abandoned libraries are high-risk because vulnerabilities are never patched. Third, community trust. Is the library widely used, with a large install base and active community, or is it niche with few users? Widely-used libraries are more likely to be scrutinized by security researchers. Fourth, security history. Has the library ever been compromised? Have security issues been reported and resolved promptly? A history of compromises or delayed fixes is a red flag.

The vulnerability audit asks: does this dependency have known security issues? This is answered through automated vulnerability scanning. Tools like pip-audit, safety, or Snyk scan Python dependencies against databases of known vulnerabilities. Similar tools exist for other ecosystems. Vulnerability scanning runs in three contexts: before adding a dependency, during CI/CD, and periodically in production. Before adding a dependency, scan the candidate version to identify known issues. During CI/CD, scan the full dependency tree to catch transitive vulnerabilities introduced by updates. In production, scan continuously to detect newly disclosed vulnerabilities in dependencies that have been stable for months.

When a dependency fails the audit — unnecessary, untrustworthy, or vulnerable — the team has three options: replace, mitigate, or accept risk. Replace means choosing a different library that provides the same functionality with better security properties. Mitigate means adding compensating controls: sandboxing the dependency, restricting its data access, or wrapping it in validation logic. Accept risk means documenting the decision to use the dependency despite the audit failure, along with the justification and any partial mitigations. Risk acceptance requires approval from security leadership. It is never the default.

## Version Pinning Strategies

Version pinning is the practice of specifying exact versions for every dependency, rather than accepting the latest version automatically. Unpinned dependencies create two failure modes. First, they introduce silent changes. A new version of a library may change behavior, introduce bugs, or include new vulnerabilities. If your system automatically installs the new version, these changes reach production without review. Second, unpinned dependencies create reproducibility failures. A model trained today may not be reproducible tomorrow if a dependency updates between training runs. Version pinning prevents both failures.

AI systems require pinning at three levels: direct dependencies, transitive dependencies, and runtime versions. Direct dependencies are the libraries you explicitly install: PyTorch, Transformers, vLLM. Pinning direct dependencies is standard practice in 2026. Most teams use requirements files with exact versions: torch equals 2.5.0, transformers equals 4.47.0. Transitive dependencies are the libraries your direct dependencies depend on. These are harder to pin because they are managed implicitly. Pinning transitive dependencies requires lock files: pip freeze on Python, yarn.lock on Node, Cargo.lock on Rust. Lock files capture the entire dependency graph at a specific point in time. They ensure that every transitive dependency installs at the same version on every machine. Runtime versions are the Python interpreter, CUDA toolkit, and system libraries. These are pinned through container images. A Docker image with python:3.11.6-cuda12.2 specifies exact versions for both Python and CUDA, ensuring consistent runtime behavior.

The tension in version pinning is between stability and currency. Pinned versions prevent surprises but fall behind on security patches. Unpinned versions stay current but introduce instability. The compromise is controlled updates. Pin all versions by default. Update dependencies on a regular schedule — weekly, biweekly, or monthly depending on risk tolerance. When updating, increment one dependency at a time, test thoroughly, and roll back if issues are detected. Never update all dependencies simultaneously. Bulk updates make it impossible to identify which dependency caused a regression.

Pinning strategies differ by environment. In production, pin everything. No exceptions. Production dependencies are frozen and updated only through deliberate deployment. In staging, pin everything but test updates before they reach production. Staging is the environment where new versions are validated. In development, pinning is optional but recommended. Developers who work with unpinned dependencies catch integration issues early, but they must resync with production versions before deploying. The development environment should mirror production as closely as possible.

Version pinning also applies to models. When a model provider releases a new version — GPT-5.1 to GPT-5.2, Claude Opus 4.5 to Opus 4.6 — the new version may change behavior. If your system automatically switches to the new version, users may experience different outputs without your knowledge. Pin model versions in production. Test new versions in staging. Upgrade deliberately. Model version pinning is implemented differently across providers. Some providers require explicit version strings in API calls. Others allow version aliasing, where you pin to a named version that maps to a specific snapshot. Others allow date-based pinning, where you pin to the model version available on a specific date. Know your provider's versioning semantics and pin at the most granular level available.

## Automated Vulnerability Scanning

Automated vulnerability scanning is the continuous process of checking dependencies against databases of known security issues. Scanning runs at multiple points in the development lifecycle: during local development, during CI/CD, and in production. Each scanning context has different objectives.

Local scanning runs when developers add or update dependencies. The scan checks the new dependency version against vulnerability databases and reports issues before the code is committed. Local scanning prevents vulnerable dependencies from entering the codebase. It is fast, noisy, and advisory. Developers see warnings but can override them if necessary. Local scanning tools include pip-audit, Snyk CLI, and safety. Integration with development environments — VSCode extensions, pre-commit hooks — makes scanning automatic and low-friction.

CI/CD scanning runs during build and test pipelines. The scan checks the full dependency tree, including transitive dependencies, and blocks merges or deployments if critical vulnerabilities are detected. CI/CD scanning is the enforcement gate. It is slower than local scanning because it analyzes the complete dependency graph. It is precise because it runs in a controlled environment with pinned versions. It fails builds when critical issues are found, forcing the team to remediate before deploying. CI/CD scanning tools include GitHub Dependabot, GitLab Dependency Scanning, and Jenkins plugins for vulnerability scanning.

Production scanning runs continuously on deployed systems. The scan checks the deployed dependency versions against newly disclosed vulnerabilities. When a new CVE is published, production scanning identifies which systems are affected within hours. Production scanning enables rapid response to zero-day vulnerabilities. It is passive — it does not block deployments — but it triggers alerts and incident workflows. Production scanning tools include cloud-native scanners like AWS Inspector, Azure Defender, and Google Security Command Center, as well as standalone tools like Trivy and Grype.

Vulnerability databases are not uniform. Different databases track different vulnerabilities. Some focus on language-specific ecosystems. Others focus on CVEs from the National Vulnerability Database. AI-specific vulnerabilities are under-represented in traditional databases. A vulnerability in a PyTorch operator or a fine-tuning library may not appear in the NVD for weeks or months. AI security requires monitoring multiple sources: language-specific databases like PyPI Advisory Database, ecosystem-specific feeds like Hugging Face model security advisories, and security mailing lists for AI libraries. Comprehensive scanning integrates multiple databases.

False positives are common in vulnerability scanning. A vulnerability may be reported in a library, but the vulnerable code path may not be reachable in your usage. A critical vulnerability in a web framework is irrelevant if you only use the framework's logging module. Automated scanning cannot distinguish between reachable and unreachable vulnerabilities. It reports everything. This creates alert fatigue. The solution is contextual triaging. Security teams review scan results, assess whether the vulnerability is exploitable in your context, and suppress false positives. Suppression decisions are documented and reviewed periodically. Triaging is manual and time-consuming, but it is necessary to keep scanning actionable.

## The Review Process for New Dependencies

Adding a new dependency is a security decision, not a technical convenience. The review process ensures that dependencies are evaluated before they are integrated. The process has four stages: proposal, security audit, approval, and monitoring.

The proposal stage requires the team to document why the dependency is needed. What functionality does it provide? Why cannot existing dependencies or in-house code provide this functionality? What is the risk tier of the system that will use this dependency? What data will the dependency access? The proposal forces the team to justify the addition and surfaces the security implications.

The security audit evaluates the dependency against the trustworthiness criteria described earlier: maintainer identity, maintenance activity, community trust, security history, and known vulnerabilities. The audit also includes code review for high-risk dependencies. If the dependency will process sensitive data or control critical behavior, security engineers review the source code for obvious backdoors, suspicious network calls, or insecure patterns. Code review is not exhaustive — auditing a 50,000-line library is impractical — but it catches blatant issues.

The approval decision is binary: approve with pinned version, approve with mitigations, or reject. Approval with pinned version means the dependency passes the audit and can be used immediately. Approval with mitigations means the dependency has minor issues but can be used with controls: sandboxing, restricted data access, or output validation. Rejection means the dependency fails the audit and cannot be used. The team must find an alternative or implement the functionality in-house. Approval decisions are recorded in the SBOM with a timestamp and approver identity.

Monitoring begins immediately after approval. The dependency is added to the continuous vulnerability scanning pipeline. If a new vulnerability is disclosed, the security team is alerted. If the maintainer abandons the project, the security team is alerted. If the dependency is updated, the team evaluates whether to upgrade. Monitoring ensures that approved dependencies remain trustworthy over time.

The review process applies to all dependencies, but the depth of review scales with risk. A low-risk utility library in a development tool receives lightweight review: automated vulnerability scan, quick trustworthiness check, approval. A high-risk training library that processes customer PII receives deep review: manual code audit, maintainer verification, sandbox testing, documented approval. Risk-appropriate review balances thoroughness with velocity.

## From Dependency Security to Vendor Resilience

Dependency review secures the libraries and tools your team controls. But many AI systems depend on external vendors: model providers, embedding providers, vector database services, annotation platforms, and monitoring tools. These vendors are dependencies, but they cannot be pinned, audited, or scanned the way libraries can. Vendor dependencies require a different security strategy: diversification, abstraction, and failover. When a vendor suffers an outage, a security breach, or a policy change, your system must continue operating. Vendor resilience is the final layer of supply chain defense.

