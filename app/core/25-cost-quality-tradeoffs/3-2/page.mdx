# 3.2 — Tiered Model Routing: Matching Model Power to Task Complexity

In March 2025, a legal technology company serving mid-market law firms was running every document analysis request through Claude Opus 4.5, their most capable model at the time. The system handled everything from simple contract clause identification — "does this agreement contain an indemnification clause?" — to complex multi-document reasoning tasks that required synthesizing information across hundreds of pages. The monthly model spend was $95,000. The CEO knew the number was high. The engineering team knew most queries were simple. But nobody had the bandwidth to build something smarter, and the simple queries worked fine on the expensive model, so the cost was treated as the price of doing business.

Then a new VP of Engineering arrived and asked a question nobody had asked before: what percentage of queries actually need the most expensive model? The data team ran the analysis. Of 1.4 million monthly requests, 68 percent were simple clause detection and extraction tasks. Another 22 percent were moderate complexity — summarization, comparison of two clauses, standard legal language interpretation. Only 10 percent were genuinely complex — multi-document synthesis, novel legal reasoning, ambiguous clause interpretation that required frontier-level capability. The team built a routing layer over the next six weeks. Simple queries went to GPT-5-nano. Moderate queries went to Claude Sonnet 4.5. Complex queries stayed on Claude Opus 4.6. The monthly bill dropped to $28,000. Quality metrics across all three tiers remained within one percentage point of the all-Opus baseline. The company saved $804,000 in the first year without a single user complaint.

## The Core Architecture of Tiered Routing

**Tiered model routing** is the practice of classifying incoming requests by complexity and sending each request to the cheapest model capable of handling it well. Instead of one model for all traffic, you maintain two to four model tiers and a routing mechanism that decides which tier each request belongs to. The architecture has three components: the model tiers, the router, and the quality monitor.

The model tiers are your menu of models, arranged from cheapest to most expensive. A typical three-tier setup in 2026 looks like this. The lightweight tier handles simple, well-defined tasks — classification, extraction, formatting, short factual answers. Models in this tier include GPT-5-nano, Gemini 3 Flash, or self-hosted Llama 4 Scout. They cost pennies per thousand requests. The mid-range tier handles moderate tasks — summarization, standard generation, multi-step extraction, comparisons. Claude Sonnet 4.5, GPT-5-mini, or Gemini 3 Flash with extended reasoning fit here. They cost roughly five to fifteen times more than the lightweight tier but deliver substantially better coherence and reasoning. The frontier tier handles complex tasks — novel reasoning, nuanced generation, ambiguous inputs, tasks where errors have high consequences. Claude Opus 4.6, GPT-5, or Gemini 3 Pro live here. They cost ten to fifty times more than the lightweight tier but bring capabilities that smaller models genuinely lack.

The router sits in front of these tiers and makes the per-request decision. It examines each incoming request and assigns it to a tier. The quality monitor sits behind the tiers and validates that routing decisions are correct — that requests routed to cheaper tiers are actually producing acceptable outputs. Without the quality monitor, you are flying blind. You might be saving money while silently degrading the experience for a subset of users.

## Building the Router: Rules First, ML Later

The router is the heart of the system, and teams consistently overthink it. You do not need a sophisticated machine learning classifier on day one. You need rules that capture the most obvious routing signals, deployed quickly, generating savings while you collect the data to build something smarter.

Start with heuristic rules based on observable request properties. Query length is the simplest signal: short queries — fewer than 50 tokens — are almost always simple tasks that lightweight models handle well. Task type, if your API distinguishes between classification, summarization, and generation endpoints, is an even stronger signal. Customer tier matters in B2B products: free-tier users can receive lightweight model responses while enterprise customers get the frontier model. Input complexity indicators like the number of documents in a RAG context, the presence of technical jargon, or the specificity of the instruction all correlate with the model power required.

A basic rule-based router built in a day might look like this: if the task is classification or extraction and the input is under 200 tokens, route to the lightweight tier. If the task is summarization or standard generation, route to the mid-range tier. Everything else goes to the frontier tier. This simple router, applied to the legal technology company's traffic, would have captured roughly 60 percent of the savings a perfect router would achieve. Sixty percent of optimal, built in one day, is better than zero percent of optimal built never.

After the rule-based router has run for two to four weeks, you have labeled data: requests that were routed to each tier, along with quality scores from your evaluation pipeline. Use this data to train a lightweight classifier — a small model or even a logistic regression — that predicts the minimum tier each request needs based on the request features. The classifier will find routing signals that your rules missed: certain phrasings that correlate with complexity, certain topic patterns that the lightweight model handles poorly, certain input structures that require frontier reasoning. The trained classifier typically improves routing accuracy by 15 to 25 percent over rules alone, capturing an additional 20 to 30 percent of remaining savings.

The important principle is iteration, not perfection. A rule-based router generating 60 percent of optimal savings is infinitely better than a perfect router that takes three months to build. Ship the rules. Collect the data. Train the classifier. Improve over time. This is the same good-enough philosophy from Chapter 1 applied to your cost infrastructure.

## The Economics of Tiered Routing

The math of tiered routing is compelling because most AI workloads follow a power law: a large majority of requests are simple, and a small minority are complex. If 70 percent of your traffic is simple enough for the cheapest tier, the savings are dramatic even if the remaining 30 percent stays on the most expensive model.

Consider a concrete example. A customer support chatbot handles 500,000 requests per month. Without routing, all requests go to Claude Opus 4.6 at $5 per million input tokens and $25 per million output tokens. With an average of 600 input tokens and 400 output tokens per request, the monthly cost is roughly $6,500. Significant, but manageable. Now imagine the same chatbot scaled to five million requests per month as the company grows. Without routing, the monthly cost rises to $65,000. With routing that sends 70 percent of requests to Claude Haiku 4.5 at $1 per million input tokens and $5 per million output tokens, 20 percent to Claude Sonnet 4.5, and 10 percent to Claude Opus 4.6, the monthly cost drops to approximately $16,000. That is a 75 percent reduction. Over a year, the savings are $588,000.

The savings scale linearly with traffic. At one million requests per month, the annual savings from routing might be $60,000 — meaningful but not transformative. At fifty million requests per month, the annual savings exceed $3 million. This is why routing becomes more valuable as you grow. The fixed cost of building and maintaining the router stays constant while the savings grow with traffic. At scale, the routing infrastructure pays for itself in days.

But the economics are not pure savings. There is a cost to building and maintaining the router. The initial build — rules, classifier, quality monitoring — typically takes two to four weeks of engineering time. Ongoing maintenance — retraining the classifier as traffic patterns change, adjusting tier boundaries as model capabilities evolve, monitoring quality across tiers — requires one to two days per month. For teams with more than a few hundred thousand monthly requests, this investment pays back within the first month. For teams with fewer requests, the payback takes longer but usually arrives within a quarter.

## The Quality Risk: Misroutes and Their Cost

Tiered routing introduces a risk that does not exist in a single-model architecture: misroutes. A misroute occurs when the router sends a request to a tier that lacks the capability to handle it well. The cheap model produces a poor response. The user has a bad experience. If misroutes happen frequently enough, the cost savings from routing are offset by quality degradation.

Misroutes come in two flavors. **Downward misroutes** send a complex request to a cheap model. These are the dangerous ones. The cheap model does not refuse the request — it tries its best and produces an output that looks plausible but is wrong, incomplete, or incoherent. The user might not realize the output is poor until they act on it. A legal clause analysis that misses a critical exception. A financial summary that omits a material risk. A customer support response that gives incorrect instructions. Downward misroutes erode trust, and trust erosion has a cost that exceeds the savings from the cheaper model.

**Upward misroutes** send a simple request to an expensive model. These are wasteful but harmless. The user gets a high-quality response. The only cost is the unnecessary model spend. Upward misroutes reduce your savings but do not hurt the user experience. In the tradeoff between downward and upward misroutes, you should always bias toward upward. A routing system that occasionally sends simple requests to the expensive model wastes some money. A routing system that occasionally sends complex requests to the cheap model damages the product.

The practical implication is that your router should have an asymmetric confidence threshold. When the router is confident that a request is simple, route it to the cheap tier. When the router is uncertain, route it to the expensive tier. The default should be the expensive model, with cheap routing as the optimization. This means you capture savings only on the requests where you are confident the cheap model is sufficient, and you avoid the quality risk on requests where you are not sure. A confident router that captures savings on 60 percent of traffic with zero downward misroutes is far more valuable than an aggressive router that captures savings on 85 percent of traffic with a 5 percent downward misroute rate.

## Detecting Misroutes in Production

You cannot prevent all misroutes. You can detect them quickly and correct the routing rules before they affect too many users. Misroute detection requires a quality monitoring layer that samples outputs across all tiers and flags potential failures.

The simplest approach is **LLM-as-judge evaluation**. For a random sample of requests routed to the cheap tier — typically 1 to 5 percent — send the same request to the frontier tier as well. Compare the two outputs using an automated evaluator or a lightweight judge model. If the outputs are substantially different, the cheap model may have produced an inadequate response. Flag the request for human review. Over time, the flagged requests reveal patterns: certain query types, certain input structures, certain topics where the cheap model consistently underperforms. Feed these patterns back into the router as exclusion rules — "never route queries about topic X to the cheap tier."

A more sophisticated approach uses **confidence scoring**. Many models can be prompted to output a confidence score alongside their response. If the cheap model's confidence on a routed request falls below a threshold, automatically re-route the request to the frontier tier and serve the better response. This adds latency for re-routed requests but prevents the user from ever seeing a low-confidence cheap-model response. The re-routing rate tells you how well your router is calibrated. If 2 percent of cheap-tier requests get re-routed, your router is well-calibrated. If 15 percent get re-routed, your router is too aggressive and the savings are less than they appear, because the re-routed requests are hitting the expensive model anyway, plus the wasted cost of the cheap model's unused response.

Track your misroute rate as a first-class metric, alongside your quality metrics and cost metrics. Set an alert threshold: if the estimated misroute rate exceeds 3 percent, trigger a review of routing rules. The misroute rate is the price you pay for routing savings. Keep it low, and the savings are pure. Let it creep up, and you are trading user trust for dollars — a trade that never ends well.

## The Tier Boundary Problem

One of the hardest design decisions in tiered routing is where to draw the boundaries between tiers. Which tasks are "simple enough" for the lightweight tier? Which are "moderate enough" for the mid-range tier? The boundaries are not fixed properties of the tasks. They are properties of the models. A task that was too complex for a lightweight model six months ago might be well within its capabilities today, because model capabilities improve with each release.

The practical approach is to start with conservative boundaries — route only the most obviously simple tasks to the cheap tier — and widen them over time as you gather evidence. Your first router should route maybe 40 to 50 percent of traffic to the cheap tier, even if your analysis suggests that 70 percent could go there. Run this conservative configuration for two to four weeks. Monitor quality. Review misroutes. If the misroute rate is near zero, widen the boundary to capture the next tranche of traffic. Repeat until the misroute rate reaches your tolerance threshold.

This iterative widening is safer than an aggressive launch. If you launch with aggressive routing and quality drops, the team panics, disables routing, and declares the approach a failure. If you launch conservatively and gradually expand, each expansion is backed by data, and the team builds confidence in the system incrementally. The end state is the same — 65 to 75 percent of traffic on the cheap tier — but the path is lower risk and builds organizational trust in the routing infrastructure.

Tier boundaries also need to shift when you update models. When you upgrade your lightweight tier from GPT-5-nano to a newer or better-tuned variant, the boundary should expand because the new model handles more cases well. When you add a new task type to your product, the initial routing for that task type should be conservative — default to the expensive tier — until you have data on how cheaper models handle it. Tier boundaries are not set-and-forget. They are a living configuration that evolves with your models, your traffic, and your quality requirements.

## Multi-Dimensional Routing

Simple routers make one-dimensional decisions: this request goes to tier one, two, or three. More sophisticated routers make multi-dimensional decisions that consider not just task complexity but also latency requirements, user value, and business context.

Latency-aware routing sends time-sensitive requests to faster models regardless of complexity. If a user is waiting for a real-time response, a mid-range model that responds in 400 milliseconds might be preferable to a frontier model that responds in 2.5 seconds, even if the frontier model would produce a marginally better answer. The user's patience has economic value: a slow response increases abandonment, which reduces engagement, which reduces revenue. Routing for latency means the router considers not just "which model is cheapest?" but "which model delivers the best quality within the latency budget?"

User-value routing differentiates based on who is making the request. In a B2B product, an enterprise customer paying $50,000 per year receives frontier-model responses because the cost of a bad experience — potential churn of a high-value account — dwarfs the model cost. A free-tier user exploring the product receives mid-range model responses because the cost of a bad experience — potential loss of a user who is not yet paying — is lower. This is not about treating free users poorly. It is about allocating expensive resources where the return on that expense is highest.

Context-aware routing uses information from the conversation or session history to make routing decisions. The first message in a customer support conversation — where the system needs to understand the user's intent and set the tone — goes to a more capable model. Subsequent messages in the same conversation — clarifying questions, confirmations, simple follow-ups — go to a cheaper model. The user experiences high quality at the moment that matters most, and the cost averages down across the conversation.

## The Routing Tax: When Routing Costs More Than It Saves

Routing is not free. The router itself consumes compute. If you use an ML classifier, it adds latency and cost to every request. If you use a small model as the router, that model call has its own token cost. At low volumes, the cost of the routing infrastructure can exceed the savings from routing.

The breakeven point depends on your traffic volume and the price spread between tiers. As a rule of thumb, if your monthly model spend is below $2,000, the engineering and infrastructure cost of building and maintaining a routing layer is unlikely to pay back within a year. Save routing for when you scale. If your monthly spend is between $2,000 and $10,000, a simple rule-based router with minimal infrastructure overhead is worth building. If your monthly spend exceeds $10,000, a full ML-based routing system with quality monitoring and iterative optimization will pay back within weeks.

There is also a complexity tax. Routing turns a single-model system into a multi-model system. You now have three sets of prompts to maintain, three sets of model-specific quirks to manage, three sets of output formats to normalize. When a model provider updates their API, you need to verify compatibility across all tiers. When you change a prompt, you need to test it across all models. The operational overhead is real and must be weighed against the cost savings. Teams that build routing systems without investing in the operational tooling to manage them efficiently often find that the maintenance burden erodes the financial benefit within six to twelve months.

The solution is to treat your routing infrastructure as a product, not a one-off optimization. Invest in automated testing across tiers. Build dashboards that show per-tier quality and cost. Automate the model update and prompt testing process. The teams that do this find routing to be one of the highest-ROI infrastructure investments they make. The teams that treat it as a hack — a quick cost cut without long-term investment — find it becomes a maintenance headache that nobody wants to own.

## The Organizational Challenge

The hardest part of routing is not technical. It is convincing the team that different users can receive different model tiers without it being unfair or risky. Product managers worry that cheaper models will degrade the experience. Engineers worry that multi-model systems are harder to debug. Quality teams worry that routing creates blind spots in evaluation. These concerns are legitimate, and addressing them requires more than technical arguments.

The most effective approach is to run a controlled experiment. Take 10 percent of your traffic and route it using the proposed tiered system. Keep 90 percent on the current single-model setup. Run the experiment for two weeks. Compare quality metrics, user satisfaction metrics, and cost. If the routed traffic shows equivalent quality at lower cost, the data speaks for itself. If the routed traffic shows a quality drop, you know exactly where to adjust the routing boundaries. Either way, the experiment produces evidence that moves the conversation from opinion to data.

Start with the least risky routing decision — the task type where you are most confident the cheap model is sufficient — and demonstrate success. Use that success to build credibility for expanding routing to more task types. The first routing win creates organizational momentum. Each subsequent win expands the scope. Within three to six months, routing becomes an accepted part of the architecture rather than a controversial cost-cutting measure.

Tiered routing captures savings by matching model power to task complexity within your existing model lineup. But the savings from routing have a ceiling — you are still limited to the commercially available model tiers. The next frontier in cost optimization is creating your own small models that match frontier quality on your specific tasks. That is the domain of distillation, which we explore next.
