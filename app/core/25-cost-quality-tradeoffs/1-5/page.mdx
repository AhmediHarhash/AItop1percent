# 1.5 — Measuring What Matters: Quality-Adjusted Cost as the Core Metric

The single most important metric in AI economics is not cost per request. It is not monthly spend. It is not even cost per user. The metric that actually tells you whether your system is economically healthy is **quality-adjusted cost** — the cost to produce a successful outcome, not the cost to run a model call. This distinction sounds subtle, but it changes how you make every cost decision. A team optimizing for cost per request will choose the cheapest model. A team optimizing for quality-adjusted cost will choose the model that produces the cheapest successful outcomes, and those are often very different models at very different price points.

Most AI teams track the wrong cost metric. They watch their monthly invoice, divide by the number of requests, and report a cost-per-request number to leadership. That number is dangerously misleading. It treats every request as equal — the request that produced a perfect answer and the request that produced garbage the user immediately discarded. It treats a system with a 95% success rate the same as a system with a 60% success rate, as long as the token cost per call is identical. It hides the true economics of quality failures behind an average that nobody should trust.

Quality-adjusted cost exposes what raw cost metrics hide. It forces you to answer the question that matters: how much does it actually cost your business to produce one unit of value?

## The Failure Tax: Why Cheap Models Are Often Expensive

Imagine two models serving the same summarization task. Model A costs $0.002 per request. Model B costs $0.008 per request. A team optimizing for raw cost per request will choose Model A without hesitation — it is four times cheaper. But Model A produces acceptable summaries only 68% of the time. The other 32% are either incomplete, inaccurate, or miss the key points the user needed. Model B produces acceptable summaries 94% of the time.

Now calculate the quality-adjusted cost. For Model A, the cost per successful outcome is $0.002 divided by 0.68, which equals approximately $0.0029. For Model B, the cost per successful outcome is $0.008 divided by 0.94, which equals approximately $0.0085. Model A still looks cheaper per successful outcome, but we are not done accounting.

When Model A fails, what happens? The user either retries manually, which costs another $0.002 and adds latency that degrades the experience. Or the user contacts support, which costs $4 to $8 per ticket depending on your support infrastructure. Or the user silently loses trust and eventually churns, which costs the lifetime value of that customer. The $0.002 failure is not free. It carries downstream costs that dwarf the original request cost.

Industry experience consistently shows that the downstream cost of a quality failure is 50 to 200 times the cost of the original request. A $0.002 failed request that triggers a support ticket costing $5 has a true cost of $5.002. A $0.002 failed request that contributes to a customer churning has a true cost measured in hundreds or thousands of dollars. When you fold these downstream costs into the calculation, Model A — the "cheap" model — becomes dramatically more expensive than Model B.

This is the **failure tax**. Every quality failure levies a tax on your system that does not appear on your AI invoice but appears everywhere else: in support costs, in user retention, in engineering time spent investigating complaints, in product reputation. The failure tax is invisible to teams that only track cost per request. It is glaringly obvious to teams that track quality-adjusted cost.

## Calculating Quality-Adjusted Cost: The Core Formula

The basic formula is simple enough to explain in a sentence: divide your total cost by your success count. If you spent $10,000 this month and produced 400,000 successful outcomes out of 500,000 total requests, your quality-adjusted cost is $10,000 divided by 400,000, which equals $0.025 per successful outcome. Your raw cost per request is $10,000 divided by 500,000, which equals $0.020. The quality-adjusted cost is 25% higher than the raw cost because 20% of your requests produced no value.

That is the basic version. The more useful version incorporates downstream costs. The full quality-adjusted cost formula works like this: take your total AI spend, add the cost of retries, add the cost of support tickets caused by AI failures, add the cost of manual interventions or human escalations, and divide by the number of outcomes that met your quality bar without requiring any of those corrections. This gives you the true cost per unit of value.

For the basic version, you need two inputs: total spend and success count. Total spend is straightforward — it is your model API costs plus any infrastructure costs for running inference. Success count requires a definition of success, which means you need an evaluation framework that classifies each output as successful or not. If you do not have that evaluation framework, you cannot calculate quality-adjusted cost, and you cannot make informed cost-quality decisions. This is why evaluation infrastructure is a prerequisite to cost optimization, not a nice-to-have.

For the full version, you need to attribute downstream costs to AI failures. This is harder but not impossible. Start by tagging support tickets that result from AI output quality issues. Track how many requests trigger retries. Measure how many requests require human review or escalation. Assign dollar costs to each of these events. Even rough estimates are better than ignoring downstream costs entirely. A team that estimates support tickets cost $5 each and tags 200 AI-related tickets per month knows they are paying an additional $1,000 per month in failure tax, even if the estimate is off by 30%.

## Why Raw Cost Metrics Mislead Decision-Makers

Raw cost metrics create three specific failure modes in decision-making.

The first failure mode is premature downgrading. A team sees their monthly AI spend climbing and decides to switch to a cheaper model. They look at the cost per request before and after: $0.010 becomes $0.003. The CFO is delighted. But the cheaper model fails on 25% of requests that the previous model handled correctly. Support tickets increase. User satisfaction drops. Three months later, the team switches back to the original model, having lost users and credibility in the process. The raw cost metric told them the downgrade was working. The quality-adjusted cost metric would have told them it was a disaster from week one.

The second failure mode is false optimization. A team implements prompt caching and reduces their cost per request by 40%. Leadership celebrates. But the cache is serving stale results for 12% of requests — cases where the context has changed but the cached response does not reflect it. Users receive outdated information. The quality-adjusted cost actually increased because the success rate dropped by more than the cost savings. The team was optimizing for the wrong metric and made the system worse while the dashboard showed improvement.

The third failure mode is misallocated investment. When leadership looks at a cost dashboard showing cost per request by feature, they naturally focus on the most expensive features. But the most expensive features might also be the most successful — they cost more because they use better models and produce higher quality. The features that actually need investment are the ones with high quality-adjusted cost: they might have low raw cost per request but abysmal success rates, meaning they deliver terrible value per dollar. Raw cost metrics direct investment toward the wrong features.

These failure modes are not hypothetical. They happen in organizations every quarter. The fix is not to stop tracking raw cost — you still need it for budgeting and infrastructure planning. The fix is to make quality-adjusted cost the primary metric for all cost-quality decisions. Raw cost tells you what you are spending. Quality-adjusted cost tells you what you are getting for it.

## Building a Quality-Adjusted Cost Dashboard

A quality-adjusted cost dashboard requires four data streams: model costs, request outcomes, retry events, and downstream impact signals. Assembling these streams is not trivial, but each component is buildable with standard observability tools.

The model cost stream comes from your inference layer. Every request should log the model used, the input token count, the output token count, and the calculated cost based on your provider's pricing. If you are using multiple models through a router, each route should log independently. This data should flow into your observability stack in real time, not aggregated monthly from invoices. Invoice-based cost tracking is too slow and too coarse for operational decisions.

The request outcome stream comes from your evaluation layer. Every response should be classified as successful or unsuccessful based on your quality criteria. For some applications, this is binary: the output parsed correctly and passed validation, or it did not. For others, it is graded: the output scored above your quality threshold on an automated evaluator, or it fell below. The classification method depends on your application, but the key requirement is that every request gets a quality label, not just a sample. Sampling introduces uncertainty that undermines the metric.

The retry stream is the simplest to capture. Log every retry event with the original request ID, the reason for the retry, and the outcome of the retry attempt. This lets you calculate the retry multiplier — the average number of model calls per user-visible request. A retry multiplier of 1.0 means no retries. A retry multiplier of 1.25 means you are making 25% more model calls than your users see. Multiply your base cost per request by the retry multiplier to get your true cost per user-visible request, before quality adjustment.

The downstream impact stream is the hardest to build and the most valuable. It requires connecting AI quality events to business impact events. When a user contacts support after receiving a bad AI output, that support ticket needs to be tagged as AI-related. When a user retries a task manually, that behavioral signal needs to be captured. When a user downgrades or churns within 30 days of a quality incident, that correlation needs to be logged. None of these connections happen automatically. They require deliberate instrumentation and cross-team cooperation between engineering, support, and product analytics.

Once you have all four streams, the dashboard shows three tiers of quality-adjusted cost. Tier one is the basic quality-adjusted cost: total AI spend divided by successful outcomes. This is available immediately and useful for most decisions. Tier two adds retry costs: total AI spend including retries divided by successful outcomes after retries. This reveals the hidden cost of brittle parsing and unreliable outputs. Tier three adds downstream costs: total AI spend plus attributed business costs divided by fully successful outcomes. This is the true cost of value delivery and the metric that should drive strategic decisions about model selection, quality investment, and pricing.

## Comparing Models the Right Way

The standard model comparison is a table of prices per million tokens. Input tokens, output tokens, maybe a benchmark score. This comparison is worse than useless — it is actively misleading. It tells you the cost of computation, not the cost of outcomes. Two models with identical per-token pricing can have wildly different quality-adjusted costs if their success rates differ.

The right way to compare models is to run both on your actual workload — not a benchmark, not a synthetic test, but your real production traffic or a representative sample of it. Measure the success rate of each model against your quality criteria. Calculate the quality-adjusted cost for each. Then compare.

Here is what this looks like in practice. A customer service team evaluated three models for their ticket response generation. Model A cost $0.012 per request and produced acceptable responses 91% of the time. Model B cost $0.004 per request and produced acceptable responses 72% of the time. Model C cost $0.007 per request and produced acceptable responses 88% of the time.

Raw cost ranking: Model B is cheapest, then Model C, then Model A. This is the ranking most teams would use.

Quality-adjusted cost ranking: Model A costs $0.0132 per successful outcome, Model C costs $0.00795 per successful outcome, and Model B costs $0.00556 per successful outcome. By this basic calculation, Model B still looks best.

But now add the failure tax. Each failed response triggers a human review costing $3.50 on average. Model A: 9% failure rate means $0.315 in expected human review cost per request, giving a fully loaded cost of $0.327 per request, or $0.359 per successful outcome. Model B: 28% failure rate means $0.98 in expected human review cost per request, giving a fully loaded cost of $0.984 per request, or $1.367 per successful outcome. Model C: 12% failure rate means $0.42 in expected human review cost per request, giving a fully loaded cost of $0.427 per request, or $0.485 per successful outcome.

The ranking inverted completely. Model A — the most expensive per token — is the cheapest per successful outcome when you account for the cost of failures. Model B — the cheapest per token — is the most expensive by a factor of nearly four. The team that chose Model B to save money would spend four times more per successful outcome than the team that chose Model A. This is not a contrived example. This is the math that plays out in every AI system where quality failures have real costs.

## Quality-Adjusted Cost Across Different Product Types

The failure tax varies dramatically by product type, which means quality-adjusted cost favors different strategies for different applications.

In consumer products with low per-failure costs — a chatbot that sometimes gives a mediocre but harmless answer — the failure tax is low. Users shrug, rephrase, and try again. The downstream cost of a failure is a slightly worse user experience and a marginal increase in churn risk. For these products, the cheaper model with a lower success rate might genuinely be the better economic choice. The failure tax does not outweigh the cost savings.

In enterprise products where failures trigger human workflows, the failure tax is high. A contract analysis tool that misses a key clause forces a lawyer to review the document manually, costing $200 to $500 in billable time. A medical coding assistant that assigns the wrong code triggers an audit cycle costing $150 to $300 in compliance staff time. For these products, the quality-adjusted cost heavily favors higher-quality models because every failure is expensive.

In safety-critical applications, the failure tax is potentially unlimited. A clinical decision support tool that provides an incorrect medication interaction warning could contribute to patient harm. A financial advisory tool that gives wrong tax guidance could expose the company to regulatory action. For these products, quality-adjusted cost is almost irrelevant as a cost optimization metric — you set the quality floor at near-perfection and optimize cost only within that constraint.

Understanding your product's failure tax profile is a prerequisite to making quality-adjusted cost decisions. If you do not know what a failure costs in your specific context, you cannot calculate quality-adjusted cost accurately, and you will make decisions based on incomplete data. The first step is always to measure or estimate the downstream cost of a quality failure for your specific application.

## Using Quality-Adjusted Cost to Set Quality Floors

Quality-adjusted cost provides a principled way to determine your quality floor — the minimum acceptable success rate for your system. The principle is straightforward: your quality floor is the success rate at which further quality reductions increase your total cost rather than decrease it.

Below a certain success rate, the failure tax exceeds the savings from using a cheaper model. That crossover point is your economic quality floor. Operating below it is not just a quality problem — it is a cost problem. You are paying more per successful outcome, not less, because the failures are eating your budget.

To find the crossover point, calculate the quality-adjusted cost at several success rate levels using your estimated failure tax. Plot the curve. You will see that quality-adjusted cost decreases as you move from a 60% success rate toward 80%, because the failure tax drops faster than model cost increases. At some point the curve hits a minimum — that is your economic sweet spot. Below that success rate, you are overpaying for failures. Above that success rate, you are overpaying for quality. The minimum is where you want to operate.

This analysis often produces surprising results. Teams that assumed their quality floor was 90% discover that the economic sweet spot is actually 85% — the cost of the last five percentage points of quality exceeds the savings from reduced failures. Other teams that assumed they could operate at 75% discover that the failure tax at that level is so high that they are better off paying three times more per token for a model that hits 92%.

The point is not to set your quality floor by gut feel. It is to calculate it. Quality-adjusted cost gives you the math to make the decision with evidence rather than intuition.

## Tracking Quality-Adjusted Cost Over Time

A single quality-adjusted cost calculation is a snapshot. Useful, but not sufficient. The real power comes from tracking quality-adjusted cost over time, because it reveals trends that neither cost tracking nor quality tracking alone can show.

A rising quality-adjusted cost means one of three things: your model costs increased, your success rate decreased, or your downstream failure costs increased. Each cause requires a different response. If model costs increased because your provider raised prices, the response is to evaluate alternative models. If success rate decreased because of data drift or prompt degradation, the response is to investigate and fix the quality issue. If downstream failure costs increased because you changed your support process or user expectations shifted, the response is to recalibrate your failure tax estimates.

A falling quality-adjusted cost is the goal, but it needs decomposition. Did quality-adjusted cost fall because you switched to a cheaper model? Verify that the success rate held. Did it fall because you improved your success rate? That is the best kind of improvement — it means you are delivering more value per dollar. Did it fall because you reduced downstream costs through better error handling? That is also a real improvement.

The trend line is more important than any single number. A team whose quality-adjusted cost is $0.05 and falling is in better shape than a team whose quality-adjusted cost is $0.03 and rising. The first team is improving. The second team is degrading. The absolute number matters less than the direction.

Track quality-adjusted cost weekly. Review it in your regular engineering review or product sync. Make it as visible as uptime or latency. When quality-adjusted cost spikes, treat it like you would treat a latency spike — investigate immediately, identify root cause, fix it before the next billing cycle compounds the damage.

## The Organizational Shift: From Cost Center to Value Metric

Adopting quality-adjusted cost as your primary metric requires an organizational shift. Finance is accustomed to tracking AI spend as a cost center — a line item to be minimized. Product is accustomed to tracking quality as a user experience metric — a number to be maximized. Quality-adjusted cost sits at the intersection, and neither team owns it naturally.

The shift starts with a shared definition. Finance, product, and engineering must agree on what counts as a successful outcome. This is harder than it sounds. Finance might consider any completed API call a success. Product might consider only responses that users rated positively. Engineering might consider only responses that passed automated quality checks. These definitions produce wildly different success rates and therefore wildly different quality-adjusted cost numbers. The team must pick one definition — ideally the one closest to actual user value — and use it consistently.

The shift continues with shared accountability. If quality-adjusted cost is rising, who is responsible? If model costs rose, that is infrastructure's problem. If success rates dropped, that is engineering's problem. If downstream costs increased, that is product's problem. Quality-adjusted cost creates shared ownership because no single team controls all the inputs. This is a feature, not a bug. Shared ownership forces cross-functional conversation about tradeoffs, which is exactly where those conversations should happen.

The shift is complete when quality-adjusted cost appears in budget reviews alongside raw spend. When the CFO asks "what is our cost per successful outcome?" instead of "what is our AI spend this month?", the organization has made the transition from tracking expenses to tracking value. That transition changes every cost decision the company makes, from model selection to prompt engineering to quality investment.

The next subchapter introduces the mechanism for making these decisions explicit and auditable: the tradeoff register, a living document that records every significant cost-quality decision along with its rationale, alternatives, and review triggers.
