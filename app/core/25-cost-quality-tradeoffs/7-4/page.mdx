# 7.4 — Async Processing: Trading Latency for Dramatic Cost Reduction

Moving a workload from synchronous to asynchronous processing typically reduces its cost by 40 to 70 percent. The savings come from three sources: batch pricing discounts, off-peak compute utilization, and the freedom to use slower but cheaper models when there is no user waiting for an immediate response. For any workload that does not require an answer in the same interaction, async is the single largest cost lever available. It is not an optimization. It is a category change — moving work from "expensive and urgent" to "cheap and patient." Yet most teams leave this lever untouched, running every inference call synchronously because that is how the system was built when latency was the only metric anyone measured.

The reason async saves so much money is not a single discount. It is the compounding of three independent cost reductions that multiply together. Understanding each mechanism separately lets you calculate the real savings for your specific workload and decide which portions of your system deserve the shift.

## The Three Cost Reduction Mechanisms

The first mechanism is **batch API pricing**. Both OpenAI and Anthropic offer batch inference APIs at 50 percent of their synchronous pricing. You submit a file of requests, the provider processes them within a 24-hour window, and you retrieve the results. The discount applies to both input and output tokens across every model in the lineup. GPT-5 input tokens drop from $1.25 to $0.625 per million. Claude Sonnet 4.5 output tokens drop from $15 to $7.50 per million. Google Cloud's Vertex AI offers comparable discounts on batch prediction jobs. The discount exists because batch processing lets providers schedule work during periods of low utilization, filling idle GPU capacity that would otherwise generate zero revenue. You get a 50 percent discount. They get revenue from hardware that was sitting idle. Both sides win.

The second mechanism is **off-peak infrastructure utilization**. If you self-host models or run inference on reserved cloud instances, async processing lets you schedule heavy workloads during nights and weekends when your real-time traffic is low. Your GPU fleet that handles 10,000 synchronous requests per minute during business hours drops to 800 requests per minute at 2 AM. That unused capacity represents compute you are already paying for. Routing async batch jobs to fill those idle hours means you extract more work from the same infrastructure without provisioning additional hardware. The effective cost per inference drops because the denominator — total inferences processed — increases while the numerator — total infrastructure cost — stays constant.

The third mechanism is **model flexibility**. When a user is waiting for a response, you are constrained to models that respond within your latency budget. That often means paying for a fast, expensive model even when a slower, cheaper one would produce equivalent quality. When nobody is waiting, the latency constraint disappears. A classification task that runs in 200 milliseconds on GPT-5-mini at $0.30 per million tokens might run in 2 seconds on Llama 4 Scout self-hosted at $0.04 per million tokens. The quality is identical for a binary classification. The cost difference is nearly 8x. Async processing unlocks this flexibility because the 1.8-second latency difference is irrelevant when the result will be consumed hours later.

These three mechanisms compound. A workload that gets a 50 percent batch pricing discount, runs on already-paid-for off-peak infrastructure, and uses a cheaper model can see 70 to 85 percent total cost reduction compared to the same work done synchronously on a frontier model during peak hours. The math is not theoretical. It is arithmetic that every team can run against their own workload distribution.

## Identifying Async-Eligible Workloads

Not every workload can be async. The dividing line is simple: does the user need the result in the same interaction? If the user submits a query and sits waiting for a response, that workload is synchronous. If the user submits a request and can come back later — minutes, hours, or the next day — that workload is async-eligible.

The surprising discovery for most teams is how much of their inference volume falls into the async-eligible category once they actually audit it. The audit process is straightforward. Pull your last month of inference logs. For each request type, answer three questions. First, did a user directly trigger this request and wait for the response? Second, could the user have received a "processing" confirmation and retrieved the result later? Third, is this request triggered by an internal system rather than a user action? Any request where the answer to the second or third question is yes is a candidate for async processing.

**Evaluation pipelines** are the most obvious async candidate. When you run your model's outputs through a judge model to score quality, relevance, safety, or factual accuracy, nobody is sitting at a screen waiting for the eval result. The production model has already served the user. The evaluation is a background quality measurement that can run minutes or hours later. Most teams run evaluation synchronously out of habit, processing each response through the judge immediately after generation. Switching evaluation to batch processing cuts its cost in half with zero user impact.

**Content generation queues** are another natural fit. Product descriptions, marketing copy, email drafts, report sections — any workflow where a user requests content that will be reviewed before use does not need synchronous generation. A content team that requests 500 product descriptions does not need all 500 to appear instantly. They need them by tomorrow morning. That 12-hour window turns a $150 synchronous job into an $75 batch job.

**Data enrichment pipelines** process existing records through a language model to add metadata, classify content, extract entities, or generate summaries. A company enriching 100,000 customer support tickets with sentiment scores, category labels, and summary fields has no user waiting on any individual ticket. The entire pipeline can run as a nightly batch. At synchronous rates, processing 100,000 tickets through GPT-5-mini might cost $600. At batch rates, $300. At batch rates on a cheaper model like Gemini 3 Flash, potentially $90.

**Training data preparation** — generating synthetic examples, cleaning and reformatting datasets, running quality checks on labeled data — is inherently async. The fine-tuning run that will consume this data is days or weeks away. There is no reason to process it at synchronous speed and synchronous cost.

**Bulk classification and moderation** is another category. Scanning a backlog of user-generated content for policy violations, categorizing a document archive, labeling a corpus for search — these are batch jobs by nature. Teams that process them synchronously because the moderation API only offers a synchronous endpoint are leaving money on the table. Wrapping the synchronous API in a batch scheduler that manages rate limits and collects results is a few hours of engineering work that pays for itself in the first week.

## Architecture Patterns for Async Processing

Moving a workload from synchronous to async requires more than just switching from a real-time API call to a batch API call. You need infrastructure to manage the lifecycle of async jobs: submission, tracking, completion notification, and result delivery.

The simplest pattern is the **job queue with polling**. The user or system submits a request, which is placed in a queue — Amazon SQS, Redis-backed Celery, Google Cloud Tasks, or any standard message queue. A worker service pulls jobs from the queue, processes them through the batch API or a self-hosted model, and writes results to a data store. The requesting system polls the data store periodically to check for completion. This pattern is straightforward to build but has a latency floor: the result is available only when the next polling interval hits. For workloads where "within 15 minutes" is fast enough, polling works fine. For workloads that need faster notification, you need a push mechanism.

The **webhook callback pattern** eliminates polling. When a job completes, the processing service sends an HTTP callback to a registered URL, notifying the requesting system that results are available. Both OpenAI and Anthropic support webhook-style notifications for batch API completion. This pattern reduces the gap between job completion and result availability to seconds rather than the polling interval. It is the standard pattern for production async pipelines that feed into downstream workflows — when the enrichment batch finishes, the webhook triggers the indexing pipeline that makes the enriched data searchable.

The **email or notification delivery pattern** is appropriate for human-consumed results. A user requests a complex report — a 30-page market analysis, a regulatory compliance audit, a competitive landscape summary. The system acknowledges the request, processes it in batch, and delivers the completed report via email, in-app notification, or a download link. This pattern is common in enterprise products where the output is a document rather than a conversational response. The user expects to wait — they initiated a big job and understand it takes time. The critical design requirement is setting clear expectations: "Your report will be ready within 2 hours" is a promise your batch pipeline needs to reliably keep.

For teams using provider batch APIs directly, the architecture is even simpler. You construct a file containing your requests in the provider's batch format, upload it, receive a batch identifier, and poll for completion or register a callback. The provider handles queuing, scheduling, and processing. Your engineering effort is limited to building the request file, managing the batch lifecycle, and routing results to the right destination. This is the lowest-engineering-cost path to async savings, and for many teams it is sufficient.

## The Economics in Detail: A Worked Example

Consider a mid-market SaaS company running three distinct inference workloads. Their customer-facing chatbot handles 200,000 synchronous queries per day using Claude Sonnet 4.5 at $3 per million input tokens and $15 per million output tokens. Their evaluation pipeline scores every chatbot response on four dimensions using Claude Opus 4.6 at $15 per million input tokens and $75 per million output tokens. Their nightly data enrichment pipeline processes 50,000 new records through GPT-5-mini at $0.30 per million input tokens and $1.25 per million output tokens.

The chatbot is genuinely synchronous — users are waiting for responses. It stays synchronous. No change.

The evaluation pipeline processes 200,000 responses per day across four dimensions — 800,000 evaluation calls. Average input is 500 tokens and output is 100 tokens per evaluation. Daily cost: input tokens total 400 million at $15 per million equals $6,000, output tokens total 80 million at $75 per million equals $6,000. Daily evaluation cost: $12,000. Monthly: $360,000. Switching to the batch API at 50 percent discount cuts this to $180,000 per month — a $180,000 monthly savings. But the team can go further. Evaluation does not require Claude Opus 4.6 for all four dimensions. Two of the four dimensions — format compliance and length adherence — can use Claude Haiku 4.5 at $0.80 per million input and $4 per million output with 97 percent agreement. Moving those two dimensions to Haiku in batch cuts their cost from $6,000 per day to approximately $320 per day. The combined evaluation savings: from $360,000 per month to roughly $105,000 per month. A 71 percent reduction.

The data enrichment pipeline processes 50,000 records per day. Average input is 800 tokens and output is 200 tokens per record. Daily cost at synchronous GPT-5-mini rates: input tokens total 40 million at $0.30 per million equals $12, output tokens total 10 million at $1.25 per million equals $12.50. Daily cost: $24.50. Monthly: $735. Modest savings from batch pricing alone — $367 per month. But the team switches the enrichment pipeline to self-hosted Llama 4 Scout running on their existing GPU infrastructure during off-peak hours. The marginal inference cost drops to near zero because the GPUs are already provisioned for daytime chatbot traffic and sit idle at night. Monthly enrichment cost drops to approximately $50 in electricity and maintenance overhead. Savings: $685 per month. Small in absolute terms, but the pattern demonstrates the compounding of batch scheduling with model flexibility.

Total monthly savings across both async-eligible workloads: approximately $255,685. That is real money recaptured from workloads that were running synchronously for no good reason.

## Queue Backlogs and Processing Guarantees

Async processing introduces a new failure mode that synchronous systems do not have: the backlog. When requests arrive faster than the processing pipeline can consume them, the queue grows. If the queue growth is sustained — because of a traffic spike, a provider rate limit reduction, or a processing service failure — jobs start missing their delivery promises.

The most common cause of backlog buildup is rate limiting. Provider batch APIs have rate limits separate from their synchronous APIs, but those limits still exist. A team that assumes "batch means unlimited throughput" submits 500,000 requests in a single batch file and discovers the provider processes them at a fixed rate, completing the batch in 18 hours instead of the expected 4. The downstream pipeline that expected results by 6 AM gets them at 2 PM. Reports that should have been in user inboxes at the start of the business day arrive after lunch.

The mitigation is capacity planning for your async pipeline with the same rigor you apply to synchronous systems. Know your provider's batch throughput limits. Know your self-hosted inference throughput at off-peak capacity. Size your queue depths to match. Set monitoring alerts on queue depth and processing lag. When queue depth exceeds your processing capacity for the target delivery window, you need a release valve — either throttling new submissions, temporarily routing overflow to synchronous processing at higher cost, or extending the delivery promise.

The second common failure is stale results. A data enrichment pipeline that runs nightly produces results that are current as of last night. If a record changes during the day and the enrichment result is consumed before the next nightly run, the consuming system uses stale data. For many workloads this is acceptable — a sentiment score computed 12 hours ago is still useful. For some workloads it is not. The decision of whether staleness is tolerable must be made per workload, not as a blanket policy.

## User Expectation Management

The hardest part of async processing is not the engineering. It is the product design. Users who are accustomed to synchronous responses need clear signals when a workload shifts to async delivery.

The worst async experience is the silent one. The user clicks "generate report" and nothing happens for 45 minutes. They click again. And again. Each click submits another async job, tripling the processing cost and creating duplicate results. The system technically works, but the user experience is hostile.

The best async experience has three properties. First, immediate acknowledgment: the moment the user submits a request, the interface confirms receipt and sets an expectation. "Your compliance report is being generated. You will receive an email when it is ready, typically within 90 minutes." Second, progress visibility: a status page, progress indicator, or periodic update that confirms the job is advancing. "Processing 847 of 2,400 records" tells the user the system is working, not stuck. Third, reliable delivery: the result arrives within the promised window every time. A promise of "within 2 hours" that occasionally takes 6 hours trains users not to trust async workflows, and they will demand synchronous processing for workloads that should be async.

Teams that invest in these three UX elements discover something counterintuitive: users are more satisfied with a well-designed async experience that delivers a polished result in 30 minutes than with a synchronous experience that delivers a partial or lower-quality result in 5 seconds. The async model gives the system permission to use more compute, more validation, and more refinement — producing better output at lower per-unit cost. When the user expectation is "this will take some time and you will be notified," the system can afford to be thorough.

## Hybrid Architectures: Sync-First, Async-Behind

Not every workload is purely synchronous or purely async. Many production systems benefit from a hybrid architecture where the user-facing response is synchronous but expensive background processing happens asynchronously.

A customer support chatbot illustrates this pattern. The user asks a question and receives an immediate response — synchronous, fast, using a cost-efficient model. Behind the scenes, an async pipeline kicks off: the response is evaluated for quality and safety, the conversation is classified for routing to a human agent if needed, the interaction is enriched with metadata for analytics, and a follow-up recommendation is generated for the next interaction. None of this background processing blocks the user. All of it runs at batch pricing on cheaper models.

This hybrid pattern is particularly valuable for systems that need both speed and depth. The synchronous layer provides the speed — immediate response to the user. The async layer provides the depth — thorough evaluation, enrichment, and preparation for future interactions. The synchronous layer costs whatever the latency budget demands. The async layer costs 50 to 80 percent less because it has no latency constraint.

The architectural requirement is clean separation between the synchronous response path and the async processing path. The user request triggers both paths, but the synchronous path returns independently of the async path. If the async pipeline fails, the user never knows — they already have their response. If the async pipeline is slow, the user does not care — they are not waiting for it. This decoupling is what makes the hybrid model robust. The sync path has tight SLAs. The async path has loose SLAs. Each path is optimized for its own constraint profile.

## Measuring Async ROI

The return on investment from async processing is straightforward to measure, but you need to track the right metrics to avoid self-deception.

The primary metric is **cost per inference**, split by processing mode. Track the average cost of a synchronous inference and the average cost of an async inference for equivalent workloads. The gap between these two numbers, multiplied by the volume of work shifted from sync to async, is your gross savings. If you shifted 400,000 evaluations per day from synchronous at $0.015 each to batch at $0.007 each, your daily savings are $3,200.

The secondary metric is **delivery reliability** — the percentage of async jobs that complete within the promised window. If you promise "within 2 hours" and 92 percent of jobs meet that promise, you have an 8 percent failure rate that is eroding user trust and may be generating support tickets. The cost of those support tickets, the re-submissions, and the user churn attributable to broken promises must be subtracted from your gross savings to get net savings. A team that saves $3,200 per day on inference but spends $800 per day on support overhead from unreliable async delivery is really saving $2,400.

The third metric is **async coverage** — the percentage of your total inference volume that runs asynchronously. Most teams start at zero and gradually shift workloads. Tracking coverage over time tells you whether you are fully exploiting the async opportunity or leaving savings on the table. If your audit identified 60 percent of workloads as async-eligible but only 25 percent of volume actually runs async, you have a 35-percentage-point gap that represents unrealized savings.

The final metric is **quality parity** — the difference in output quality between sync and async processing for the same workload. If you switched to a cheaper model for async processing, you need to verify that the cheaper model produces equivalent results. Run your standard evaluation suite on both sync and async outputs monthly. If async quality is within your tolerance threshold, the savings are real. If async quality has degraded, you are not saving money — you are spending less money on worse output, which is a different thing entirely.

Async processing trades the constraint users feel most — waiting — for the constraint they feel least — delayed delivery of background results. The savings are real, the engineering is manageable, and the opportunity is almost certainly larger than most teams realize. But latency is not the only dimension with geographic variance. Where your requests are processed — which data center, which region, which country — creates its own collision between cost, speed, and legal compliance, and that is where we turn next.
