# 7.2 — Latency Budgets and Cost Implications: Where Milliseconds Have Dollar Signs

In March 2025, a conversational AI team at a mid-size insurance company set a 500-millisecond end-to-end latency SLA for their claims assistant chatbot. The number came from a product manager who had read an article about user engagement dropping when response times exceed half a second. No one questioned whether insurance customers filing claims — people who had just been in car accidents or had pipes burst in their basements — actually needed half-second responses. The engineering team took the SLA at face value and built accordingly. Meeting that target required GPT-5-mini on dedicated inference endpoints rather than shared capacity, deployment to three geographic regions rather than one, a warmed prompt cache refreshed every 15 minutes, and a stripped-down retrieval pipeline with pre-computed embeddings in a low-latency vector store. The infrastructure bill landed at $78,000 per month. When a new engineering director joined the team five months later and ran a user experience study, they found that claimants were perfectly satisfied with responses delivered in 1.5 to 2 seconds. The difference between 500 milliseconds and 1,800 milliseconds was imperceptible to users who were already stressed, already thinking about their next sentence, and already accustomed to waiting for human agents. The team relaxed the SLA to 1,200 milliseconds, moved to shared inference endpoints in a single region, and cut infrastructure cost to $13,000 per month. They saved $65,000 per month — $780,000 per year — because nobody had asked the only question that matters before setting a latency target: what response time do our actual users actually need?

This story is not unusual. It is the default. **Latency budgets** — the maximum acceptable time from request to response for a given user interaction — are the most consequential cost decisions most AI teams make, and they are almost always set by instinct, competitive benchmarking, or borrowed conventional wisdom rather than by analysis of what users actually tolerate.

## What a Latency Budget Actually Contains

A latency budget is not a single number. It is a stack of time allocations across every component in your pipeline, and the total must add up to less than whatever target you have set. Understanding this stack is essential because different components have different cost curves — some are cheap to speed up, some are ruinously expensive, and some cannot be accelerated at any price.

The **network round-trip** is the floor. A user in New York talking to a server in Virginia has roughly 10 to 20 milliseconds of network latency. A user in Singapore talking to a server in Virginia has 200 to 250 milliseconds. No engineering optimization, no amount of spending, can make photons travel faster through fiber-optic cable. The only way to reduce network latency is to move the server closer to the user, which means multi-region deployment, which means multiplying your infrastructure cost.

**Preprocessing** comes next. In a RAG system, this includes embedding the user's query, running a retrieval search, fetching and ranking documents, and assembling the context. In a simple chatbot, preprocessing might be minimal — just constructing the prompt from the conversation history. Preprocessing typically consumes 50 to 400 milliseconds depending on the pipeline's complexity. A vector search against a million-document index takes longer than one against ten thousand documents. A reranking step that uses a cross-encoder model adds 100 to 200 milliseconds. Each preprocessing component has its own cost and latency profile.

**Model inference** is usually the largest chunk. It breaks into two distinct phases: **time-to-first-token** (TTFT), which is how long the model takes to begin generating output, and **inter-token latency** (ITL), which is how fast tokens arrive after the first one. TTFT depends on the model size, the input length, and the serving infrastructure. For a frontier model like Claude Opus 4.6 or GPT-5.2, TTFT on shared infrastructure can be 500 to 1,500 milliseconds. For a lightweight model like Gemini 3 Flash or Claude Haiku 4.5, TTFT is typically 80 to 200 milliseconds. ITL determines how long the full response takes to complete — a 200-token response at 30 milliseconds per token takes 6 seconds after the first token appears. If you are not streaming, the user waits for the entire response. If you are streaming, they see tokens arrive progressively, and TTFT becomes the primary perceived-latency metric.

**Postprocessing** covers everything after the model generates its output: safety filtering, format validation, citation extraction, response parsing, and any business logic that runs on the output. Postprocessing typically consumes 20 to 100 milliseconds unless you are running a secondary model — for example, a safety classifier or a quality judge on every response — in which case postprocessing can add 200 to 800 milliseconds depending on the classifier's speed.

The total latency is the sum of all of these. A system with 20 milliseconds of network latency, 150 milliseconds of preprocessing, 600 milliseconds of TTFT, 3 seconds of generation for a 200-token response, and 50 milliseconds of postprocessing has a total latency of roughly 3.8 seconds. Without streaming, the user sees nothing for 3.8 seconds. With streaming, they see the first token after 770 milliseconds and the full response after 3.8 seconds. Same total time. Very different experience.

## The Cost Curve of Latency Reduction

Reducing latency is not linearly expensive. It follows a curve that is shallow at first and then steep — the first big improvements are cheap, and the last small improvements are ruinously expensive. Understanding where you sit on this curve determines whether further latency investment is justified.

The first latency reduction — from "no optimization" to "basic optimization" — is often free or nearly free. Enabling prompt caching, pre-computing embeddings, and tuning your vector database index can cut 30 to 50 percent of preprocessing latency at minimal cost. Switching from synchronous postprocessing to parallel postprocessing — running the safety classifier while the model is still generating — can eliminate postprocessing latency entirely from the user's perceived response time. These are engineering improvements, not infrastructure investments.

The second latency reduction — from "basic optimization" to "well-optimized" — requires moderate infrastructure investment. Moving from shared inference endpoints to dedicated endpoints reduces TTFT by eliminating queuing delays. Adding prompt caching at the provider level — where you pay a reduced rate for cached input tokens — reduces both latency and cost simultaneously. Deploying to a second geographic region to serve users closer to your infrastructure cuts network latency for those users. These improvements typically reduce total latency by another 30 to 40 percent at an infrastructure cost increase of 50 to 100 percent. This sounds expensive, but if your starting infrastructure cost is $8,000 per month, a 100 percent increase to $16,000 is likely worth it for a 35 percent latency improvement.

The third latency reduction — from "well-optimized" to "aggressive" — is where the cost curve turns steep. Cutting the last 200 milliseconds requires dedicated GPU instances with reserved capacity, ultra-low-latency networking, custom inference runtimes with optimizations like speculative decoding or continuous batching tuned to your workload, and deployment to three or more regions with traffic routing that sends each request to the lowest-latency endpoint. The infrastructure cost can triple or quadruple compared to the "well-optimized" tier. You are now spending $50,000 or more per month to shave 200 milliseconds off a response time that was already acceptable to most users at the previous tier.

The fourth latency reduction — from "aggressive" to "extreme" — is reserved for voice AI, real-time trading, and a handful of other use cases where sub-200-millisecond total latency is a hard requirement. At this level, you may need custom hardware, on-premises GPU deployments, pre-computed response trees, or small fine-tuned models that sacrifice significant quality for raw speed. Monthly costs can reach six figures for a single product. This is the far end of the cost curve, and only a small fraction of AI products need to be here.

The key question is not "how fast can we make it?" but "where on this curve does the marginal latency improvement stop being worth the marginal cost?" For most products, the answer is somewhere in the second tier — well-optimized, with moderate infrastructure investment, and latency that is good enough for the use case without being impressive enough to present at a conference.

## Setting Latency Budgets from User Tolerance, Not Engineering Ambition

The most expensive mistake in latency budgeting is setting the target based on what feels "fast" rather than what users actually need. Different use cases have radically different latency tolerances, and matching your budget to the actual tolerance prevents massive over-investment.

**Interactive chat interfaces** have the most commonly misunderstood tolerance. Product teams often benchmark against web search, where users expect results in 200 to 400 milliseconds. But chat is not search. In a conversation, the user submits a message and then shifts their attention — they re-read their own message, glance at the conversation history, start thinking about their next question. Research on chatbot user experience consistently shows that users tolerate 2 to 4 seconds of response time in text-based chat without measurable drops in satisfaction. The key factor is not speed but predictability — a consistent 2.5-second response is perceived as faster than a response that sometimes comes in 500 milliseconds and sometimes takes 5 seconds. An insurance claims assistant, a customer support chatbot, or an internal knowledge bot can target 1.5 to 3 seconds total latency and deliver an excellent user experience.

**Voice and speech interfaces** have genuinely tight latency requirements because silence on a call is perceived as a system failure. In human conversation, the typical turn-taking gap is 200 to 500 milliseconds. Voice AI systems need to match this, which means the total time from the user finishing their sentence to the system beginning to speak should be under 800 milliseconds, ideally under 500. This drives the extreme end of the latency cost curve and is the primary reason voice AI is expensive to serve. Every 100 milliseconds you add to voice response time degrades the conversational experience measurably.

**Autocomplete and inline suggestions** require the lowest latency of any AI interface. A code completion tool like GitHub Copilot or an email auto-suggest needs to present options while the user is still thinking about their next keystroke. Latency targets of 100 to 300 milliseconds are common. Above that, the suggestion arrives after the user has already typed the next word, making it useless. This latency requirement is why autocomplete tools use small, fast models — typically fine-tuned models under 10 billion parameters — rather than frontier models.

**Batch and report generation** has effectively unlimited latency tolerance. A compliance report generated overnight, a batch of document summaries produced for a Monday meeting, a dataset enrichment job running over the weekend — these can take hours and nobody cares. The latency budget for batch work is not measured in milliseconds. It is measured in "before the deadline." This tolerance unlocks the cheapest possible inference: batch APIs at 50 percent discounts, off-peak scheduling, queue-based processing with no real-time infrastructure.

**Background processing within interactive flows** falls in between. If your chatbot kicks off a retrieval-augmented generation pipeline while the user is reading a previous response, the pipeline has a soft latency budget equal to the user's reading time — typically 5 to 15 seconds. This is not foreground latency. The user is not waiting. The system is pre-computing the next likely response. Background budgets are where significant cost savings hide, because work that happens while the user is occupied does not need to be fast.

## Allocating the Budget Across Pipeline Stages

Once you have a total latency budget — say, 2 seconds for a conversational interface — the next step is **budget allocation**: deciding how much of that budget each component gets. This is where teams either save or waste significant money, because the cost of accelerating different components varies enormously.

The allocation principle is simple: give the most budget to the most expensive-to-accelerate component, and optimize aggressively on the components that are cheap to speed up. In most AI pipelines, model inference is the most expensive to accelerate — moving from a 1.2-second TTFT to a 400-millisecond TTFT requires either a smaller model or dedicated serving infrastructure, both of which cost real money. Preprocessing, by contrast, is often cheap to accelerate — pre-computing embeddings, tuning index parameters, or adding a cache can cut retrieval time by 50 percent at minimal cost.

A practical allocation for a 2-second budget in a RAG-based chatbot might look like this. Network: 50 milliseconds, assuming same-region serving. Preprocessing and retrieval: 300 milliseconds, achievable with pre-computed embeddings and a tuned vector index. Model TTFT: 800 milliseconds, achievable with a mid-tier model on standard shared endpoints. Token generation for the remainder of the response: up to 800 milliseconds if streaming, irrelevant if the user sees the first token at the 1,150-millisecond mark. Postprocessing: 50 milliseconds, running safety checks in parallel with token generation. Total time to first visible token: approximately 1,150 milliseconds. Total time to complete response: approximately 2 to 4 seconds depending on response length. User sees text flowing after just over a second, which feels responsive.

Now consider what happens if you tighten the budget to 500 milliseconds for the same pipeline. Network stays at 50 milliseconds — you cannot change physics. That leaves 450 milliseconds for everything else. Preprocessing needs to drop to 100 milliseconds, which requires pre-computed retrieval results or aggressive caching. Model TTFT needs to drop to 250 milliseconds, which requires either a small lightweight model or dedicated serving with warmed caches. Postprocessing needs to run in under 100 milliseconds, which may require removing the safety classifier or running it asynchronously after the response is sent. You have now changed your model choice, added caching infrastructure, and potentially weakened your safety checks — all to hit a latency target that your users may not have needed. The 500-millisecond budget forced expensive compromises on cost, quality, and safety that the 2-second budget avoided entirely.

## The Latency-Cost Multiplier Table

Different latency targets imply different infrastructure requirements, which imply different monthly costs. While exact figures depend on traffic volume and model choice, the relative multipliers are remarkably consistent across teams and use cases.

Taking a mid-tier model on shared infrastructure as the baseline — call it a cost multiplier of 1.0 — the typical cost trajectory as you tighten latency looks like this. A 3-second latency target with a mid-tier model on shared infrastructure runs at the 1.0 baseline. A 1.5-second target with the same model on dedicated endpoints runs at roughly 1.5 to 2.0 times the baseline — you are paying for reserved capacity. A 1-second target that requires a smaller model plus dedicated endpoints runs at 1.2 to 1.8 times — cheaper model, but more infrastructure. A 500-millisecond target that requires a small model, multi-region deployment, and aggressive caching runs at 2.5 to 4.0 times — you are paying for geographic distribution and cache infrastructure. A 200-millisecond target that requires a tiny model, edge deployment, and pre-computed responses runs at 5 to 10 times — you are building custom infrastructure for an extreme requirement.

These multipliers compound with traffic. A system handling 100,000 requests per day at the 1.0 baseline might cost $3,000 per month. Tightening from 3 seconds to 500 milliseconds — a 4.0 multiplier — pushes the cost to $12,000. That $9,000 difference is the price of 2.5 seconds. Whether those 2.5 seconds are worth $9,000 per month depends entirely on whether your users value them. For a voice assistant, absolutely. For an internal document summarizer, absolutely not.

The multiplier table is most useful as a negotiation tool. When a product manager requests a tighter latency target, pull up the multiplier table and say: "Moving from 2 seconds to 500 milliseconds triples our infrastructure cost. Here is what our user research says about satisfaction at each latency tier. Do you want to spend the additional $24,000 per month for an improvement our users cannot distinguish from the current experience?" That conversation, grounded in specific numbers, prevents the most common source of latency-budget waste: targets set by aspiration rather than evidence.

## The Hidden Costs of Latency SLAs

The sticker price of a latency target — the model cost plus infrastructure cost to meet it on average — understates the true cost. Latency SLAs introduce hidden costs that compound over time.

The first hidden cost is **tail latency management**. A 500-millisecond SLA at the 50th percentile is easy to hit. A 500-millisecond SLA at the 95th percentile is hard. A 500-millisecond SLA at the 99th percentile is expensive. The tail of the latency distribution — the slowest 1 to 5 percent of requests — is driven by queuing delays, cold starts, network jitter, garbage collection pauses, and other infrastructure-level variability. Meeting a strict SLA at the 99th percentile requires over-provisioning: more capacity than you need on average, so that even during traffic spikes and infrastructure hiccups, the slowest requests still meet the target. Over-provisioning can add 30 to 100 percent to your infrastructure cost compared to serving the same traffic without a strict tail-latency guarantee.

The second hidden cost is **monitoring and alerting infrastructure**. Once you commit to a latency SLA, you need to measure compliance — which means latency tracking at every pipeline stage, real-time dashboards, alerting when percentile thresholds are breached, and on-call engineers who respond to latency degradation. This monitoring infrastructure costs money to build, operate, and staff. A team with a strict latency SLA typically dedicates one to two engineers partially to latency monitoring and optimization — an ongoing labor cost that does not appear on the infrastructure invoice.

The third hidden cost is **engineering velocity drag**. Every code change must be evaluated for latency impact. Adding a new preprocessing step, expanding the system prompt, or upgrading the model version all require latency testing before deployment. Teams with tight latency SLAs often maintain separate staging environments that replicate production latency characteristics, run latency regression tests on every pull request, and block deployments that increase P95 latency by more than a threshold. This infrastructure and process overhead slows down the team's ability to ship improvements, which is a real cost even though it does not show up on an invoice.

The fourth hidden cost is **quality sacrifices made to meet latency**. This is the subtlest and most damaging hidden cost. When a team is under pressure to meet a latency SLA, the temptation is to cut quality-improving features that add latency: remove the chain-of-thought reasoning step, truncate the context window, skip the reranking pass in retrieval, disable the safety classifier. Each cut shaves milliseconds and degrades quality. Over time, the system accumulates quality debt driven by latency constraints — and because quality degradation is harder to measure than latency degradation, the debt goes unnoticed until users complain or metrics decline.

## Renegotiating Latency Budgets

If your current latency budget is costing more than it should, the fix is straightforward but politically difficult: renegotiate the SLA.

Start with the data. Run a user satisfaction analysis that measures the relationship between response time and user satisfaction, engagement, or task completion rate. Plot satisfaction against latency in 500-millisecond buckets. In almost every case, you will find a plateau — a range of latency where satisfaction is flat — followed by a drop-off. The plateau tells you how much latency headroom you have. If satisfaction is flat from 500 milliseconds to 1,800 milliseconds and drops at 2,500 milliseconds, your SLA should be somewhere in the 1,500 to 2,000 range — not 500 milliseconds.

Present the data alongside the cost savings. "We can relax our SLA from 500 milliseconds to 1,500 milliseconds with no measurable impact on user satisfaction, saving $52,000 per month in infrastructure costs." That framing turns a technical discussion into a business discussion. No product leader will defend an SLA that costs $52,000 per month and delivers no user value. But they will defend it if you present the request as "engineering wants to make things slower" without the data to show that "slower" is indistinguishable from "fast" for their users.

The renegotiation should happen at least annually, and ideally every time you change models, change infrastructure, or see a significant shift in traffic patterns. Latency budgets that were appropriate for GPT-4o on 2024 infrastructure may be achievable at half the cost on 2026 infrastructure — or they may be unnecessarily tight given how user expectations have evolved. The budget should be a living number, not a number carved into a contract and forgotten.

The latency budget tells you how much you can spend on total response time. But there is a technique that fundamentally changes the relationship between total response time and perceived response time, making expensive latency budgets unnecessary for many use cases. That technique is streaming, and it changes the math of the entire triangle.
