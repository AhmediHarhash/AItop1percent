# 5.3 — Embedding and Retrieval Caching: Amortizing the Heaviest Computation

In a typical RAG pipeline, the embedding and retrieval stages account for 30 to 50 percent of per-request latency and 15 to 35 percent of per-request cost. Yet these stages are the most cacheable components in the entire pipeline, because the same documents get retrieved for similar queries repeatedly, and document embeddings are deterministic — the same text produces the same vector every time. A document that was embedded yesterday does not need to be embedded again today unless the document changed. A retrieval result that returned the top five chunks for a query about refund policies ten minutes ago will return the same five chunks for the same query right now, because the document corpus did not change in those ten minutes.

This is the quiet waste in RAG systems. Teams obsess over model selection, prompt engineering, and response caching — the visible, high-profile components — while the embedding and retrieval layers churn through the same computations thousands of times a day without anyone noticing. The waste is invisible because it happens inside the pipeline, not at the response level. No user sees the embedding computation. No dashboard tracks per-query retrieval cost by default. The money disappears into API calls and vector database queries that produce the same results they produced an hour ago.

Fixing this waste requires caching at three distinct levels within the RAG pipeline: document embeddings, query embeddings, and retrieval results. Each level captures a different type of redundancy, operates at a different timescale, and delivers different economics. Together, they can cut RAG pipeline costs by 40 to 60 percent while simultaneously improving latency — because a cache hit from memory is always faster than a round-trip to an embedding API or a vector search across millions of documents.

## Level One: Document Embedding Caching

Every document in your knowledge base must be embedded before it can be retrieved. The embedding converts the text into a dense vector that the retrieval system uses to match against query vectors. For most RAG systems, this embedding is computed using an API call to an embedding model — OpenAI's text-embedding-3-large, Cohere's embed-v4, or a self-hosted model like BGE-M3.

**Document embedding caching** means computing each document's embedding exactly once and storing it persistently, reusing the stored embedding for all future retrieval operations until the document changes. This is the most straightforward caching layer because document embeddings are perfectly deterministic and rarely need invalidation. The same text always produces the same embedding from the same model. A document that was embedded on Monday produces the identical vector on Tuesday, Wednesday, and every day thereafter — unless the document is updated or the embedding model changes.

The implementation is simple. When a document enters your knowledge base, compute its embedding and store both the embedding vector and a hash of the document content. On subsequent operations, check the hash: if the document content has not changed, use the stored embedding. If the hash differs, recompute the embedding and update the store. This is the same pattern as any content-addressable cache, and it works perfectly for document embeddings because the computation is a pure function of the input.

The economics are striking. Consider a knowledge base of 100,000 document chunks. Embedding all of them with text-embedding-3-large at $0.13 per million tokens, with an average chunk length of 300 tokens, costs roughly $3.90. That is the total cost, paid once. Without caching, a system that re-indexes or re-embeds documents daily — common in pipelines that rebuild the index from scratch — pays $3.90 per day, or $117 per month, for a computation that produces identical results every time. At 500,000 chunks, the cost rises to $585 per month in wasted re-embedding. The caching layer costs nothing beyond the storage of the vectors themselves — roughly 1.2 gigabytes for 500,000 chunks at 1,536 dimensions in float32 — which is negligible on any cloud storage tier.

The more significant savings come when document embeddings are computed on-the-fly per request rather than pre-indexed. Some RAG architectures embed fresh documents at query time — for example, when processing uploaded files or pulling live data. Without caching, a document that is queried fifty times is embedded fifty times. With caching, it is embedded once, and the subsequent forty-nine lookups are free. A legal tech company processing contract reviews found that their system was embedding the same standard contract templates 200 to 400 times per day because each user upload triggered a fresh embedding, even when multiple users uploaded the same template. Caching document embeddings by content hash eliminated 92 percent of their embedding API calls overnight.

## Level Two: Query Embedding Caching

While document embedding caching addresses the corpus side, **query embedding caching** addresses the query side. Every incoming user query must be embedded to perform the vector similarity search against the document index. If two users ask the same question — or even a very similar question — the embedding computation is duplicated.

Query embedding caching works by storing the embedding of each incoming query, keyed by the query text. When a new query arrives that matches a previously cached query — either by exact text match or by a very high similarity threshold — the stored embedding is used instead of calling the embedding API. This differs from semantic response caching in a critical way: it caches the intermediate computation (the embedding), not the final output (the response). The model still runs. The retrieval still executes. But the embedding step is skipped for cache hits.

The hit rate for query embedding caching is typically lower than for document embedding caching, because user queries are more varied than document content. But in systems with high traffic, even a 15 to 25 percent hit rate delivers meaningful savings. A customer support RAG system receiving 100,000 queries per day with an embedding cost of $0.002 per query spends $200 per day on query embeddings. A 20 percent cache hit rate saves 20,000 embedding calls per day — $40 per day, or $1,200 per month. Not transformative on its own, but this is only one layer of the caching stack, and every layer compounds.

The more valuable benefit of query embedding caching is latency. Embedding API calls typically add 20 to 80 milliseconds to each request. On a cache hit, the embedding is retrieved from local memory or a fast key-value store in sub-millisecond time. For latency-sensitive applications — voice assistants, real-time search, interactive chatbots — eliminating 20 to 80 milliseconds from the critical path on 20 percent of requests measurably improves user experience.

The invalidation model for query embedding caches is simpler than for document or response caches, because query embeddings do not go stale in the traditional sense. The same query text always produces the same embedding from the same model. The only invalidation trigger is an embedding model change — when you upgrade from one embedding model version to another, all cached query embeddings must be flushed because the new model produces a different vector space. This happens infrequently — perhaps once or twice a year — and the flush is a simple cache clear, not a complex data migration.

One subtlety deserves attention. Query embedding caching is most effective when the cache key is the raw query text, not the full prompt. If your system prepends instructions or metadata to the query before embedding — "Given the following user question, find relevant documents: [query]" — the cache key should be the query alone, not the full instrumented string. The instructions are constant across all queries, so they do not add discriminative information, but including them in the cache key would cause misses whenever the instruction text changes. Normalize your cache keys to the minimal differentiating content.

## Level Three: Retrieval Result Caching

The third and most valuable caching layer in a RAG pipeline is **retrieval result caching** — storing the set of documents or chunks returned by the retrieval system for a given query, so that subsequent identical or near-identical queries skip the vector search entirely and use the stored results.

Retrieval result caching delivers the highest savings per cache hit because vector similarity search is the most expensive operation in the retrieval pipeline — more expensive than embedding a single query and often more expensive than storing or fetching the results. A vector search across a million-chunk index involves computing similarities against potentially thousands of candidate vectors, even with approximate nearest-neighbor algorithms. On managed vector databases, this is billed per query — typically $0.01 to $0.05 per search depending on the provider and index size. On self-hosted infrastructure, the cost is measured in GPU or CPU time. Either way, eliminating redundant searches delivers immediate savings.

The architecture for retrieval result caching is a key-value store where the key is the query embedding (or a hash of it) and the value is the ordered list of retrieved chunk IDs and their relevance scores. When a query arrives and its embedding is computed (or retrieved from the query embedding cache), the system first checks the retrieval cache. If a match exists — either an exact embedding match or a high-similarity match — the stored chunk IDs are used to fetch the actual chunks from the document store, bypassing the vector search entirely.

The hit rate for retrieval result caching depends on the same factors as semantic caching: how repetitive is the query distribution? In customer support systems, where the same questions recur constantly, retrieval cache hit rates of 30 to 50 percent are common. In research or exploratory domains, where queries are more diverse, hit rates of 10 to 20 percent are typical. Even at the lower end, the savings are meaningful because the per-query retrieval cost is high relative to other pipeline components.

The invalidation challenge for retrieval caches is more complex than for embedding caches. Document embeddings only need invalidation when documents change. Query embeddings never go stale unless the model changes. But retrieval results go stale in two ways: when the underlying documents change (a cached result might point to a document that has been updated or deleted), and when new documents are added that would have been retrieved had they existed when the original search ran. Both conditions mean the cached results no longer represent the best retrieval output for the query.

The practical solution is a layered TTL strategy. Set a short TTL — fifteen minutes to two hours — for retrieval results in corpora that change frequently. Set a longer TTL — four to twenty-four hours — for corpora that are relatively static, such as product documentation, policy documents, or legal texts. Supplement the TTL with event-driven invalidation: when a document is updated or added to the corpus, invalidate any cached retrieval results that included that document or that covered the document's topic space. Event-driven invalidation is harder to implement than TTL-based expiration, but it prevents the scenario where a cached result points to a document that was updated five minutes ago while the TTL has not yet expired.

## The Compound Economics: All Three Layers Together

The real power of embedding and retrieval caching emerges when you stack all three layers. Each layer eliminates a different class of redundant computation, and their savings compound.

Consider a concrete scenario. A B2B knowledge platform processes 200,000 RAG queries per day across a corpus of 2 million document chunks. Before caching, the per-query cost breakdown is: query embedding at $0.002, vector search at $0.015, document chunk retrieval from storage at $0.001, and model inference at $0.018. Total per-query cost: $0.036. Monthly cost at 6 million queries: $216,000.

After implementing all three caching layers: document embeddings are precomputed and cached, eliminating re-embedding costs entirely — savings are modest here because the corpus was already pre-indexed, but the team avoids $585 per month in unnecessary re-indexing. Query embedding caching hits on 22 percent of queries, saving $2,640 per month in embedding API costs and cutting 50 milliseconds of latency on 1.32 million queries per month. Retrieval result caching hits on 38 percent of queries, saving $34,200 per month in vector search costs and cutting 80 to 150 milliseconds of latency on 2.28 million queries per month.

Total monthly savings from the three caching layers: approximately $37,400. The infrastructure cost for the caching system — a Redis instance for query and retrieval caches, additional storage for document embeddings, monitoring — is roughly $450 per month. Net savings: approximately $37,000 per month. The caching infrastructure pays for itself 82 times over.

But the savings do not stop at the embedding and retrieval layers. Because retrieval caching reduces the load on the vector database, the team can use a smaller vector database tier, saving an additional $2,000 per month in infrastructure costs. Because query embedding caching reduces the load on the embedding API, the team stays within a lower pricing tier. These second-order savings are harder to quantify but real.

The latency improvements are equally significant. Before caching, the median end-to-end latency was 1.4 seconds, with the embedding and retrieval stages accounting for 520 milliseconds. After caching, on cache hits, the embedding and retrieval stages complete in under 10 milliseconds — a 98 percent reduction for those stages. The median end-to-end latency dropped to 940 milliseconds, and the p95 dropped from 2.8 seconds to 1.6 seconds. Users noticed the improvement before anyone told them about it.

## Handling Document Updates Without Losing Your Cache

The hardest operational challenge in embedding and retrieval caching is handling document updates gracefully. When a document changes, its cached embedding is stale. Any retrieval results that included that document are potentially stale. Any downstream responses generated from those retrieval results may be incorrect. The cascade from a single document update can invalidate entries across all three caching layers.

The disciplined approach is a **change propagation pipeline** that traces the impact of each document update through the caching stack. When a document is updated, the pipeline executes three operations in sequence. First, it recomputes the document's embedding and updates the embedding cache. Second, it identifies all cached retrieval results that included the updated document — by maintaining a reverse index from document IDs to cache keys — and invalidates those entries. Third, if you maintain a response cache, it invalidates any cached responses that were generated using the now-stale retrieval results.

This propagation pipeline adds complexity, but it is the only way to maintain cache correctness when documents change. The alternative — waiting for TTL expiration — creates a window during which the cache serves stale results. For some domains, a stale window of thirty minutes is acceptable. For others — financial data, medical information, legal text — even a few minutes of stale data is unacceptable.

The engineering effort can be managed by partitioning your corpus by change frequency. Static content — product manuals, historical documentation, regulatory text — changes rarely and can use long TTLs with infrequent invalidation. Moderate content — pricing pages, FAQ responses, policy documents — changes weekly or monthly and benefits from event-driven invalidation. Dynamic content — real-time data, user-generated content, rapidly changing inventories — should not be cached at the retrieval level at all, or should use TTLs measured in minutes. This tiered approach applies invalidation effort proportionally to change risk, avoiding both the overhead of universal event-driven invalidation and the staleness risk of universal TTL-based expiration.

## The Self-Hosted Advantage for Embedding Caching

One architectural decision that significantly affects embedding caching economics is whether to use a hosted embedding API or a self-hosted embedding model. For systems with high query volumes, self-hosting changes the cost calculus of embedding caching dramatically.

With a hosted API, every embedding computation has a marginal cost — typically $0.01 to $0.13 per million tokens, depending on the model. Caching eliminates these marginal costs on cache hits, which is the primary value proposition. With a self-hosted model, the marginal cost of each embedding computation is near zero — the cost is the fixed infrastructure (GPU instance, memory, networking) amortized over all queries. Caching still provides a latency benefit on self-hosted systems, but the cost benefit is smaller because the marginal cost being avoided is smaller.

This means that embedding caching has the highest cost ROI on systems using expensive hosted embedding models at high volume. A system processing 500,000 queries per day using Cohere embed-v4 spends more on embedding API calls than a system using a self-hosted BGE-M3 model on a single GPU, and therefore saves more from caching.

The flip side is that self-hosting introduces its own caching value through capacity management. A self-hosted embedding model has a throughput ceiling — it can process a fixed number of embeddings per second on its allocated hardware. Embedding caching reduces the load on that hardware, effectively increasing the system's capacity without adding more GPUs. For systems that are bottlenecked on embedding throughput, caching is a capacity optimization as much as a cost optimization. A team running a self-hosted embedding model at 90 percent utilization might face a choice between caching 30 percent of queries (reducing utilization to 63 percent) or adding another GPU instance at $2,000 per month. Caching is almost always cheaper.

## Warming the Cache: Cold-Start Strategies

A fresh cache is an empty cache, and an empty cache has a zero hit rate. The first day after deploying embedding and retrieval caching, every query is a cache miss. The savings ramp up gradually as the cache fills with entries from production traffic. This **cold-start problem** can be mitigated with deliberate cache warming strategies.

The simplest approach is **historical replay**. Take a sample of your recent production traffic — the last week or month of queries — and replay them through the caching pipeline before enabling caching in production. This pre-populates the cache with entries for your most common queries, so that when live traffic starts flowing, the cache already contains the high-frequency entries that deliver the most hits.

A more targeted approach is **frequency-based warming**. Analyze your query logs to identify the most common queries — the top 1,000 or top 5,000 by frequency. Embed these queries and execute their retrieval in advance, storing the results in the cache. Because query distributions follow power laws, the top 1,000 queries might account for 30 to 50 percent of all traffic. Pre-caching them ensures that the highest-impact entries are available from day one.

For document embedding caching, warming is even simpler: embed your entire corpus before going live. This is a batch operation that runs once and populates the document embedding cache completely. The cost is the one-time embedding of all documents — which you were going to pay anyway — and the result is a document embedding cache that starts at 100 percent coverage and only needs updates when documents change.

Cache warming reduces the time to full ROI from weeks to hours. Without warming, it takes days or weeks for the cache to accumulate enough entries to reach its steady-state hit rate. With warming, the cache reaches 70 to 80 percent of its steady-state hit rate on day one. For teams that are tracking monthly cost savings, this acceleration matters — it is the difference between seeing the savings on the first monthly bill and waiting until the second or third.

## Monitoring Embedding and Retrieval Caches

Cache monitoring for embedding and retrieval layers requires tracking metrics that are specific to each layer, beyond the generic hit rate and latency numbers that apply to all caches.

For document embedding caches, track the **staleness rate** — the percentage of documents in the corpus whose cached embedding is out of date because the document was modified after the embedding was last computed. If your corpus changes frequently and your invalidation pipeline has gaps, this rate can climb without anyone noticing. A staleness rate above 5 percent means your retrieval results are increasingly based on outdated representations, which degrades retrieval quality silently. Monitor this rate daily and investigate any sudden increases.

For query embedding caches, track the **cache diversity** — the number of unique query embeddings in the cache relative to the total query volume. If the diversity is very low — fewer than a few thousand unique entries for hundreds of thousands of queries — the cache is effectively a small lookup table serving a narrow set of queries. If the diversity is very high — close to the total query volume — most queries are unique and the cache is accumulating entries it will never serve again. Understanding the diversity helps you set appropriate cache size limits and eviction policies.

For retrieval result caches, track the **consistency rate** — how often a cached retrieval result, if re-executed against the current index, would produce the same set of documents. This requires periodic sampling: take a random sample of cached results, re-execute the underlying vector search, and compare the fresh results against the cached results. If the consistency rate drops below 90 percent, your TTL is too long or your invalidation pipeline is not catching document updates. This metric directly measures whether the retrieval cache is serving accurate results or whether it has drifted from the current state of the corpus.

Track the **cache size growth rate** for all three layers. Embedding and retrieval caches can grow large if not managed — a million cached query embeddings at 1,536 dimensions in float32 is roughly 6 gigabytes, and retrieval result caches that store full chunk content can grow even larger. Set size limits and eviction policies — least recently used is the standard choice — and monitor the eviction rate. A high eviction rate means the cache is too small for the query distribution. Either increase the cache size or accept a lower hit rate on the long tail of rare queries.

Embedding and retrieval caching eliminates the heaviest redundant computations in the RAG pipeline. But there is a category of optimization that goes further than caching recent results — it anticipates future needs and does the expensive work before the user ever asks. That is the discipline of precomputation, which we explore in the next subchapter.
