# 3.6 — Batch vs Real-Time: Choosing Processing Modes for Cost and Quality

Batch API pricing from major providers typically runs at 50 percent of real-time pricing for the same model and the same tokens. Both OpenAI and Anthropic offer batch endpoints that process requests asynchronously within a 24-hour window at half the standard per-token rate. For workloads that can tolerate minutes or hours of latency, batch processing is the single largest cost reduction available without any quality compromise. The model is identical. The output is identical. The only difference is timing. You pay half as much because you are letting the provider schedule your inference during off-peak capacity windows instead of demanding immediate execution. For a team spending $40,000 per month on inference, moving even 30% of that volume to batch processing saves $6,000 per month — $72,000 per year — with zero engineering effort beyond integrating the batch API and zero quality impact.

Yet most teams leave this money on the table. They process everything in real time because that is how they built the system on day one. The user-facing chatbot needs real-time responses, so the entire architecture runs in real-time mode, including the evaluation pipelines that run nightly, the content generation jobs that populate knowledge bases, the embedding computations for new documents, and the bulk classification tasks that tag incoming data. None of these background workloads need sub-second response times. All of them are paying full price for immediacy they do not use. The first step in batch optimization is not engineering. It is classification: which of your workloads actually need real-time processing, and which are paying a premium for urgency they do not require.

## The Economics of Waiting

The 50% batch discount exists because of how inference infrastructure works. Real-time API requests require providers to maintain idle GPU capacity ready to serve requests instantly. That idle capacity costs money whether or not anyone uses it. Peak hours consume all available capacity. Off-peak hours leave expensive hardware sitting idle. Batch APIs solve this problem for the provider by letting them fill idle capacity with deferred work. You get a discount because your job runs when the GPUs would otherwise be doing nothing. The provider gets revenue from hardware that would otherwise be a sunk cost. Both sides win.

But the 50% headline discount is the floor, not the ceiling. When batch processing is combined with prompt caching, the savings stack. Anthropic's prompt caching reduces input token costs by 90% for repeated context, and this discount applies on top of the batch discount. A batch job that reuses the same system prompt across thousands of requests pays 50% of the already-reduced cached price. For workloads with large, repeated system prompts — which describes most bulk processing tasks — the combined discount can reach 95% or more compared to real-time uncached pricing. A request that costs $0.03 in real-time mode might cost $0.0015 in batch mode with prompt caching. Multiply by hundreds of thousands of requests and the difference is the salary of an engineer.

The economic argument for batch processing strengthens as your volume grows. At 10,000 requests per month, the savings from batch processing might be $150 — meaningful but not transformative. At 500,000 requests per month, the savings might be $15,000 to $25,000 per month. At 5 million requests per month, batch processing is not an optimization. It is a strategic cost lever that determines whether your unit economics work at all.

## Classifying Workloads: The Latency Tolerance Test

Every workload in your AI system falls somewhere on a latency tolerance spectrum, and your job is to classify each workload honestly rather than defaulting everything to real-time.

**Latency-intolerant workloads** are those where the user is actively waiting for a response. A chatbot conversation. A real-time search query. An autocomplete suggestion. A live content moderation decision. These workloads genuinely need sub-second to low-second response times, and batch processing is not an option. The user's experience degrades directly with latency. Pushing these to batch would break the product.

**Latency-tolerant workloads** are those where the consumer of the output is not a user waiting in real time. An evaluation pipeline that scores yesterday's production outputs. A content generation job that creates product descriptions for a catalog update. An embedding pipeline that processes newly uploaded documents for a RAG knowledge base. A classification job that tags support tickets for routing. A synthetic data generation pipeline that creates training examples. A report generation system that produces weekly analytics summaries. None of these need immediate execution. All of them can wait minutes or hours without any impact on user experience.

**Latency-ambiguous workloads** are the interesting category. These are workloads where real-time execution seems necessary but is not, once you examine the actual user flow. Consider a document analysis feature where users upload a contract and receive a summary. The user expects the summary "quickly" — but does "quickly" mean two seconds or two minutes? If the user uploads a document, closes the tab, and comes back later to check the results, they have demonstrated latency tolerance that the product design treats as latency intolerance. User research often reveals that what users want is acknowledgment that their request was received and confidence that it will be processed, not necessarily instant completion. A system that says "your analysis is processing and will be ready in about three minutes" and delivers a batch-processed result can provide a better user experience than one that says nothing for thirty seconds while processing in real time and then shows a loading spinner.

The classification exercise typically reveals that 30 to 60 percent of a team's total inference volume is latency-tolerant. The exact percentage depends on the product, but the pattern is remarkably consistent: teams overestimate how much of their work requires real-time processing. The customer-facing interaction needs real time. Everything behind it — preparation, evaluation, enrichment, classification — usually does not.

## When Batch Works: The High-Volume Background Workloads

Certain workload categories are almost always better served by batch processing. Recognizing them prevents the default-to-real-time bias that costs teams thousands of dollars per month.

**Evaluation pipelines** are the clearest batch candidate. If you are running automated evaluations on production outputs — scoring quality, checking for hallucinations, measuring format compliance — those evaluations do not need to happen in real time. A nightly or hourly batch evaluation job processes the same outputs at the same quality with the same model but at half the cost. A team running 200,000 evaluation calls per month at $0.008 per call spends $1,600 on real-time evaluation. Move that to batch at $0.004 per call and the cost drops to $800 per month. If the evaluation is not blocking a user-facing workflow, there is no reason to run it in real time.

**Knowledge base enrichment** is another strong batch candidate. When new documents enter your RAG system, they need to be embedded, chunked, summarized, and indexed. These preprocessing steps happen once per document, not once per query. They can run in a batch job that processes new documents every hour, every six hours, or overnight. The freshness tradeoff is real — a document uploaded at 9am might not be searchable until the next batch run — but for most knowledge bases, a lag of one to six hours is acceptable, and the cost savings on embedding and summarization calls are substantial.

**Bulk content generation** — product descriptions, personalized email drafts, report sections, data summaries — is inherently batch work. The content does not need to exist the instant the request is created. It needs to exist by the time someone reads it. A team generating 50,000 product descriptions per month at $0.015 per real-time call spends $750. Batch processing cuts that to $375. When prompt caching is layered on top — because all 50,000 descriptions share the same system prompt and format instructions — the cost can drop below $100. That is a 7x reduction for identical output quality.

**Synthetic data generation** for training and evaluation is almost exclusively batch work. You are generating thousands or millions of examples, none of which have a user waiting. The 50% batch discount on high-volume synthetic data generation can save tens of thousands of dollars per training cycle.

**Scheduled reports and analytics** that run daily or weekly are batch by nature. A system that generates AI-powered analytics summaries for 3,000 enterprise customers every Monday morning should absolutely run as a batch job overnight Sunday, not as 3,000 individual real-time calls on Monday morning that spike your costs and stress your rate limits simultaneously.

## When Batch Breaks: The Tasks That Need Real Time

Batch processing is not a universal solution. Some tasks cannot tolerate any latency without degrading the core product experience, and forcing them into batch mode costs more in lost value than it saves in inference pricing.

**Conversational interfaces** require real-time processing because the user is waiting in a turn-based interaction. Every second of latency between the user's message and the system's response degrades the conversational experience. At ten seconds, the interaction feels broken. Batch processing a chatbot conversation is a contradiction — the "batch" of one request with a 24-hour delivery window is useless.

**Real-time content moderation** cannot wait. If user-generated content is being posted to a platform and needs AI screening before publication, the moderation must happen between the user clicking "post" and the content appearing. A batch moderation system that checks posts six hours after publication has already let harmful content reach the audience.

**Interactive decision support** — where a human is making a time-sensitive decision and needs AI analysis to inform it — requires real-time processing. A clinician reviewing a patient case and requesting an AI-generated differential diagnosis needs the analysis during the appointment, not six hours later. A trader requesting market analysis needs it while the market condition exists, not after it has changed.

**Streaming responses** in user-facing interfaces need real-time inference because the user is watching tokens appear. The perception of speed from streaming — where the user starts reading the response before it finishes generating — is incompatible with batch processing, which returns complete responses after an indeterminate delay.

The pattern: if a human is waiting, watching, or deciding in real time, the workload is latency-intolerant. If the output will be stored, queued, indexed, or consumed later, the workload is batch-eligible.

## The Hybrid Architecture: Real-Time Surface, Batch Core

The most cost-efficient architecture in 2026 is neither fully real-time nor fully batch. It is a hybrid that uses real-time processing for the user-facing interaction layer and batch processing for everything else. This architecture captures the 50% batch discount on the majority of inference volume while preserving the sub-second latency where users actually experience it.

The hybrid works by decomposing your system into two layers. The **real-time layer** handles user-facing requests: chatbot responses, search queries, interactive analysis. It runs on standard API pricing because users are waiting and latency matters. The **batch layer** handles everything behind the scenes: evaluation, embedding, classification, content generation, data enrichment, report building, and reprocessing. It runs on batch API pricing because no user is waiting and timing does not matter.

A document intelligence company implemented this hybrid in late 2025. Their system had three main workloads: a real-time Q and A interface where users asked questions about their documents (35% of total inference volume), a document processing pipeline that analyzed new uploads and extracted structured data (45% of volume), and an evaluation pipeline that scored output quality and flagged issues for review (20% of volume). Before the hybrid architecture, all three workloads ran in real-time mode. Total monthly inference cost: $52,000.

After implementing the hybrid, only the Q and A interface remained on real-time pricing. Document processing moved to batch with a 2-hour processing window — users uploaded documents and received results within two hours instead of immediately. The evaluation pipeline moved to batch with a 12-hour window, running nightly. The real-time Q and A cost stayed at $18,200 per month (35% of volume at real-time rates). Document processing dropped from $23,400 to $11,700 per month (45% of volume at batch rates). Evaluation dropped from $10,400 to $5,200 per month (20% of volume at batch rates). Total monthly cost: $35,100, down from $52,000. The 32% total reduction came entirely from using batch pricing on workloads that did not need real-time processing. Quality was identical across all three workloads because the same models served the same prompts. The only change was when the processing happened.

The user experience impact of the hybrid was minimal. For the Q and A interface, nothing changed — responses were still real-time. For document processing, users saw a "processing" status for up to two hours instead of immediate results. The team surveyed affected users and found that 78% did not notice the change because they had never waited at their computers for processing to finish anyway — they uploaded documents and came back later. The 22% who noticed were neutral; none cited it as a negative experience.

## Managing Batch Latency Expectations

The 24-hour window on batch APIs does not mean your results take 24 hours. In practice, most batch jobs complete far sooner. Both OpenAI and Anthropic process batch requests as capacity becomes available, and most requests complete within one to six hours. But the 24-hour window is a maximum, not a guarantee of faster delivery, and your architecture must handle the full range.

This means your system needs to handle three states for batch-submitted work: pending (submitted, not yet processed), processing (provider is working on it), and complete (results available). Your product must communicate these states clearly to any user or downstream system waiting for results. For internal workloads like evaluation and enrichment pipelines, this is straightforward — a job status tracker checks completion and triggers downstream processing. For user-facing features like document analysis, you need notification mechanisms: email alerts, dashboard indicators, or push notifications that tell the user their results are ready.

Design your batch jobs for idempotency and restartability. Batch API calls can fail partially — some requests in a batch complete while others error. Your system needs to identify which requests succeeded and resubmit only the failures, not the entire batch. This is particularly important for large batch jobs of 100,000 or more requests, where a 2% error rate means 2,000 requests that need resubmission. Without idempotent request handling, you risk duplicate processing — and duplicate cost — on the successful requests.

Set internal SLAs that are tighter than the provider's 24-hour window but loose enough to avoid unnecessary escalation. If your document processing batch typically completes in 3 hours, set an internal alert at 6 hours, a warning at 12 hours, and an escalation at 18 hours. This gives you time to investigate slow batches without alarming users who were told their results would arrive "within a few hours."

## The Batch Cost Equation: Calculating Your Specific Savings

The theoretical 50% discount is the starting point. Your actual savings depend on your workload composition, your batch eligibility rate, and any additional discounts that stack with batch pricing. Here is how to calculate your specific savings potential.

Start by auditing your current inference volume. Categorize every workload by latency tolerance: must be real-time, can tolerate minutes, can tolerate hours, can tolerate a day. For each category, calculate the current monthly inference cost. This gives you the batch-eligible spend — the total cost of workloads that can move to batch processing without affecting user experience.

Apply the base 50% batch discount to the batch-eligible spend. This is your floor savings. Then check for stacking opportunities. If your batch workloads share common system prompts across requests — and most batch workloads do, because they are running the same task across many inputs — prompt caching applies. Anthropic's prompt caching reduces input token costs by 90% for cached content, and this discount stacks with the batch discount. OpenAI's caching mechanisms offer similar savings on repeated context. For a batch job with a 1,200-token system prompt and a 300-token per-request variable portion, 80% of the input tokens are cacheable. The effective input token discount for cached tokens on batch pricing approaches 95%.

A practical example. A team runs 400,000 evaluation calls per month. Each call uses a 1,000-token system prompt (shared across all calls) and a 500-token variable portion (the output being evaluated). The model is Claude Sonnet 4.5 at $3 per million input tokens and $15 per million output tokens at real-time rates. Average output per evaluation call: 200 tokens.

Real-time cost: input tokens per call are 1,500, output tokens per call are 200. Monthly input cost: 400,000 times 1,500 divided by 1,000,000 times $3, which equals $1,800. Monthly output cost: 400,000 times 200 divided by 1,000,000 times $15, which equals $1,200. Total real-time cost: $3,000 per month.

Batch cost with prompt caching: the 1,000-token system prompt is cached after the first call. Cached input tokens cost 90% less, then the batch discount applies on top. Effective cached input price: $3 times 0.10 times 0.50, which equals $0.15 per million tokens. Non-cached input (500 tokens per call) at batch rate: $3 times 0.50, which equals $1.50 per million tokens. Output at batch rate: $15 times 0.50, which equals $7.50 per million tokens.

Monthly cached input cost: 400,000 times 1,000 divided by 1,000,000 times $0.15, which equals $60. Monthly non-cached input cost: 400,000 times 500 divided by 1,000,000 times $1.50, which equals $300. Monthly output cost: 400,000 times 200 divided by 1,000,000 times $7.50, which equals $600. Total batch cost with caching: $960 per month.

The savings: $3,000 minus $960 equals $2,040 per month, or $24,480 per year, from a single workload migration. The quality is identical — same model, same prompt, same output. The only difference is that the evaluation calls run in a batch window instead of real time.

## Migrating Workloads: The Batch Conversion Playbook

Moving a workload from real-time to batch is straightforward in theory but requires careful execution to avoid disrupting downstream dependencies.

**Step one: identify the workload and its consumers.** Every workload has something that consumes its output. An evaluation pipeline's consumers might be a quality dashboard and an alerting system. A content generation pipeline's consumers might be a CMS and a review queue. Map these consumers before migrating, because they need to handle asynchronous delivery instead of synchronous response.

**Step two: implement the batch submission layer.** The batch APIs from OpenAI and Anthropic accept JSONL files containing request payloads. Your system needs to collect individual requests, package them into batch files, submit them to the batch API, poll for completion, and route completed results to the appropriate consumers. Most teams build this as a lightweight orchestration service — a scheduled job that collects pending requests, submits a batch, and processes results when available.

**Step three: run a shadow migration.** Before fully switching over, run the workload in both modes simultaneously for one to two weeks. Submit every request to both the real-time API and the batch API. Compare the outputs. They should be identical — same model, same prompt, same results — but verify this empirically. Occasionally, differences emerge due to model version updates between real-time and batch endpoints, or due to timeout settings that cause slightly different behavior. The shadow period catches these discrepancies before they affect production.

**Step four: switch over and monitor.** Once the shadow period confirms identical output, route the workload to batch. Monitor the transition for one week: check that batch jobs complete within your expected window, that downstream consumers process results correctly, and that error rates match the baseline from the shadow period.

**Step five: set up batch health monitoring.** Track batch job submission times, completion times, success rates, and cost per batch. Alert when completion times exceed your internal SLA. Alert when error rates spike. Track cost per request in batch mode to verify that you are actually capturing the expected discount. Some teams discover that their batch packaging introduces overhead — extra tokens from request formatting, duplicate system prompts that should be cached but are not — that erodes the savings. Monitoring catches these leaks.

## The Organizational Argument for Batch Processing

Moving workloads to batch often requires convincing stakeholders who are not thinking about inference costs. The product team may resist any latency increase on any feature, even background features. Engineering may resist the added complexity of asynchronous processing. Leadership may not understand why "the same thing but slower" is worth the engineering investment.

Frame the argument in terms the audience cares about. For product: "users will not notice because these workloads are not user-facing. The user-facing features remain real-time." For engineering: "batch APIs are simpler than real-time APIs in many ways — no retry storms, no rate limit management, no timeout handling. The provider manages scheduling and execution." For leadership: "moving 40% of our inference volume to batch saves $X per month with zero quality impact. The migration takes one engineer two weeks."

The strongest argument is demonstration. Pick one workload — ideally the largest batch-eligible workload — migrate it, and show the cost savings in the next monthly review. Numbers on a dashboard are more persuasive than arguments in a deck. Once leadership sees that $8,000 per month disappeared from the inference bill with no corresponding quality or user experience change, the conversation shifts from "should we do this?" to "what else can we move to batch?"

Batch processing optimizes when a model runs. Prompt simplification optimizes what you send to the model. The next lever optimizes how many models you use in a single request flow: multi-model pipelines that combine cheap and expensive models in the same workflow to capture the best of both.
