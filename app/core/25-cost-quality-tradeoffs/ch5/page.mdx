# Chapter 5 — Caching, Precomputation, and Reuse

The fastest and cheapest inference call is the one you never make. Every AI system generates redundant work — identical queries hitting the same model, similar questions producing nearly identical retrievals, embeddings recomputed for documents that have not changed, tool calls repeated for data that updates once a day. Eliminating this redundancy is not a minor optimization. For systems with any repetition in their query patterns, caching and precomputation routinely cut costs by 30 to 60 percent while simultaneously improving latency. It is the rare optimization that makes everything better at once.

But AI caching is harder than traditional web caching. User queries are not exact-match URLs. Two questions can be worded differently and mean the same thing, or worded identically and mean different things depending on the user's context. Cached answers go stale when underlying data changes, when models are updated, or when the world shifts beneath a response that was correct yesterday. This chapter teaches you to build caching and reuse systems that capture the cost savings without introducing the correctness failures that make caching dangerous in AI systems.

---

- **5.1** — The Caching Imperative: Why Repeat Work Is the Easiest Cost to Eliminate
- **5.2** — Semantic Caching: Matching Similar Queries to Existing Responses
- **5.3** — Embedding and Retrieval Caching: Amortizing the Heaviest Computation
- **5.4** — Precomputation Strategies: Doing Expensive Work Before the User Asks
- **5.5** — Cache Invalidation for AI: When Cached Answers Become Wrong Answers
- **5.6** — The Freshness-Cost Tradeoff: How Often to Recompute
- **5.7** — Response Reuse Across Users: Shared Caches and Privacy Boundaries
- **5.8** — Measuring Cache Effectiveness: Hit Rates, Staleness, and Quality Impact

---

*Caching eliminates redundant inference. But there is another category of cost that grows silently alongside your system: the cost of evaluation itself — and managing that investment is the subject of the next chapter.*
