# 6.3 — LLM-as-Judge Cost Optimization: Cheaper Judges for Routine Checks

Most teams use their most expensive model as a judge for every evaluation. This is like hiring a senior partner at a law firm to review parking tickets. Match the judge to the judgment.

The instinct behind over-powered judges is understandable. Teams want accurate evaluation, and accuracy feels like it requires the best model available. But evaluation tasks vary enormously in difficulty. Checking whether a response is in English does not require the same model as assessing whether a medical summary omits critical contraindications. Scoring format compliance does not demand the same intelligence as evaluating the logical coherence of a multi-step reasoning chain. When you use a frontier model for every evaluation dimension, you pay frontier prices for work that a model costing one-twentieth as much could handle with equivalent accuracy.

## The Hierarchy of Judge Difficulty

Not all evaluation tasks are created equal. They fall into a natural hierarchy based on the cognitive complexity required to make an accurate judgment.

At the bottom are **routine checks** — format compliance, length requirements, language detection, response completeness, and structural validation. These are essentially pattern-matching tasks. A response either contains the required sections or it does not. It either stays within the word limit or it exceeds it. Models like GPT-5-nano, Claude Haiku 4.5, or Gemini 3 Flash handle these with near-perfect accuracy because the judgment criteria are unambiguous.

In the middle are **moderate checks** — relevance scoring, basic factual consistency, tone assessment, and topic adherence. These require some understanding of the content but follow relatively clear rubrics. A mid-tier model like GPT-5-mini or Claude Sonnet 4.5 performs these reliably when given well-structured evaluation prompts. The key is that the rubric must be explicit. When you tell a mid-tier model to score relevance on a scale of one to five with clear definitions for each level, it agrees with frontier model judgments over 90% of the time.

At the top are **complex judgments** — nuanced factual accuracy against domain-specific knowledge, safety assessment for edge cases, reasoning quality evaluation, and detecting subtle hallucinations that require deep domain expertise. These genuinely benefit from frontier models like Claude Opus 4.6 or GPT-5. The judgment requires inference, world knowledge, and the ability to catch errors that cheaper models would miss. But these complex judgments typically represent less than 20% of your total evaluation volume.

## The Benchmarking Method

The way to determine which judge level each evaluation dimension needs is empirical, not theoretical. Take your existing evaluation pipeline and run the following experiment.

Select a sample of 200-500 responses that have been evaluated by your current expensive judge. Run the same evaluation on the same responses using a cheaper model candidate. Compare the scores. Calculate agreement rate — the percentage of cases where the cheap judge and the expensive judge give the same score or fall within one point on your scale.

If agreement exceeds 95%, the cheap judge is a drop-in replacement for that dimension. If agreement falls between 85% and 95%, the cheap judge works for screening but should escalate uncertain cases to the expensive judge. If agreement falls below 85%, the cheap judge is not reliable for that dimension and you should keep the expensive one.

This experiment costs very little. Running 500 evaluations through a cheap model costs a few dollars. The potential savings from discovering that half your evaluation dimensions can use a cheap judge are enormous. A team spending $8,000 per month on evaluation that shifts 60% of their dimensions to a cheap judge can save $4,500 per month — permanently.

## The Cascade Pattern

The most sophisticated approach combines cheap and expensive judges in a cascade. The cheap judge evaluates every response first. When the cheap judge is confident — when its score is clearly pass or clearly fail — that judgment stands. When the cheap judge is uncertain — when the score falls in an ambiguous middle range — the response escalates to the expensive judge.

This pattern captures most of the savings of using a cheap judge while maintaining the accuracy of the expensive judge for difficult cases. In practice, the cheap judge is confident on 70-85% of responses, meaning only 15-30% escalate to the expensive tier.

The cascade requires defining a confidence threshold. For a five-point scale, you might define scores of one or two as confident fails and scores of four or five as confident passes, with only scores of three escalating. For a binary pass-fail judge, you can use the model's log probabilities or ask it to self-report confidence. Responses where the model expresses high confidence in either direction stay at the cheap tier. Responses where the model expresses uncertainty escalate.

The economics are compelling. If your expensive judge costs $0.02 per evaluation and your cheap judge costs $0.001, and 75% of responses resolve at the cheap tier, your blended cost per evaluation drops from $0.02 to $0.00575 — a 71% reduction with minimal accuracy loss.

## Model-Specific Judge Performance in 2026

The 2026 model landscape offers particularly strong options for evaluation at different price points. Claude Haiku 4.5 has become a standard for routine evaluation tasks — its speed and low cost make it ideal for high-volume format and compliance checks. GPT-5-mini serves well for moderate evaluation tasks, handling relevance scoring and basic factual checking with strong rubric adherence. Gemini 3 Flash offers competitive pricing for classification-style judgments.

For complex evaluations, Claude Opus 4.6 and GPT-5 remain the standard because their reasoning capabilities catch subtle errors that cheaper models miss. But even here, the cascade pattern means you only pay frontier prices for the cases that need it.

Open-source models offer another cost tier entirely. Running a fine-tuned Llama 4 Scout as a judge on your own infrastructure eliminates per-query API costs, replacing them with fixed infrastructure costs that become cheaper per evaluation as volume increases. This makes sense when your evaluation volume exceeds roughly 100,000 judgments per month, at which point the infrastructure cost per judgment drops below API pricing for comparable models.

## Calibrating Cheap Judges Over Time

A cheap judge that agrees with your expensive judge today might drift as either model gets updated. The solution is periodic calibration — re-running the benchmarking experiment quarterly or after any model update.

Build a **calibration dataset** of 200-300 responses with gold-standard judgments from your most expensive judge or from human experts. This dataset should be diverse — include responses that span the full quality range, cover different topics and formats, and include the edge cases where judge disagreement is most likely. Do not build the calibration set from easy cases. Include the borderline responses where the judgment is genuinely difficult, because those are the cases where cheap judge drift shows up first.

Run every judge in your pipeline against this dataset monthly. Track agreement rates over time on a simple chart: one line per judge-dimension pair, plotted monthly. A healthy chart shows stable agreement rates with minor fluctuation. A degrading chart shows a slow downward trend — dropping from 96% agreement to 93% to 89% over three months. That trend means the cheap judge's model was updated and its evaluation behavior changed, or the responses themselves shifted in a way that exposes a weakness in the cheap judge's rubric.

When agreement drops below your threshold, you have three options in order of preference. First, retune the evaluation prompt for the cheap judge — often a small rubric clarification restores agreement without changing models. Second, if prompt tuning does not restore agreement, try the next model up in the cheap tier — sometimes a slightly larger model within the same cost tier handles the dimension better after an update. Third, if no cheap model restores acceptable agreement, escalate that dimension back to the expensive tier and flag it for re-evaluation at the next quarterly review.

This calibration practice costs very little — a few hundred evaluations per month, typically under $50 — but prevents the slow degradation of evaluation quality that happens when cheaper judges silently drift from your standards.

## The Organizational Dynamics of Judge Selection

Teams resist downgrading their evaluation models for the same reasons they resist downgrading their production models: fear of missing something. The argument is always "what if the cheap judge misses a critical quality issue that the expensive judge would have caught?"

The benchmarking data answers this question directly. If the cheap judge agrees with the expensive judge 96% of the time on format checks, the risk of downgrading format evaluation is minimal. The 4% disagreement is not 4% missed failures — it is 4% of cases where the two judges gave different scores, and in many of those cases, the cheap judge's score is equally valid. When you examine the disagreement cases manually, you will typically find that half are genuine misses by the cheap judge, a quarter are cases where the cheap judge is arguably more correct than the expensive judge, and a quarter are cases where both scores are defensible. The actual miss rate — cases where the cheap judge passes a response that is genuinely bad — is typically half the raw disagreement rate or less.

Frame the conversation around specific dimensions, not blanket downgrades. You are not replacing your entire evaluation pipeline with a cheap model. You are replacing specific, low-complexity dimensions while keeping expensive judges for the dimensions that need them. This framing matters because the emotional resistance to "using a cheaper model" collapses when the discussion shifts to "using a cheaper model specifically for checking whether the response is in English, while keeping the expensive model for evaluating medical accuracy."

Present the savings in terms that matter to the organization. If shifting routine checks to cheap judges saves $5,000 per month, that is $60,000 per year that could fund an additional human reviewer, build a better eval dataset, or reduce the eval tax that Section 6.1 described. The money saved on judges should be reinvested in evaluation quality, not just pocketed as cost reduction. The most effective pitch is not "we can spend less on evaluation" but "we can spend the same amount on evaluation and get better quality signal by redirecting dollars from cheap-judge-compatible dimensions to the complex dimensions that actually need expensive judges."

## Building the Judge Allocation Map

Once you have benchmarking data for each evaluation dimension, build a **judge allocation map** — a document that lists every eval dimension, its assigned judge model, the agreement rate with the expensive judge, and the last calibration date. This map becomes the authoritative reference for your eval pipeline configuration.

The map serves three purposes. First, it makes the judge assignment decision explicit and reviewable. Anyone can see which dimensions use cheap judges and why. Second, it tracks calibration freshness — if a dimension has not been re-benchmarked in six months, it needs recalibration. Third, it reveals the total cost profile of your eval pipeline at a glance. You can see exactly how much you spend on each dimension and where the opportunities for further optimization lie.

A well-maintained judge allocation map typically shows that 40-60% of eval dimensions can use the cheapest available judge, 25-35% use a mid-tier judge, and only 10-20% require a frontier model. Teams that have never done this analysis are almost always paying frontier prices for the majority of their evaluation — a mistake that can cost tens of thousands per month at scale.

## When Cheap Judges Are Not Enough

Some evaluation dimensions resist cost optimization. Safety evaluation for high-risk applications — medical, financial, legal — should use the best available judge because the cost of a false negative is asymmetric. Missing a safety issue that reaches production costs far more than the savings from using a cheap judge.

Similarly, evaluation dimensions where your business differentiation depends on quality — the dimensions your customers directly perceive — deserve premium judges. If your product competes on accuracy and your eval pipeline is the primary defense against accuracy regression, a cheap judge on that dimension is false economy.

There is also a temporal dimension. When you first deploy a new feature or make a significant model change, use expensive judges across all dimensions until you have confidence the system is stable. Once stability is established and you have enough data to benchmark cheaper judges, begin the downgrade process dimension by dimension. This phased approach limits risk while capturing the long-term savings.

The discipline is matching judge power to judgment difficulty, not minimizing judge cost. Some evaluations are worth every dollar of the expensive model. Most are not. The benchmarking method tells you which is which.

The next challenge is not which model evaluates each response, but which humans review the cases that automated judges cannot handle — and how to allocate their expensive time where it creates the most value.
