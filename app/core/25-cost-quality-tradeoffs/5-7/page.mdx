# 5.7 — Response Reuse Across Users: Shared Caches and Privacy Boundaries

The cache serves a perfect response. Accurate, well-formatted, exactly what the user asked. Except the response was generated for a different user, from a different company, and it contains references to that company's internal data. The cache just leaked confidential information to a competitor. Nobody noticed for three weeks, because the cache hit rate dashboard showed green and the latency metrics showed improvement. The leak was discovered when a customer support agent at one enterprise client saw another client's contract terms embedded in a response about pricing policy. By the time the engineering team traced the issue to a cache key that did not include tenant scope, the data had been served to fourteen different users across six organizations.

This is the fundamental tension of **shared caching** in AI systems. The more broadly you share cached responses, the higher your hit rates and the lower your costs. But the more broadly you share them, the higher your privacy risk. A global cache that serves any user's response to any other user can achieve hit rates of 40 to 60 percent on common query patterns. A per-user cache that only serves responses back to the same individual might achieve 5 to 15 percent. The gap between those numbers represents real money — tens of thousands of dollars per month at production scale. But the gap also represents the distance between a safe system and a system that is one cache misconfiguration away from a GDPR violation, a broken enterprise contract, and front-page news.

## The Three Scopes of Cache Sharing

Every caching architecture makes a scoping decision, whether the team realizes it or not. The scope determines who can receive a cached response, and that determination is the single most consequential privacy decision in your caching layer.

**Global caches** share responses across all users, all tenants, all contexts. When user A asks "what is the return policy for electronics?" and the system generates a response, that response is stored with a cache key derived from the query. When user B from a different company asks a semantically similar question, the cache serves user A's response. Global caches produce the highest hit rates because the pool of potential matches is the entire user population. For genuinely generic queries — "explain the difference between TCP and UDP," "what are common interview questions for product managers" — global caching is safe and enormously cost-effective. The responses contain no user-specific or tenant-specific information. They are pure knowledge, reusable by anyone.

**Tenant-scoped caches** share responses within an organization but never across organizational boundaries. When an employee at Company A asks about their company's travel expense policy and the system generates a response using Company A's policy documents, that response is cached with a key that includes Company A's tenant identifier. Other employees at Company A who ask the same question get the cached response. Employees at Company B never see it. Tenant-scoped caching captures intra-organization query duplication — which can be substantial, especially around policy questions, onboarding topics, and common workflows — while maintaining the isolation that enterprise customers demand.

**User-scoped caches** serve responses only back to the individual who triggered the original generation. When a user asks a question and the system generates a response, only that same user benefits from the cache on subsequent identical or similar queries. User-scoped caching has the smallest sharing pool and therefore the lowest hit rates. But it is the only scope that protects against both cross-tenant leakage and intra-tenant privacy violations. In systems where users have different permission levels — a manager and a direct report using the same HR assistant, for example — even tenant-scoped caching can leak information that one user should not see to another user in the same organization.

## The Economics of Scope Selection

The financial difference between these scopes is not marginal. It is the difference between caching being a major cost optimization and caching being barely worth the infrastructure overhead.

Consider a B2B SaaS platform serving 200 enterprise customers, each with an average of 500 users, processing 2 million queries per month. With global caching and a 50% hit rate, the platform avoids 1 million inference calls per month. At an average cost of $0.02 per query, that is $20,000 in monthly savings. With tenant-scoped caching at a 25% hit rate, savings drop to $10,000 per month. With user-scoped caching at a 10% hit rate, savings drop to $4,000 per month. The difference between global and user-scoped caching is $16,000 per month — $192,000 per year. That is enough to fund an engineer. It is enough to change a product's unit economics.

But those numbers assume no privacy incidents. A single cross-tenant data leak in a regulated industry does not cost $192,000. It costs millions. The 2024 Air Canada chatbot lawsuit, which involved the company's AI making promises the company did not authorize, resulted in damages that far exceeded any operational savings the system generated. Cross-tenant data leakage in healthcare, finance, or legal contexts triggers HIPAA violations, GDPR fines, and breach notification requirements that dwarf caching savings by orders of magnitude. The EU AI Act, now enforced in 2026, adds additional penalties for AI systems that violate data protection requirements. The expected value calculation is not "how much do we save with global caching?" It is "how much do we save, multiplied by the probability of no incident, minus the cost of an incident multiplied by its probability." For most B2B systems handling any customer-specific data, the expected value calculation favors tenant-scoped or user-scoped caching, despite the lower hit rates.

The exception is genuine commodity knowledge. If your system answers general-knowledge questions that never reference customer data — a coding assistant answering "how do I reverse a linked list in Python," a writing tool responding to "give me a synonym for 'improve'" — global caching is safe and the hit rate gains are pure profit. The key discipline is drawing the line correctly between responses that are genuinely generic and responses that could contain even a trace of customer-specific information.

## Determining What Is Safe to Share

The hard problem is not choosing a cache scope. It is accurately determining which responses are safe for which scope. A response to "what is our company's PTO policy?" is obviously tenant-specific. A response to "what is the Pythagorean theorem?" is obviously global-safe. But most queries fall into ambiguous territory, and the ambiguous cases are where leaks happen.

**Query-based classification** analyzes the incoming query to determine whether it references user-specific or tenant-specific information. Queries that include personal names, account numbers, internal project names, or references to "my" or "our" are flagged as scoped. Queries that are purely general knowledge are flagged as global-safe. This approach is fast — you can classify before checking the cache — but it misses cases where a generic query triggers a tenant-specific response. "What is the refund policy?" sounds generic, but if the system has access to the company's specific refund policy, the response will contain tenant-specific information even though the query did not.

**Response-based classification** analyzes the generated response to determine whether it contains scoped information before writing it to the cache. This is more accurate than query-based classification because it examines the actual content being cached, not just the trigger. But it has two problems. First, it requires generating the response before deciding whether to cache it globally, which means you cannot avoid generation on cache miss — you pay the inference cost either way. Second, automated response classification has a false negative rate. If the classifier misses a company name, a contract term, or an internal reference embedded in the response, the response gets cached at the wrong scope.

**Hybrid classification** combines both. The query is classified first. If the query clearly references scoped information, the response is cached at the appropriate scope without further analysis. If the query appears generic, the response is generated and then classified before cache storage. Responses that the classifier flags as containing scoped content are stored at the appropriate scope. Responses that appear generic are stored in the global cache. The hybrid approach reduces both the false negative rate and the unnecessary generation cost, but it still depends on the accuracy of the response classifier.

The safest production systems add a fourth layer: **deny-listing query patterns and response patterns** that have historically caused leakage. When a leakage incident is detected, the query pattern and the response characteristics are added to a deny list that prevents similar content from entering the global cache in the future. Over time, the deny list accumulates institutional knowledge about what kinds of queries are risky, creating a progressively safer global cache.

## The PromptPeek Problem: Side-Channel Leakage

Even when your response-level caching is properly scoped, the caching infrastructure itself can leak information through side channels. Research published at NDSS 2025 demonstrated a class of attacks called PromptPeek, which exploits timing discrepancies created by KV-cache sharing in multi-tenant LLM serving. The attack works because a cache hit is faster than a cache miss. An attacker who can measure response latency can determine whether their query matches a cached prefix from another user's prompt. By systematically guessing prefixes and observing latency, the attacker can reconstruct portions of other users' prompts without ever seeing the cached content directly.

This is not a theoretical concern. Modern LLM serving frameworks like vLLM and SGLang share KV-cache entries for identical token sequences across users as a memory optimization. The sharing is invisible at the application layer — your caching code might be perfectly scoped, but the inference server is sharing computation underneath. The latency difference between a KV-cache hit and a KV-cache miss can be measured from the client side, creating an information channel that bypasses your application-level privacy controls entirely.

Mitigation requires infrastructure-level awareness. If your threat model includes adversarial tenants — and in any multi-tenant system serving paying customers, it should — you need either per-tenant KV-cache isolation at the serving layer, latency normalization that pads responses to eliminate timing signals, or deployment architectures that physically separate tenants onto different serving instances. Each mitigation has a cost. Per-tenant KV-cache isolation sacrifices the memory efficiency that shared caching provides. Latency normalization adds artificial delay, degrading the user experience for cached responses. Physical separation multiplies infrastructure costs. The right choice depends on your risk profile, your customer base, and whether your customers include competitors who might actively probe each other's cached content.

## Tenant Isolation Architecture

Building a properly scoped cache is not just a software configuration. It requires architectural choices that make cross-tenant leakage structurally impossible rather than merely unlikely.

The simplest and most reliable architecture is **physically separate cache stores per tenant**. Each tenant gets their own Redis instance, their own vector store partition, or their own cache table with no shared keyspace. Cross-tenant leakage requires a configuration error in the infrastructure layer, not just a bug in the application code. This architecture is expensive — you pay for per-tenant infrastructure — but it provides the strongest isolation guarantee and is the easiest to audit. When a customer asks "is our data isolated?" you can show them that their cache is a physically separate system with no shared resources.

The more common architecture is **logically partitioned caching with tenant-scoped keys**. A single cache store serves all tenants, but every cache key includes the tenant identifier as a mandatory prefix or namespace. A query from Tenant A with the key "what is the PTO policy" becomes a cache key like "tenantA-what-is-the-pto-policy." A query from Tenant B with the same content becomes "tenantB-what-is-the-pto-policy." These are different keys that never collide. This architecture is more efficient — one cache infrastructure serves all tenants — but it depends on the application layer to always include the tenant identifier in every cache operation. A single code path that forgets to include the tenant prefix creates a cross-tenant leak.

The vulnerability of logical partitioning is that it relies on every code path, in every service, in every edge case, correctly including the tenant scope. New engineers who do not understand the requirement, edge cases that bypass the normal cache path, batch operations that process multiple tenants in a loop — these are the scenarios where tenant prefixes get dropped. Defense in depth means adding a secondary check: a cache middleware layer that validates every cache read and write against the authenticated tenant context. If a cache read returns a value whose tenant identifier does not match the requesting tenant, the middleware rejects the result and logs an alert. This catch layer does not prevent the bug from occurring, but it prevents the bug from leaking data.

## Intra-Tenant Privacy Boundaries

Tenant-scoped caching solves cross-tenant leakage but introduces a subtler privacy challenge: not everyone within a tenant should see the same information. Organizations have permission hierarchies, role-based access controls, and information compartmentalization. A cached response about upcoming layoffs, generated for an HR director, should not be served to a front-line employee who asks the same question. A cached response about a patient's diagnosis, generated for the treating physician, should not be served to the hospital's billing clerk who happens to query about the same patient.

**Role-aware caching** extends the cache key to include the user's role or permission level. The cache key for an HR director's query about layoff plans includes the HR role in the key. The same query from an employee without HR access is a different cache key and generates a fresh response — one that reflects the model's behavior when it does not have access to HR-restricted documents. This approach is straightforward when roles are static and well-defined, but it becomes complex in organizations with fine-grained or dynamic permissions.

The alternative is to scope the cache not by role but by **information access**. Instead of keying on who is asking, key on what information was available when the response was generated. If the response was generated using documents from the HR-restricted collection, the cache key includes a reference to that collection. Users who do not have access to that collection never match that cache key, because their version of the query does not include that collection in its context. This approach ties cache scoping directly to the data access layer rather than to user roles, which is more robust when permissions are complex or change frequently.

Both approaches reduce hit rates compared to naive tenant-scoped caching. A tenant-scoped cache without role awareness might achieve 25% hit rates. Adding role-based scoping might drop that to 15%. Adding information-access scoping might drop it further to 10%. Each refinement improves privacy at the cost of fewer cache hits. The decision is the same cost-privacy tradeoff that governs every level of cache scoping — but at the intra-tenant level, the consequences of getting it wrong are internal policy violations and potential regulatory issues rather than cross-customer data leaks.

## GDPR, HIPAA, and the Regulatory Dimension

Shared caching in AI systems does not exist in a regulatory vacuum. Every cached response is stored data, and stored data has compliance requirements.

Under GDPR, a cached response that contains personal data about an EU resident is subject to data minimization, purpose limitation, and the right to erasure. If a user exercises their right to be forgotten, every cached response that contains their personal data must be identified and deleted — not just from primary storage, but from every cache tier, every replica, and every backup. Most caching architectures are not designed for selective deletion by data subject. They are designed for bulk expiration through TTL. The gap between GDPR's requirements and caching's natural behavior is a compliance risk that teams discover during audit, not during implementation.

Under HIPAA, cached responses containing protected health information must be encrypted at rest and in transit, access-controlled, and audit-logged. A cache hit that serves a medical response to a user must be logged in the same way that a database read of a medical record is logged. Most in-memory caches — Redis, Memcached — do not provide the access logging that HIPAA requires without additional instrumentation. Teams that cache medical AI responses without adding audit logging to their cache layer are violating HIPAA's access tracking requirements even if the responses themselves are correctly scoped.

The EU AI Act, fully enforced in 2026, adds requirements for AI systems classified as high-risk. If your system falls into a high-risk category — healthcare, employment, credit scoring, law enforcement — cached responses must be traceable. You must be able to explain why a particular response was generated and what data influenced it. A cached response that was generated weeks ago using a different model version, different retrieval context, and different system prompt is difficult to trace. The traceability requirement creates tension with long-lived caches, because the conditions under which a cached response was generated may no longer be reconstructable.

The practical consequence is that regulated systems need shorter cache TTLs, more aggressive invalidation, and richer metadata stored alongside cached responses. Instead of caching only the response, cache the response along with the model version, the system prompt version, the retrieval context summary, and the timestamp. This metadata enables both regulatory traceability and staleness detection, but it increases cache storage requirements and complexity.

## The Privacy-Cost Frontier

There is no single correct cache scoping strategy. There is a frontier — a curve that traces the tradeoff between privacy protection and cost savings. At one extreme, global caching with no scoping maximizes savings and minimizes privacy. At the other extreme, per-user caching with role-aware keys maximizes privacy and minimizes savings. Your system lives somewhere on this curve, and your job is to choose the point that matches your risk profile.

The practical approach is to use multiple scopes simultaneously. Build your cache in layers. The first layer is a global cache that stores only responses that are provably generic — verified by both query classification and response classification, with a deny list that grows over time. This layer captures the high-volume, high-hit-rate commodity queries. The second layer is a tenant-scoped cache that stores responses generated using tenant-specific context. This layer captures the intra-organization duplication. The third layer, if needed, is a role-scoped or user-scoped cache for sensitive domains.

A healthcare AI platform in early 2025 implemented this three-layer approach. Their global cache handled 22% of total queries — general medical knowledge questions like symptom explanations and drug interaction lookups. Their tenant-scoped cache handled 18% — hospital-specific protocol questions and formulary lookups. Their user-scoped cache handled 8% — patient-specific queries that referenced individual medical records. Total cache hit rate: 48%. Total cost savings: $37,000 per month. If they had used only global caching, hit rates would have been higher but the first cross-patient data leak would have cost them their HIPAA compliance status. If they had used only user-scoped caching, hit rates would have been 8% and the infrastructure overhead would have barely justified the savings.

The layered approach is more complex to build and operate. It requires three cache stores, three sets of cache keys, three invalidation policies, and classification logic that routes queries to the correct layer. But it captures most of the economic value of caching while respecting the privacy boundaries that each layer demands. The complexity is the cost of operating in a world where performance and privacy are both non-negotiable.

## Auditing and Incident Response

Even the best-designed cache scoping system will eventually have an incident. A code change that bypasses the tenant prefix. A classifier that misses a customer name in a response. A configuration migration that temporarily disables scope enforcement. The question is not whether it will happen but how quickly you detect and contain it.

**Proactive auditing** means regularly sampling cached responses and verifying that their scope is correct. Pull a random sample of responses from the global cache and manually inspect them for tenant-specific or user-specific content. If you find even one, you have a classification gap that needs immediate repair. The sample size does not need to be large — 50 to 100 responses per week is enough to catch systematic issues. The audit should be automated where possible: run a classifier over the sampled responses and flag anything that looks scoped. Manual review catches what the classifier misses.

**Incident response for cache leakage** must be faster than for most data incidents because cached responses are actively being served to users. The moment a cross-tenant cache leak is detected, the global cache must be flushed entirely. Not just the offending entry — the entire global cache. You do not know how many other entries have the same classification error, and the cost of a full cache flush (a temporary hit rate drop to zero, which recovers over hours) is trivial compared to the cost of continuing to serve leaked data. After the flush, identify the root cause, fix the classification or scoping bug, rebuild the deny list to include the pattern that caused the leak, and re-enable global caching only after the fix is verified.

The organizational impact of a cache leakage incident goes beyond the immediate technical remediation. Enterprise customers who learn that their data was served to another customer lose trust in ways that are difficult to recover. The incident disclosure, the root cause analysis, the remediation plan, the third-party audit — these are months of work that consume executive attention, legal resources, and customer success capacity. Every hour invested in proper cache scoping, classification accuracy, and proactive auditing reduces the probability of this scenario. The investment is not overhead. It is insurance that pays off by not being needed.

Measuring the effectiveness of your caching system — not just its hit rates, but the quality of the responses it serves — is the final piece of the caching puzzle and the subject of the next subchapter.