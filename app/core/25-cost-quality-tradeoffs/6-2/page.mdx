# 6.2 — Sampling Strategies: Evaluating a Fraction Without Losing Signal

You do not need to evaluate every response to know whether your system is working. You need to evaluate enough responses, of the right kind, at the right frequency, to detect problems before they reach users at scale. This is not a compromise. It is statistics. A well-designed sample of 400 responses drawn from a population of 500,000 tells you as much about your system's quality as evaluating every single one — with a margin of error so small that it would not change any decision you make. The difference is that evaluating 400 responses costs $12 in LLM judge fees. Evaluating 500,000 costs $15,000. The information gained is nearly identical. The cost is not.

The resistance to sampling usually comes from a reasonable fear: what if the sample misses the one failure that matters? What if the critical defect is in the 499,600 responses you did not evaluate? This fear is real but statistically manageable. The entire discipline of quality engineering — from manufacturing to software testing to clinical trials — is built on the principle that you can draw reliable conclusions from representative samples. AI evaluation is no different. The key word is representative. A bad sample is worse than no sample because it gives you false confidence. A good sample is nearly as informative as exhaustive evaluation at a fraction of the cost.

## The Statistics You Need (and Nothing More)

Statistical sampling for evaluation rests on a few core concepts that you need to understand well enough to make sound decisions. You do not need a statistics degree. You need to understand sample size, confidence level, margin of error, and how they interact.

**Sample size** is the number of responses you evaluate. **Confidence level** is the probability that your sample result reflects the true population value — a 95 percent confidence level means that if you repeated the sampling process 100 times, 95 of those samples would produce a result within the margin of error of the true value. **Margin of error** is the range around your sample estimate within which the true value is likely to fall. A margin of error of plus or minus 2 percentage points means that if your sample shows a 94 percent pass rate, the true pass rate is almost certainly between 92 and 96 percent.

These three values are linked by a formula that every evaluation engineer should have internalized. For a population proportion — like your defect rate — the required sample size depends on the confidence level, the expected proportion, and the desired margin of error. At a 95 percent confidence level with an expected defect rate of 5 percent and a desired margin of error of plus or minus 2 percentage points, the required sample size is approximately 457 responses. At a 99 percent confidence level with the same defect rate and margin, it rises to approximately 790. At 95 percent confidence with a tighter margin of plus or minus 1 percentage point, it jumps to approximately 1,825.

The critical insight is that sample size depends on the margin of error and confidence level you need, not on the population size. Whether your system generates 50,000 responses per month or five million, the sample size required for the same confidence and margin is approximately the same. This is counterintuitive but mathematically ironclad. A sample of 500 from a population of 50,000 and a sample of 500 from a population of 5,000,000 provide the same statistical precision. The sample size formula has a population correction factor, but it only matters when the sample is a large fraction of the population — say, more than 5 percent. For most production AI systems where you are sampling a tiny fraction of traffic, population size is irrelevant.

## The Economics of Sampling

The economic case for sampling is overwhelming once you see the numbers. Consider a system generating one million responses per month, evaluated on three quality dimensions using an LLM judge at $0.03 per evaluation call. Exhaustive evaluation costs three million judge calls at $0.03 each — $90,000 per month. A random sample of 1,000 responses evaluated on three dimensions costs 3,000 judge calls — $90 per month. The sample provides a 95 percent confidence estimate with a margin of error of approximately plus or minus 3 percentage points for a 5 percent defect rate.

That is a 99.9 percent reduction in evaluation cost for a measurement that is statistically robust enough to drive every quality decision you need to make. If your defect rate is 5 percent and your sample says it is somewhere between 2 and 8 percent, that is enough information to know whether you have a problem. If your defect rate shifts from 5 percent to 12 percent, your sample will detect it. If you need tighter precision — say, distinguishing 5 percent from 6 percent — increase the sample to 5,000 at a cost of $450 per month. Still a rounding error compared to $90,000.

The savings scale with traffic. At ten million responses per month, exhaustive evaluation costs $900,000. The same 1,000-response sample still costs $90. The gap between exhaustive and sampled evaluation widens as traffic grows, which means sampling becomes more valuable — not less — as your system scales. This is exactly the property you want in a cost optimization strategy: it gets better at exactly the moment you need it most.

## Random Sampling: The Baseline

**Random sampling** is the simplest and most robust sampling strategy. Every response has an equal probability of being selected for evaluation. The implementation is trivial: generate a random number for each response, and if it falls below your sampling rate, route the response to the evaluation pipeline. A 0.2 percent sampling rate on a system generating 500,000 responses per month yields 1,000 samples — enough for solid statistical estimates.

Random sampling's strength is its lack of bias. Because every response has an equal chance of selection, the sample naturally represents the overall distribution of your traffic. If 15 percent of your traffic is customer support queries, approximately 15 percent of your sample will be customer support queries. If 3 percent of your responses contain defects, approximately 3 percent of your sample will contain defects. You do not need to know anything about your traffic distribution in advance. The randomness handles the representativeness for you.

Random sampling's weakness is that it is indiscriminate. It does not know which responses are more important to evaluate. A random sample of 1,000 from a system that serves enterprise clients and free-tier users will contain samples proportional to traffic — which means if free-tier users generate 80 percent of traffic, 80 percent of your evaluation budget goes to free-tier quality even if enterprise quality is what matters most to your business. Random sampling treats all responses as equally valuable for evaluation purposes, which is almost never true from a business perspective.

Random sampling is the right starting point when you have no prior knowledge about where quality issues concentrate, when you need a baseline defect rate, or when your traffic is relatively homogeneous. It is also the right fallback when your stratification logic breaks or when you are evaluating a new feature with no historical data.

## Stratified Sampling: Evaluating What Matters Most

**Stratified sampling** divides your traffic into subgroups — strata — and samples from each stratum independently. The strata are defined by characteristics that correlate with quality variation or business importance: customer tier, query type, model version, geographic region, time of day, or any other dimension along which quality might differ.

The power of stratified sampling is that it lets you allocate your evaluation budget proportionally to importance rather than proportionally to traffic volume. Consider a SaaS product with three customer tiers: free (80 percent of traffic), professional (15 percent), and enterprise (5 percent). Enterprise clients pay 100 times more than free users and churn at a cost of $200,000 per lost account. A random sample of 1,000 would include approximately 50 enterprise responses — too few for a reliable quality estimate on the segment that matters most. A stratified sample might allocate 400 samples to enterprise, 400 to professional, and 200 to free, over-sampling the high-value segments to ensure you have enough data for statistically valid estimates where the business stakes are highest.

Stratified sampling also improves statistical precision for the same total sample size. When quality varies across strata — which it almost always does, because different query types pose different challenges — sampling within strata reduces the variance of your estimate compared to a single random sample. A customer support system where billing queries have a 2 percent defect rate and technical queries have an 8 percent defect rate will produce a more precise overall estimate with stratified sampling than with random sampling, because the within-stratum variance is lower than the overall variance.

The implementation requires defining your strata, which means you need metadata on each response: customer tier, query category, model version, whatever dimensions you choose. Most production systems already have this metadata in their logging pipelines. The stratification logic then routes each response to the appropriate stratum's evaluation queue, where it is sampled at the rate defined for that stratum. Enterprise responses might be sampled at 2 percent. Free-tier responses might be sampled at 0.1 percent. The total evaluation volume is the same, but the distribution shifts toward the segments that matter most.

The risk of stratified sampling is getting the strata wrong. If you stratify by customer tier but the real quality variation is driven by query complexity — which cuts across tiers — your strata do not capture the variation that matters, and your estimates are less precise than they should be. The strata should be defined by the dimensions that drive quality variation, not just the dimensions that are convenient. If you do not know which dimensions drive quality variation, start with random sampling, analyze the results for patterns, and then build your stratification from the patterns you discover.

## Adversarial Sampling: Hunting for Rare-But-Critical Failures

**Adversarial sampling** deliberately over-samples from categories where quality failures are most likely or most consequential. Instead of representing traffic proportionally, adversarial sampling concentrates evaluation effort on the edges — the hard cases, the ambiguous queries, the high-risk categories where defects cluster.

The motivation is statistical: rare events require large samples to detect reliably. If your overall defect rate is 3 percent but defects concentrate in a specific query type that represents 5 percent of traffic, a random sample of 1,000 would include only 50 responses from that category, of which approximately 1 to 2 would be defective. That is not enough to estimate the category-specific defect rate with any precision. Adversarial sampling might allocate 200 of your 1,000 samples to that category, giving you enough data to estimate whether the category defect rate is 3 percent, 10 percent, or 25 percent — a distinction that matters enormously for prioritizing quality improvements.

There are three approaches to identifying which categories to over-sample. The first is **historical analysis**: examine past evaluation data for categories with elevated defect rates and over-sample from those categories going forward. This is backward-looking but reliable — categories with quality problems tend to maintain them until the root cause is fixed. The second is **heuristic flagging**: use cheap, fast heuristics to identify responses that might have quality issues — responses that are unusually long or short, responses on sensitive topics, responses that triggered a guardrail check, responses where the model's confidence score was low. These heuristics are not evaluations themselves, but they are signals that the response deserves evaluation. The third is **novelty detection**: identify queries that are dissimilar to the training distribution or to recent traffic patterns. Novel queries are more likely to produce quality failures because the system has less experience with them. Embedding the query and measuring its distance from the centroid of recent queries provides a novelty score that can be used for sampling prioritization.

A fraud detection company applied adversarial sampling to their AI-powered case narrative generator. They identified three high-risk categories: cases involving cross-border transactions, cases with incomplete data fields, and cases where the model's generation length exceeded twice the median. These three categories represented 12 percent of traffic but accounted for 65 percent of quality failures. By allocating 40 percent of their evaluation sample to these categories, they detected defect rate changes in the high-risk segments within days rather than weeks — catching a 5-point quality regression in cross-border narratives just two days after a model update that random sampling would have taken ten days to surface.

## Temporal Sampling: When You Evaluate Matters

Sampling is not only about which responses to evaluate. It is also about when. **Temporal sampling** adjusts your sampling strategy based on time — increasing evaluation density at high-risk moments and reducing it during stable periods.

The highest-risk moment for any AI system is immediately after a change: a model update, a prompt modification, a retrieval pipeline change, a new data source. Quality is most likely to regress in the hours and days following a change, and this is when evaluation is most valuable. A temporal sampling strategy might set a baseline sampling rate of 0.1 percent during stable periods and increase it to 2 percent for the first 48 hours after any deployment. The increased sampling provides rapid feedback on whether the change introduced regressions, at a cost that is temporary — once the post-deployment period ends and quality is confirmed, sampling returns to the baseline rate.

Temporal sampling also accounts for cyclical patterns. Many AI systems experience quality variation correlated with usage patterns — quality might degrade during peak hours when latency increases, or during specific days of the week when traffic composition shifts. Sampling more heavily during these periods provides better visibility into the quality patterns that matter to users who experience the system during its worst moments, not its average moments.

The implementation is a sampling rate function that takes the current time, the time since last deployment, and any contextual signals as inputs and outputs a sampling rate. During stable periods far from any deployment, the rate is low. During the first hours after a deployment, the rate spikes. During historically problematic periods — Monday mornings, end-of-quarter traffic surges — the rate increases modestly. The total evaluation volume stays roughly constant over time, but the distribution shifts toward the moments when evaluation is most valuable.

## The Risks of Under-Sampling

Sampling is powerful, but under-sampling is dangerous. There are three failure modes that teams must guard against.

**Failure mode one: insufficient sample for rare defects.** If your defect rate is 0.5 percent and you sample 200 responses, you expect to see one defect in your sample. One observation is not enough to estimate anything. You cannot tell whether the defect rate is 0.1 percent or 2 percent from a single observation. For rare defect rates, you need larger samples or you need adversarial sampling that over-represents the categories where defects occur. The general rule: if you expect fewer than 20 defective items in your sample, your sample is too small for reliable estimation of that defect rate.

**Failure mode two: non-representative samples from convenience sampling.** Some teams "sample" by evaluating whatever is easiest to evaluate — the first 100 responses of the day, responses from a specific customer, responses shorter than a certain length. This is not random sampling. It is convenience sampling, and it produces biased estimates that may not reflect the true population quality. The first 100 responses of the day may come from a different time zone with different query patterns. Responses from a specific customer may not represent your customer base. Short responses may be systematically easier or harder than average. Convenience sampling is the most common sampling mistake, and it creates false confidence that is worse than no evaluation at all.

**Failure mode three: stale sampling parameters.** A sampling strategy designed for a system generating 100,000 responses per month may under-sample when the system scales to one million. A stratification scheme built on last quarter's traffic patterns may not reflect this quarter's traffic composition. Sampling parameters must be reviewed regularly — at least quarterly, and immediately after any significant change in traffic volume, traffic composition, or system architecture. The review should answer: are we sampling enough in each stratum for statistically valid estimates? Has our traffic composition shifted? Are our adversarial categories still the right ones?

## Combining Sampling Strategies

In practice, the most effective evaluation sampling is not a single strategy but a combination. The **Layered Sampling** approach combines random, stratified, and adversarial sampling into a unified framework.

The base layer is a random sample that provides an unbiased estimate of overall system quality. This sample is your ground truth — it tells you the true defect rate without any selection bias. Set this at a level that provides 95 percent confidence with a plus or minus 2 percentage point margin. For most systems, this means 500 to 2,000 random samples per evaluation cycle.

The stratification layer sits on top of the random sample, adding additional samples from high-importance strata. This layer ensures that you have enough data for stratum-specific quality estimates on the segments that matter most to your business. The additional samples from over-represented strata are excluded from the overall defect rate calculation — they are used for stratum-specific analysis only — to avoid biasing the overall estimate.

The adversarial layer adds targeted samples from high-risk categories identified by heuristics, historical analysis, or novelty detection. These samples are used for early warning detection of emerging quality problems. They are not included in overall quality metrics because they are intentionally biased toward problematic cases.

The total cost of the layered approach is typically 2,000 to 5,000 evaluated responses per cycle, regardless of total traffic volume. At $0.03 per evaluation call across three judge dimensions, that is $180 to $450 per cycle. Even with weekly cycles, the monthly cost stays under $2,000 — a fraction of what exhaustive evaluation would cost.

## Calculating Your Sampling Parameters

Here is the practical sequence for setting up your sampling strategy from scratch.

Step one: determine your baseline defect rate. If you have historical evaluation data, use it. If not, run an exhaustive evaluation on one day of traffic — accept the one-time cost — to establish the baseline. This is the single most important number in your sampling design.

Step two: choose your confidence level and margin of error. For most production systems, 95 percent confidence with a margin of error of plus or minus 2 percentage points is sufficient for overall quality estimation. For safety-critical systems, use 99 percent confidence with a margin of error of plus or minus 1 percentage point.

Step three: calculate the required random sample size. With a 5 percent defect rate, 95 percent confidence, and a 2-point margin, you need approximately 457 responses. Round up to 500 for clean numbers. With a 1 percent defect rate under the same parameters, you need approximately 380 — the lower the defect rate, the smaller the sample, because there is less variability in the population.

Step four: define your strata and allocate additional samples. Identify the two to four dimensions that drive quality variation or business importance. For each stratum, determine the minimum sample size for a stratum-specific quality estimate — typically 100 to 200 per stratum. Allocate additional samples beyond the random base to meet these stratum-specific minimums.

Step five: identify your adversarial categories and set their sampling rates. These should be the categories where defects are most likely or most costly. Set the adversarial sampling rate high enough to detect defect rate changes of 5 percentage points or more within one evaluation cycle.

Step six: set your evaluation frequency. Weekly cycles are standard for stable systems. Daily cycles — or continuous sampling — are appropriate during post-deployment windows. Monthly cycles are acceptable only for very mature systems with consistently low defect rates. Review and adjust all parameters quarterly.

## From Sampling to Action

Sampling produces data. Data without action is waste. Every sampling result should feed directly into a decision: is quality stable, improving, or degrading? If degrading, in which stratum? If improving, can evaluation effort be reduced?

Build automated alerts on your sampled metrics. If the sampled defect rate in any stratum exceeds its historical baseline by more than 3 percentage points, trigger an alert. If the overall defect rate trends upward across three consecutive cycles, trigger a review. If a specific category's defect rate spikes — which adversarial sampling will surface faster than random sampling — trigger a root cause investigation.

The goal is a feedback loop: sample, measure, decide, act. The sampling strategy tells you what to evaluate. The evaluation tells you where quality stands. The analysis tells you what to fix. The fix reduces the defect rate. The reduced defect rate allows you to reduce your sample size. The reduced sample size lowers your eval tax. This is the virtuous cycle that evaluation cost management aims for — not cutting eval spend for its own sake, but reaching a state where quality is high enough and evaluation is efficient enough that the eval tax is small and sustainable.

Sampling tells you how much to evaluate. But there is another lever that determines how much each evaluation costs: the model you use as a judge. Not every evaluation dimension needs a frontier model, and matching the judge to the judgment is one of the highest-return optimizations in the eval cost toolkit. That is the focus of the next subchapter.
