# 9.4 — Build vs Buy Cost Curves: When Custom Models Beat API Providers

The default assumption in AI engineering is that API providers are cheaper than self-hosting. You call an endpoint, you pay per token, and someone else worries about GPUs, uptime, and model optimization. This is true at low volume. It is true at moderate volume. And then, at a specific volume and use-case specificity, it stops being true. The cost curves cross. Building becomes cheaper than buying. Most teams either switch too early, burning six months of engineering effort to save money they would not have spent, or too late, hemorrhaging tens of thousands of dollars per month on API bills that a single GPU cluster could have eliminated. Understanding exactly where the curves cross, and what forces shift the crossover point, is the difference between strategic infrastructure investment and expensive guesswork.

The crossing is not hypothetical. A logistics company processing 4 million natural-language shipment queries per month through a major API provider was spending $38,000 per month on inference alone. When they benchmarked Llama 4 Scout running on two leased H100 GPUs at a total infrastructure cost of $7,200 per month, the quality difference on their specific use case was negligible — within 1.2 percentage points on their eval suite. The API had been the right choice at 200,000 queries per month. At 4 million, it was the wrong choice by a factor of five. But the team that made the switch spent three months building the serving infrastructure, hiring an MLOps engineer, and handling the operational complexity that API providers abstract away. The cost savings were real. So were the hidden costs.

## The Two Cost Curves

Every AI inference decision involves two cost models, and they behave fundamentally differently as volume scales.

**The API cost curve** is linear. You pay per token or per request. At 100,000 requests per month, you pay X. At 1 million requests, you pay 10X. At 10 million, you pay 100X. There may be volume discounts — most providers offer tiered pricing that reduces per-token cost at higher volumes — but the relationship remains roughly proportional. Double your volume, roughly double your cost. The slope of this line is your per-request API price, and it does not change with your infrastructure decisions. The beauty of this model is simplicity: your cost is predictable, scales with usage, and requires zero infrastructure investment.

**The self-hosted cost curve** is a step function with a declining per-request cost. Your first GPU cluster has a fixed monthly cost regardless of whether it serves one request or one million. That fixed cost might be $3,500 per month for a leased H100 or $7,000 per month for a cluster of two. At low volume, your per-request cost is astronomical — $3,500 divided by 10,000 requests is $0.35 per request, far more than any API. But as volume increases, the per-request cost drops. At 500,000 requests per month, that same $3,500 cluster costs $0.007 per request. At 2 million requests, it costs $0.00175. The cost per request approaches zero as volume approaches the throughput capacity of your infrastructure.

The crossover is where these two lines meet. Below the crossover volume, the API is cheaper because you are paying only for what you use, with no fixed costs. Above the crossover volume, self-hosting is cheaper because you are amortizing fixed costs across enough requests to beat per-token pricing. Every build-versus-buy decision is ultimately a bet on which side of the crossover your volume will land.

## Calculating the Crossover Point

The math is straightforward, but it requires honest accounting on both sides.

Start with your monthly API cost at current volume. If you are processing 1 million requests per month with an average of 600 input tokens and 200 output tokens per request, and your API provider charges $0.50 per million input tokens and $1.50 per million output tokens, your monthly API cost is 600 million input tokens times $0.50 per million plus 200 million output tokens times $1.50 per million. That works out to $300 in input costs plus $300 in output costs, totaling $600 per month. At that volume and those prices, the API is almost certainly cheaper than self-hosting.

Now calculate the self-hosted alternative. A single leased H100 80GB running an optimized Llama 4 Scout instance with vLLM or TensorRT-LLM can serve roughly 300 to 600 requests per minute for a workload with 600-token inputs and 200-token outputs, depending on batching configuration and quantization settings. That is 430,000 to 860,000 requests per day, or roughly 13 million to 26 million requests per month at full utilization. The lease cost for a single H100 through a cloud GPU provider in early 2026 ranges from $2,500 to $4,000 per month on a reserved contract, depending on the provider and contract length. Add $500 to $1,000 per month for associated infrastructure: networking, storage, monitoring, and the serving framework itself. Your total self-hosted cost is roughly $3,000 to $5,000 per month for capacity that can handle 13 to 26 million requests per month.

The crossover happens when your monthly API cost exceeds your monthly self-hosted cost. In this example, if your API cost is $0.60 per thousand requests and your self-hosted cost is $4,000 per month, the crossover is at roughly 6.7 million requests per month. Below that, the API is cheaper. Above that, self-hosting wins. But this calculation is incomplete — it counts only the infrastructure cost of self-hosting, not the full cost.

## The Hidden Costs That Shift the Crossover

The infrastructure lease is the smallest cost of self-hosting. The costs that teams consistently underestimate are the ones that push the crossover point higher — sometimes dramatically higher.

**Engineering time** is the largest hidden cost. Running a production inference endpoint requires someone to set up the serving infrastructure, optimize batching parameters, configure auto-scaling, handle GPU failures, manage model updates, and debug performance regressions. Industry experience consistently shows that a minimum viable self-hosted inference operation requires 0.5 to 1.0 full-time equivalent engineers dedicated to the serving infrastructure. At a fully loaded cost of $180,000 to $250,000 per year for an ML infrastructure engineer in 2026, that is $15,000 to $21,000 per month in engineering cost alone. Add this to your $4,000 infrastructure cost and your true self-hosted cost is $19,000 to $25,000 per month. The crossover just moved from 6.7 million requests to somewhere between 32 million and 42 million requests per month.

**Reliability engineering** is the second hidden cost. API providers run redundant infrastructure across multiple availability zones, handle failover automatically, and maintain 99.9 percent or higher uptime SLAs. Self-hosted infrastructure means you own the uptime. A GPU failure at 2 AM is your problem. A driver update that breaks CUDA compatibility is your problem. Maintaining redundancy — running a second GPU that sits mostly idle as a failover — doubles your infrastructure cost. Most teams start without redundancy and learn why they need it after their first multi-hour outage.

**Model management** is the third hidden cost. When you use an API, the provider handles model updates, performance optimizations, and deprecation transitions. When you self-host, you manage the model lifecycle: downloading new model weights, converting them to your serving framework's format, running eval suites against the new version, performing gradual rollouts, and maintaining rollback capability. Each model update cycle takes engineering time and introduces risk.

**GPU procurement complexity** is the fourth hidden cost. In 2026, high-end GPU availability has improved compared to the severe shortages of 2023-2024, but demand still outpaces supply for the latest generation hardware. Reserved contracts offer the best pricing but lock you into one to three year commitments. On-demand instances are more flexible but cost 50 to 150 percent more. Spot instances are cheapest but can be preempted, making them unsuitable for production inference without a robust fallback strategy. Navigating this landscape — choosing the right provider, the right commitment level, and the right GPU generation — is itself a cost in executive and engineering attention.

## The Non-Cost Factors That Move the Crossover

Cost is not the only variable in the build-versus-buy equation. Several non-cost factors shift the crossover point, some earlier and some later.

**Data privacy requirements** pull the crossover earlier. If your use case involves sensitive data — patient health records, financial transactions, legal documents, classified government data — sending that data to a third-party API creates regulatory and compliance risk. The EU AI Act, HIPAA, and various sector-specific regulations impose constraints on where data can be processed and who can access it. Self-hosting eliminates the third-party data exposure entirely. For teams in regulated industries, the compliance cost of using an external API — data processing agreements, audit trails, vendor risk assessments, and the ongoing legal review of API provider terms of service — can be significant enough that self-hosting becomes the cheaper option even at relatively low volume. A healthcare AI company processing 500,000 queries per month might find that self-hosting is cheaper not because the infrastructure math works out, but because the compliance overhead of using an external API adds $8,000 per month in legal and audit costs.

**Custom model requirements** pull the crossover earlier. If you need a fine-tuned model trained on your proprietary data, self-hosting gives you complete control over the training and serving pipeline. Some API providers offer fine-tuning services, but they constrain your choices: you fine-tune their supported base models using their infrastructure with their training configurations. Self-hosting lets you choose any open-weight model, apply any training technique — LoRA, full fine-tuning, DPO, whatever fits your use case — and serve the result without per-token API markup on your custom weights. The markup that API providers charge on fine-tuned model inference is often two to five times the per-token rate of serving the same model yourself.

**Speed of iteration** pushes the crossover later. API providers let you switch models with a single parameter change. Moving from GPT-5-mini to Claude Sonnet 4.5 takes one API call change. Self-hosted infrastructure makes model switching a multi-day project: downloading new weights, adapting your serving configuration, re-running eval suites, redeploying. If your product is in active experimentation mode — testing different models weekly, A/B testing model variants, rapid-prototyping new features — the iteration speed advantage of APIs is worth real money in engineering time and opportunity cost.

**Latency control** can push either direction. API providers introduce network latency between your application and the inference endpoint, typically 20 to 80 milliseconds of overhead beyond the model's generation time. Self-hosted infrastructure co-located with your application servers eliminates this overhead. For latency-sensitive applications — real-time voice assistants, interactive coding tools, trading systems — the latency advantage of self-hosting can justify earlier adoption. For latency-tolerant applications — batch processing, asynchronous content generation, email summarization — the latency difference is irrelevant.

## The 2026 Cost Landscape: Specific Comparisons

The numbers change rapidly, but the framework is stable. Here is a realistic comparison as of early 2026 for a common workload: a customer support chatbot processing 3 million requests per month with 800 average input tokens and 300 average output tokens per request.

Option A is an API-hosted frontier model. Using Claude Sonnet 4.5 at $3 per million input tokens and $15 per million output tokens, the monthly inference cost is 2.4 billion input tokens times $3 per million plus 900 million output tokens times $15 per million. That works out to $7,200 plus $13,500, totaling $20,700 per month. No infrastructure cost, no engineering overhead, no GPU procurement.

Option B is a self-hosted open-weight model. Using Llama 4 Scout, a mixture-of-experts model that achieves comparable quality to mid-tier commercial APIs on customer support tasks, running on a cluster of four H100 GPUs leased at $3,200 per month each. Total infrastructure cost is $12,800 per month. Associated infrastructure adds $2,000 per month. Engineering allocation at 0.75 FTE costs $14,000 per month. Total self-hosted cost is $28,800 per month.

At 3 million requests per month, the API is $8,100 per month cheaper. The crossover does not happen until roughly 5 to 6 million requests per month for this specific comparison, assuming the API pricing remains fixed.

Option C is the hybrid approach that sophisticated teams increasingly adopt. Use the self-hosted Llama 4 Scout for the 70 percent of requests that are straightforward — common questions, simple lookups, standard responses — and route the remaining 30 percent of complex or ambiguous queries to the API-hosted frontier model. The self-hosted infrastructure handles 2.1 million requests per month at $14,800 in infrastructure and engineering cost. The API handles 900,000 requests per month at $6,200 in inference cost. Total hybrid cost is $21,000 per month — roughly matching the API-only option but with a path to savings as volume grows, since the self-hosted portion scales at near-zero marginal cost.

## The Hybrid Strategy: Why Pure Build or Pure Buy Is Rarely Optimal

The cleanest build-versus-buy framing is also the most misleading. In practice, the optimal strategy is almost always a hybrid, and the mix shifts as volume and requirements evolve.

The hybrid works because different requests have different cost-quality requirements. In a customer support system, 60 to 70 percent of incoming queries are repetitive, well-understood, and require no frontier-level reasoning. A self-hosted small or medium model handles these at a fraction of the API cost. The remaining 30 to 40 percent involve nuanced judgment, complex multi-step reasoning, or sensitive topics that benefit from a frontier model's capabilities. Routing these to an API-hosted frontier model delivers the quality where it matters while containing total cost.

The hybrid also manages risk. If your self-hosted infrastructure goes down, the API provides automatic fallback. If the API provider raises prices, has an outage, or deprecates your model, the self-hosted infrastructure provides continuity. Neither pure build nor pure buy offers this resilience. The dual infrastructure has a cost — you maintain two serving paths instead of one — but that cost is insurance against the operational risks of depending entirely on either approach.

The transition path from buy to hybrid to build typically follows a predictable pattern. Phase one is pure API: you launch with an API provider, validate the product, and gather production traffic data. Phase two is hybrid: you introduce self-hosted infrastructure for the highest-volume, lowest-complexity workloads while keeping the API for everything else. Phase three is majority self-hosted: as volume grows and your team's operational maturity increases, you shift more traffic to self-hosted infrastructure, keeping the API only for overflow, fallback, and the most demanding queries. Phase four — full self-hosting — is rare and usually only justified for organizations with massive scale, deep ML engineering teams, and workloads that have stabilized enough that iteration speed no longer requires API flexibility.

## Decision Framework: When to Start the Transition

The decision to begin investing in self-hosted infrastructure should not be triggered by a single threshold. It should be triggered by the convergence of multiple signals.

**Volume signal:** your monthly API spend has exceeded $15,000 to $20,000 and is growing. Below this threshold, the engineering investment in self-hosting almost never pays for itself. Above it, the economics start to shift.

**Predictability signal:** your traffic patterns are stable enough that you can forecast monthly volume within 20 percent accuracy. Self-hosted infrastructure economics depend on consistent utilization. If your traffic is wildly variable — 500,000 requests one month, 3 million the next — you cannot size infrastructure efficiently, and the API's pay-per-use model is more forgiving.

**Specificity signal:** your use case has narrowed to a well-defined task where an open-weight model, possibly fine-tuned, matches API model quality. General-purpose applications where you need the best possible model for open-ended queries are harder to self-host. Specific applications where you have tested and validated that Llama 4 Scout or Mistral Large 3 meets your quality bar are strong candidates.

**Talent signal:** you have or can hire at least one engineer with production ML serving experience. Attempting self-hosting without this expertise is a common and expensive failure mode. The GPU lease is cheap. The engineer who knows how to make vLLM serve requests at 95th percentile latency under 400 milliseconds is not.

**Compliance signal:** your regulatory environment requires data residency, on-premises processing, or audit trails that external APIs cannot satisfy. This is the one signal that can justify self-hosting even at low volume, because the alternative is not cheaper API pricing — it is legal non-compliance.

When three or more of these signals are present simultaneously, the build-versus-buy calculus has shifted. Start with a proof-of-concept: deploy a self-hosted model alongside your API, route a small percentage of traffic to it, and measure cost, quality, and operational burden in parallel. If the proof-of-concept confirms the economics and your team can sustain the operational load, begin the gradual transition to hybrid.

## The Crossover Is Not Permanent

One of the most underappreciated dynamics in build-versus-buy decisions is that the crossover point itself moves over time. API prices have dropped consistently since 2023 — frontier model inference that cost $60 per million tokens in early 2024 costs $3 to $5 per million tokens in early 2026 for comparable quality. Every price drop pushes the crossover point higher, meaning you need more volume to justify self-hosting.

Simultaneously, GPU costs are also dropping, and open-weight model quality is rising. Llama 4 Scout and Maverick, DeepSeek V3.2, and Mistral Large 3 deliver quality that was only available from closed commercial APIs eighteen months ago. Every quality improvement in open-weight models pulls the crossover point lower, meaning you need less volume to justify self-hosting.

These opposing forces mean that the crossover is a moving target. A team that calculated the crossover at 5 million requests in January 2026 might find it has moved to 7 million by July if API prices drop faster than GPU costs. Or it might have moved to 3 million if a new open-weight model closes the quality gap on their specific task. The implication is that the build-versus-buy analysis is not a one-time decision. It is a quarterly review. Recalculate the economics every three months using current pricing, current model quality benchmarks, and current volume projections. The team that made the right decision six months ago may be making the wrong decision today.

## What Happens When You Get It Wrong

Switching too early is expensive in engineering opportunity cost. A startup that spends three months building self-hosted inference infrastructure to save $4,000 per month in API costs has invested roughly $60,000 to $90,000 in engineering time (at typical startup engineering costs) to generate $48,000 per year in savings. The payback period is one to two years, assuming the savings hold — and they may not if the API provider drops prices. Those three months of engineering time could have been spent building product features that drive revenue.

Switching too late is expensive in direct cost. An enterprise processing 20 million requests per month through an API at $0.01 per request is spending $200,000 per month. Self-hosted infrastructure for the same workload might cost $40,000 per month in infrastructure and $20,000 per month in engineering — a savings of $140,000 per month. Every month of delay costs $140,000. A team that waits six months too long to start the transition has wasted $840,000.

The asymmetry matters. Switching too early wastes engineering time, which is recoverable — the engineers go back to product work. Switching too late wastes cash, which is gone. If you are uncertain about the timing, the conservative strategy is to begin the proof-of-concept phase slightly before you think the economics justify it, so that you are ready to transition quickly when the crossover arrives.

The build-versus-buy crossover is the most visible inflection point. But there is a deeper, more general version of the same dynamic: the moment when any fixed-cost infrastructure investment starts winning against variable-cost alternatives. That infrastructure amortization inflection applies not just to GPU clusters but to caching infrastructure, routing logic, safety pipelines, and every other component of your AI system.
