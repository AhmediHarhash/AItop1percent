# 3.7 — Multi-Model Pipelines: Orchestrating Cheap and Expensive Models Together

In early 2025, a legal technology company processing corporate contracts ran every stage of their document pipeline through a single frontier model. The pipeline had four stages: extracting key clauses from uploaded PDFs, classifying each clause by legal category, summarizing the contract's obligations and risks, and generating a plain-language explanation for the client's legal team. Every stage called Claude Opus 4. Every stage paid frontier-model pricing. The monthly bill ran to $87,000 for roughly 120,000 contracts processed.

A new engineering lead joined in April and audited the pipeline. She ran a week-long experiment: for each stage independently, she swapped in progressively cheaper models and measured whether the downstream output changed. The results exposed a staggering amount of waste. The extraction stage — pulling structured fields like party names, dates, and dollar amounts from semi-structured text — performed identically on a model one-fifth the cost. The classification stage, which assigned each clause to one of twenty-three categories, actually performed better on a fine-tuned small model that had been trained specifically on the company's taxonomy. The summarization stage lost measurable quality with anything below a mid-tier model, but did not require frontier capability. Only the final generation stage — producing nuanced, client-facing explanations — benefited from the frontier model's reasoning depth. When the team decomposed the pipeline and assigned each stage to the cheapest model that met its quality bar, the monthly cost dropped from $87,000 to $27,800. A 68% reduction. The client-facing output was indistinguishable in blind evaluations.

The lesson was not that the frontier model was overpriced. The lesson was that using one model for an entire pipeline is like hiring a surgeon to take your blood pressure, run your lab work, and then perform the operation. Most stages do not need the most expensive capability. **Multi-model pipelines** match the right model to the right task at the right cost, and the savings compound across every stage.

## Why Single-Model Pipelines Waste Money

The instinct to use one model for everything is understandable. It simplifies architecture. You have one API key, one set of prompt templates, one latency profile, one error-handling pattern. Every stage speaks the same language. When something breaks, you know where to look. Single-model pipelines are the path of least engineering resistance, and for prototypes and early products, they are often the right choice.

But simplicity has a price, and in production at scale, that price is paid in dollars. A single frontier model charges the same rate per token whether you are asking it to extract a name from a document — a task that a model fifty times cheaper handles perfectly — or to synthesize a nuanced argument from contradictory evidence. The model does not discount itself based on task difficulty. You pay peak pricing for every call, regardless of whether the task demands peak capability.

The waste is proportional to pipeline length. A two-stage pipeline overpays on at most one stage. A six-stage pipeline might overpay on four or five. Industry experience consistently shows that in a typical multi-stage AI pipeline, only one or two stages are genuinely model-sensitive — meaning quality drops noticeably when you substitute a cheaper model. The remaining stages are model-insensitive: they produce equivalent output across a wide range of model tiers. Paying frontier prices for model-insensitive stages is the single largest source of unnecessary cost in production AI systems.

The waste also compounds with volume. If each stage processes the same input once, a six-stage pipeline on a frontier model costs six times the per-call rate. But many pipelines have fan-out: a single document becomes ten extracted clauses, each classified independently, each summarized separately. What started as one input becomes sixty model calls. At frontier pricing, the fan-out transforms a modest cost into a large one. At right-sized pricing — cheap models for cheap stages, expensive models only where necessary — the fan-out is financially manageable.

## Model Sensitivity: Identifying Which Stages Need Power

The critical skill in multi-model pipeline design is distinguishing **model-sensitive stages** from **model-insensitive stages**. Get this distinction wrong and you either overpay for stages that do not need power or underinvest in stages where quality collapses.

A model-sensitive stage is one where output quality degrades sharply when you substitute a cheaper model. These stages typically involve complex reasoning, nuanced language generation, ambiguous inputs, or tasks that require broad world knowledge. Generating a client-facing analysis of legal risk is model-sensitive because the output must be precise, contextually appropriate, and defensibly accurate. A weaker model might miss subtle implications, produce awkward phrasing, or fail to connect related clauses across different sections of a contract.

A model-insensitive stage is one where output quality holds steady across model tiers. These stages typically involve structured extraction, classification into predefined categories, template-based generation, or pattern matching against known schemas. Extracting a date and dollar amount from a contract header is model-insensitive because the task is well-defined, the input format is consistent, and even small models handle it reliably. Classification into a fixed taxonomy is often model-insensitive because a fine-tuned small model that has seen thousands of examples of your specific categories will match or exceed a frontier model that must rely on general knowledge.

The detection method is empirical. For each stage in your pipeline, run your existing eval suite against three model tiers: your current model, one tier down, and two tiers down. If quality scores remain within your acceptable range at the cheaper tier, the stage is model-insensitive. If quality drops below your threshold at one tier down, the stage is model-sensitive. This test takes a day or two per stage and pays for itself within weeks of deployment.

A common mistake is assuming that stages early in the pipeline are automatically insensitive and stages late in the pipeline are automatically sensitive. This heuristic fails often enough to be dangerous. Some extraction tasks are surprisingly model-sensitive — extracting implied obligations from legal prose requires understanding context, not just pulling tagged fields. Some generation tasks are surprisingly model-insensitive — generating a standardized acknowledgment email from extracted data requires no more capability than a well-prompted small model. Test every stage. Do not assume.

## Designing the Multi-Model Architecture

Once you have mapped each stage's model sensitivity, the architecture follows naturally. Assign each stage to the cheapest model tier that meets its quality bar. The result is a pipeline where costs concentrate in the stages that genuinely need capability and drop to near zero everywhere else.

A concrete pipeline for the contract processing company might look like this. Stage one, clause extraction: a fine-tuned small model that has been trained on 5,000 annotated contracts, running at roughly 0.3 dollars per thousand calls. Stage two, clause classification: the same fine-tuned small model, adding negligible marginal cost since the model is already loaded and the classification task is a lightweight additional prompt. Stage three, contract summarization: a mid-tier model capable of synthesizing multiple clauses into a coherent summary, running at roughly 2.5 dollars per thousand calls. Stage four, client-facing explanation generation: a frontier model with strong reasoning and natural language generation, running at roughly 18 dollars per thousand calls.

The total cost per contract processed through all four stages is dominated by stage four. But the savings from downgrading stages one through three are substantial: those three stages represent roughly 60% of the total token volume but now consume less than 15% of the total cost. The frontier model handles only the stage where its capability is justified.

The architecture also opens up possibilities that a single-model pipeline cannot support. You can fine-tune the cheap models on your specific data without worrying about frontier-model fine-tuning costs and constraints. You can swap individual stages independently when a better model becomes available, without redesigning the entire pipeline. You can scale different stages on different infrastructure, placing the cheap models on lower-cost GPU instances and reserving premium compute for the frontier calls.

## Orchestration Complexity: The Hidden Cost

Multi-model pipelines are not free. They trade model cost for engineering complexity, and that complexity must be managed or it becomes its own form of waste.

The first complexity is endpoint management. Instead of one model API to configure, monitor, and maintain, you have three or four or six. Each endpoint has its own authentication, rate limits, error codes, retry behavior, and latency profile. A stage that calls a self-hosted fine-tuned model behaves differently from a stage that calls a cloud-hosted frontier API. Your orchestration layer must abstract these differences so that the pipeline logic does not depend on which specific provider or deployment each stage uses.

The second complexity is latency management. A single-model pipeline has predictable latency: one call, one response time. A multi-model pipeline chains calls sequentially, and the total latency is the sum of all stage latencies plus the overhead of passing data between stages. If stage one takes 400 milliseconds, stage two takes 200 milliseconds, stage three takes 800 milliseconds, and stage four takes 1,200 milliseconds, your total pipeline latency is 2.6 seconds plus overhead. For batch processing, this is irrelevant. For real-time user-facing applications, it may exceed acceptable response times. You can mitigate this by running independent stages in parallel where the pipeline graph allows it, or by aggressively caching intermediate results, but both add engineering effort.

The third complexity is error propagation. In a single-model pipeline, an error in one stage is an error in the pipeline. In a multi-model pipeline, an error in stage one propagates silently through stages two, three, and four, producing a plausible but incorrect final output. If the extraction stage misidentifies a party name, the classification stage classifies the wrong entity's clauses, the summarization stage summarizes obligations that do not apply, and the generation stage produces a confident, well-written, completely wrong analysis. The multi-model pipeline did not fail visibly. It failed invisibly, which is worse.

The remedy is validation gates between stages. After each stage, run a lightweight check on the output before passing it to the next stage. Does the extracted data contain the expected fields? Does the classification produce a valid category from the known taxonomy? Does the summary reference entities that were present in the extraction? These gates add latency and engineering cost, but they prevent the cascading error problem that makes multi-model pipelines dangerous without guardrails.

## The Cost Decomposition: Seeing Where Money Goes

One of the most valuable exercises in multi-model pipeline management is the **stage cost decomposition** — a breakdown showing what percentage of your total model spend goes to each pipeline stage, and what percentage of your total output quality is attributable to each stage.

Build this decomposition by measuring two things per stage. First, the monthly dollar cost: the number of calls times the per-call cost, including both input and output tokens. Second, the quality contribution: run your end-to-end eval suite with each stage individually degraded to measure how much the final output quality drops when that specific stage uses a weaker model.

The result is a table — described here in prose since this is a book, not a spreadsheet — where each row is a pipeline stage and the columns show cost percentage and quality contribution percentage. In a well-optimized pipeline, the stage that contributes the most to quality should also consume the most cost. If you find that stage three consumes 40% of your budget but only contributes 10% to final quality, you have found a downgrade candidate. If you find that stage four consumes 15% of your budget but contributes 60% to final quality, you know where to protect investment.

A document intelligence company that built this decomposition in mid-2025 discovered that their entity resolution stage — which reconciled names and references across document sections — consumed 28% of their total model budget but contributed only 6% to final quality because the downstream summarization model was already handling name reconciliation implicitly. They eliminated the stage entirely, reduced cost by 28%, and saw no change in end-to-end quality scores. The stage had been redundant, and without the decomposition, nobody would have known.

## When Stages Should Share a Model

Multi-model pipelines do not require that every stage uses a different model. Sometimes two or three stages have similar model sensitivity and similar capability requirements, and consolidating them onto a single model reduces orchestration complexity without increasing cost.

The decision to consolidate depends on three factors. First, are the stages at the same model tier? If extraction and classification are both model-insensitive, running them on the same cheap model simplifies the pipeline without cost penalty. Second, can the stages be combined into a single prompt? If extraction and classification can be accomplished in one call rather than two sequential calls, you cut latency and API overhead in half for those stages. Third, does combining the stages introduce interference? Sometimes a model performs better when extraction and classification are separate tasks with separate prompts. Combining them into one prompt may reduce quality on one or both tasks, especially if the combined prompt becomes long and complex.

Test consolidation empirically. Run your eval suite with stages separated and with stages consolidated. If quality holds and cost drops, consolidate. If quality drops, keep them separate. The sweet spot for most production pipelines is three to four distinct model tiers: a cheap tier for structured extraction and classification, a mid tier for synthesis and summarization, and an expensive tier for nuanced generation. Collapsing all the way to one model is overpaying. Expanding to six or seven distinct models is over-engineering.

## The Maintenance Tax: More Models, More Monitoring

Every model in a pipeline is a component that can degrade, drift, or break. A single-model pipeline requires one eval suite, one monitoring dashboard, one set of quality alerts, and one version management process. A four-model pipeline requires four of each. This is the **maintenance tax** of multi-model architectures, and teams that ignore it build pipelines that save money for six months and then silently degrade for the next twelve.

The maintenance tax manifests in several ways. Each model needs its own eval suite that tests stage-specific quality. A change to the extraction model requires re-evaluating extraction accuracy, but also re-evaluating end-to-end pipeline quality because downstream stages depend on extraction output. Each model needs its own monitoring for latency, error rates, and output distribution. A drift in the classification model's output distribution — suddenly assigning 30% of clauses to a category that historically received 15% — could indicate a data distribution shift or a model regression, and the detection system must catch it at the stage level, not just at the pipeline output level.

Version management is particularly complex. When a model provider releases a new version of the mid-tier model you use for summarization, you must test the new version in the context of your full pipeline, not just in isolation. The new version might produce slightly different output formatting that confuses the downstream generation stage. The new version might handle edge cases differently, causing the generation model to receive inputs it has never seen before. These inter-stage compatibility issues are invisible in single-model evaluations and only surface in end-to-end pipeline testing.

The practical discipline is to budget monitoring and maintenance effort proportionally to the number of distinct models in your pipeline. If you have four models, plan for four times the monitoring infrastructure, four times the eval suite maintenance, and roughly twice the incident investigation time, since debugging failures across model boundaries is harder than debugging within a single model. If the maintenance tax exceeds the savings from multi-model architecture, you have too many models. Simplify until the economics work.

## Building Multi-Model Pipelines Incrementally

The wrong way to build a multi-model pipeline is to design the full architecture on a whiteboard, assign models to every stage, build all the orchestration infrastructure, and deploy the whole thing at once. This approach is slow, risky, and usually produces an over-engineered system based on theoretical model sensitivity assessments rather than empirical ones.

The right way is incremental. Start with a single-model pipeline. Get it to production. Measure costs per stage. Then identify the most expensive model-insensitive stage — the one where you are paying the most for capability you do not need — and downgrade that stage first. Measure the cost savings and quality impact. If the tradeoff is favorable, move to the next most expensive model-insensitive stage. Repeat until every stage is on the cheapest model that meets its quality bar.

This incremental approach has three advantages. First, you make data-driven decisions about each stage rather than theoretical ones. You have production data on quality per stage, cost per stage, and inter-stage dependencies before you downgrade anything. Second, you limit the blast radius of each change. If downgrading stage two causes an unexpected quality drop, you catch it immediately and revert just that stage, not the entire pipeline. Third, you deliver cost savings continuously rather than in a single big bang. The first downgrade — typically the largest saving — goes live in weeks, not months.

A healthcare AI company followed this incremental approach across six months in 2025. They started with a five-stage pipeline running entirely on GPT-4o at $62,000 per month. In month one, they downgraded the data extraction stage to GPT-4o-mini, saving $14,000 with no quality change. In month two, they replaced the classification stage with a fine-tuned Llama model running on their own infrastructure, saving another $9,000. In months three and four, they tested the summarization stage on several mid-tier models and found that Claude Sonnet 4.5 met their quality bar at roughly half the cost of their frontier model for that stage. By month six, they were running a four-model pipeline at $23,000 per month — a 63% reduction — with end-to-end quality scores within 1.2% of the original single-model pipeline.

## The Cascade Pattern: Cheap First, Expensive on Failure

A powerful variant of the multi-model pipeline is the **cascade pattern**, where every request starts at the cheapest model and escalates to more expensive models only when the cheap model's output fails a confidence or quality check. Unlike a standard multi-model pipeline where each stage has a fixed model assignment, the cascade pattern dynamically routes each request based on difficulty.

The cascade works by adding a confidence check after each model tier. The cheap model processes the request and produces an output along with a confidence signal — either an explicit confidence score from the model or an implicit one from a lightweight classifier that evaluates the output. If the confidence exceeds a threshold, the output is returned and no further models are called. If the confidence falls below the threshold, the request is escalated to the next tier, which processes it independently. The process repeats until a model produces an output above the confidence threshold, or the request reaches the most expensive tier, which always produces the final answer.

Research from late 2024 and 2025 on LLM cascading shows that well-designed cascade systems can reduce average cost per request by 60% to 80% compared to always calling the most expensive model, while maintaining quality within 1% to 3% of the expensive-model baseline. The savings come from the distribution of request difficulty: in most production workloads, 50% to 70% of requests are simple enough that a cheap model handles them perfectly. Only the remaining 30% to 50% require more expensive models, and only a fraction of those require the frontier tier.

The risk of the cascade pattern is confidence calibration. If the confidence threshold is too low, the cheap model handles requests it should escalate, and quality suffers. If the threshold is too high, too many requests escalate to expensive models, and cost savings disappear. Calibrating the threshold requires a labeled evaluation set where you know which requests the cheap model can handle correctly and which it cannot. The threshold is set to maximize cost savings while keeping the error rate below your quality floor. This calibration must be repeated periodically as the distribution of incoming requests shifts over time.

## When Not to Use Multi-Model Pipelines

Multi-model pipelines are not always the right choice. Three scenarios favor a single-model approach, and recognizing them saves you from over-engineering.

The first scenario is low volume. If your pipeline processes fewer than a few thousand requests per day, the absolute dollar savings from multi-model architecture may not justify the engineering and maintenance overhead. A pipeline that costs $800 per month on a single frontier model is not worth redesigning to cost $400 per month if the redesign requires two engineers for a month and ongoing maintenance complexity. The breakeven calculation is straightforward: estimate the monthly savings, estimate the engineering cost of building and maintaining the multi-model pipeline, and compute the payback period. If the payback exceeds twelve months, a single-model pipeline is probably the right choice until your volume grows.

The second scenario is when the entire pipeline is model-sensitive. Some tasks are genuinely hard at every stage. A pipeline that takes a complex medical case description, extracts relevant symptoms and history, correlates them with clinical literature, and generates a differential diagnosis may require frontier-class capability at every stage. If your model sensitivity testing shows that quality drops unacceptably at every stage when you downgrade, a multi-model pipeline offers no cost advantage — you would just be paying the orchestration overhead for no savings.

The third scenario is extreme latency sensitivity. If your pipeline must respond within a tight time window — say, under 500 milliseconds for a real-time user interaction — the overhead of orchestrating multiple model calls may be unacceptable. A single fast model call may be the only architecture that meets your latency requirement, even if it costs more per request. In these cases, the cost optimization lever is not multi-model architecture but model selection: choosing the fastest model that meets your quality bar, which may be a distilled or fine-tuned model specifically optimized for your task.

Multi-model pipelines give you granular control over cost at each stage of processing. But the cost of each stage depends not just on which model you call but on which version of that model you call — and whether that version is pinned or floating. That decision, seemingly minor, has cost and quality implications that catch teams off guard, which is where we turn next.
