# 2.3 — Contribution Margin Per Query: The Unit Economics That Determine Survival

Every AI query your system processes either makes money or loses money. Most teams do not know which. They know their total AI spend. They know their total revenue. They might even know their overall gross margin. But they do not know the economics at the level that matters: the individual query. A support chatbot that resolves a billing question in one turn at a cost of $0.004 is printing money if it prevents a $12 support ticket. The same chatbot handling a complex technical troubleshooting session that consumes $0.38 in model calls, retrieval, and tool use while the customer still escalates to a human agent is incinerating value. Both queries count equally in your aggregate metrics. They could not be more different in your economics. Until you understand contribution margin at the query level, you are flying blind — and the more users you acquire, the faster the ground approaches.

## What Contribution Margin Per Query Actually Means

**Contribution margin per query** is the revenue attributable to a single query minus the variable costs of serving it. It is the AI-native equivalent of the unit economics that determine whether any business can survive at scale. In traditional SaaS, the variable cost of serving an additional user is near zero — the software is already running, and one more login costs almost nothing. In AI-native products, every query consumes real resources: inference compute, retrieval operations, embedding generation, tool calls, and sometimes human review. The variable cost is not near zero. It ranges from fractions of a cent for simple classification tasks to multiple dollars for complex multi-step agent workflows. That range is the territory where survival and death are decided.

The formula is conceptually simple. Take the revenue attributable to the query — directly or through allocation — and subtract the variable costs. If the result is positive, the query contributes to covering your fixed costs and generating profit. If the result is negative, the query destroys value. Every negative-margin query that hits your system is a tiny withdrawal from your bank account. At low volume, negative queries are survivable. At scale, they are fatal.

The subtlety is in both components. Revenue attribution is rarely straightforward, and variable cost has more components than teams initially realize. Getting both right — not perfectly, but directionally — is what separates teams that scale from teams that scale themselves into bankruptcy.

## The Revenue Side: Attributing Revenue to Individual Queries

Revenue attribution varies by business model, and getting it wrong leads to either complacency (thinking unprofitable queries are profitable) or panic (thinking profitable queries are unprofitable). There are three attribution models, and most products need a blend.

**Direct revenue attribution** applies when the query itself generates revenue. A pay-per-query API product charges $0.05 per call. A usage-based SaaS product charges $0.10 per AI analysis. An e-commerce search query that leads to a purchase within the same session generates measurable revenue. Direct attribution is clean and unambiguous. If you charge per query, the revenue per query is the price. If you charge per outcome, the revenue is the price times the outcome probability. A search query that leads to a $45 purchase with a 12% gross margin on the product contributes $5.40 in revenue if the purchase is fully attributed to the search, or some fraction if attribution is shared with other touchpoints.

**Retention-based revenue attribution** applies when the query does not directly generate revenue but contributes to retaining a paying customer. A customer support chatbot in a subscription product does not generate revenue on the query. But successful resolution prevents churn, and prevented churn preserves future subscription revenue. The attribution math requires two numbers: the probability that an unresolved query leads to churn, and the lifetime value of the retained customer. If 3% of unresolved support interactions eventually lead to churn, and the average customer lifetime value is $8,400, then each successfully resolved query has a churn-prevention value of roughly $252 multiplied by 3%, which is $7.56 in retained revenue. This is a probabilistic attribution, not a certainty, but it provides a defensible basis for contribution margin calculation.

**Allocated revenue attribution** applies when the query is part of a feature that users pay for as part of a bundle, but you cannot isolate how much of their payment is for that specific feature. A SaaS product that charges $200 per month and includes AI-powered search, AI-generated reports, and AI-assisted data entry cannot cleanly attribute revenue to any single feature. The pragmatic approach is to use feature usage weights: if the AI search feature accounts for 40% of total AI-related feature usage across your customer base, allocate 40% of subscription revenue to search. Then divide by the number of search queries to get a per-query revenue allocation. This is imprecise. It is also the best available method for bundled products, and imprecise economics are infinitely better than no economics.

## The Cost Side: What It Actually Costs to Serve a Query

The variable cost of a query has more components than teams initially account for. Missing even one component understates your cost and overstates your margin, sometimes dramatically.

**Inference cost** is the most visible component. It is the amount you pay your model provider — or the compute cost of self-hosted models — for the tokens consumed by the query. For a typical conversational query using a mid-tier model like GPT-5-mini or Claude Haiku 4.5, the inference cost might be $0.001 to $0.005. For a complex query using a frontier model like GPT-5.2 or Claude Opus 4.6 with a long system prompt and multi-turn context, the inference cost might be $0.02 to $0.15. For agent workflows with multiple chained calls, tool invocations, and reasoning steps, the cost can exceed $0.50 per query. The range spans three orders of magnitude, which is why aggregate averages are meaningless. You need cost at the query level, not the system level.

**Retrieval cost** applies to RAG systems and any architecture that queries a vector database, search index, or knowledge base before generating a response. Each retrieval call has a cost: embedding generation for the query, vector search compute, document fetching, and re-ranking. For hosted vector databases, this might be $0.0001 to $0.001 per retrieval operation. For self-hosted infrastructure, the cost is the proportional compute, storage, and bandwidth consumed by the operation. A RAG pipeline that performs three retrieval passes — initial retrieval, re-ranking, and context expansion — might cost $0.002 to $0.008 in retrieval alone.

**Tool call cost** applies to agent architectures where the model invokes external tools — API calls, database queries, web searches, code execution. Each tool call has its own cost. An external API call might cost $0.01. A database query against a large analytics store might cost $0.001 in compute. A web search might cost $0.005 to $0.02 depending on the provider. Agent workflows that invoke five to ten tools per query can accumulate $0.05 to $0.20 in tool costs alone, before accounting for the model calls that orchestrate them.

**Post-processing cost** includes any computation performed after the model generates its response: safety filtering, output formatting, PII detection and redaction, response validation, and logging. Individually these costs are small, typically fractions of a cent. Collectively they add up, especially for safety-critical applications that run multiple validation passes.

**Proportional infrastructure cost** is the per-query share of your fixed infrastructure: load balancers, API gateways, monitoring systems, caching layers, and the engineering team that keeps it all running. Strictly speaking, these are fixed costs, not variable costs. But for contribution margin analysis, allocating a proportional share to each query gives you a more realistic picture of your true per-query economics. The allocation method is straightforward: divide your monthly infrastructure cost by your monthly query volume. If your infrastructure costs $12,000 per month and you serve 2.4 million queries, the proportional infrastructure cost is $0.005 per query.

## The Death Spiral of Negative Contribution Margin

When contribution margin per query is negative, a specific and dangerous dynamic takes hold. The more users you acquire, the faster you lose money. This is the **negative margin death spiral**, and it has killed more AI startups than any technical failure.

The spiral works like this. You launch an AI product. Users love it. Growth is strong. Each new user brings additional queries, and each query costs more to serve than it generates in revenue. At 10,000 queries per day, the loss is $300 per day — manageable, a rounding error in your burn rate. At 100,000 queries per day, the loss is $3,000 per day. At 1 million queries per day, the loss is $30,000 per day, which is $900,000 per month. Your investors start asking questions. You cannot raise prices without losing users. You cannot reduce quality without losing users. You cannot stop growing without killing your valuation narrative. You are trapped.

A developer tools startup experienced this in mid-2025. They launched an AI code assistant that provided real-time code suggestions, explanations, and debugging help. The product was free during beta and priced at $15 per month per developer for the paid tier. Users averaged 140 queries per day during active coding sessions. The inference cost per query averaged $0.009, using a mix of a frontier model for complex queries and a smaller model for simple completions. With retrieval, tool calls, and infrastructure, the total variable cost per query was $0.014. At 140 queries per day, the daily variable cost per user was $1.96. At $15 per month, the daily revenue per user was $0.50. Each active user cost $1.46 per day more than they paid. With 8,200 paying users, the monthly loss on variable costs alone was $359,000.

The team had three options: raise prices, reduce costs, or change the architecture. They chose a combination. They implemented aggressive model routing that sent 70% of queries — simple completions and formatting suggestions — to a fine-tuned small model at one-tenth the cost. They added response caching for common queries, which eliminated 22% of model calls entirely. They raised the price to $25 per month and introduced a usage cap of 500 queries per day, with overage pricing above that. The combined effect brought the average variable cost per query down to $0.006 and the daily revenue per user up to $0.83. Contribution margin per query turned positive. The death spiral reversed. But it took four months of emergency engineering and a painful pricing change that caused 15% of users to churn. If the team had calculated contribution margin per query before launch, they would have designed the architecture and pricing differently from the start.

## Profitable Queries Versus Subsidized Queries

Not all queries are created equal, even within the same product. Some queries generate strong positive contribution margin. Others are deeply negative. The aggregate contribution margin — positive or negative — is a blend of these two populations. Understanding the mix, and actively managing it, is the operational discipline that sustains AI businesses.

**Profitable queries** are those where the revenue attributable to the query exceeds the variable cost of serving it. In an e-commerce search product, a query that leads to a purchase is profitable. In a support chatbot, a query that resolves an issue and prevents an escalation is profitable. In a pay-per-query API, any query where the price exceeds the variable cost is profitable. Profitable queries are the fuel of the business. You want more of them.

**Subsidized queries** are those where the variable cost exceeds the attributable revenue. In the e-commerce search product, a query where the user browses but does not buy generates no direct revenue but still costs inference and retrieval compute. In the support chatbot, a query where the chatbot fails to resolve the issue and the user escalates anyway generates negative value — you paid for the AI attempt and then paid again for the human agent. In the pay-per-query API, a query that triggers an unexpectedly expensive agent workflow can cost more to serve than the customer paid.

Every AI product has both. The question is not whether you have subsidized queries — you do — but whether the profitable queries generate enough surplus to cover the subsidized ones. The **subsidy ratio** is the percentage of queries that are subsidized. An enterprise SaaS product might have a subsidy ratio of 30% — most queries are profitable, and the profitable ones generate enough margin to cover the 30% that are not. A consumer freemium product might have a subsidy ratio of 80% — most queries come from free users who generate no direct revenue, and the paying users must generate enough margin to cover the entire base.

The subsidy ratio becomes dangerous when it rises over time. This happens when unprofitable use patterns grow faster than profitable ones. A common pattern in AI products: power users who generate the most queries are often the least profitable on a per-query basis, because they consume the most resources and they have already converted — their incremental revenue is low. New and casual users who generate fewer queries are often more profitable per query because their queries are simpler and their conversion potential is higher. As the product matures and the user base shifts toward power users, the subsidy ratio can creep upward until the aggregate margin turns negative.

## Shifting the Mix: From Subsidized to Profitable

Once you understand which queries are profitable and which are subsidized, you can take deliberate action to shift the mix. This is not about refusing to serve certain queries. It is about designing the system so that subsidized queries become less expensive, profitable queries become more common, or the pricing model aligns revenue more closely with cost.

The first lever is **cost-based routing**. Not all queries need the same quality level, and lower quality often means lower cost. A query that is likely to be profitable — a search query with high purchase intent signals, a support query with a known resolution pattern — can receive the full-quality treatment: frontier model, deep retrieval, multiple validation passes. A query that is likely to be subsidized — a browsing query with no purchase intent, a casual question that does not need a comprehensive answer — can be routed to a cheaper model with lighter retrieval. The user still gets a good answer. The system spends less delivering it. The contribution margin on the subsidized query moves closer to zero, even if it does not turn positive.

The second lever is **usage pricing alignment**. If your pricing model charges a flat subscription but your cost model is usage-based, you have a structural mismatch. Heavy users cost more to serve and pay the same as light users. Usage-based pricing — per query, per token, per outcome — aligns revenue with cost and eliminates the subsidy that heavy users receive from light users. The AI SaaS industry moved heavily toward usage-based and hybrid pricing models in 2025 and 2026, with industry analysis showing that over 90% of AI software companies now use mixed pricing models that combine subscription fees with usage components. The shift is driven directly by the contribution margin math: flat pricing makes it impossible to maintain positive contribution margins when usage varies by 100x across your user base.

The third lever is **query deflection for negative-margin categories**. If certain query types consistently generate negative margin — queries that always require expensive multi-step processing and rarely lead to revenue — you can design the product to handle them differently. Provide precomputed answers, redirect to documentation, or handle them asynchronously at lower cost through batch processing. Deflection is not the same as refusing to serve. It is serving more efficiently, in a way that recognizes the economic reality of each query type.

## How to Calculate Contribution Margin Per Query in Practice

The theoretical framework is clear. The practical implementation requires data infrastructure that most teams do not have on day one. Here is how to build it incrementally.

Start with cost instrumentation. Your inference layer should log the model used, tokens consumed, and cost per call for every query. If you use a managed API, the provider gives you this data. If you self-host, you need to calculate the proportional compute cost per inference call. Your retrieval layer should log the number of retrieval operations per query and the estimated cost of each. Your tool layer should log every external call and its cost. Aggregate these per query, and you have the variable cost side. This instrumentation can be built in a few days for most architectures.

Next, build revenue attribution at the query level. For direct-revenue products, this means linking each query to the downstream revenue event — purchase, subscription, API charge. For retention-based products, this means associating each support query with the customer's retention outcome over the following billing period. For bundled products, this means applying the feature usage allocation described earlier. Revenue attribution is harder than cost instrumentation and takes longer to build. Start with direct attribution for the queries where it is straightforward, and add probabilistic attribution for indirect revenue over time.

Then calculate the contribution margin per query: attributed revenue minus variable cost. Do this daily, or at least weekly. Aggregate by query type, by user segment, by model used, by time of day. The aggregations reveal patterns that the overall average hides.

A fintech company that built this instrumentation in the second half of 2025 discovered that their overall contribution margin per query was positive at $0.012. That number felt comfortable. But when they disaggregated by query type, they found that their most common query type — balance inquiries — had a contribution margin of $0.002, barely positive. Their second most common type — transaction categorization — had a contribution margin of negative $0.03, deeply unprofitable. Their third most common type — spending insights — had a contribution margin of $0.18, highly profitable. The aggregate number was a blend that obscured a loss-making query type consuming 35% of their traffic. The fix was straightforward: they routed transaction categorization queries to a smaller, cheaper model that was accurate enough for the task but cost one-eighth as much. The category contribution margin went from negative $0.03 to positive $0.005. The overall average rose from $0.012 to $0.021. The annual impact was $340,000 in saved losses. None of this would have been visible without per-query contribution margin tracking.

## The Contribution Margin Floor

Every AI product needs a **contribution margin floor** — a minimum acceptable contribution margin per query below which the query should be handled differently. The floor is not zero. A query with a contribution margin of $0.001 is technically profitable but contributes almost nothing to covering your fixed costs, which include engineering salaries, office space, marketing, and everything else that keeps the business running. A meaningful contribution margin floor accounts for these fixed costs.

The calculation starts with your total fixed costs per month. Divide by your monthly query volume. This gives you the fixed cost allocation per query. Your contribution margin floor should be at least this amount, because a query that contributes less than its share of fixed costs is not covering its full economic burden. If your fixed costs are $180,000 per month and you serve 3 million queries, the fixed cost allocation is $0.06 per query. Your contribution margin floor is $0.06. Any query contributing less than $0.06 in margin is not paying its way. Any query contributing a negative margin is actively destroying value.

This does not mean you refuse to serve queries below the floor. It means you flag them, monitor them, and take deliberate action to improve their economics. For some queries, the action is cost reduction through cheaper models or caching. For others, the action is revenue improvement through better monetization or upselling. For a persistent population of deeply negative queries, the action might be product redesign — changing the feature so that the query is handled more efficiently, or removing the feature if it consistently destroys value.

The contribution margin floor also serves as a design constraint for new features. Before building a new AI feature, estimate the expected contribution margin per query. If the estimate is below the floor, the feature needs a different architecture — a cheaper model, a simpler pipeline, a different pricing model — before it should be built. The teams that apply this discipline avoid launching features that degrade their unit economics. The teams that do not apply it often discover the problem only after the feature has been live for six months and has accumulated hundreds of thousands of dollars in losses.

## Contribution Margin as a Growth Signal

Contribution margin per query is not just a survival metric. It is a growth signal. When contribution margin is strongly positive, growth is fuel. Every additional user, every additional query, generates surplus that covers fixed costs and eventually produces profit. In this state, the rational strategy is to grow aggressively — acquire users, expand features, increase usage. The unit economics reward scale.

When contribution margin is near zero or negative, growth is a trap. Every additional user adds cost faster than revenue. The rational strategy is to fix the unit economics before growing. This is counterintuitive for teams accustomed to the traditional SaaS playbook, where marginal user cost is near zero and growth is almost always the right strategy. In AI products, growth without positive contribution margin is the fastest path to insolvency.

The most dangerous moment is when contribution margin is slightly positive at current scale but the cost structure does not scale linearly. Some AI costs are concave — they increase slower than query volume because of caching effects, batch processing efficiencies, and infrastructure amortization. These costs make contribution margin look better at scale. Other AI costs are convex — they increase faster than query volume because of context window growth, increased complexity per query as power users arrive, and the need for more expensive models as the easy queries are saturated and the remaining growth comes from harder use cases. Convex costs make contribution margin worse at scale. The teams that survive know which cost category dominates their system and plan accordingly.

A legal technology company learned this distinction in early 2026. At 50,000 queries per month, their contribution margin was positive at $0.14 per query. They projected linear scaling and expected the margin to hold as they grew. At 200,000 queries per month, the contribution margin had dropped to $0.04. The reason: their growing user base included more complex legal queries that required longer context windows, more retrieval passes, and more frequent use of frontier models. The average cost per query had risen from $0.08 to $0.16 while the average revenue per query remained flat at $0.20. The easy queries that early adopters submitted had been replaced by hard queries that power users submitted. The unit economics that looked healthy at 50,000 queries were marginally survivable at 200,000. The team had to restructure their pricing to include complexity-based tiers, charging more for queries that consumed more resources. The restructuring was painful but necessary. Without it, their projected break-even volume — originally 300,000 queries per month — would have receded to 800,000 as the query mix continued shifting toward expensive cases.

## From Unit Economics to Quality Investment

Contribution margin per query connects directly to every quality investment decision. When you consider improving a quality dimension, the question is not just "does this improve the user experience?" It is "does this change the contribution margin per query?"

A quality improvement that increases revenue per query — by lifting conversion, reducing churn, or enabling higher pricing — improves contribution margin. A quality improvement that increases cost per query — by requiring a more expensive model, more retrieval, or more processing — reduces contribution margin. The net effect determines whether the quality investment is worth making. An improvement that increases revenue by $0.03 per query and increases cost by $0.01 per query improves contribution margin by $0.02 per query. At a million queries per month, that is $20,000 per month in additional contribution. An improvement that increases revenue by $0.005 per query and increases cost by $0.02 per query reduces contribution margin by $0.015 per query. At the same volume, that is $15,000 per month in destroyed value.

The contribution margin lens forces you to evaluate quality investments the way a business evaluates capital expenditures: by their return, not by their impressiveness. The most impressive quality improvement is worthless if it destroys your unit economics. The most boring quality improvement — a slightly better cache hit rate, a marginally cheaper model that performs almost as well — might be the most valuable one you make all year because it improves contribution margin on every query your system serves.

This is the metric that determines whether your AI business can survive. Not your model's accuracy. Not your evaluation scores. Not your user growth rate. Your contribution margin per query, tracked at the granularity of individual query types and user segments, tells you whether each interaction makes money or loses it. Fix the queries that lose money. Protect the queries that make money. And never launch a feature without knowing which category its queries will fall into.

The contribution margin framework tells you the economics of each query. But some quality improvements that look economically justified are invisible to users — they show up in your metrics but not in your product experience. Understanding which improvements users actually notice, and which ones they never will, is where we turn next.
