# 9.5 — Infrastructure Amortization: When Fixed Costs Start Winning

Variable cost means you pay per request. Fixed cost means you pay per month regardless of volume. At low volume, variable costs win because you only pay for what you use — a system handling 1,000 requests per day on an API spends almost nothing during idle hours. At high volume, fixed costs win because the per-request share of your monthly infrastructure bill approaches a number so small it becomes irrelevant. The inflection point where smart infrastructure teams switch from one model to the other is called the **amortization threshold**, and it applies to every layer of your AI system — not just model inference, but caching, safety pipelines, monitoring, and even human review operations.

This is not a theoretical exercise. It is the mechanism behind the most common cost transformation in scaling AI systems: the moment when a team stops renting capacity by the request and starts owning capacity by the month. Get the timing right and your cost-per-request drops by 40 to 80 percent. Get it wrong in either direction and you either bleed money on API bills you could have avoided or pay for infrastructure that sits mostly idle.

## The Amortization Calculation

The core math is a single division: your monthly fixed cost divided by your monthly request volume. The result is your amortized per-request cost for the fixed infrastructure. Compare that number against your variable per-request cost from an API or pay-per-use service. When the amortized cost drops below the variable cost, the fixed infrastructure wins.

Walk through a concrete example. You are running a document classification pipeline that processes incoming customer emails and routes them to the right department. The current implementation uses an API-hosted model at $0.80 per million input tokens. Average email length is 400 tokens, average classification output is 50 tokens at $2.40 per million output tokens. Your per-request API cost is $0.00032 for input plus $0.00012 for output, totaling $0.00044 per request. At 200,000 requests per month, your monthly API cost is $88. At 2 million requests per month, it is $880. At 20 million requests per month, it is $8,800.

The self-hosted alternative is a dedicated instance running a fine-tuned Llama 4 Scout on a single A100 GPU. Monthly lease cost: $1,800. Associated infrastructure — networking, load balancer, monitoring, storage: $400 per month. Total fixed cost: $2,200 per month. This instance can handle the email classification workload at roughly 1,200 requests per minute with optimized batching, giving a theoretical monthly capacity of approximately 52 million requests.

At 200,000 requests per month, the amortized cost is $2,200 divided by 200,000, which equals $0.011 per request. The API at $0.00044 per request is 25 times cheaper. The fixed infrastructure is wildly uneconomical. At 2 million requests per month, the amortized cost is $0.0011 per request — still 2.5 times more expensive than the API. At 5 million requests per month, the amortized cost drops to $0.00044 per request — exactly equal to the API cost. This is the amortization threshold: 5 million requests per month. Above this point, the fixed infrastructure is cheaper per request, and the gap widens with every additional request.

At 20 million requests per month, the amortized cost is $0.00011 per request — four times cheaper than the API. The monthly saving is $8,800 minus $2,200, or $6,600 per month. At 50 million requests per month, the amortized cost drops to $0.000044 per request, and the monthly saving is $21,800 minus $2,200, or $19,600. The fixed infrastructure that looked absurd at low volume has become a competitive weapon at high volume.

## The Utilization Problem

The amortization calculation assumes your infrastructure is actually serving requests. In practice, utilization is the variable that makes or breaks the economics. A GPU that sits idle 60 percent of the time costs the same as one running at full capacity, but the amortized per-request cost triples.

**Utilization** is the percentage of your infrastructure's theoretical capacity that is actually consumed by production traffic. If your GPU can serve 52 million requests per month at peak throughput but your actual volume is 15 million, your utilization is roughly 29 percent. Your effective per-request cost is not $2,200 divided by 52 million — it is $2,200 divided by 15 million, or $0.000147 per request.

Traffic patterns are the primary driver of underutilization. Most AI systems experience significant variation across the day, the week, and the month. A customer support chatbot might process 80 percent of its daily volume during business hours, with near-zero traffic between midnight and 6 AM. An email classification system might spike on Monday mornings and taper off by Friday afternoon. A content moderation pipeline might surge during product launches and settle during quiet periods.

This variation means that sizing your infrastructure for peak load guarantees low average utilization. A system that needs capacity for 2,000 requests per minute during peak but averages 600 requests per minute across the day is running at 30 percent utilization if provisioned for peak. The amortized cost is three times higher than the peak-based calculation would suggest.

The solution is not to under-provision. Running out of capacity during peak traffic is worse than paying for idle capacity — dropped requests, increased latency, and degraded user experience cost more than a few thousand dollars in excess infrastructure. The solution is to manage the gap between peak and average utilization through three strategies.

## Strategy One: Right-Sizing With Reserved Plus On-Demand

The most effective utilization strategy combines reserved capacity for your baseline load with on-demand or spot capacity for peaks. Your baseline is the minimum volume your system reliably processes even during the lowest-traffic period. This is the volume you should cover with reserved infrastructure — long-term leases or reserved instances that offer 40 to 70 percent discounts over on-demand pricing. Your peak-minus-baseline gap is covered with on-demand instances that spin up when traffic exceeds the baseline and spin down when it falls.

A concrete example: your AI system processes between 400,000 and 1.2 million requests per day, with the minimum occurring on weekends and the maximum during weekday business hours. Your baseline is 400,000 requests per day — roughly 280 requests per minute. A single reserved GPU instance handles this easily, costing $1,800 per month on a one-year commitment. Your peak demand requires capacity for 840 requests per minute — roughly three times your baseline. You add two on-demand GPU instances during weekday business hours — roughly 12 hours per day, five days per week. At $3.50 per hour per instance on-demand, that is two instances times 12 hours times 22 weekdays times $3.50, totaling roughly $1,848 per month.

Your total monthly infrastructure cost is $1,800 for the reserved instance plus $1,848 for the on-demand instances, totaling $3,648. Your average monthly volume is roughly 24 million requests. Your amortized per-request cost is $0.000152. Compare that to a naive approach of reserving three instances for peak capacity at $5,400 per month — the right-sized approach saves $1,752 per month, or 32 percent, with no capacity compromise during peak.

The reserved-plus-on-demand strategy requires auto-scaling infrastructure that can spin instances up and down in response to real-time traffic. In 2026, serving frameworks like vLLM and TensorRT-LLM support horizontal scaling out of the box, and orchestration platforms like Kubernetes make auto-scaling GPU workloads significantly smoother than it was even two years ago. The engineering investment in auto-scaling is a one-time cost that pays back every month through improved utilization.

## Strategy Two: Multi-Tenant Infrastructure

If your organization runs multiple AI workloads, sharing GPU infrastructure across them dramatically improves utilization. The key insight is that different workloads often have different traffic patterns. A customer-facing chatbot peaks during business hours. A batch content moderation pipeline runs overnight. An internal summarization tool peaks mid-morning. If each workload has its own dedicated infrastructure, each runs at low utilization during its off-peak hours. If all three share a GPU cluster, the combined traffic pattern is more uniform, and average utilization rises.

A three-workload example: Workload A processes 500,000 requests per day, concentrated in a 10-hour business window. Workload B processes 300,000 requests per day, running as batch jobs overnight. Workload C processes 200,000 requests per day, spread relatively evenly. With dedicated infrastructure, each workload needs capacity for its peak, resulting in three separate GPU instances running at individual utilization rates of 30 to 50 percent. With shared infrastructure, the combined load is more evenly distributed across the day, and two GPU instances handle the aggregate peak with 65 to 75 percent average utilization. The infrastructure cost drops by one-third while serving the same total volume.

Multi-tenancy introduces complexity. Different workloads may use different models, requiring the serving infrastructure to handle model switching or run multiple models simultaneously. Workloads may have different latency requirements — the batch pipeline can tolerate seconds of latency, while the customer-facing chatbot needs sub-second responses. Priority management, request queuing, and SLA enforcement add operational overhead. But for organizations with three or more distinct AI workloads, the utilization improvement from shared infrastructure almost always justifies the complexity.

## Strategy Three: Workload Shaping

The third utilization strategy is shaping your workload to fit your infrastructure rather than shaping your infrastructure to fit your workload. Not all AI processing is time-sensitive. Batch jobs — daily report generation, overnight data enrichment, weekly model evaluation — can be scheduled during off-peak hours to fill the capacity gap left by real-time traffic.

A content platform that runs its AI moderation system at full capacity during the day and near-zero at night can schedule its nightly data enrichment pipeline — extracting metadata, generating embeddings, creating search indexes — on the same GPU infrastructure during the overnight hours. The moderation workload utilizes the GPU from 8 AM to 10 PM. The enrichment workload utilizes it from 11 PM to 6 AM. Combined utilization rises from 58 percent to 87 percent. The amortized per-request cost drops by a third with no additional infrastructure spend.

Workload shaping requires a job scheduling system that understands GPU availability and can preempt batch jobs when real-time traffic demands capacity. It also requires that your batch workloads are genuinely flexible on timing — a nightly enrichment pipeline that must complete by 7 AM is flexible, but a "batch" pipeline that stakeholders actually expect results from within an hour is not. The distinction matters: misclassifying a latency-sensitive workload as deferrable creates user-visible failures that are more expensive than the utilization improvement is worth.

## The Amortization Threshold Across System Components

GPU inference is the most discussed amortization opportunity, but the same math applies to every component with a fixed-cost alternative.

**Caching infrastructure.** A managed Redis instance on a cloud provider might cost $200 per month for a basic tier that handles your caching needs. A self-hosted Redis cluster on dedicated instances might cost $800 per month but handles ten times the throughput. At low volume, the managed instance is cheaper. At high volume, the self-hosted cluster's per-request amortized cost is a fraction of the managed service's pricing. The crossover for caching infrastructure typically happens earlier than for GPU inference because caching infrastructure is cheaper and simpler to operate.

**Safety and filtering pipelines.** Running content moderation through an external API at $0.002 per request costs $4,000 per month at 2 million requests. Self-hosting a content moderation model on a dedicated GPU costs $2,000 per month in infrastructure and handles 20 million requests per month. The crossover is at approximately 1 million requests per month — much earlier than the inference crossover, because safety models are typically smaller and cheaper to host.

**Embedding generation.** Computing embeddings through an API at $0.10 per million tokens is simple and cheap at low volume. Self-hosting an embedding model on a single A10G GPU costs $600 per month and processes tens of millions of embedding requests. For RAG systems processing large document volumes, the embedding amortization threshold can be as low as 200,000 to 500,000 embedding operations per month.

**Monitoring and observability.** Managed observability platforms charge per GB of log data ingested or per span of trace data stored. At low volume, these are cost-effective. At high volume — tens of millions of spans per month — self-hosted observability using open-source tools like Grafana, Prometheus, and Jaeger on dedicated infrastructure becomes significantly cheaper. The crossover varies widely but typically occurs around $5,000 to $8,000 per month in managed observability spend.

The pattern is consistent: every pay-per-use service has an amortization threshold beyond which fixed infrastructure is cheaper. The thresholds differ by component, and the operational complexity differs by component, but the economic principle is identical.

## The Risk of Premature Amortization

The amortization story is seductive, and that seductiveness creates risk. Teams that fall in love with the economics of amortization sometimes commit to fixed infrastructure before their volume justifies it. This is called **premature amortization** — locking in fixed costs when your traffic has not yet reached the threshold to make them economical.

The damage from premature amortization is twofold. First, you pay more per request than you would have with variable pricing. A team that leases a $3,500 per month GPU cluster expecting 10 million requests per month but actually processes 2 million is paying $0.00175 per request — potentially more than the API alternative. Second, you consume engineering time managing infrastructure that is not yet paying for itself. That engineering time has an opportunity cost: product features, quality improvements, and customer acquisition that generate revenue.

The detection signal for premature amortization is utilization. If your GPU utilization is consistently below 30 percent and your monthly volume is not growing at a rate that will cross the threshold within six months, you have committed too early. The mitigation is to set a utilization floor — typically 40 to 50 percent average utilization — and transition infrastructure back to variable-cost alternatives if utilization falls below the floor for two consecutive months.

The opposite risk — delayed amortization, where a team keeps paying variable costs long after the fixed-cost threshold has been crossed — is discussed in detail in the previous subchapter on build-versus-buy curves. Both errors are costly. The discipline is quarterly review: recalculate the amortization threshold for each component using current pricing, current volume, and six-month volume projections. Adjust your infrastructure mix accordingly.

## The Amortization Cascade

As a system scales, amortization thresholds are not crossed one at a time. They tend to cascade. The first component to cross the threshold is typically the one with the lowest fixed cost and the highest per-request variable cost — often embedding generation or content moderation. As the team builds operational confidence from running that first self-hosted component, they tackle the next: perhaps the caching layer. Then the main inference endpoint. Then the monitoring stack.

Each component that moves from variable to fixed cost reduces the total per-request cost, which often enables the product team to lower prices, which drives higher volume, which pushes more components past their amortization thresholds. This is the **amortization cascade** — a positive feedback loop where fixed-cost transitions at one layer create volume growth that enables fixed-cost transitions at other layers.

A document processing company illustrates the cascade. At 1 million documents per month, everything ran on APIs and managed services: $14,000 per month total. At 3 million documents, they self-hosted the embedding model, saving $2,800 per month. The savings funded a price reduction that attracted more customers, pushing volume to 5 million. At 5 million, they self-hosted the classification model, saving another $4,200 per month. At 8 million, they self-hosted the main inference endpoint. By 12 million documents per month, their total infrastructure cost was $22,000 per month — processing twelve times the volume for less than double the original cost. Their per-document cost had dropped from $0.014 to $0.0018, a reduction of 87 percent.

The cascade is not automatic. Each transition requires engineering investment, operational maturity, and the discipline to time the transition correctly. But recognizing the cascade pattern helps teams plan their infrastructure roadmap: which component will cross the threshold first, which will follow, and what volume growth must occur to trigger each subsequent transition.

## When Variable Costs Still Win

Fixed-cost amortization is not universally superior. Several scenarios favor keeping variable costs even at volumes that appear to justify the switch.

**Rapid model evolution.** If you expect to change your model every two to three months as better options become available, the switching cost of self-hosted infrastructure — re-deploying, re-benchmarking, re-optimizing — erodes the amortization savings. APIs let you switch models with a configuration change. Self-hosted infrastructure makes model switching an engineering project.

**Unpredictable growth trajectory.** A startup that might process 2 million requests this month, 500,000 next month if a key customer churns, and 8 million the month after if a launch goes well cannot reliably size fixed infrastructure. The variance in their volume forecast is wider than the amortization savings at any given point. Variable costs flex with reality. Fixed costs do not.

**Opportunity cost of engineering focus.** A ten-person startup where every engineer is needed for product development cannot afford to dedicate even half an engineer to infrastructure operations. The $5,000 per month they would save on inference costs is less than the value of the features that engineer would have built. The opportunity cost calculation changes as the company grows and can hire dedicated infrastructure engineers, but at early stage, the engineering constraint dominates the cost math.

**Multi-model requirements.** If your system routes to five different models depending on request type and three of those models change quarterly, self-hosting all five models is operationally complex and capital-intensive. The API approach lets you access the full model ecosystem without owning the infrastructure for each one. The hybrid approach — self-hosting your one or two highest-volume models while using APIs for the rest — is often the best balance.

The decision is never permanent. The right infrastructure mix at 1 million requests per month is different from the right mix at 10 million, which is different from the right mix at 100 million. The discipline is regular reassessment, honest utilization tracking, and the willingness to reverse a transition if the economics change.

The amortization threshold tells you when to switch from buying capacity to owning it. But there is a harder decision ahead — one that has nothing to do with infrastructure economics and everything to do with product judgment. Sometimes the highest-leverage cost move is not building cheaper infrastructure or negotiating better API rates. It is deliberately reducing the quality of what you serve. The next subchapter covers the decision that makes engineers uncomfortable and finance teams relieved: when cutting quality is the right call.
