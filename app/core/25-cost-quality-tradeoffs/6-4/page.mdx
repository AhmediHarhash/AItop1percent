# 6.4 — Human Review Budgets: Allocating Expert Time Where It Moves the Needle

The review queue holds 12,000 flagged responses. Two domain experts are available for a total of forty hours this week. At eight minutes per review, they can get through roughly 300. The other 11,700 will wait, age out, or never be reviewed at all. This is not a hypothetical. This is Monday morning at any organization running AI systems at scale with human review in the loop. The math is always brutal: you have more responses than humans can read, and the gap widens with every week of growing traffic.

The instinct is to hire more reviewers. But human review is the most expensive evaluation resource you have, and the solution is not to throw more money at it. The solution is to spend the money you have on the reviews that actually change outcomes. Most teams spread their review budget evenly across the queue, or worse, let reviewers self-select what to look at. Both approaches waste the majority of expert time on responses that are obviously fine or obviously broken, neither of which benefits from expert judgment. The responses that need an expert are the ambiguous ones — the cases where automated checks cannot tell whether the output is good enough, where the stakes of getting it wrong are high, and where a single well-placed review can improve the system for hundreds or thousands of future responses.

This subchapter teaches you how to budget human review as a strategic investment rather than a sampling exercise. Where to allocate expert hours, how to calculate the cost of each review, and how to build feedback loops that turn one human review into systemic improvement.

## The Real Cost of a Human Review

Human review costs are easy to underestimate because most teams think in terms of hourly rates. A domain expert costs $75 to $150 per hour depending on the field. An internal subject-matter expert loaded with benefits and overhead costs the company $90 to $200 per hour. A contract reviewer in a specialized domain like clinical medicine or securities law can run $200 to $400 per hour. These are the direct costs. They do not include the coordination overhead of managing a review queue, the tooling costs of building a review interface, or the opportunity cost of pulling experts away from other work.

The per-review cost depends on complexity. A simple quality check — did the AI summarize this correctly, is the tone appropriate, does the answer match the source document — takes three to five minutes per response. At $100 per hour, that is $5 to $8 per review. A complex quality check — is this medical advice safe, does this legal analysis identify the right precedents, is this financial recommendation appropriate for the customer's risk profile — takes eight to fifteen minutes. At $150 per hour, that is $20 to $38 per review. An edge case that requires the reviewer to research the question before judging the response can take thirty minutes or more. At $150 per hour, one edge case review costs $75 or more.

Now apply these costs to volume. A system producing 50,000 responses per day that routes 5 percent to human review generates 2,500 reviews per day. At an average of $12 per review, that is $30,000 per day — $900,000 per month — just on human evaluation. At 2 percent routing, the cost drops to $12,000 per day or $360,000 per month. At 0.5 percent, it is $3,000 per day or $90,000 per month. The difference between 5 percent routing and 0.5 percent routing is $810,000 per month. That number explains why human review budget allocation is not a quality question. It is a financial architecture decision.

## The Priority Stack: Where Expert Time Creates the Most Value

Not all reviews carry equal weight. A review that confirms an obviously correct response teaches you nothing and improves nothing. A review that catches a dangerous error in a high-stakes domain prevents harm and generates training signal. A review that resolves a disagreement between two automated judges produces the judgment data needed to calibrate those judges going forward. The value of a human review is determined by what changes as a result of the review, and that value varies by orders of magnitude.

The **Priority Stack** is a framework for ranking review categories by the expected value of human judgment. It has four tiers, and your review budget should flow from the top down, moving to lower tiers only when higher tiers are fully covered.

**Tier one: high-stakes domains where errors cause harm.** Medical advice, legal guidance, financial recommendations, safety-critical instructions, content involving minors, and any response where a wrong answer creates liability, injury, or regulatory exposure. These reviews are non-negotiable. Even if automated judges rate the response as high quality, human eyes should verify a meaningful sample. The cost of a missed error in these categories is measured in lawsuits, regulatory fines, and reputational damage — not in customer satisfaction points. A healthcare AI company in early 2025 calculated that a single undetected clinical error that reached a patient cost an average of $340,000 in legal exposure and remediation. At that cost, reviewing 500 clinical responses per day at $25 per review — $12,500 per day — was trivially justified.

**Tier two: disagreement cases where automated judges conflict.** When your LLM-as-judge pipeline includes multiple judges or multiple evaluation dimensions, some responses will produce split verdicts. One judge rates the response as acceptable, another as problematic. One dimension scores high while another scores low. These disagreements are gold. They mark the exact boundary where your automated evaluation system cannot decide, and a human judgment on these cases produces the highest-leverage training signal for improving your judges. Every disagreement resolved by a human calibrates the automated system for similar cases in the future. A team that resolves 200 judge disagreements per week can measurably improve judge accuracy within a month, reducing the number of disagreements and the need for human review over time.

**Tier three: novel query types and distribution shifts.** When your system encounters queries that fall outside the patterns in your evaluation data — new topics, new user populations, new use cases — automated judges may score these responses confidently but incorrectly, because the judges were calibrated on a different distribution. Human review of novel patterns catches the cases where confident automation is confidently wrong. Detecting novelty requires monitoring your query distribution and flagging responses to queries that are distant from your training and evaluation data in embedding space or topic classification.

**Tier four: random sampling for calibration.** After the higher tiers are covered, remaining review budget goes to random sampling across the full response population. This is not for catching individual errors — it is for calibrating the overall system. If your automated judges say 94 percent of responses are acceptable and your random human sample agrees at 93 percent, your judges are well-calibrated. If the human sample shows 87 percent acceptable, your judges have a systematic blind spot that needs investigation. Random sampling is the cheapest form of human review because the responses are straightforward — no pre-selection bias, no edge case difficulty — but it serves a critical monitoring function.

## Calculating Your Review Allocation

Turning the Priority Stack into a concrete budget requires knowing four numbers: your total available review hours, your per-review cost at each tier, your volume at each tier, and the minimum sample size for statistical validity.

Start with the total budget. If you have three domain experts available for twenty hours per week each, your total weekly capacity is sixty hours. At eight minutes per review on average, that is 450 reviews per week. That is your hard ceiling. No prioritization scheme can change the fact that 450 reviews is what you can afford.

Now allocate downward. Tier one — high-stakes responses — might produce 800 flagged responses per week. You cannot review all of them with 450 reviews. Set a target of reviewing 40 percent of high-stakes responses: 320 reviews, consuming roughly 43 hours. Tier two — judge disagreements — might produce 400 cases per week. Allocate 80 reviews to this tier: roughly 11 hours. Tier three — novel queries — might produce 200 flagged cases per week. Allocate 30 reviews: roughly 4 hours. Tier four — random sampling — gets whatever is left: 20 reviews from the remaining 2 hours. Total: 450 reviews, 60 hours, budget exhausted.

This allocation is not arbitrary. It reflects the principle that a dollar of review time in tier one prevents more damage than a dollar in tier four. But it also reveals the constraint clearly: you can only afford to review 40 percent of high-stakes responses. If that coverage level is unacceptable — if your risk model says you need 80 percent coverage on high-stakes categories — you have three options. Hire more reviewers. Improve automated pre-filtering to reduce the tier-one volume by removing the clearly correct responses before they hit the human queue. Or improve the upstream system so fewer high-stakes responses get flagged in the first place. The allocation math tells you where the bottleneck is. It does not solve it for free.

## Automated Pre-Filtering: Making Human Hours Go Further

The single highest-leverage technique for stretching a human review budget is not hiring more humans. It is making sure humans never see the responses that do not need them. **Automated pre-filtering** uses cheap, fast checks to remove the obvious cases before they enter the human review queue.

The mechanism is straightforward. Before a flagged response reaches a human reviewer, it passes through a sequence of automated filters. The first filter checks whether the response meets basic format and safety requirements — if an automated safety check already passed with high confidence, the response does not need human safety review. The second filter checks the confidence level of the automated judge — if the judge rated the response as clearly acceptable with a score above 0.95 on a zero-to-one scale, the response is likely fine and can skip human review unless it is in a tier-one category. The third filter checks for near-duplicates — if a response is semantically identical to a response that was already human-reviewed and approved within the past two weeks, it does not need another review.

The impact of pre-filtering is dramatic. A legal technology company routed all AI-generated contract summaries through human review in 2024 — roughly 1,800 reviews per week, consuming their entire review team of four people. After implementing a three-stage pre-filter in early 2025, the queue dropped to 420 reviews per week. The first filter removed 500 summaries that had passed automated accuracy checks at confidence levels above 0.97. The second filter removed 680 summaries that were near-duplicates of previously reviewed and approved outputs — the same contract clause types generating the same summary patterns. The third filter removed 200 summaries where all three automated judges agreed on both quality and completeness. The remaining 420 summaries were the genuinely ambiguous, novel, or borderline cases. The team's effective review coverage of the cases that mattered jumped from 100 percent with exhausted reviewers to 100 percent with capacity to spare.

The risk of pre-filtering is false negatives: responses that the automated filters incorrectly classify as not needing human review. A filter that removes responses where the judge scored above 0.95 will miss any case where the judge is confidently wrong. The mitigation is tier-four random sampling. By randomly sampling pre-filtered-out responses and having humans review them, you can detect systematic false negatives. If the random sample finds that 1 percent of pre-filtered responses actually had problems, you know the filter is removing a small number of bad responses along with the good ones. If the sample finds 8 percent, the filter threshold needs tightening.

## Review Leverage: One Review That Improves a Thousand Responses

The difference between human review as a cost center and human review as an investment is **review leverage** — the number of future responses improved by each individual review.

When a reviewer marks a response as incorrect and provides a correction, two things can happen. In the low-leverage scenario, the correction is logged, the specific response is flagged, and nothing else changes. The next time the system encounters a similar query, it makes the same mistake. The human review caught one failure but prevented zero future failures. In the high-leverage scenario, the correction flows back into the system: the corrected response becomes a training example for future fine-tuning, the error pattern is encoded as a new rule in the automated judge, or the query category is added to a retrieval augmentation that prevents the same error class. Now one review has improved the system's handling of an entire category of responses.

The difference between these two scenarios is not reviewer skill. It is feedback infrastructure. If you do not build the pipeline that turns human corrections into system improvements, then human review is consumption: you spend the money, you get one verified answer, and the value ends there. If you do build the pipeline, human review is investment: you spend the money, you get one verified answer, and the system gets permanently better at handling similar cases.

The feedback pipeline has three channels. The first is **judge calibration**: human judgments on the same responses that automated judges scored are used to measure and correct judge accuracy. If humans consistently disagree with the automated judge on a particular response type, the judge's criteria or threshold for that type can be adjusted. Over time, the automated judge becomes more aligned with human judgment, reducing the volume of cases that need human escalation.

The second channel is **training data generation**: corrected responses become supervised examples for the next round of model fine-tuning or prompt optimization. A human reviewer who rewrites an incorrect medical response from scratch produces a gold-standard example that is worth more than ten synthetic examples. If your system produces a class of errors consistently — say, it always misinterprets a specific type of insurance clause — ten human-corrected examples of that error pattern can eliminate the issue in the next model version.

The third channel is **pattern documentation**: reviewers annotate not just whether a response is good or bad but why. "The model cited a regulation that was superseded in 2024." "The response used the right framework but applied it to the wrong entity type." "The answer is technically correct but uses terminology that would confuse a non-specialist." These annotations build a library of failure patterns that can be used for prompt improvement, retrieval tuning, and automated judge rule creation. The annotations are more valuable than the binary good-or-bad judgment because they explain the mechanism of failure.

A B2B customer support platform measured their review leverage over a six-month period in 2025. Without the feedback pipeline, each human review improved exactly one response — the one being reviewed. After building judge calibration, training data, and pattern documentation channels, each human review improved an estimated 340 future responses on average. Some reviews — particularly those that identified a systematic failure pattern — improved thousands. The cost per review stayed at $14. But the effective cost per improved response dropped from $14 to $0.04. That is the economics of review leverage.

## Reviewer Fatigue and Quality Degradation

Human review budgets have a ceiling that is not about money. It is about reviewer fatigue. A domain expert who spends eight hours reviewing AI outputs does not produce eight hours of expert-quality judgments. Quality degrades predictably as the session continues, and teams that ignore this degradation waste their most expensive evaluation resource.

Research on expert judgment tasks consistently shows that decision quality drops after 90 to 120 minutes of continuous review. Reviewers become more lenient — they start approving borderline cases that they would have flagged at the start of the session. They become less thorough — they skim responses instead of reading carefully. They develop anchoring effects — the last few responses influence their judgment of the current one, even when the responses are unrelated.

The mitigation is session design. Limit review sessions to 90 minutes maximum with a 15-minute break before the next session. Randomize the order of responses in the queue so that reviewers do not see a long sequence of similar cases, which accelerates fatigue. Embed calibration checks — responses with known-correct ratings — at random intervals in the queue to measure reviewer accuracy over time. If a reviewer's agreement with calibration checks drops below 85 percent, the system should end the session rather than collecting unreliable data.

Practical scheduling matters more than most teams realize. Two 90-minute sessions per day is the maximum most experts can sustain at high-quality review before cognitive fatigue degrades their judgment below the level of a well-calibrated automated judge. At that point, additional human review is not just expensive — it is counterproductive. A fatigued expert who approves a dangerous medical response because they have been reviewing for five hours straight has not added safety. They have added a false sense of safety that is worse than no review at all.

## Structuring the Review Queue for Maximum Impact

The order in which responses appear in the review queue affects the quality and value of the reviews. Most teams present the queue in arrival order — first flagged, first reviewed. This is the worst possible ordering for review value because it ignores both priority and reviewer state.

The optimal queue structure follows three principles. First, present tier-one (high-stakes) items at the beginning of each session when reviewer acuity is highest. If a reviewer has 90 minutes before fatigue sets in, the first 60 minutes should contain the cases where the cost of a wrong judgment is highest. Second, alternate difficulty levels. Follow a complex case with two or three straightforward cases to give the reviewer cognitive recovery time. This is not about being nice to the reviewer — it is about maintaining judgment quality across the session. A queue that presents fifteen consecutive complex cases produces worse judgments on cases ten through fifteen than a queue that interleaves complex and simple cases.

Third, batch by topic within tiers. A reviewer switching between medical, financial, and customer service responses every two minutes loses context-switching time and makes more errors than a reviewer who evaluates twenty medical responses, then twenty financial responses. Topic batching preserves the reviewer's domain context and allows them to develop calibration within the topic — by the fifth medical response in a batch, the reviewer has a sharper sense of what "acceptable" looks like for that domain than they did on the first.

A healthcare technology company restructured their review queue in mid-2025 using these three principles. Before restructuring, their average review time was 11 minutes per response with an inter-reviewer agreement rate of 76 percent. After restructuring — prioritizing clinical safety cases first, alternating complex with simple, and batching by clinical specialty — the average review time dropped to 8 minutes per response and inter-reviewer agreement rose to 89 percent. The reviewers were not faster because they were less thorough. They were faster because context switching was eliminated and cognitive load was managed. The cost per review dropped from $28 to $20, and the quality of the reviews improved simultaneously.

## When to Expand the Review Budget

The question is not whether your review budget is big enough in absolute terms. It is whether the marginal value of the next review hour exceeds its cost. If your review leverage is high — each review improving hundreds of future responses — then expanding the review budget has a positive return on investment even at $150 per hour. If your review leverage is low — each review catching one error with no systemic improvement — then expanding the budget is pure consumption, and you should invest in feedback infrastructure instead.

There are three signals that your review budget genuinely needs to expand. The first is tier-one under-coverage: if you can only review 30 percent of high-stakes responses and the unreviewed 70 percent has a materially different error rate than the reviewed portion, you are accepting risk that the budget is supposed to mitigate. The second signal is rising disagreement rates: if your automated judges are producing more split verdicts over time — because the model changed, the query distribution shifted, or new use cases appeared — your tier-two queue is growing and your current budget cannot keep up. The third signal is declining judge accuracy: if your tier-four random sample reveals that the automated judges are becoming less aligned with human judgment, the calibration feedback loop needs more data, which means more human reviews.

There are also three signals that your review budget is bigger than it needs to be. The first is consistent over-agreement: if human reviewers agree with the automated judge on more than 98 percent of cases, the humans are not catching anything the automation misses, and the review budget is providing reassurance rather than signal. The second is low review leverage: if fewer than 10 percent of human corrections flow back into system improvements, the reviews are generating consumption value, not investment value. Fix the feedback pipeline before spending more on reviews. The third signal is reviewer underutilization: if your review queue consistently empties before the allocated hours are used, your routing is too aggressive and could be loosened, or the pre-filtering is already doing most of the work.

## The Organizational Politics of Review Budgets

Human review budgets sit at the intersection of three organizational tensions that have nothing to do with statistics and everything to do with power.

Product teams want maximum review coverage because they fear the PR headline about an AI mistake. Safety teams want maximum review coverage because they are measured on incident rates. Finance wants minimum review spending because human review is a line item that scales linearly with volume and has no natural ceiling. Engineering wants minimum review spending because managing a review queue with tooling, routing, and feedback infrastructure is complex to build and maintain. These four stakeholders rarely sit in the same meeting, and the review budget is usually set by whichever stakeholder made the most persuasive case at the last quarterly planning session.

The fix is the same as every other cost-quality tradeoff in this section: make it explicit, data-driven, and jointly owned. Define the review coverage target for each tier. Define the per-review cost ceiling. Define the review leverage target — the minimum number of future responses each review should improve through the feedback pipeline. Present all three numbers together, quarterly, to all four stakeholders. Adjust together, not independently. When product asks for more reviews, the response is not "that costs too much" — it is "at our current review leverage of 340 improvements per review, adding 100 reviews per week will improve 34,000 future responses per week at a cost of $1,400 per week, which is a return of $0.04 per improved response." When finance asks for fewer reviews, the response is not "that risks safety" — it is "reducing tier-one coverage from 40 percent to 25 percent increases unreviewed high-stakes exposure by 320 responses per week, with an estimated uncaught error rate of 3.2 percent based on current data."

Numbers turn arguments into decisions. Review budgets without numbers are political negotiations. Review budgets with numbers are engineering tradeoffs.

Progressive evaluation takes this logic further — not just deciding which responses get human review, but designing a multi-tier architecture where the cheapest possible check runs first and expensive checks, including human review, only trigger when cheaper layers cannot provide a verdict.