# 5.4 — Precomputation Strategies: Doing Expensive Work Before the User Asks

The fastest and cheapest response is the one you computed last night. Precomputation flips the economics of AI systems: instead of paying for computation at request time under latency pressure, you pay for it in advance, in batch, at lower prices, with no user waiting. The shift sounds obvious once you hear it. Yet most teams run every inference call in real time, even when the answer could have been generated hours ago. They treat every user request as a fresh problem because that is how they built the system on day one — single request in, single response out, billed at on-demand rates. They never stopped to ask which of those requests were predictable.

**Precomputation** is the practice of performing expensive AI operations ahead of time and serving stored results at request time. It does not replace real-time inference entirely. It replaces real-time inference for the portion of your workload that is predictable, stable, and latency-sensitive — which, in many products, is a larger portion than teams expect. The economics are compelling. Both OpenAI and Anthropic offer batch APIs at 50 percent discounts compared to synchronous pricing. The work runs during off-peak hours, often at even better effective rates. And because the results are precomputed and stored, the user sees sub-200-millisecond latency instead of multi-second generation times. You get cheaper computation, better user experience, and simpler request-time infrastructure. The only cost is freshness — your results are as current as the last batch run, not as current as the moment the user asks.

This subchapter teaches you the three primary precomputation strategies, when each one works, when each one fails, and how to calculate whether the tradeoff makes economic sense for your system.

## The Three Precomputation Strategies

Precomputation is not a single technique. It is a family of strategies that differ in what you compute ahead of time, how you store it, and how you serve it. Understanding the differences lets you pick the right strategy for your workload instead of defaulting to the most obvious one.

The first strategy is **query-response precomputation**: generating full answers to anticipated user queries before anyone asks. This is the FAQ approach at scale. You identify the most common queries your system receives, run them through your full inference pipeline in batch, store the responses, and serve them directly when a matching query arrives. The match can be exact — the user types a query you have precomputed — or approximate, using semantic similarity to map a user's question to the closest precomputed response. Query-response precomputation works best for systems with concentrated query distributions, where a modest number of question patterns account for a large percentage of traffic.

The second strategy is **document-level precomputation**: processing your knowledge base with AI before any user asks a question about it. Instead of waiting for a user to query a document and then summarizing or analyzing it on the fly, you pre-generate summaries, extract key metadata, identify themes, and create structured representations of every document in your corpus. When the user asks a question, your system retrieves the precomputed summary or metadata rather than running the full document through a language model at request time. This strategy works best for systems with large, relatively stable document collections — legal databases, knowledge bases, product catalogs, research repositories.

The third strategy is **embedding and index precomputation**: generating vector embeddings for your documents and building search indexes before any retrieval query arrives. This overlaps with standard RAG pipeline setup, but the distinction is in how aggressively you precompute. Naive systems recompute embeddings when documents are queried. Precomputation-oriented systems generate embeddings the moment a document enters the corpus, build optimized index structures during off-peak hours, and sometimes pre-generate chunking variations for different retrieval contexts. The user query at request time hits a pre-built, pre-optimized index rather than triggering computation.

These three strategies are not mutually exclusive. Many production systems combine all three: precomputed embeddings power the retrieval layer, precomputed document summaries enrich the retrieved context, and precomputed query-response pairs serve as a first-pass cache that handles the most common questions without touching the model at all.

## When Precomputation Works: The Predictability Test

Precomputation pays off when your workload passes what you might call the **predictability test** — a set of three conditions that determine whether computing ahead of time is economically justified.

The first condition is concentrated query patterns. If a small number of query types account for a large share of traffic, precomputation captures disproportionate value. A banking chatbot where 30 percent of customer questions are variants of the same 100 intents is a precomputation goldmine. A creative writing assistant where every query is unique is not. The metric to check is your query distribution: plot the cumulative percentage of traffic against the number of unique query clusters. If 20 percent of query clusters handle 60 percent or more of traffic, you have enough concentration for query-response precomputation. If the distribution is flat — every query is about equally likely — precomputation covers too little traffic to justify the batch cost.

The second condition is stable underlying data. Precomputed results are only valid as long as the source data has not changed. A legal research platform whose case law database updates weekly is ideal — you precompute on Sunday night and serve accurate results all week. An e-commerce platform whose prices change every hour is a poor fit, because precomputed pricing answers go stale before most users see them. The staleness tolerance of your domain determines the economic window: how many hours or days can a precomputed result remain valid before it becomes wrong enough to hurt the user experience?

The third condition is latency sensitivity. Precomputation only improves perceived performance when the user would otherwise wait for real-time inference. If your real-time pipeline already responds in 400 milliseconds and users are satisfied, precomputation adds complexity without meaningful UX improvement. But if your real-time pipeline takes 3 to 6 seconds — common for multi-step RAG pipelines or complex agentic workflows — precomputed results that serve in 150 milliseconds represent a dramatic improvement. The greater the gap between real-time latency and precomputed serving latency, the stronger the user experience argument for precomputation, independent of the cost savings.

When all three conditions hold — concentrated queries, stable data, high real-time latency — precomputation is almost always worth implementing. When only two hold, the economics are favorable but the implementation needs more care. When only one holds, precomputation is usually not worth the complexity it adds.

## The Economics: Batch Pricing Plus Zero Latency Cost

The financial case for precomputation rests on three cost levers that multiply together.

The first lever is batch pricing. Both OpenAI and Anthropic offer their batch inference APIs at 50 percent of synchronous pricing, with a 24-hour turnaround window. Google Cloud's Vertex AI offers similar discounts for batch prediction. This means every token processed in batch costs half what it costs in real time. If your precomputation covers 40 percent of your total inference volume, and all of that volume runs through the batch API, you save 20 percent of your total inference cost from the pricing discount alone.

The second lever is infrastructure efficiency. Real-time inference requires infrastructure that handles peak load — load balancers, auto-scaling groups, rate limit buffers, retry logic, timeout handling, and fallback paths. All of this infrastructure exists to handle the unpredictable timing of user requests. Precomputed results are served from a simple key-value store or database. The infrastructure cost for serving a precomputed response is negligible compared to running a model in real time. For the fraction of traffic served from precomputation, you eliminate not just the inference cost but the serving infrastructure overhead.

The third lever is the hidden cost of latency. When real-time inference takes 3 to 5 seconds, users abandon sessions, retry queries, or lose trust in the product. Each abandoned session is revenue you did not capture. Each retry doubles your inference cost for that user. Precomputed responses at sub-200-millisecond latency eliminate abandonment and retries for the covered query population. This lever is harder to quantify than the first two, but for consumer-facing products where engagement metrics drive revenue, it is often the most valuable.

To estimate the total savings, multiply three factors: the fraction of traffic that precomputation can cover, the per-request cost savings from batch pricing plus reduced infrastructure, and the revenue recovered from lower abandonment rates. A system processing 500,000 requests per day at $0.03 per request, where precomputation covers 35 percent of traffic at a 50 percent cost reduction, saves roughly $2,625 per day — over $78,000 per month. Add the latency improvement's effect on user retention and the number grows substantially.

## A Legal Research Platform: Precomputation in Practice

A mid-sized legal technology company provides a case law research platform used by 8,000 attorneys across 150 firms. Their original architecture processed every query in real time: the attorney asks a question, the system retrieves relevant case law from a database of 2.4 million documents, the language model synthesizes the retrieved documents into a structured answer with citations. Average response time was 4.2 seconds. Per-query cost was $0.085, driven by the combination of embedding generation, retrieval, and synthesis on Claude Sonnet 4.5. Monthly inference cost for 1.8 million queries: $153,000.

The engineering team analyzed their query logs and found three patterns. First, 42 percent of queries were about the same 5,000 legal topics — contract interpretation, employment discrimination standards, fiduciary duty definitions, and similar core areas that attorneys researched repeatedly. Second, the underlying case law database updated only when new court decisions were published, which happened at a predictable cadence: federal appellate decisions weekly, state decisions daily, but with most documents remaining stable for months or years. Third, the 4.2-second response time was a constant source of user complaints; attorneys used the tool less often than they would if it were faster, and three firms had cited response time as a factor in contract renewal negotiations.

The team implemented a three-layer precomputation strategy. Layer one: they identified the top 50,000 case documents by query frequency and pre-generated comprehensive summaries, key holding extractions, and citation networks for each document during a weekend batch run. Cost of the batch run: $4,200 at 50 percent batch pricing, running Claude Sonnet 4.5 over all 50,000 documents. These precomputed summaries became enrichment data that the system could reference without re-reading the full document at query time.

Layer two: they generated precomputed answers for the 5,000 most common legal topics, using the enriched document summaries as input. Each precomputed answer included the synthesis, the citations, and a confidence score. The batch run for these 5,000 answers cost $620. The answers were stored in a low-latency database with semantic keys for approximate matching.

Layer three: they pre-generated embeddings for the entire 2.4 million document corpus during off-peak hours, replacing the previous approach of generating embeddings incrementally as documents were queried. The batch embedding cost was $1,800, significantly cheaper than the cumulative cost of on-demand embedding generation over the same document set.

Results after one month. The 42 percent of queries matching precomputed topics were served in an average of 180 milliseconds — down from 4.2 seconds. The per-query cost for precomputed responses was $0.0004, accounting only for the database lookup and serving cost. The remaining 58 percent of queries still ran through real-time inference, but benefited from the precomputed document summaries and pre-built embedding index, reducing their average response time from 4.2 seconds to 2.8 seconds and their per-query cost from $0.085 to $0.062 (because the model received precomputed summaries instead of raw documents, reducing input tokens).

The monthly cost calculation shifted dramatically. The 42 percent precomputed traffic — 756,000 queries — cost $302 per month to serve, plus $4,820 per month for the weekly batch refresh runs. The 58 percent real-time traffic — 1,044,000 queries — cost $64,728 per month. Total monthly inference cost: $69,850. Down from $153,000. A 54 percent reduction. The weekly batch refresh added $4,820 per month in precomputation cost, but the net savings were $83,150 per month. User satisfaction scores increased by 22 percent. Two of the three firms that had cited response time in renewal negotiations renewed their contracts.

## Freshness Risk: The Price of Precomputation

The savings are real. So is the risk. Every precomputed result carries a freshness risk — the possibility that the underlying data has changed since the result was generated, making the served response partially or fully incorrect.

The severity of freshness risk depends entirely on the domain. For the legal research platform, a precomputed summary of a 2019 appellate decision has essentially zero freshness risk — the decision is not going to change. But a precomputed answer about "current standards for non-compete enforcement" has significant freshness risk, because new court rulings can shift legal standards overnight. When the Texas Supreme Court issues a ruling that changes non-compete enforceability, every precomputed answer about Texas non-compete law becomes potentially misleading until the next batch refresh.

Freshness risk is not abstract. It has a concrete user impact and a concrete business cost. A user who receives a stale precomputed answer and makes a decision based on it — an attorney who cites an overruled precedent, a financial advisor who quotes outdated regulatory guidance, a customer support agent who gives information about a discontinued policy — suffers real consequences. The business suffers reputational damage, potential liability, and loss of user trust. The cost savings from precomputation evaporate if users stop trusting the system.

The mitigation is not to avoid precomputation but to be honest about freshness boundaries. Every precomputed result should carry metadata: when it was generated, what data it was based on, and when the next refresh is scheduled. For sensitive domains, the serving layer should check whether the source data has changed since the precomputation timestamp and fall back to real-time inference if it has. This conditional serving approach captures the cost savings of precomputation for stable content while maintaining real-time accuracy for volatile content. The legal platform implemented exactly this: stable content (historical case law) served from precomputation with no freshness check, volatile content (actively developing legal areas) served from precomputation only if the source data had not been updated since the last batch run, and flagged topics (areas where regulatory change was imminent) always routed to real-time inference regardless of cache state.

## Building the Precomputation Pipeline

A precomputation pipeline has five stages, and skipping any stage leads to either wasted batch computation or stale results reaching users.

Stage one is **query analysis**: mining your production logs to identify which queries or query patterns are precomputable. This is not a one-time analysis. It is a recurring process — weekly or biweekly — because query patterns shift as users adopt new features, as your document corpus grows, and as external events change what people are asking about. The output of query analysis is a prioritized list of precomputation targets: the queries, documents, or embeddings that would deliver the most value if precomputed.

Stage two is **batch execution**: running the identified targets through your inference pipeline using batch APIs. The key engineering decisions here are scheduling and parallelism. Schedule batch runs during off-peak hours to take advantage of lower provider rates and avoid competing with real-time traffic for rate limits. Parallelize across multiple batch jobs to complete large precomputation runs within the available window. A legal platform precomputing 50,000 document summaries cannot afford a serial pipeline that takes four days — it needs parallel batch processing that completes in hours.

Stage three is **quality validation**: checking precomputed results before they reach users. Not every batch output is usable. Models can hallucinate in batch just as they can in real time. Run a sample of precomputed results through your evaluation pipeline — check factual accuracy, format compliance, citation correctness, whatever your quality dimensions are. If the failure rate exceeds your threshold, investigate the batch run before deploying the results. A batch run that produces 50,000 summaries with a 3 percent hallucination rate will push 1,500 incorrect summaries to users if you skip validation.

Stage four is **storage and indexing**: placing precomputed results in a serving layer optimized for low-latency retrieval. This is typically a key-value store like Redis or DynamoDB, with the query (or a normalized version of it) as the key and the precomputed response as the value. For semantic matching, you need an additional similarity search layer that maps incoming queries to the nearest precomputed response. The serving layer must support fast reads, efficient updates (for batch refreshes), and metadata queries (for freshness checks).

Stage five is **refresh orchestration**: scheduling and executing regular updates to the precomputed results. The refresh cadence depends on your freshness requirements, which we will address in detail in subchapter 5.6. The orchestration layer needs to handle partial refreshes — updating only the precomputed results whose source data has changed, rather than recomputing everything from scratch. A full recomputation of 50,000 document summaries costs $4,200. A partial refresh that updates only the 2,000 documents that changed since the last run costs $168. The difference compounds dramatically over months.

## What Precomputation Cannot Do

Precomputation has structural limits that no amount of engineering cleverness can overcome. Understanding these limits prevents you from over-investing in precomputation for workloads where it will never pay off.

Precomputation cannot handle highly personalized queries. If the correct response depends on the specific user's history, preferences, context, or permissions, you cannot precompute it without precomputing a separate response for every user — which quickly becomes more expensive than real-time inference. A personalized financial planning tool that tailors advice to each user's portfolio, risk tolerance, and tax situation cannot precompute meaningful answers. The combinatorial space of user-specific inputs is too large.

Precomputation cannot handle rapidly changing data in real-time-critical applications. If the underlying data changes every minute and users expect up-to-the-minute accuracy, the batch refresh cadence required to maintain freshness eliminates the cost advantage. A stock trading assistant that needs to reflect current market conditions cannot precompute answers about price targets or portfolio allocation. By the time the batch run completes, the data is stale.

Precomputation cannot handle open-ended creative or conversational tasks. If every interaction is genuinely unique — creative writing prompts, open-ended brainstorming, therapeutic conversations — there is no predictable query distribution to target. Precomputation works by exploiting repetition. Where there is no repetition, there is no leverage.

The teams that get precomputation right are the teams that are honest about these limits. They precompute what they can — the stable, predictable, high-volume portion of their workload — and run everything else in real time. The savings come from correctly sizing the precomputable fraction and investing the engineering effort there, not from trying to force-fit precomputation onto workloads where it does not belong.

## Combining Precomputation with Caching and Routing

Precomputation is most powerful when it operates as one layer in a multi-tier serving architecture. The typical architecture has three tiers. The first tier is exact-match and semantic caching, which captures queries that have been answered recently. The second tier is precomputation, which captures queries that match precomputed patterns even if they have not been asked recently. The third tier is real-time inference, which handles everything else.

The serving logic at request time is a cascade. The incoming query first checks the cache. If there is a hit, serve the cached response — cost is essentially zero, latency is sub-50 milliseconds. If there is no cache hit, check the precomputed response store. If there is a match above the similarity threshold, serve the precomputed response — cost is the storage lookup, latency is 100 to 200 milliseconds. If there is no precomputed match, route to real-time inference — full cost, full latency.

In this architecture, the cache handles recent repetition, precomputation handles predictable repetition, and real-time inference handles novelty. The combined effect is that a surprisingly small fraction of queries actually hit the full inference pipeline. For the legal research platform, the cache captured 18 percent of traffic (recent queries re-asked within the same day), precomputation captured 42 percent (common legal topics), and real-time inference handled the remaining 40 percent. Sixty percent of traffic never touched the language model at request time. The combined cost reduction was 67 percent compared to the original all-real-time architecture.

The key design decision is the ordering and thresholds between tiers. A semantic cache with a loose similarity threshold might overlap with precomputed responses, causing the cache to return results for queries that precomputation would handle more accurately (because precomputed results are quality-validated, while cached results are simply prior real-time responses that might have been lower quality). The tiers need distinct roles: the cache handles exact and near-exact repeats with tight similarity thresholds, precomputation handles broader topic coverage with quality-validated responses, and real-time inference handles the long tail.

## Measuring Precomputation ROI

Precomputation is an investment: you spend money on batch processing and engineering to save money on real-time inference and latency. Like any investment, it needs a measurable return.

The primary metric is **precomputation coverage** — the percentage of production queries that are served from precomputed results. Track this daily. If coverage is 40 percent in week one and drops to 28 percent by week four, your precomputation targets are drifting away from actual user behavior. You need to refresh your query analysis and update the precomputation targets.

The secondary metric is **cost per served result**, broken down by tier. Calculate the amortized cost of a precomputed result: the batch processing cost divided by the number of times the result is served before the next refresh. A precomputed document summary that costs $0.08 to generate and is served 200 times before refresh has an amortized cost of $0.0004 per serve. If the real-time alternative costs $0.06 per serve, the savings per serve are $0.0596 — multiplied by 200 serves, that is $11.92 of value from a single precomputed result. The results that are served frequently deliver enormous returns. The results that are rarely served deliver minimal returns and might not justify the batch processing cost.

The third metric is **freshness compliance** — the percentage of precomputed responses that were served while still within their freshness window. If 15 percent of precomputed responses are being served after their source data has changed, you have a freshness problem that needs either faster refresh cycles or better staleness detection. This metric is your early warning system for the quality risks of precomputation.

Precomputation handles the work you can predict. Caching handles the work you have seen recently. But both strategies share a critical vulnerability: the results they store can become wrong. Knowing when a precomputed or cached result has crossed the line from "slightly stale" to "dangerously incorrect" is the problem of cache invalidation, and it is the subject of the next subchapter.
