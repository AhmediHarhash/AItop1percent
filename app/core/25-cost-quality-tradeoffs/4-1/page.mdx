# 4.1 — The Token Budget: Why Context Length Is Your Biggest Variable Cost

In late 2025, a Series B insurtech company launched an AI claims assistant that processed customer inquiries, pulled relevant policy documents, and generated personalized coverage explanations. At launch, the average request consumed about 1,800 input tokens: a concise system prompt, the customer's question, and one or two retrieved policy excerpts. The per-request cost was manageable — roughly $0.01 on Claude Sonnet 4.5. The monthly bill for 400,000 requests settled around $4,000. Everyone was happy.

Six months later, the monthly bill was $13,200. Traffic had grown modestly — up 15 percent to 460,000 requests. But the average input token count had quietly climbed from 1,800 to 5,400. Three times the original. The product team had added a "policy comparison" feature that retrieved four documents instead of two. The engineering team had expanded the system prompt with new edge-case handling, compliance language, and formatting instructions. The conversation history feature — launched to improve multi-turn interactions — was appending every previous exchange in the thread. Nobody had tracked the aggregate effect because nobody was measuring context length as a metric. Each addition was individually reasonable. Together, they tripled the cost without anyone making a conscious decision to triple the cost. The engineering director who finally noticed described the bill as "death by a thousand tokens."

This story is not unusual. It is the default trajectory for any AI product where context length is not actively managed. Context length is the single most variable and least controlled cost in AI systems, and the teams that do not treat it as a budget will watch it grow until the invoice forces a crisis.

## The Mechanism: How You Pay for Tokens

Every call to a large language model has two token dimensions, and both cost money. You pay for the tokens you send in — the **input tokens** — and you pay for the tokens the model generates — the **output tokens**. The distinction matters because input and output tokens are priced differently, and the levers you have for controlling each are completely different.

Input tokens include everything in the prompt: the system instructions, the user's message, any retrieved documents or context, conversation history from previous turns, and any structured metadata you attach. Output tokens are whatever the model generates in response. In most 2026 pricing, output tokens cost substantially more per token than input tokens. Claude Opus 4.6 charges $5 per million input tokens and $25 per million output tokens — a 5x ratio. GPT-5 charges $1.25 per million input tokens and $10 per million output tokens — an 8x ratio. Even mid-tier models like Claude Sonnet 4.5 maintain a similar spread, at $3 input versus $15 output per million tokens.

This asymmetry means that controlling output length is high-leverage per token, but controlling input length is high-leverage per request — because input tokens are the ones that grow silently as features accumulate. A system prompt that expands from 300 to 1,200 tokens costs those extra 900 tokens on every single request. If you handle a million requests per month, that is 900 million additional input tokens — which on Claude Sonnet 4.5 translates to $2,700 per month in cost that nobody asked for and nobody approved.

## The Five Components of Input Token Cost

Your input token count on any given request is the sum of five components, and understanding each one is essential for setting a budget that holds.

First is the **system prompt** — the instructions that tell the model how to behave. System prompts start lean and grow over time. A new feature requires a new instruction. An edge case requires a new constraint. A compliance review adds a new disclosure requirement. A quality issue prompts a few-shot example. Over six to twelve months, a system prompt that started at 200 tokens can reach 1,500 without anyone intending it. System prompts have the unique property of costing tokens on every request, regardless of the request's content. They are the fixed cost component of your variable cost.

Second is the **user message** — whatever the human actually sent. This is the one component you have the least control over. A short query might be 20 tokens. A detailed question with pasted context might be 2,000 tokens. You can set maximum input lengths, but aggressive limits hurt the user experience on legitimate long queries. User message tokens are inherently variable, and your budget must account for the distribution, not just the average.

Third is the **retrieved context** — the chunks, documents, or passages that your retrieval system pulls from a knowledge base or vector database. This is where token growth is most explosive. A RAG pipeline configured to return the top five chunks at 500 tokens each adds 2,500 tokens per request. If a product improvement increases retrieval to ten chunks — because more context seems like more quality — you have doubled the retrieval cost without changing the model, the user, or the task. Retrieval tokens scale with the ambition of your retrieval pipeline, and ambition tends to grow faster than budgets.

Fourth is the **conversation history** — previous turns in a multi-turn interaction. Each additional turn adds the user's message and the model's response from that turn. A five-turn conversation at 300 tokens per turn adds 1,500 tokens of history. A twenty-turn conversation adds 6,000. For products with long-running conversations — customer support threads, coaching sessions, ongoing analysis — conversation history becomes the dominant cost component by turn eight or nine. Without pruning, it grows linearly and eventually consumes the entire context window.

Fifth is **metadata and formatting overhead** — the structural tokens that wrap your actual content. XML-style delimiters, role markers, JSON-like structures for tool calls, and separator tokens between context sections all consume tokens. A verbose prompt architecture with heavy structural markup can add 200 to 500 tokens of pure overhead per request. This component is easy to ignore because it seems small in isolation, but at scale it adds up: 300 overhead tokens across a million requests is 300 million tokens per month.

## Introducing the Token Budget

A **token budget** is a hard ceiling on total input tokens per request, allocated deliberately across the five components. It is the context-length equivalent of a financial budget: a conscious decision about how much you are willing to spend, broken down by category, with each category justified by its contribution to output quality.

The concept is simple. The discipline is not. Most teams do not have a token budget. They have a context window limit — typically 128,000 or 200,000 tokens — and they treat that as the ceiling. But the economic ceiling is far lower than the technical ceiling. Just because a model can accept 200,000 input tokens does not mean you should send 200,000 input tokens. On Claude Sonnet 4.5, a 200,000-token input costs $0.60 per request. At 100,000 requests per day, that is $60,000 per day — $1.8 million per month on input tokens alone. The technical ceiling would bankrupt you. The economic ceiling is what keeps you solvent.

Setting a token budget starts with your cost ceiling. Determine the maximum cost per request your product economics can sustain. If you charge customers $0.10 per query and your target gross margin on AI cost is 70 percent, your maximum cost per request is $0.03. At Claude Sonnet 4.5 pricing of $3 per million input tokens and $15 per million output tokens, and assuming an average output of 400 tokens, your output cost per request is $0.006. That leaves $0.024 for input tokens — which buys you 8,000 input tokens. That is your total token budget. Not the model's context window. Not what feels like enough context. Eight thousand tokens, because that is what the business can afford.

## Allocating the Budget Across Components

Once you have a total budget, you allocate it across the five components based on their contribution to quality. This allocation is not a guess — it is an empirical exercise that requires testing each component's marginal contribution.

A reasonable starting allocation for a RAG-powered application with conversation history might look like this: system prompt gets 400 tokens, user message gets 1,000 tokens, retrieved context gets 4,000 tokens, conversation history gets 2,000 tokens, and formatting overhead gets 600 tokens. Total: 8,000 tokens. Each allocation is a maximum, not a target. If the user's message is only 50 tokens, the remaining 950 are not filled with padding — they are available as buffer or simply unused.

The allocation reflects priorities. Retrieved context gets the largest share because it directly determines whether the model has the information needed to answer correctly. Conversation history gets a meaningful share because multi-turn coherence matters for user experience. The system prompt gets a tight allocation because every token in it costs money on every request, so it must be ruthlessly efficient. The user message allocation sets a practical ceiling that handles most queries while preventing a single verbose user from blowing the budget.

These allocations are not permanent. They shift based on the product and the task. A single-turn question-answering system might allocate zero tokens to conversation history and give everything to retrieval. A conversational coaching product might allocate heavily to conversation history and minimally to retrieval. A classification endpoint might need only 200 tokens total — system prompt plus user message — and have no retrieval or history at all. The token budget adapts to the product. What does not adapt is the discipline of having one.

## The Non-Linear Economics of Context Length

For most models in 2026, token pricing is linear — you pay the same per-token rate whether you send 500 tokens or 50,000. But the economic impact is not linear, because context length affects quality in ways that create hidden non-linearities in your cost structure.

The first non-linearity is the **lost-in-the-middle** phenomenon. Research has consistently demonstrated that models attend more strongly to information at the beginning and end of their context window than to information in the middle. As your context grows, the information you add to the middle of the window contributes less to output quality than the information already at the edges. You are paying linearly for tokens whose quality contribution is sublinear. The practical implication: doubling your retrieved context from 2,000 to 4,000 tokens does not double the model's knowledge. It increases knowledge by some fraction — maybe 30 to 60 percent — while doubling the retrieval portion of your cost.

The second non-linearity is inference latency. Longer contexts increase the time the model takes to process the input, which affects time-to-first-token and total response time. For real-time applications where latency matters, a longer context does not just cost more money — it degrades the user experience. The cost of that degradation is harder to measure but real: higher abandonment rates, lower satisfaction scores, fewer repeat sessions. You are paying more for tokens and getting a worse experience.

The third non-linearity emerges from prompt caching. Providers like Anthropic and Google offer substantial discounts on cached input tokens — up to 90 percent off standard input pricing. But caching only works on the static prefix of your prompt. The system prompt and any shared context can be cached. The user-specific content — the user's message, conversation history, and retrieved documents — cannot. As the variable portion of your context grows relative to the cached portion, you get less benefit from caching. A system where 70 percent of input tokens are cacheable gets massive savings from prompt caching. A system where 15 percent of input tokens are cacheable — because retrieval and history dominate — gets almost none. Your token budget allocation directly determines whether caching can save you money.

## Monitoring Token Budgets in Production

A token budget is only as good as the monitoring that enforces it. Without monitoring, the budget is a plan that reality ignores. The insurtech company from the opening did not lack cost awareness — they lacked visibility into how token consumption was growing request by request.

Instrument every model call with four measurements: total input tokens, total output tokens, the breakdown by component (system prompt, user message, retrieval, history, overhead), and the cost at current pricing. Store these measurements as time-series metrics. Build dashboards that show the distribution of input tokens per request, not just the average. The average is misleading — a system where 80 percent of requests use 2,000 tokens and 20 percent use 15,000 tokens has an average of 4,600, which obscures the fact that one-fifth of your traffic is blowing your budget.

Set alerts on percentile thresholds, not averages. If your budget is 8,000 input tokens, set an alert when the 90th percentile exceeds 7,500 tokens. This catches budget creep before it hits the invoice. Set a separate alert when any individual request exceeds 12,000 tokens — a hard warning that something in the pipeline is producing unexpectedly large contexts.

Track the per-component breakdown over time. If system prompt tokens are growing, someone is adding instructions without removing old ones. If retrieval tokens are growing, the retrieval pipeline configuration changed or the document corpus grew. If conversation history tokens are growing, your pruning strategy is not keeping up with session length. The breakdown tells you where to intervene. The total tells you whether to panic.

## The Budget as an Engineering Contract

The most effective teams treat the token budget not as a guideline but as an engineering contract between product, engineering, and finance. The contract specifies the total budget, the per-component allocation, the conditions under which the budget can be increased, and the approval process for increases.

When product wants to add a new feature that requires additional context — say, a comparison feature that retrieves more documents — the request goes through the same process as a request for additional compute budget. Product must justify the additional tokens by demonstrating the quality improvement the feature delivers. Engineering must show that the additional tokens fit within the overall budget, either by reducing another component or by explicitly requesting a budget increase. Finance must approve the cost impact of the increase, with a specific dollar figure attached.

This process sounds heavy. It is not, in practice. Most feature additions that increase context length are obvious wins — the quality improvement clearly justifies the cost. The process takes five minutes and a Slack conversation. But the process catches the additions that are not obvious wins: the extra 500 retrieval tokens that improve quality by 0.3 percent but cost $1,200 per month. Without the contract, those additions accumulate silently. With the contract, they require justification.

The contract also prevents the reverse problem: premature optimization that degrades quality. When an engineer proposes cutting the system prompt from 500 tokens to 200 tokens to save money, the contract requires them to show that quality holds by running the evaluation suite. The budget is a ceiling and a negotiation framework, not a mandate to minimize tokens regardless of consequences.

## When the Budget Does Not Fit

Sometimes the math does not work. Your cost ceiling implies a token budget of 4,000, but your product requires 8,000 tokens of context to produce acceptable quality. The budget does not fit the product.

This is not a failure of budgeting. It is a signal that something else needs to change. There are four responses, and the right one depends on your situation. First, you can change the model. If your budget constraint comes from using an expensive model, switching to a cheaper model with equivalent quality buys you more tokens within the same cost ceiling. Moving from Claude Opus 4.6 at $5 per million input tokens to Claude Haiku 4.5 at $1 per million input tokens lets you send five times more input tokens at the same price. Second, you can change the pricing. If your per-query price to customers does not cover the AI cost at the required quality level, the product is underpriced. Raise the price or restructure the pricing model. Third, you can change the architecture. Context compression, better retrieval precision, conversation summarization, and prompt optimization all reduce the tokens needed to achieve the same quality. These techniques are the subject of the remaining subchapters in this chapter. Fourth, you can accept the loss and invest in growth. Some products operate at a per-query loss during the growth phase, expecting to reach profitability through scale or through future model price reductions. This is a legitimate strategy when the unit economics have a credible path to viability — and a dangerous one when they do not.

The token budget forces this conversation. Without it, teams discover the math does not work only when the invoice arrives. With it, they discover the math does not work during planning, when they can still change the architecture, the pricing, or the model.

## The Budget as Cost Culture

Beyond its mechanical function, the token budget serves an organizational purpose. It makes cost a first-class engineering concern — not something the finance team worries about after the product ships, but something the engineering team designs around from the beginning.

When every request has a visible token cost, engineers naturally write tighter system prompts. When every new feature requires a token allocation, product managers naturally prioritize features by their quality-per-token return. When the monthly dashboard shows per-component token trends, the whole team naturally asks whether each component is pulling its weight. The budget creates a cost-conscious culture that compounds over time, because every decision is made with an awareness of what tokens cost.

The alternative is a culture where context length grows unchecked because nobody is looking. Where the system prompt accumulates instructions like a closet accumulates junk. Where retrieval adds chunks because more feels safer than less. Where conversation history grows because pruning feels risky. That culture produces the insurtech company's outcome: a tripled bill and a scramble to cut costs under pressure, which always produces worse results than cutting costs by design.

Your token budget is set. Now the question is what to do when the budget is tight — how to remove tokens from the context window without removing the information the model needs. That is the discipline of context pruning, which we explore next.
