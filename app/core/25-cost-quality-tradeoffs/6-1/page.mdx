# 6.1 — The Eval Tax: When Quality Measurement Costs More Than Quality Improvement

In early 2025, a Series B health-tech company building a clinical documentation assistant hired a quality engineering lead whose first act was to build what they called a "comprehensive eval layer." Every response generated by the system passed through four LLM-as-judge evaluations: one for medical accuracy, one for tone appropriateness, one for completeness, and one for formatting compliance. Each evaluation called Claude Opus 4.5 with a detailed rubric prompt averaging 1,200 input tokens and 400 output tokens. The system processed 180,000 responses per month. Four judge calls per response meant 720,000 LLM judge invocations per month. At the input and output token rates for Opus 4.5, the evaluation pipeline alone cost $42,000 per month — more than the inference cost of the production model it was evaluating. When the quality team analyzed the judge outputs, they found that 97.2 percent of responses passed all four evaluations. The eval pipeline was spending $42,000 a month to confirm that things were fine, catching defects on fewer than 3 percent of responses. The team could have fixed every defect the judges found — manually, one by one — for a fraction of that cost.

This is **the Eval Tax**: the total cost your organization pays to measure, score, and validate AI output quality. Like any tax, it is necessary up to a point and destructive beyond it. The eval tax includes every LLM-as-judge call, every human review hour, every compute cycle spent running regression suites, every embedding generated for semantic similarity checks, and every dollar spent on annotation platforms that support evaluation workflows. And like any tax, it tends to grow faster than the activity it supports — because adding a new eval dimension is easy, removing one requires proving a negative, and nobody wants to be the person who cut the quality check that would have caught the next incident.

## The Anatomy of the Eval Tax

The eval tax is not one cost. It is a stack of costs that compound across every layer of your evaluation infrastructure, and each layer has its own growth dynamics.

**LLM-as-judge costs** are typically the largest component. Every time you use a language model to score or grade another model's output, you are making an inference call that consumes tokens and burns money. A single judge call might cost $0.01 to $0.05 depending on the judge model and the length of the prompt. That sounds trivial until you multiply it by the number of evaluation dimensions, the number of responses evaluated, and the frequency of evaluation. A team running three judge dimensions on 500,000 responses per month at $0.03 per judge call spends $45,000 per month on LLM judging alone. If they add a fourth dimension — safety, say, or brand voice — the cost jumps to $60,000 without any change in traffic or inference spend.

**Human review costs** are the second-largest component for most teams. Expert reviewers — whether domain specialists, linguists, or dedicated QA staff — cost $25 to $75 per hour depending on specialization and geography. A team with four reviewers spending 20 hours per week on evaluation burns $8,000 to $24,000 per month in review labor. Human review is slower to scale than LLM judging, which is both its strength — it forces prioritization — and its weakness, because the backlog of items awaiting review can grow faster than the team can process them.

**Compute and infrastructure costs** for evaluation are the most invisible component. Running regression test suites on every deployment means spinning up eval infrastructure — embedding models for semantic similarity, retrieval pipelines for reference matching, batch processing jobs for large-scale judge runs. These costs live on infrastructure invoices rather than AI-specific dashboards, which makes them easy to miss. A team that runs a full regression suite of 5,000 test cases on every pull request, with each test case requiring an LLM call and a semantic similarity check, might spend $200 per regression run. If the team merges three pull requests per day, that is $600 per day or $18,000 per month on regression eval compute alone.

**Annotation platform costs** round out the picture. Tools like Scale AI, Labelbox, Surge AI, and custom annotation platforms charge per-task or per-seat fees that contribute to the eval tax. Even open-source annotation tools require hosting, maintenance, and operational investment that counts against the eval budget.

## How the Eval Tax Compounds

The eval tax does not grow linearly. It compounds through three mechanisms that catch teams off guard.

The first mechanism is **dimension creep**. Every quality incident triggers a new evaluation dimension. A customer complains about tone, so the team adds a tone judge. A compliance audit reveals formatting inconsistencies, so they add a format judge. A safety incident surfaces, so they add a safety judge. Each new dimension is justified individually — the incident was real, the risk is real, the judge catches real problems. But nobody tracks the aggregate cost of all dimensions combined. After twelve months of dimension creep, a team that started with one judge dimension at $8,000 per month might be running seven dimensions at $56,000 per month, with each individual dimension still looking reasonable in isolation.

The second mechanism is **coverage expansion**. Teams start by evaluating a sample of responses — maybe 5 or 10 percent. Then an incident occurs on an unevaluated response, and someone asks why the eval pipeline did not catch it. The answer is that it was not in the sample. The solution, politically, is to increase coverage. The sample grows to 25 percent, then 50 percent, then "we should just evaluate everything." Each expansion is driven by a real failure and a reasonable desire for completeness. But 100 percent coverage at seven judge dimensions on a growing traffic base is a cost trajectory that only bends upward.

The third mechanism is **model upgrades for judges**. Teams periodically upgrade their judge models because newer, more capable models produce more accurate evaluations. The upgrade from GPT-4o to GPT-5 as a judge improved scoring consistency — but it also increased per-call cost. The upgrade from Claude Sonnet 4.5 to Claude Opus 4.6 as a judge improved nuanced assessment — but at a significant price premium. Each upgrade improves eval quality marginally while increasing eval cost substantially. Over time, the judge model becomes more expensive than the production model it evaluates, which is the quality engineering equivalent of the tail wagging the dog.

## The Eval-to-Inference Cost Ratio

The single most useful metric for detecting an excessive eval tax is the **Eval-to-Inference Cost Ratio**, or EICR. It is calculated by dividing your total monthly evaluation cost — all judge calls, human review hours, regression compute, and annotation platform fees — by your total monthly inference cost for production responses. The result tells you how many cents you spend on measuring quality for every dollar you spend on producing output.

An EICR of 0.05 means you spend five cents evaluating for every dollar of inference. That is lean and possibly under-invested in quality measurement. An EICR of 0.15 means fifteen cents on the dollar — a healthy investment in quality for most applications. An EICR of 0.30 means you spend thirty cents measuring quality for every dollar producing it — high, but potentially justified for safety-critical applications like healthcare or financial advice. An EICR above 0.50 means your eval costs are more than half your inference costs. Unless you are in a domain where a single quality failure has catastrophic consequences — medical diagnosis, autonomous systems, legal compliance — an EICR above 0.50 signals that your evaluation infrastructure has grown beyond what is justified by the quality improvements it delivers.

The health-tech company from the opening had an EICR of 1.05 — they were spending more on evaluation than on inference. For their risk profile, an EICR of 0.20 to 0.30 would have been appropriate. They were over-spending on evaluation by roughly three times.

To calculate your own EICR, start by tallying every evaluation-related cost. Include LLM judge calls — check your API invoices for calls tagged as evaluation or judge. Include human review labor — hours times hourly rate for everyone who spends time reviewing AI outputs. Include compute for regression suites and batch eval jobs. Include platform fees for any annotation or evaluation tooling. Divide the sum by your total production inference spend — the API costs for generating the responses your users actually see. If the number surprises you, it should. Most teams have never calculated it.

## The Paradox: Measuring Quality vs. Improving Quality

Here is the uncomfortable truth that the eval tax reveals: measuring quality and improving quality are different activities, and money spent on measurement does not automatically improve the thing being measured.

A team spending $42,000 per month on evaluation and catching defects on 3 percent of responses has two options. Option one: continue spending $42,000 per month to catch those defects as they occur, review each one, and fix it manually or let it reach the user. Option two: invest a fraction of that $42,000 into fixing the root causes — improving the prompt, fine-tuning the model, adding guardrails, fixing the retrieval pipeline — so that the defect rate drops from 3 percent to 0.5 percent. Then reduce the evaluation spend to match the lower defect rate.

Option two is almost always better. Root cause fixes are multiplicative — they reduce defects across all future responses, permanently. Evaluation is additive — it catches defects one at a time, at a cost per catch. A $5,000 investment in prompt engineering that reduces defects from 3 percent to 1 percent saves more quality than $42,000 per month of evaluation that catches and flags defects after they occur. But teams default to option one because evaluation is visible and satisfying — you can see the dashboard, count the catches, report the metrics. Root cause improvement is invisible until it works, harder to measure, and provides no comforting dashboard of catches to report to leadership.

The **Measurement-Improvement Ratio** is a useful frame. For every dollar you spend on evaluation, ask: how many dollars are you spending on improving the quality that evaluation measures? If the ratio is heavily skewed toward measurement — if you spend ten times more measuring quality than improving it — your evaluation infrastructure has become a monitoring system that watches problems happen rather than a quality system that prevents them.

## The Eval Tax in Different Contexts

Not all systems should have the same EICR. The appropriate eval tax depends on the consequences of quality failures and the maturity of your quality improvement efforts.

**High-stakes systems** — medical advice, legal document generation, financial recommendations — justify a higher eval tax because a single quality failure can cause real harm. An EICR of 0.25 to 0.40 may be appropriate. But even in high-stakes systems, the eval tax should be concentrated on the dimensions that correspond to the highest-consequence failures. A medical documentation system should invest heavily in accuracy evaluation and conservatively in formatting evaluation. Spending equal amounts on both treats all quality dimensions as equally critical, which they are not.

**Medium-stakes systems** — customer support, content generation, internal productivity tools — should target an EICR of 0.10 to 0.20. Quality matters, but individual failures are recoverable. The eval tax should focus on the failure modes that degrade user trust and business metrics — factual errors, unhelpful responses, off-brand tone — and should not exhaustively evaluate dimensions where failures are cosmetic.

**Low-stakes systems** — brainstorming assistants, internal summarization tools, draft generators — should target an EICR of 0.03 to 0.10. A lightweight sampling-based evaluation with periodic human spot-checks is sufficient. Spending heavily on evaluation for a system where users expect imperfection and routinely edit outputs is a misallocation of resources that could be invested in higher-stakes systems.

The mistake is applying a uniform eval tax across all systems. A team running five AI products with a single evaluation framework — same judge models, same evaluation dimensions, same coverage rate — is over-evaluating their low-stakes systems and possibly under-evaluating their high-stakes ones. The eval tax should be proportional to the consequences of failure, not uniform across the portfolio.

## Detecting an Excessive Eval Tax

Beyond the EICR, there are four diagnostic signals that your eval tax has grown beyond what your quality requires.

**Signal one: high pass rates across all dimensions.** If 95 percent or more of evaluated responses pass every evaluation dimension, you are spending heavily to confirm that things are working. A 95 percent pass rate means your system is already good. Evaluation should be concentrated on the 5 percent that fails, not uniformly applied to the 95 percent that passes. This is the strongest signal that sampling — rather than exhaustive evaluation — is appropriate.

**Signal two: eval cost growing faster than traffic.** If your traffic grows 20 percent per quarter and your eval cost grows 40 percent per quarter, the eval tax is compounding from dimension creep, coverage expansion, or model upgrades. Plot eval cost and traffic on the same chart over time. If the eval line pulls away from the traffic line, something is growing that should not be.

**Signal three: eval findings not leading to action.** If your evaluation pipeline flags 2,000 defects per month and your quality improvement team addresses 200, the other 1,800 flagged defects were identified but not acted upon. You paid to find them and then did nothing. Either the flagged items are not important enough to fix — in which case you should stop evaluating for them — or your improvement capacity is the bottleneck, in which case more evaluation spend yields no additional quality.

**Signal four: no one can explain which eval dimensions drive quality.** Ask your quality team which of their seven evaluation dimensions has the strongest correlation with user satisfaction, retention, or revenue. If they cannot answer — if they have never measured the relationship between eval scores and business outcomes — then some of their eval dimensions may be measuring things that do not matter. Every eval dimension should be traceable to a business outcome. Dimensions that are not traceable are candidates for elimination.

## Reducing the Eval Tax Without Losing Signal

The goal is not to minimize the eval tax. The goal is to maximize the quality signal per dollar of eval spend. There are five levers for achieving this, and they work best in combination.

**Lever one: sample instead of exhausting.** Evaluate a statistically representative sample of responses rather than every response. With the right sample size and sampling strategy, you can detect quality issues with high confidence while evaluating a small fraction of traffic. We cover this in detail in the next subchapter.

**Lever two: match the judge to the judgment.** Use cheap models for simple evaluations and expensive models only for complex judgments. Format compliance does not require a frontier model. Nuanced accuracy assessment might. Tiering your judges by evaluation complexity reduces cost without reducing signal on the dimensions that matter most. We cover this in Subchapter 6.3.

**Lever three: evaluate progressively.** Start with cheap, fast checks and escalate to expensive, thorough checks only when the cheap checks flag a concern. A cheap format check that passes means you can skip the expensive accuracy check on that response — or at least deprioritize it. Progressive evaluation reduces the average cost per evaluated response because most responses pass the cheap checks and never reach the expensive ones.

**Lever four: retire low-value dimensions.** Audit every evaluation dimension annually. For each dimension, ask: when was the last time this eval caught a defect that led to a quality improvement? If the answer is "never" or "more than six months ago," the dimension is measuring a risk that has already been mitigated. Retire it. You can always reinstate it if the risk reappears.

**Lever five: invest in prevention over detection.** Shift budget from evaluation to root cause improvement. If your prompt engineering, guardrails, and fine-tuning are good enough that defect rates are consistently low, you need less evaluation to maintain confidence. Every dollar moved from detection to prevention compounds — prevention reduces defects permanently, while detection catches them one at a time.

## The Eval Tax as a Leading Indicator

Here is the reframe that changes how engineering leaders think about evaluation cost. The eval tax is not just a cost to manage. It is a leading indicator of your quality engineering maturity.

A high eval tax with a high defect rate means your system is broken and you are paying heavily to measure how broken it is. The priority is improvement, not more measurement.

A high eval tax with a low defect rate means your system is good but your evaluation infrastructure has not been right-sized. The priority is reducing eval spend to match the lower risk.

A low eval tax with a low defect rate means your system is good and your evaluation is efficient. This is the target state — lean evaluation that maintains confidence without over-spending.

A low eval tax with a high defect rate means you are flying blind. You are under-invested in evaluation and quality problems are reaching users undetected. The priority is increasing eval spend — strategically, not uniformly — until you have enough signal to drive improvement.

The healthiest eval tax trajectory looks like this: high early, when the system is new and quality is uncertain, then declining as quality improves and evaluation becomes more targeted. Teams that see their eval tax rising year over year without a corresponding increase in traffic or risk are accumulating evaluation debt — spending more and more on measurement without extracting proportionally more value. The quarterly review of your EICR should be as routine as reviewing your inference costs. Both are line items that grow when nobody watches them.

You do not need to evaluate every response your system generates. You need to evaluate enough of the right responses, with enough statistical rigor, to maintain confidence in quality while spending a fraction of what exhaustive evaluation would cost. That is the discipline of sampling, which we turn to next.
