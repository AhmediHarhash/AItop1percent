# 9.1 â€” Cost Curves at Scale: How Everything Changes Between 10K and 50M Requests

In early 2025, a consumer health startup launched an AI symptom checker that served roughly 10,000 queries per day. The architecture was clean and sensible for its scale: a single frontier model behind a managed API, a Redis cache that stored exact-match responses for the fifty most common symptom descriptions, a lightweight monitoring dashboard running on a $200-per-month observability platform, and a model routing layer that directed simple queries to a mid-tier model while sending complex diagnostic reasoning to Claude Opus 4.5. The team had done their cost-quality homework. Average cost per query was $0.028, latency sat comfortably at 1.1 seconds at the 95th percentile, and quality scores on their eval suite hovered above 91 percent. They had optimized thoughtfully for their scale and it worked.

Then the product went viral. A partnership with a major pharmacy chain drove adoption from 10,000 queries per day to 80,000 within six weeks, then to 500,000 within four months. By the time they hit half a million daily queries, every optimization they had built was either inadequate or actively harmful. The Redis cache that handled fifty common queries now needed to handle tens of thousands of semantic variations, and its hit rate had dropped from 34 percent to 9 percent because most queries at high volume were unique phrasings of similar questions that exact-match caching could not capture. The model routing layer, which saved $4,000 per month at low volume, was now adding 180 milliseconds of latency to every request because the routing classifier itself had become a bottleneck under load. The monitoring dashboard that cost $200 per month at 10,000 queries now ingested so much telemetry data that it cost $14,200 per month, and the team could not turn it down without losing visibility into the quality degradation they suspected was happening. The architecture that was elegant at 10,000 requests per day was gasping at 500,000.

This is not a story about a team that built badly. It is a story about a team that built correctly for one scale and discovered that scale changes everything.

## The Three Scale Regimes

Cost-quality dynamics do not shift gradually as volume increases. They shift in regimes, with relatively stable economics within each regime and sharp transitions between them. Understanding these regimes lets you anticipate the transitions instead of being blindsided by them.

The **startup regime** covers systems handling fewer than roughly 50,000 requests per day. In this regime, per-request API costs are your dominant variable expense, fixed infrastructure costs are low in absolute terms but high per request, and your team's engineering time is more valuable than any infrastructure optimization. The math at this scale is simple: pick the right model, write good prompts, cache what you can, and focus your engineering energy on product quality rather than cost engineering. A team at 20,000 requests per day that spends three engineering weeks building a sophisticated caching layer to save $800 per month has made a poor allocation decision. That engineering time would have been better spent improving the eval suite or fixing the prompt failures that drive user complaints.

The **growth regime** covers roughly 50,000 to one million requests per day. This is where cost engineering becomes a real discipline rather than an afterthought. Per-request API costs are now a significant budget line, often $15,000 to $80,000 per month depending on model choice and query complexity. Caching moves from nice-to-have to necessary. Model routing becomes worth the complexity. Monitoring costs start to compound. And the organizational questions get harder: do you hire a dedicated cost engineer, or does the existing team add cost optimization to their responsibilities? Do you negotiate volume discounts with your API provider, or start evaluating self-hosted alternatives? The growth regime is where most teams first feel real cost pressure, and where the decisions you make determine whether you scale gracefully or painfully into the next regime.

The **scale regime** starts above one million requests per day and extends into the tens of millions. At this volume, the economics invert in ways that are counterintuitive to teams who built their intuitions in the startup regime. API-provider pricing, which was cheap and simple at low volume, becomes the single largest cost driver and the most important thing to optimize. Fixed infrastructure investments that seemed extravagant at low volume, such as self-hosted models on dedicated GPU clusters, become dramatically cheaper per request than API pricing. Operational complexity, including monitoring, alerting, on-call rotations, and incident management, grows super-linearly because the blast radius of any failure is measured in hundreds of thousands of affected users rather than hundreds. And human review, if you have any, becomes a scaling crisis because even a 1 percent escalation rate at 5 million daily requests means 50,000 reviews per day, which requires a team of 200 or more full-time reviewers working in shifts.

Each regime has its own cost structure, its own dominant optimization levers, and its own architectural requirements. The team that tries to apply startup-regime thinking at scale-regime volume will overspend by multiples. The team that tries to apply scale-regime infrastructure at startup-regime volume will over-engineer and under-ship.

## Sub-Linear Costs: The Economies You Gain

Some cost components grow slower than request volume. These are your friends at scale, and understanding them lets you forecast where your per-request cost will naturally decline as you grow.

**Fixed infrastructure amortization** is the most straightforward sub-linear cost. A monitoring platform that costs $3,000 per month serves you equally well at 50,000 requests per day and 200,000 requests per day. A Kubernetes cluster with baseline compute capacity supports a wide range of traffic before needing additional nodes. Audit logging infrastructure has step-function costs: you pay for a certain tier of write throughput and storage, and within that tier, the per-request cost drops as volume rises. A team paying $2,000 per month for logging infrastructure at 50,000 daily requests has an amortized logging cost of $0.0013 per request. At 500,000 daily requests on the same infrastructure, the amortized cost drops to $0.00013 per request, a tenfold improvement.

**Caching returns** also grow sub-linearly with investment. A basic exact-match cache requires minimal infrastructure and delivers modest hit rates, typically 10 to 20 percent for most AI applications. As you invest in semantic caching, the hit rate improves because you start matching queries that are phrased differently but mean the same thing. Industry analysis from 2025 shows a pattern that holds across most production AI systems: roughly 18 percent of queries are exact duplicates that basic caching catches, another 47 percent are semantic matches that advanced caching captures, and the remaining 35 percent are genuinely novel. This means your cache infrastructure investment has a ceiling of roughly 65 percent hit rate, but approaching that ceiling delivers enormous savings because each cache hit replaces a full model inference call that might cost ten to fifty times more than the cache lookup.

**Negotiated pricing discounts** also operate sub-linearly. API providers offer volume discounts that reduce per-token costs at higher tiers. A team consuming $5,000 per month in API costs has little negotiating leverage. A team consuming $80,000 per month can negotiate 15 to 30 percent discounts. A team consuming $500,000 per month can negotiate custom pricing that may reduce costs by 40 percent or more. These discounts are not automatic, but they are available, and they create a natural per-request cost reduction as volume grows.

## Linear Costs: The Proportional Burden

Some costs grow in direct proportion to request volume. These are neither friends nor enemies, but they define the floor of your per-request cost and determine whether your unit economics work at scale.

**Per-token API costs** are the most visible linear cost. If your average request costs $0.025 in model inference and you serve 100,000 requests per day, your daily inference bill is $2,500. At one million requests per day, it is $25,000. At ten million, it is $250,000. There is no structural discount built into the per-token pricing itself, though the volume discounts mentioned above can bend this curve slightly. The linearity of token costs is why model routing and caching are so important at scale: they are the primary mechanisms for reducing the effective per-request inference cost when the nominal cost per token stays flat.

**Egress and data transfer costs** scale linearly with response volume. Every response your system returns is bytes moving through the network, and cloud providers charge for that transfer. At low volume, egress costs are invisible, typically a few dollars per month. At high volume, they become material. A system generating 500-token responses at one million requests per day transfers roughly 2 to 3 GB of response data daily. At standard cloud egress rates, this adds $50 to $150 per month, still manageable. But add monitoring telemetry, cross-region replication, and backup data flows, and the total data transfer bill can reach $1,000 to $5,000 per month at scale.

**Safety and content filtering** scales linearly if you run checks on every request. A content classifier that costs $0.0005 per request adds $50 per day at 100,000 requests but $5,000 per day at ten million. This linearity is why risk-proportional safety, which applies heavyweight filtering only to high-risk queries, becomes essential at scale. Running the same content filter on every request regardless of risk profile is affordable at startup volume and ruinous at scale volume.

## Super-Linear Costs: The Scaling Traps

The most dangerous cost components are the ones that grow faster than request volume. These are the costs that break systems at scale, because they violate the intuition that doubling traffic merely doubles cost.

**Database query costs** often scale super-linearly because of index complexity, write contention, and lock escalation. A RAG system that retrieves context from a vector database might perform one similarity search per request. At 10,000 requests per day, the database handles the load easily on a single instance. At 500,000 requests per day, the same database needs sharding, replica sets, and connection pooling, and the infrastructure cost jumps from $300 per month to $8,000 per month, a 26-fold increase for a 50-fold traffic increase. But at 5 million requests per day, the cost does not simply multiply again. Write-heavy workloads that update embeddings or log retrieval results encounter lock contention that degrades throughput, requiring more powerful instances or more shards than a linear projection would predict. Teams that budget for database costs based on linear extrapolation from low volume consistently underestimate by 30 to 60 percent at high volume.

**Human review queues** are the most predictable super-linear cost, though teams routinely underestimate them. If 3 percent of requests require human review and each review takes two minutes at a loaded labor cost of $28 per hour, the per-review cost is roughly $0.93. At 50,000 requests per day, that is 1,500 reviews costing $1,395 per day, manageable with a team of twelve reviewers working eight-hour shifts. At one million requests per day, that is 30,000 reviews costing $27,900 per day, requiring a team of 250 reviewers. But the cost does not merely grow 20-fold. Managing 250 reviewers requires a management layer, quality assurance processes, training infrastructure, and inter-annotator agreement monitoring that 12 reviewers do not need. The operational overhead adds 30 to 50 percent on top of the direct labor cost, making human review at scale one of the most expensive components in the entire system.

**Cache invalidation complexity** is the subtle super-linear cost that almost nobody forecasts. A simple cache with a few hundred entries can be invalidated with straightforward TTL expiration. A semantic cache with millions of entries across thousands of topic clusters requires sophisticated invalidation logic, because updating a piece of knowledge, for example a drug interaction warning, means finding and invalidating every cached response that referenced that knowledge. The computational cost of this invalidation search grows with the cache size, and the cache size grows with traffic volume. Teams that build caching strategies for growth-regime volume without planning for scale-regime cache invalidation often find that their cache becomes a source of stale, incorrect answers rather than a cost optimization.

## The Transition Tax: What It Costs to Cross Regimes

Crossing from one regime to the next is not free. There is a **transition tax**, a one-time cost in engineering effort, architectural changes, and temporary performance degradation that every team pays when it outgrows its current regime.

The startup-to-growth transition typically costs two to four engineering months of dedicated work. The team needs to implement proper caching, set up cost monitoring with per-request attribution, renegotiate API provider contracts or evaluate alternatives, and introduce at least basic model routing. During the transition, the system often runs less efficiently than it did in the startup regime, because the old architecture is straining under load while the new architecture is partially built. A team handling 80,000 requests per day on a system designed for 20,000 often sees a temporary per-request cost increase of 20 to 40 percent during the transition, because the caching is not yet tuned, the routing is not yet optimized, and the team is running parallel infrastructure during the migration.

The growth-to-scale transition is more expensive, typically four to eight engineering months and often requiring new hires with specialized infrastructure experience. The team needs to evaluate self-hosted models, build or adopt a serving infrastructure stack like vLLM or TensorRT-LLM, implement semantic caching with proper invalidation, redesign monitoring for high-cardinality data, and potentially rearchitect the human review pipeline. This transition often involves a temporary cost spike of 50 to 100 percent as the team runs parallel systems during migration. A team that crosses the one-million-requests-per-day threshold without having started this transition three to six months earlier will find itself in crisis mode: spending too much on API costs to sustain current operations while simultaneously investing in infrastructure to reduce those costs.

The teams that handle transitions well are the ones that start planning for the next regime when they are halfway through the current one. If you are at 30,000 requests per day and growing 20 percent month over month, you have roughly five months before you cross the 50,000-request threshold. Start the growth-regime architecture work at month three, not month five. The transition tax is lower when you pay it gradually from a position of stability than when you pay it all at once from a position of crisis.

## Forecasting Your Cost Curve

The practical output of understanding scale regimes is the ability to forecast your cost curve before you hit the transitions, rather than after.

Build a **three-horizon cost model** that projects your per-request cost at your current volume, at three times your current volume, and at ten times your current volume. For each horizon, identify which cost components are sub-linear, linear, and super-linear. Sum them to get the projected per-request cost at each level. The shape of this curve tells you when your economics will change.

If the curve is flat or gently rising, your current architecture will scale reasonably well. You have time. If the curve rises steeply between the current and three-times horizons, you are approaching a regime transition and need to start planning now. If the curve rises steeply between the three-times and ten-times horizons but is flat until then, you have time but should be aware of the cliff ahead.

The model does not need to be precise. A forecast that says "our per-request cost will rise from $0.028 to approximately $0.035 at three times volume and approximately $0.052 at ten times volume" is enormously more useful than no forecast at all, because it tells you that the ten-times horizon requires structural changes while the three-times horizon does not. It lets you plan infrastructure investments, negotiate API contracts, and hire specialist engineers on a timeline that matches your growth trajectory rather than your crisis trajectory.

A fintech startup that built this model in mid-2025 discovered that their per-request cost would rise from $0.031 to $0.044 at five times their volume, driven almost entirely by the super-linear growth of their database costs and human review queue. They made two changes before hitting that volume: they moved their vector database from a managed service to a self-hosted cluster on dedicated hardware, reducing the database cost curve from super-linear to approximately linear, and they invested in an automated quality classifier that reduced their human review escalation rate from 4.2 percent to 1.8 percent. By the time they actually reached five times their original volume seven months later, their per-request cost was $0.033 instead of the $0.044 their original model predicted. The three-horizon model paid for itself in the first month at the higher volume.

## The Scale-Regime Mindset

The deepest shift when moving from startup to scale is not architectural. It is mental. At startup scale, cost optimization is about choosing the right model and writing efficient prompts. At growth scale, it is about caching, routing, and negotiation. At true scale, it is about infrastructure engineering, operational discipline, and organizational design.

The engineering leader managing a system at ten million requests per day is not thinking about which model is cheapest per token. They are thinking about GPU utilization rates, inference serving throughput, cache hit ratios, escalation rate trends, and the total cost of their operations team. They are negotiating multi-year infrastructure contracts, evaluating whether to build a dedicated inference cluster or lease GPU time on a cloud provider's reserved instances, and managing a team of twenty or more people whose sole job is keeping the system running efficiently.

This is not a world that the startup-regime team imagined when they were picking between Claude Haiku 4.5 and GPT-5-mini for their prototype. But it is the world they will inhabit if their product succeeds. Every successful AI product eventually crosses every regime boundary. The ones that survive the crossing are the ones that saw it coming.

The first and most impactful optimization that changes character between regimes is caching. At low volume, caching is an engineering nicety. At high volume, it is an economic imperative. The next subchapter examines exactly when and why caching transitions from optional to mandatory, and how to calculate the specific volume threshold where that transition occurs for your system.
