# 1.6 — The Tradeoff Register: Making Cost-Quality Decisions Explicit and Auditable

The engineering lead stares at the Slack thread. Three teams, three opinions on whether to downgrade the model for the summarization endpoint. The backend team argues the cheaper model is "good enough" based on eyeballing twenty outputs. The product team insists users will notice the quality drop and sends three cherry-picked examples of bad summaries. The data science team says they cannot evaluate the change because nobody defined what "good enough" means for this endpoint. The thread is forty-seven messages long. No data has been shared. No metrics have been cited. The engineering lead scrolls to the top, reads the first message — posted three days ago — and realizes the entire argument is happening without a framework. Nobody knows what quality the current model is actually delivering. Nobody knows what the cheaper model would deliver. Nobody has written down the criteria for making this decision. So everyone is arguing from intuition, anchored to anecdotes, and the decision will eventually be made by whoever has the most political capital or the most patience for Slack arguments.

This scene plays out in AI teams every week. Cost-quality decisions — which model to use, how much context to include, whether to add a human review step, when to cache versus recompute — are some of the most consequential choices a team makes. They directly affect the product's economics, the user's experience, and the engineering team's workload. And in most organizations, these decisions are made in hallway conversations, Slack threads, or meetings with no written record. The rationale evaporates. The alternatives considered are forgotten. The metrics that informed the decision are never recorded. Six months later, a new engineer asks "why are we using this expensive model for summarization?" and nobody can answer with certainty. The decision gets relitigated. The same arguments replay. The same time is wasted.

The fix is not better meetings or longer Slack threads. The fix is a **Tradeoff Register** — a structured, living document where every significant cost-quality decision is recorded with enough context that anyone can understand what was decided, why, what alternatives were rejected, and when the decision should be revisited.

## What a Tradeoff Register Is

A Tradeoff Register is not a spreadsheet of costs. It is not a project management board. It is not a design document. It is a decision log — a chronological record of every point where the team deliberately chose a position on the cost-quality spectrum. Each entry captures a single decision and the full context surrounding it.

The register answers five questions for every decision. First, what was decided? "We switched the summarization endpoint from GPT-5 to GPT-5-mini." Second, why was it decided? "Quality-adjusted cost analysis showed GPT-5-mini achieves 89% success rate versus GPT-5's 93% success rate, at one-fifth the per-token cost. The 4-point quality drop reduces quality-adjusted cost by 62%." Third, what alternatives were considered? "We also evaluated Gemini 3 Flash, which achieved 84% success rate at one-eighth the cost, and Claude Sonnet 4.5, which achieved 91% success rate at one-third the cost." Fourth, what metrics will tell us if this decision was wrong? "If user-reported summary quality complaints exceed 15 per week, or if the automated eval score drops below 0.85, we will revisit." Fifth, when will we review this decision regardless of signals? "Scheduled review: 90 days from today."

These five questions seem simple, but they impose a discipline that transforms how teams make decisions. The act of writing down the rationale forces the decision-maker to have a rationale. The act of listing alternatives forces the team to consider alternatives. The act of defining review triggers prevents the decision from becoming permanent by default.

## Why Implicit Tradeoffs Are Dangerous

Every AI system embodies dozens of cost-quality tradeoffs. The model you chose is a tradeoff. The context window length is a tradeoff. The retry policy is a tradeoff. The caching strategy is a tradeoff. The human review threshold is a tradeoff. Whether you wrote them down or not, these tradeoffs exist. The question is whether they are explicit — made deliberately with documented rationale — or implicit — made by accident, by default, or by the engineer who happened to be on call that day.

Implicit tradeoffs are dangerous for four reasons.

First, they cannot be evaluated. If nobody recorded the rationale for choosing GPT-5 for the summarization endpoint, nobody can assess whether that rationale still holds when GPT-5-mini improves its quality or when Gemini 3 Flash drops its pricing. The decision becomes permanent not because it is still correct, but because nobody knows what would make it incorrect.

Second, they get relitigated. When a decision has no written record, new team members or new leadership will question it. This is healthy in principle — decisions should be questioned — but the relitigation happens without the original context. The team re-argues from scratch, spending days or weeks reaching the same conclusion (or a different conclusion based on incomplete information). A written record allows anyone to read the rationale, evaluate whether the conditions have changed, and either reaffirm or revise the decision in hours instead of weeks.

Third, implicit tradeoffs create phantom constraints. "We cannot use a cheaper model because last time we tried it did not work." That sentence, spoken confidently by someone who was on the team eight months ago, kills optimization projects. But when you ask what "did not work" means — what model was tested, on what workload, with what prompts, against what quality criteria — nobody can answer. The failure was never documented. It has become a phantom constraint: a belief about a limitation that may or may not still be true, preventing the team from even attempting an optimization that might save tens of thousands of dollars per month.

Fourth, implicit tradeoffs create accountability gaps. When a cost-quality decision is never recorded, nobody owns it. When quality degrades, who is responsible? When costs spike, who made the choice that led to the spike? Without a register, blame becomes political. With a register, accountability is structural. The decision-maker is named. The rationale is documented. The review trigger is set. If the decision turns out to be wrong, the review process catches it. If the decision-maker left the company, the register preserves their reasoning.

## What Goes Into Each Entry

A Tradeoff Register entry needs enough structure to be useful but not so much that filling it out becomes burdensome. The most effective format has eight fields.

**Decision name.** A short, descriptive label. "Switched summarization endpoint to GPT-5-mini." "Increased human review threshold from 0.7 to 0.8." "Disabled retry logic for classification endpoint." The name should be specific enough that someone scanning the register can understand what changed without reading the full entry.

**Date and decision-maker.** When was this decided, and who made the call? This is not about blame. It is about traceability. When conditions change and the decision needs revisiting, the team needs to know who can provide additional context about the original reasoning.

**Context at the time.** What were the conditions when this decision was made? What was the current cost? What was the current quality? What was the traffic volume? What was the business situation? Context decays faster than any other kind of information. Six months later, nobody remembers that the decision was made during a cost crunch when the company was trying to extend runway, or that the quality metric was calculated using an eval suite that has since been replaced. Recording the context preserves the frame of reference.

**The decision itself.** Precisely what changed. Which model, which endpoint, which parameter, which threshold. Ambiguity in this field makes the entry useless. "We optimized the summarization pipeline" is not a decision. "We switched the summarization endpoint from GPT-5 at the 128K context window to GPT-5-mini at the 32K context window, with document chunking for inputs longer than 32K tokens" is a decision.

**Rationale.** Why this option was chosen. This is the most important field. The rationale should reference specific metrics: quality scores, cost calculations, quality-adjusted cost comparisons. It should explain the reasoning, not just the conclusion. "GPT-5-mini was cheaper" is a conclusion. "GPT-5-mini achieved 89% success rate on our eval suite compared to GPT-5's 93%, and the quality-adjusted cost dropped from $0.032 to $0.012 per successful outcome, a 62% reduction. The 4-point quality drop was concentrated in long-document cases that represent 8% of our traffic" is a rationale.

**Alternatives considered.** What other options were evaluated and why they were rejected? This field prevents the team from relitigating without reading the analysis. If someone later suggests "why don't we try Gemini 3 Flash?", the register shows that Gemini 3 Flash was evaluated and rejected because its success rate was 84%, below the team's quality floor of 85%. The suggestion is answered in seconds, not days.

**Review triggers.** What specific, measurable conditions should prompt a re-evaluation of this decision? "If weekly quality complaints exceed 15" is a review trigger. "If it stops working" is not. Review triggers should be connected to metrics the team already tracks, so that the trigger fires automatically when conditions change.

**Scheduled review date.** Regardless of whether any trigger fires, every decision should be reviewed on a set cadence. Ninety days is a good default for most cost-quality decisions in the AI space, because model pricing, model capabilities, and traffic patterns all shift substantially over a quarter. Some high-impact decisions warrant 30-day reviews. Some stable decisions can be reviewed semi-annually. But no decision should be left unreviewed indefinitely.

## How the Register Prevents Re-Litigation

Re-litigation is one of the most expensive hidden costs in AI teams. It is the phenomenon where a decision that was carefully analyzed and made three months ago is questioned, debated, and re-decided — often reaching the same conclusion — because nobody recorded the original analysis.

A mid-size AI team at a document processing company tracked their decision-making patterns over six months. They found that their engineering leads spent an average of four hours per week in discussions about cost-quality tradeoffs. Of those four hours, roughly 60% was spent re-discussing decisions that had already been made. The team was relitigating past decisions not because conditions had changed, but because institutional memory had failed. Engineers who were not in the original meeting did not trust the decision. Managers who rotated onto the team questioned choices they did not understand. Product managers who heard customer complaints assumed the decision was wrong without checking the data.

After implementing a Tradeoff Register, the same team measured again. The time spent on cost-quality discussions dropped from four hours to about ninety minutes per week. The discussions that remained were productive — focused on new decisions or genuine re-evaluations triggered by changed conditions. The unproductive re-litigation had been eliminated because anyone questioning a past decision could simply read the register entry. If the conditions had not changed and the triggers had not fired, the decision stood. If conditions had changed, the register provided the original context needed to make a better decision quickly.

The register does not prevent people from questioning decisions. It makes questioning productive rather than repetitive. A question like "why are we still using the expensive model?" becomes a two-minute exercise: read the register entry, check whether the review triggers have fired, check whether the conditions described in the context have changed. If nothing has changed, the answer is "because the conditions that justified it are still in place." If something has changed, the answer is "you are right, we should re-evaluate, and here is the original analysis to build on."

## The Register as Institutional Memory

AI teams have high turnover. Engineers change teams. Managers rotate. Contractors leave. When key people depart, they take their knowledge of why decisions were made with them. In organizations without a Tradeoff Register, every personnel change triggers a period of uncertainty where the remaining team does not understand the reasoning behind their own system's architecture.

The register solves this by externalizing decision context. A new engineer joining the team can read the last twelve months of cost-quality decisions in an afternoon. They will understand why the summarization endpoint uses GPT-5-mini instead of GPT-5. They will see that Gemini 3 Flash was evaluated and rejected. They will know that the decision is up for review in six weeks. They will have the quality metrics and cost calculations that informed each choice. They arrive with context that would otherwise take months to absorb through conversation and tribal knowledge.

This institutional memory also protects against revisionist history. Without a register, teams often misremember why decisions were made. "We chose GPT-5-mini because it was faster" — no, the register shows the decision was driven by cost, not latency. "We tried human review and it was too expensive" — no, the register shows human review was rejected because it added too much latency, not because of cost. Accurate memory of past decisions prevents the team from learning the wrong lessons.

The institutional memory function becomes critical during audits and compliance reviews. Under the EU AI Act's GPAI Code of Practice and the broader transparency requirements that took effect in 2025, organizations deploying AI systems may need to demonstrate that their cost-quality decisions were made deliberately and with appropriate consideration. A Tradeoff Register provides exactly this documentation. The decision was made by a named person, on a known date, with documented rationale, against defined criteria. This is the kind of evidence that satisfies an auditor.

## How a Team of Twelve Used the Register to Cut Decision Time

A product engineering team at a B2B analytics company had twelve members: four backend engineers, three data scientists, two product managers, a design lead, a QA lead, and an engineering manager. They shipped an AI-powered insights feature that used three different models across five endpoints. Every month, they faced two or three cost-quality decisions: should they downgrade a model, adjust a caching TTL, change a human review threshold, modify a prompt to reduce token count.

Before the register, each of these decisions followed the same pattern. Someone would raise the question in a team meeting. A debate would ensue. The data scientists would be asked to run an analysis. The analysis would take a week because it was not prioritized. The results would be presented at the next team meeting. Another debate. A decision. No written record. The average cycle time for a cost-quality decision was 18 days.

After implementing the register, the pattern changed. Anyone proposing a decision was required to fill out a register entry template before the discussion. The template forced them to gather the data upfront: current quality, current cost, proposed change, expected quality impact, expected cost impact, alternatives considered. The discussion became focused because the data was already on the table. The decision could be made in the same meeting because the analysis was done before the meeting, not after. The average cycle time dropped from 18 days to 4 days.

More significantly, the quality of decisions improved. Before the register, decisions were often made on intuition because the data was not available in the moment. After the register, every decision was backed by specific metrics. The team made fewer decisions they later regretted because the data requirement filtered out impulse decisions. An engineer who wanted to downgrade a model "because it feels like the quality is close enough" had to back that feeling with numbers before the proposal could be discussed. Sometimes the numbers confirmed the intuition. Sometimes they contradicted it. Either way, the decision was better.

The team also discovered an unexpected benefit: the register became a learning resource. New team members read the register to understand not just what the system looked like, but how it got that way. They could trace the evolution of cost-quality decisions over time and see the reasoning behind each step. This accelerated onboarding and gave new members confidence to participate in future decisions because they understood the patterns the team had already established.

## Building Your Register: Practical Implementation

The register does not require special tooling. Some teams use a shared document. Some use a wiki page. Some use a dedicated channel or database. The format matters less than the discipline. What matters is that every significant cost-quality decision gets an entry, and every entry has the eight fields described above.

Start by defining what counts as "significant." Not every parameter change needs a register entry. A useful threshold: any decision that affects cost per request by more than 10%, quality metrics by more than 2 points, or user-visible behavior in any measurable way. Below that threshold, the decision is operational and can be made by the responsible engineer without a register entry. Above it, the decision needs documentation.

Assign ownership of the register to one person. Not to write every entry — the decision-maker writes the entry — but to enforce that entries are written, that they are complete, and that scheduled reviews actually happen. Without an owner, the register will be used enthusiastically for two months and then abandoned. The owner does not need to be senior. A diligent junior engineer or a project manager can fill this role effectively. The critical requirement is that the owner follows up when an entry is incomplete or when a review date passes without a review.

Template the entry format so that writing an entry takes fifteen minutes, not an hour. If the template is too burdensome, people will skip it. If the template is too light, the entries will lack the context needed to be useful later. The eight fields described earlier are a good balance. Pre-populate the template with the current date and the name of the person opening it. Include a dropdown or shortlist of common decision categories: model change, prompt optimization, caching change, human review adjustment, retry policy change. These small affordances reduce friction and increase adoption.

Make the register searchable. When someone asks "have we ever evaluated Gemini for this endpoint?", they should be able to search the register and find the answer in seconds. If your register is a shared document, ensure it has a consistent naming convention and a table of contents. If it is a wiki, use tags. If it is a database, build a simple search interface. The register's value is proportional to how quickly someone can find a past decision.

## Connecting the Register to Your Review Cadence

A register without reviews is a graveyard of outdated decisions. The review process is what keeps the register alive and the decisions current.

Establish a regular review cadence. Once per month, the register owner pulls all entries whose scheduled review date has passed or whose review triggers have fired. These entries go on the agenda for the next team sync. For each entry, the team asks three questions. Have the conditions described in the context changed? Have the review triggers been hit? Given what we know now, would we make the same decision? If the answer to all three is yes, the review date is extended by another cycle. If the answer to any is no, the decision is reopened and a new analysis is conducted.

The review should also incorporate changes in the external environment that might not trigger any internal metric. Model pricing changes every quarter. New models are released every few months. A model that was too expensive six months ago might now be affordable. A model that was not available when the original decision was made might now be the best option. The review is the moment to check whether the landscape has shifted enough to warrant reconsideration.

Keep reviews short. Most reviews should take five minutes or fewer — a quick check that nothing has changed, followed by a date extension. Only decisions where conditions have actually shifted need deeper discussion. If your reviews are consistently taking thirty minutes per entry, your entries are either too vague (requiring re-analysis at review time) or your review cadence is too infrequent (allowing too much to change between reviews).

## The Register as a Strategic Asset

Over time, the Tradeoff Register becomes more than a decision log. It becomes a strategic asset. The accumulated entries reveal patterns in how your team makes decisions, which models and configurations perform well for your workload, and how your cost-quality frontier evolves as the AI landscape shifts.

A team that has maintained a register for a year can answer questions that most teams cannot. "How often do we downgrade models and regret it?" Check the register — count the downgrades, count the reversals. "What is our typical quality-adjusted cost improvement when we optimize a prompt?" Check the register — look at the before and after metrics across prompt optimization entries. "How long does a model selection decision typically stay valid before conditions change enough to warrant reconsideration?" Check the register — measure the time between decision dates and re-evaluation dates.

These patterns inform future decisions. If the register shows that model downgrades are reversed 40% of the time, the team knows to set a higher evidence bar for future downgrades. If prompt optimizations consistently deliver 15% to 25% cost reductions with minimal quality impact, the team knows where to invest engineering effort. If model selections typically stay valid for four to six months, the team knows to set review cadences accordingly rather than defaulting to the arbitrary 90-day cycle.

The register also provides evidence for budget discussions. When the CFO asks why AI costs increased last quarter, the register shows exactly which decisions were made, when, by whom, and based on what data. When the VP of Engineering proposes a cost optimization initiative, the register shows which decisions are ripe for re-evaluation and which have been recently reviewed and confirmed. The conversation moves from "we need to cut costs" to "these five decisions are due for review, and the landscape has shifted enough that we expect to find savings in at least two of them."

The Tradeoff Register turns cost-quality decisions from political arguments into engineering discipline. It transforms implicit choices into explicit commitments. It converts tribal knowledge into institutional memory. And it gives every team member — from the newest engineer to the CFO — a shared, searchable, auditable record of how the system came to be the way it is and whether it should stay that way.

The next subchapter addresses the organizational question that the register surfaces but cannot answer on its own: when a cost-quality decision needs to be made, who actually has the authority to decide where the line falls?
