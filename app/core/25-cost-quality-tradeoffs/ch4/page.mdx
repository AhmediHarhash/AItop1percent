# Chapter 4 — Context and Token Economics

Token usage is the cost driver that hides in plain sight. Teams obsess over model selection, negotiate enterprise pricing tiers, and build elaborate caching layers — then send 12,000-token prompts where 3,000 would have produced the same result. In systems with retrieval, the problem compounds: every extra chunk stuffed into the context window costs tokens on input, slows inference, and often degrades quality at the same time. The relationship between context size and output quality is not linear, and most teams are operating far past the point of diminishing returns without knowing it.

This chapter teaches you to treat tokens as a budget, not an afterthought. You will learn how to prune context down to what the model actually needs, how to balance retrieval cost against generation cost in RAG systems, how to compress prompts without losing the instructions that matter, and how to manage the growing expense of long conversations. You will also confront the stuffing anti-pattern — the widespread practice of cramming every available piece of context into the prompt on the theory that more is always better. It is not. More context costs more money and, past a threshold, produces worse answers.

---

- **4.1** — The Token Budget: Why Context Length Is Your Biggest Variable Cost
- **4.2** — Context Pruning Strategies: Removing What the Model Does Not Need
- **4.3** — Retrieval Cost vs Generation Cost: Balancing the RAG Equation
- **4.4** — Prompt Compression: Achieving the Same Output with Fewer Input Tokens
- **4.5** — Conversation History Management: The Cost of Long Conversations
- **4.6** — Dynamic Context Allocation: Spending Tokens Where They Matter Most
- **4.7** — The Stuffing Anti-Pattern: When More Context Hurts Both Cost and Quality
- **4.8** — Measuring Token Efficiency: Quality Per Token as a Design Metric

---

*Once you have right-sized your models and your context, the single highest-ROI optimization remaining is eliminating work you have already done — through caching, precomputation, and reuse.*
