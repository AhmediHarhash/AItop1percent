# 6.8 — Building an Eval Cost Model: Forecasting Spend as Volume Grows

An eval cost model predicts how much you will spend on evaluation as your traffic grows. Without it, eval costs surprise you at every scale milestone — and surprise costs get cut, leaving quality unmonitored at exactly the moment it matters most. The team processing 100,000 responses per month knows what their eval costs. The team that grows to 1 million responses per month discovers, usually in a budget review meeting, that their eval spend grew tenfold while nobody was watching. The finance team flags it. Engineering scrambles to explain. The eval budget gets slashed not because evaluation is unnecessary but because nobody planned for the increase. A cost model prevents this scenario entirely. It tells you, before you grow, exactly what evaluation will cost at 2x, 5x, and 10x your current volume — and it tells you which optimization levers to pull before each milestone arrives.

## Why Eval Costs Surprise Teams

Most engineering costs scale in ways that teams intuitively understand. Database costs scale with storage and queries. Compute costs scale with processing volume. CDN costs scale with bandwidth. Engineers have mental models for these costs, and procurement teams have historical data to forecast them. Evaluation cost scaling is less intuitive because eval systems have multiple layers that scale at different rates, and the layer that dominates at low volume is rarely the layer that dominates at high volume.

At low volume — say, 50,000 responses per month — the most expensive eval layer is usually human review. A team reviewing 200 responses per week at $40 per hour spends roughly $2,000 per month on human eval, while their LLM-as-judge costs might be $200 per month and their rule-based checks cost essentially nothing. The human layer accounts for 85% of eval spend. If a finance team looks at this cost breakdown and extrapolates linearly, they predict that human review will remain the dominant cost as volume grows.

But that prediction is wrong because the layers scale differently. Human review scales with the absolute number of escalated responses — which grows, but slowly if the escalation rate is tuned properly. LLM-as-judge costs scale with the number of evaluated responses times the cost per evaluation. Rule-based checks scale with total traffic but at near-zero marginal cost. As volume increases tenfold, human review might double (because escalation rates drop as the team tightens criteria), LLM-as-judge costs increase fivefold (because you maintain the sampling rate), and rule-based costs remain negligible. The dominant cost layer shifts from human review at low volume to LLM-as-judge at high volume. A team that optimized only their human review costs — because that was the biggest number — invested in the wrong layer.

This is why a cost model matters. It does not just predict total cost. It predicts which layer becomes the bottleneck at each scale milestone, so you can invest in optimizing the right layer before it becomes expensive.

## The Three Layers and Their Scaling Functions

Every eval system is built from three fundamental layers, each with a distinct cost structure and scaling behavior. Understanding these scaling functions is the foundation of the cost model.

**Rule-based checks** include format validation, length constraints, toxicity keyword filters, personally identifiable information pattern matching, and any deterministic logic that runs on the response text without calling an external model. The cost per check is the compute time to execute the logic — typically measured in microseconds on a standard server. At $0.001 to $0.01 per thousand checks, rule-based evaluation is effectively free at any volume. A system processing 10 million responses per month and running five rule-based checks per response generates 50 million check executions at a cost of $50 to $500 per month. Rule-based checks run on 100% of traffic because the marginal cost is negligible. They never become the cost bottleneck. Their scaling function is linear with near-zero slope.

**LLM-as-judge evaluation** calls a language model to assess response quality on dimensions like accuracy, completeness, relevance, or safety. Each evaluation consumes tokens — typically the original prompt, the response being evaluated, and the judge's scoring prompt, totaling 1,000 to 4,000 tokens per evaluation depending on response length and rubric complexity. At current 2026 pricing, using a mid-tier judge model like GPT-5-mini or Claude Haiku 4.5 costs $0.02 to $0.08 per evaluation. Using a frontier judge like Claude Opus 4.6 or GPT-5 costs $0.10 to $0.40 per evaluation. LLM-as-judge costs scale with the number of responses evaluated — which is the product of total traffic volume and sampling rate. A system processing 1 million responses per month with a 3% sampling rate evaluates 30,000 responses. At $0.05 per evaluation, that is $1,500 per month. At 10 million responses per month with the same 3% sampling rate, the cost is $15,000 per month. The scaling function is linear with total traffic, but the slope depends on sampling rate and judge model cost — both of which are controllable.

**Human review** involves trained evaluators reading model responses and scoring them against a rubric. Reviewer throughput varies with task complexity: simple quality ratings take 2 to 3 minutes per response, detailed rubric scoring takes 5 to 8 minutes, and expert domain review can take 15 to 30 minutes. At $30 to $60 per hour for trained reviewers and $80 to $150 per hour for domain experts, human review costs $1 to $75 per response depending on depth and expertise required. Human review scales with the number of escalated responses — the subset of traffic flagged by automated checks or LLM judges for human attention. The escalation rate determines how many responses reach human reviewers. A system with a 0.5% escalation rate processing 1 million responses per month sends 5,000 responses to human review. At $3 per response for standard review, that is $15,000 per month. The scaling function is linear with total traffic times escalation rate, but the escalation rate typically declines as the automated layers improve their precision — making human review costs scale sublinearly in practice.

## Constructing the Model

The eval cost model is a spreadsheet, a script, or a simple calculator that takes your current traffic volume, projects future volume at multiple growth milestones, and outputs the total eval cost at each milestone broken down by layer. Building it requires six inputs, most of which you already have or can estimate.

**Input one: current monthly traffic volume.** This is the total number of responses your system generates per month. Not requests — responses. A request that triggers three model calls (retrieval, generation, and summarization) may count as one response from the user's perspective but three evaluable outputs from the eval system's perspective. Use the number that matches how your eval system counts evaluable units.

**Input two: projected growth multipliers.** Pick three to five milestones that represent realistic growth scenarios. Common milestones are 2x, 5x, 10x, and 25x current volume. If your product is early-stage and growing fast, you might add a 50x milestone. If you are at scale and growing moderately, 2x and 5x may be sufficient. These multipliers drive the projections.

**Input three: sampling rates by layer.** For each eval layer, define the percentage of traffic that passes through it. Rule-based checks typically run at 100%. LLM-as-judge evaluation runs at 1 to 10%, depending on your confidence requirements and budget. Human review runs on escalated responses, typically 0.1 to 1% of total traffic. These rates are your primary cost control lever — adjusting them is how you manage the tradeoff between eval coverage and eval cost.

**Input four: cost per evaluation by layer.** For rule-based checks, this is the compute cost per check — usually negligible. For LLM-as-judge, this is the token cost per evaluation, which depends on your judge model, the average response length, and the rubric complexity. For human review, this is the per-response cost based on reviewer hourly rate and throughput. Measure these from your actual system if possible. If you are planning eval infrastructure you have not yet built, use the ranges from the previous section as starting estimates and refine once you have actual data.

**Input five: escalation rates between layers.** The percentage of responses that each layer escalates to the next layer. What percentage of LLM-judge evaluations get flagged for human review? What percentage of rule-based check failures get escalated for investigation? These rates determine how much traffic flows through the more expensive layers. Escalation rates are both a cost input and a quality input — lowering them reduces cost but also reduces the number of edge cases that receive expert human attention.

**Input six: cost trends.** LLM inference costs have been declining 30 to 50 percent year over year since 2023, and the trend accelerated in 2025 with the release of cheaper frontier-class models. Your cost model should account for this. A projection based on today's prices overstates future costs. A reasonable assumption is 20 to 40 percent annual cost reduction for LLM-as-judge evaluations, with human review costs remaining flat or increasing slightly with inflation. Build the model with current prices, then run a second projection with discounted prices to see the range.

## Running the Projections

With the six inputs defined, the projection for each growth milestone follows a simple calculation. For each layer, multiply the projected traffic volume by the sampling rate for that layer, then multiply by the cost per evaluation for that layer. Sum across layers. The result is the total eval cost at that volume.

A worked example makes the calculation concrete. Consider a B2B SaaS platform processing 500,000 responses per month with three eval layers. Rule-based checks run on 100% of traffic at $0.005 per thousand responses — effectively $2.50 per month, negligible at any scale. LLM-as-judge evaluation runs on 3% of traffic at $0.04 per evaluation — 15,000 evaluations at $600 per month. Human review handles responses escalated by the LLM judge at a 5% escalation rate, meaning 750 responses per month reviewed at $4 per response — $3,000 per month. Total current eval cost: $3,603 per month, with human review dominating at 83%.

At 2x volume (1 million responses per month): rule-based costs double to $5 — still negligible. LLM-judge costs double to $1,200. Human review, if escalation rates hold constant, doubles to $6,000. Total: $7,205 per month. Human review still dominates at 83%.

At 5x volume (2.5 million responses per month): rule-based costs are $12.50. LLM-judge costs reach $3,000. Human review reaches $15,000. Total: $18,013 per month. The LLM-judge layer is growing but human review still accounts for 83%.

At 10x volume (5 million responses per month): rule-based costs are $25. LLM-judge costs reach $6,000. Human review reaches $30,000 — and at this point, the team likely cannot hire enough reviewers to maintain throughput at the current escalation rate. Total: $36,025 per month. The human review layer has become a bottleneck not just in cost but in operational capacity.

This is where the model's value becomes clear. It does not just predict cost — it identifies the bottleneck before it arrives. At 10x volume, the team needs to either reduce the escalation rate to human review (from 5% to 2%, cutting human review cost from $30,000 to $12,000), increase LLM-judge precision to reduce false escalations, or build internal tooling that increases reviewer throughput. Without the model, the team hits 10x volume, discovers the bottleneck, and makes a hasty decision — usually cutting human review entirely, which degrades eval quality at the exact moment the system is serving the most users.

## The Eval Cost Per Production Response

One of the most useful outputs of the cost model is a single number: the **eval cost per production response**. This is the total monthly eval spend divided by the total monthly response volume. It tells you what fraction of each response's cost goes to quality assurance.

For the worked example above, the current eval cost per response is $3,603 divided by 500,000 — approximately $0.0072, or seven-tenths of a cent per response. If each response costs $0.03 in inference, the eval cost is 24% of the inference cost. That ratio is high but not unusual for a young system with conservative sampling rates and a heavy human review component.

The critical insight is that eval cost per response should decline as volume grows, if you manage sampling rates correctly. Statistical sampling theory tells you that the number of samples needed for a given confidence level does not increase linearly with population size. At 500,000 responses per month, a 3% sample gives you 15,000 data points — more than enough for any statistical analysis. At 5 million responses per month, you do not need 150,000 data points to maintain the same statistical power. A 1% sample gives you 50,000 data points, which provides even higher statistical power than the original 3% sample at lower volume. Reducing the sampling rate from 3% to 1% as volume grows from 500,000 to 5 million responses per month reduces LLM-judge costs from the projected $30,000 to $10,000 — a 67% reduction — while actually improving statistical coverage.

The target eval cost per response for a mature system at scale is 5 to 15% of inference cost. Below 5%, you are likely under-evaluating and missing quality signals. Above 15%, you have optimization opportunities in your eval pipeline — cheaper judge models, lower sampling rates, or more precise escalation criteria that would reduce cost without reducing signal. The ratio gives you a quick health check on your eval economics and a target to optimize toward.

## Identifying the Bottleneck Layer

The cost model reveals which layer will become the bottleneck at each growth milestone. The bottleneck is not just the most expensive layer — it is the layer whose cost growth will force either a budget increase or a quality reduction.

For most systems, the bottleneck progression follows a predictable pattern. At low volume (fewer than 100,000 responses per month), the bottleneck is human review because the fixed costs of hiring and managing reviewers dominate. At medium volume (100,000 to 1 million responses per month), the bottleneck shifts to LLM-as-judge because sampling rate times volume times per-evaluation cost grows fastest. At high volume (greater than 1 million responses per month), the bottleneck can be either LLM-as-judge or human review, depending on escalation rates and whether the team has invested in reducing judge costs through cheaper models or cached evaluations.

Each bottleneck has a standard set of optimization levers. When human review is the bottleneck, the levers are: reduce the escalation rate by improving LLM-judge precision, increase reviewer throughput through better tooling and workflows, replace some human review with more sophisticated automated checks, and limit human review to the highest-value response categories. When LLM-as-judge is the bottleneck, the levers are: reduce the sampling rate to the minimum needed for statistical confidence, use cheaper judge models for routine evaluations and reserve expensive judges for ambiguous cases, cache judge evaluations for semantically similar responses, and batch evaluations to take advantage of volume pricing. When rule-based checks become even slightly expensive — which only happens at extreme scale, typically greater than 100 million responses per month — the levers are: optimize check execution efficiency, parallelize checks, and sunset checks that no longer catch real failures.

The cost model should not just predict costs but recommend which levers to pull at each milestone. A model that says "at 5x volume, eval costs will be $18,000 per month" is useful. A model that says "at 5x volume, eval costs will be $18,000 per month, with human review accounting for $15,000. Reducing escalation rate from 5% to 2% brings total cost to $9,000 with minimal coverage loss because the LLM judge is catching 94% of the issues that human reviewers catch" is actionable. The second version tells the team exactly what to optimize and what the impact will be.

## Incorporating Cost Trends

The eval cost model is a living document, not a one-time calculation. Costs change. LLM inference prices have dropped dramatically and continue to drop. When GPT-4-class models first appeared, inference cost approximately $30 to $60 per million tokens. By late 2025, equivalent capability was available for $0.40 to $2 per million tokens — a 30x to 100x reduction in roughly two years. The trend has not stopped. Specialized evaluation models, smaller distilled judges, and competition among providers continue to drive down the per-evaluation cost.

Your cost model should include a cost trajectory for each layer. For LLM-as-judge evaluations, assume 20 to 40 percent annual cost reduction and run projections at both ends of that range. For human review, assume flat to 5% annual increase. For rule-based checks, assume flat — the cost is already negligible. Running the model with both current prices and projected future prices gives you a range rather than a point estimate, which is more useful for budgeting.

The cost trajectory also affects which optimizations are worth investing in now versus later. If LLM-as-judge costs are dropping 30% per year, an engineering investment to reduce judge costs by 40% has a shorter payoff window than the same investment in reducing human review costs, which are not dropping. Optimize the layer that is both currently expensive and structurally expensive. Let price trends handle the layer that is currently expensive but declining.

A practical example: in early 2025, a team spent $12,000 per month on LLM-as-judge evaluations using GPT-4o at $10 per million input tokens. They considered building a custom fine-tuned judge model to reduce costs by 60%, a project estimated at three months of engineering time. By the time the project would have been completed, GPT-5-mini was available at $0.40 per million input tokens — a 96% cost reduction that required zero engineering effort. The custom judge project would have been obsolete before it launched. The cost model, had they consulted one, would have shown that LLM-as-judge costs were on a steep decline trajectory and the better investment was optimizing the human review layer, which was not declining.

## Planning Optimizations Before the Milestone

The final purpose of the cost model is to trigger optimization work before costs become unsustainable — not after. This is the difference between proactive cost management and reactive budget cuts.

The model should include trigger thresholds: volume milestones at which specific optimizations should begin. A typical set of triggers looks like this. At 2x current volume, review sampling rates and reduce any that provide more statistical power than needed. At 3x current volume, evaluate whether a cheaper LLM judge model can replace the current one without quality loss, using A/B evaluation to validate. At 5x current volume, implement tiered judging — route routine evaluations to a cheap fast judge and reserve the expensive judge for ambiguous cases flagged by the first tier. At 10x current volume, reduce human review to edge cases only and invest in automated escalation criteria that precisely target the responses most likely to benefit from human attention.

Each trigger includes not just the optimization action but the validation step. Reducing sampling rate requires running a comparison between the old and new rates on historical data to confirm that the lower rate would have caught the same regressions. Switching judge models requires running both judges on the same evaluation set and confirming that agreement rate exceeds 90%. Implementing tiered judging requires measuring what the cheap judge misses and confirming that the expensive judge catches it on escalation. None of these validations are expensive, but they are necessary — optimizing your eval pipeline without validating the optimization is the same mistake as optimizing your inference pipeline without eval.

An e-commerce platform built this kind of proactive model in mid-2025 and used it to navigate a period of rapid growth. When they hit their 3x trigger at 1.5 million responses per month, they switched their LLM judge from Claude Sonnet 4.5 to a fine-tuned Llama 4 Scout judge that cost 80% less per evaluation. When they hit their 5x trigger at 2.5 million responses per month, they implemented tiered judging that routed 70% of evaluations to the fine-tuned judge and escalated 30% to Claude Opus 4.5 for ambiguous cases. When they hit their 10x trigger at 5 million responses per month, they reduced their sampling rate from 5% to 1.5% — still 75,000 evaluations per month, more than enough for statistical confidence — and reduced human review to a weekly sample of 100 responses focused exclusively on edge cases. Their eval cost grew from $4,200 per month at 500,000 responses to $11,800 per month at 5 million responses — a 2.8x increase on a 10x volume increase. Their eval cost per response dropped from $0.0084 to $0.0024. Quality coverage remained comprehensive. No regressions went undetected during the growth period.

## The Model as Communication Tool

The eval cost model is not just an engineering planning tool. It is a communication tool that bridges the gap between engineering, finance, and leadership.

When finance asks "why did eval costs increase 40% this quarter?" the cost model provides the answer before the question is asked. Traffic grew 50%. Eval costs grew 40% because sampling rate reductions partially offset the volume increase. The model predicted this increase six months ago. Here is the projection for next quarter.

When leadership asks "can we cut eval costs by 30%?" the cost model shows the tradeoffs. A 30% reduction requires either reducing sampling rate from 3% to 1.5%, which extends regression detection time from two days to four days, or eliminating human review entirely, which removes the final quality check on ambiguous cases, or switching to a cheaper judge model, which requires validation time. Here are the options and their quality implications.

When product asks "we are launching in three new markets, how much additional eval cost should we budget?" the cost model provides the answer. Three new markets means roughly 40% traffic increase, plus additional evaluation for language-specific quality in each market. Here is the projected cost by layer, with and without multilingual eval additions.

The cost model turns eval budgeting from a negotiation into a calculation. It removes the emotion and replaces it with numbers. Finance cannot argue with the math. Leadership cannot claim the cost was unexpected. Product cannot launch without budgeting for eval. The model creates accountability and predictability — which, for an engineering function that is perpetually at risk of being cut, is the most valuable asset you can build.

The evaluation cost layers you have now learned to model — sampling rates, judge costs, human review budgets, escalation thresholds — interact with two other forces that shape every real-time AI decision: latency and inference cost. Managing the three-way tradeoff between these forces is the subject of the next chapter.