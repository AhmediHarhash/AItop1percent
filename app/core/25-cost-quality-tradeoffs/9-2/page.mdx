# 9.2 â€” When Caching Becomes Mandatory: The Volume Threshold That Changes the Math

Below 50,000 requests per day, caching is a nice optimization. It shaves cost, improves latency, and demonstrates engineering discipline, but the system functions without it. Above 200,000 requests per day, caching becomes an architectural requirement. Without it, your API bill alone will consume most of your gross margin. The threshold between optional and mandatory is not a fixed number. It depends on your per-request cost, your revenue per request, and the gap between the two. But every AI system that charges money for its output has this threshold, and crossing it without a caching strategy in place is one of the fastest ways to turn a growing product into a money-losing one.

The reason the threshold exists is arithmetic. Model inference costs scale linearly with volume. Revenue often does not. Freemium models generate zero revenue from free-tier users and capped revenue from paid tiers. Usage-based pricing generates decreasing marginal revenue as customers hit volume discounts. Enterprise contracts generate fixed revenue regardless of how many queries the customer sends. In every pricing model, there is a volume at which the linear growth of inference cost outpaces the sub-linear or stepped growth of revenue. Caching breaks this trajectory by eliminating a fraction of inference calls entirely, replacing a $0.02 model call with a $0.0002 cache lookup.

## The Caching Threshold Calculation

Your **caching threshold** is the daily request volume at which your total inference cost, without caching, exceeds your target gross margin. Calculating it requires three numbers: your average cost per request without caching, your average revenue per request, and your target gross margin percentage.

Start with the per-request economics. If your average model inference cost is $0.025 per request, your infrastructure cost is $0.004 per request, and your safety filtering cost is $0.003 per request, your total per-request cost is $0.032. If your average revenue per request is $0.08, either through direct per-query pricing or by dividing monthly subscription revenue by expected monthly query volume, your gross margin per request is $0.048, or 60 percent. At this margin, the system is healthy.

Now project what happens as volume increases without caching. At 50,000 requests per day, your daily cost is $1,600 and your daily revenue is $4,000. Gross margin is $2,400 per day, which is 60 percent. Comfortable. At 200,000 requests per day, daily cost is $6,400 and daily revenue depends on your pricing structure. If you are on pure usage-based pricing, revenue scales linearly to $16,000 and your margin stays at 60 percent. But almost nobody has pure linear revenue. If you have a freemium model where 60 percent of queries come from free-tier users, your revenue at 200,000 requests is only $6,400 per day because 120,000 of those requests generate zero revenue. Your gross margin has dropped from 60 percent to zero. You are breaking even on the queries you charge for and losing money on every free query.

The caching threshold for this business is somewhere between 50,000 and 200,000 requests per day, the point where the linear cost line crosses the sub-linear revenue line. Below that point, the revenue buffer is large enough that caching is an optimization. Above it, caching is the only thing preventing your unit economics from going negative.

The precise calculation is: threshold equals the request volume at which total daily cost divided by total daily revenue exceeds one minus your minimum acceptable gross margin percentage. For most AI products, this threshold falls between 75,000 and 300,000 requests per day, depending on pricing structure and model cost. The wider the gap between your per-request cost and your per-request revenue at low volume, the higher the threshold. The more your revenue curves sub-linearly as volume grows, the lower the threshold.

## Four Tiers of Caching Investment

Caching is not a single technology. It is a spectrum of investments, each appropriate at different volumes and delivering different returns. Teams that jump straight to sophisticated caching at low volume over-invest. Teams that stick with basic caching at high volume leave enormous savings on the table.

**Tier one: exact-match caching.** This is the simplest and cheapest layer. Store the exact prompt-response pair and return the cached response when the identical prompt appears again. Implementation requires nothing more than a key-value store like Redis or Memcached, with the prompt hash as the key and the response as the value. At most AI systems, exact-match caching delivers a hit rate of 10 to 20 percent because many users ask the same questions in the same words, especially for informational queries, FAQ-type interactions, and structured workflows. The investment is minimal: a few hours of engineering time and a Redis instance that costs $50 to $200 per month depending on the volume of cached entries. Exact-match caching is appropriate for any system above 10,000 requests per day and should be considered table stakes by the time you reach 50,000.

**Tier two: prompt caching at the provider level.** Major API providers, including OpenAI, Anthropic, and Google, now offer built-in prompt caching that stores the intermediate computations, specifically the key-value attention states, for repeated prompt prefixes. This does not cache the full response. It caches the work the model does to process the system prompt and any static context you include before the user's specific query. The savings are dramatic: up to 90 percent reduction in input token costs and 80 percent reduction in latency for the cached prefix portion. The investment is near zero, just restructuring your prompts so that static content comes first and variable content comes last. Any system with a system prompt longer than 1,024 tokens should be using provider-level prompt caching. The savings accumulate invisibly and require no infrastructure on your side.

**Tier three: semantic caching.** This is where the economics shift dramatically. Semantic caching converts each incoming query into a vector embedding and searches a vector store for previously answered queries with high semantic similarity. If a user asks "what are the side effects of ibuprofen" and another user asks "what adverse reactions can ibuprofen cause," exact-match caching treats these as different queries. Semantic caching recognizes them as the same question and returns the cached response. Production data from 2025 consistently shows that roughly 47 percent of queries in a typical AI application are semantic duplicates of previous queries, meaning they ask the same thing in different words. Combined with the 18 percent that are exact duplicates, semantic caching can achieve a total hit rate of approximately 65 percent for applications with repetitive query patterns.

The investment for semantic caching is meaningfully higher than exact-match caching. You need an embedding model to convert queries into vectors, which adds 5 to 20 milliseconds of latency and a small per-query cost. You need a vector database, such as Pinecone, Weaviate, Qdrant, or a self-hosted FAISS index, to store and search the embeddings. You need a similarity threshold that determines how close two queries must be to count as a match, which requires tuning to avoid false positives where semantically different queries return incorrect cached responses. And you need a quality verification mechanism to ensure that cached responses remain accurate over time. The infrastructure cost for a production semantic cache ranges from $500 to $3,000 per month at growth-regime volumes. The engineering investment is two to four weeks for initial implementation and ongoing tuning.

Semantic caching becomes essential when your system crosses roughly 200,000 requests per day, which is the volume at which the savings from a 65 percent hit rate, replacing $0.025 model calls with $0.0003 cache lookups, easily exceeds the infrastructure cost of maintaining the cache.

**Tier four: dedicated caching infrastructure with intelligent invalidation.** At scale-regime volumes, above one million requests per day, your cache is no longer a simple layer. It is a critical production system that requires its own reliability engineering. The cache needs multi-region replication so that users in Europe and Asia get cache hits with the same frequency as users in North America. It needs intelligent invalidation that can identify and flush cached responses when the underlying knowledge changes, without flushing the entire cache and losing all accumulated hit-rate value. It needs monitoring that tracks hit rate, staleness, false-positive rate, and cache population by topic cluster. And it needs capacity planning that projects cache growth, eviction rates, and infrastructure costs over a six-to-twelve-month horizon.

The investment for tier-four caching is $5,000 to $20,000 per month in infrastructure and typically requires a dedicated engineer, either full-time or a significant fraction of someone's time, to maintain and optimize. At one million requests per day with a $0.025 per-request model cost, a 65 percent hit rate saves roughly $16,250 per day, or approximately $487,500 per month. The $20,000 monthly infrastructure cost is less than five percent of the savings it generates.

## The Cache Hit Rate Ceiling and How to Approach It

No cache achieves 100 percent hit rate, and understanding the ceiling for your specific application is essential for realistic cost projections.

The **novelty floor** is the fraction of queries that are genuinely new, meaning no previous query in your cache is semantically close enough to match. For a customer support system that answers questions about a well-defined product catalog, the novelty floor might be as low as 15 percent, meaning 85 percent of queries are answerable from cache. For a creative writing assistant where users generate unique fiction prompts, the novelty floor might be 70 percent or higher, meaning only 30 percent of queries can realistically be cached. For general-purpose assistants like broad chatbot products, the novelty floor typically sits around 35 percent based on production data from multiple systems in 2025.

Your theoretical maximum cache hit rate is one minus your novelty floor. Your practical maximum is lower, because semantic similarity matching is imperfect: some queries that are semantically similar will have different best answers due to context, user history, or time-sensitive information. A realistic practical ceiling is 80 to 90 percent of the theoretical maximum. For a customer support system with a 15 percent novelty floor, the theoretical ceiling is 85 percent and the practical ceiling is roughly 70 to 75 percent.

The return on caching investment follows a diminishing curve. Moving from zero caching to exact-match caching captures 10 to 20 percent of queries for minimal investment. Adding semantic caching captures an additional 30 to 45 percent of queries for moderate investment. Moving from basic semantic caching to highly tuned semantic caching with topic-specific similarity thresholds captures another 5 to 10 percent for significant investment. Each tier delivers less incremental value than the one before it. The art is knowing when you have captured enough cache value to justify stopping further investment and redirecting engineering effort elsewhere.

## The Staleness Problem: When Caching Hurts Quality

Caching is not free in quality terms. Every cached response is, by definition, an answer that was generated at some point in the past and is being served without the model seeing the current query. This creates the **staleness problem**: cached responses that were correct when generated but are no longer correct because the underlying information has changed.

For some applications, staleness is irrelevant. A cached response to "what is the boiling point of water at sea level" is correct indefinitely. For others, staleness is a critical quality risk. A cached response to "what are the current COVID-19 guidelines" from three months ago might contain outdated medical advice. A cached response to "what is the stock price of Apple" from yesterday is wrong today.

The staleness problem scales with cache size. A small cache with a few thousand entries is easy to audit and refresh. A large cache with millions of entries at scale-regime volumes cannot be manually audited. Stale entries accumulate silently, and because they are served as cache hits, they never pass through the model, which means the model has no opportunity to generate an updated answer.

The mitigation is a **time-to-live policy** that varies by content type. Factual responses about stable information, such as product specifications, company policies, and scientific facts, can have long TTLs of 24 to 72 hours. Responses involving time-sensitive information, such as pricing, availability, and current events, need short TTLs of one to four hours. Responses involving rapidly changing information, such as stock prices, weather, and live scores, should not be cached at all or should have TTLs measured in minutes.

Beyond TTL, invest in **event-driven invalidation** where possible. When your knowledge base updates, such as a new product being added or a policy changing, trigger a targeted cache invalidation that flushes only the entries related to the changed information. This is architecturally harder than TTL-based expiration but preserves cache value for the vast majority of entries that are still valid. A customer support system that flushes its entire cache every time any knowledge base article changes loses days of accumulated cache value. One that flushes only the entries related to the changed article loses minutes of value on a small subset of entries.

## The Quality Monitoring Requirement

Caching introduces a category of quality failure that does not exist without it: serving an incorrect cached response to a query that the model would have answered correctly. This failure is invisible to standard quality monitoring that only evaluates model-generated responses, because cached responses never reach the model.

You must monitor cache-served responses with the same rigor you apply to model-generated responses. Sample a fraction of cache hits, typically 1 to 3 percent, and evaluate them using your standard quality pipeline. Compare the quality scores of cache-served responses to model-generated responses over time. If the quality of cached responses is lower than model-generated responses, your cache is degrading quality. Common causes include stale entries, overly aggressive similarity thresholds that match semantically similar but answer-different queries, and cached responses that were generated by a previous model version and do not reflect the quality improvements of the current model.

A healthcare information system that implemented semantic caching in late 2025 discovered after six weeks that cached responses scored 4 percentage points lower on their clinical accuracy eval than fresh model responses. Investigation revealed two causes. First, 12 percent of cached responses had been generated by an earlier model version with a known weakness in medication dosage guidance, and the team had not flushed the cache when they upgraded models. Second, their semantic similarity threshold of 0.92 was too permissive for medical queries, matching questions about similar but medically distinct conditions and serving cached responses that were accurate for the original question but inaccurate for the matched question. They tightened the threshold to 0.96, implemented automatic cache flushes on model upgrades, and added a domain-specific validation check on cached responses. The quality gap closed within two weeks.

## A Cost Trajectory: Before and After the Threshold

Follow one team's cost trajectory across the caching threshold to see how the economics transform in practice.

A B2B document intelligence startup processed customer contracts using an AI summarization and extraction pipeline. At launch, they served 15,000 documents per day at an average model inference cost of $0.04 per document, which included both summarization and entity extraction passes. Monthly inference cost: $18,000. Monthly revenue: $45,000 from thirty enterprise customers. Gross margin: 60 percent. Healthy.

At this volume, they had no caching. Every document was processed fresh. The engineering team discussed caching but correctly concluded that the investment was not justified: most documents were unique customer contracts with low semantic overlap, and the expected cache hit rate was under 10 percent. The monthly savings from caching, roughly $1,800, did not justify two weeks of engineering effort.

Twelve months later, the product had grown to 120,000 documents per day. Revenue had grown to $120,000 per month from 90 customers. But monthly inference cost had grown to $144,000, because every document was still processed fresh. Gross margin had fallen to negative 20 percent. The business was losing $24,000 per month on inference alone, not counting infrastructure and team costs.

The crisis forced a rapid caching investment. The team discovered that while full documents were mostly unique, the extraction queries within each document, the prompts that asked the model to identify parties, dates, obligations, and termination clauses, were highly repetitive. The same extraction prompt templates appeared across 85 percent of documents. By caching the extraction responses and only running the summarization pass fresh, they achieved a blended cache hit rate of 52 percent on total inference calls. Monthly inference cost dropped from $144,000 to $69,000. Gross margin recovered to 42 percent.

The team then invested in semantic caching for the summarization pass, recognizing that many contracts contained similar boilerplate language and similar clause structures. Semantic caching on summarization added another 18 percent hit rate on the remaining uncached calls. Total blended cache hit rate reached 61 percent. Monthly inference cost dropped to $56,000. Gross margin recovered to 53 percent.

The total caching infrastructure cost was $2,800 per month for the Redis cluster and vector database, plus a one-time engineering investment of five weeks. The system saved $88,000 per month. Without caching, the business would have been unsustainable by the time it reached 200,000 documents per day. With caching, it was profitable at every volume level above the threshold.

## Planning for the Threshold Before You Hit It

The worst time to build caching is when you are already past the threshold. At that point, you are losing money on every request while simultaneously trying to build the infrastructure that will stop the bleeding. The engineering is rushed, the quality verification is skipped, and the staleness mitigation is deferred, all of which create technical debt that compounds for months.

The right time to plan for caching is when you are at 50 percent of your estimated threshold volume. At that point, you have enough traffic to measure query repetition patterns, enough data to estimate achievable hit rates, and enough runway before the economics turn negative that you can build the caching layer carefully rather than desperately.

Start by measuring your **query repetition profile**: what percentage of today's queries are exact duplicates of queries from the past 24 hours? What percentage are semantic duplicates? This measurement tells you the ceiling of what caching can achieve for your specific application. If the answer is "8 percent exact duplicates and 35 percent semantic duplicates," your maximum practical cache hit rate is roughly 40 percent. If the answer is "22 percent exact and 55 percent semantic," your ceiling is roughly 70 percent. The higher the repetition, the more urgently you need caching and the more value it will deliver.

Then build the simplest possible caching layer, exact-match caching with provider-level prompt caching, and measure the real hit rate and cost savings over two weeks. Use those measurements to project the savings at your threshold volume, and use that projection to justify the investment in semantic caching before the threshold arrives.

The teams that cross the caching threshold gracefully are the ones that started building six months before they hit it. The teams that cross it painfully are the ones that waited until the monthly bill triggered an emergency meeting. The difference is not engineering skill. It is planning discipline.

Caching reduces the cost of queries you have seen before. But the biggest cost savings at scale come not from caching but from routing, sending different queries to different models based on their complexity. The next subchapter examines when multi-model routing pays for the complexity it introduces, and how to calculate the specific volume at which routing infrastructure earns back its investment.
