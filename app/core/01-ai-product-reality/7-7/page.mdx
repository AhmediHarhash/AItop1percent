# 7.7 â€” The Minimum Viable AI Product

In early 2025, a customer support platform launched its AI assistant to 15,000 daily active users. The team had spent seven months building a system that could handle every type of customer inquiry, from simple password resets to complex billing disputes to technical troubleshooting across three product lines. They trained it on 10,000 historical support tickets. They built personalization features that adapted responses to user tenure and subscription tier. They integrated it with six backend systems so it could pull real-time account data, initiate refunds, and trigger provisioning workflows.

Launch day crashed in four hours. The AI handled simple queries well enough, but on anything moderately complex, it hallucinated. It told one user their subscription was canceled when it wasn't. It walked another user through a technical fix for the wrong product. It generated refund confirmations that never actually processed. Support ticket volume spiked 40% because users had to escalate AI mistakes on top of their original problems. The company pulled the feature within a week, issued a public apology, and spent the next three months rebuilding trust manually. The root cause wasn't technical incompetence. It was scope misjudgment. They tried to build everything at once instead of the minimum that could actually work.

An AI MVP is not a traditional software MVP. In traditional software, minimum viable means the smallest feature set that delivers value. Ship login and a dashboard, defer reporting until later. Ship read-only access, add editing in version two. The features are discrete, and you can carve them cleanly. AI doesn't work that way. An AI MVP might have just one capability, one task, one use case. But the infrastructure beneath that one task needs to be more robust than you'd expect from a traditional MVP, because the AI's failure modes are less predictable and the cost of user-facing failure is higher. The art of the AI MVP is knowing where to cut scope aggressively and where to invest deeply even on day one.

## What Minimum Viable Means for AI

In traditional MVPs, you ship incomplete features and users understand they're getting version one. They tolerate rough edges because the value is clear and the limitations are explicit. AI MVPs don't get that same tolerance. When an AI makes a mistake, users don't think "this is version one, they'll fix it." They think "this AI is unreliable," and that judgment sticks. Trust, once lost, is nearly impossible to recover. You can't launch an AI assistant that answers questions correctly 60% of the time and expect users to give you a second chance. They'll just stop using it.

This creates the quality floor problem. Your AI MVP must cross a quality threshold high enough that users trust it, even if the scope is narrow. That threshold varies by task type and user expectation, but it's never low. A customer support AI that handles the top ten most common questions with 90% accuracy is viable. A customer support AI that handles a hundred question types with 60% accuracy is not. The first earns trust. The second destroys it.

Scope is how you solve this. You narrow the problem space until you can deliver high quality on a small surface area. You don't ship every capability. You ship one capability done well. You defer the rest until you've proven the foundation works. This is counterintuitive for teams coming from traditional product development, where the pressure is to ship more features, not fewer. In AI, shipping more features before you've nailed quality is the fastest way to fail.

The minimum viable AI product is the narrowest scope at which the AI delivers clear, reliable value to users. Everything else is deferred, not because you're cutting corners, but because you're being disciplined about where quality must be high from day one.

## What an AI MVP Must Include

Every AI MVP, regardless of domain, must have four components. These are not optional. These are the difference between a viable product and a prototype.

First, the core AI capability. One task, done well enough to be useful. Not five tasks done poorly. Not ten tasks where three work and seven are aspirational. One task where quality is high and consistent. If your product is a legal contract review AI, the MVP reviews one contract type for one class of risk. Not all contract types, not all risks. If your product is a customer support AI, the MVP answers the ten most common questions with accuracy high enough that users trust the answers. Everything else gets escalated to humans. The core capability is deliberately narrow. You expand later, after proving this narrow scope works in production.

Second, a quality bar that is measurable and achieved before launch. Your go/no-go evaluation, the one you built in the previous subchapter, proves the AI meets your quality threshold on the core task. You can articulate the quality level in concrete terms: "92% of responses to the top ten questions are rated acceptable or better by our rubric." You've run this evaluation on a representative dataset, the results are documented, and the team agrees the threshold is met. This is not a gut feeling. This is evidence.

Third, a fallback path for when the AI can't handle a request. An MVP without fallbacks is a demo, not a product. The fallback can be simple. It can be "I'm not sure about this, let me connect you with a human agent." It can be "This question is outside my current scope, here's a link to our full documentation." It can be silent routing, where the AI detects uncertainty and escalates without telling the user it's uncertain. The specific mechanism depends on your product and user expectations. What matters is that the fallback exists, that it preserves user trust when the AI fails, and that you've tested it. Users tolerate AI limitations if the product handles those limitations gracefully. They don't tolerate silent failures or confidently wrong answers.

Fourth, basic monitoring that tells you when quality degrades in production. At MVP stage, this doesn't need to be sophisticated. You don't need automated LLM-as-judge pipelines, real-time dashboards, or anomaly detection systems. You need something. The simplest version: log all interactions, sample twenty per day for human review, track user feedback via thumbs up or thumbs down, and watch for patterns. If quality in production diverges from quality in your eval, you need to know within days, not weeks. Without monitoring, you're flying blind. You won't catch regressions, you won't see emerging failure modes, and you won't know if a model provider change silently degraded your product.

These four components are the foundation. Everything else is scope negotiation.

## What an AI MVP Should Not Include

The instinct when building an MVP is to add features that make the product feel complete. Resist this. The AI MVP is not about completeness. It's about proving that the core capability works and users value it. Strip everything else.

Do not launch with every possible use case. Launch with the narrow use case where quality is highest and user value is clearest. If your customer support AI can handle ten question types with 92% accuracy and twenty question types with 78% accuracy, launch with the ten. Defer the other ten until you've proven the first ten in production and built the iteration discipline to improve them. Users would rather have a narrow tool that works than a broad tool that's unreliable.

Do not aim for perfect accuracy. Your MVP quality bar should be "useful," not "flawless." Users tolerate imperfection when the product is genuinely helpful and honest about its limitations. A summarization tool that misses minor details but captures the key points is useful. A summarization tool that hallucinates facts or misrepresents tone is not. The gap between useful and flawless is large. Your MVP needs to cross useful. Flawless comes later, if ever.

Do not build sophisticated personalization. Personalization is tempting because it feels like a differentiator, but it multiplies the surface area you need to evaluate. Personalization means quality varies by user segment, which means you need separate eval sets per segment, separate quality thresholds, and separate monitoring. Save personalization for version two. Version one needs to work well for everyone, not perfectly for a few.

Do not attempt complex multi-step workflows. If your vision includes a ten-step agent workflow where the AI researches a topic, drafts a proposal, sends it for feedback, incorporates edits, and finalizes a document, your MVP is step one done well, with the rest handled by humans or existing tools. Multi-step workflows multiply failure modes. Each step introduces uncertainty, and the errors compound. A workflow with 90% accuracy per step has 35% chance of completing all ten steps without error. That's not viable. Start with one step. Prove it works. Add the next step when you've built the discipline to maintain quality as complexity grows.

Do not over-engineer infrastructure. Your MVP monitoring doesn't need Grafana dashboards, PagerDuty alerts, and real-time anomaly detection. A daily spreadsheet review works. Your MVP evaluation doesn't need automated LLM-as-judge pipelines. Manual review works. Your MVP deployment doesn't need blue-green releases and automated rollback. A staged rollout with manual verification works. Build infrastructure when scale demands it, not before. Premature infrastructure is a distraction from the actual question the MVP exists to answer: do users want this, and does it work well enough to be useful?

## The Quality Floor and the Trust Cliff

The hardest lesson for teams new to AI products is understanding the trust cliff. In traditional software, users tolerate bugs. They complain, they report them, they wait for a fix, and they keep using the product. The trust degradation is gradual. AI is different. When an AI makes a serious mistake, users don't wait for a fix. They stop trusting it, and they don't come back. Trust is binary. You have it or you don't.

This is why the quality floor is non-negotiable. If your AI MVP launches below the quality threshold where users trust it, you don't get iterative feedback and gradual improvement. You get abandonment. The customer support AI that gave one user wrong information about their subscription didn't lose that one user. It lost every user who heard about the mistake. Word spreads. The error becomes the story. The team spent seven months building a broad system and four hours destroying trust.

The quality floor is task-dependent. A creative writing assistant can have a lower floor than a medical triage tool. A sentiment analysis feature can have a lower floor than a legal contract analyzer. But every task has a floor, and launching below it is not viable. Your go/no-go evaluation exists to tell you whether you're above the floor. If you're not, you defer launch. You narrow scope further, you improve quality, or you reconsider whether the task is viable at all given current AI capabilities.

This is uncomfortable for teams used to shipping fast and iterating. The startup mantra is "launch quickly, learn, and improve." That works for traditional products. It doesn't work for AI products where trust is fragile and first impressions are permanent. You still launch quickly, but you launch with discipline. You launch when quality is proven, not when the calendar says it's time.

## How to Scope Your AI MVP

Start with the full vision of what you want the AI to do. Write down every use case, every task type, every capability. Now rank them by two dimensions: user value and your confidence in achieving quality. User value is how much users care about this capability. Confidence is how certain you are that you can deliver high quality on this capability given current AI limitations, your eval results, and your understanding of the problem space.

The MVP is the intersection of high user value and high confidence. If users desperately need contract review but your eval shows 70% accuracy, that's not the MVP. If you have 95% accuracy on a task users don't care about, that's not the MVP either. You need both. Find the smallest capability where users will notice the value and you can prove the quality.

For the customer support AI, the team's mistake was trying to launch everything. Password resets, billing disputes, technical troubleshooting, account changes, feature requests. When they scoped the MVP properly after the failed launch, they focused on the top ten questions that represented 60% of ticket volume and where accuracy exceeded 90%. That was the MVP. Everything else was deferred. Version two added ten more questions. Version three added another twenty. By version five, they covered 95% of ticket volume. But they didn't start there. They started narrow and expanded with discipline.

For a legal contract review tool, the MVP might be identifying missing clauses in one contract type. Not all contract types, not all clause types, not all risks. One contract type, one set of clauses, proven accuracy. For a code review assistant, the MVP might be flagging security issues in one language on one class of vulnerability. Not all languages, not all vulnerabilities, not full code review. The minimum that delivers clear value and crosses the quality floor.

Once you've identified the core capability, you define the boundaries explicitly. You document what the MVP does and what it doesn't do. This clarity prevents scope creep during development and sets user expectations at launch. The customer support AI says "I can help with account login issues, password resets, and billing questions. For other topics, I'll connect you with a specialist." The contract review tool says "This analysis covers non-disclosure agreements and identifies missing confidentiality, term, and liability clauses. For other contract types, please consult legal counsel." The boundaries are not apologies. They're precision.

## The MVP Launch Checklist

Before launching your AI MVP, verify eight things. If all eight are checked, you launch. If they're not, you fix what's missing. You don't add features. You fix the foundations.

One, the core task passes your quality evaluation at the go threshold. You've run your eval, the results are documented, and quality is above the floor. Two, the fallback path works and is tested. You've manually triggered the fallback, confirmed it routes correctly, and verified the user experience is acceptable. Three, monitoring is in place, even if simple. You know how you'll track quality in production, who's responsible for the daily review, and what you'll do if quality degrades. Four, the feedback mechanism exists. Users have a way to tell you when the AI is wrong, whether that's a thumbs down button, an escalation path, or a feedback form.

Five, safety guardrails are tested, especially for user-facing products. You've tested adversarial inputs, you've confirmed the AI doesn't generate harmful content, and you've verified that your input validation catches malicious requests. Six, cost per query is within your budget. You've measured token counts on representative inputs, you've calculated cost per interaction, and you've confirmed the economics work at your expected scale. Seven, latency is within your target for the core task. You've measured P50 and P95 latency, you've tested under load, and you've confirmed the user experience is acceptable. Eight, your team knows who handles escalations and how. There's a documented process, the responsible people have been trained, and you've tested the escalation workflow.

These eight items are the minimum. They're not aspirational. They're the line between a product and a prototype. If you can't check all eight, you're not ready to launch.

## The MVP Exists to Learn

The purpose of the MVP is not to impress users with breadth. It's not to match competitors feature-for-feature. It's not to fulfill the original product vision on day one. The purpose of the MVP is to learn. You're learning whether users actually want this capability. You're learning whether the AI performs well on real production traffic or whether your eval set missed important failure modes. You're learning what users try to do that you didn't design for. You're learning which quality dimensions matter most to users and which you overweighted in your rubric.

Every answer to these questions makes version two better. The team that ships a narrow MVP quickly, learns from production data, and iterates with discipline will outperform the team that spends months building a broad system that collapses under its own complexity. The discipline is in resisting the urge to build everything at once. The leverage is in the iteration cycle that starts the moment you launch.

The AI MVP is not the product you want to build. It's the product you need to build first to learn how to build the product you want. Scope it ruthlessly. Launch it with discipline. Learn from it systematically. That's how you avoid the trust cliff and build the foundation for a product that compounds in quality over time.

Your first production deployment is where theory meets reality, and reality has a way of surfacing every assumption you didn't test.
