# Chapter 7.7 — The Minimum Viable AI Product

An MVP in traditional software means shipping the smallest feature set that provides value. An MVP in AI is different — because in AI, the feature set might be one feature, but the infrastructure beneath it needs to be more robust than you'd expect.

Shipping too little means the AI isn't useful. Shipping too much means you wasted months building capabilities nobody wanted. The art is finding the right minimum.

---

### What an AI MVP Must Include

**The core AI capability.** One task, done well enough to be useful. Not five tasks done poorly. If your product is a customer support AI, the MVP answers the top 10 most common questions accurately. Not all questions — the top 10. Everything else gets escalated to humans.

**A quality bar.** Your go/no-go evaluation (Chapter 7.6) proves the AI meets your quality threshold on the core task. You can articulate the quality level: "92% of responses to top-10 questions are rated acceptable or better."

**A fallback path.** What happens when the AI can't handle a request? An MVP without fallbacks is a demo, not a product. The simplest fallback: "I'm not sure about this — let me connect you with a human agent." The fallback preserves user trust when the AI fails.

**Basic monitoring.** You need to know if quality degrades in production. At MVP stage, this can be simple: log all interactions, sample 20 per day for human review, track user feedback. You don't need a sophisticated monitoring pipeline — but you need something.

**A feedback mechanism.** Users need a way to tell you when the AI is wrong. Thumbs up/down, a "this was wrong" button, or an escalation path that captures the failure. Without feedback, you're flying blind after launch.

---

### What an AI MVP Should Not Include

**Every possible use case.** Launch with the narrow use case where quality is highest. Expand later based on user demand and quality readiness.

**Perfect accuracy.** Your MVP quality bar should be "useful," not "flawless." Users tolerate imperfection when the product is genuinely helpful and honest about its limitations.

**Sophisticated personalization.** Save personalization for V2. V1 needs to work well for everyone, not perfectly for a few.

**Complex multi-step workflows.** If your vision includes a 10-step agent workflow, your MVP might be step 1 done well, with the rest handled by humans or existing tools.

**Over-engineered infrastructure.** Your MVP monitoring doesn't need Grafana dashboards and PagerDuty alerts. A daily spreadsheet review works. Your MVP evaluation doesn't need automated LLM-as-judge pipelines. Manual review works. Build infrastructure when scale demands it.

---

### The MVP Launch Checklist

Before launching your AI MVP, verify:

1. Core task passes your quality evaluation at the go threshold
2. Fallback path works and is tested
3. Monitoring is in place (even if simple)
4. Feedback mechanism exists
5. Safety guardrails are tested (especially for user-facing products)
6. Cost per query is within your budget
7. Latency is within your target for the core task
8. Your team knows who handles escalations and how

If all eight are checked, launch. If they're not, fix what's missing. Don't add features — fix the foundations.

---

### The MVP Mindset

The MVP exists to learn, not to impress. You're learning: Do users actually want this? Does the AI perform well on real production traffic? What failure modes appear that your eval set didn't cover? What do users try to do that you didn't design for?

Every answer to these questions makes V2 better. Ship the MVP fast, learn fast, iterate fast. That's how AI products win.

---

*Next: your first production deployment — the checklist for going live safely.*
