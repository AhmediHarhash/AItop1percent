# 6.3 — Foundation Model Dependency and Vendor Lock-In

In September 2024, an education technology company built their entire product around a specific fine-tuned version of GPT-3.5. They had spent four months optimizing prompts, building evaluation datasets, and training their customer success team on the model's behavior. Their product generated personalized learning content for students based on curriculum standards and individual performance data. It worked well. Customers were happy. Then OpenAI announced that the GPT-3.5 model series would be deprecated in six months. The company had two options: migrate to GPT-4 at three times the cost per request, which would destroy their unit economics, or migrate to a different provider and rebuild their entire prompt architecture. They chose the latter. The migration took eleven weeks, required re-evaluating all their test cases, and introduced regressions they did not catch until customers complained. By the time they stabilized on the new provider, they had burned through their product development buffer for the quarter and delayed two planned features. The migration worked, but it was a near-death experience. Their entire roadmap had been derailed by a decision they had no control over.

This is the reality of building on foundation model APIs. You are building your business on infrastructure owned, controlled, and operated by someone else. They can change the pricing. They can deprecate your model. They can modify behavior without notice. They can experience outages that take your product down. And you have zero say in any of it. The question is not whether to depend on foundation models — most products must. The question is how to manage that dependency so it does not become a single point of failure.

## The Dependency Risks You Must Plan For

The first risk is pricing changes. Foundation model providers have changed pricing multiple times since 2023. Usually, prices decrease, which benefits customers. But not every pricing change is a decrease, and not every model sees the same pricing trajectory. If your unit economics depend on a specific cost per token, an unexpected pricing increase can break your business model overnight. You will not receive advance warning. Providers announce pricing changes and implement them within days or weeks. If your margins are thin and your pricing is locked into annual customer contracts, a provider price increase can make every transaction unprofitable.

The second risk is model deprecation. Every model you build on today will eventually be replaced. GPT-3.5 is already on the deprecation path. Models have lifecycles. When a provider deprecates a model, they give you a migration deadline, usually three to six months. That sounds like enough time, but migration is not a simple find-and-replace operation. The replacement model will not behave identically to the deprecated model. Prompts that worked perfectly on the old model may produce different outputs on the new one. Evaluation benchmarks will shift. Edge cases you had solved will resurface. Migration is a product development project, not a configuration change.

The third risk is behavior changes without deprecation. Even when models are not deprecated, their behavior evolves. Providers update models to improve performance, fix bugs, or address misuse patterns. These updates are usually not announced in advance. You discover them in production when outputs change. A model that reliably followed a specific output format might start deviating after an update. A model that handled a particular edge case gracefully might start failing. Continuous evaluation is the only way to detect these shifts before they impact users at scale.

The fourth risk is rate limiting and throttling. When demand spikes, providers throttle API requests to manage load. If your product depends on real-time responses and the provider throttles you during your peak usage period, your users experience degraded performance that you cannot fix. You can request higher rate limits, but approval is not instant. During high-demand periods like major product launches or global events, even customers with approved higher limits can experience throttling. Your product's reliability is constrained by the provider's capacity management.

The fifth risk is outages. Every major foundation model provider has experienced outages in the past eighteen months. OpenAI had multiple incidents where ChatGPT and API access were unavailable for hours. Anthropic had service disruptions. Google had Gemini outages. When the provider goes down, your product goes down unless you have architected for resilience. For products that are user-facing and real-time, even a two-hour outage can result in user churn, support ticket floods, and reputational damage.

The sixth risk is policy changes. Providers update their acceptable use policies as they learn about misuse patterns and regulatory requirements. A use case that was permitted under yesterday's policy might be restricted under tomorrow's. If your product operates in a domain that is adjacent to prohibited use cases — for example, medical diagnosis, financial advice, or content moderation — a policy tightening can force you to shut down features or exit markets. Policy changes are unilateral. You do not negotiate them.

The seventh risk is data handling changes. Providers have updated their data retention, training, and privacy policies multiple times. If your product handles sensitive data and you depend on specific contractual guarantees from your model provider about how that data is used, policy changes can create compliance gaps. Enterprise customers often require explicit data handling agreements. If your provider changes their terms and you cannot meet your customer's requirements, you are in breach of contract.

## How to Architect for Provider Portability

The foundational principle is to abstract your model calls behind an interface that is provider-agnostic. Do not scatter OpenAI API calls throughout your codebase. Build an abstraction layer that defines the inputs and outputs your application expects, and implement provider-specific adapters behind that interface. This allows you to swap providers by changing configuration, not by rewriting application logic.

This is not theoretical architecture astronautics. Teams that built provider abstraction layers survived the OpenAI outage of November 2024 by routing traffic to Anthropic within minutes. Teams that had hard-coded dependencies on OpenAI's API went down and stayed down until the provider recovered. The difference was not technical sophistication. It was architectural foresight.

The abstraction layer does not need to be complex. At minimum, it should decouple three things: the prompt construction logic, the provider API interaction, and the response parsing logic. Prompt construction should produce a provider-neutral representation of what you want the model to do. The provider adapter translates that representation into the specific API format the provider expects. Response parsing converts the provider's response back into a standard structure your application consumes. This three-layer design makes provider switching a matter of swapping the adapter, not rewriting the application.

The second principle is to maintain evaluation parity across providers. Your evaluation suite should not assume a specific provider. It should test the outcomes you care about — accuracy, latency, output format compliance, edge case handling — regardless of which model produced them. When you need to switch providers, you run your evaluation suite against the new provider and compare results. This tells you immediately whether the new provider meets your quality bar and where regressions exist.

The third principle is to design prompts and system architecture with portability in mind. Provider-specific features are lock-in vectors. If you build your entire product around OpenAI's function calling format or Anthropic's extended context window, migrating to a provider without those features becomes a product redesign, not a configuration change. Use features that are broadly available across providers, or abstract provider-specific features so they degrade gracefully when unavailable.

The fourth principle is to test your fallback paths regularly. Having a fallback provider is worthless if you have never actually routed traffic to them. Production is not the time to discover that your fallback provider's rate limits are too low, or that their output format differs in subtle ways your parsing logic does not handle, or that their latency is unacceptable for your use case. Test your fallback paths in staging. Route a small percentage of production traffic to your fallback provider continuously so you have real-world confidence that it works.

## The Multi-Provider Strategy

Running on multiple providers simultaneously is more expensive than committing to one, but it reduces risk. A multi-provider strategy means you route requests to different providers based on availability, cost, latency, or capability. The simplest version is active-passive: you have a primary provider and a fallback that activates during outages. The more sophisticated version is active-active: you route traffic to multiple providers based on real-time conditions.

Active-passive configurations are the minimum viable resilience strategy. You have a primary provider that handles all traffic under normal conditions. You monitor the primary provider's availability and error rates. If the primary provider becomes unavailable or error rates exceed a threshold, you automatically route traffic to the fallback provider. This prevents total outages but does not optimize for cost or performance. It optimizes for uptime.

Active-active configurations offer more flexibility but more complexity. You might route high-volume, latency-insensitive requests to a cheaper open-source model running on your own infrastructure, while routing complex, high-value requests to a frontier proprietary model. You might A/B test providers to see which produces better outcomes for specific task types. You might load-balance across providers to stay under rate limits. Active-active requires monitoring and orchestration infrastructure, but it gives you control over cost, performance, and risk.

The trade-off is operational complexity. Every additional provider is another integration to maintain, another set of API changes to track, and another source of potential failure. Multi-provider strategies make sense when the risk of single-provider dependency exceeds the cost of managing multiple providers. For most products, that threshold is reached when revenue exceeds the point where a day-long outage would cause material financial or reputational damage.

## Negotiating Contractual Protections

For products with significant scale or enterprise SLAs, API terms of service are not sufficient. You need a contract. Enterprise agreements with foundation model providers can include protections that the standard API terms do not offer.

Deprecation notice periods can be extended. Standard deprecation timelines are three to six months. Negotiated agreements can extend that to twelve months, giving you more time to migrate. This matters when your product is complex and migration requires re-evaluation, retraining customer success teams, and updating documentation.

Pricing guarantees can be negotiated for a fixed term. A contract might lock in per-token pricing for twelve months, protecting you from mid-contract price increases. This allows you to commit to annual customer contracts without worrying that a provider price increase will make those contracts unprofitable.

SLA commitments with financial penalties give you recourse when the provider fails to meet availability targets. Standard API access comes with no SLA. Enterprise agreements can include uptime guarantees and service credits when those guarantees are breached. This does not prevent outages, but it provides financial compensation when they occur.

Data handling guarantees are critical for regulated industries. You need contractual language that specifies how your data is stored, whether it is used for training, how long it is retained, and what happens to it when you terminate the agreement. Standard API terms are often vague on these points. Enterprise agreements can be explicit.

Support commitments matter when you are troubleshooting production incidents. Standard API access comes with community support and documentation. Enterprise agreements can include dedicated support channels, faster response times, and access to engineering teams. When you are debugging a production issue that is costing you revenue, the difference between community forum support and a direct line to the provider's engineers is material.

These protections cost money. Providers charge premiums for enterprise agreements. The question is whether the risk reduction justifies the cost. For products with thin margins and low scale, it usually does not. For products with enterprise customers, regulatory requirements, or significant revenue at risk, it usually does.

## When to Self-Host and When to Use APIs

The decision to run models on your own infrastructure versus using provider APIs is a trade-off between control and convenience. APIs are easy. You pay per token, and the provider handles infrastructure, scaling, and updates. Self-hosting is hard. You manage infrastructure, optimize inference, and handle model updates yourself. But self-hosting gives you control over costs, availability, latency, and data handling.

Self-hosting makes sense when your usage volume is high enough that per-token API costs exceed the cost of running your own infrastructure. The break-even point depends on your request volume, average request size, and infrastructure efficiency. For many products, that break-even point is reached at millions of requests per day. Below that threshold, APIs are cheaper.

Self-hosting also makes sense when you have strict data residency or privacy requirements that APIs cannot meet. If you cannot send customer data to a third-party provider due to regulatory constraints, contractual obligations, or security policies, you must run models on your own infrastructure. This is common in healthcare, finance, and government sectors.

Self-hosting makes sense when latency is critical and you need models geographically close to users. API calls add network round-trip time. If your product requires sub-100ms response times, hosting models in the same data center as your application can make the difference between acceptable and unacceptable performance.

Self-hosting does not make sense when you need frontier model capabilities. The best open-source models lag behind the best proprietary models. If your product's value depends on frontier capabilities — complex reasoning, nuanced instruction following, or state-of-the-art performance on benchmarks — you need API access to proprietary models. Self-hosting open-source models will not deliver the same quality.

Self-hosting also does not make sense when you lack the infrastructure and ML operations expertise to run models reliably. Managing model inference at scale is not trivial. It requires expertise in GPU infrastructure, inference optimization, monitoring, and failover. If you do not have that expertise in-house, self-hosting will be more expensive and less reliable than using APIs.

The hybrid approach is increasingly common. Use APIs for frontier model capabilities and self-host open-source models for high-volume, cost-sensitive, or latency-critical workloads. This gives you the best of both worlds: cutting-edge capabilities where you need them and cost-effective, low-latency inference where volume and speed matter more than absolute quality.

## Building for the Long Term

Foundation model dependency is not a problem you solve once. It is a risk you manage continuously. The landscape changes. New providers emerge. Existing providers change terms. Model capabilities evolve. Your strategy must evolve with it.

The durable approach is to treat model providers as interchangeable infrastructure, not strategic partners. Do not build your product around the unique features of one provider. Build your product around the outcomes you need to deliver, and use whichever provider best delivers those outcomes at acceptable cost and risk. When a better option emerges, you should be able to switch without rewriting your application.

This requires discipline. It is tempting to use provider-specific features that make development faster in the short term. It is tempting to optimize for one provider's API and assume you will never need to switch. It is tempting to skip building abstraction layers because they add complexity. These short-term conveniences create long-term lock-in.

The teams that thrive in the AI product market are the ones that build for portability from day one. They abstract provider dependencies. They maintain evaluation parity across providers. They test fallback paths. They negotiate contracts that protect them from unilateral provider changes. They treat model providers as fungible infrastructure, not as dependencies they cannot escape.

The foundation model landscape in 2026 is more mature than it was two years ago, but it is still evolving rapidly. Providers will continue to change pricing, deprecate models, and update policies. New providers will enter the market. Regulatory requirements will tighten. The teams that succeed are not the ones that bet on one provider and hope for the best. They are the ones that architect for change and manage dependency risk as a first-class concern.

You are building a business, not a demo. Businesses require durability. Durability requires resilience to external dependencies. Foundation model providers are external dependencies. Manage them accordingly.

---

*Next: the trade-offs between proprietary and open-source models — and why the choice is more nuanced than cost versus capability.*
