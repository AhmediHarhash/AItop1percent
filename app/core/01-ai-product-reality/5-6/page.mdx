# Chapter 5.6 — Fallback Design: What Happens When AI Fails

Your AI will fail. Not sometimes — regularly. The question isn't whether you'll have failures. The question is whether your failures are graceful or catastrophic.

Fallback design is the art of making failures invisible — or at least acceptable — to the user. It's the least glamorous part of AI product development and the most important for user experience.

---

### The Fallback Hierarchy

Every AI product should have a layered fallback strategy. When the primary path fails, you fall to the next level:

**Level 1: Primary AI response.**
The model gives a good answer. This is the happy path, and it should work 80-95% of the time depending on your product and risk tier.

**Level 2: Degraded AI response.**
The model gives a partial or lower-quality answer. Maybe it can answer part of the question but not all of it. Maybe it can provide general guidance but not specific details. This is still useful and beats no response.

**Level 3: Canned/template response.**
The AI can't generate a useful response, so you serve a pre-written response that's always safe and relevant. "I don't have enough information to answer that. Here are some resources that might help: [link]." Not great, but not harmful.

**Level 4: Human escalation.**
Route the user to a human agent. This is the most expensive fallback but the most reliable. "Let me connect you with a team member who can help." This should always be available for customer-facing products.

**Level 5: Graceful error.**
Everything has failed — the model is down, the fallback systems are down, and humans aren't available. Show a clear, honest error message: "I'm unable to help right now. Please try again later or contact us at [channel]." Never show a raw error page, a blank screen, or a generic "something went wrong."

---

### When to Trigger Fallbacks

**Confidence-based triggers.** The model returns a confidence score below your threshold. Below 70%? Don't show the AI response — go to Level 3 or 4.

**Content-based triggers.** The model's response triggers a safety filter, contains hallucination indicators (like fabricated citations), or doesn't match the expected format. Route to fallback.

**Latency-based triggers.** The model hasn't responded in 5 seconds. Don't make the user wait forever. Show a loading state, then fall back to a canned response or human escalation after a timeout.

**Error-based triggers.** The model API returns an error, rate limit, or timeout. Fall back immediately — don't retry more than twice or you'll multiply the user's wait time.

**User-initiated triggers.** The user says "this isn't helpful" or explicitly asks for a human. Respect the request immediately. Don't try to convince them to keep talking to the AI.

---

### Fallback Design Principles

**Fallbacks should feel intentional, not broken.** "I'm not able to help with that specific question, but I can connect you with someone who can" feels intentional. A blank screen or a "sorry, error" message feels broken. The difference is design effort.

**Fallbacks should preserve context.** When you escalate to a human, pass the full conversation history. Nothing frustrates a user more than repeating everything they just told the AI.

**Fallbacks should be fast.** The user is already having a bad experience (the AI failed). Don't make it worse with slow fallback responses. Canned responses should be instant. Human escalation should have a clear, short time commitment.

**Fallbacks should be tracked.** Every time you fall back, log it. Fallback rate is one of your most important operational metrics. A rising fallback rate means your AI is degrading. A specific query type with high fallback rates reveals an eval gap.

**Test your fallbacks.** Most teams never test their fallback paths until a real outage. Test them deliberately: simulate model failures, inject timeouts, trigger safety filters intentionally. Make sure every level of the hierarchy actually works.

---

### The "No AI" Fallback

For critical products, build a version that works without AI at all. If the model provider has a total outage (it happens — OpenAI, Anthropic, and Google have all had significant outages), your product should still function, even if in a reduced capacity. A customer support product that becomes completely non-functional because the AI is down is a single-point-of-failure architecture, and that's a design bug.

*Next: the data requirements you'll discover too late — unless you read this first.*
