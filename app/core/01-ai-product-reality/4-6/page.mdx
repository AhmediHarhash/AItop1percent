# Chapter 4.6 — The Role of Domain Experts

Here's a pattern I see repeatedly: a team of talented engineers builds an AI product for healthcare, finance, or legal — without anyone on the team who has actually worked in healthcare, finance, or law. They build something technically impressive that fails every domain-specific test an expert would have anticipated.

Domain experts aren't a luxury. For any product operating above Tier 1, they're a requirement.

---

### What Domain Experts Bring

**They know what "right" looks like.** An engineer can build a system that produces fluent text. A domain expert knows whether that fluent text is correct. In medical AI, "right" means clinically accurate, following current treatment guidelines, and appropriate for the patient context. In legal AI, "right" means legally sound, jurisdiction-specific, and reflecting current precedent. No amount of general AI knowledge substitutes for this domain knowledge.

**They know the failure modes.** Every domain has specific ways AI fails that outsiders don't anticipate. In medical AI, the model might give accurate information for the wrong patient population. In legal AI, it might cite a case that was overturned. In financial AI, it might produce analysis that's correct but violates a specific regulatory requirement. Domain experts have seen these failures in non-AI contexts and can predict where AI will make them.

**They calibrate quality standards.** What level of accuracy is acceptable? In a customer support chatbot, 85% might be fine. In a medical diagnostic tool, 85% might be malpractice. Domain experts know where the bar is because they've worked under that bar their entire career.

**They create ground truth.** Evaluation requires correct answers to compare against. For domain-specific products, only domain experts can provide those correct answers. A software engineer can't determine whether a legal analysis is accurate. A PM can't verify a medical recommendation. The ground truth that your entire evaluation framework rests on comes from domain expertise.

---

### How to Work with Domain Experts

**Embedded, not consulting.** The worst model is hiring a domain expert for a two-week review at the end of the project. By then, the architecture, data, and evaluation are all built on assumptions that might be wrong. The best model is embedding a domain expert in the team from day one — or at minimum, from the requirements phase.

**Regular review cadence.** If full-time embedding isn't feasible, establish a regular review: weekly output reviews where the domain expert examines a sample of AI outputs and provides structured feedback. This feedback directly improves your evaluation framework.

**Structured feedback, not just vibes.** "This looks wrong" isn't actionable. Build a structured review form: Is the output factually correct? Is it complete? Is it appropriate for the context? What would a professional do differently? Structured feedback feeds your eval metrics.

**Respect their time.** Domain experts (especially doctors, lawyers, and financial analysts) are expensive and busy. Don't waste their time on tasks a non-expert could do. Use them for the things only they can do: validating correctness, identifying domain-specific failure modes, and calibrating quality standards.

---

### The Domain Expert Gap

In 2026, there's a massive shortage of domain experts who understand AI well enough to work effectively on AI product teams. Most domain professionals don't speak "model," "eval set," or "hallucination." Most AI engineers don't speak "differential diagnosis," "precedent," or "regulatory capital."

Bridging this gap requires investment from both sides:
- Train domain experts on AI basics: what models can do, how they fail, what evaluation means, and how to structure feedback
- Train AI engineers on domain basics: terminology, standards of care, common failure modes, and regulatory requirements

The teams that bridge this gap fastest win. Not because their models are better, but because their understanding of what the models need to do is more accurate.

---

### When You Can't Hire Domain Experts

If you can't hire or embed domain experts (budget, availability, or stage constraints), use these alternatives:

- **Domain expert contractors** for periodic reviews (monthly output audits, quarterly eval set validation)
- **Published domain guidelines** as a proxy for expert judgment (clinical guidelines, legal standards, industry benchmarks)
- **Customer feedback** from users who are domain experts (doctors using your medical AI will tell you when it's wrong)
- **Academic partnerships** where researchers provide domain validation in exchange for research access

These are compromises, not substitutes. A team with embedded domain expertise will always outperform one without.

*Next: building vs hiring — how to compose your team at each stage.*
