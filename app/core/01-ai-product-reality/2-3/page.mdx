# Chapter 2.3 — Search & Retrieval Products (RAG, Semantic Search, Recommendations)

Search and retrieval products represent one of the most practical and commercially proven AI architectures in 2026. Unlike pure generation, these systems don't invent answers — they find them. And that distinction changes everything about how you build, evaluate, and trust them.

---

### The Core Idea

A search and retrieval product takes a user query, finds relevant information from a knowledge base, and presents it — either as raw results or synthesized into a natural language answer.

The most common architecture is RAG (Retrieval-Augmented Generation): retrieve relevant documents, feed them to a language model, and generate an answer grounded in those documents. But retrieval products also include semantic search (finding documents by meaning, not keywords), recommendation engines (suggesting content based on user behavior or profiles), and hybrid systems that combine multiple retrieval strategies.

---

### Three Product Shapes

**Document Q&A systems.** "Ask questions about your documents." Legal research tools, internal knowledge bases, customer support systems that answer from help docs. The user asks a natural language question, the system finds the relevant passage, and either shows the passage or generates an answer from it.

The unique challenge: attribution. Users need to know where the answer came from. "The system says X" is not enough. "The system says X, based on paragraph 3 of document Y" is what builds trust. Without attribution, your Q&A system is just a chatbot with extra steps.

**Semantic search.** Traditional search matches keywords. Semantic search matches meaning. The user types "how to handle angry customers" and finds documents about "de-escalation techniques" and "complaint resolution" — even though those documents never use the word "angry."

The unique challenge: relevance ranking. Semantic similarity isn't the same as relevance. Two documents can be semantically similar but one answers the question and the other doesn't. Tuning your retrieval to prioritize genuinely useful results over merely similar ones is an ongoing challenge.

**Recommendation systems.** "People who bought X also bought Y." "Based on your reading history, you might like Z." Recommendations use retrieval to match users with content, products, or actions.

The unique challenge: diversity vs relevance. A recommendation system that only shows what the user already likes creates a filter bubble. One that's too diverse shows irrelevant items. The balance between "give them what they want" and "show them something new" is a product decision, not a technical one.

---

### Why Retrieval Products Fail

**Bad retrieval kills good generation.** If your retrieval step returns irrelevant documents, even the best language model will generate a bad answer. This is the most common failure mode in RAG systems: the generation looks polished and confident, but the underlying sources are wrong. Users blame the AI for hallucinating when the real problem is retrieval quality.

**Chunking strategy matters more than model choice.** How you split your documents into chunks — by paragraph, by section, by semantic boundary — directly determines what your retrieval can find. Bad chunking means the relevant information is split across two chunks that the system retrieves individually but can't connect. Most teams spend too much time on prompt engineering and not enough on chunking.

**Stale knowledge bases.** Your retrieval system is only as good as the data it retrieves from. If your knowledge base hasn't been updated in three months, your system is confidently serving outdated information. Retrieval products need a content freshness strategy: automated ingestion pipelines, staleness alerts, and regular audits.

---

### Eval Strategy for Retrieval Products

You need to evaluate two things separately:

**Retrieval quality:** Did the system find the right documents? Measure recall (did it find all relevant documents?), precision (were the retrieved documents actually relevant?), and ranking quality (were the best documents ranked highest?).

**Generation quality:** Given the retrieved documents, did the system generate a correct, faithful answer? Measure faithfulness (does the answer reflect what the documents say, without adding information?), completeness (does the answer address the full question?), and attribution accuracy (do the citations point to real, relevant passages?).

Evaluating only the final answer without evaluating retrieval separately is the biggest mistake in RAG evaluation. A great answer from wrong sources is a ticking time bomb.

---

*Next: extraction and structuring products — where AI turns unstructured chaos into clean data.*
