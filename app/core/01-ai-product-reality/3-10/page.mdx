# 3.10 — Risk Tier Misclassification: The Mistakes That Kill Products

In late 2025, a SaaS company launched an AI-powered contract review assistant for their sales team. The product analyzed customer contracts and flagged potential legal risks. The team classified it as Tier 1—an internal productivity tool. They built lightweight evaluation, deployed to production in six weeks, and celebrated the launch. Three months later, sales representatives were sending AI-generated contract redlines directly to customers without legal review. When one of those redlines contained a pricing error that cost the company $1.8 million in a long-term enterprise deal, the CEO asked a simple question: "Why did we let AI make legal decisions without oversight?" The answer revealed a fundamental mistake. The product was never Tier 1. It was always Tier 3. The team had misclassified the risk, built the wrong infrastructure, and learned the lesson the expensive way.

Risk tier misclassification is one of the most common and most costly mistakes in AI product development. It happens in two directions, and both are dangerous. Under-classification treats high-risk products as low-risk, leading to incidents, liability, and regulatory exposure. Over-classification treats low-risk products as high-risk, leading to paralysis, wasted resources, and competitive disadvantage. The teams that get this right understand that risk classification is not a one-time decision made at launch. It's a continuous judgment that requires honest assessment, regular review, and the willingness to upgrade infrastructure when the evidence demands it.

## The Under-Classification Trap

Under-classification is the more dangerous direction. Your product is actually Tier 3, but you're operating with Tier 1 infrastructure. You don't have the evaluation depth, the monitoring systems, the safety protocols, or the compliance documentation that your actual risk level requires. This gap creates exposure that you won't discover until something breaks.

The most common rationalization is intent-based classification. The team says "it's just a suggestion, the user makes the final decision," and treats the product as advisory. This logic works for some products. It fails for others. The question is not what you intend the user to do. The question is what users actually do in practice. If ninety percent of users accept AI suggestions without independent verification—and they will, because accepting suggestions is faster than verifying them—then the AI is effectively making decisions, not suggesting them. Your intent doesn't determine your risk tier. User behavior determines your risk tier.

Consider what happened at a healthcare technology company in early 2025. They built an AI symptom checker for their patient portal. The team classified it as informational—Tier 1. The product displayed a disclaimer saying "this is not medical advice, consult your doctor." But usage data showed that seventy-two percent of users who received a "low concern" assessment did not follow up with their doctor, while eighty-eight percent who received a "high concern" assessment scheduled an appointment within three days. Users were treating the AI's assessment as medical guidance, not information. The disclaimer was legally protective but behaviorally meaningless. When a patient with atypical heart attack symptoms received a "low concern" assessment and delayed care, resulting in permanent cardiac damage, the company faced a lawsuit arguing the symptom checker was practicing medicine without appropriate safeguards. The product was reclassified as Tier 4 and pulled from production pending regulatory review.

Another common rationalization is phased deployment. The team says "we're only doing internal testing right now," and treats the product as Tier 1 because it's not customer-facing yet. But internal testing with a clear plan to go customer-facing in three months requires building the customer-facing infrastructure in parallel, not after launch. If you wait until the product is customer-facing to build Tier 2 evaluation and monitoring, you've already created a gap. The product is customer-facing before the safety systems are ready. This pattern shows up constantly in startups racing to market. They launch internally to "test" the product with the full intention of expanding to customers. But internal launch decisions get made with internal-tier rigor, and when the customer launch happens, nobody stops to ask whether the evaluation and monitoring infrastructure needs to upgrade.

Regulatory boundary confusion drives under-classification in healthcare, finance, and other regulated industries. A wellness app that suggests "you might have condition X based on your symptoms" might be classified by the FDA as a medical device regardless of how you brand it. A financial planning tool that recommends specific investment actions might trigger securities regulations regardless of whether you call it "educational content." The regulatory classification of your product is not determined by your marketing language. It's determined by what the product actually does and how users rely on it. In 2025, multiple fintech companies discovered this when their "educational" investment recommendation apps were deemed to be providing investment advice without proper registration.

Geographic assumptions create under-classification for global products. The team says "we're in a jurisdiction where this isn't regulated," and treats the product as low-risk. But regulations follow users, not companies. If your product serves EU users, the EU AI Act applies. If it processes US health data, HIPAA applies. If it handles financial information for UK customers, UK data protection law applies. You don't get to choose which regulations apply based on where your company is headquartered. Your regulatory obligations are determined by where your users are and what data you process. A company based in Singapore serving EU customers must comply with EU regulations. A company based in the US serving Saudi customers must comply with Saudi data localization requirements.

When under-classification leads to an incident, the consequences cascade. The immediate cost is the incident itself—the contract error, the data breach, the discrimination claim, the safety failure. The secondary cost is remediation—pulling the product, rebuilding evaluation infrastructure, implementing monitoring, conducting a root cause analysis. The tertiary cost is regulatory exposure. If regulators investigate and find that you were operating a high-risk product without appropriate safeguards, the penalties are severe. The quaternary cost is reputation damage. Enterprise customers who trusted you with sensitive use cases will ask hard questions about your judgment and your processes. The total cost can dwarf the original development cost.

## The Over-Classification Trap

Over-classification is the less dangerous but more common direction. Your product is actually Tier 1, but you're operating with Tier 3 infrastructure. You've built evaluation frameworks, compliance documentation, safety protocols, and monitoring systems that are appropriate for high-risk products but excessive for low-risk ones. The cost is time and resources. Your competitors ship in four weeks. You ship in four months. By the time your over-engineered product launches, the market has moved on.

The most common driver of over-classification is legal team conservatism. Legal teams are trained to minimize risk, and the safe answer to "what tier is this product?" is always "the highest one." If you ask Legal whether an internal document summarization tool needs Tier 3 compliance documentation, the answer will often be yes, not because the product actually warrants that level of scrutiny, but because saying yes eliminates the legal team's exposure if something goes wrong later. This is rational behavior from Legal's perspective. It's the engineering and product teams' job to push back with a proportionate risk analysis and get Legal to agree on a tier assignment that matches the actual risk, not the imagined worst-case risk.

One enterprise software company spent seven months building compliance documentation for an internal email summarization tool before launch. The tool helped employees process their inbox faster by generating one-paragraph summaries of long email threads. Legal classified it as Tier 3 because "what if sensitive customer information is in the emails?" Engineering pushed back but ultimately acquiesced. Seven months later, the product launched to thirty employees who found it mildly useful. Meanwhile, a competitor launched a similar tool in six weeks, gained traction with five thousand users, and established market position. The compliance documentation that took seven months was never read by anyone outside the legal team.

Another driver is future-proofing. The team says "what if this goes customer-facing someday?" and builds Tier 2 infrastructure for a Tier 1 internal tool. This seems prudent. It's not. Design for the tier you're at now, not the tier you might be at in two years. Build upgrade paths, not premature infrastructure. If the product goes customer-facing later, you can add evaluation depth, monitoring, and safety protocols at that time. Building them now costs months of engineering effort that could be spent on features, reliability, or the next product. The right approach is to document what you would need to add if the tier increases, not to build it speculatively.

A related mistake is scope creep in risk assessment. The team starts with a narrow use case and a clear Tier 1 classification. Then someone asks "but what if a user does X?" and adds that hypothetical scenario to the risk assessment. Then someone else asks "what if we expand to industry Y?" and adds that future possibility. The risk assessment balloons to cover every possible future use case, and the tier assignment inflates to match the most extreme scenario. A good risk assessment evaluates the product you're building now for the users you're serving now, not every hypothetical future version.

Over-classification is expensive in ways that are hard to measure because the cost is opportunity. Every month spent on compliance documentation for a Tier 1 product is a month not spent building the Tier 3 product that actually needs that rigor. Every sprint spent on safety testing for an internal tool is a sprint not spent on user-facing features. Over-classification doesn't make your product safer. It makes your team slower. In competitive markets, slowness is fatal.

## The Classification Mistakes That Teams Make Repeatedly

Some misclassification patterns show up in almost every organization. Recognizing them helps you avoid repeating them.

Mistake one is classifying by intent instead of by impact. You intended the product for low-risk advisory use. Users use it for high-risk decision-making. A general-purpose AI assistant intended for drafting emails gets used by employees to answer customer legal questions, draft HR policies, or provide medical advice. Your intent is irrelevant. The actual usage determines the risk tier, and you're responsible for the actual usage whether you intended it or not. If you can't control how users use the product, you need to classify it based on the highest-risk plausible use case, not the lowest-risk intended use case.

This mistake is particularly common with general-purpose tools. A chatbot designed for "employee productivity" can be used for countless purposes, some low-risk and some high-risk. If you can't technically constrain the use cases—and most general-purpose tools can't—you need to classify based on the highest-risk plausible use, not the lowest-risk intended use. The alternative is to build constraints into the product that prevent high-risk usage, but that requires knowing which use cases are high-risk and having the technical capability to block them.

Mistake two is classifying at launch and never re-assessing. Your product was Tier 1 when it launched internally to twenty employees. Then it went customer-facing and reached two thousand users. Then it started handling regulated data. Then it gained autonomous capabilities. But your risk tier assignment never changed, and your evaluation and monitoring infrastructure never upgraded. Risk tiers are not static. They change as products evolve, and you need scheduled re-assessment to catch the drift before it creates exposure.

The most dangerous version of this mistake is the product that slowly accumulates features over two years and silently transitions from Tier 1 to Tier 3 without anyone noticing. Each individual feature addition seems small. The cumulative risk increase is massive. But because there's no discrete moment where the team says "we are now building a Tier 3 product," the infrastructure never upgrades. Two years later, you're operating a Tier 3 product with Tier 1 infrastructure, and nobody remembers making the decision to take that risk.

Mistake three is classifying the feature in isolation instead of the feature in context. The AI feature itself is low-risk. An AI-powered search feature that retrieves documents based on natural language queries is Tier 1 on its own. But embed that search in a medical records system, and it inherits the system's risk tier. A doctor using AI search to find patient records is making clinical decisions based on the search results. If the search returns incomplete results or misses a critical document, the consequences are medical, not informational. Context determines risk, and you need to evaluate the feature in the system where it will actually operate.

This mistake shows up when AI teams are organizationally separate from product teams. The AI team builds a search feature, evaluates it in isolation, and classifies it as Tier 1. The product team integrates it into a Tier 3 medical records system. Nobody re-evaluates the risk classification because each team thinks the other team handled it. The feature launches with Tier 1 evaluation in a Tier 3 context.

Mistake four is assuming the model provider handles all safety. The team says "we use OpenAI" or "we use Anthropic," and treats safety as solved. This is a category error. The model provider handles model-level safety—preventing the model from generating harmful content, ensuring the model follows instructions, reducing hallucination rates. You handle product-level safety—validating inputs, filtering outputs, implementing use-case-specific guardrails, monitoring for misuse, and ensuring compliance with regulations. The provider's safety measures are a foundation, not a complete solution. You're responsible for the safety of your product, regardless of how safe the underlying model is.

This mistake is particularly dangerous because model provider safety is genuinely good in 2026. Models from OpenAI, Anthropic, Google, and others have extensive safety training and refusal mechanisms. It's easy to assume that this model-level safety is sufficient. It's not. Model-level safety prevents the model from giving instructions on building bombs or generating hate speech. Product-level safety prevents your specific application from producing outputs that are harmful in your specific context. These are different problems requiring different solutions.

Mistake five is treating compliance as a binary. The team asks "are we compliant?" and expects a yes or no answer. Compliance is not binary. It's a spectrum that corresponds to your risk tier. A Tier 1 product might need basic privacy compliance and data retention policies. A Tier 4 product might need clinical trial evidence, regulatory submission, third-party audits, and ongoing surveillance. Asking "are we compliant?" without specifying which tier's compliance requirements you're measuring against produces useless answers.

## How to Audit Your Risk Tier Assignment

If you suspect your product might be misclassified, here's how to check.

Start with actual usage data, not intended usage. Instrument your product to capture what users are actually doing. What queries are they asking? What outputs are they accepting? What decisions are they making based on AI recommendations? If you see a pattern of high-stakes usage—legal questions, medical advice, financial decisions, hiring recommendations—your risk tier might be higher than you think. Usage data reveals reality. Product documentation reveals intent. Reality determines risk.

This requires more than basic analytics. You need qualitative analysis of actual user sessions. Read the queries. Read the outputs. Understand the context. A query like "should I hire this candidate?" in an internal recruiting tool is Tier 2 or Tier 3. A query like "summarize this document" might be Tier 1 or Tier 3 depending on what the document contains and what the user does with the summary. Context matters, and you can't infer context from aggregate statistics.

Review your incident history. What has gone wrong? Have you had outputs that embarrassed the company, violated policy, or required intervention? The severity and frequency of incidents is a signal. If you're seeing one significant issue per month in a Tier 1 product, you're probably not actually Tier 1. Tier 1 products have occasional minor issues. They don't have regular significant issues.

Track not just incidents that reached escalation, but near-misses. A near-miss is an output that would have caused an incident if a user hadn't caught it or if circumstances had been slightly different. Near-misses are leading indicators. A high rate of near-misses means your risk tier is higher than your infrastructure can safely support.

Compare your infrastructure to your tier's baseline. If you classified the product as Tier 2, do you have the evaluation depth, monitoring systems, release process, and compliance documentation that Tier 2 requires? If you're missing major components—no adversarial testing, no output monitoring, no compliance documentation—you're either under-resourced for your tier or misclassified in a higher tier than your infrastructure can support.

Make a checklist from Chapters 3.6 through 3.9. What does your tier require for evaluation, monitoring, release process, and compliance? Check off what you have. The gaps reveal either an infrastructure deficit or a classification error.

Talk to your legal and compliance teams with specific scenarios. Don't ask "is this compliant?" Ask "if a user does X with our product and it produces outcome Y, what is our legal exposure?" Specific scenarios force specific answers. You'll learn quickly whether Legal believes your risk classification matches your actual exposure.

This conversation is most productive when you bring data. Show Legal actual usage patterns, actual outputs, and actual incidents. Abstract discussions about risk produce abstract answers. Concrete discussions about what actually happened produce concrete risk assessments.

Check regulatory guidance. If you're in a regulated industry, read the actual regulations and the enforcement guidance. The FDA publishes guidance on what constitutes a medical device. The CFPB publishes guidance on algorithmic lending. The FTC publishes guidance on algorithmic harm and deception. These documents are not abstract. They contain examples, criteria, and case studies that will tell you whether your product fits a regulated category.

Don't rely on summaries or secondhand interpretations. Read the primary sources. Regulatory guidance is written to be specific enough that you can self-assess. If the FDA guidance says "software that analyzes patient data to detect or diagnose disease is a medical device," and your product does that, you're a medical device regardless of what you call yourself.

Run the five-dimension test from Chapter 3.1. Decision impact—what decisions does this enable, and what happens if they're wrong? Data sensitivity—what data does this process? User vulnerability—who uses this, and can they assess the outputs? Automation level—how much human oversight exists? Regulatory exposure—what laws might apply? Score each dimension honestly, and let the scores determine your tier, not the tier you want to be.

Make this exercise collaborative, not individual. Get product, engineering, legal, and domain experts in a room. Walk through each dimension together. When there's disagreement, discuss it. The disagreements are where the insights live.

## The Re-Classification Trigger List

Build a formal process to re-assess your risk tier whenever certain events occur. These events are leading indicators that your risk profile might have changed.

The product moves from internal to customer-facing. This is an automatic tier increase in most cases. Internal tools affect your employees. Customer-facing tools affect your users and their trust in your brand. The consequences of failure are broader, the regulatory exposure is higher, and the reputational risk is greater.

You enter a new industry or regulatory jurisdiction. Expanding from unregulated industries into healthcare, finance, or legal services changes your risk tier. Expanding from the US into the EU, UK, or GCC changes your compliance obligations. Each new market is a re-assessment trigger.

The product starts handling sensitive data. If your product didn't process PII, PHI, or financial data before and does now, your tier has changed. Data sensitivity is one of the five dimensions, and changes in data sensitivity require re-classification.

Users start using the product for decisions it wasn't designed for. If your usage data shows high-stakes use cases that you didn't anticipate, your actual risk tier is higher than your intended risk tier. Re-classify based on reality.

The product gains agent capabilities. Adding tool use, API access, or autonomous action turns a Tier 1 chat interface into a Tier 2 or Tier 3 agent. This is a significant risk increase that requires infrastructure upgrades.

A new regulation takes effect that covers your use case. The EU AI Act, state-level AI regulations, industry-specific algorithmic accountability rules—all of these can change your product's risk tier overnight without you changing a single line of code.

You have a significant quality incident. If something goes wrong that requires executive attention, regulatory notification, or public disclosure, treat it as a signal that your risk tier might have been wrong. Incidents reveal gaps.

Your user base grows by an order of magnitude. A product serving one hundred users has a different risk profile than one serving ten thousand users. Scale changes the blast radius of failure, which changes the risk tier.

You integrate with new systems or data sources. Each integration point is a potential risk increase. Integrating with a CRM adds customer data. Integrating with a payment processor adds financial data. Integrating with an HR system adds employee data. Each integration should trigger a re-assessment.

Beyond event-based triggers, schedule a formal risk tier review every six months. Put it on the calendar. Make it a required meeting for product, engineering, legal, and compliance leads. Walk through the five dimensions. Review usage data, incident history, and regulatory changes. Re-score your tier. Update your infrastructure roadmap if the tier has changed. Risk drift is real, gradual, and invisible until something breaks. Scheduled reviews catch it early.

Risk tier classification is not a box to check at launch. It's a discipline that runs throughout the product lifecycle. The teams that get this right are honest about their risk, proportionate in their infrastructure investments, and vigilant in their re-assessment. They avoid the twin traps of under-classification and over-classification. They build exactly the evaluation, monitoring, and safety systems their actual risk requires—nothing less, nothing more.

Next, we'll explore what happens when your risk tier changes over time—and how to manage those transitions without halting progress or creating exposure.
