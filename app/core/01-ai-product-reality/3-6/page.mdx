# Chapter 3.6 — Risk by Outcome vs Risk by Action: Why Agents Change Everything

Traditional AI risk models ask: "What if the system says something wrong?" Agent AI asks a much scarier question: "What if the system does something wrong?"

This distinction — risk by outcome vs risk by action — is the most important development in AI risk thinking in 2026, and most risk frameworks haven't caught up.

---

### The Old Model: Risk by Outcome

Until agents came along, AI risk was primarily about outputs. A chatbot says something incorrect, offensive, or harmful. A classifier makes a wrong prediction. A recommendation engine suggests something irrelevant.

In all these cases, the AI produces an output and something else (a human, a downstream system) acts on it. The AI's risk is mediated by whatever sits between its output and real-world consequences. A human can ignore a bad suggestion. A downstream system can validate a prediction.

This is risk by outcome: the AI's output influences an outcome, but doesn't directly cause it.

---

### The New Model: Risk by Action

Agents take actions directly. They call APIs, send messages, modify databases, execute code, transfer money, submit forms, and interact with real-world systems. There's no human buffer between the agent's decision and its real-world effect.

This changes the risk calculus completely:

**Irreversibility.** A chatbot's wrong answer can be corrected. An agent's wrong action might be irreversible. A deleted database record, a sent email, a submitted order, an executed trade — you can't undo these. The recovery cost for an agent error can be orders of magnitude higher than for a text error.

**Cascading scope.** An agent with access to multiple tools can chain actions together. One wrong decision doesn't just produce one bad outcome — it produces a sequence of bad outcomes across multiple systems. An agent that misidentifies a customer could send the wrong email, update the wrong record, and trigger the wrong workflow, all in seconds.

**Authorization and access.** Agents operate with permissions. Those permissions determine the blast radius of failures. An agent with read-only database access can make bad decisions but can't corrupt data. An agent with admin access can cause catastrophic damage. The permissions you grant your agent define your maximum possible loss.

**Speed of harm.** A human making mistakes processes one task at a time. An agent making mistakes can process thousands of tasks per minute. The same error rate that's manageable at human speed becomes catastrophic at agent speed.

---

### The OWASP Agentic AI Top 10

In 2026, OWASP released the Agentic AI Top 10, recognizing that agent-specific risks need their own framework. The key risks include:

- **Excessive agency** — agents given more permissions than needed
- **Misaligned goals** — agents optimizing for the wrong objective
- **Tool misuse** — agents using tools in unintended ways
- **Privilege escalation** — agents finding ways to exceed their authorized scope
- **Insecure output handling** — downstream systems trusting agent outputs without validation
- **Data exfiltration** — agents accessing and transmitting sensitive data through tool calls

These risks don't exist in traditional chatbot or classifier products. They're unique to agent architectures.

---

### How to Classify Agent Risk

For agent products, add these questions to your risk assessment:

**1. What's the maximum damage a single agent session can cause?**
Sum up the worst-case impact of every tool the agent has access to. If the agent can send emails and modify database records, the maximum damage is: spam to your entire customer list plus corrupted database. Design your permissions so this maximum is survivable.

**2. Can the agent's actions be reversed?**
Categorize every action as reversible or irreversible. Reversible actions (draft an email, create a document) can be automated. Irreversible actions (send an email, delete a record, execute a payment) need human approval gates or at minimum confirmation steps.

**3. What's the agent's loop budget?**
How many iterations can the agent run before a hard stop? How much can it spend per session? How many API calls can it make? Without hard limits, a confused agent can burn resources indefinitely.

**4. What happens when the agent is confused?**
A well-designed agent knows when it's stuck and escalates to a human. A poorly designed agent keeps trying different approaches, potentially making the situation worse with each attempt. Your agent needs an explicit "I'm stuck" behavior.

---

### The Bottom Line

Agent risk isn't just higher than text risk — it's a different category. Building an agent product without addressing action-level risk is like building a self-driving car without brakes. The technology is impressive, but the safety infrastructure is what determines whether it's usable.

*Next: how your risk tier determines your entire evaluation strategy.*
