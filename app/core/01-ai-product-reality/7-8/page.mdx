# Chapter 7.8 — Your First Production Deployment

The jump from prototype to production is where most AI products stumble. The prototype works in a controlled environment with curated inputs. Production is the wild — real users, unexpected inputs, peak loads, edge cases, and everything in between.

Here's the deployment approach that minimizes risk while getting you to market.

---

### The Staged Rollout

Don't flip a switch and expose your AI to all users on day one. Stage the rollout to limit blast radius.

**Stage 1: Internal dogfood (1-2 weeks).** Your own team uses the product for real work. Not a demo — actually using it to do their jobs. Your team is more forgiving than customers and better at articulating what went wrong. Fix the issues they find.

**Stage 2: Closed beta (2-4 weeks).** 10-50 selected users who've opted in, know it's AI, and are willing to provide feedback. Choose users who represent your target market but who will tolerate imperfection. Collect feedback aggressively. Track every quality signal.

**Stage 3: Limited GA (2-4 weeks).** Open to a percentage of your user base — 10-25%. Monitor quality metrics, cost, latency, and user behavior. Compare to your eval benchmarks. If production quality matches or exceeds your eval quality, expand.

**Stage 4: Full GA.** Open to everyone. Continue monitoring. The first month of full GA is still a learning period — expect to discover failure modes your eval set didn't cover.

---

### The Production Deployment Checklist

**Infrastructure.**
- Model API calls are behind a retry-and-fallback layer
- Rate limiting is configured (per user, per minute)
- Timeouts are set (don't let a hung API call block your application)
- Error handling returns a graceful user-facing message, not a stack trace

**Quality.**
- Eval suite has been run on the production model and prompt configuration
- Quality meets your go threshold
- A daily sample of production interactions will be reviewed (manually or automated)

**Safety.**
- Input validation catches obviously malicious or out-of-scope requests
- Output monitoring flags potentially harmful responses
- Escalation path to human agents is tested and working

**Observability.**
- Logging captures: inputs, outputs, model version, prompt version, latency, token counts, errors
- Alerts are set for: error rate spikes, latency degradation, cost anomalies
- A dashboard (even simple) shows daily metrics

**Operations.**
- On-call rotation is defined (who gets the alert at 2 AM?)
- Rollback procedure is documented and tested (how do you revert to the previous prompt or model?)
- Escalation policy is clear (when does an issue go from engineer to PM to leadership?)

---

### What Goes Wrong in the First Week

**Unexpected inputs.** Users will type things you never imagined. Gibberish, extremely long inputs, inputs in languages you didn't plan for, copy-pasted HTML, emoji-only messages. Your input handling needs to be robust, not just functional.

**Latency spikes.** Your P95 latency in testing was 2 seconds. In production, during peak hours, it's 8 seconds. Model provider load, network latency, and concurrent requests all contribute. Monitor latency from the first hour.

**Cost surprises.** Some users will send much longer inputs than your average. Some will use the product 10x more than expected. Track cost per user from day one.

**The "it works but users don't care" problem.** Sometimes the AI works great but users don't adopt it. This is a product problem, not an AI problem. Are users discovering the feature? Is it solving a real pain point? Is the UX intuitive?

---

### The First-Week Ritual

For the first week of production, do a daily review: look at 20 random interactions, check all monitoring dashboards, review any user feedback, and discuss as a team. By the end of the first week, you'll have a clear picture of production reality versus eval expectations. That gap is your iteration roadmap.

---

*Next: what comes after launch — the iteration cycle that turns an MVP into a mature AI product.*
