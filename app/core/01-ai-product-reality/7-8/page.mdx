# 7.8 — Your First Production Deployment

In mid-2025, a healthcare technology company launched an AI appointment scheduling assistant to 8,000 patients across twelve clinic locations. The prototype had worked flawlessly in internal testing. The team had tested it with realistic data, run hundreds of scenarios through their eval suite, and achieved 94% success rate on booking accuracy. They felt confident. On launch day, they turned it on for all patients at once, announced it via email, and expected a smooth rollout.

Within six hours, the system was underwater. Patients were calling in frustrated because the AI had booked them at the wrong location, confused morning and evening time slots, and double-booked providers who were already at capacity. The AI handled expected inputs well, but production traffic included edge cases the team never tested: patients requesting appointments in languages the system didn't support, patients with complex multi-provider needs, patients who changed their minds mid-conversation and wanted to cancel and rebook in the same session. The appointment database, which handled low query volume fine in testing, started timing out under real load. Latency spiked from two seconds to twelve. The fallback mechanism, which was supposed to route failures to human schedulers, routed so many failures that the scheduling team's queue filled up and stopped accepting new requests. The company disabled the feature after eight hours, manually corrected 200 incorrect bookings, and spent the next two weeks handling the support backlog.

The root cause wasn't that the AI didn't work. It worked in the controlled environment where they tested it. The root cause was that production is not a controlled environment. Production has traffic spikes, unexpected inputs, infrastructure load, edge cases that never appear in eval datasets, and user behaviors that no amount of internal testing can predict. The jump from prototype to production is where most AI products stumble, not because the team didn't build something good, but because they didn't deploy it with the discipline that production demands.

## Why Production Is Different

Your prototype works because the environment is controlled. You test with curated inputs. You run queries during business hours when model provider latency is low. You handle one request at a time. You review outputs manually and catch errors before they reach users. Production removes all of that control. Users send inputs you never imagined. They use your product at 2 AM when model provider infrastructure is under maintenance. They send a hundred requests at once. They don't manually review outputs. They trust the AI to be right, and when it's not, they escalate.

This gap between prototype and production is predictable, but teams underestimate it. They think "we tested this thoroughly" and assume that thoroughness transfers to production. It doesn't. The eval suite proves the AI works on the scenarios you anticipated. Production surfaces the scenarios you didn't. The difference between a failed launch and a successful one is not how well you tested. It's how you deploy with the assumption that production will surface problems you didn't catch, and how you structure the deployment to limit blast radius when those problems emerge.

Production also introduces infrastructure dependencies that prototypes don't have. Your prototype calls the model API directly. Production needs retry logic, timeout handling, rate limiting, error logging, fallback mechanisms, and graceful degradation. Your prototype runs on your local machine. Production runs on shared infrastructure with variable load, network latency, and occasional outages. Your prototype doesn't have cost constraints. Production does. Every query costs money, and unexpected usage patterns can blow through your budget in hours.

The deployment approach that minimizes risk is staged, monitored, and disciplined. You don't flip a switch and expose your AI to all users on day one. You roll out in stages, watch what happens at each stage, and only proceed when the data proves it's safe.

## The Staged Rollout

The first stage is internal dogfooding. Your own team uses the product for real work for one to two weeks. Not a demo, not a test environment. Actually using it to do their jobs. If it's a customer support AI, your support team uses it to handle real customer queries. If it's a contract review tool, your legal team uses it to review real contracts. If it's a scheduling assistant, your operations team uses it to book real appointments.

Your team is more forgiving than customers. They know it's new. They know who to talk to when it breaks. They're better at articulating what went wrong than external users are. But they're also real users with real needs, and if the AI doesn't work for them, it won't work for customers. Dogfooding surfaces usability issues, unexpected failure modes, and integration problems that testing doesn't catch. You fix those issues before external users see them.

The second stage is closed beta. Ten to fifty selected users who've opted in, know they're using an AI, and are willing to provide feedback. You choose users who represent your target market but who will tolerate imperfection. They're early adopters, they're engaged, and they're motivated to help you improve the product. You collect feedback aggressively. You ask them to rate every interaction. You review their usage patterns daily. You track every quality signal: accuracy, latency, cost, error rate, user satisfaction.

Closed beta lasts two to four weeks. This is where you learn whether production quality matches eval quality. Your eval suite said 94% accuracy. Does production traffic hit 94%, or does it hit 82% because real users phrase questions differently than your eval dataset anticipated? Your eval said median latency is two seconds. Does production latency stay at two seconds, or does it spike to eight seconds during peak usage? The gap between eval and production is your iteration roadmap. You close that gap before expanding further.

The third stage is limited general availability. You open the product to a percentage of your user base, typically ten to twenty-five percent. You monitor quality metrics, cost, latency, and user behavior. You compare production quality to your eval benchmarks. If production quality matches or exceeds eval quality, you expand. If production quality is lower, you investigate. You identify the failure modes, you update your eval dataset to include those modes, you improve the system, and you re-evaluate before expanding.

Limited GA lasts two to four weeks. This stage exists to catch issues that only appear at moderate scale. A system that works for fifty beta users might fail for 5,000 users because of database load, API rate limits, or cost dynamics. You need to see the system under real load before committing to full rollout.

The fourth stage is full general availability. You open the product to everyone. You continue monitoring. The first month of full GA is still a learning period. You will discover failure modes your eval set didn't cover. You will see usage patterns you didn't anticipate. You will find edge cases that only appear at full scale. This is normal. The staged rollout ensured that when these issues appear, you have the monitoring, the processes, and the team discipline to handle them without destroying trust.

## The Production Deployment Checklist

Before deploying to any stage beyond internal dogfooding, you verify that five categories of requirements are met. These are not optional. These are the difference between a deployment that succeeds and one that crashes in hours.

Infrastructure first. Your model API calls are behind a retry-and-fallback layer. If the API call fails, the system retries with exponential backoff. If retries fail, the system falls back to a safe default: escalate to a human, return a generic response, or gracefully tell the user the system is unavailable. You don't show users raw error messages. You don't let a single API timeout bring down your entire application. Rate limiting is configured per user and per minute. A single user sending a hundred requests per minute doesn't exhaust your API quota or blow through your budget. Timeouts are set. A hung API call doesn't block your application indefinitely. After ten seconds, you timeout, log the error, and return a fallback response. Error handling returns a graceful user-facing message, not a stack trace.

Quality second. You've run your eval suite on the production model and prompt configuration. The same model, the same prompt version, the same system message that will run in production. Quality meets your go threshold. The results are documented. A daily sample of production interactions will be reviewed, either manually or via automated evaluation. You know who's responsible for the review, what they're checking for, and what happens if quality degrades.

Safety third. Input validation catches obviously malicious or out-of-scope requests. A user sending a megabyte of text doesn't crash your system. A user sending prompt injection attempts doesn't bypass your safety guardrails. Output monitoring flags potentially harmful responses. If the AI generates content that violates your safety policies, you catch it before the user sees it, or you log it and address it immediately. The escalation path to human agents is tested and working. You've manually triggered an escalation, confirmed it routes correctly, and verified the human agent receives the context they need to handle the request.

Observability fourth. Logging captures everything you need to debug and improve: inputs, outputs, model version, prompt version, latency, token counts, errors. You don't log personally identifiable information unless you have the legal and security infrastructure to handle it, but you log enough to reconstruct what happened when something goes wrong. Alerts are set for error rate spikes, latency degradation, and cost anomalies. If error rate jumps from 2% to 15%, you get an alert within minutes. If median latency doubles, you know immediately. If daily cost spikes 10x because a user found a way to trigger expensive queries, you catch it before it drains your budget. A dashboard, even a simple one, shows daily metrics: query volume, error rate, latency percentiles, cost per query, user feedback distribution.

Operations fifth. The on-call rotation is defined. Who gets the alert at 2 AM when the system goes down? Who has the access and the knowledge to investigate and fix it? The rollback procedure is documented and tested. How do you revert to the previous prompt version? How do you switch to a different model if the current one is degraded? You've tested the rollback, you know it works, and the responsible person knows how to execute it. The escalation policy is clear. When does an issue go from the engineer on call to the product manager to leadership? What's the threshold for disabling the feature entirely versus letting it run degraded?

These five categories are the foundation. If any one is missing, your deployment is fragile. When production surfaces the unexpected, and it will, you won't have the infrastructure to handle it gracefully.

## What Goes Wrong in the First Week

The first week of production is when you learn the gap between your assumptions and reality. Some failures are predictable. Some are not. The predictable ones you can prepare for. The unpredictable ones you handle with monitoring and process.

Unexpected inputs are the most common failure. Users will type things you never imagined. Gibberish, extremely long inputs, inputs in languages you didn't plan for, copy-pasted HTML, emoji-only messages, SQL injection attempts, prompt injection attempts, conversations that start mid-context because the user assumes the AI remembers their last session when it doesn't. Your input handling needs to be robust, not just functional. You validate input length. You strip or escape special characters. You detect and reject obviously malicious patterns. You handle non-English inputs gracefully, either by processing them if your model supports it or by telling the user the system only supports specific languages.

Latency spikes are the second most common issue. Your P95 latency in testing was two seconds. In production, during peak hours, it's eight seconds. Why? Model provider load varies by time of day. Network latency varies. Concurrent requests queue up. Database queries that were fast with low traffic slow down under load. You monitor latency from the first hour, you set alerts for degradation, and you have a plan for what to do when it spikes. Sometimes the answer is caching. Sometimes it's switching to a faster model. Sometimes it's accepting higher latency during peak hours and communicating that to users.

Cost surprises are the third common issue. Some users will send much longer inputs than your average. Some will use the product ten times more than you expected. Some will trigger workflows that make multiple API calls per interaction when you budgeted for one. You track cost per user from day one. You set budget alerts. You analyze the highest-cost interactions and decide whether they're legitimate use cases you need to optimize for or abuse you need to rate-limit.

The "it works but users don't care" problem is the fourth issue, and it's the hardest to fix because it's not technical. Sometimes the AI works great by your quality metrics, but users don't adopt it. Adoption is below projections. Engagement drops after the first try. Users give it neutral feedback, not negative, just indifferent. This is a product problem, not an AI problem. Are users discovering the feature? Is it solving a real pain point, or a pain point you assumed existed? Is the user experience intuitive, or does it require explanation? You iterate on positioning, on UX, on the problem you're solving. You talk to users. You watch session recordings. You figure out why the gap exists between "this works" and "users love this."

## The First Week Ritual

For the first week of production, you run a daily ritual. It takes thirty minutes. It's non-negotiable. You look at twenty random interactions from the previous day. You check all monitoring dashboards: query volume, error rate, latency, cost, user feedback. You review any user feedback that came in through your feedback mechanism. You discuss as a team: what surprised us? What worked better than expected? What worked worse? What should we investigate further?

By the end of the first week, you have a clear picture of production reality versus eval expectations. That gap is your iteration roadmap. The issues you found are the issues you fix first. The patterns you saw are the patterns you add to your eval dataset. The surprises become test cases. The daily ritual ensures you're learning systematically, not reacting haphazardly.

## The Confidence Ladder

Each stage of the rollout is a gate. You don't proceed to the next stage until you've met the criteria for the current stage. Internal dogfood succeeds when your team uses the product for real work without significant issues for at least one week. Closed beta succeeds when production quality matches eval quality and user feedback is net positive. Limited GA succeeds when quality, latency, cost, and adoption metrics are within your targets. Full GA succeeds when the system runs stably for a month without major incidents.

This staged approach feels slow. It is slow compared to flipping a switch and shipping to everyone on day one. But it's fast compared to the alternative: launching to everyone, crashing in hours, pulling the feature, spending weeks rebuilding trust, and starting over. The staged rollout limits blast radius. When something goes wrong, it affects ten users, not 10,000. You fix it before expanding. You learn before you scale.

The confidence ladder also builds team discipline. Each stage requires the same rigor: monitor, evaluate, decide whether to proceed. By the time you reach full GA, the team has run this process four times. They know what good looks like. They know how to spot problems early. They know how to escalate when needed. That discipline compounds. It's what separates teams that launch once and iterate from teams that launch once and fail.

## What to Watch in the First 24 Hours, First Week, First Month

The first 24 hours: error rate and latency. If either spikes, you investigate immediately. You don't wait to see if it resolves. You check the logs, you identify the failure mode, and you decide whether to roll back, apply a hotfix, or tolerate the issue while you build a proper fix. The first 24 hours also reveal whether your monitoring and alerting actually work. If an error rate spike happens and you don't get an alert, your monitoring is broken. Fix it.

The first week: quality and user behavior. Does production quality match eval quality? Are users using the product the way you designed it, or are they doing something unexpected? Are they satisfied, frustrated, or indifferent? The first week is when you validate assumptions. If the assumptions hold, you proceed. If they don't, you adjust.

The first month: stability and iteration velocity. Does the system run without major incidents? When issues appear, does the team identify and fix them quickly, or do they linger? Are you iterating based on production data, or are you firefighting? The first month reveals whether your deployment discipline is sustainable. If you're constantly firefighting, something in your process is broken. If you're iterating systematically, you're on the path to a mature product.

## The Deployment is the Beginning

Deployment is not the finish line. It's the starting line. The prototype was built in isolation. The product is built in production, iteratively, using real data from real users. The deployment gets you to production. The iteration cycle, which starts the moment you deploy, is what turns the MVP into a product users love.

Teams that treat deployment as the end celebrate too early and stagnate. Teams that treat deployment as the beginning build the discipline that compounds into long-term success. The staged rollout, the monitoring, the daily reviews, the iteration based on production data—these are not overhead. They are the work. They are how AI products get better. They are how you turn a system that works in a controlled environment into a system that works in the wild.

Your first production deployment taught you what works and what doesn't. The iteration cycle that follows is how you act on those lessons, systematically and relentlessly, until the product compounds into something great.
