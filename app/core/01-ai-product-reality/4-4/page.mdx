# 4.4 — Where Evaluation Ownership Lives

In mid-2025, a healthcare technology company spent seven months building an AI-powered clinical note summarization tool. They ran evaluations sporadically throughout development. Sometimes the ML engineer checked accuracy. Sometimes the product manager reviewed outputs. Sometimes nobody did. When they finally reached their planned launch date, they discovered that 40 percent of their summaries were omitting critical clinical details. The discovery came not from internal testing but from a pilot physician who caught a dangerous omission during patient rounds. The launch was postponed for three months while they rebuilt their evaluation framework from scratch. When the CEO asked who was supposed to catch this before launch, five different people pointed at each other. Nobody had been explicitly responsible for evaluation quality.

The problem wasn't technical competence. The team had talented engineers and a strong ML lead. The problem was organizational: evaluation was everyone's job, which meant it was nobody's job. In traditional software, testing has a clear owner with clear authority. In AI products, evaluation touches so many disciplines that ownership becomes diffuse. And diffuse ownership means critical failures slip through.

## The Ownership Problem

Evaluation in AI products spans multiple domains in ways that traditional software testing does not. Building an evaluation set requires understanding user behavior and business context, which typically lives with product managers. It requires knowing what correct answers look like, which lives with domain experts. It requires structuring data for automated testing and building infrastructure to run evaluations at scale, which lives with engineers. Running evaluations requires scoring methodology expertise, which lives with ML teams. Interpreting results and deciding whether quality is acceptable requires product judgment and domain knowledge. Acting on bad results requires the authority to block releases, which typically lives with product leadership, and the technical ability to diagnose and fix issues, which lives with ML engineering.

No single role has all these capabilities. A product manager understands what users need but can't build evaluation infrastructure. An ML engineer can build sophisticated evaluation pipelines but often lacks the business context to judge whether a given accuracy level is acceptable for the specific use case. A domain expert knows what correctness means but doesn't know how to translate that into automated metrics. A traditional QA engineer knows how to write test cases for deterministic systems but faces a fundamental challenge with probabilistic outputs.

The natural response to this multi-domain challenge is shared ownership. Every team member contributes their piece. The problem with shared ownership, in practice, is that it creates accountability diffusion. When you ask who owns evaluation and everyone raises their hand, nobody is really responsible. Evaluation becomes a side task for everyone rather than a primary responsibility for anyone. Side tasks get done when there's time, which means they get done inconsistently or not at all.

The consequences show up slowly. Evaluation sets go stale because nobody is explicitly responsible for keeping them current. Evaluations don't run before releases because nobody has it as their primary job to ensure they do. Quality thresholds drift or get ignored because nobody has clear authority to enforce them. Problems that should have been caught internally get discovered by users after launch. The team knows evaluation matters, but without clear ownership, it doesn't happen with the rigor the product requires.

## The Three Ownership Models That Work

There are three organizational models for evaluation ownership that actually function in practice. Each works in different contexts. The key is choosing one explicitly rather than leaving ownership ambiguous.

The first model is the dedicated evaluation owner, typically an engineer whose primary responsibility is evaluation. This isn't someone who runs evaluations as a side task alongside feature development. This is someone whose job description, performance reviews, and time allocation reflect evaluation as their core function. They build and maintain the evaluation infrastructure. They curate the evaluation set with input from product managers and domain experts. They run evaluations before every release. They analyze results and present them to the team. They have the standing and authority to flag quality issues and recommend blocking a release when results don't meet thresholds.

This model works best for teams of five to fifteen people building Tier 2 or higher products where quality directly impacts business outcomes or user safety. At this scale, a dedicated evaluation specialist is affordable, and the product complexity justifies the investment. The evaluation owner becomes the team's quality conscience. They're not building features, so they don't have the conflict of interest that comes from wanting to ship what you just built. Their incentives align with quality, not velocity.

The second model is evaluation as a product management responsibility. The PM owns the evaluation framework as part of owning the product's quality definition. They define what to measure, set quality thresholds, curate the evaluation set, and make ship-or-wait decisions based on evaluation results. They don't run the evaluations themselves—engineering builds the infrastructure to support this—but they own the framework and the decisions. This makes evaluation a first-class product concern rather than a technical afterthought.

This model works best for smaller teams, typically under five people, where there's no budget or headcount for a dedicated evaluation role and the PM is technically strong enough to understand evaluation methodology. It keeps evaluation tightly coupled to product decisions. The risk is that PMs are often overwhelmed with competing priorities, and evaluation can get deprioritized when feature pressure increases. To make this model work, the organization must protect evaluation time in the PM's schedule and measure the PM on quality outcomes, not just feature delivery.

The third model is a centralized evaluation team or platform that serves multiple AI products. This team builds shared evaluation infrastructure, maintains best practices, provides evaluation as a service to product teams, and ensures consistency across products. Individual product teams use the platform and define their own evaluation sets and thresholds, but the evaluation team owns the tooling, methodology, and overall quality standards. They might review every product's evaluation approach, provide templates and frameworks, and serve as internal consultants on evaluation strategy.

This model works best for organizations with multiple AI products where consistent evaluation quality matters at an organizational level and shared infrastructure reduces duplication. It's common in larger companies where AI has proliferated across teams and leadership wants to ensure baseline quality standards. The risk is that a centralized team can become disconnected from individual product contexts. To make this model work, the evaluation platform team must be deeply embedded with product teams, not operating at arm's length.

## What Doesn't Work

The anti-patterns are more common than the functional models. The most prevalent is informal shared ownership with no explicit accountability. Everyone on the team knows evaluation matters. Everyone agrees it should happen. But nobody has it as their primary job, nobody gets measured on it, and nobody has clear authority to block releases based on evaluation results. In this model, evaluation happens when someone remembers, which means it happens inconsistently. Evaluation sets get created during initial development and then never updated as the product evolves. Evaluations run sporadically, often just before major releases when someone raises a concern, but not as a regular cadence. Quality thresholds exist in documentation but not in practice—there's no enforcement mechanism.

Another common anti-pattern is assigning evaluation to ML engineering because they understand models. ML engineers are excellent at building evaluation infrastructure and designing metrics. They can tell you the precision is 0.88 and the recall is 0.76. What they often can't tell you is whether those numbers are good enough for your specific business context. Is 88 percent precision acceptable for your use case? That depends on the cost of false positives, the user's tolerance for errors, the competitive landscape, regulatory requirements, and business priorities. Those are product and domain questions, not ML questions. When ML owns evaluation in isolation, you get technically rigorous evaluations that may or may not measure what actually matters to users and the business.

A related anti-pattern is assigning evaluation to traditional QA because they own testing in the rest of the company. Traditional QA skills don't transfer directly to AI evaluation. Testing deterministic software means verifying that given input X, the system produces output Y every time. You write assertions. You check for regressions. You verify that the button click does what it's supposed to do. AI evaluation is fundamentally different. Given input X, the system produces output Y1 today and Y2 tomorrow, and both might be acceptable, or both might be wrong, or one might be better than the other. You're not checking deterministic correctness. You're evaluating the quality of probabilistic outputs against nuanced criteria that often require domain expertise to assess.

QA professionals can learn AI evaluation, but they need training, support, and often partnership with domain experts. Just assigning it to QA because they own testing elsewhere in the company sets them up to fail. The skills don't map. The tools don't transfer. The very nature of what you're evaluating is categorically different.

The most dangerous anti-pattern is no ownership at all, which manifests as "we evaluate when we remember" or "we'll check quality before launch." No designated owner, no scheduled evaluation runs, no quality gates in the release process. Evaluation happens sporadically and inconsistently, driven by individual initiative rather than organizational process. Quality problems are discovered by users after launch, not by the team before launch. This is professional negligence for any product above Tier 1, yet it remains surprisingly common in teams that understand AI technically but haven't adapted their organizational practices.

## Making Ownership Stick

Whatever model you choose, formalization is what makes it work. Informal agreements decay under pressure. When feature deadlines loom, informal commitments to evaluation get deprioritized. Formalization creates accountability that survives pressure.

First, name the evaluation owner explicitly and in writing. This might be a person, a role, or a team, but it must be explicit. Put it in job descriptions. Put it in team documentation. Put it on the org chart if you have one. When someone asks who owns evaluation, there should be one clear answer that everyone knows. This sounds trivial, but most teams skip this step. They assume everyone knows who's responsible. They don't.

Second, define the evaluation cadence with the same rigor you define release schedules. When do evaluations run? Before every release, no exceptions? Weekly regardless of release schedule? After every significant prompt or model change? On-demand when requested by the team? The answer depends on your product and risk tier, but the key is making it explicit and regular rather than ad hoc. Put it on the calendar. Make it a recurring meeting or automated job. Treat it with the same priority as a sprint planning meeting or a production deployment.

Third, define the escalation path for bad results. When evaluation results don't meet thresholds, what happens? Who decides whether to ship anyway or delay the release? Who has veto authority? What's the process for deciding whether to lower the threshold, improve the model, or change the product? These decisions often involve tradeoffs between quality, time, and scope. If you don't define the decision-making process in advance, every bad evaluation result becomes a contentious debate about whether the threshold was realistic, whether the evaluation is fair, and whether we can ship anyway just this once.

Fourth, budget time for evaluation work. Evaluation is labor-intensive. Building evaluation sets requires reviewing outputs, writing test cases, and recruiting domain experts to validate correctness. Maintaining evaluation sets requires regular updates as the product evolves. Running evaluations requires infrastructure maintenance. Analyzing results requires human judgment. If you don't budget dedicated time for this work, it gets squeezed by feature development. Successful teams allocate 15 to 25 percent of their AI engineering time to evaluation activities. That feels expensive until you discover a major quality problem in production that could have been caught with proper evaluation.

Fifth, review the evaluation framework quarterly. Your evaluation set was representative when you built it six months ago. Is it still representative now that your user base has grown? Your metrics were relevant when you defined them. Are they still measuring what matters now that your product has evolved? Your thresholds were calibrated to your initial quality bar. Are they still appropriate now that you understand your users better? Evaluation frameworks need maintenance. They decay if you don't actively tend them. Quarterly reviews keep them aligned with your current product and user reality.

## The Organizational Signal

Where evaluation ownership lives in your organization sends a signal about what you value. If evaluation is owned by ML engineering, you're signaling that you value technical quality metrics. If it's owned by product, you're signaling that you value user-facing quality and business outcomes. If it's owned by a dedicated quality team, you're signaling that quality is a first-class organizational concern, not an afterthought.

None of these is inherently right or wrong, but the signal should match your stated priorities. If you say quality is paramount but evaluation is nobody's explicit job, your organizational structure contradicts your stated values. People trust structure more than they trust speeches. If you want a quality-first culture, build evaluation ownership into your structure in a way that gives it authority and resources.

The organizational placement of evaluation also affects hiring and retention. Engineers who care about quality want to work on teams where quality has real authority. When evaluation ownership is clear and well-resourced, it signals that the organization takes quality seriously. When it's an afterthought, your best quality-focused engineers leave for companies that treat it as a first-class concern.

## The Evolution of Ownership as Products Scale

Evaluation ownership often needs to evolve as products grow. What works for a three-person team building a Tier 1 prototype won't work for a twenty-person team supporting a Tier 3 product serving millions of users. The ownership model that got you to launch may not be the one that sustains you at scale.

In the early prototype phase, evaluation is often informal and owned by whoever built the feature. The PM or lead engineer reviews outputs, judges quality subjectively, and makes go-or-no-go decisions based on gut feel. This works when the team is small, the stakes are low, and you're iterating quickly to find product-market fit. Formal evaluation infrastructure would slow you down more than it would help.

As you approach initial launch with paying customers or high-stakes use cases, evaluation ownership needs to formalize. This is typically when you designate an evaluation owner—either a specific engineer or the PM—and build basic evaluation infrastructure. You create your first formal evaluation set, define initial quality thresholds, and establish a cadence for running evaluations before releases. The evaluation isn't yet sophisticated, but it's systematic and owned.

As the product scales to serve thousands or millions of users, evaluation needs to become more sophisticated and likely requires dedicated ownership. A single engineer whose primary job is evaluation, or a small evaluation team, becomes necessary. The evaluation infrastructure becomes more complex, handling multiple evaluation sets for different use cases, automated pipelines that run continuously, and dashboards that surface quality trends over time. The evaluation owner becomes a peer to the engineering lead and product lead, with equal authority in quality decisions.

For organizations with multiple AI products, evaluation often centralizes into a platform or quality team that serves all products. This typically happens when you have five or more AI products and you're seeing duplicated effort, inconsistent quality standards, and evaluation expertise gaps across teams. The centralized team provides shared infrastructure, establishes organizational standards, and acts as internal consultants to product teams. They don't make product-specific decisions—those stay with product teams—but they ensure consistency and provide leverage.

The key is recognizing when your current model has stopped working and being willing to evolve it. Many teams stay in informal evaluation mode too long because formalizing feels like overhead. Many others create centralized evaluation teams too early, before they have enough products to justify the coordination cost. Match the model to your current scale and be ready to change it as you grow.

## Measuring the Evaluation Owner

If evaluation ownership is a real role with real authority, the person or team in that role needs clear success metrics. You can't just say someone owns evaluation and then not measure whether they're doing it well. The wrong metrics will drive the wrong behavior. The right metrics align evaluation work with actual product quality outcomes.

The most common mistake is measuring evaluation owners on coverage—how many test cases exist, how many evaluations ran, how comprehensive the evaluation set is. Coverage metrics drive evaluation owners to build bigger evaluation sets and run more evaluations, but they don't drive quality. You can have comprehensive evaluation coverage and still ship a bad product if the evaluation set doesn't represent real usage or the metrics don't capture what users care about.

Better metrics measure the quality of the evaluation system itself. Does the evaluation set catch real issues before launch? When production quality problems occur, were they represented in the evaluation set or did they slip through? When you update the evaluation set, does quality improve or does it just get bigger? These questions focus on whether evaluation is actually serving its purpose—catching problems before users see them.

The best metrics tie evaluation to product outcomes. What percentage of production quality issues were caught in evaluation before launch versus discovered by users after launch? How many launches were delayed or blocked due to evaluation results that turned out to be correct—meaning evaluation caught a real problem that would have harmed users? How has user-reported quality trended over time as evaluation has improved? These metrics show whether evaluation is actually making the product better for users.

You also need to measure the efficiency of the evaluation process. How long does it take to run a full evaluation? How much engineering time goes into maintaining the evaluation infrastructure? What percentage of releases get delayed by false alarms from evaluation—cases where evaluation flagged a problem that turned out to be a measurement issue rather than a real quality problem? Evaluation should catch real issues without creating excessive overhead or false urgency.

The evaluation owner should be measured on all of these dimensions: the quality of the evaluation system, the product quality outcomes it drives, and the efficiency with which it operates. If you measure only one dimension, you'll get pathological behavior. Measure only coverage and you get bloated evaluation sets. Measure only blocking launches and you get risk-averse evaluation owners who block everything. Measure the full picture and you get evaluation that serves the product.

## The Relationship Between Evaluation Ownership and Product Velocity

There's a common fear that formalizing evaluation ownership will slow down product velocity. If someone has the authority to block releases based on evaluation results, won't they block everything? Won't evaluation become a bottleneck that prevents shipping?

This fear is understandable but backwards. Teams with clear evaluation ownership ship faster than teams without it, because they catch problems earlier when they're cheaper to fix and they ship with confidence rather than anxiety. Teams without evaluation ownership ship tentatively, cross their fingers, and then spend weeks firefighting production issues that should have been caught before launch. That's slower than catching the issues in evaluation.

The key is that the evaluation owner's incentives need to align with shipping good products, not with preventing all risk. If the evaluation owner gets rewarded for catching problems but not penalized for delays, they'll be overly conservative. If they get rewarded for letting things ship but penalized for production issues, they'll be overly permissive. The right incentive structure rewards shipping high-quality products on schedule, which means both catching real problems and knowing when quality is good enough.

Clear evaluation ownership also speeds up decision-making. When everyone owns evaluation, every quality question becomes a debate. When one person owns it, they make the call based on data and move on. Debate is slow. Clear ownership with clear authority is fast.

The teams that ship fastest aren't the ones that skip evaluation. They're the ones that made evaluation ownership clear, resourced it properly, and gave it authority proportional to risk. Those teams catch problems early, fix them fast, and ship with confidence. That's how you move fast without breaking things.

## Starting From Zero

If you're reading this and realizing your team has no clear evaluation ownership, here's how to fix it. Don't try to implement the perfect model immediately. Start with minimal formalization and evolve from there.

First, name one person as the interim evaluation owner. This might be the PM, the tech lead, or an engineer who cares about quality. This person's job is to inventory what evaluation currently exists, identify the gaps, and propose a plan for formalizing it. Give them one week to do this assessment and present to the team.

Second, based on that assessment, choose one of the three models—dedicated owner, PM-owned, or centralized platform—that fits your current team size and product risk. Write down who owns evaluation, what authority they have, and what they're measured on. Share this with the team and get explicit buy-in from leadership.

Third, establish a minimal evaluation cadence. Don't try to build the perfect evaluation infrastructure immediately. Start with a simple manual process: before every release, the evaluation owner reviews a sample of outputs, checks them against basic quality criteria, and makes a ship-or-wait recommendation. This takes a few hours per release and immediately improves quality.

Fourth, iterate. As you run evaluations, you'll discover what's missing. You'll realize you need a better evaluation set, or automated tooling, or domain expert input. Build those capabilities incrementally based on what you learn, not based on what you think you should have. Let real problems drive your investment.

Within a month, you can go from no evaluation ownership to clear ownership with a working process. It won't be perfect, but it will be infinitely better than everyone's job and therefore nobody's job. From there, you evolve it as the product and team grow.

The teams that ship AI products with confidence are the ones that made evaluation someone's explicit, measured, resourced responsibility. The teams that ship with crossed fingers are the ones where evaluation is everyone's job and therefore nobody's job. The difference isn't technical sophistication. It's organizational clarity about who owns quality and what authority they have to enforce it.

Evaluation ownership isn't glamorous work. It doesn't generate headlines. It doesn't create new features that excite users. But it's the structural foundation that determines whether your AI product is reliably good or sporadically lucky. Get the ownership model right early, formalize it, and resource it properly. Everything else in your quality system depends on this foundation being solid.

The next challenge is how decisions actually get made when they require input from product, engineering, ML, legal, and domain experts simultaneously, and no single person has all the information needed to decide alone.
