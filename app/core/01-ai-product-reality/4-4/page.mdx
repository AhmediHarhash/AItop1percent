# Chapter 4.4 — Where Evaluation Ownership Lives

If you ask five people on an AI team "who owns evaluation?" and get five different answers, you have a problem. And most teams have this problem.

Evaluation is the most important function in AI product development, and it's the one most likely to fall through the cracks — because it touches everyone's work but lives in nobody's job description.

---

### Why Eval Ownership Is Uniquely Hard

In traditional software, testing has a clear owner: QA. They write test cases, run them, report results, and block releases when tests fail. The ownership is unambiguous.

AI evaluation is harder because it spans multiple domains:

- **Building the eval set** requires understanding user behavior (PM), knowing what correct answers look like (domain expert), and structuring data for automated testing (engineer).
- **Running evaluations** requires infrastructure (engineering), scoring methodology (ML), and interpretation (product + domain).
- **Acting on results** requires authority to block releases (PM or leadership) and technical ability to diagnose and fix issues (ML + engineering).

No single role has all these skills. So eval ends up as a shared responsibility — which in practice means nobody's primary responsibility.

---

### The Three Models That Work

**Model 1: Eval-owner engineer.**
One engineer is explicitly designated as the eval owner. Evaluation is their primary responsibility, not a side task. They build and maintain the eval infrastructure, curate the eval set (with input from PM and domain experts), run evaluations before releases, and present results. They have the authority to flag quality issues and recommend blocking a release.

Best for: Teams of 5-15 where a dedicated eval specialist is affordable and the product is Tier 2+.

**Model 2: Eval as a PM responsibility.**
The PM owns the eval framework as part of owning the product quality definition. They define what to measure, set the thresholds, and make ship/no-ship decisions. Engineering builds the infrastructure to support this. The PM doesn't run the eval themselves — they own the framework and the decisions.

Best for: Smaller teams (under 5) where there's no dedicated eval role and the PM is technically strong.

**Model 3: Eval team or platform.**
A cross-functional evaluation team or platform that serves multiple AI products. They build shared eval infrastructure, maintain best practices, provide eval-as-a-service, and ensure consistency across products. Individual product teams use the platform but the eval team owns the tooling and methodology.

Best for: Organizations with multiple AI products where consistent evaluation quality matters and shared infrastructure reduces duplication.

---

### The Anti-Patterns

**"Everyone owns eval" = nobody owns eval.** Shared ownership without specific accountability means eval happens when someone remembers, eval sets go stale, and releases ship without evaluation.

**"ML owns eval because they understand models."** ML engineers are great at building eval infrastructure but often too deep in the technical details to make business-level quality judgments. They can tell you the F1 score is 0.87. They can't always tell you whether that's good enough for your specific business context.

**"QA owns eval because they own testing."** Traditional QA skills don't transfer directly to AI evaluation. Testing deterministic software (does the button click?) is fundamentally different from evaluating probabilistic outputs (is this response good enough?). QA people can learn AI evaluation, but they need training and support.

**"We evaluate when we remember."** No designated owner, no scheduled eval runs, no gating. Evaluation happens sporadically and inconsistently. Quality problems are discovered by users, not by the team.

---

### Making It Stick

Whatever model you choose, formalize it:

1. **Name the eval owner.** One person or team with explicit, written responsibility.
2. **Define the eval cadence.** When do evals run? Before every release? Weekly? On a schedule?
3. **Define the escalation path.** When eval results are bad, who decides what to do? Who can block a release?
4. **Budget eval time.** Evaluation is work. It takes time. If you don't budget for it, it won't happen. Successful teams allocate 15-25% of AI engineering time to evaluation activities.
5. **Review the eval framework quarterly.** Is the eval set still representative? Are the metrics still relevant? Is the process still working?

Evaluation ownership isn't glamorous. But it's the difference between teams that ship with confidence and teams that ship with crossed fingers.

*Next: cross-functional decision making — how AI product decisions actually get made.*
