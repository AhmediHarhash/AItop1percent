# 5.12 — Unit Economics: Cost-to-Serve, Margins, and Pricing Constraints

In March 2025, a startup launched an AI-powered legal document review service. The product was impressive. It analyzed contracts, flagged risks, and provided recommendations with quality that matched junior associates. Customers loved it. Usage grew rapidly. Six months in, the company had 15,000 active users processing 80,000 documents per month. The growth looked phenomenal — until the finance team ran the numbers.

Cost per document: $4.20. Revenue per document: $2.50. The company was losing $1.70 on every document processed. At current usage levels, they were burning $136,000 per month on AI costs alone. The more successful the product became, the faster they lost money. The unit economics were inverted. They had built a product that could not be profitable at any scale.

The mistake was not a technical failure. The AI worked well. The mistake was economic. The team had not modeled their cost-to-serve before setting their pricing. They had assumed that scale would drive costs down, but AI costs don't behave like traditional software costs. Traditional software has high fixed costs and near-zero marginal costs — each additional user costs almost nothing to serve. AI has lower fixed costs but significant marginal costs — each additional user, each additional query, each additional token consumed costs real money. Scale does not save you if your unit economics are broken from the start.

This is the reality: you can build the best AI product in the world, but if it costs more per query than the value it creates, it's a hobby, not a business. Unit economics is where AI product dreams meet financial constraints, and it's the conversation most teams have too late.

## The Unit Economics Framework

Every AI product has a unit — the atomic interaction that creates value. For a chatbot, it's a conversation or a message. For a document processor, it's a document. For a search tool, it's a query. For an agent, it's a completed task. Your unit economics answer one fundamental question: does each unit generate more value than it costs?

The framework is simple. Calculate cost per unit. Calculate value per unit. If cost is less than value, your economics work. If cost exceeds value, you need to either reduce costs, increase value, or accept that the product is not economically viable.

**Cost per unit** is the sum of all costs incurred to serve one unit of value. It includes multiple components, and teams consistently underestimate the total.

Model inference cost is the most visible component. Input tokens plus output tokens, multiplied by the model's per-token pricing. For GPT-4, a query with 500 input tokens and 300 output tokens costs roughly three cents. For Claude Opus 4.5, it's closer to two cents. For GPT-5, it's around one cent. The cost varies by model, by input length, by output length, and by provider pricing, which changes frequently.

Infrastructure cost is the hosting, compute, storage, and networking overhead required to run your system, amortized per unit. If your infrastructure costs $10,000 per month and you serve 200,000 queries per month, that's five cents per query. This includes your application servers, your databases, your caching layers, your load balancers, your monitoring systems. It's easy to overlook, but it's real.

Human cost is the cost of moderation, review, and escalation, amortized per unit. If 5% of queries require human review and each review costs $2 in labor, that's ten cents per query on average. If 1% of queries escalate to a human agent who spends five minutes resolving the issue at $30 per hour, that's 2.5 cents per query. Human costs are highly variable and often underestimated.

Tool and API costs are any third-party services your AI calls during processing. If your AI calls a geocoding API, a payment API, a CRM API, or any external service, those costs add up. A single external API call might cost a fraction of a cent, but if your agent makes ten API calls per task, that's meaningful.

Monitoring and evaluation overhead includes the cost of running quality evaluations, safety classifiers, and logging infrastructure on production traffic. If you run an LLM-as-judge evaluation on 5% of production traffic and each evaluation costs two cents, that's 0.1 cents per query. Small, but non-zero.

The total cost per unit is the sum of all these components. Most teams calculate only the model inference cost and are shocked when the real cost is two to five times higher.

**Value per unit** is harder to quantify, but it's the constraint that determines whether your economics work.

Revenue generated is the direct financial value. If you charge $5 per document processed, that's your revenue per unit. If you charge a $50 monthly subscription and the average user processes 100 documents per month, that's 50 cents per document. If your pricing is per-seat and usage varies widely, your revenue per unit is the average across all users, but your cost per unit might be far higher for heavy users.

Cost avoided is the value created by replacing or reducing human labor. If your AI automates a task that previously cost $10 in human labor, the value per unit is $10, even if you charge less than that. This is how you justify pricing — you're capturing a fraction of the value created.

Business value created is harder to measure but often the most significant. Faster decisions, better outcomes, higher customer satisfaction, reduced risk — these create value that doesn't show up as direct revenue but justifies the product's existence. For internal tools, this is often the primary value metric.

If your cost per unit is less than your value per unit, your economics are viable. If cost exceeds value, they are not. The gap between cost and value is your margin, and that margin needs to be wide enough to cover sales, marketing, support, overhead, and profit.

## Where AI Costs Surprise You

AI costs behave differently from traditional software costs. The surprises are predictable if you know where to look.

**Token costs at scale** multiply faster than you expect. A single GPT-4 query might cost three cents. That sounds cheap. At 100,000 queries per day, it's $3,000 per day, or $90,000 per month — just for the model. Add infrastructure, monitoring, and human operations, and your monthly AI spend might exceed $150,000. The unit cost is small, but the aggregate is large.

**Retry and fallback costs** occur when things go wrong. When a query fails and retries, you pay twice. When a query falls back to a more expensive model, you pay the premium. When a query escalates to a human agent, you pay the full human cost. Your effective cost per unit is not your average model cost — it's your average model cost plus the cost of retries, fallbacks, and escalations. If 10% of queries retry once, your effective model cost is 10% higher than your base cost.

**Long input costs** happen because users don't behave like your test cases. You might design for 100-token queries, but users paste entire documents, write multi-paragraph questions, or have 30-turn conversations with full context. Your cost model based on average query length is wrong if 10% of users send inputs that are ten times longer than average. The tail drives your costs more than the median.

**Agent loop costs** are unique to agent products. Agents run in loops — each loop iteration costs model tokens plus tool API calls. A task that takes five iterations costs five times what you might estimate from a single prompt-response pair. Complex tasks might take twenty to thirty iterations. Each iteration consumes tokens for reasoning, planning, tool calls, and reflection. The cost per completed task is far higher than the cost per query for a simple chatbot.

**Evaluation costs** are ongoing, not one-time. Running quality evaluations on production traffic costs money — both the model costs for automated scoring and the human costs for manual review. If you evaluate 5% of production traffic with an LLM-as-judge at two cents per evaluation, and you serve 100,000 queries per day, that's $100 per day just for quality monitoring. This is a real, recurring cost that most teams forget to budget.

The cumulative effect of these surprises is that your actual cost per unit is often two to three times what you initially estimated. Plan accordingly.

## The Pricing Constraint

Your pricing has a ceiling and a floor. The ceiling is what users will pay. The floor is what it costs you to serve them. Your viable pricing range is the space between those two bounds, and for many AI products, that space is narrower than you think.

**Per-seat pricing** is the traditional SaaS model: a monthly subscription per user. Your cost depends on how much each user actually uses the product. Heavy users cost you more. Light users cost you less. If your heaviest users cost more to serve than their subscription fee, your pricing model breaks at scale. This is the inverse economy of scale — the more successful you are at driving engagement, the more money you lose.

The fix is usage-based caps or tiered pricing within each seat. Unlimited usage only works if your cost per unit is low enough that even your heaviest users stay profitable. For AI products with high per-query costs, unlimited usage is financial suicide.

**Per-transaction pricing** is pay-per-use: you charge per query, per document, per task. Your margin is visible on every transaction. If you charge $5 per document and it costs you $2 to process, you make $3 per document. The economics are transparent. But users may be reluctant to pay per-use, especially if they can't predict their monthly spend. Usage-based pricing creates unpredictable revenue for you and unpredictable costs for your customers. That friction reduces adoption.

The fix is predictable pricing with overages. Offer a base plan that includes a certain number of transactions per month, with per-transaction pricing beyond that. This gives users predictability and gives you a revenue floor.

**Tiered pricing** is free tier, pro tier, enterprise tier. The free tier costs you money. Every free user consumes compute, storage, and support. The pro tier needs to cover both the free tier's costs and its own. The enterprise tier needs to cover custom requirements, dedicated support, and SLA commitments. If your free tier is too generous, your pro tier pricing needs to be high enough to subsidize it, which might price you out of the market.

The fix is strict usage limits on the free tier. Free users get limited queries per month, limited features, and no human support. The free tier is marketing, not a product. It exists to convert users to paid tiers, not to serve them indefinitely at your expense.

For each pricing model, calculate the breakeven point: at what usage level does the marginal user cost you more than they pay? That breakeven point defines your profitable range. Every user below that point is subsidized by users above it. If too many users cluster below the breakeven point, your aggregate economics fail.

## Cost Optimization Levers

When the math doesn't work, you have five primary levers to pull, in order of impact.

**Model routing** is the highest-impact lever. Send 80% of queries to a cheaper model and 20% to the expensive one. If your cheap model costs one cent per query and your expensive model costs three cents, routing reduces your average cost from three cents to 1.4 cents — a 53% reduction. The challenge is classification: how do you decide which queries need the expensive model? The classifier itself costs money and might route incorrectly. But when done well, model routing is the single most effective cost optimization.

**Caching** stores and reuses responses for repeated or similar queries. If 20% of your queries are identical or near-identical to previous queries, you can serve them from cache at near-zero cost. Cache hit rates of 20% to 40% are typical for products with repeated queries. The savings are proportional to the hit rate. A 30% cache hit rate reduces costs by 30%. The challenge is cache invalidation: how do you ensure cached responses stay accurate when underlying data changes?

**Prompt compression** reduces the token count of your prompts without reducing quality. Shorter system prompts, fewer examples, more efficient instructions. If you reduce your average input from 800 tokens to 500 tokens, you reduce input costs by 37.5%. The challenge is maintaining quality: overly aggressive compression can degrade output quality, which increases retry rates and human review costs, offsetting the savings.

**Batch processing** defers non-real-time tasks to process during off-peak hours at lower compute costs. If your tasks don't need to complete within seconds, batch them and run them overnight when compute is cheaper. Cloud providers often offer lower pricing for batch workloads. Savings: 20% to 40% on infrastructure costs. The tradeoff is latency — users wait hours instead of seconds. Only viable for non-urgent tasks.

**Fine-tuning a smaller model** trades API costs for training costs. Instead of using GPT-4 at three cents per query, fine-tune a smaller model like GPT-5-mini to match GPT-4's quality for your specific use case, then use the fine-tuned model at 0.5 cents per query. Savings: 60% to 80% on per-query cost. The upfront cost is training — data collection, labeling, training runs, evaluation. This is only economically viable if your query volume is high enough to amortize the training investment. Break-even is typically around 500,000 queries for most fine-tuning projects.

These levers are not mutually exclusive. The best cost optimization strategy uses all five: route most queries to a cheaper model, cache repeated queries, compress prompts, batch non-urgent tasks, and fine-tune for your highest-volume use cases. The cumulative effect can reduce costs by 70% to 85% without sacrificing quality.

But here is the critical point: don't optimize costs before you have product-market fit. Premature cost optimization is a waste of engineering effort. Your first priority is building something users want. Your second priority is proving that users will pay for it. Your third priority is optimizing the economics. If you optimize too early, you optimize for a product that might not survive.

Do, however, model costs before you build. You need to know whether the unit economics can ever work. If your cost per query is $5 and users will pay at most $2, no amount of optimization will save you. You need a different product, a different pricing model, or a different market.

## When AI Products Are Economically Viable

Not all AI products can be profitable. The economics depend on the use case, the value created, and the cost to serve.

**High-value, low-frequency tasks** are the sweet spot. Legal contract review, medical diagnostics, financial analysis — tasks where a single AI interaction creates hundreds or thousands of dollars in value, and users perform them occasionally, not continuously. You can charge $50 per interaction, spend $10 on costs, and still deliver overwhelming value. The unit economics work because the value-to-cost ratio is ten-to-one or better.

**High-volume, low-cost tasks** can work if you achieve extreme cost efficiency. Search queries, content recommendations, spam filtering — tasks where each interaction creates modest value but volume is massive. You charge fractions of a cent per query, spend fractions of a cent on costs, and make money on volume. This only works if you optimize aggressively and operate at massive scale. For startups, this is a hard model.

**Subscription models with predictable usage** work if you can cap costs. Enterprise tools where each user performs a predictable number of queries per month, and you price based on that expected usage. You charge $100 per seat per month, users average 2,000 queries per month, your cost is $40 per seat, your margin is 60%. This works as long as usage stays predictable. The risk is that a small number of power users consume far more than average and turn your profitable users into loss leaders.

**Freemium models with strict free-tier limits** work if your free-to-paid conversion rate is high enough. You let users try the product for free with tight usage caps — ten queries per day, basic features only. Enough to demonstrate value, not enough to replace a paid plan. Your free tier costs you $2 per user per month. Your paid tier is $20 per user per month and costs you $6 to serve. If 10% of free users convert to paid, the economics work. If conversion is 2%, they don't.

**Human-in-the-loop models** work when the AI reduces human effort by 70% or more but doesn't eliminate it entirely. The AI does the first pass, the human reviews and corrects. You charge based on the human time saved, not the AI time used. If the task previously took thirty minutes of human time at $30 per hour and now takes five minutes with AI assistance, you've saved $12.50 in labor. You can charge $8 for the AI-assisted service, spend $2 on AI costs, and still deliver value. This model works in industries where full automation is not trusted but augmentation is.

The common pattern: AI products are economically viable when the value created is significantly higher than the cost to serve. The ratio needs to be at least three-to-one, ideally five-to-one or better. Anything less, and your margins are too thin to sustain the business through the inevitable cost fluctuations, model pricing changes, and competitive pressures.

## The Unit Economics Conversation Happens Now

Most teams defer the unit economics conversation until after launch. They focus on product, user experience, growth. They assume they can figure out the economics later. This is a mistake.

You need to model your unit economics during the scoping phase, before you commit to building the product. Calculate your estimated cost per unit. Estimate the value per unit. Identify your pricing model. Determine your breakeven point. Assess whether the economics can work at scale.

If the math doesn't work, you have three options. First, change the product to reduce costs — use cheaper models, reduce token usage, simplify the task. Second, change the pricing to increase revenue — charge more, add premium tiers, switch to usage-based pricing. Third, change the market — target higher-value use cases where users will pay more.

If none of those options make the economics viable, you should not build the product. A product with broken unit economics does not get fixed by scale. It gets worse. The more successful you are, the faster you lose money. That is not a business. That is a liability.

Unit economics is not an afterthought. It is a scoping constraint. It determines whether your AI product is a viable business or an expensive experiment. The next question is whether you can explain why your AI makes the decisions it does — because in many contexts, explainability is not optional.
