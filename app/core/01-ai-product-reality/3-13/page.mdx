# Chapter 3.13 — GPAI Model Obligations: What the EU AI Act Means for Your Foundation Model Choice

The EU AI Act doesn't just regulate applications — it regulates the foundation models underneath them. If you're building on GPT-4, Claude, Gemini, or any other general-purpose AI model, you need to understand what obligations flow from the model provider to you, and what obligations you carry yourself.

---

### What GPAI Means

General-Purpose AI (GPAI) is the EU AI Act's term for foundation models that can be used across many different tasks. Unlike narrow AI built for one specific purpose, GPAI models are trained on broad data and adapted to specific use cases through prompting, fine-tuning, or integration into applications.

The Act creates two tiers of GPAI models:

**Tier 1: Standard GPAI models.** Models trained below 10 to the power of 25 floating point operations. Think smaller open-source models like Phi, Mistral-7B, or Llama-8B. These face lighter requirements: technical documentation, transparency about training data, and compliance with EU copyright law.

**Tier 2: High-impact GPAI models (systemic risk).** Models trained above that compute threshold. This includes GPT-4, Claude Opus, Gemini Ultra, and similar frontier models. These face additional requirements: adversarial testing (red teaming), systemic risk assessment, incident reporting, and cybersecurity protections.

---

### What This Means for You as a Builder

You're probably not training foundation models. You're building applications on top of them. But the EU AI Act creates a chain of responsibility:

**The model provider's obligations.** The company that trained the GPAI model must provide: technical documentation, information about training data, copyright compliance, a summary of the content used for training, and (for Tier 2 models) adversarial testing results and systemic risk assessments.

**Your obligations as a deployer.** You inherit some obligations and carry others:

- **Transparency.** You must inform users that they're interacting with an AI system. If your product uses a GPAI model, users need to know.
- **Downstream risk.** Even if the model provider has done their compliance work, you're responsible for the risks your specific application creates. If you use a compliant model to build a non-compliant application, you're liable.
- **Technical documentation.** For high-risk applications (Tier 3 and 4 products), you need to document which model you're using, how you've adapted it, and what evaluation you've performed.

---

### How This Affects Model Choice

In 2026, your choice of foundation model is no longer just a technical decision — it's a compliance decision.

**Provider compliance status.** Is your model provider compliant with the EU AI Act's GPAI requirements? Can they provide the technical documentation and training data transparency that the Act requires? If not, using their model might create compliance gaps for you.

**Compute threshold awareness.** The 10^25 FLOPs threshold determines which tier your model falls into. If you're using a Tier 2 model, the regulatory burden is higher — but the model is also likely more capable. Understand the tradeoff.

**Open-source considerations.** Open-source GPAI models benefit from some exemptions under the EU AI Act, but not all. If you're using an open-source model, understand which obligations still apply and whether you (as the deployer who modifies or fine-tunes the model) might take on additional obligations.

**Multi-model strategies.** Some teams use different models for different regions. A Tier 2 model (like Claude Opus) for non-EU markets where regulation is lighter, and a smaller model or locally-hosted model for EU markets where compliance is simpler. This adds architectural complexity but can simplify compliance.

---

### Practical Steps

**1. Map your model supply chain.**
Document every GPAI model you use, its provider, its compute tier, and the provider's compliance status. If you use multiple models (routing between them based on task type), map all of them.

**2. Get compliance documentation from your provider.**
Request the technical documentation, training data summary, and (for Tier 2 models) adversarial testing results that the EU AI Act requires providers to share with downstream deployers.

**3. Build your own compliance layer on top.**
Provider compliance doesn't equal your compliance. You need to document your specific application, your evaluation methodology, your risk assessment, and your monitoring practices. The provider handles model-level compliance; you handle application-level compliance.

**4. Monitor the regulatory timeline.**
The GPAI provisions of the EU AI Act have specific compliance deadlines. Know them. Build your compliance roadmap around them. Don't wait for enforcement to start preparing.

---

*That wraps up Chapter 3. You now have a complete risk framework that covers four tiers, the unique risks of agents, how risk dictates evaluation and release processes, monitoring depth, misclassification dangers, dynamic risk changes, data residency, and GPAI model obligations. In Chapter 4, we'll build the team that brings all of this to life.*
