# 3.13 — GPAI Model Obligations: What the EU AI Act Means for Your Foundation Model Choice

In October 2025, a European startup building an AI-powered legal research platform discovered that their choice of foundation model had become a compliance issue. They'd built their product on GPT-4, assuming that OpenAI's compliance with the EU AI Act meant they were compliant by extension. During a diligence review for a Series A round, the investors' counsel asked for documentation of the model's risk assessment, adversarial testing results, and training data transparency—all required under the EU AI Act's General Purpose AI provisions for high-impact models. The startup couldn't provide it. They'd never requested it from OpenAI. When they did request it, they learned that some of the documentation was available under enterprise agreements, some required additional negotiation, and some wasn't available at all in the format their investors wanted. The diligence process stalled for three weeks while they worked through what compliance actually meant for deployers versus providers. The startup eventually closed the round, but the lesson was clear: your foundation model choice is not just a technical decision. It's a compliance decision that affects your regulatory obligations, your customer commitments, and your ability to raise capital.

The EU AI Act doesn't just regulate applications. It regulates the foundation models underneath them. If you're building on GPT-4, Claude, Gemini, Llama, or any other general-purpose AI model, you need to understand what obligations the model provider carries, what obligations flow down to you as a deployer, and how these obligations affect your product development process. The teams that get this right treat model selection as a compliance decision with technical constraints, not a technical decision with compliance afterthoughts.

## What GPAI Means and Why It Matters

General-Purpose AI is the EU AI Act's term for foundation models that can be used across many different tasks and domains. Unlike narrow AI systems built for one specific purpose, GPAI models are trained on broad data and adapted to specific use cases through prompting, fine-tuning, or integration into applications. The distinction matters because GPAI models have systemic risk potential. A narrow AI model that classifies images affects only the application using it. A GPAI model that powers thousands of downstream applications affects all of them. One model's failure can cascade across an entire ecosystem.

The Act creates two tiers of GPAI models based on training compute. The threshold is ten to the twenty-fifth floating point operations—written as ten with twenty-five zeros, or one followed by a decimal point and twenty-five digits. Models trained with less compute than this threshold are standard GPAI models. Models trained with more are high-impact GPAI models with systemic risk obligations.

Standard GPAI models face lighter requirements focused on transparency. Providers must publish technical documentation describing the model's capabilities and limitations, provide information about the data used for training including copyright compliance, publish a summary of the content used for training, and maintain policies for compliance with EU copyright law. These requirements ensure that deployers know what they're building on and can assess whether the model fits their use case.

High-impact GPAI models face additional requirements designed to address systemic risk. Providers must conduct adversarial testing—commonly called red teaming—to identify weaknesses and failure modes. They must assess systemic risks including potential misuse patterns and societal impacts. They must report serious incidents to authorities. They must implement cybersecurity protections appropriate to the risk level. These requirements recognize that the most capable models carry the highest risk and require the most oversight.

The threshold isn't arbitrary. Ten to the twenty-fifth FLOPs roughly corresponds to models like GPT-4, Claude Opus, and Gemini Ultra. Smaller models like Llama 4 8B, Mistral 7B, or Phi models fall below the threshold. The practical effect is that frontier models from major labs are high-impact GPAI, while smaller open-source models are standard GPAI. Your choice of model determines which tier of obligations applies.

## The Division of Responsibility Between Providers and Deployers

Understanding who is responsible for what is critical. The EU AI Act creates a chain of responsibility from model providers to application deployers, and both have obligations.

Model providers carry primary responsibility for model-level compliance. If you're OpenAI training GPT-4, or Anthropic training Claude, or Google training Gemini, you must publish technical documentation, provide training data transparency, conduct adversarial testing for high-impact models, assess systemic risks for high-impact models, report incidents, and implement cybersecurity protections. These obligations apply regardless of how downstream deployers use the model. You can't delegate them. You can't contractually transfer them. They're statutory obligations that rest with the entity that trained the model.

Deployers—the companies building applications on top of GPAI models—carry responsibility for application-level compliance. If you're building a legal research platform on GPT-4, or a customer service chatbot on Claude, or a content generation tool on Gemini, you must comply with the EU AI Act's requirements for your specific use case. If your application is high-risk under the Act's classification, you need risk assessment, data governance, transparency, human oversight, accuracy requirements, cybersecurity measures, and quality management systems. These obligations apply regardless of how compliant your underlying model is.

The critical distinction is level of abstraction. Model providers ensure the model itself is safe, documented, and tested. Deployers ensure their specific application of that model is appropriate, evaluated, and monitored for their use case. Neither party's compliance covers the other's obligations.

This creates a documentation requirement. Deployers need information from providers to fulfill their own obligations. If you're building a high-risk application, you need to document which model you're using, its capabilities and limitations, its known failure modes, and how you've evaluated it for your use case. You can't document this without information from the provider. The EU AI Act recognizes this and requires providers to make sufficient information available to deployers.

In practice, this means providers publish model cards, technical documentation, and transparency reports that deployers reference in their own compliance documentation. Enterprise agreements often include additional documentation that isn't publicly available. If you're building a regulated application, you should request detailed compliance documentation from your provider as part of your procurement process.

## How GPAI Classification Affects Model Selection

In 2026, your choice of foundation model is no longer just about accuracy, latency, and cost. It's also about compliance capability.

Provider compliance status matters. Is your model provider compliant with the EU AI Act's GPAI requirements? Can they provide the technical documentation and training data transparency that the Act requires? Have they conducted adversarial testing for high-impact models? Have they assessed systemic risks? If the answer to any of these questions is no, using their model creates a compliance gap for you. You can't be compliant if your provider isn't compliant, because you need their documentation to fulfill your own obligations.

The major providers—OpenAI, Anthropic, Google, Meta—have all committed to EU AI Act compliance and published varying levels of documentation. But commitment and compliance are different. Compliance is demonstrated through documentation, testing results, and ongoing reporting. As a deployer, you should ask providers for evidence of compliance, not just assurances of compliance. If they can't provide evidence, that's a signal.

Compute threshold awareness affects risk assessment. If you're using a high-impact GPAI model above the ten to the twenty-fifth FLOPs threshold, your provider has higher obligations and you inherit higher scrutiny. Regulators will expect you to have documentation from the provider about adversarial testing and systemic risk assessment. If you're using a standard GPAI model below the threshold, the compliance burden is lighter. For some use cases, choosing a smaller model reduces regulatory complexity even if it reduces technical capability slightly.

One fintech company evaluated GPT-4 versus GPT-5-mini for their financial advice chatbot. GPT-4 was more capable, but the compliance documentation requirements were more extensive because it's a high-impact GPAI model. GPT-5-mini, while still capable, fell below the threshold and had lighter documentation requirements. They chose GPT-5-mini for the MVP to reduce time-to-market, with a plan to upgrade to GPT-4 after establishing their compliance infrastructure. The technical tradeoff was acceptable given the compliance simplification.

Open-source considerations are complex. Open-source GPAI models like Llama, Mistral, or Qwen benefit from some exemptions under the EU AI Act, particularly around requirements that assume a traditional provider-deployer relationship. But if you fine-tune or significantly modify an open-source model, you might take on provider-level obligations, not just deployer-level obligations. The distinction depends on the extent of modification. Using an open-source model as-is makes you a deployer. Training your own model or heavily fine-tuning an existing one might make you a provider. The legal analysis is fact-specific and requires counsel.

Multi-model strategies are increasingly common. Some teams use different models for different use cases or different geographic regions. A high-impact GPAI model for non-EU markets where regulation is lighter, and a smaller model or locally-hosted model for EU markets where compliance is heavier. This adds architectural complexity—you need model routing logic, separate evaluation for each model, and potentially different prompting strategies—but it can simplify compliance. The tradeoff is only worth it if compliance costs or risks are significantly different across models.

## What Documentation You Need From Your Provider

To fulfill your obligations as a deployer, you need specific information from your model provider. Here's what to request.

Model capabilities and limitations. A clear description of what the model can and can't do, what tasks it's designed for, what tasks it's not designed for, and what known limitations exist. This isn't marketing material. It's technical documentation that lets you assess whether the model fits your use case and what additional safeguards you need.

Training data information. A description of the data used to train the model, including data sources, data types, any filtering or curation applied, and compliance with copyright law. For high-impact models, this includes a summary of the content used for training. This information lets you assess whether the model's training is appropriate for your use case and whether there are bias risks you need to mitigate.

Adversarial testing results. For high-impact GPAI models, evidence that the provider conducted red teaming to identify failure modes, safety vulnerabilities, and potential misuse patterns. You don't need the raw test cases—those are often confidential—but you need summary results that describe what was tested and what was found.

Systemic risk assessment. For high-impact GPAI models, evidence that the provider assessed risks beyond individual applications, including potential societal impacts, misuse potential, and cascade effects. This assessment informs your own risk analysis for your specific application.

Incident reporting and response. Information about how the provider handles incidents, what they consider a reportable incident, and what notification you'll receive if an incident affects the model you're using. This informs your own incident response planning.

Data processing agreements. For models accessed via API, clear documentation of where data is processed, how long it's retained, whether it's used for training, and what access controls exist. This was covered in Chapter 3.12 on data residency, but it's also relevant here as part of GPAI compliance.

Updates and versioning. How the provider handles model updates, whether updates are automatic or require opt-in, and how you'll be notified of changes that might affect your application. Model updates can change behavior in ways that break your application or change your risk profile. You need visibility into this.

Most of this information is available in some form from major providers. Model cards, technical documentation, and transparency reports cover much of it. Enterprise agreements can provide more detailed documentation that isn't publicly available. The gap is typically in adversarial testing results and systemic risk assessments, which providers treat as partially confidential. You need enough information to satisfy regulators without requiring providers to disclose competitive details. This balance is still being negotiated across the industry.

## Building Your Own Compliance Layer

Provider compliance doesn't equal your compliance. You need to build your own compliance layer on top of the model's compliance.

Document your specific application. The provider documents the model. You document how you use it. What use case are you addressing? What data do you process? What decisions does your application enable? What safeguards have you implemented? This documentation is separate from the provider's documentation and specific to your product.

Evaluate for your use case. The provider's testing is generic. Your testing must be specific. If you're building a financial advice chatbot, test it on financial advice scenarios, not general knowledge questions. If you're building a medical information system, test it on medical information, not creative writing. Use the provider's documentation as a starting point, but conduct your own evaluation that reflects your actual usage.

Implement use-case-specific guardrails. The provider's safety measures are broad. Your guardrails must be narrow. If your chatbot shouldn't provide legal advice, implement input filtering and output filtering that catches legal questions and blocks legal advice outputs. If your system shouldn't process certain types of sensitive data, implement validation that rejects those inputs. Provider-level safety doesn't cover application-level policy.

Monitor for your risk profile. The provider monitors the model. You monitor your application. Implement logging that captures your inputs, outputs, user actions, and downstream decisions. Monitor for the failure modes that matter in your use case—incorrect financial advice, discriminatory hiring recommendations, unsafe medical information. Your monitoring must be more specific than the provider's monitoring.

Maintain compliance documentation. Create a compliance file that includes your risk assessment, your use-case evaluation, your guardrails documentation, your monitoring approach, your incident response plan, and your provider documentation. This file is what regulators or auditors will request. It should be comprehensive enough that someone unfamiliar with your product can understand your risk profile and your safeguards.

## Practical Steps for Deployers

Here's how to handle GPAI obligations as a deployer building on foundation models.

Map your model supply chain. Document every GPAI model you use, its provider, its compute tier, and the provider's compliance status. If you use multiple models—routing between them based on task type or user location—map all of them. This gives you visibility into your compliance obligations.

Request compliance documentation during procurement. Don't wait until you're in diligence or facing a regulatory audit. When evaluating model providers, request their EU AI Act compliance documentation. Technical documentation, training data transparency, adversarial testing results for high-impact models, and data processing agreements. Make this part of your standard procurement checklist.

Assess gaps between provider obligations and your needs. What does the provider give you? What do you need for your use case? Where are the gaps? Common gaps include use-case-specific testing, domain-specific guardrails, and application-level monitoring. Identify these early and plan to build them yourself.

Build application-level compliance infrastructure. You need your own risk assessment, evaluation framework, monitoring systems, and compliance documentation regardless of how good your provider's compliance is. Don't assume provider compliance covers you. It covers the model, not your application.

Monitor regulatory developments. The GPAI provisions of the EU AI Act have phased implementation timelines. Know the deadlines that affect you. Track enforcement actions and regulatory guidance. The interpretation of GPAI obligations is still evolving, and you need to stay current.

Maintain flexibility in model choice. Don't become so dependent on one model that you can't switch if compliance, performance, or cost considerations change. Build abstraction layers that let you swap models without rewriting your application. This gives you options if your provider's compliance status changes or if a better option emerges.

## The Regulatory Timeline and What It Means for You

The EU AI Act's GPAI provisions have specific compliance deadlines. Understanding these deadlines helps you plan your compliance roadmap.

The Act entered into force in August 2024. Certain provisions, including prohibitions on unacceptable AI practices, took effect immediately. GPAI provider obligations phase in over two years from entry into force, with different requirements having different deadlines. High-impact GPAI providers face the earliest deadlines because they carry the highest systemic risk.

Deployer obligations for high-risk AI systems phase in over three years, with requirements around quality management systems, risk assessments, and technical documentation among the earliest. If you're building a high-risk application on a GPAI model, you face deployer obligations regardless of when provider obligations fully phase in.

The practical implication is that you can't wait for providers to be fully compliant before you start your own compliance work. Provider obligations and deployer obligations are on different timelines, and your obligations don't wait for your provider's obligations to be met. Start building your compliance infrastructure now, even if your provider is still working through their own obligations.

Enforcement is phased as well. Regulators are focusing on the highest-risk applications first—those affecting safety, fundamental rights, or critical infrastructure. If your application falls into these categories, expect scrutiny sooner. Lower-risk applications will face enforcement later, but not much later. Build compliance proportionate to your risk tier, but don't assume you have unlimited time.

## The Strategic Implications

GPAI obligations change how you think about model selection, vendor relationships, and product architecture.

Model selection is now a multi-dimensional decision. Technical capability, cost, latency, and compliance capability all matter. The best technical model is useless if you can't get the compliance documentation you need. The cheapest model is expensive if it creates regulatory risk. Evaluate providers holistically, not just technically.

Vendor relationships need deeper engagement. You can't treat your model provider as a black box. You need visibility into their compliance status, access to their documentation, and confidence in their ongoing commitment to regulatory obligations. This requires enterprise relationships, not just API access. For high-risk applications, plan to invest in provider engagement.

Product architecture should minimize dependence on any single model. Build abstraction layers that let you switch models if needed. Use model routing to distribute risk across multiple providers. Design evaluation and monitoring systems that work across models, not just for one specific model. Flexibility is valuable in a rapidly evolving regulatory and technical landscape.

The teams that handle GPAI obligations well treat them as product requirements, not legal overhead. They select models based on compliance capability. They build their own compliance layers on top of provider compliance. They maintain documentation that satisfies regulators and customers. They monitor regulatory developments and adapt as the landscape evolves.

The EU AI Act's GPAI provisions don't make AI product development impossible. They make it more structured. The structure benefits everyone—providers know what's expected, deployers know what they need, and regulators can oversee the ecosystem without stifling innovation. The companies that embrace this structure and build compliance into their product development process will succeed in regulated markets. The companies that treat it as an afterthought will struggle.

That completes Chapter 3 and Section 1. You now have a complete framework for understanding AI product risk—four tiers based on decision impact and stakes, the unique challenges of agent systems, how risk determines evaluation depth and release rigor, the importance of monitoring and compliance, the dangers of misclassification, the reality of dynamic risk as products evolve, the necessity of data residency planning for global products, and the implications of GPAI model obligations for foundation model selection. In Section 2, we'll move from risk framework to problem framing—how to define AI problems precisely enough that they can be solved, evaluated, and deployed safely.
