# Chapter 5.7 — Data Requirements You'll Discover Too Late

Every AI product team has the same experience around month two: someone asks a question about data, and the room goes quiet. The model is working. The prompts are good. But the data — the data nobody thought about during planning — is a mess.

Here's the data reality check you should do before you start building, so you don't discover these problems when it's expensive to fix them.

---

### The Data You Need (That Nobody Budgeted For)

**Evaluation data.** You need labeled examples to test whether your system works. Not production data — curated, annotated examples with known correct answers. For most AI products, creating this evaluation dataset is the single biggest hidden cost. Budget 2-4 weeks and significant domain expert time just for V1 of your eval set.

**Training or fine-tuning data.** If you're fine-tuning a model (which most teams shouldn't do in V1), you need hundreds to thousands of high-quality examples. This data needs to be labeled, cleaned, deduplicated, and representative of your real use case. Most teams discover their "proprietary data" is too noisy, too sparse, or too biased to be useful for training.

**Knowledge base data.** If you're building a RAG system, you need the documents, articles, or data that the system retrieves from. This sounds easy ("we have all our docs") until you realize: the docs are outdated, inconsistent, stored in five different formats, contain conflicting information, and have no metadata for filtering or prioritization.

**Production feedback data.** Once your system is live, you need to capture user interactions, feedback signals, and quality assessments. This requires logging infrastructure, consent mechanisms (especially under GDPR), and storage that can handle your query volume.

**Ground truth data.** For ongoing evaluation, you need ground truth: the objectively correct answer for each test case. Creating ground truth requires domain expertise, and maintaining it requires regular updates as your product and policies evolve.

---

### The Data Problems Nobody Warns You About

**Your data is stale.** The knowledge base hasn't been updated in six months. The eval set was created for V1 and doesn't cover features added in V2. Stale data creates stale evaluations, which create false confidence.

**Your data is biased.** Your eval set over-represents easy cases and under-represents the hard ones. Your knowledge base reflects one customer segment but not others. Your training data comes from one geography but you serve globally. Bias in your data becomes bias in your product.

**Your data has PII.** Customer support transcripts contain names, email addresses, account numbers. Medical records contain protected health information. Financial data contains account details. You can't use this data for evaluation or training without sanitization, and sanitization is harder than it sounds.

**Your data doesn't match production.** The examples you carefully crafted for evaluation don't look like what real users actually type. Your eval set has clean, well-formed questions. Real users type fragments, misspellings, and multi-part questions that don't match any of your test cases.

**You don't have enough data.** You thought you had "lots of data." Then you filtered for quality, removed duplicates, sanitized PII, and balanced across categories. Now you have 200 usable examples instead of the 2,000 you thought you had.

---

### The Data Audit

Before you build, answer these questions:

1. **Do we have evaluation data?** If no, how will we create it? Who will label it? How long will that take?
2. **Is our knowledge base current and consistent?** When was it last updated? Are there contradictions?
3. **Does our data contain PII?** If yes, what's our sanitization plan?
4. **Is our data representative?** Does it cover the full range of inputs we'll see in production?
5. **Do we have consent to use this data?** For training, evaluation, and logging?
6. **Where is the data stored and who has access?** Data residency and access control.
7. **How will we keep the data fresh?** What's the update cadence?

If you can't answer these questions with confidence, you're not ready to build. You're ready to do a data audit — which is a prerequisite to building.

---

*Next: compliance and legal requirements by industry — the requirements that exist whether you know about them or not.*
