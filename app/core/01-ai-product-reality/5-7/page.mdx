# 5.7 — Data Requirements You Will Discover Too Late

In March 2025, a legal technology company launched an AI-powered contract review tool after four months of development. The model worked well in testing. The prompts were solid. The team had validated the concept with three law firms. Two weeks before launch, someone from Product asked the ML team a simple question: "How do we know the accuracy numbers we're showing in the dashboard are correct?" The room went silent. They had accuracy numbers — 94% precision, 89% recall — but those numbers came from a test set of fifty contracts the ML engineer had labeled himself over a weekend. The contracts were all the same type. All from one client. All in English. All from 2023. The company served clients across twelve industries, in seven languages, with contracts dating back to 1998. They had built an entire product on a test set that represented less than two percent of their actual use case.

They delayed launch by six weeks to build a proper evaluation dataset. The final cost: 120 hours of lawyer time at an average billing rate of 400 dollars per hour, plus three weeks of ML engineering time to restructure the evaluation pipeline. The data problem they discovered two weeks before launch should have been identified on day one, before a single line of code was written. This pattern repeats across every AI product category. Teams build first and audit their data assumptions later, discovering foundational gaps when fixing them is expensive and disruptive.

## The Five Data Types You Need Before You Write Code

Every AI product requires five distinct types of data, and most teams budget for only one or two. You need **evaluation data** — labeled examples with known correct answers that let you measure whether your system works. You need **knowledge base data** if you're building retrieval-augmented generation, which means the corpus of documents your system searches through to find relevant context. You need **production logging data** so you can monitor quality and detect degradation after launch. You need **ground truth data** for ongoing measurement, which means the objectively correct answer for test cases, validated by domain experts. And if you're fine-tuning a model, you need **training data** — high-quality, labeled, representative examples that teach the model your specific task.

Most teams discover these requirements in reverse order. They start thinking about training data because that's what the AI tutorials focus on. Then they realize they need evaluation data to measure quality. Then they realize their knowledge base is a disaster. Then they realize they have no logging infrastructure for production. Then they realize they have no process for generating ground truth. By the time they've worked backward through all five data types, they're three months into a project that should have started with a data audit.

The evaluation data gap is the most common. Teams assume they can "just use production data" for evaluation, but production data doesn't come with labels. You don't know what the correct answer was. You can log what the model said and whether the user clicked a button, but that's not ground truth. Ground truth requires human judgment from someone who knows the right answer. For a contract review tool, that's a lawyer. For a medical triage system, that's a clinician. For a financial analysis product, that's an analyst. Domain expertise is expensive, and you need it at volume.

A healthcare technology company building a clinical documentation assistant budgeted two weeks for "data prep" in their project plan. They assumed data prep meant exporting records from their database and converting them to a format the model could read. What they actually needed was 200 physician-reviewed examples of clinical notes with verified accuracy labels, 50 examples per specialty, covering common cases and edge cases. Each note took a physician fifteen to thirty minutes to review and label. They needed 100 hours of physician time. Physician time costs between 200 and 400 dollars per hour when pulled from clinical work. The final cost of their evaluation dataset was 35,000 dollars and five weeks of calendar time coordinating physician availability. The two weeks they budgeted covered less than 20 percent of the actual requirement.

## The Knowledge Base Problem Nobody Mentions

If you're building a retrieval-augmented generation system, your knowledge base is not a technical challenge. It's an information architecture and governance challenge. You need documents that are current, consistent, structured, tagged, deduplicated, and maintained. Most companies have none of these qualities in their knowledge bases. They have documents scattered across SharePoint, Confluence, Google Drives, old wikis, and email archives. The documents contradict each other. The metadata is missing or wrong. The formatting is inconsistent. The ownership is unclear.

A financial services company in mid-2024 tried to build an AI assistant that answered employee questions about internal policies. They had "thousands of documents" according to the VP who sponsored the project. The reality: 3,000 files across five systems. When the ML team audited the corpus, they found 900 duplicates, 400 documents that were drafts or outdated versions still in circulation, 200 documents that were meeting notes someone had saved with vague filenames, and 150 documents that contained customer data that couldn't be used for training or retrieval without violating their own data handling policies. The usable, current, relevant knowledge base was 800 documents, not 3,000. The retrieval system they built returned outdated policy information 30 percent of the time because nobody had tagged documents with effective dates or deprecation markers.

Knowledge base quality is a content problem, not a technical problem. You can't fix it with better embeddings or smarter retrieval algorithms. You fix it by doing the work that content teams do: auditing, tagging, deprecating, deduplicating, and maintaining. Most AI teams have no budget for this work because it's invisible in the project plan. They allocate sprints for model development, prompt engineering, and evaluation, but they don't allocate time for content governance. Then they discover at launch that the biggest source of AI errors is not the model — it's the documents the model retrieves from.

## The PII Problem That Blocks Evaluation

Your data contains personally identifiable information, and PII is a legal and practical blocker. You can't use customer support transcripts for evaluation if they contain names, email addresses, account numbers, or anything else that identifies individuals. You can't use medical records if they contain protected health information. You can't use financial records if they contain account details. Privacy laws are strict and the penalties are significant. GDPR fines can reach 20 million euros or four percent of annual global revenue, whichever is higher. HIPAA violations can cost 50,000 dollars per record.

Most teams assume they can "just anonymize" the data. Anonymization is harder than find-and-replace. Names appear in context. Email addresses are referenced without being spelled out. Account numbers show up in descriptions. Dates of birth combined with zip codes can re-identify individuals even when names are removed. Proper anonymization requires specialized tools and review by someone who understands privacy risk. The cost is measured in weeks and tens of thousands of dollars for datasets of any meaningful size.

An insurance technology company in late 2024 spent six weeks trying to sanitize claims data for use in model evaluation. They hired a consultant who specialized in healthcare data privacy. The consultant's first question was whether they had consent from claimants to use the data for AI development. They didn't. They had consent to process claims, but consent doesn't automatically extend to secondary uses like model training or evaluation. The legal team determined they couldn't use the data at all without going back to claimants for additional consent, which was impractical for historical data. The project switched to using synthetic data, which introduced new problems around representativeness and realism. The six weeks they spent trying to sanitize real data was wasted effort that should have been identified in week one with a basic privacy impact assessment.

## The Production Data Pipeline You Didn't Plan For

Once your system is live, you need to capture every query, every model response, every user interaction, and every feedback signal. This data feeds your ongoing evaluation and monitoring. It's how you detect quality degradation. It's how you find gaps in your evaluation dataset. It's how you measure real-world performance versus test set performance. But capturing this data requires infrastructure that most teams don't build until after launch.

You need logging that handles your query volume without adding latency. You need storage that scales to millions of interactions. You need consent mechanisms because logging user interactions is data processing under GDPR and similar laws. You need access controls so only authorized team members can view production logs. You need retention policies so you're not storing data longer than legally permitted. You need export pipelines so you can pull samples for review. You need anonymization if the logs contain PII.

A customer service platform launched an AI agent in early 2025 with basic logging that captured query text and response text. Three weeks after launch, they wanted to analyze why users were rating certain responses poorly. They had the ratings, but they didn't have the conversation context. They hadn't logged the previous five turns in the conversation. They hadn't logged which knowledge base documents were retrieved. They hadn't logged the prompt sent to the model. They had individual data points but not the full trace needed to diagnose issues. They spent two weeks rebuilding the logging infrastructure and lost the opportunity to learn from the first three weeks of production data.

## The Ground Truth Problem That Never Ends

Ground truth is not a one-time data collection task. It's an ongoing operational process. You need ground truth to validate your evaluation metrics. You need ground truth to detect drift. You need ground truth to verify that your automated quality checks are still accurate. Ground truth requires domain experts, and domain experts are expensive and hard to schedule at the frequency you need.

A legal research platform needed lawyer review of AI-generated case summaries. They budgeted for an initial evaluation dataset of 100 examples. They got the 100 examples reviewed and launched. Then they discovered they needed fresh ground truth every month to verify the model wasn't degrading as case law evolved. They needed ground truth for new edge cases users discovered. They needed ground truth when they updated the prompt or changed providers. The ongoing cost of ground truth was five times the initial evaluation dataset cost, and it was a permanent operational expense, not a one-time project cost.

The assumption that you collect data once and you're done is wrong for AI products. Data requirements are continuous. Your evaluation dataset needs to evolve as your product evolves. Your knowledge base needs updates as information changes. Your production logs need ongoing review to catch new failure modes. Ground truth needs regular refresh to stay valid. Most teams budget for data as a project phase, not as an operational function. Then they're surprised when data becomes their highest ongoing cost after model API fees.

## The Representativeness Gap

Your evaluation data needs to represent the full distribution of inputs you'll see in production. Most evaluation datasets don't. They over-represent the easy cases and under-represent the hard ones. They cover common scenarios and miss edge cases. They reflect one type of user and ignore others. This happens because creating evaluation data is expensive, and teams naturally focus on the cases they understand best and can label most quickly.

A fintech startup built an AI that answered questions about investment accounts. Their evaluation dataset had 150 examples, all carefully crafted by the product manager. The examples were clear, well-formed questions like "What was my account balance on March 15?" Real users didn't ask clear, well-formed questions. They asked "how much do I have," "what's my balance," "why did my balance go down," "did my deposit go through," and questions that mixed multiple intents like "what's my balance and when is my next fee." The evaluation dataset had zero examples of these variations. The model scored 92 percent on the evaluation set and 67 percent on real production queries in the first week after launch. The evaluation set wasn't representative, so the evaluation results were misleading.

Representativeness requires deliberate effort. You need to sample across user types, query types, difficulty levels, and edge cases. You need to include the messy, ambiguous, poorly phrased inputs that real users produce, not just the clean examples that are easy to label. This means your evaluation data collection process needs to include real production samples, not just manufactured test cases. But production samples contain PII, which brings you back to the sanitization problem. The cycle repeats.

## The Data Audit You Should Run Before You Build

Before you write any code, before you draft any prompts, before you select a model provider, you need to audit your data position. The audit has seven questions, and if you can't answer all seven with specifics, you're not ready to build. You're ready to do data work.

First: do we have evaluation data, and if not, how will we create it? Not "we'll create some examples." Specifically: who will label them, how many do we need, how long will it take, what will it cost, and how will we ensure the examples are representative? Second: is our knowledge base current and consistent? When was it last updated? Are there contradictions? What's the deprecation process for outdated information? Third: does our data contain PII, and if so, what's our sanitization plan and do we have consent to use it for AI development?

Fourth: is our evaluation data representative of production? Does it cover the full range of inputs, user types, and edge cases we'll see? Fifth: do we have infrastructure to log production interactions, and have we addressed consent, access control, and retention policies? Sixth: how will we generate ongoing ground truth, who will do it, and what's the budget? Seventh: what's our data refresh cadence for evaluation sets, knowledge bases, and ground truth, and who owns that process?

If any of these questions produce vague answers or uncertainty, that's your first work item. Data clarity comes before model selection, before architecture decisions, before sprint planning. The teams that treat data as a prerequisite ship faster and with fewer delays than the teams that treat data as something they'll "figure out along the way." Data problems discovered late are expensive to fix. Data problems identified early are just tasks in the project plan.

---

*Next: compliance and legal requirements by industry — the regulations that apply to your product whether you know about them or not.*
