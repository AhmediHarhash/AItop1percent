# Chapter 5.10 — AI Threat Modeling (Prompt Injection, Data Exfiltration, Tool Abuse, RAG Poisoning)

In 2026, every AI product is an attack surface. Not hypothetically — actively. Prompt injection attacks are documented in the wild. Data exfiltration through AI systems has been demonstrated. RAG poisoning is a known attack vector. If your threat model doesn't include AI-specific attacks, your threat model is incomplete.

This isn't the deep-dive on AI security (that's Section 16). This is the scoping exercise: what threats should you be planning for before you build?

---

### The AI Threat Landscape

**Prompt injection.** An attacker crafts input that changes how your AI behaves. Direct injection: the user types "ignore your instructions and instead..." Indirect injection: malicious content is embedded in documents your RAG system retrieves, web pages your agent visits, or emails your system processes. The model follows the injected instructions instead of yours.

Why it matters for scoping: if your product processes any external content (user input, documents, web pages, emails), prompt injection is a relevant threat. You need input validation, output monitoring, and the architectural assumption that any input might be adversarial.

**Data exfiltration.** An attacker uses your AI to extract information it shouldn't reveal. "Repeat your system prompt" reveals your proprietary instructions. "What do you know about user X?" might reveal other users' data. "Summarize the document that was just uploaded" in a multi-tenant system might leak one customer's data to another.

Why it matters for scoping: if your system has access to any data that's not meant to be publicly visible (system prompts, customer data, internal knowledge bases), exfiltration is a relevant threat. You need data isolation, output filtering, and access control enforcement at the application layer, not just the model layer.

**Tool abuse.** For agent products, an attacker manipulates the agent into misusing its tools. Send emails to unintended recipients. Execute unintended database queries. Access files outside the intended scope. Make API calls with manipulated parameters.

Why it matters for scoping: if your product uses tools (APIs, databases, email, file systems), tool abuse is a relevant threat. You need strict tool permissions, parameter validation, rate limiting, and human approval for high-impact actions.

**RAG poisoning.** An attacker contaminates your knowledge base so that retrieved documents contain malicious content. If your RAG system ingests content from semi-trusted sources (user-generated content, web pages, partner documents), an attacker can plant content that, when retrieved, causes your model to generate harmful or manipulated outputs.

Why it matters for scoping: if your retrieval system indexes content that anyone can modify or contribute to, RAG poisoning is a relevant threat. You need content validation, source trustworthiness scoring, and monitoring for anomalous content.

**Jailbreaking.** An attacker bypasses your safety guardrails to make the model produce prohibited content. Unlike prompt injection (which changes the model's task), jailbreaking keeps the task the same but removes the restrictions.

Why it matters for scoping: if your product has any safety restrictions (and it should), jailbreaking attempts will occur. You need layered defenses: system-prompt-level restrictions, input classifiers that detect jailbreak patterns, and output classifiers that catch prohibited content.

---

### The Scoping Threat Model

Before you build, fill in this matrix for your product:

For each threat type, answer:
- **Relevant?** Does this threat apply to our product? (Yes/No/Partially)
- **Impact?** If this attack succeeds, what's the worst-case outcome? (Low/Medium/High/Critical)
- **Mitigation plan?** What will we build to defend against it?
- **Detection plan?** How will we know if this attack is happening?

You don't need to solve every threat before V1. But you need to know which threats exist, which are high-priority, and what your plan is.

---

### The Minimum Security Bar

At any risk tier, your AI product should have:

1. **Input validation.** Don't pass raw user input directly to the model without sanitization.
2. **Output monitoring.** Check model outputs for prohibited content, data leakage, and anomalous behavior.
3. **Least privilege.** Give the model (and agents) only the permissions they need. Nothing more.
4. **Rate limiting.** Cap the number of requests per user, per session, and per time period.
5. **Logging.** Record all inputs and outputs for security review and incident investigation.

Section 16 goes deep on implementation. For now, plan for these five elements in your architecture from day one.

---

*Next: SLOs for AI products — translating chaos into commitments.*
