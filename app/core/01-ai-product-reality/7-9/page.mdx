# Chapter 7.9 — After Launch: The Iteration Cycle That Builds Great AI Products

Launch is the beginning, not the end. The best AI products aren't great at launch — they become great through disciplined iteration. The difference between a product that stagnates and a product that compounds in quality is the iteration cycle you run after day one.

---

### The Weekly Iteration Cycle

Great AI teams run a weekly cycle that looks like this:

**Monday: Review.** Look at the past week's production data. Review quality metrics. Read user feedback. Examine the 10 worst interactions from the week. Identify the top 3 failure patterns.

**Tuesday-Wednesday: Fix.** For each failure pattern, decide the right fix: prompt improvement, additional few-shot examples, better input handling, a new guardrail, or an eval gap that needs closing. Implement the fixes.

**Thursday: Evaluate.** Run the updated system against your eval suite. Verify the fixes improved quality on the failure cases without degrading quality on existing cases. This is your regression test.

**Friday: Deploy.** If evaluation shows improvement without regression, deploy the changes. If not, hold and investigate. Update your eval dataset with new examples from the week's production data.

This cycle means your product improves 50 times per year. Over 12 months, the compound effect is transformative.

---

### What to Iterate On (In Priority Order)

**1. Fix the top failure modes.** Every week, your worst interactions reveal where the product breaks. Fix the most common and most severe failures first. This has the highest impact per hour of work.

**2. Expand the eval set.** Add real production examples to your evaluation dataset every week. Especially add examples that expose failures. Over months, your eval set grows from 50 examples to 500 to 5,000 — each one making your evaluation more comprehensive and your product more robust.

**3. Improve prompts.** Prompt engineering is never done. As you discover new failure patterns, you refine instructions, add examples, tighten constraints, and adjust the prompt architecture. Small prompt improvements compound over time.

**4. Refine the quality bar.** Your initial rubric was a best guess. After seeing real production data, you'll realize some criteria are too strict, others too lenient, and some are missing entirely. Update your rubric to reflect what "good" actually means for your users.

**5. Expand coverage.** Once your core use case is stable, expand to adjacent use cases. If you launched with the top 10 customer questions, expand to the top 50. Each expansion should follow the same evaluate-then-deploy discipline.

---

### The Quarterly Reset

Every quarter, step back from the weekly cycle and ask bigger questions:

- **Is our quality improving?** Plot your eval scores over time. If the line isn't going up, your iteration cycle has a problem.
- **Are we measuring the right things?** Production reality might have revealed that your metrics miss important quality dimensions. Update them.
- **Should we change models?** New model releases happen regularly. Evaluate whether a newer or different model improves quality, reduces cost, or both.
- **What's the next big quality leap?** Weekly iterations produce incremental improvement. Quarterly, identify what would produce a step-change: fine-tuning, RAG, a fundamentally different architecture, or new data sources.

---

### The Long Game

AI products are living systems. They don't ship and stabilize like traditional software — they ship and evolve. The team that builds the tightest iteration loop, the richest evaluation dataset, and the most disciplined weekly cadence will outperform every competitor. Not because they started smarter, but because they improved faster.

Your first launch is version 0.1. The product your users love is version 0.47, shaped by hundreds of small improvements driven by data, not guesses.

---

*That wraps up Chapter 7 and Section 1 — AI Product Reality & Risk Tiers. You now have the complete foundation: what AI products are, the categories they fall into, how to assess risk, how to build the right team, how to scope and define requirements, how to navigate the market, and how to go from idea to your first shipped product. In Section 2, we'll dive into the evaluation core — Ground Truth & Dataset Design — because the quality of your product is only as good as the quality of your evaluation.*
