# Chapter 4.1 — Who Owns What: PM, Eng, ML, QA, Legal

The single biggest source of confusion in AI product teams isn't technical. It's organizational. Nobody knows who's responsible for what, because AI products create responsibilities that don't exist in traditional software — and they fall into gaps between existing roles.

Let me map the territory.

---

### The Ownership Gap

In traditional software, ownership is clear. The PM defines what to build. Engineering builds it. QA tests it. Legal reviews contracts. Everyone has a lane.

AI blows up these lanes. Who decides what quality level is acceptable? The PM? The ML engineer? The business? Who owns the eval set — is that a QA responsibility or an ML responsibility? Who decides when to ship — is it the PM's call if the ML team says the model isn't ready? Who's responsible when the AI says something wrong to a customer — engineering, product, or legal?

These questions don't have default answers. Every AI team needs to explicitly decide them, or they'll get answered by whoever shouts loudest during the incident.

---

### The RACI for AI Products

Here's how ownership typically shakes out on high-performing AI teams:

**Product Manager:**
- Owns the problem definition and success criteria
- Defines what "good enough" means in business terms
- Owns the tradeoff between quality, cost, and speed
- Makes the ship/no-ship decision (with input from ML and legal)
- Communicates AI capabilities and limitations to stakeholders

**ML/AI Engineer:**
- Owns model selection, prompting, fine-tuning, and architecture
- Owns the eval framework — building, maintaining, and running evaluations
- Provides quality metrics and recommendations to the PM
- Owns monitoring and alerting for model performance
- Flags when quality isn't meeting the bar

**Software Engineer:**
- Owns the application layer: API design, integration, infrastructure, deployment
- Owns reliability: uptime, latency, scaling, error handling
- Builds the pipeline that connects the model to the product
- Implements safety filters, rate limiting, and abuse prevention

**QA/Evaluation Specialist:**
- Owns the eval dataset: creation, maintenance, and quality assurance of test cases
- Runs regression testing before releases
- Conducts exploratory testing for edge cases
- Works with domain experts to validate quality standards

**Legal/Compliance:**
- Reviews AI outputs for legal risk (terms of service, liability, IP)
- Ensures regulatory compliance (EU AI Act, HIPAA, etc.)
- Drafts user-facing disclaimers and transparency notices
- Reviews data handling practices for privacy compliance

**Domain Expert (when applicable):**
- Validates that AI outputs are correct within the professional domain
- Creates and reviews ground truth for evaluation
- Identifies domain-specific failure modes
- Provides calibration for quality standards

---

### The Three Debates Every Team Has

**Debate 1: Who owns quality?**
The right answer: everyone, but with specific accountability. The ML engineer owns the quality measurement infrastructure. The PM owns the quality target. QA owns the eval data. Domain experts own the ground truth. And ultimately, the PM makes the call on whether quality is sufficient to ship — but that call must be informed by data from the ML team, not by vibes.

**Debate 2: Who decides to ship?**
The right answer depends on your risk tier. At Tier 1, the PM can decide. At Tier 2, PM and ML lead jointly. At Tier 3, PM, ML lead, domain expert, and legal must all sign off. At Tier 4, add compliance. Never let a single person decide to ship a Tier 3+ product.

**Debate 3: Who's on-call for AI quality issues?**
The right answer: the ML/AI engineer, with an escalation path to the PM for business-impact decisions and to the domain expert for domain-specific issues. Traditional on-call for infrastructure goes to software engineers. AI quality on-call is a separate rotation.

---

### Writing It Down

Whatever ownership model you choose, write it down. Put it in a document that every team member reads. Review it quarterly. Update it when roles change or the product's risk profile evolves.

The teams that fail at AI aren't the ones who make wrong ownership decisions. They're the ones who never make the decisions at all.

*Next: the AI Product Manager role in 2026 — what's changed and what's required.*
