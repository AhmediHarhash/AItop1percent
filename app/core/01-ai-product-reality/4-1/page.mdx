# 4.1 — Who Owns What: PM, Eng, ML, QA, Legal

In August 2025, a legal technology company launched an AI-powered contract review tool that promised to identify missing clauses and flag legal risks in customer agreements. The product worked beautifully in demos. Three weeks after launch, a mid-sized customer used it to review a seven-figure distribution contract, signed based on the AI's green light, and later discovered they'd agreed to unlimited liability exposure. The customer sued. The post-mortem revealed that nobody on the product team had actually been responsible for validating the legal accuracy of AI outputs. The PM thought the ML engineer was checking. The ML engineer thought the legal team was reviewing. Legal thought they were just advising on terms of service, not reviewing every AI decision. The responsibility gap cost the company $1.8 million in settlement, another $400,000 in legal fees, and the customer relationship.

The failure wasn't technical. The model worked exactly as designed. The failure was organizational: nobody owned the decision about whether this AI was actually safe to ship for high-stakes legal work. Traditional software ownership models assume clear boundaries. AI products create responsibilities that fall into the gaps between existing roles, and if you don't explicitly assign them, they don't get done.

## The Ownership Collapse in AI Products

In traditional software development, ownership is unambiguous. Product Managers define what to build and why. Engineering builds it according to spec. QA tests whether it works as specified. Legal reviews contracts, compliance, and intellectual property. Operations handles deployment and uptime. Every function has a lane, and those lanes rarely overlap in confusing ways.

AI products destroy this clarity. Consider a seemingly simple question: who decides what quality level is acceptable for an AI feature? In traditional software, quality is binary. A button either works or it doesn't. A calculation is either correct or it's not. In AI, quality is a distribution. The system gets the right answer 87% of the time on your evaluation set. Is that good enough to ship? Who makes that call?

You might think the PM makes that call, because PMs own product decisions. But the PM doesn't understand what 87% really means without context from the ML team about what's driving failures, what improving it would cost, and what the realistic ceiling is. You might think the ML engineer makes that call, because they understand model performance. But the ML engineer doesn't understand the business impact of the 13% failure rate without input from the PM about how users will encounter these errors and what the downstream consequences are.

The answer is that both the PM and the ML engineer need to make this decision together, with input from domain experts if the task has specialized knowledge requirements, and sign-off from Legal if the failures carry liability risk. But most teams don't establish this joint decision-making process explicitly. They assume someone is responsible, discover during a crisis that nobody was, and then spend the post-mortem assigning blame instead of fixing the structural gap.

Here's another responsibility that doesn't map to traditional roles: who owns the evaluation dataset? In traditional software, QA owns test cases. But in AI products, the evaluation dataset is both a testing artifact and a product specification. It defines what good looks like. It shapes how the model is optimized. It determines what quality metrics you can even measure. Is that a QA responsibility, an ML responsibility, or a product responsibility? Different teams answer this differently, and there's no universally correct answer, but the teams that fail are the ones that never explicitly answer it at all.

Who's responsible when the AI says something factually wrong to a customer? In traditional software, if a bug causes customer harm, Engineering is responsible for the bug, Product is responsible for the decision to ship, and the company is responsible for the outcome. In AI products, wrongness isn't a bug. It's a probability. The system will sometimes be wrong. You knew that when you shipped. So who's responsible for deciding that the probability of wrongness was acceptable for this use case? Who's responsible for explaining that to the customer? Who's responsible for monitoring whether wrongness rates are increasing in production? These are distinct responsibilities, and they fall across PM, ML, Engineering, Support, and Legal.

The teams that succeed at AI products are the ones who map these responsibilities explicitly before they build anything. The teams that fail are the ones who assume existing roles will naturally extend to cover AI-specific responsibilities.

## The Ownership Map for High-Performing AI Teams

You need a clear division of labor. Here's how ownership typically distributes across roles on teams that ship AI products successfully and don't spend half their time in meetings arguing about whose job something is.

The Product Manager owns the problem definition and success criteria. They define what business problem the AI is solving, what success looks like in measurable terms, and what tradeoffs are acceptable between cost, speed, and quality. They decide what level of AI performance is good enough to ship for this use case, but that decision must be informed by data and recommendations from the ML team, not made in a vacuum. They own stakeholder communication about AI capabilities and limitations, which means they need to understand the system well enough to translate technical realities into business language. They make the final ship or no-ship decision for lower-risk products, and they co-own that decision with ML and Legal for higher-risk products.

The ML or AI Engineer owns model selection, architecture, prompting, fine-tuning, and all technical decisions about how the AI system works. They own the evaluation framework: designing it, building it, maintaining it, running it, and interpreting results. They provide quality metrics and improvement recommendations to the PM. They own monitoring and alerting for model performance in production, which means they're the first to know when quality degrades. They flag when the system isn't meeting the quality bar, and they propose what it would take to improve. They don't make the business decision about whether to ship, but they provide the technical evidence that informs that decision.

The Software Engineer owns the application layer that wraps the AI. They design and build APIs, handle integration with the rest of the product, manage infrastructure, implement deployment pipelines, and own reliability concerns like uptime, latency, and scaling. They build the connective tissue between the model and the user. They implement safety filters, rate limiting, input validation, and abuse prevention mechanisms. They own error handling, fallback logic, and graceful degradation when the AI isn't available or isn't confident. They ensure the product works reliably even when the AI doesn't work perfectly.

The QA or Evaluation Specialist owns the evaluation dataset as an artifact. They create test cases, maintain the dataset over time as the product evolves, ensure the eval set remains representative of real-world usage, and coordinate with domain experts to validate ground truth labels. They run regression testing before releases to ensure new changes don't break existing capabilities. They conduct exploratory testing to find edge cases and failure modes that weren't covered in the formal evaluation. On mature teams, this is a distinct role. On early-stage teams, it's often shared between the AI engineer and the PM.

Legal and Compliance own risk assessment and regulatory adherence. They review AI outputs for legal exposure, especially in regulated domains like healthcare, finance, and hiring. They ensure compliance with relevant regulations: GDPR for privacy, HIPAA for healthcare data, the EU AI Act for high-risk systems, SOX for financial controls. They draft user-facing disclaimers, transparency notices, and terms of service that accurately represent what the AI does and doesn't do. They review data handling practices to ensure you're not creating liability through how you collect, store, or use training or inference data. For high-risk products, Legal has veto power on shipping decisions.

Domain Experts own correctness within their field of expertise. If you're building medical AI, doctors validate that outputs are medically sound. If you're building legal AI, lawyers validate legal correctness. If you're building financial AI, financial analysts validate the numbers. Domain experts create and review ground truth for evaluation. They identify failure modes that only someone with specialized knowledge would recognize. They calibrate quality standards: what level of accuracy is acceptable in this domain, what kinds of errors are tolerable, and what kinds are catastrophic. For high-risk domains, domain expert sign-off is non-negotiable before shipping.

## The Three Ownership Debates Every Team Has

Every AI product team eventually has the same three arguments about ownership. You can have these arguments proactively and resolve them with documentation, or you can have them reactively during a crisis and resolve them with blame. The successful teams choose the former.

**Debate one: who owns quality?** The wrong answer is to assign it to a single role. Quality in AI products is multi-dimensional, and different roles own different dimensions. The ML engineer owns the measurement infrastructure: the evaluation framework, the metrics, the dashboards that show quality over time. The PM owns the quality target: defining what threshold is good enough for this use case, and making the call on whether current quality meets that bar. The QA specialist owns the evaluation data: ensuring test cases are comprehensive, representative, and correctly labeled. Domain experts own ground truth: determining what the right answer actually is for complex cases. Engineering owns reliability: ensuring the system fails gracefully when quality is low.

The synthesis is that quality is a shared responsibility with specific accountability for each dimension. When quality is insufficient, the ML engineer is accountable for diagnosing why and proposing solutions. The PM is accountable for deciding whether to fix it now or ship anyway with mitigations. QA is accountable for ensuring the eval set catches the problems before production does. Domain experts are accountable for validating that fixes actually improve correctness. Nobody owns quality alone, but everyone owns their piece of it, and the PM is the final integrator who weighs all inputs and makes the shipping call.

**Debate two: who decides to ship?** The answer depends on your risk tier. For Tier 1 products where AI failures cause minor inconvenience and no material harm, the PM can make the shipping decision unilaterally, informed by data from the ML team. For Tier 2 products where failures cause business disruption or moderate user harm, the PM and ML lead jointly decide, and both need to agree that quality is sufficient. For Tier 3 products where failures cause significant financial loss, reputational damage, or user harm, you need sign-off from PM, ML lead, domain expert, and Legal. For Tier 4 products in heavily regulated domains where failures could cause severe harm or regulatory penalties, add Compliance to the sign-off list.

The failure mode is letting a single person decide to ship a high-risk product. It's not that PMs are reckless or ML engineers are too cautious. It's that no single person has enough context across all the dimensions that matter. The PM understands business impact but might not grasp the technical limitations. The ML engineer understands model behavior but might not grasp regulatory exposure. Legal understands liability but might not grasp how users will actually interact with the feature. You need all perspectives in the room for high-stakes decisions.

**Debate three: who's on-call for AI quality issues?** This is distinct from traditional on-call for infrastructure reliability. If your API goes down, that's a software engineering problem, and software engineers are on-call. If your AI starts producing wrong answers at a higher rate than normal, that's an AI quality problem, and the ML or AI engineer should be on-call. If quality degrades badly enough to impact business metrics or user safety, the on-call ML engineer escalates to the PM for business-impact decisions and to domain experts for domain-specific validation.

Many teams make the mistake of putting software engineers on-call for everything, including AI quality issues. The problem is that software engineers can diagnose and fix infrastructure problems but often can't diagnose or fix model behavior problems. You need the person who understands why the model behaves the way it does to be the first responder for quality incidents. That person might determine the root cause is actually infrastructure—a timeout causing truncated prompts, for example—and then loop in software engineering. But the initial triage needs to come from someone who understands the AI system.

## Writing Down the Ownership Model

Whatever ownership model you choose, and it will vary based on your team size, risk profile, and organizational structure, you need to write it down. Create a document that maps every AI-specific responsibility to a role. Be specific. Not "ML engineer owns quality" but "ML engineer owns evaluation framework design and implementation, quality metric reporting, production monitoring, and recommendations for quality improvements." Not "PM makes shipping decisions" but "PM makes shipping decisions for Tier 1 and 2 products, and co-owns shipping decisions with ML lead and Legal for Tier 3 and 4 products."

Make every new team member read this document during onboarding. Review it quarterly as a team and update it when roles change, when the product's risk profile evolves, or when you discover gaps that caused problems. Treat it as a living document, not a one-time exercise.

Include escalation paths. When the ML engineer flags a quality issue, who do they escalate to? When Legal identifies a compliance risk, who makes the call on whether to delay shipping? When a domain expert disagrees with the ML team's assessment of correctness, who mediates? These escalation paths prevent deadlock and ensure decisions get made even when there's disagreement.

Include decision-making protocols for common scenarios. What's the process for deciding to retrain or update a model? What's the process for adding new features that change the AI's behavior? What's the process for responding to user reports of bad AI outputs? If you document these processes in advance, you don't need to reinvent them under pressure during incidents.

The teams that fail at AI products aren't the ones who choose the wrong ownership model. There's no single correct model, and different team structures and company cultures will lead to different choices. The teams that fail are the ones who never explicitly choose an ownership model at all. They assume traditional software roles will naturally extend to cover AI-specific responsibilities, and they discover during a crisis that the responsibilities fell through the cracks because nobody thought they were theirs to own.

Make ownership explicit. Write it down. Enforce it. Update it. That's the difference between an AI team that ships successfully and an AI team that spends all its time in meetings arguing about whose job something is.

The next question, once you've mapped ownership, is what each role actually does in practice. The AI Product Manager role has evolved significantly in 2025 and 2026, and most companies are still hiring for the old version.
