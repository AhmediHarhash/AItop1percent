# 7.2 â€” Your First Evaluation: Turning Gut Feeling Into Numbers

In June 2025, a healthcare technology company launched an AI tool that summarized patient notes for physicians. The team had tested it extensively during development. It "seemed pretty good" to the product manager. The outputs "looked reasonable" to the engineers. The physicians who tried it in beta said it was "helpful." Three weeks after launch, the medical director pulled the tool offline. Physicians had been relying on summaries that missed critical clinical details in 18% of cases. The team had never measured miss rates. They had never defined what "critical clinical details" meant. They had never counted how often the AI failed in specific, measurable ways. Their quality assessment was a feeling, not a number, and feelings are a terrible foundation for products where accuracy matters.

The team's mistake was not that they didn't care about quality. They cared deeply. Their mistake was that they never turned that care into measurement. "It seems good" is not an evaluation. "It passes 83% of cases on our factual accuracy criteria" is an evaluation. The difference is the difference between shipping a product that works and shipping a product that you hope works.

## The Evaluation Mindset

Evaluation is not a phase that comes later, after you've built the product. Evaluation is the second thing you do, right after your first prototype. Before you write production code, before you design the interface, before you plan the architecture, you establish a way to measure quality. This measurement becomes the foundation of every decision you make for the next year.

Most teams resist early evaluation because it feels premature. "We don't even know what we're building yet, how can we evaluate it?" But that's backward. You can't know what you're building until you can measure it. The act of defining evaluation criteria forces you to clarify what good means. What counts as a successful output? What counts as a failure? What's the threshold between acceptable and unacceptable? These questions have answers, and the answers shape your product.

Without measurement, you're navigating by intuition. Intuition works when the problem is familiar and the feedback is immediate. AI products are neither. The problem is new, the failure modes are subtle, and the feedback is delayed. Your gut feeling about whether a summarization is good might be completely wrong. The only way to know is to measure.

## The Fifty-Example Evaluation

Your first evaluation should cover 50 examples. Not five, not 500. Fifty. Five examples are too few to see patterns. You might get lucky and pick five easy cases, or unlucky and pick five edge cases, and either way your assessment will be wrong. Five hundred examples are too many for a first evaluation. You don't have production infrastructure yet, you don't have automated scoring yet, and you don't need statistical significance yet. You need directional information fast. Fifty examples give you enough signal to make decisions without drowning in annotation work.

Start by collecting 50 real inputs that represent your use case. Pull them from production data if you have it. Pull them from historical records, customer conversations, support tickets, documents, databases, wherever your real inputs live. If you don't have production data yet, create realistic simulations based on your domain knowledge. The key is that these inputs must represent the actual diversity of what users will send you.

Diversity matters more at this stage than volume. You want easy cases, hard cases, edge cases, and everything in between. You want inputs from different user types, different domains, different complexity levels. If you're building a contract analyzer, you want simple two-page agreements and complex fifty-page enterprise contracts. If you're building a support chatbot, you want straightforward questions and ambiguous complaints. The goal is not to make your accuracy look good. The goal is to find out where the system breaks so you can decide whether those breaks are fixable.

Aim to include inputs that represent your expected production distribution. If 70% of your production queries will be simple and 30% will be complex, your evaluation set should roughly match that distribution. This representative sampling ensures your accuracy metrics predict production performance rather than overstating or understating it.

Run all 50 inputs through your prototype. Save every input-output pair. You need the full record because you're going to judge these outputs multiple times as your evaluation criteria evolve, and you don't want to regenerate them each time.

## Defining the Rubric

Before you judge a single output, you write down what "good" means. This is your evaluation rubric. It's a set of criteria, each with a clear definition and a simple rating scale. For a summarization product, your rubric might have four criteria. First, completeness: does the summary include all critical information from the source? Second, accuracy: are there any factual errors or misrepresentations? Third, conciseness: is the summary within the target length? Fourth, clarity: is the summary understandable to the target audience? Each criterion gets rated on a three-point scale: pass, partial, or fail.

Your rubric should have three to five criteria. Fewer than three and you're not measuring enough dimensions of quality. More than five and you're making the evaluation too complex to execute consistently. Each criterion should be specific enough that two people can apply it to the same output and reach the same conclusion most of the time. "The summary is good" is not specific enough. "The summary includes all information marked as high-priority in the source document" is specific enough.

The criteria should match what matters to your users. If users care most about factual accuracy, accuracy should be a criterion. If they care about tone, tone should be a criterion. If they don't care about output length, don't include length as a criterion. The rubric should measure what makes an output valuable in your specific context, not what sounds important in the abstract.

The rating scale should be simple. Pass/fail works for some criteria. A three-point scale (good, acceptable, poor) works for others. A five-point scale gives you more granularity but also more room for inconsistency. Start simple. You can always add nuance later. The goal is to make judgments quickly and consistently, not to capture every possible dimension of quality with perfect precision.

Write this rubric down. Put it in a document. Share it with everyone who will be doing evaluation. The rubric serves two purposes. First, it makes your evaluation consistent. If quality criteria live in your head, they shift. You're stricter on Monday than Friday. You're harsher in the morning than after lunch. A written rubric keeps the standard stable. Second, it makes your quality bar legible to stakeholders. When you tell leadership that your accuracy is 85%, they can read the rubric and understand what accuracy means in your context.

The act of writing the rubric also forces clarity about your product requirements. You can't write clear evaluation criteria without knowing what the product needs to do and what success looks like. Teams that struggle to write a rubric usually struggle because they haven't defined their product clearly. The rubric is a product definition tool disguised as an evaluation tool.

## The Judging Process

Now you judge all 50 outputs against the rubric. You go through each input-output pair, apply each criterion, assign a rating, and record the result. This is manual work. You are a human looking at AI outputs and making judgments. There's no shortcut at this stage. Automated evaluation comes later, after you've established what good looks like and validated that automated metrics correlate with human judgment.

If possible, have two people judge independently. Two perspectives catch things one perspective misses. Where the two judges agree, you have confidence in the rating. Where they disagree, you have ambiguity that needs resolution. Those disagreements are valuable. They reveal places where your rubric is unclear, where the task itself is subjective, or where domain expertise matters. You sit down, discuss the disagreement, clarify the rubric, and re-rate.

Keep track of the disagreements. If two judges disagree on 40% of examples, your rubric needs work. If they disagree on 10% of examples, that's normal variation in judgment. The disagreement rate tells you how well-defined your task is and how clear your criteria are. High disagreement means you need to invest in clearer criteria or accept that the task has inherent subjectivity.

This process takes time. Judging 50 examples with a four-criteria rubric might take three hours. That feels slow when you're eager to build. But this is the most important three hours of your project. These 50 judgments tell you whether your product can work. They establish the baseline you'll measure all future improvements against. They reveal the failure modes you need to address. Three hours now saves you three months later.

Do the judging in one session if possible. Spreading it across multiple days introduces variability as your judgment and mood change. Block off a few hours, eliminate distractions, and work through all 50 examples with consistent focus. The quality of your evaluation depends on the consistency of your judgment.

## The Baseline Number

After you've judged all 50 examples, you calculate your baseline accuracy. What percentage of outputs met your quality bar? If your rubric has multiple criteria, you might need multiple baseline numbers. Overall pass rate, per-criterion pass rate, average score, whatever metric makes sense for your use case. The specific metric matters less than the act of calculating it and writing it down.

This number is your day-zero baseline. You document it with the date, the model you used, the prompt version, and the evaluation rubric. This documentation is essential because you're going to reference this baseline for months. When you improve your prompt, you'll measure the new accuracy against this baseline. When you add retrieval, you'll measure against this baseline. When you switch models, you'll measure against this baseline. The baseline is the foundation of your entire quality narrative.

Document everything about how you calculated the baseline. Who did the judging? What instructions did they follow? Were there any edge cases that required special handling? This documentation ensures you can replicate the measurement later. Six months from now, when you want to compare your production system to the initial prototype, you need to know exactly how the baseline was measured so you can measure the same way.

If your baseline is above 80%, you have a viable product. The core task works well enough that you can focus on improving the edge cases and building production infrastructure. If your baseline is between 50% and 80%, you have potential but the product isn't ready for users. You need to improve the core approach before you invest in scaling it. If your baseline is below 50%, the current method isn't working and you need fundamental changes. Either the task is too hard for the model, your prompt needs complete rethinking, or you need a different architecture.

These thresholds aren't universal. They depend on your domain and your risk tolerance. A 70% accuracy rate is unacceptable for medical summarization but might be fine for a creative writing tool. A 90% accuracy rate is mandatory for legal document extraction but might be overkill for marketing copy generation. The thresholds you choose depend on what happens when the AI is wrong. High-stakes failures require higher baselines.

## What the Failures Tell You

Don't just count the failures. Analyze them. Group them by pattern. Do the failures cluster around specific input types? Does the AI struggle with long documents but handle short ones well? Does it fail on technical jargon but succeed on plain language? Does it make certain types of errors repeatedly, like hallucinating dates or misidentifying entities?

These patterns tell you what to fix next. If the AI struggles with long documents, you might need chunking strategies or a different model with a larger context window. If it fails on jargon, you might need domain-specific examples in your prompt or a retrieval step that provides glossary definitions. If it hallucinates dates, you might need output validation that checks date formats or a prompting technique that emphasizes accuracy over fluency.

The failure patterns also reveal whether the task is well-defined. If failures are evenly distributed across all input types with no clear pattern, the task itself might be too subjective or ambiguous for consistent AI performance. If failures cluster in predictable ways, you have a defined problem you can solve with targeted improvements.

Some failure patterns indicate fundamental limitations. If the AI fails on inputs that require real-time information it couldn't have in its training data, you need retrieval. If it fails on inputs that require specialized expertise beyond general knowledge, you need fine-tuning or expert-in-the-loop workflows. If it fails on inputs that require multi-step reasoning, you might need chain-of-thought prompting or an agentic architecture. The failure pattern points to the solution.

The failure analysis also tells you what to measure in your ongoing evaluation. If hallucination is your primary failure mode, your evaluation framework needs fact verification. If tone inconsistency is the problem, you need tone scoring. If the AI sometimes misses key information, you need completeness checks. Your first 50 examples are not just a quality assessment. They're a requirements-gathering exercise for your entire evaluation and monitoring infrastructure.

## Common Mistakes Teams Make

The first mistake is judging your own outputs too generously. You built this system. You want it to work. You'll unconsciously excuse failures or rate marginal outputs as acceptable when they're not. This bias is human and inevitable. The solution is to have someone who didn't build the prototype do at least half the judging. A colleague from another team, a domain expert, a potential user, anyone with enough context to evaluate but without attachment to the outcome.

The second mistake is using only easy examples in your evaluation set. If all 50 examples are straightforward cases, your accuracy will look great and won't predict production performance at all. Include the hard cases deliberately. Include the edge cases. Include the examples you know will be challenging. Your evaluation set should be harder than your average production case, not easier, because you want to understand your system's limits.

The third mistake is not writing down the rubric. If quality criteria live in your head, they drift. What counted as "good" yesterday might not count as "good" today. You become stricter as you see more outputs, or more lenient as you get tired, and your ratings become inconsistent. A written rubric keeps the standard stable across all 50 examples and across time as you re-evaluate with new prompt versions.

The fourth mistake is changing the prompt and the evaluation criteria at the same time. If you modify your prompt to improve accuracy and simultaneously decide to measure accuracy differently, you can't tell whether the quality change came from the prompt improvement or the rubric change. Change one thing at a time. Measure with a consistent rubric. Then change the next thing. This discipline is the only way to know what actually improves quality.

## The Habit You're Building

This first evaluation, 50 examples with a written rubric and a baseline number, is not just a one-time exercise. It's the foundation of an evaluation habit that will define how your team works for the life of the product. You're establishing the practice of measuring before you ship, of defining quality criteria explicitly, of tracking improvements against baselines, of taking quality assessment seriously enough to invest time in it.

Teams that build this habit early ship better products. They catch problems in development instead of production. They make data-driven decisions about prompts, models, and architectures instead of guessing. They can tell stakeholders with confidence what their quality level is and how it's trending over time. When a VP asks, "how good is our AI?" they have an answer grounded in data, not hopeful projections.

The habit also changes how you prioritize work. When you have evaluation data, you can see exactly which improvements move the quality metrics and which don't. You stop wasting time on changes that feel like they should help but don't actually improve measured quality. You focus on the changes that demonstrably work. This focus compounds over time. Teams with good evaluation practices improve faster because they're not guessing about what to fix.

Teams that skip early evaluation never recover. They build on assumptions instead of evidence. They ship products with unknown quality levels and discover the problems only after users complain. They can't tell whether their improvements are actually improving anything because they never established a baseline to improve from. Six months in, when leadership asks about quality, they have no answer. They know the system "seems better" but they can't quantify how much better or whether it's good enough.

The evaluation habit also protects you from regressions. When you make a change to improve one aspect of quality, you might accidentally hurt another aspect. With a stable evaluation set, you catch these regressions immediately. Without it, regressions slip into production and erode user trust before you notice. The evaluation set is your safety net. It lets you move fast because you can see immediately when something breaks.

Your first evaluation is crude, small-scale, and manual. It will grow into automated scoring, larger test sets, production monitoring, regression testing, and continuous evaluation. But it starts here, with 50 examples and honest judgment about whether the outputs are good enough. The sophistication comes later. The discipline starts now.

The next step is choosing the right model, because the best model is not always the right model for your product.