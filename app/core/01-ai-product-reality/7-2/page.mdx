# Chapter 7.2 — Your First Evaluation: Turning Gut Feeling Into Numbers

"It seems pretty good" is not a quality assessment. It's a feeling. And feelings are a terrible foundation for product decisions. The first thing you need after a working prototype is a way to measure quality — even a rough one.

Your first evaluation doesn't need to be sophisticated. It needs to exist.

---

### The 50-Example Eval

Start with 50 examples. Not 5 (too few to see patterns), not 500 (too many for your first iteration). 50 gives you enough signal to make decisions without drowning in annotation work.

**Step 1: Collect inputs.** Gather 50 real inputs that represent your use case. Pull them from production data, customer conversations, existing documents, or realistic simulations. Diversity matters: include easy cases, hard cases, edge cases, and cases from different user segments.

**Step 2: Generate outputs.** Run all 50 inputs through your prototype. Save every input-output pair.

**Step 3: Define your rubric.** Before you judge anything, decide what "good" means. Write it down. For a summarization product, it might be: "The summary captures the three most important points, contains no factual errors, and is under 100 words." For a classification product: "The classification matches the correct category and the confidence level is appropriately calibrated."

Your rubric should have 3-5 criteria, each rated on a simple scale (pass/fail, or 1-3, or 1-5). Don't overcomplicate it. A rubric you'll actually use beats a perfect rubric you abandon.

**Step 4: Judge the outputs.** Go through all 50 examples and rate each one against your rubric. If possible, have two people judge independently and compare. Where they disagree, discuss — those disagreements reveal ambiguity in your rubric that you need to resolve.

**Step 5: Calculate your baseline.** What percentage of outputs met your quality bar? This is your baseline number. Write it down. Date it. Every future improvement will be measured against this number.

---

### What Your First Eval Tells You

**If 80%+ are good:** You have a viable product. Focus on understanding and fixing the remaining 20%. Those failures are your evaluation roadmap.

**If 50-80% are good:** You have potential, but the product isn't ready for users. Analyze the failures: are they prompt issues (fixable quickly), model limitations (harder to fix), or task definition problems (requires rethinking)?

**If less than 50% are good:** The current approach isn't working. Either the task is too hard for the model, your prompt needs fundamental rethinking, or you need a different architecture (maybe RAG, maybe fine-tuning, maybe a different model).

---

### Common First-Eval Mistakes

**Judging your own outputs too generously.** You built this. You want it to work. You'll unconsciously excuse failures. Have someone who didn't build the prompt do at least some of the judging.

**Using only easy examples.** If your eval set is all straightforward cases, your accuracy will look great — and won't predict production performance. Include the hard cases deliberately.

**Not writing down the rubric.** If quality criteria live in your head, they shift. Yesterday "close enough" was acceptable. Today you're stricter. Tomorrow you're lenient again. Write the rubric down. Use it consistently.

**Changing the prompt and the eval at the same time.** If you change your prompt AND your evaluation criteria simultaneously, you can't tell what caused any quality change. Change one thing at a time. Measure. Then change the next thing.

---

### This Is Your Evaluation Foundation

This first eval — 50 examples, a written rubric, a baseline number — is the seed of your entire evaluation infrastructure. It'll grow into automated scoring, larger test sets, production monitoring, and regression testing. But it starts here, with 50 examples and honest judgment.

---

*Next: choosing the right model for your product — because the best model isn't always the right model.*
