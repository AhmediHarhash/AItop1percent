# Chapter 1.6 — The AI Product Lifecycle (Prototype → MVP → Scale)

Every AI product goes through stages, and each stage has different rules. The mistake most teams make is applying scale-stage thinking to prototype-stage work, or prototype-stage shortcuts to scale-stage systems.

Let me walk you through what each stage actually looks like, what matters at each one, and when to move forward.

---

### Stage 1: Prototype (Days to Weeks)

The goal here is simple: **can this work at all?**

You're not building for users. You're building for learning. Pick a model, write some prompts, throw your best examples at it, and see if the outputs are in the right ballpark.

What matters at this stage:
- Speed of iteration. Change a prompt, see the result, adjust. Repeat fifty times.
- Honest assessment. Don't cherry-pick the good outputs. Look at the bad ones and ask: are these fixable, or is this fundamentally not going to work?
- Cost sanity check. Run a rough calculation: at my target scale, will this cost $100/month or $100,000/month?

What doesn't matter yet:
- Production infrastructure
- Comprehensive eval sets
- Monitoring and alerting
- Perfect prompt engineering

The output of the prototype stage is a decision: **proceed, pivot, or kill.** Most teams skip this decision and slide into MVP mode by default. That's how zombie projects are born.

---

### Stage 2: MVP (Weeks to Months)

You've decided the concept works. Now the question is: **does it work for real users?**

The MVP is where you go from "this gives good answers on my laptop" to "this handles actual user inputs and delivers value they'd pay for."

What changes:
- **You need an eval set.** Not 500 examples — 50 to 100 carefully chosen ones that represent your real input distribution. Run every prompt change against this set before shipping.
- **You need error handling.** What happens when the model returns garbage? When the API times out? When the user input is 10x longer than you expected?
- **You need a feedback loop.** Put the product in front of real users (even 10 of them) and build a way to capture what's working and what's not. Thumbs up/down, bug reports, session recordings — anything.
- **You need cost tracking.** Not a projection — actual per-query cost measurement. You'll be surprised.

What doesn't matter yet:
- Multi-region deployment
- Sophisticated A/B testing
- Automated retraining pipelines
- Enterprise compliance certifications

The output of the MVP stage is validation: **do users actually want this, and can we deliver it at acceptable quality and cost?**

---

### Stage 3: Production (Months)

You have user validation. Now the question is: **can we run this reliably at real scale?**

This is where most of Section 3 (Evaluation Strategy) and Section 17 (Production Monitoring) come into play. Everything that was "good enough" in MVP needs to be hardened.

What changes:
- **Comprehensive eval coverage.** Your 50-example set becomes 500+, covering edge cases, adversarial inputs, and demographic slices.
- **Monitoring and alerting.** Real-time quality signals, latency tracking, cost dashboards, drift detection.
- **Deployment infrastructure.** CI/CD for prompts and configs, canary deployments, rollback capability.
- **Safety and compliance.** Input/output filtering, abuse detection, audit logging, regulatory requirements.
- **On-call and incident response.** When quality drops at 2 AM, someone needs to know and have a playbook.

---

### Stage 4: Scale (Ongoing)

Your product is live and growing. Now the question is: **can we make this better, cheaper, and more reliable over time?**

What changes:
- **Cost optimization.** Model routing (use cheaper models for easy queries), caching, prompt compression, batch processing.
- **Quality improvement.** The data flywheel kicks in — production data feeds your eval sets, which improve your prompts, which improve production quality.
- **Platform thinking.** If you have multiple AI features, you start building shared infrastructure: eval frameworks, prompt management, model routing layers.
- **Team growth.** You can't run a scaling AI product with the same two-person team that built the prototype. Evaluation, ops, and domain expertise become full-time roles.

---

### The Stage Mismatch Problem

The most common lifecycle mistake is building at the wrong level of rigor for your stage:

- **Over-engineering at prototype:** Spending three weeks on monitoring infrastructure before you know if the concept works. You just wasted three weeks.
- **Under-engineering at production:** Running the same hacky setup from MVP in production because "it works fine." It works fine until it doesn't, and then you're scrambling.

Match your investment to your stage. Prototypes should be scrappy. Production should be solid. Scale should be optimized. Don't skip stages, and don't gold-plate early ones.

*Next: the gap between what stakeholders expect and what's technically realistic — and how to manage it.*
