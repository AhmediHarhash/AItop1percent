# 5.8 — Compliance and Legal Requirements by Industry

In June 2024, a healthcare startup launched an AI-powered symptom checker that helped patients understand their conditions and decide whether to seek care. The product worked. Users loved it. The press coverage was positive. Three months after launch, a healthcare attorney reviewing their vendor agreements noticed something their legal team had missed: the product was providing medical advice without any clinical validation, without physician oversight, and without the regulatory clearances required for medical device software. The startup wasn't just non-compliant with FDA guidance — they were practicing medicine without a license in 47 states. Their options were to shut down immediately, hire a team of regulatory experts and physicians to redesign the product from scratch, or risk enforcement action. They chose shutdown. Nine months of development, half a million dollars in investment, and a product users genuinely valued, all lost because nobody asked the compliance question before building.

Compliance requirements exist whether you know about them or not. Ignorance is not a defense. "We didn't know" has never stopped a regulator from issuing fines, and in 2026, AI-specific regulations are multiplying faster than most teams can track. If you're building AI products, legal and compliance requirements are part of your product requirements from day one, not something you "handle before launch" or "figure out with legal later." The cost of retrofitting compliance into a finished product ranges from expensive to impossible.

## The Universal Requirements That Apply to Every AI Product

Before you consider industry-specific regulations, there are baseline requirements that apply to nearly every AI product in 2026 regardless of what you're building or who you're selling to. These are not optional. These are not best practices. These are legal obligations.

**Data privacy** comes first. If your product processes personal data — and personal data means names, email addresses, IP addresses, device identifiers, behavioral data, anything that can identify or relate to an individual — privacy laws apply. GDPR in the European Union and UK. CCPA and CPRA in California. LGPD in Brazil. PIPL in China. POPIA in South Africa. Virginia's CDPA, Colorado's CPA, Connecticut's CTDPA, and a growing list of state-level privacy laws in the US. The specific obligations vary by jurisdiction, but the common requirements are consent before data collection, data minimization so you only collect what you need, the ability for users to request deletion, and documentation of your data processing activities including purpose, legal basis, retention periods, and third-party sharing.

Privacy compliance is not a checkbox exercise. You can't add a consent banner and call it done. You need to map every data flow in your system. You need to know what data you collect, where it's stored, who has access, how long you keep it, and what happens when a user requests deletion. For AI products, this gets complicated fast. Does your training data contain personal information? If you fine-tune a model on customer data, can you delete that data when the customer requests it, or is it now embedded in model weights? When you log production queries for evaluation, are you getting explicit consent for that use? Most teams discover these questions too late, after they've already architected a system that makes compliance difficult or impossible.

**Transparency** is now legally required in multiple jurisdictions. The EU AI Act mandates disclosure when users interact with AI systems. Several US states have followed with similar requirements. The FTC has taken enforcement action against companies that failed to disclose AI involvement in customer interactions. The rule is simple: tell users they're interacting with AI, not a human. This applies to chatbots, customer service agents, content generation, and any other context where a user might reasonably assume they're interacting with a person. The disclosure needs to be clear, not buried in terms of service. A small disclaimer in a footer is not sufficient. Users need to know, up front and unambiguously, that they're talking to an AI.

**Non-discrimination** laws apply to AI products that make decisions about people. If your AI is involved in hiring, lending, insurance underwriting, housing, credit decisions, or any other context covered by civil rights or consumer protection law, the AI must not discriminate on the basis of protected characteristics. In the US, that means Title VII of the Civil Rights Act for employment, the Equal Credit Opportunity Act for lending, the Fair Housing Act for housing, and multiple other federal and state laws depending on the context. In the EU, it means the AI Act's high-risk provisions and existing anti-discrimination frameworks. The obligation is not just to avoid intentionally discriminating. It's to ensure your AI doesn't produce discriminatory outcomes even when trained on data that reflects historical biases. You need to test for disparate impact across protected groups, and if you find it, you need to fix it.

**Consumer protection** laws prohibit deceptive claims about what your AI can do. The FTC has made this clear in multiple enforcement actions. If you say "our AI guarantees accurate results," you need to be able to prove it. If you claim your AI can do something it can't, that's deceptive advertising. If your marketing materials imply capabilities your product doesn't have, that's a violation. AI products get scrutinized more carefully than traditional software because the public is less familiar with the limitations. Marketing teams often overpromise. Legal teams need to review every claim before it goes public.

## Healthcare: Where Compliance Costs More Than Development

Healthcare AI operates in the most regulated environment in technology. If your product processes protected health information, diagnoses conditions, recommends treatments, or influences clinical decisions, you are operating in a space where regulatory mistakes can end your company.

**HIPAA** applies if you're handling protected health information in the United States. Protected health information means any health data that can be linked to an individual — diagnoses, treatment histories, test results, even the fact that someone is a patient. HIPAA requires encryption in transit and at rest, access controls so only authorized personnel can view PHI, audit trails that log who accessed what and when, and business associate agreements with every vendor who touches PHI. If you use a cloud provider, they need a BAA. If you use a model provider and you're sending PHI to their API, you need a BAA with them. Most major model providers won't sign BAAs for their standard APIs. You need enterprise contracts with specific terms. This affects architecture. You can't just call OpenAI's public API with patient data. You need dedicated instances, contractual protections, and technical controls.

**FDA regulation** applies if your software meets the definition of a medical device, which means it diagnoses, treats, cures, mitigates, or prevents disease. Many AI products in healthcare meet this definition without the founders realizing it. The FDA has published guidance on clinical decision support software. If your AI provides patient-specific recommendations that a clinician is likely to rely on without independent review, it's regulated. If your AI analyzes medical images and flags findings, it's regulated. The regulatory pathway depends on risk classification. Low-risk devices can sometimes use enforcement discretion pathways. Higher-risk devices need premarket clearance through 510k or premarket approval. The process takes months to years and costs hundreds of thousands to millions of dollars. You can't build first and seek clearance later. You need to design for regulatory clearance from the beginning, with clinical validation, risk management, and quality management systems built into your development process.

The **EU Medical Device Regulation** applies if you're selling in Europe. The requirements are similar but not identical to FDA. You need a CE mark. You need a notified body to assess your conformity. You need clinical evidence. The timelines are long and the costs are substantial. A healthcare AI company that wants to sell in both the US and EU needs to design for both regulatory frameworks simultaneously, which often means building more conservative products with more documentation and more clinical validation than a purely technical team would consider necessary.

**Clinical validation** is required before deployment in patient care, regardless of regulatory pathway. You need evidence that your AI performs as intended in the target population with the target use case. This means prospective studies, not just retrospective validation on historical data. It means testing in the clinical environment where the product will be used, not just in a lab. It means sample sizes large enough to detect performance differences across patient subgroups. Clinical validation is expensive and slow, and it's the primary reason why healthcare AI takes longer to reach market than AI in other verticals.

## Financial Services: Where Explainability Is Not Optional

Financial services AI faces a different regulatory landscape focused on fairness, explainability, and systemic risk. If your AI is making lending decisions, trading decisions, credit scoring, fraud detection, or any other function that affects people's financial lives, multiple regulatory frameworks apply.

**Fair lending laws** require that credit decisions not discriminate on the basis of protected characteristics. The Equal Credit Opportunity Act in the US prohibits discrimination based on race, color, religion, national origin, sex, marital status, age, or receipt of public assistance. The problem for AI is that these characteristics are often correlated with other variables in the data. An AI trained on historical lending data will learn patterns that reflect historical discrimination, even if the protected characteristics themselves are removed from the training data. You need to test for disparate impact, and if you find it, you need to mitigate it. This often means sacrificing some predictive accuracy to achieve fairness. Regulators are clear: fairness is not optional, even if it reduces model performance.

**Explainability** is required for adverse action notices. When you deny someone credit, you need to tell them why. "The AI said no" is not an acceptable explanation. You need specific reasons tied to factors the applicant can understand and potentially address. This creates technical challenges for complex models. Black-box models that can't produce human-interpretable explanations are difficult to use in regulated lending contexts. Many financial institutions use simpler, more interpretable models for this reason, even when more complex models would be more accurate.

**Model risk management** frameworks like SR 11-7 in the US require validation, ongoing monitoring, and governance for models used in risk assessment or financial reporting. The framework distinguishes between model development and model validation, requiring independence between the two. You can't just validate your own model. You need a separate team with appropriate expertise to review the model's conceptual soundness, test its performance, and verify its ongoing accuracy. This is resource-intensive and often requires hiring specialists who understand both finance and machine learning.

**SOX compliance** applies to systems that affect financial reporting. If your AI generates numbers that end up in financial statements, you need controls, audit trails, and documentation that satisfy Sarbanes-Oxley requirements. This affects logging, access controls, change management, and documentation standards.

**PCI-DSS** applies if you process payment card data. The requirements are technical and specific: encryption, tokenization, network segmentation, access controls, and regular security testing. Most AI products don't need to touch raw card data, but if yours does, PCI compliance is mandatory and auditable.

## Legal Services: The Unauthorized Practice Problem

AI products in legal services face a unique challenge: they risk crossing the line into the unauthorized practice of law. Every US state regulates who can provide legal services, and providing legal advice without a license is a crime. The line between legal information and legal advice is blurry, and AI makes it blurrier.

**Legal information** is general explanations of the law. "Breach of contract requires offer, acceptance, and consideration." That's legal information. **Legal advice** is applying the law to a specific person's specific situation. "Based on your contract, you have a strong case for breach." That's legal advice, and providing it without a law license is unauthorized practice. AI legal products walk this line constantly. A chatbot that explains contract terms is providing information. A chatbot that tells a user whether they should sign a contract is providing advice. The distinction matters legally, and regulators are starting to enforce it.

**Attorney-client privilege** protections are critical for AI systems that process privileged communications. Law firms use AI for document review, contract analysis, and legal research. If the AI vendor can access the content of privileged communications, privilege may be waived. This requires contractual protections, technical controls, and careful vendor selection. Most law firms won't use AI tools that don't provide explicit privilege protections in their terms of service.

**Court rules** on AI-generated content are emerging quickly. Multiple federal courts now require disclosure when AI is used to draft legal filings. Some courts require certification that AI-generated content has been reviewed for accuracy. This is a response to several high-profile cases where lawyers submitted AI-generated briefs containing fabricated case citations. If your AI product is used by lawyers, you need to make the limitations clear and provide tools for verification.

## Education: Privacy Protections for Student Data

Educational AI is subject to privacy laws that go beyond general data protection. Student data receives heightened protection under both federal and state law.

**FERPA** — the Family Educational Rights and Privacy Act — protects student educational records. Schools that receive federal funding can't disclose personally identifiable information from education records without consent. This applies to AI vendors who process student data on behalf of schools. You need contracts that establish the school's control over the data, limit how you can use it, and require deletion when the contract ends. FERPA violations can result in loss of federal funding for the school, which makes schools extremely cautious about vendor compliance.

**COPPA** — the Children's Online Privacy Protection Act — applies to online services directed at children under 13. COPPA requires parental consent before collecting personal information from children, limits what data can be collected, and requires reasonable security measures. For educational AI used by young students, COPPA compliance is mandatory. The verifiable parental consent requirement is technically challenging and operationally complex.

**State student privacy laws** vary widely. Some states have laws similar to FERPA but with broader coverage. Others have specific restrictions on ed-tech vendors. California's SOPIPA, New York's Education Law 2-d, and similar laws in other states create a patchwork of requirements that vary by geography. An ed-tech AI product that operates nationally needs to comply with the strictest state requirements across its entire user base, because it's not practical to vary functionality by state.

**Accessibility requirements** under Section 504 and Section 508 of the Rehabilitation Act and the Americans with Disabilities Act mean that AI products used in educational settings must be accessible to students with disabilities. This includes screen reader compatibility, keyboard navigation, caption support for audio content, and alternative text for images. Accessibility is not optional for educational technology. It's a legal requirement and a condition of purchase for most school districts.

## The Compliance Action Plan You Need Before Architecture

Compliance is not something you add at the end. It shapes architecture, data handling, feature design, and vendor selection. The teams that succeed treat compliance as a product requirement from day one. The teams that fail treat it as a legal checkbox.

**Step one** is identifying your regulatory surface. List every regulation that might apply based on your industry, your data types, the geographies you operate in, and the decisions your AI makes. When in doubt, assume it applies. Get a legal opinion if you're not sure. The cost of a legal opinion is measured in thousands of dollars. The cost of a compliance failure is measured in millions.

**Step two** is getting specialized counsel. General corporate lawyers don't know AI regulations. You need lawyers who specialize in your industry and understand AI specifically. Healthcare AI lawyers. Financial services AI lawyers. Privacy lawyers who understand GDPR and CCPA and the technical implementation. The legal market has matured enough that these specialists exist. Use them.

**Step three** is building compliance into the product from the beginning. Consent flows, audit logging, access controls, data retention policies, explainability features, transparency disclosures, bias testing — all of these need to be part of the initial architecture, not features you add later. Retrofitting compliance is expensive and often requires breaking changes that affect users.

**Step four** is documentation. Regulators don't trust your word. They want evidence. Document your risk assessments. Document your evaluation results. Document your bias audits. Document your data handling procedures. Document your decision-making processes. Document your vendor due diligence. When regulators come asking — and in regulated industries, they will come asking — you need to be able to produce evidence of compliance, not just assertions.

**Step five** is ongoing monitoring of regulatory changes. AI regulation is evolving rapidly. The EU AI Act was finalized in 2024. Multiple US states passed AI-specific legislation in 2024 and 2025. The SEC, CFPB, FTC, FDA, and other regulators have all issued AI guidance. What was optional last year may be mandatory this year. Assign someone on your team to track regulatory developments in every market where you operate. Subscribe to regulatory newsletters. Attend industry conferences. Join trade associations that monitor policy. You can't comply with regulations you don't know about.

## The Cost of Getting Compliance Wrong

The direct financial penalties for compliance failures are substantial. GDPR fines can reach 20 million euros or four percent of global annual revenue, whichever is higher. HIPAA violations can cost 50,000 dollars per record, with total penalties in the millions. State attorneys general are actively enforcing consumer protection laws against AI companies. The FTC has made AI a priority enforcement area. The financial risk is real and growing.

But the direct penalties are not the biggest risk. The biggest risk is that a compliance failure forces you to shut down a product or substantially redesign it after launch. The healthcare startup that had to shut down their symptom checker didn't get fined. They lost the entire product. The cost wasn't a penalty. It was nine months of wasted development time and half a million dollars in sunk investment. That's the cost that kills companies.

Compliance is not an obstacle to innovation. Compliance is a constraint that defines what responsible innovation looks like in your domain. The teams that treat it as a constraint to work within ship compliant products that last. The teams that treat it as an obstacle to work around eventually discover that regulations are not optional.

---

*Next: the pre-mortem — predicting failure before you ship so you can prevent it.*
