# 4.5 — Cross-Functional Decision Making for AI

In late 2024, a financial services company spent four months debating whether their AI-powered fraud detection system was ready to launch. The ML team said the model achieved 94 percent precision and 89 percent recall on their test set, which exceeded their original target. The product manager wanted to ship immediately to meet a board commitment. The compliance team said they needed more documentation about how the model made decisions before they could approve it for production use. The fraud operations team—the people who would actually use the system—said 89 percent recall meant they'd miss 11 percent of fraud cases, which translated to roughly $2.3 million in annual losses based on their transaction volume. Legal wanted to understand the false positive rate's impact on customer experience and potential discrimination claims. Engineering said the model was technically ready but the monitoring infrastructure to catch drift wasn't fully built yet.

Each team had legitimate concerns based on their domain expertise. Each team was looking at the same system through a different lens and reaching different conclusions about readiness. The decision kept getting escalated and re-litigated. Meetings were scheduled, canceled, and rescheduled. Different executives heard different versions of the situation and issued contradictory guidance. The launch slipped by six weeks, not because of technical problems but because nobody had defined in advance how this type of decision should be made, who had input, who had veto authority, and who made the final call.

The eventual decision—to launch with enhanced monitoring and a human review layer for high-confidence fraud predictions—was the right one. But it took six weeks of organizational thrashing to reach a decision that should have taken six days if the decision-making process had been clear.

## Why AI Decisions Are Different

Traditional software products have established decision-making patterns. The product manager generally decides what features to build and when to ship them. Engineering decides how to build them and whether the technical implementation is ready. These domains are relatively separable. A PM can define requirements without understanding the database architecture. An engineer can build features without understanding the market positioning.

AI product decisions don't separate cleanly. Every significant decision requires input from multiple disciplines because the disciplines are interdependent in ways that don't occur in traditional software. Quality thresholds are simultaneously technical questions—what accuracy can we achieve with available data and compute—and business questions—what accuracy does the business model require—and domain questions—what accuracy meets professional standards—and legal questions—what accuracy satisfies our liability and regulatory obligations. No single person has enough information to make this decision well.

The same interdependence appears in ship-or-wait decisions, cost versus quality tradeoffs, safety policy, data handling, model selection, evaluation methodology, and deployment strategy. Each decision cuts across functional boundaries. The ML team knows what's technically achievable but not what's commercially necessary. The product team knows what users need but not what's technically feasible at what cost. The domain experts know what correctness means but not how to balance correctness against latency and cost. Legal knows what's required for compliance but not what's possible given technical and business constraints.

This isn't a coordination problem that you solve with better communication. It's a structural reality. Good AI product decisions require synthesizing information and perspectives that live in different heads. The question is how you make those decisions efficiently without either descending into endless consensus-seeking or making unilateral decisions with incomplete information.

## The Decision Types That Need Cross-Functional Input

Not every decision requires cross-functional input. Changing a prompt's wording might be a unilateral product decision. Optimizing an inference pipeline is probably a unilateral engineering decision. But certain decision types inherently require multiple perspectives.

Quality thresholds are the most common cross-functional decision. What accuracy level is acceptable for this use case? This requires input from product—what's the business impact of errors at different rates?—from ML—what accuracy is achievable with current data and methods?—from domain experts—what's the professional standard of care in this domain?—from legal—what does our liability exposure look like at different error rates?—and from finance—what does it cost to achieve different quality levels? If product sets the threshold alone, you might commit to unachievable quality. If ML sets it alone, you might accept quality that's technically achievable but commercially unacceptable. If legal sets it alone, you might over-index on risk avoidance at the expense of product utility.

Ship-or-wait decisions require knowing the current quality metrics from ML, the market timing and user urgency from product, the infrastructure and monitoring readiness from engineering, and the compliance status from legal. Missing any of these perspectives creates blind spots. I've seen teams ship before monitoring was ready and lose visibility into production quality. I've seen teams delay shipping for technical perfection while users desperately needed the product and would have accepted lower quality. The right decision requires seeing the full picture.

Cost versus quality tradeoffs show up constantly in AI products. Should you use the expensive frontier model or the cheap fast model? Should you run comprehensive evaluation on every release or sample-based evaluation? Should you invest in human review layers or accept higher error rates? These decisions require understanding the quality difference from ML, the user's quality expectations from product, the budget and unit economics from finance, and the performance and scalability implications from engineering. The optimal tradeoff depends on information held by all four functions.

Safety policy decisions—what should the AI refuse to do, what guardrails should exist, how should it handle edge cases—require input from product about user expectations, from legal about liability and compliance exposure, from trust and safety teams about abuse patterns they've observed in similar products, and from engineering about implementation feasibility. A safety policy that's legally bulletproof but makes the product unusable serves nobody. A safety policy that ignores legal exposure creates existential risk. You need both perspectives in the room.

Data handling decisions about what data you can use for training, evaluation, and monitoring require legal expertise on privacy and consent requirements, engineering expertise on data infrastructure and feasibility, product expertise on data availability and quality, and compliance expertise on regulatory obligations across jurisdictions. Using data you don't have rights to creates legal liability. Not using data you do have rights to leaves performance on the table. The decision requires legal and technical perspectives simultaneously.

## The Three Patterns That Work

The first pattern is the decision matrix, sometimes called a RACI matrix adapted for AI products. For each major decision type, you predefine who has input, who has veto authority, and who makes the final call. You write this down and share it with the entire team so that when a decision comes up, everyone knows the process.

For quality thresholds, you might define that the product manager makes the final decision, ML and domain experts provide required input, and legal has veto authority for Tier 3 and above products. For ship decisions on Tier 1 or Tier 2 products, the PM decides with ML input. For Tier 3 products, you require explicit sign-off from PM, ML, legal, and domain experts. For cost tradeoffs, the PM decides with input from finance and ML. For safety policy, legal and trust and safety decide jointly with input from product and engineering.

The specific assignments matter less than having them be explicit and known. The worst outcome is ad hoc decision-making where every decision becomes a negotiation about who gets to decide. The decision matrix eliminates that negotiation. It won't eliminate disagreement—the ML lead might still think the quality threshold is too high—but it clarifies that disagreement is input, not veto, and the PM makes the call.

The second pattern is a standing quality review board for Tier 3 and above products. This is a regular meeting, typically weekly or biweekly, where representatives from product, ML, engineering, legal, domain experts, and sometimes trust and safety review quality metrics, discuss issues, and make collective decisions about significant changes. The membership is fixed, the meeting is recurring, and the scope is predefined. The board doesn't make every decision—that would be paralyzing—but it makes the decisions that affect core quality, safety, or compliance.

This pattern prevents quality decisions from being made in silos. It ensures that when you're deciding whether to switch models or change evaluation methodology or adjust safety filters, all the relevant perspectives are in the room. It also creates regular checkpoints where quality is reviewed, which prevents drift. The risk is that the board becomes a bottleneck or a rubber-stamp. To prevent bottlenecking, you need clear scope about what comes to the board versus what teams decide independently. To prevent rubber-stamping, you need members who have real authority and expertise, not just representatives filling seats.

The third pattern is a decision log that records every significant AI product decision: what was decided, by whom, based on what data, with what alternatives considered, and who had input. This serves three purposes. First, it creates accountability—you can trace decisions back to decision-makers and their rationale. Second, it enables learning—you can review past decisions and analyze which ones worked and which ones didn't and why. Third, it supports compliance—you can show regulators and auditors that you had a thoughtful decision process, not just cowboy engineering.

The decision log doesn't need to be elaborate. A shared document or database with date, decision, decision-maker, data used, alternatives considered, and participants is sufficient. The key is making it a habit. After every significant decision, someone—typically the PM or a technical lead—writes it down. Over time, the log becomes organizational memory and a learning resource.

## The Communication Problem

The biggest obstacle to good cross-functional decisions isn't disagreement about what to do. It's miscommunication about what the options are and what the tradeoffs entail. Each function speaks a different language.

ML says "the model's perplexity improved by 12 percent." The PM hears technical jargon with no clear business meaning. What does 12 percent perplexity improvement mean for user experience? The PM can't translate this into a product decision.

Product says "users are unhappy with response quality." ML hears vague feedback with no actionable direction. What dimension of quality? What types of responses? What would make them happy? The ML team can't translate this into a model improvement.

Legal says "we need a Data Protection Impact Assessment before launch." Engineering doesn't know what that is, how long it takes, or what's required to complete one. They can't plan around it.

Domain experts say "this answer is clinically inaccurate because it doesn't account for contraindications." Everyone else understands that it's wrong but doesn't know how to make it right. What data would fix this? What does the model need to learn?

The fix is a translation layer—typically the product manager or a technical product lead who can bridge between disciplines. This person doesn't need to be an expert in all domains, but they need enough literacy in each domain to translate between them. They can take "perplexity improved by 12 percent" and ask "does that mean users will notice better responses?" They can take "users are unhappy with quality" and ask "which quality dimensions matter most—accuracy, relevance, tone, completeness?" They can take "we need a DPIA" and ask "what's the timeline and what do we need to provide?" They can take "clinically inaccurate due to contraindications" and ask "can we solve this with better prompting or do we need different training data?"

Investing in this translation capability is more valuable than most technical improvements. A team with mediocre models but excellent cross-functional communication will outperform a team with excellent models but poor communication. The limiting factor is usually not technical capability. It's the ability to synthesize information and perspectives from different disciplines into coherent decisions.

## The Speed Tax and How to Minimize It

Cross-functional decision-making is slower than unilateral decision-making. That's the tradeoff. A single engineer can make a decision in five minutes. Getting input from product, ML, legal, and domain experts takes a day or a week. If you involve all stakeholders in every decision, you'll move slowly enough that competitors will outrun you.

The answer is scope definition. Which decisions require cross-functional input and which can be made unilaterally? What's the threshold that triggers broader involvement?

For Tier 1 products with low business and safety stakes, most decisions can be unilateral. The PM can set quality thresholds without legal review. Engineers can change prompts without ML sign-off. The team can move fast because the downside risk of a bad decision is limited.

For Tier 3 products with high stakes, more decisions require cross-functional input. Changing the model requires review. Adjusting safety filters requires legal input. Launching new features requires domain expert validation. The process is slower because the risk of mistakes is higher and the cost of getting it wrong is severe.

The key is matching the process to the stakes. Define clear thresholds for your product. What constitutes a significant change that requires broader review? Model swaps probably trigger review. Prompt wording changes probably don't. What's the dollar threshold or user impact threshold that triggers cross-functional involvement? Establish these thresholds explicitly so that teams know when they can move fast and when they need to slow down and involve others.

Some organizations use a tiered decision framework where decisions are categorized as low, medium, or high impact, and each tier has a predefined approval process. Low-impact decisions can be made by individuals. Medium-impact decisions require input from relevant stakeholders. High-impact decisions require formal review and sign-off. The categorization should be clear enough that teams can self-classify most decisions without needing to ask permission to make a decision about how to make a decision.

## Making It Stick

Cross-functional decision-making fails when it's informal. Informal agreements to "involve the right people" decay under schedule pressure. When the deadline is tomorrow, the natural response is to skip consultations and move fast. Formalization prevents this decay.

Write down your decision matrix. Which decisions require which inputs and who decides? Make it a shared document that everyone knows exists. When a decision comes up and there's disagreement about process, you point to the document. The document should live somewhere accessible to everyone on the team—a wiki page, a shared folder, or a section in your product documentation. Update it when you discover decision types you didn't anticipate. Treat it as a living document, not a one-time exercise.

Schedule your quality review board if you have one and protect the calendar time. Make attendance mandatory for the relevant functions. Treat it with the same priority as a sprint planning meeting. If you're canceling the quality review board to make room for feature work, your priorities are backwards. Quality decisions are product decisions. They're not optional overhead. They're core work.

Maintain your decision log consistently. Make it someone's job—typically the PM or a technical program manager—to record significant decisions. If it's everyone's job, it won't happen. The log should capture context, not just outcomes. What data informed the decision? What alternatives were considered and why were they rejected? Who participated? This context is valuable when you revisit the decision later or when new team members need to understand why things are the way they are.

Train your team on cross-functional communication. Teach ML engineers to translate technical metrics into business language. Teach product managers to ask better questions of domain experts. Teach everyone the basics of each other's domains so that communication isn't completely across foreign languages. This training doesn't need to be formal classroom education. It can be brown-bag lunches where an ML engineer explains how evaluation works or a lawyer explains what a Data Protection Impact Assessment entails. Small investments in shared understanding pay large dividends in communication efficiency.

## When Cross-Functional Decision-Making Breaks Down

Even with good processes, cross-functional decision-making can break down. Recognizing the failure modes helps you address them before they become chronic.

The most common failure mode is functional silos where each discipline makes decisions independently and then hands off to the next function. Product decides what to build without ML input, then hands requirements to ML. ML builds a model without engineering input on deployment constraints, then hands it to engineering. Engineering deploys without legal review, then legal discovers compliance issues after launch. This serial handoff model is fast but produces bad outcomes because each function optimizes for their local concerns without seeing the full picture.

The fix is overlapping decision windows where functions work in parallel rather than sequence. Product shouldn't finalize requirements before talking to ML about feasibility. ML shouldn't finalize the model before talking to engineering about deployment. Engineering shouldn't deploy before legal reviews compliance. This overlap feels slower initially but produces better decisions and avoids expensive rework.

Another failure mode is veto overload where too many functions have veto authority and nothing can ship because someone always objects. This typically happens when organizations over-correct for previous failures where important perspectives were ignored. They give everyone veto power and then nothing moves.

The fix is distinguishing between input and veto. Most functions should have required input—their perspective must be heard—but not veto authority. Veto should be reserved for functions whose concerns are existential. Legal might have veto on compliance issues because shipping a non-compliant product creates existential risk. Domain experts might have veto on Tier 3 products where quality failures create safety risk. But most functions most of the time should have input, not veto.

A third failure mode is authority confusion where it's unclear who has final decision authority. Everyone gives input, discussion happens, but when it's time to decide, nobody knows whose call it is. The discussion continues indefinitely or the decision gets escalated to ever-higher levels of leadership until someone finally makes a call.

The fix is explicit decision authority in your decision matrix. For each decision type, one person or role makes the final call. They consider all input, but when discussion ends, they decide. This person is accountable for the outcome. If the decision turns out badly, it's on them. This accountability focuses decision-making and prevents endless debate.

## Cross-Functional Decisions at Different Risk Tiers

The cross-functional decision process should scale with product risk. Tier 1 products need lightweight processes. Tier 3 products need heavyweight processes. Applying heavyweight processes to low-risk products slows you down unnecessarily. Applying lightweight processes to high-risk products is negligence.

For Tier 1 products, most decisions can be made by individuals or small groups without formal cross-functional review. The PM can set quality thresholds in consultation with ML. Engineers can deploy without legal review. The team can move fast because the stakes are low. You might have a weekly team meeting where everyone shares updates and raises concerns, but you don't need formal review boards or decision logs.

For Tier 2 products, quality and ship decisions need cross-functional input, but you can keep the process lightweight. A weekly sync between PM, ML, engineering, and domain experts where you review metrics, discuss concerns, and make decisions together. A simple decision log in a shared document. Clear authority about who decides, but not heavy approval gates.

For Tier 3 products, major decisions need formal cross-functional review with documented approvals. A quality review board that meets regularly and has veto authority over launches. A detailed decision log that captures rationale and alternatives. Explicit sign-off from legal, domain experts, and product leadership before major changes. This feels heavyweight because it is heavyweight. The product risk justifies the process weight.

The key is matching process to risk, not applying one-size-fits-all processes across all products. Organizations that treat every product as Tier 3 move slowly and frustrate their teams. Organizations that treat Tier 3 products as Tier 1 create existential risk.

## Building Cross-Functional Trust

Cross-functional decision-making works well when functions trust each other. Trust means each function believes the others are competent in their domain, operating in good faith, and committed to the product's success. Without trust, cross-functional decisions become political battles where functions fight for their position rather than synthesizing toward the best outcome.

Building trust requires consistent behavior over time. ML needs to see that when product sets aggressive quality thresholds, they're based on real user needs, not arbitrary perfectionism. Product needs to see that when ML says something isn't feasible, they've actually explored the space, not just dismissed the request. Engineering needs to see that when legal raises compliance concerns, they're real risks, not legal covering themselves. Legal needs to see that when engineering pushes back on compliance requirements, they're looking for workable solutions, not trying to ignore risk.

This trust builds through repeated interactions where each function demonstrates competence and good faith. It also builds through shared pain—when the team ships something that fails and everyone owns the failure together rather than blaming each other. Teams that have been through production incidents together and fixed them collaboratively trust each other more than teams that have only had smooth sailing.

Trust also requires mutual respect for domain expertise. Engineers need to respect that legal knows compliance better than they do. Legal needs to respect that engineers know technical feasibility better than they do. Product needs to respect that ML knows what models can achieve. ML needs to respect that product knows what users need. When each function respects the others' expertise, cross-functional discussions become collaborative rather than adversarial.

One concrete practice that builds trust is function rotation or shadowing. Have engineers sit in on legal reviews to understand what legal cares about. Have legal sit in on technical design reviews to understand technical constraints. Have ML engineers shadow customer support to understand user pain points. These experiences build empathy and understanding that improve cross-functional communication.

## The Role of Leadership in Cross-Functional Decisions

Leadership sets the tone for whether cross-functional decision-making works. If leadership publicly overrides cross-functional decisions based on political considerations rather than merit, the team learns that the process doesn't matter and reverts to politics. If leadership reinforces the process even when it's inconvenient, the team learns to trust it.

Leadership also breaks ties when cross-functional perspectives genuinely conflict and there's no clear right answer. Should you launch with 92 percent accuracy or wait until you hit 95 percent? ML says 95 percent is achievable in two more weeks. Product says users need it now and 92 percent is acceptable. Both perspectives are legitimate. Someone has to make the call. That's leadership's job.

Good leaders make these calls based on explicit criteria tied to the product's mission and risk tier. What's the user need urgency? What's the cost of waiting? What's the cost of shipping lower quality? What's our risk tolerance given the product tier? They explain their reasoning so the team understands it's not arbitrary. They own the outcome.

Bad leaders make calls based on who argued most forcefully or which executive they want to please. They don't explain their reasoning. They change their mind when political pressure shifts. This behavior destroys trust in cross-functional processes.

Leadership also needs to protect dissent. When legal raises compliance concerns and product wants to ship anyway, legal needs to know they won't be punished for raising the concern even if leadership overrides it. Protecting dissent ensures that all perspectives get heard. Punishing dissent ensures that functions stop raising concerns and problems go underground.

The teams that build great AI products aren't the ones with the most talented individual contributors. They're the ones that make cross-functional decisions well, synthesizing expertise from multiple domains into coherent action. That synthesis doesn't happen naturally. It happens because the organization built explicit processes, invested in translation capability, matched process weight to risk tier, built cross-functional trust, and had leadership that reinforced good decision-making even when it was inconvenient.

The next structural challenge is integrating the most undervalued team members in most AI product organizations: the domain experts who know what correctness actually means.
