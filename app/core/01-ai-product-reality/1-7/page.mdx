# Chapter 1.7 — Stakeholder Expectations vs Technical Reality

I've been in this meeting. You probably have too.

The CEO watched a demo on Tuesday. By Wednesday, they want it shipped to all customers. By Thursday, they're asking why it can't also do three things the demo never showed. By Friday, the engineering team is quietly panicking because the gap between what was promised and what's possible is wider than anyone wants to admit.

Managing stakeholder expectations isn't a soft skill in AI — it's a survival skill.

---

### Why AI Makes This Harder

Traditional software has a nice property: if you demo something working, it usually works that way for everyone. A button that clicks in the demo will click in production.

AI doesn't work like that. You can demo a system giving perfect answers to five questions, and stakeholders naturally assume it will give perfect answers to all questions. They don't understand that you hand-picked those five, that the model gets it wrong 20% of the time on real inputs, and that "wrong" in AI can mean anything from slightly off to completely hallucinated.

This creates three specific expectation gaps:

**The accuracy gap.** Stakeholders expect near-perfect accuracy because the demo looked perfect. Reality: even the best systems have meaningful error rates, and those error rates are often non-obvious until you measure them at scale.

**The timeline gap.** Stakeholders expect AI products to ship as fast as traditional features because "the AI does the hard part." Reality: the AI does the easy part (generating outputs). The hard part is evaluation, safety, monitoring, and edge case handling — which takes longer than the model integration.

**The capability gap.** Stakeholders see what the model can do and assume the product can do everything the model can do. Reality: your product is a constrained application of the model's capabilities, with guardrails, policies, and limitations that are there for good reasons.

---

### How to Close the Gaps

**1. Show the failures, not just the successes.**
When you demo to stakeholders, include examples where the system fails. Not to be dramatic — to set realistic expectations. "Here's what it does well. Here's where it struggles. Here's our plan to improve." This reframes the conversation from "why isn't it perfect?" to "what's the path to good enough?"

**2. Define quality in numbers, not vibes.**
Replace "it works well" with "it gives acceptable answers 85% of the time on our eval set, and we're targeting 92% by launch." Numbers give stakeholders something concrete to track and remove the subjective arguments about whether it's "good enough."

**3. Make the cost visible early.**
Before stakeholders get attached to the "AI does everything" vision, show them the cost model. "At our projected scale, this costs $X per month. Here's what it costs per user, per query, per resolved task." Cost has a clarifying effect on feature requests.

**4. Separate what the model can do from what the product should do.**
The model might be able to answer medical questions, write legal contracts, and generate code. Your product should do one of those things well. Help stakeholders understand that "can" and "should" are different conversations, and that doing fewer things at higher quality beats doing everything at mediocre quality.

**5. Create a shared vocabulary.**
Stakeholders don't need to understand transformer architectures. But they do need to understand:
- **Hallucination:** the model confidently says something false
- **Eval set:** the test suite we use to measure quality
- **Regression:** quality getting worse after a change
- **Latency:** how long the user waits for a response
- **Cost per query:** what each AI interaction costs

When everyone uses the same words, the conversations get much more productive.

---

### The Quarterly Reality Check

Build a habit of showing stakeholders three things every quarter:

1. **Current quality metrics** — where you are vs where you need to be
2. **Cost trajectory** — what you're spending and where it's headed
3. **Top failure modes** — what goes wrong most often and what you're doing about it

This turns "is the AI working?" from a subjective argument into a data-driven conversation. Stakeholders who see regular, honest updates are far more patient than ones who only hear about problems when something breaks.

---

*Next: the hardest conversation of all — when not to use AI at all.*
