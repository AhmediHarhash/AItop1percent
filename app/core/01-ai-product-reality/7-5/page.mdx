# Chapter 7.5 — The Data Flywheel: Turning Usage Into Improvement

The most powerful advantage in AI isn't the model you use — it's the learning loop you build around it. Every user interaction is a data point. Every data point is an opportunity to improve. The question is whether you're capturing those data points and using them, or letting them evaporate.

This is the data flywheel, and it's what separates AI products that get better over time from AI products that stay frozen at launch quality.

---

### How the Flywheel Works

**Stage 1: Users interact.** Users ask questions, submit documents, make requests. Each interaction generates an input-output pair — what the user asked, and what the AI responded.

**Stage 2: Collect signals.** For each interaction, capture quality signals. Explicit signals: thumbs up/down, user ratings, user corrections. Implicit signals: did the user accept the suggestion? Did they edit it? Did they abandon the conversation? Did they escalate to a human?

**Stage 3: Identify patterns.** Analyze the signals to find patterns. What types of queries get low ratings? Where do users consistently edit the AI's output? Which topics generate the most escalations? These patterns reveal your product's weakness — and each weakness is an improvement opportunity.

**Stage 4: Improve.** Use the patterns to improve your product. Update prompts to handle common failure cases. Add few-shot examples for tricky queries. Expand your eval dataset with real production examples that expose failures. Adjust your routing or model selection based on query difficulty.

**Stage 5: Evaluate.** Measure whether your improvements actually improved quality. Run your updated system against your eval suite. Compare before and after. If quality improved, deploy. If not, investigate.

**Then repeat.** Every cycle makes the product better. Over months, the compound effect is enormous. A product running this flywheel weekly for six months has gone through 25+ improvement cycles. A competitor without a flywheel is still at launch quality.

---

### What to Collect

**Always collect:**
- Full input-output pairs (with user consent and appropriate privacy handling)
- Timestamps and session context
- Which model and prompt version generated the output
- Any explicit feedback (ratings, corrections)

**Collect when possible:**
- Implicit behavioral signals (edits, acceptance, abandonment)
- Downstream outcomes (did the customer's issue get resolved? Did the suggested email get sent?)
- Human reviewer scores on sampled interactions

**Never collect without consent:**
- Personal data beyond what's needed
- Data in jurisdictions where collection isn't permitted
- Data that users have explicitly asked to delete

---

### Building the Flywheel From Day One

You don't need a sophisticated data infrastructure to start the flywheel. You need three things:

**1. Logging.** Log every interaction. Input, output, model version, prompt version, timestamp. Store it somewhere you can query it. This is the minimum.

**2. A feedback mechanism.** Even a simple thumbs up/down on the UI gives you signal. More sophisticated feedback (edit tracking, follow-up questions, satisfaction surveys) gives you more signal. Start simple and expand.

**3. A review cadence.** Set a recurring time (weekly is ideal) to review production interactions. Look at low-rated interactions. Look at patterns. Decide what to improve. Without a cadence, the data sits unused.

The flywheel is the reason AI products get better while traditional software stays the same. Traditional software improves when engineers ship code. AI products improve when engineers ship code AND when users use the product. That second channel of improvement is a structural advantage — but only if you build for it.

---

*Next: setting your go/no-go criteria — the decision framework for whether to proceed, pivot, or stop.*
