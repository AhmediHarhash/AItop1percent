# Chapter 6.6 — Provider Resilience: Outages, Rate Limits, Policy Changes & Model Deprecations

On November 20, 2024, OpenAI experienced a major outage that lasted several hours. Every product built exclusively on their API went down with them. Customer support bots stopped responding. Document processing pipelines stalled. Real-time analysis tools returned errors. The companies that survived unscathed were the ones that had planned for exactly this scenario.

Provider resilience isn't paranoia — it's engineering.

---

### The Four Provider Risks

**Outages.** Every major provider has had multi-hour outages. Some have had full-day degradations. When your provider goes down, your product goes down — unless you've built a fallback path. For Tier 2+ products (customer-facing or higher), a single-provider architecture is an unacceptable risk.

**Rate limits.** Providers impose rate limits — requests per minute, tokens per minute, concurrent connections. During peak demand (both yours and other customers'), you may get throttled below your normal capacity. Your product slows down or queues requests, and users experience degradation even though nothing in your system is broken.

**Policy changes.** Acceptable use policies evolve. A use case that was fine yesterday might violate updated terms tomorrow. Providers have suspended accounts for policy violations — sometimes with little warning. If your product operates anywhere near a policy boundary, this is a real risk.

**Model deprecations.** Every model has a lifecycle. When a model is deprecated, you get a migration window — typically 3-6 months. During that window, you need to: evaluate the replacement model against your test suite, update prompts that depend on the deprecated model's specific behavior, regression test your entire product, and deploy. If you don't have eval infrastructure, this migration is a scramble. If you do, it's a process.

---

### Building Provider Resilience

**Tier 1: Basic resilience (all products).**
- Monitor provider status pages and set up alerts
- Implement retry logic with exponential backoff
- Track error rates and latency to detect degradation early
- Maintain a known-working model version and prompt set that you can revert to

**Tier 2: Failover resilience (customer-facing products).**
- Integrate a secondary model provider (e.g., primary Claude, fallback GPT-4)
- Build a model abstraction layer that can route requests between providers
- Run your eval suite on both providers regularly so you know the quality delta
- Test failover monthly — don't wait for a real outage to discover it doesn't work

**Tier 3: Full resilience (high-risk and regulated products).**
- Self-host an open-source model as the last-resort fallback
- Implement graceful degradation: when all AI providers fail, what does the user experience? A queue? A human agent? A simplified rule-based response?
- Maintain contractual SLAs with providers that include financial penalties for outages
- Run chaos engineering exercises — deliberately simulate provider failures and verify your system handles them

---

### The Resilience Test

Ask your team: "If our primary model provider went down right now for 6 hours, what happens to our users?"

If the answer is "our product stops working," you have work to do. If the answer is "we automatically fail over to our secondary provider with a known quality level," you're in good shape. If the answer is "we have three layers of fallback and our users might not even notice," you've built resilience.

The best time to build provider resilience is before you need it. The second best time is now.

---

*Next: building for the model generation after this one — future-proofing your architecture.*
