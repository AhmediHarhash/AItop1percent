# 6.6 — Provider Resilience: Outages, Rate Limits, Policy Changes, and Model Deprecations

On November 8, 2024, OpenAI experienced a major outage that lasted four hours and seventeen minutes. Every product built exclusively on their API went down with them. Customer support chatbots stopped responding mid-conversation. Document processing pipelines stalled with thousands of documents queued. Real-time analysis tools returned error messages instead of insights. Revenue-generating products were offline. SLA commitments were violated. Customers were frustrated.

The companies that survived this outage unscathed had one thing in common: they'd architected for provider failure before it happened. They had fallback providers configured, tested, and ready. Their monitoring detected the outage within seconds. Their systems automatically failed over to secondary models. Their users experienced brief degradation in response quality or latency, not complete product failure.

The companies that went down completely had treated their model provider as infallible infrastructure. They'd built their entire product on the assumption that the API would always be available, always responsive, and always operating under the same terms. This assumption is professionally negligent. Every provider has outages. Every provider has rate limits. Every provider evolves their policies. Every model has a lifecycle. If your architecture doesn't account for these realities, you're building on sand.

## The Four Provider Risks

Provider risk comes in four forms, each requiring different mitigation strategies. Understanding the difference is the first step toward resilience.

The first risk is outages. No provider has perfect uptime. OpenAI has had multiple multi-hour outages. Anthropic has had incidents where latency degraded to unusable levels. Google's Vertex AI has experienced regional failures. Microsoft Azure OpenAI has had authentication issues that blocked all requests. These outages happen without warning. They affect all customers simultaneously. Your product goes down regardless of how well your own infrastructure is operating.

The impact of an outage depends on your product's criticality. If your AI product is an internal tool used occasionally by a few employees, an outage is an inconvenience. If your AI product is customer-facing and handles support requests, sales inquiries, or critical workflows, an outage means lost revenue, violated SLAs, and damaged trust. If your AI product is part of a safety-critical system — medical decision support, financial fraud detection, infrastructure monitoring — an outage can have serious consequences.

The second risk is rate limiting and throttling. Every provider imposes limits on requests per minute, tokens per minute, and concurrent connections. These limits exist to ensure fair resource allocation and prevent abuse. When you exceed your limit, requests are rejected or queued. During peak demand periods, either your peak or the provider's aggregate peak, you may experience throttling even if you're within your nominal limits.

Rate limiting creates unpredictable performance degradation. Your product works fine most of the time, then suddenly slows to a crawl when traffic spikes. Users experience delays. Operations queue up. Your application still technically works, but the user experience degrades to the point where it might as well be down. This is particularly problematic for products with unpredictable traffic patterns or tight latency requirements.

The third risk is policy changes. Acceptable use policies evolve as providers learn about new abuse patterns, face regulatory pressure, or shift their strategic priorities. A use case that was perfectly acceptable under yesterday's terms might violate today's updated policy. Providers have suspended accounts for policy violations with minimal warning, sometimes based on automated detection systems that produce false positives.

If your product operates in a sensitive domain — content moderation, legal analysis, healthcare, financial services — you're always one policy update away from potential disruption. The provider's risk tolerance may not align with yours. Their policy interpretation may differ from your understanding. The appeals process may be slow or nonexistent. Your entire product can be blocked while you wait for human review of an automated suspension.

The fourth risk is model deprecation and lifecycle management. Every model has a lifecycle. Providers launch new models and deprecate old ones on a regular cadence. When a model is deprecated, you typically receive three to six months notice to migrate. During that window, you need to evaluate the replacement model, update prompts that rely on the old model's specific behaviors, regression test your entire product, and deploy the changes to production.

If you have comprehensive evaluation infrastructure, this migration is a managed process. If you don't, it's a crisis. You're scrambling to test thousands of use cases manually. You're discovering breaking changes in production. You're choosing between rushing a migration that might introduce quality regressions or staying on a deprecated model with no support.

## Resilience Tiers

Provider resilience isn't binary. It's a spectrum based on your product's risk tolerance and resources. Most teams should be operating at Tier 2 minimum. High-stakes products require Tier 3.

Tier 1 is basic monitoring and graceful degradation. At this level, you're not preventing outages from affecting your users, but you're detecting them quickly and handling failures cleanly. You monitor provider status pages and set up automated alerts. You implement retry logic with exponential backoff so transient errors don't cascade into complete failures. You track error rates, latency percentiles, and throughput to detect degradation before it becomes critical. You have runbooks that tell your team what to do when the provider is down.

This tier doesn't prevent outages from impacting your users, but it prevents outages from turning into chaos. Your team knows immediately when something is wrong. Your system doesn't amplify the problem with retry storms. Your users get clean error messages instead of hung requests. For internal tools and low-criticality products, this may be sufficient.

Tier 2 is multi-provider failover. At this level, you integrate at least two model providers and build logic to automatically fail over when the primary provider is unavailable or degraded. Your primary provider might be Claude. Your secondary provider might be GPT-4. When Claude is down or slow, requests automatically route to GPT-4. When both are available, you use the primary.

This architecture requires abstraction. Your application code can't be tightly coupled to a specific provider's API. You need an interface layer that normalizes different providers' response formats, error codes, and capabilities. You need configuration that specifies routing logic: under what conditions do we fail over? How do we fail back? Do we split traffic across providers or keep the secondary as pure backup?

You also need continuous evaluation. You can't wait for an outage to discover that your secondary provider performs poorly on your specific tasks. You need to run your evaluation suite on both providers regularly — monthly at minimum, weekly ideally — so you understand the quality delta. If your primary provider delivers 95% accuracy on your benchmark and your secondary delivers 89%, you know that failover means accepting a temporary quality reduction. This is acceptable during an outage. It's not acceptable if you're failing over due to misconfiguration.

Testing is critical. Many teams configure failover but never test it. When the real outage happens, they discover their failover logic has bugs, their secondary provider's API keys are expired, or their rate limits on the secondary provider are too low to handle production traffic. Failover testing should happen monthly. Deliberately disable your primary provider in a staging environment and verify that traffic routes to secondary cleanly.

Tier 3 is full resilience with self-hosted fallback. At this level, you have commercial provider fallbacks plus a self-hosted open source model as the ultimate fallback. If all commercial providers are unavailable or unacceptably slow, you fail over to your self-hosted model. This model may be less capable than the commercial options, but it's under your complete control. No API can be rate-limited. No provider can go down. No policy can block you.

This tier also includes graceful degradation logic. When you're forced to fall back to progressively less capable models, what happens to the user experience? Do you continue serving results with a warning about reduced quality? Do you queue requests for later processing when the primary provider is available again? Do you route users to human agents? The specific strategy depends on your product, but the key is that you've designed for degradation, not just failure.

Tier 3 also includes contractual protections. At enterprise scale, you negotiate SLAs with your providers that include uptime guarantees and financial penalties for violations. You establish direct support channels that bypass standard ticketing systems when you have production outages. You get advance notice of deprecations and policy changes. These contractual protections don't prevent problems, but they reduce frequency and improve your ability to respond.

## Multi-Provider Architecture Patterns

Building multi-provider resilience requires careful architecture. The naive approach is to write provider-specific code in multiple places throughout your application. This creates maintenance burden and makes it nearly impossible to add new providers or change routing logic. The better approach is abstraction.

The model abstraction layer is an interface that your application code calls without knowing which provider will fulfill the request. The interface accepts a standardized input format — typically a prompt or structured request — and returns a standardized output format. Behind this interface, you have provider-specific adapters that translate the standard format to each provider's API format and translate responses back.

This abstraction allows you to change providers by modifying configuration, not code. Your routing logic sits at the abstraction layer. It decides which provider to use based on request type, current provider health, cost constraints, or A-B testing assignments. Your application code is completely decoupled from provider choice.

The abstraction also normalizes error handling. Different providers return different error codes and structures. Your abstraction layer translates provider-specific errors into standard error types: rate limit exceeded, authentication failed, service unavailable, invalid request. This standardization allows consistent error handling and retry logic regardless of provider.

Request routing logic can be simple or sophisticated depending on your needs. The simplest routing is primary-with-failover: always use provider A unless it's unavailable, then use provider B. More sophisticated routing includes capability-based routing: use provider A for complex reasoning tasks, provider B for simple classification, provider C for high-volume low-value requests. Cost-based routing: use the cheapest provider that meets quality thresholds for each request type. Latency-based routing: use the fastest provider for user-facing requests, slower but cheaper provider for batch processing.

Health checking is critical for reliable failover. Your system needs to continuously assess whether each provider is healthy. Naive health checking just tests whether the API responds to a simple request. Better health checking measures actual latency and error rates on real traffic. Best health checking also runs evaluation queries to detect quality degradation that might not show up as explicit errors.

When a provider fails health checks, how long before you fail back? Immediately might cause thrashing if the provider is intermittently available. Too long leaves you on the secondary provider unnecessarily. A common pattern is exponential backoff: after failing over, wait 30 seconds before testing primary. If still unhealthy, wait 60 seconds. Then 120 seconds. Once the primary provider passes health checks consistently for several minutes, fail back.

## Policy Risk Mitigation

Outages are temporary. Policy violations can permanently terminate your access. If your product relies on a provider and they suspend your account for policy violations, you're down until the issue is resolved. Resolution can take days or weeks. For some suspensions, there is no appeal.

The first mitigation is understanding the policy thoroughly. Don't skim the terms of service. Read them carefully. Understand what's prohibited, what requires special approval, and where the ambiguities are. If your use case is anywhere near a policy boundary — anything involving minors, health information, financial advice, regulated content — get explicit written approval before going to production.

The second mitigation is building policy compliance into your product. If the provider prohibits using outputs for automated decision-making without human review, build human review into your workflow. If the provider requires certain disclaimers for generated content, embed those disclaimers automatically. If the provider prohibits processing personal information without consent, implement consent capture. Making compliance automatic prevents accidental violations.

The third mitigation is monitoring for policy drift. Providers update their policies periodically. Subscribe to their policy update notifications. Review changes when they're announced. Assess whether your current use case remains compliant. If you're in a gray area, reach out to the provider proactively to clarify. Finding out your use case violates new policy from a suspension notice is too late.

The fourth mitigation is multi-provider diversity. If one provider's policy becomes incompatible with your product, having active relationships with other providers means you can migrate rather than shut down. This only works if you've built the abstraction layer that makes provider migration feasible. If you're tightly coupled to one provider, policy risk can kill your product.

## Model Deprecation and Migration

Model deprecation is inevitable. Providers are constantly releasing new models and retiring old ones. The migration process can be smooth or chaotic depending on your preparation.

Comprehensive evaluation is the foundation. When a new model is announced, you should be able to run your full evaluation suite on it within hours. This immediately tells you where the new model improves, where it regresses, and what needs adjustment. Teams without evaluation infrastructure spend weeks manually testing. Teams with strong evaluation infrastructure have preliminary results in a day.

Prompt adaptation comes next. Models have different behaviors, different strengths, different quirks. A prompt optimized for one model may perform suboptimally on another. When you migrate models, you need to review and often revise your prompts. With good evaluation, you can A-B test prompt variations systematically. Without evaluation, you're guessing.

Shadow deployment is the safest migration path. Run the new model in parallel with the old model on production traffic, but don't serve the new model's outputs to users yet. Log both outputs. Compare them. Analyze where they differ. Use this analysis to identify issues before users see them. Once you're confident the new model performs well, gradually ramp up the percentage of traffic using it.

Keeping the old model available as a fallback is critical. Even after successful migration, unexpected edge cases can appear in production. If you can quickly revert to the old model when problems arise, a bad migration becomes a temporary incident instead of a crisis. Only fully decommission the old model once the new model has been stable in production for weeks.

Early adoption creates competitive advantage. When a new model generation launches, the teams that can evaluate, adapt, and deploy it in weeks gain temporary performance advantage over teams that take months. If the new model is meaningfully better, this advantage shows up in product quality, user satisfaction, and win rates. The teams with strong evaluation infrastructure and clean abstraction layers can move fast. The teams without them move slowly or not at all.

## The Resilience Test

The simplest way to assess your provider resilience is to ask: if our primary model provider went down right now for six hours, what would happen?

If the answer is our product stops working completely, you're at Tier 0. This is unacceptable for any customer-facing product. You have work to do.

If the answer is we'd manually switch to a backup provider, you're at Tier 1. Manual failover means your product is down for at least the time it takes someone to notice, make the decision, deploy the change, and verify it works. This might be acceptable for internal tools. It's not acceptable for production services.

If the answer is we automatically fail over to a secondary provider within 30 seconds with known quality and performance characteristics, you're at Tier 2. This is the minimum acceptable standard for customer-facing products.

If the answer is we have three layers of fallback and our users might experience slightly degraded quality but wouldn't notice an outage, you're at Tier 3. This is the standard for high-stakes products.

Build the resilience you need before you need it. Implementing multi-provider failover during an outage is too late. The time to build provider resilience is now, when everything is working. Because eventually, something will break.

The model you're building on today will be obsolete in eighteen months. How do you design products that improve with better models rather than break?
