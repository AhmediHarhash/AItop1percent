# 7.3 â€” Choosing the Right Model for Your Product

In August 2025, a fintech startup built their entire fraud detection system on Claude 3 Opus. The model was exceptional. It caught subtle patterns in transaction data that smaller models missed. The fraud detection rate was 94%, well above their target. The product launched to early customers with strong results. Then the bill came. At production volume, their AI costs were running $47,000 per month for a product with $12,000 in monthly revenue. The unit economics were catastrophic. They had chosen the best model available when what they needed was the cheapest model that could meet their quality bar.

The team made a classic mistake. They optimized for capability when they should have optimized for capability per dollar. The best model is the most expensive, often the slowest, and frequently overkill for the task you're solving. The right model is the one that meets your quality threshold at a cost and latency your product can sustain. This is not a one-time decision you make at the start. This is an ongoing optimization you revisit as your product evolves, as new models launch, and as your volume scales.

## The Four Variables

Every model decision balances four variables. Quality, cost, latency, and availability. You cannot optimize all four simultaneously. Every choice is a tradeoff, and the right tradeoff depends entirely on your product requirements.

Quality is whether the model produces outputs that meet your evaluation criteria. This is the non-negotiable baseline. If the model cannot consistently pass your evaluation rubric, nothing else matters. A model that's fast and cheap but produces unacceptable outputs is not a model you can use. Quality is the floor. Everything else is optimization above that floor.

Cost is what each query costs you in API fees. For most teams in 2026, this is the dominant variable. Model costs range from fractions of a cent per query for small models to dollars per query for frontier models on complex tasks. At low volume, these differences don't matter. At high volume, they determine whether your product is a business or a charity. You need to calculate not just the per-query cost but the monthly cost at your projected volume, the cost per user, and the cost as a percentage of revenue.

Latency is how fast the model responds. For real-time products like chatbots, search interfaces, and voice applications, latency is a feature that users experience directly. Users abandon conversations that take more than three to five seconds to respond. Latency above that threshold kills engagement. For batch products like document processing, nightly analysis jobs, and report generation, latency matters much less. A process that takes 30 seconds instead of three seconds is usually acceptable if it runs in the background.

Availability is the model's uptime, rate limits, and throughput capacity. Can you get the API access you need? What happens during peak load? Are there geographic restrictions? Does the provider have the reliability you need for your SLA? These operational concerns matter more as you scale. A model that works perfectly in testing but has rate limits too low for your production traffic is not a viable choice.

## The Model Evaluation Process

You choose a model the same way you evaluate quality. With data, not intuition. You run a structured comparison using your actual evaluation set from the previous chapter. Take the 50 examples you already judged, run them through multiple models, and score each model's outputs against your rubric.

Start with three to five candidate models. Include a frontier model, a mid-tier model, and a fast cheap model. If you have specific requirements around data sovereignty or customization, include an open-source option. You're not trying to test every available model. You're trying to establish the capability-cost curve for your specific task.

Run all 50 evaluation examples through each model using the same prompt. Keep everything else constant. Same system prompt, same examples, same input format. The only variable is the model itself. Generate all the outputs and save them with clear labels for which model produced which output.

Now score every output using your evaluation rubric. The same rubric you established in the previous chapter. You go through each of the 50 examples, for each model, and apply the same criteria consistently. This is tedious. If you have 50 examples and four models, you're judging 200 outputs. But this is how you get real data about model performance on your task, not the provider's benchmark numbers on their tasks.

Calculate the quality score for each model. Overall pass rate, per-criterion pass rate, whatever metrics matter for your rubric. Now you know which models meet your quality threshold and which don't. Any model that doesn't meet your quality bar is eliminated regardless of cost or speed.

## Measuring Cost and Latency

For each model that meets your quality bar, measure the cost per query. The model providers publish pricing per million tokens, but that's not useful for decision-making. You need cost per query based on your actual inputs and outputs. Count the tokens in your evaluation set, apply the pricing, and calculate the average cost. Then multiply by your expected monthly query volume to get projected monthly spend.

This calculation reveals surprising things. A model that costs twice as much per token might cost the same per query if it generates more concise outputs. A model that looks expensive for a single query might be the cheapest option at high volume if it has volume discounts. The only way to know is to calculate using your real data.

Measure latency the same way. Don't trust the provider's benchmarks. Run your actual evaluation examples through each model and record the response time for each query. Calculate the median latency (P50), the 95th percentile latency (P95), and the 99th percentile latency (P99). These percentiles matter more than averages because the slow queries are what users notice. A model with a median latency of two seconds but a P95 latency of 12 seconds will feel slow in production even though the average looks fine.

Now you have a comparison table. For each model that meets your quality threshold, you have a quality score, a P95 latency, a cost per query, and projected monthly cost at volume. This table is your decision framework.

## The Comparison Reality

The results are almost never obvious. If they were, model selection would be easy. Instead, you find that a frontier model scores 92% on your quality rubric, responds in four seconds at P95, and costs three cents per query. A mid-tier model scores 89% on quality, responds in two seconds, and costs one cent per query. A fast model scores 83% on quality, responds in 800 milliseconds, and costs two-tenths of a cent per query.

Which one is right? It depends entirely on your product requirements. If your quality bar is 90% and latency doesn't matter, you choose the frontier model. If your quality bar is 85% and you need sub-second latency, you choose the fast model. If your quality bar is 85%, latency can be two to three seconds, and you're optimizing for cost, you choose the mid-tier model. There's no universal answer. The right answer is the one that matches your constraints.

This is why you need the evaluation data. Without it, you're guessing about quality differences and whether they matter. With it, you can make a deliberate tradeoff. You can say, "We're accepting a three-point quality drop to cut costs by 70%" and make that decision with full understanding of what you're trading.

## Model Tiers in Production

Most successful AI products in 2026 don't use a single model. They use a tiered model strategy where different queries go to different models based on complexity, urgency, and importance. This routing approach optimizes cost without sacrificing quality where it matters.

Frontier models are for the hardest tasks. Complex reasoning, nuanced generation, multi-step analysis, cases where you need the highest capability available. These models are expensive and slower but they're the only option for tasks at the edge of AI capability. You use them sparingly, only when mid-tier models demonstrably fail on your evaluation set.

Mid-tier models are the workhorses of production AI. Models like Claude Opus 4.5, GPT-5, and Gemini 1.5 Pro. They handle most production tasks well. Strong quality at moderate cost, reasonable latency, broad capability. If your evaluation shows that a mid-tier model meets your quality bar, this is usually the right choice. They're the default until you have a specific reason to go up or down the capability ladder.

Fast cheap models are for simple tasks and high-volume use cases. Classification, extraction, routing, intent detection, anywhere that speed matters more than depth. Models like Claude Haiku, GPT-5 Mini, and Gemini Flash. They cost a fraction of what frontier models cost and they respond in hundreds of milliseconds instead of seconds. If your task doesn't require complex reasoning and your evaluation shows these models pass your quality bar, they're the economically rational choice.

Open-source models are for specialized requirements. Data sovereignty mandates that prohibit sending data to third-party APIs. Costs that need to be near-zero at high volume. Deep customization through fine-tuning. Control over the inference environment. These models require more infrastructure investment but give you control and flexibility that hosted models don't.

## The Routing Strategy

A customer support product might route queries through three tiers. First, a fast model classifies the intent. Is this a question, a complaint, a request? What category does it fall into? This classification takes 400 milliseconds and costs a tenth of a cent. Second, a mid-tier model drafts the response using the classification and retrieval from the knowledge base. This takes two seconds and costs a cent. Third, if the mid-tier model flags the case as complex or high-stakes, it escalates to a frontier model for a more nuanced response. This takes four seconds and costs three cents, but it only happens for 8% of queries.

The blended cost of this tiered system is less than half of what it would cost to run all queries through the frontier model, and the quality is nearly identical because you're using the frontier model exactly where it adds value. This routing strategy is how products achieve both quality and sustainable economics.

The routing logic itself is usually simple. Rule-based routing based on metadata, classification-based routing using a fast model, or confidence-based routing where the mid-tier model flags cases it's uncertain about. You don't need sophisticated orchestration. You need clear criteria for when each tier is appropriate and instrumentation to measure whether your routing decisions are working.

The instrumentation is critical. You need to track which queries go to which tier, what the quality outcomes are for each tier, and what the cost distribution looks like. If you find that frontier model queries have the same quality score as mid-tier queries, you're over-routing to the expensive tier. If you find that mid-tier queries are failing quality checks, you're under-routing to the frontier tier. The routing thresholds should be dynamic, tuned based on observed performance data.

Some teams resist routing because it adds complexity. They'd rather use one model for everything and keep the architecture simple. This instinct is wrong. The complexity of routing is minimal, usually just a conditional statement and some logging. The cost savings are massive. A well-designed routing strategy typically cuts AI costs by 50-70% while maintaining or improving quality. That economic difference is often what makes a product viable.

## The Good Enough Principle

The central principle of model selection is this: don't use a more expensive model when a cheaper model passes your evaluation. Quality above your threshold is wasted money. You're paying for capability you don't need and users don't value. Aim for the cheapest model that consistently meets your quality requirements and invest the savings in better evaluation, better data, better retrieval, and better product features.

This principle runs counter to engineering instinct. Engineers want to use the best tools available. But the best model is not the best tool for your product if it makes your unit economics unsustainable. Your goal is not to maximize quality. Your goal is to meet your quality bar at the lowest sustainable cost. Once you meet the bar, additional quality improvements are worth less than cost reductions.

This doesn't mean you always choose the cheapest model. It means you choose the cheapest model that meets your requirements. If your requirements demand frontier model capability, you use a frontier model. But you verify that demand with evaluation data, not assumptions.

## Model Selection as Ongoing Optimization

Model selection is not a one-time decision you make at the beginning of your project. It's an ongoing optimization you revisit every few months. New models launch constantly. Pricing changes. Your product requirements evolve. Your volume scales. A model that was right at launch might not be right six months later.

You should re-run your model evaluation quarterly. Take your current evaluation set, run it through the latest models, measure quality and cost, and see if a different choice makes sense. Maybe a new mid-tier model now meets your quality bar when it didn't three months ago. Maybe your volume has grown enough that the economics favor a different tier. Maybe a model you're using had a quality regression in a recent update that you need to catch.

This ongoing evaluation is not optional. It's part of operating an AI product. Models are infrastructure, and infrastructure choices need to be revisited as the landscape changes. Teams that lock in a model choice at launch and never reconsider it end up overpaying, underperforming, or both.

## The Decision Framework

When you choose a model, you need four pieces of information. First, what is your quality threshold? What pass rate on your evaluation rubric is acceptable for your product to ship? Second, what is your latency requirement? What response time do users need for acceptable experience? Third, what is your cost constraint? What per-query cost or monthly cost can your product sustain? Fourth, what are your operational requirements? Data sovereignty, uptime, rate limits, geographic availability.

Once you have those requirements, you run the evaluation process. Test candidate models, score quality, measure latency, calculate cost, eliminate any option that doesn't meet your requirements, and choose the cheapest option that remains. This is a structured decision based on data, not intuition or habit.

The decision becomes easier when you frame it as a constrained optimization problem. Quality is the constraint. Cost is what you're optimizing. You set the minimum acceptable quality level based on your product requirements and user expectations. Then you find the cheapest model that clears that bar. If multiple models clear the bar, you choose based on secondary factors like latency, operational simplicity, or future flexibility.

This framework prevents both over-spending and under-delivering. Over-spending happens when you use frontier models for tasks that mid-tier models can handle. Under-delivering happens when you choose cheap models that can't meet your quality threshold. Both mistakes are common. Both are avoidable with structured evaluation.

The teams that succeed with AI products are the teams that make this decision deliberately. They know their quality thresholds, they measure model performance against those thresholds, and they optimize cost within the constraint of acceptable quality. The teams that fail are the teams that use the best available model without measuring whether they need it or the cheapest available model without verifying it works. Both extremes lead to bad outcomes. The right path is measurement, thresholds, and optimization.

The model landscape in 2026 gives you real choices. Five years ago, you used whatever model was available and hoped it worked. Today, you have frontier models, mid-tier models, fast models, and open-source options across multiple providers. This abundance of choice makes the decision harder but also more important. The right model choice can cut your costs by 80% while maintaining quality. The wrong choice can make your product economically unviable even if it works technically.

Your model choice shapes your product's quality, cost structure, and user experience. Make the choice carefully, based on evaluation data from your actual use case, and revisit the choice regularly as the model landscape evolves. The next step is building the prompt architecture that makes your chosen model consistently perform at the level your evaluation measured.