# Chapter 7.3 — Choosing the Right Model for Your Product

"Use the best model available" sounds like good advice. It isn't. The best model is the most expensive, the slowest, and often overkill for your task. The right model is the one that meets your quality bar at a cost and latency you can sustain.

---

### The Model Selection Matrix

Every model decision balances four variables:

**Quality.** Does the model produce outputs that meet your evaluation criteria? This is the non-negotiable — if the model can't pass your eval, nothing else matters.

**Latency.** How fast does the model respond? For real-time products (chatbots, search, voice), latency is a feature. Users abandon interactions that take more than 3-5 seconds. For batch products (document processing, analysis), latency matters less.

**Cost.** What does each query cost? At your expected volume, what's the monthly spend? As we covered in Chapter 5.12, unit economics determine whether your product is a business or a hobby.

**Availability.** What's the model's uptime? What are the rate limits? Can you get the throughput you need during peak hours?

---

### The Model Evaluation Process

**Step 1: Start with your eval set.** Use the evaluation dataset from Chapter 7.2 (or expand it). Run the same inputs through multiple models.

**Step 2: Score each model.** Apply the same rubric to every model's outputs. Now you have a quality comparison.

**Step 3: Measure latency.** For each model, record P50, P95, and P99 latency on your actual inputs (not the provider's benchmarks, which use different input lengths).

**Step 4: Calculate cost.** Based on input/output token counts from your actual eval set, calculate the cost per query for each model.

**Step 5: Build the comparison table.** For each model: quality score, P95 latency, cost per query, and any availability constraints.

Often, the results are surprising. A model that scores 5% lower on quality might cost 80% less and respond 3x faster. Whether that tradeoff is worth it depends entirely on your product requirements.

---

### Model Tiers in Practice

Most products in 2026 use a tiered model strategy:

**Frontier models** (Claude 3.5 Opus, GPT-4-class, Gemini Ultra): Use for the hardest tasks — complex reasoning, nuanced generation, multi-step analysis. Expensive, slower, highest capability.

**Mid-tier models** (Claude 3.5 Sonnet, GPT-4o, Gemini Pro): Use for most production tasks. Strong quality at moderate cost. The workhorses of AI products.

**Fast/cheap models** (Claude Haiku, GPT-4o-mini, Gemini Flash): Use for simple tasks — classification, extraction, routing, and anywhere speed matters more than depth. Low cost, fast, good enough for many tasks.

**Open-source models** (Llama 3, Mistral, Qwen): Use when data sovereignty is required, costs need to be minimal at high volume, or you need deep customization.

The art is routing each query to the right tier. A customer support product might use a fast model to classify the intent, a mid-tier model to draft the response, and a frontier model only for complex escalations. This routing strategy can cut costs by 50-70% while maintaining quality where it matters.

---

### The "Good Enough" Principle

Don't use a frontier model when a mid-tier model passes your eval. Don't use a mid-tier model when a fast model passes your eval. Quality above your bar is wasted money. Aim for the cheapest model that consistently meets your quality requirements — then invest the savings in better evals, better data, and better product features.

---

*Next: your first prompt architecture — the system prompt, few-shot examples, and output structure that become the foundation of your product.*
