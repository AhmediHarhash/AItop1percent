# 3.9 â€” How Risk Tier Dictates Your Monitoring Depth

In March 2025, a financial services company discovered that their AI-powered fraud detection system had been underperforming for six weeks. The system was flagging forty percent fewer suspicious transactions than it had in January. Investigators eventually traced the issue to a subtle shift in transaction patterns after a payment processor update. The shift was not dramatic enough to trigger the company's error rate alerts. It was not obvious in their weekly dashboard reviews. It emerged only when a compliance officer happened to compare month-over-month fraud detection volumes and noticed the drop. By the time they caught it, the company had missed an estimated 1.8 million dollars in fraudulent transactions. Their monitoring had been appropriate for a Tier 1 internal tool: basic error tracking and weekly reviews. But they were operating a Tier 3 system that affected customer money and regulatory compliance. The gap between their monitoring depth and their actual risk tier cost them money, reputation, and a regulatory fine.

Building an AI product without monitoring is like driving at night without headlights. You might be fine for a while. The road might be straight. The weather might be clear. But when something goes wrong, and it will, you will not see it until you have already crashed. Monitoring is how you see. The depth of monitoring you need, the frequency at which you check, and the cost you must bear all scale with your risk tier. Match your monitoring investment to your tier, or accept that you are operating blind in the dark.

## Tier 1 Monitoring Tells You If Anyone Uses It

At Tier 1, your product is internal, experimental, or human-reviewed before any action is taken. The consequences of a quality issue are minor. A confused employee asks a colleague for clarification. A draft document gets edited before it is sent. A tool suggestion gets ignored. Your monitoring exists to answer basic questions: is anyone using this? Is it broken? Is it annoying people? You do not need sophisticated instrumentation. You need basic health signals and a lightweight way to collect feedback.

Track usage volume first. How many queries is the system handling per day? Is usage growing, stable, or declining? If you launched a feature and nobody is using it, you need to know. If usage suddenly drops to zero, something is broken. A simple counter is sufficient. You do not need percentile breakdowns, cohort analysis, or funnel tracking. You need to know whether the system is being used.

Track error rates second. How many requests are failing? An error rate of zero is normal for some periods. An error rate of fifty percent means the system is broken. Set up a basic alert: if the error rate exceeds ten percent for more than an hour, send a notification. The threshold and the time window depend on your system, but the principle is universal. You want to know when the system is failing before users stop using it entirely.

Track user feedback third. If you have a thumbs up and thumbs down mechanism, track the ratio. If thirty percent of users are giving thumbs down, something is wrong with quality. If you have a feedback form or a Slack channel where users report issues, monitor it. Read the reports. If multiple people are complaining about the same issue, prioritize fixing it. User feedback at Tier 1 is informal and qualitative. That is fine. You are not trying to calculate statistically significant quality metrics. You are trying to learn what is working and what is not.

Track basic latency fourth. Is the system responding quickly enough that users do not abandon the interaction? You do not need to measure the ninety-ninth percentile. You do not need to break down latency by query type or user segment. You need to know whether responses are taking two seconds or twenty seconds. If they are taking twenty seconds, users will stop using the system. Measure median latency. If it creeps above five seconds, investigate.

How often should you check these metrics? Weekly dashboard reviews are sufficient. Open your monitoring dashboard once a week. Look at usage volume, error rates, feedback scores, and latency. If everything looks normal, move on. If something looks wrong, investigate. You do not need daily reviews. You do not need real-time alerting except for error rate spikes. This is Tier 1. The stakes are low. Weekly is enough.

What should you do when something is wrong? Fix it during work hours. You do not need an on-call rotation. You do not need an incident response process. You do not need a formal root cause analysis. If the system is broken, fix it. If the quality has dropped, improve it. The Tier 1 monitoring and response process is lightweight because the Tier 1 risk profile is low. Over-investing in monitoring wastes time. Under-investing means you miss obvious problems. Calibrate correctly.

## Tier 2 Monitoring Tells You If Quality Drops

At Tier 2, your product is customer-facing. Quality issues damage your brand, frustrate paying customers, and generate support tickets. Your monitoring exists to detect quality degradation before customers notice, or at minimum as soon as customers notice, so you can respond quickly. You need more than basic health signals. You need quality signals, business metrics, and infrastructure signals that let you diagnose issues quickly.

Track everything from Tier 1 first. Usage volume, error rates, user feedback, and latency are still foundational. Add precision. Instead of just median latency, track the fiftieth percentile, the ninety-fifth percentile, and the ninety-ninth percentile. The median tells you what most users experience. The ninety-ninth percentile tells you what your slowest users experience. If the ninety-ninth percentile latency is ten times the median, you have a tail latency problem that affects a meaningful number of users.

Add quality scores on sampled production traffic. Take one to five percent of your production queries and run them through an evaluation pipeline. This pipeline might use automated scoring, LLM-as-judge evaluation, or lightweight human review, depending on your volume and budget. The goal is to measure whether the quality you are delivering in production matches the quality you measured in your evaluation set. If your evaluation set shows ninety percent accuracy but your production sample shows seventy-five percent accuracy, you have a distribution shift problem. The real-world queries are different from your evaluation examples. Fix it.

Add cost per query. How much are you spending on model API calls, retrieval, post-processing, and infrastructure per query? Multiply by your daily query volume. Is the total cost sustainable given your revenue or budget? If your cost per query is increasing over time, investigate why. Did the model provider raise prices? Did your queries get longer? Did you add a retrieval step that doubles your token usage? Cost monitoring at Tier 2 is not optional. Runaway costs kill products.

Add model provider status monitoring. Are the APIs you depend on available and performant? Major model providers publish status pages. Monitor them. Set up alerts for outages or degraded performance. If your provider is experiencing elevated latency or error rates, you need to know immediately so you can decide whether to switch to a fallback provider, enable caching more aggressively, or communicate the issue to your users.

Add user satisfaction signals beyond thumbs up and thumbs down. Track CSAT scores, NPS, support ticket volume, and support ticket topics. If customer satisfaction drops or support tickets spike, something is wrong. Correlate the changes with your recent releases. Did satisfaction drop after you deployed version 2.4? Roll back and investigate. Did support tickets about incorrect outputs double last week? Sample those outputs and figure out what changed.

Add traffic pattern anomaly detection. Is your query volume suddenly fifty percent higher than usual? That might be good news, or it might mean a bot is hammering your API. Is your query volume suddenly fifty percent lower than usual? That might mean customers are churning, or it might mean your website is broken and users cannot reach the feature. Monitor both spikes and drops. Investigate anomalies.

How often should you check these metrics? Daily automated reports are the baseline. Every morning, receive a summary of yesterday's metrics. Review it. Look for anomalies. Add real-time alerting for critical metrics. If error rate exceeds five percent, alert immediately. If latency exceeds your SLA threshold, alert immediately. If quality scores drop below your minimum threshold, alert immediately. Define your thresholds in advance. Tune them based on experience. The goal is to know about critical issues within minutes to hours, and to know about non-critical issues within a day.

What should you do when something is wrong? At Tier 2, you need a defined incident response process. Who gets alerted? What are the severity levels? What is the escalation path? For critical issues, you need rollback capability within thirty minutes. For non-critical issues, you need a triage process that decides whether to fix immediately, fix in the next sprint, or defer. Document incidents. Track trends. If you have the same incident twice, improve your monitoring or your system to prevent the third occurrence.

Your detection target at Tier 2 is to know about quality degradation within twenty-four hours. If quality dropped on Monday, you should know by Tuesday. If you are learning about quality issues from customer complaints a week later, your monitoring is insufficient. Upgrade it.

## Tier 3 Monitoring Tells You Everything, Immediately

At Tier 3, your product makes decisions that affect user safety, user money, or user legal standing. A quality failure is not an inconvenience. It is harm. Your monitoring exists to catch every significant failure, to alert immediately, and to provide the evidence you need to demonstrate that you acted with due diligence. You need not just quality signals but safety signals, fairness signals, and domain-specific signals that let you detect harm in real time.

Track everything from Tier 2 first. Add precision and segmentation. Do not just track overall quality. Track per-segment quality. Break down your quality metrics by user segment, input type, demographic category, query complexity, and any other dimension that matters to your domain. If your overall accuracy is ninety percent but your accuracy for users over sixty-five is seventy percent, you have a fairness problem. If your overall accuracy is ninety percent but your accuracy for complex queries is sixty percent, you have a capability gap. Per-segment monitoring catches issues that overall metrics hide.

Add drift detection. Is the distribution of inputs changing over time? Are the model's outputs changing in character even when the inputs are stable? Drift is insidious because it happens slowly. Your evaluation set remains static. Your model performance on that evaluation set remains stable. But the real-world distribution shifts, and your real-world performance degrades. Detect drift by monitoring input distributions, output distributions, and the relationship between them. If any of these shifts significantly, investigate.

Add safety-specific metrics. What is the rate of harmful outputs? What is the rate of policy-violating outputs? What is the rate of outputs that a human reviewer flagged as inappropriate? Define harm and policy violations precisely for your domain. If you are building a medical AI, harm includes clinically dangerous advice. If you are building a financial AI, harm includes recommendations that violate securities law or lead to significant financial loss. If you are building a content moderation AI, harm includes failing to remove content that violates platform policies. Measure your harm rate continuously. Set your tolerance threshold as close to zero as feasible. Alert immediately when the rate exceeds the threshold.

Add human override rate. In production, how often do domain experts review the AI's output and change it? A high override rate indicates that the system is not yet reliable enough. A low override rate might indicate reliability, or it might indicate that humans are trusting the system too much without adequate review. Track both the rate and the reasons. If humans are consistently overriding the system for the same reason, that reason is a bug. Fix it.

Add confidence calibration monitoring. When your system says it is ninety percent confident, is it actually correct ninety percent of the time? Measure this continuously in production. If your system is overconfident, users will trust it when they should not. If your system is underconfident, users will ignore it when they should listen. Calibration errors cause harm. Detect them and correct them.

Add outlier detection. Are there inputs or outputs that are statistically unusual? Outliers often indicate problems. An input that is ten times longer than usual might be an attempted attack. An output that uses words your system has never used before might indicate a hallucination or a jailbreak. Flag outliers for human review. Some will be benign. Some will reveal critical issues.

Add adversarial input detection. Are users attempting to manipulate or exploit your system? Common adversarial techniques include prompt injection, jailbreaking, and input perturbation designed to cause misclassification. Monitor for patterns that suggest adversarial behavior. Log the attempts. Feed them back into your evaluation set and your adversarial testing process. If an attacker finds a way to break your system, you need to know immediately, and you need to fix it before the technique spreads.

How often should you check these metrics? Real-time monitoring with automated alerting is non-negotiable. Safety incidents must alert within minutes. Quality incidents must alert within an hour. Configure your alerting thresholds conservatively. It is better to investigate ten false alarms than to miss one real incident. In addition to real-time alerting, conduct hourly quality sampling. Take a random sample of the last hour's outputs. Run them through your evaluation pipeline. If quality has degraded, you will catch it within an hour. Conduct daily quality reviews by a human operator. A person with domain knowledge looks at a sample of outputs and assesses whether anything looks wrong. Conduct weekly deep dives with domain experts. Physicians, lawyers, financial analysts, or whoever your domain requires. Show them production data. Ask them whether they see patterns of concern. They will catch issues that automated systems miss.

What should you do when something is wrong? At Tier 3, your incident response process must be tiered by severity. Safety incidents require immediate automated mitigation. If the system generates a clinically dangerous output, block it before the user sees it. Enable a fallback mechanism. Alert a human. The human investigates and decides whether to continue blocking, to roll back, or to fix and resume. Quality incidents that do not rise to the level of safety incidents still require rapid human review. Within an hour, a qualified person assesses the issue, determines the severity, and decides on a response. That response might be rollback, might be a configuration change, might be increased human review, or might be acceptance if the issue is minor and temporary.

All incidents must be documented. Record what happened, when it happened, what the impact was, what the response was, and what the root cause was. Feed the lessons back into your evaluation process, your release process, and your monitoring. If the same issue happens twice, you failed to learn from the first incident. Do not fail three times.

Your detection target at Tier 3 is to know about quality degradation within one hour and to know about safety issues within fifteen minutes. If you cannot meet these targets, your monitoring infrastructure is not sufficient for Tier 3. Upgrade it.

## Tier 4 Monitoring Proves Compliance Continuously

At Tier 4, your product is subject to regulatory oversight. Your monitoring must not only detect issues and enable response but also serve as compliance evidence. Regulators will ask what you monitored, how often you monitored, what you found, and what you did about it. Your monitoring system is not just protecting users. It is building your compliance record.

Track everything from Tier 3 first. Add regulatory-specific metrics. What does the applicable regulation require you to monitor? The EU AI Act requires continuous monitoring of high-risk systems, with specific attention to performance, safety, and bias. HIPAA requires monitoring of data access and security events. The FDA requires monitoring of software performance and adverse events. Read the regulation. Identify the monitoring obligations. Build the metrics. Track them continuously.

Add serious incident detection. Many regulations require you to report serious incidents to authorities within a defined timeline. The EU AI Act requires reporting serious incidents without undue delay. The FDA requires medical device adverse event reporting within timelines that vary by severity. Your monitoring system must detect serious incidents automatically, classify them by severity, and trigger the reporting workflow. If you detect a serious incident manually three days after it occurred, you may have already missed the reporting deadline. Automate detection.

Add audit trail completeness monitoring. Are you logging every decision your system makes with full context? Are the logs immutable and tamper-proof? Are they stored with sufficient retention to meet regulatory requirements, which often range from five to ten years? Monitor the health of your logging infrastructure. If logs are being dropped, if storage is running out, if access controls are misconfigured, you need to know immediately. Incomplete audit trails during a regulatory audit are a compliance failure.

Add data processing compliance monitoring. Are you processing data within the scope authorized by your users, your privacy policy, and the applicable regulation? Are you processing data only in the geographic regions where you are authorized to operate? Are you applying the correct data retention and deletion policies? Monitor these continuously. A data processing violation is a regulatory violation.

Add system version tracking. At any moment, you must be able to answer: which model version is running? Which prompt version is running? Which configuration version is running? Which evaluation data version was used to validate this model? This tracking is not just about debugging. It is about regulatory accountability. If a regulator asks what was running on a specific date, you must answer instantly. If you cannot, you are not ready for Tier 4.

How often should you check these metrics? Continuous automated monitoring with real-time alerting is required, just as at Tier 3. Add regulatory reporting cadence. Some regulations require periodic reports: monthly, quarterly, annually. Your monitoring system must aggregate the data needed for these reports automatically. You should be able to generate a compliance report on demand without manual data collection.

What should you do when something is wrong? Follow your Tier 3 incident response process. Add regulatory notification. If a serious incident occurs, follow the regulatory reporting requirements exactly. Know the timeline. Know the format. Know the recipient. Miss the deadline or file the wrong format, and you have compounded the incident with a compliance violation. Your legal and compliance teams must be involved in every serious incident. They determine whether regulatory notification is required and handle the filing.

Document everything. Every monitoring finding, every alert, every response action, every investigation, every remediation. Store the documentation in a way that supports regulatory review. Regulators may audit you with little notice. You must be able to produce the evidence they request within days, not weeks. If your documentation is scattered across Slack threads, email chains, and people's memories, you are not ready.

## The Investment Curve Scales with Risk

Monitoring is not free. It costs engineering time to build the instrumentation, infrastructure cost to collect and store the data, operational cost to review dashboards and respond to alerts, and domain expert cost to conduct human reviews. The cost scales with your risk tier. Budget accordingly.

At Tier 1, monitoring costs are near zero. Basic logging is usually built into your platform. A simple dashboard can be set up in a few hours. Weekly reviews take ten minutes. Total investment: a few hours to set up, negligible ongoing cost.

At Tier 2, monitoring costs are moderate. You need dedicated infrastructure for quality sampling, alerting, and dashboarding. You need to instrument your code to emit the right metrics. You need to configure alerts and tune thresholds. You need to establish an incident response process. Budget several weeks of engineering time to build the system. Budget ongoing operational costs to maintain the infrastructure and respond to incidents. Expect to spend a few thousand dollars per month on infrastructure and a few hours per week on review and response.

At Tier 3, monitoring costs are significant. You need real-time quality monitoring with per-segment breakdowns. You need drift detection. You need safety alerting. You need domain expert review processes that run continuously. Budget months of engineering time to build the infrastructure. Budget ongoing operational costs that include domain expert salaries or consulting fees. Expect to spend tens of thousands of dollars per month on infrastructure and domain expert time. This is expensive. It is also non-negotiable. If you are not willing to pay for Tier 3 monitoring, you are not willing to operate a Tier 3 product responsibly.

At Tier 4, monitoring costs are major. You need everything from Tier 3 plus regulatory-specific metrics, serious incident detection, audit trail infrastructure, and compliance reporting capabilities. Budget for a dedicated team. Expect to spend hundreds of thousands of dollars per year on monitoring infrastructure, tooling, domain experts, and compliance staff. This is the cost of operating in a regulated environment. It is built into the business model of every successful regulated AI product. If your business model does not account for it, your business model is wrong.

Do not try to cut costs by under-monitoring relative to your risk tier. A Tier 3 product with Tier 1 monitoring is a product that will harm users and face regulatory consequences. The money you save by skipping real-time quality monitoring or domain expert review will be dwarfed by the cost of the incident, the lawsuit, or the regulatory fine.

Do not waste money by over-monitoring relative to your risk tier. A Tier 1 product with Tier 3 monitoring is a product that never ships because the team is spending all their time building monitoring infrastructure. The marginal value of real-time drift detection on an internal tool is zero. Invest proportionally.

## Monitoring Is Not Optional

Some teams treat monitoring as a nice-to-have feature that they will add later, after they ship, after they prove product-market fit, after they raise the next round. This is a mistake. Monitoring is not optional. It is how you know whether your product works. Without monitoring, you are operating blind. You do not know if quality has dropped. You do not know if users are frustrated. You do not know if a failure mode is emerging. You only find out when the damage is done.

Build monitoring from the beginning. It does not have to be sophisticated on day one. At Tier 1, basic usage tracking and error logging are enough. But build it. As your product evolves and your risk tier increases, evolve your monitoring with it. When you move from Tier 1 to Tier 2, add quality sampling and alerting. When you move from Tier 2 to Tier 3, add real-time monitoring and domain expert review. When you move from Tier 3 to Tier 4, add regulatory metrics and compliance reporting.

Your monitoring investment is paid back many times over. It catches incidents before they become disasters. It provides the data you need to improve your product. It gives you the confidence to ship changes knowing that you will detect problems quickly. It provides the evidence you need to demonstrate to regulators, to customers, and to stakeholders that you are operating responsibly.

Match your monitoring depth to your risk tier. Invest proportionally. Build it early. Evolve it continuously. Monitor is how you see in the dark. Without it, you will crash.

Next, we examine the most expensive mistake in risk classification: getting the tier wrong in the first place.
