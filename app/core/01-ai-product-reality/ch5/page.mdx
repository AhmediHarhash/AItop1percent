# Chapter 5 — Scoping & Requirements for AI Products

Traditional product requirements assume deterministic software: if the input is X, the output is always Y. AI products don't work that way. The output might be Y, or Y-ish, or something completely wrong that sounds confidently correct.

This means your requirements process needs to account for uncertainty, error tolerance, fallback paths, cost constraints, safety threats, and explainability — none of which appear in a standard PRD template. Teams that use traditional scoping for AI products end up with requirements that are either too vague ("the AI should be helpful") or too rigid ("the AI must always return the correct answer") — and both fail.

This chapter gives you the scoping and requirements framework built for AI. From PRDs that work, to success criteria, tradeoffs, error tolerance, fallbacks, data needs, compliance, threat modeling, SLOs, unit economics, and explainability — everything you need to define before you build.

---

## What This Chapter Covers

- **5.1** — Why traditional PRDs fail for AI
- **5.2** — Writing AI product requirements that work
- **5.3** — Defining success criteria before you build
- **5.4** — The latency, cost, and quality triangle
- **5.5** — User tolerance for errors (by product type)
- **5.6** — Fallback design (the 5-level hierarchy)
- **5.7** — Data requirements you'll discover too late
- **5.8** — Compliance and legal requirements by industry
- **5.9** — The pre-mortem (predicting failure before you ship)
- **5.10** — AI threat modeling (prompt injection, data exfiltration, tool abuse, RAG poisoning)
- **5.11** — SLOs for AI products (quality, latency, cost, safety)
- **5.12** — Unit economics (cost-to-serve, margins, pricing)
- **5.13** — Explainability (when and how to open the black box)

---

Skip this chapter and you'll spend months building the wrong thing. Invest the time here and every decision downstream gets easier.

*Let's start with why your current PRD template won't work for AI.*
