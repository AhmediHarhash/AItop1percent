# 3.7 â€” How Risk Tier Dictates Your Eval Strategy

In late 2024, a healthcare technology company launched an AI-powered symptom checker with what they described as "rigorous evaluation." They had tested it on fifty carefully selected cases reviewed by their product team. The system performed well. They shipped it. Within three weeks, emergency room physicians began reporting a pattern: the system was consistently underestimating the urgency of cardiac symptoms in women over sixty. The evaluation had been rigorous by Tier 1 standards. But they were building a Tier 3 product. The mismatch between their evaluation rigor and their actual risk tier meant they had tested for the wrong things, with the wrong people, at the wrong frequency. The system was pulled from production. The company spent seven months rebuilding their evaluation infrastructure before they could relaunch.

The lesson is simple: your risk tier is not just a label you apply after you build. It is the blueprint for how much evaluation you need, what kind of evaluation you need, and how often you need it. Teams that apply Tier 1 evaluation rigor to a Tier 3 product create incidents. Teams that apply Tier 3 evaluation rigor to a Tier 1 product waste months of time and never ship. The evaluation strategy must match the risk tier, not your preferences or your schedule.

## Tier 1 Evaluation Exists to Help You Improve

At Tier 1, your product is internal, human-reviewed, or experimental. The risk is low. The consequences of a quality issue are minor: a confused employee, a slightly unhelpful suggestion, a task that takes an extra minute. Your evaluation exists to help you improve, not to gate releases. This is not an excuse to skip evaluation. It is a directive to calibrate your investment correctly.

Your evaluation set should contain fifty to one hundred examples. These examples should cover your main use cases and a handful of known edge cases. You are not trying to be comprehensive. You are trying to be representative. If your system summarizes internal documents, your evaluation set should include the most common document types your employees actually use: meeting notes, project plans, technical specifications. If you discover that the system struggles with financial reports, add a few of those. But do not try to enumerate every possible document type. You will spend months building an evaluation set and never ship.

Run your evaluation before major changes. If you switch to a new model, run the evaluation. If you rewrite your prompt significantly, run the evaluation. If you change your retrieval strategy, run the evaluation. Weekly spot checks are valuable if you have the time, but they are not required. This is Tier 1. The system is not making decisions that affect customer money, customer safety, or customer privacy. You can afford to move quickly.

The evaluation method is manual review by the product team. You do not need domain experts. You do not need formal annotation guidelines. The people building the product can judge whether the outputs are good enough. Sit down with your designer, your engineer, and your product manager. Look at fifty outputs. Ask yourselves: is this helpful? Is it safe? Does it meet the basic quality bar we set? If yes, ship it. If no, fix the most obvious problems and run the evaluation again.

What should you measure? Core accuracy is first. Is the system answering the question correctly? Is it summarizing the document faithfully? Is it generating the code that solves the problem? User satisfaction is second. If you have a feedback mechanism, track it. Thumbs up and thumbs down are sufficient. Basic safety is third. Make sure the system is not generating harmful, offensive, or wildly inappropriate outputs. You do not need a complex taxonomy of harm categories. You need a human looking at the outputs and asking: would I be embarrassed if someone saw this?

Skip the complex metrics. Do not calculate precision and recall unless your task has a well-defined ground truth and you are certain those metrics matter. Do not build custom evaluation rubrics with seven dimensions and weighted scoring. Do not run bias audits across demographic slices. These activities are appropriate at higher tiers. At Tier 1, they slow you down without providing meaningful benefit. Your goal is to learn fast and improve quickly. Over-engineering your evaluation process works against that goal.

The release process is informal. If the evaluation set looks good and the team is confident, ship it. There are no formal gates, no sign-off meetings, no staged rollouts. You merge the change, you deploy, you watch usage for a day or two. If something breaks, you fix it. If users complain, you iterate. This is not sloppiness. This is appropriate calibration for low-risk products.

## Tier 2 Evaluation Exists to Protect the Customer Experience

At Tier 2, your product is customer-facing. Quality issues damage your brand reputation, frustrate users, and generate support tickets. Your evaluation exists to protect the customer experience and to prove, before you ship, that the system works well enough to put in front of paying customers.

Your evaluation set should contain two hundred to five hundred examples. These examples must cover all main use cases, all known edge cases, and a stratified sample across user types, input categories, and difficulty levels. If your product serves both individual users and enterprise customers, your evaluation set should include examples from both segments. If your product handles both simple and complex queries, your evaluation set should include both. If you discover a new failure mode in production, add representative examples to your evaluation set immediately. Update your evaluation set monthly as you learn what your users actually do and where your system actually struggles.

Run your evaluation before every deployment. This is non-negotiable. If you are deploying a new model version, a new prompt, a new retrieval configuration, or any change that could affect output quality, you run the full evaluation first. In addition to pre-deployment evaluation, sample production traffic daily or weekly for ongoing quality monitoring. Take one percent of your production queries, run them through your evaluation pipeline, and track whether quality is stable, improving, or degrading. This ongoing sampling catches quality drift that your pre-deployment evaluation might miss.

Your evaluation method is a combination of automated scoring and human review. Use automated scoring for measurable dimensions: accuracy against ground truth, latency, format compliance, length constraints, cost per query. Use human review for subjective dimensions: helpfulness, tone, appropriateness, clarity. If your volume is high and manual review is expensive, consider using an LLM-as-judge approach for scalable quality scoring. This means using a strong model like GPT-5 or Claude Sonnet to evaluate your production model's outputs against a rubric you define. LLM-as-judge is not perfect, but it scales better than pure human review and catches most quality issues.

What should you measure? Start with everything from Tier 1: accuracy, safety, basic user satisfaction. Add helpfulness. Is the output actually useful to the user, or is it technically correct but unhelpful? Add tone consistency. If your brand voice is friendly and conversational, are the outputs matching that tone? Add latency. Are responses fast enough that users do not abandon the interaction? Add cost per query. Are you spending fifty cents per query when your revenue per query is twenty cents? Add user satisfaction metrics with more precision: CSAT scores, NPS, or custom satisfaction surveys.

The critical measurement discipline at Tier 2 is per-segment performance. Overall quality that averages ninety percent is meaningless if quality for your enterprise customers is sixty percent. Break down your evaluation results by user segment, input type, query complexity, and any other dimension that matters to your business. If you discover that your system performs well on product questions but poorly on billing questions, you have a segment-specific quality gap. Fix it before it becomes a customer retention problem.

Your release process includes a formal evaluation gate. Define minimum thresholds for each metric you measure. Accuracy must be above eighty-five percent. Safety violations must be below one percent. Latency must be below two seconds at the ninety-fifth percentile. If any metric falls below its threshold, you block the release until the issue is fixed. This gate is automated. Your CI/CD pipeline runs the evaluation, checks the thresholds, and fails the build if any threshold is violated. No human discretion. No "let's ship it anyway and see what happens." The gate is the gate.

Integrate regression testing against your evaluation set into your continuous integration pipeline. Every pull request that touches prompt logic, model selection, or retrieval code should trigger an evaluation run. If the change causes quality to regress on your evaluation set, the pull request does not merge. This discipline prevents small, incremental quality degradation that compounds over time.

## Tier 3 Evaluation Exists to Ensure No One Is Harmed

At Tier 3, your product makes decisions that affect user safety, user money, or user legal standing. A quality failure is not an inconvenience. It is harm. Your evaluation exists to ensure that no one is harmed by your system and to provide evidence, if questioned, that you exercised due diligence.

Your evaluation set should contain at least one thousand examples, curated and validated by domain experts. If you are building a medical AI, your evaluation set must be reviewed by physicians. If you are building a financial AI, your evaluation set must be reviewed by financial analysts or compliance officers. If you are building a legal AI, your evaluation set must be reviewed by lawyers. The examples must include not just typical cases but adversarial cases designed to expose failure modes. They must include demographic slices so you can measure performance across age groups, gender, race, income levels, or whatever categories are relevant to your domain. They must include scenarios where the right answer is "I don't know" or "this requires human review." Update your evaluation set continuously as you encounter new edge cases, new adversarial inputs, or new regulatory requirements.

Run your full evaluation suite before every deployment. This is a blocking gate. No deployment proceeds until the evaluation passes. In production, implement continuous monitoring with real-time alerting. Sample production traffic at a much higher rate than Tier 2, ideally five to ten percent. Feed those samples through your evaluation pipeline and alert immediately if quality drops below your threshold. Conduct weekly detailed quality reviews with domain experts. Sit down with your physicians, your financial analysts, your lawyers. Show them a random sample of production outputs. Ask them: are these correct? Are these safe? Are there patterns we should be concerned about?

Your evaluation method is multi-layered. First layer: automated checks for measurable criteria. Format compliance, length constraints, factual accuracy against structured data, latency, cost. Second layer: domain expert review for clinical correctness, financial accuracy, legal soundness, or whatever domain-specific correctness means for your product. This is not optional. You cannot evaluate a medical AI without doctors. You cannot evaluate a financial AI without financial experts. Third layer: red team testing for adversarial robustness. Hire people to try to break your system. Give them an hour to find inputs that produce harmful, incorrect, or policy-violating outputs. If they succeed, add those inputs to your evaluation set and fix the underlying issue. Fourth layer: bias audits across protected categories. Measure performance separately for different demographic groups. If your system performs worse for women than men, for older users than younger users, for non-native English speakers than native speakers, you have a bias problem. Document it. Fix it. Prove you fixed it.

What should you measure? Everything from Tier 2, plus several critical additions. Measure your false negative rate for safety-critical categories. If your medical AI fails to flag a dangerous symptom, that is a false negative. If your financial AI fails to detect a fraudulent transaction, that is a false negative. These are the failures that cause harm. Track them separately and drive them toward zero. Measure bias metrics across demographics. Break down accuracy, precision, recall, and false negative rate by age, gender, race, income, geography, language, and any other protected or sensitive category relevant to your domain. Measure confidence calibration. When your system says it is ninety percent confident, is it actually correct ninety percent of the time? If your system is overconfident, users will trust it when they should not. If your system is underconfident, users will ignore it when they should listen. Measure your human override rate. In production, how often do domain experts change or reject the AI's recommendation? A high override rate means the system is not ready. A low override rate might mean the system is good, or it might mean humans are trusting it too much. Investigate both. Measure explanation quality. Does your system provide explanations for its outputs? Are those explanations accurate? Are they helpful? Do they give users the information they need to decide whether to trust the output?

Your release process requires multi-stakeholder sign-off. Engineering must confirm that the infrastructure, monitoring, and rollback mechanisms are ready. Product must confirm that the change aligns with product strategy and user needs. Domain experts must confirm that the outputs are clinically, financially, or legally sound. Legal and compliance must confirm that there are no new regulatory concerns. No single person can approve a Tier 3 release. The decision must be collective.

After sign-off, implement a staged rollout. Deploy to one percent of traffic. Monitor quality metrics, error rates, user feedback, and domain-expert review for a minimum period, usually twenty-four to forty-eight hours. If metrics are stable, expand to five percent. Monitor again. Expand to twenty-five percent. Monitor again. Expand to fifty percent. Monitor again. Expand to one hundred percent. Each stage requires a minimum monitoring period. You do not rush a Tier 3 rollout. The cost of a one-week delay is negligible compared to the cost of a quality incident that harms users.

The Tier 3 release process is deliberately slow. Impatient executives will push you to move faster. Resist. Explain that rushing a release in a high-risk domain is not bold decision-making. It is professional negligence. If they persist, document the conversation and escalate.

## Tier 4 Evaluation Exists to Satisfy Regulatory Requirements

At Tier 4, your product is subject to regulatory oversight. Your evaluation must not only protect users but also satisfy regulators. Everything from Tier 3 applies, plus several additional requirements.

You must build regulatory-specific test suites. These are tests designed to demonstrate compliance with the specific requirements of the regulations that govern your product. If you are subject to the EU AI Act Articles 9 through 15, your test suite must include tests for accuracy, robustness, cybersecurity, transparency, and human oversight. If you are subject to HIPAA, your test suite must include tests for data security and access controls. If you are subject to FDA validation requirements for software as a medical device, your test suite must follow FDA guidance on software validation. These test suites are not optional. They are the evidence you will provide to regulators during audits or conformity assessments.

Every evaluation run must produce formal documentation that can be shared with regulators. This documentation includes the test methodology, a description of the test data and how it was selected, the results broken down by test case and category, your analysis of the results, and any remediation actions you took in response to failures. You must store this documentation in a version-controlled repository with full traceability. If a regulator asks to see the evaluation results for version 2.3 of your model, you must be able to produce that report within minutes, not days.

Some regulations require third-party validation. Independent experts or accredited bodies must evaluate your system and confirm that it meets the required standards. Budget for this. Third-party validation is expensive and slow. It can add months to your timeline and tens of thousands of dollars to your costs. Build your evaluation infrastructure to support external access and reproducibility. The third-party evaluators will need access to your evaluation data, your evaluation code, and your system. Make it easy for them.

Every model version, every prompt version, every configuration change must be stored with full traceability. You must be able to answer questions like: what was the performance of version 2.3 on demographic group X? What prompt was running in production on March 15, 2025? What evaluation data was used to validate version 3.1? If you cannot answer these questions instantly, your evaluation infrastructure is not ready for Tier 4.

## The Budget Scales with Risk

Evaluation is not free. It costs engineering time to build the infrastructure, domain expert time to curate and review examples, compute time to run the evaluations, and operational time to monitor and respond to issues. The cost scales with your risk tier.

At Tier 1, evaluation costs almost nothing. A few hours to assemble fifty examples, a few hours per month to run manual reviews. At Tier 2, evaluation costs are moderate. You need dedicated infrastructure for automated evaluation, ongoing sampling of production traffic, and periodic human review. Budget several weeks of engineering time to build the system and ongoing operational costs to maintain it. At Tier 3, evaluation costs are significant. You need real-time monitoring, domain expert involvement, red team testing, bias audits, and multi-stakeholder review processes. Budget months of engineering time to build the infrastructure and ongoing operational costs that include domain expert salaries or consulting fees. At Tier 4, evaluation costs are major. You need everything from Tier 3 plus regulatory test suites, formal documentation, third-party validation, and audit support. Budget for a dedicated team.

Do not try to cut costs by under-investing in evaluation relative to your risk tier. A Tier 3 product with Tier 1 evaluation is a disaster waiting to happen. The money you save by skipping domain expert review or bias audits will be dwarfed by the cost of the incident, the regulatory fine, or the lawsuit.

Do not waste money by over-investing in evaluation relative to your risk tier. A Tier 1 product with Tier 3 evaluation is a team that never ships. The marginal value of the tenth bias audit on an internal tool is zero.

Match your evaluation investment to your risk tier. Then execute with discipline.

Your evaluation strategy is not a one-time decision. As your product evolves, your risk tier may change. If you add a new use case, enter a new geography, or start serving a new customer segment, reassess your risk tier. If your tier increases, upgrade your evaluation strategy immediately. Do not wait for an incident to force the upgrade.

Next, we turn to how risk tier dictates your release process, the final gate between your system and the real world.
