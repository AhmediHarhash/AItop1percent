# Chapter 3.7 — How Risk Tier Dictates Your Eval Strategy

Your risk tier isn't just a label — it's a blueprint for how much evaluation you need, what kind, and how often. Teams that apply Tier 3 rigor to a Tier 1 product waste months. Teams that apply Tier 1 rigor to a Tier 3 product create incidents. Getting the match right is one of the highest-leverage decisions you'll make.

---

### Tier 1 Evaluation: Learn Fast

At Tier 1, evaluation exists to help you improve, not to gate releases.

**Eval set size:** 50-100 examples. Cover your main use cases and a few known edge cases. Don't try to be comprehensive — try to be representative.

**Eval frequency:** Run before major changes (new model, significant prompt rewrite). Weekly spot-checks are a bonus, not a requirement.

**Eval method:** Manual review by the product team. No need for domain experts or formal annotation. The people building the product can judge the outputs.

**What to measure:** Core accuracy, user satisfaction (through feedback), and basic safety (no harmful outputs). Skip the complex metrics — they'll slow you down without meaningful benefit at this stage.

**Release process:** Informal. If the eval set looks good and the team is confident, ship it. No formal gates.

---

### Tier 2 Evaluation: Prove It Works

At Tier 2, evaluation exists to protect the customer experience and your brand reputation.

**Eval set size:** 200-500 examples. Cover all main use cases, known edge cases, and a stratified sample across user types, input categories, and difficulty levels. Update the eval set monthly as you discover new failure modes.

**Eval frequency:** Before every deployment. Daily or weekly sampling of production traffic for ongoing monitoring.

**Eval method:** Combination of automated scoring (for measurable dimensions like accuracy, latency, format compliance) and human review (for subjective dimensions like helpfulness, tone, and appropriateness). Consider LLM-as-judge for scalable quality scoring.

**What to measure:** Accuracy, helpfulness, safety, tone consistency, latency, cost per query, user satisfaction metrics (CSAT, NPS, or custom). Track per-segment performance — quality that's 90% overall but 60% for a specific user group is a problem.

**Release process:** Formal eval gate. Define minimum thresholds for each metric. If any metric falls below threshold, block the release until it's fixed. Automated regression testing against the eval set in CI/CD.

---

### Tier 3 Evaluation: Defend Every Decision

At Tier 3, evaluation exists to ensure no one is harmed and to provide evidence of due diligence.

**Eval set size:** 1,000+ examples, curated and validated by domain experts (doctors, lawyers, financial analysts, depending on your domain). Include adversarial cases, demographic slices, and scenarios designed to expose failure modes. Update continuously.

**Eval frequency:** Before every deployment (blocking). Continuous monitoring in production with real-time alerting. Weekly detailed quality reviews with domain experts.

**Eval method:** Multi-layered. Automated checks for measurable criteria. Domain expert review for clinical, legal, or financial correctness. Red team testing for adversarial robustness. Bias audits across protected categories. Cross-validation against domain benchmarks and standards.

**What to measure:** Everything from Tier 2, plus: false negative rate for safety-critical categories, bias metrics across demographics, confidence calibration (does the system's confidence match its actual accuracy?), human override rate (how often do humans change the AI's recommendation?), and explanation quality (are the system's explanations accurate and helpful?).

**Release process:** Multi-stakeholder sign-off. Engineering, product, domain experts, legal, and compliance must all approve. Staged rollout: canary deployment to a small percentage of traffic, monitored for a minimum period before wider release.

---

### Tier 4 Evaluation: Document Everything

At Tier 4, evaluation also exists to satisfy regulatory requirements and provide audit evidence.

Everything from Tier 3, plus:

**Regulatory-specific test suites.** Tests designed to demonstrate compliance with specific regulatory requirements (EU AI Act Articles 9-15, HIPAA security rules, FDA validation requirements, etc.).

**Formal documentation.** Every eval run produces a report that can be shared with regulators. This includes: test methodology, test data description, results, analysis, and any remediation actions taken.

**Third-party validation.** Independent evaluation by external parties may be required. Budget for this and build your eval infrastructure to support external access and reproducibility.

**Version-controlled evidence.** Every model version, prompt version, and eval result is stored with full traceability. If a regulator asks "what was the performance of version 2.3 on demographic group X?" you can answer in minutes.

---

### The Mapping Summary

| | Tier 1 | Tier 2 | Tier 3 | Tier 4 |
|---|---|---|---|---|
| Eval set size | 50-100 | 200-500 | 1,000+ | 1,000+ with regulatory suites |
| Frequency | Before major changes | Every deployment + ongoing | Continuous + weekly expert review | Continuous + regulatory cadence |
| Human review | Product team | Product + quality team | Domain experts required | Domain experts + external auditors |
| Release gate | Informal | Automated threshold | Multi-stakeholder sign-off | Regulatory conformity assessment |

Match your investment to your tier. Then execute.

*Next: how risk tier determines your release process.*
