# 5.1 — Why Traditional PRDs Fail for AI

In early 2025, a healthcare technology company spent six months building a patient intake chatbot that nobody could agree was ready to ship. The product manager pointed to the completed feature list from the original PRD: the chatbot collected patient demographics, medical history, and insurance information. Everything in the spec was built. Engineering said it was done. But when the clinical operations director tested it, she refused to approve the launch. The bot was technically functional but clinically dangerous. It accepted impossible medical histories without flagging them. It misunderstood common medication names. It let patients skip required safety questions if they phrased requests in unexpected ways. The PRD had specified what information to collect but said nothing about validation, error handling, or safety constraints. The document described a deterministic form-filling system, not a conversational AI that needed to handle infinite variations of how patients might describe their health.

The team spent another four months retrofitting safety checks, rewriting prompts, and building evaluation frameworks they should have defined before writing a single line of code. The root cause was not technical incompetence. It was using a requirements document format designed for deterministic software to specify non-deterministic AI behavior. Traditional product requirement documents assume predictable, repeatable outputs. They assume you can enumerate all edge cases. They assume the system either works or it doesn't. None of these assumptions hold for AI products, and every team that tries to force AI into a traditional PRD format discovers this the expensive way.

## What Traditional PRDs Are Built For

The standard product requirements document emerged from decades of building deterministic software. When you write "the system displays the user's name in the top-right corner," you mean exactly that. The same input produces the same output every time. The engineer implements the feature, QA verifies it works, and the feature either passes or fails testing. The PRD can describe precise behavior because the software has precise behavior.

This model works brilliantly for traditional applications. A button click triggers a function. A form submission validates against defined rules. An API call returns structured data from a database. You can specify exact outputs for exact inputs. You can enumerate edge cases: what happens with an empty field, a special character, a string that's too long. You can write test cases that verify the system does exactly what the PRD says it should do.

The traditional PRD is essentially a contract between product and engineering. Product says "build a system that behaves like this in these situations." Engineering says "we'll build it, and you can verify it matches the spec." The verification is binary. Either the system matches the specification or it doesn't. Either the feature works or it's buggy. Either the test passes or it fails. This binary verification only works when behavior is deterministic.

## Why AI Breaks the Traditional PRD Model

AI products are fundamentally probabilistic, not deterministic. When you send the same prompt to a language model twice, you might get different responses. When you process 1,000 customer support tickets, the AI might handle 870 of them correctly, struggle with 100, and completely misunderstand 30. The behavior is not binary pass-or-fail. It's a distribution of outcomes across a range of quality levels.

A traditional PRD for a customer support chatbot might say "the system answers questions about return policies, shipping timelines, and account management." This seems like a clear requirement until engineering starts building. What counts as answering a question? If the AI provides technically accurate information in a confusing or unhelpful format, did it meet the requirement? If it answers 95 percent of return policy questions correctly but fails on edge cases the team didn't anticipate, is the feature complete? If it refuses to answer a legitimate question because the user's phrasing triggered a safety filter, is that a bug or expected behavior?

The PRD describes a category of behavior but provides no definition of success. It doesn't specify what "correctly answering" means. It doesn't define acceptable accuracy rates. It doesn't describe what the system should do when it's uncertain or when the user asks something outside the training data. The engineering team is left guessing what quality bar they're building toward, what tradeoffs are acceptable, and what failure modes are blockers versus known limitations.

Traditional PRDs also assume you can enumerate edge cases. For a form validation system, edge cases might include empty inputs, very long strings, special characters, and date formats from different locales. You can list these, test them, and verify the system handles each one correctly. For an AI product, edge cases include every possible thing a human might say or type. You cannot enumerate them. A customer support chatbot needs to handle polite questions, angry rants, ambiguous phrasing, multi-part questions, questions in the wrong language, questions that combine multiple unrelated topics, questions that assume context from previous conversations, and questions that reveal the user fundamentally misunderstands how your product works.

You cannot write a PRD that lists every possible user input and specifies the correct system response for each one. The state space is infinite. The traditional approach of specifying behavior for enumerated scenarios fails completely. You need a different approach: defining the principles and quality standards that guide system behavior across all scenarios, not trying to specify exact behavior for each scenario individually.

## The Missing Elements in Traditional PRDs

When teams use traditional PRD templates for AI products, the documents consistently miss the same critical elements. These gaps are not oversights. They're structural absences because the traditional format was never designed to capture these concepts.

Traditional PRDs rarely specify evaluation criteria. They describe features and user flows but don't define how the team will measure whether those features work well enough to ship. For deterministic software, this isn't a major problem because "working" is relatively clear: the feature either matches the spec or it doesn't. For AI products, this absence is fatal. Without defined evaluation criteria, the team has no shared understanding of what "good enough" means. Engineers build toward one quality standard, product managers expect another, and domain experts judge against a third. The launch decision becomes a negotiation about subjective quality instead of a measurement against predefined standards.

Traditional PRDs also miss error budgets and tolerance specifications. They might say "the system should extract invoice data from uploaded documents" but won't specify acceptable error rates. If the extraction is 92 percent accurate, is that good enough to ship? What about 87 percent? What about 78 percent? The answer depends on the use case, the cost of errors, and the availability of fallback mechanisms, but none of this context appears in a traditional PRD. The team discovers the implicit error budget only when someone decides the quality isn't good enough, usually late in the development cycle when fixing it is expensive.

Latency constraints and cost budgets are also absent from most traditional PRDs. A feature spec might describe what the AI should do but ignore how fast it needs to do it and how much that will cost at scale. The engineering team optimizes for quality, ships a feature that uses an expensive model with five-second response times, and discovers too late that the cost per request makes the product economically unviable or the latency makes the user experience unacceptable. These constraints should be requirements, not afterthoughts.

Fallback behavior is another missing piece. Traditional software either works or displays an error message. AI products have a third category: uncertain or low-confidence outputs. What should the system do when it's not confident in its answer? When it encounters an input outside its training distribution? When the user asks something the system wasn't designed to handle? Traditional PRDs don't address these questions because traditional software doesn't have this category of behavior. For AI products, fallback behavior is often more important than primary behavior because it determines what happens when the system reaches the edges of its capabilities.

Data requirements are rarely specified in traditional PRDs. The document describes the desired output but not the input data quality, format, or volume needed to produce that output. An AI product might require labeled training data, structured knowledge bases, or ongoing human feedback to function. If the PRD doesn't specify these requirements, product and engineering discover midway through development that the system needs data that doesn't exist, and the team has to scramble to create it or radically redesign the approach.

## The Determinism Assumption and Its Consequences

The deepest problem with traditional PRDs for AI is the embedded assumption of deterministic behavior. The entire document format is built on the idea that you can describe what the system will do, not what the system will probably do most of the time with varying quality depending on input characteristics. This assumption surfaces in the language used throughout traditional specs.

A PRD might say "the system will generate personalized email subject lines that increase open rates." This sentence assumes the AI will successfully generate effective subject lines for all users. It doesn't acknowledge that the AI might generate excellent subject lines for some users, mediocre ones for others, and actively harmful ones for a small percentage. It doesn't specify the minimum acceptable improvement in open rates or the maximum acceptable rate of subject lines that perform worse than a generic alternative. The spec describes a feature as if it will work uniformly when the reality of AI is variable performance across a distribution.

The same assumption appears in user flow documentation. A traditional PRD might include a flow diagram: user uploads document, system extracts data, user reviews extracted data, user confirms or corrects. This flow assumes the extraction works. It doesn't show what happens when the extraction fails completely, when the system is uncertain about extracted values, or when the document format is outside the system's training data. The user flow is incomplete because it only describes the happy path, and for AI products, the unhappy paths are not edge cases—they're a fundamental part of the product experience.

Traditional PRDs also assume static behavior. The spec describes what the system does, implying that behavior will remain constant. But AI behavior changes over time even without code changes. Model providers release new versions with different performance characteristics. Prompt adjustments to fix one issue introduce regressions in another area. User behavior shifts and reveals failure modes that weren't present in the original evaluation data. The PRD becomes outdated the moment behavior changes, but the document format provides no structure for versioning behavioral specs or acknowledging that the system's behavior is not fixed.

## What AI Requirements Need Instead

AI product requirements need to specify behavior as distributions and ranges, not as deterministic outcomes. Instead of "the system answers customer questions," write "the system correctly answers customer questions about returns, shipping, and accounts with 88 percent accuracy as measured against a representative evaluation set, with less than 2 percent of responses containing factually incorrect information, and less than 0.1 percent of responses containing information that could cause customer harm or legal risk."

This requirement defines success as a quality distribution. It acknowledges that the system will not answer every question correctly. It specifies the acceptable accuracy rate, distinguishes between low-quality responses and actively harmful responses, and provides measurable criteria the team can evaluate against. Engineering knows what to optimize for. Product knows what quality bar to expect. QA knows what to measure. The requirement matches the reality of probabilistic systems.

AI requirements also need explicit specifications for behavior under uncertainty. What should the system do when its confidence is low? When the input is outside the expected domain? When multiple possible answers seem equally valid? A complete requirement might specify: "When the system's confidence in its answer falls below 0.7 on the internal confidence score, it should not provide a direct answer and instead offer to connect the user with a human agent. When the user's question is outside the documented scope of product information, the system should acknowledge the limitation and redirect the user to appropriate resources."

These uncertainty specifications are not edge cases. They're core product behavior that determines user trust. An AI that confidently gives wrong answers is worse than an AI that admits uncertainty. But traditional PRDs don't have a structure for specifying this behavior because deterministic software doesn't have uncertain states.

Example-driven specifications are also essential for AI requirements. Prose descriptions of desired behavior are inherently ambiguous. Examples are concrete. A requirement should include 20 to 30 representative inputs with expected outputs, annotated with quality levels and failure categories. These examples serve three purposes: they communicate desired behavior more clearly than prose ever can, they form the seed of the evaluation dataset used to measure quality, and they reveal edge cases and ambiguities that prose specifications miss.

AI requirements must also explicitly document known unknowns. Every AI project has questions the team cannot answer before building: How will the model handle industry jargon we didn't include in training data? How will users react to AI-generated responses in this context? How well will the system generalize beyond the specific examples we evaluated? Traditional project management treats unknowns as risks to be mitigated. For AI products, unknowns are inherent. The requirement document should list them explicitly, categorize which unknowns are blockers versus acceptable risks, and specify how the team will learn the answers, usually through staged rollout and measurement.

Finally, AI requirements need explicit iteration assumptions. Traditional PRDs often imply the first version is the final version, or at least production-ready. AI products are built iteratively by necessity. The first version will not be perfect. The requirement document should define the quality bar for initial launch and the improvement trajectory for subsequent versions: "Version one targets 80 percent accuracy with less than 1 percent harmful outputs, suitable for beta launch with 500 users. Version two targets 88 percent accuracy, suitable for general launch. Version three targets 93 percent accuracy, suitable for expansion to high-risk use cases."

This staged quality specification sets realistic expectations and prevents the common trap of teams either aiming for perfection and never shipping or aiming for nothing and shipping products that damage user trust. The requirement defines good enough for now and excellent for later, creating a clear path forward.

## The Cost of Skipping AI-Specific Requirements

Teams that use traditional PRD formats for AI products pay predictable costs. Development takes longer because the team constantly stops to clarify ambiguous requirements. Quality is inconsistent because different team members optimize toward different unstated quality bars. Launch decisions turn into arguments because nobody agreed upfront what "ready to ship" means. Stakeholders lose trust because the product doesn't match their expectations, even though it technically implements every feature in the original spec.

The healthcare chatbot from the opening story is not an outlier. It's the default outcome when teams don't adapt their requirements process for AI's probabilistic nature. The PRD said collect patient information. The team built a system that collected patient information. But nobody specified validation standards, error handling policies, or safety constraints because the PRD format didn't prompt those specifications. The result was technically complete and clinically unacceptable.

The downstream effects ripple through the entire organization. The clinical team loses confidence in the technology team's ability to understand healthcare requirements. The technology team becomes defensive, arguing they built exactly what was specified. Product management is caught in the middle, trying to mediate between groups that no longer trust each other. The executive team questions whether the organization has the capability to build AI products at all. The problem wasn't capability. It was using the wrong tool for requirements specification.

The financial cost is also significant. The healthcare company spent ten months and approximately 800,000 dollars in engineering resources building a system that couldn't launch. When they finally shipped four months later after extensive rework, they had invested 1.2 million dollars into what should have been a six-month, 500,000 dollar project. The excess cost came entirely from rework that could have been avoided if the initial requirements had specified quality standards, error handling, and safety constraints upfront.

The opportunity cost is often larger than the direct cost. While the team spent four months retrofitting safety mechanisms, competitors launched similar capabilities. The market window narrowed. The internal stakeholders who championed the project lost credibility. Future AI initiatives faced increased skepticism and longer approval processes because this project had failed to meet expectations, even though it technically delivered everything in the spec.

## Why Teams Keep Making the Same Mistake

If traditional PRD formats fail so predictably for AI products, why do teams keep using them? The answer is organizational inertia and false familiarity. Product managers know how to write traditional PRDs. They've written dozens. Engineering knows how to read them and estimate effort. Stakeholders know how to review them and provide feedback. The process is familiar, established, and comfortable.

When faced with the additional work of defining evaluation criteria, specifying error budgets, documenting uncertainty handling, and establishing quality distributions, teams take the path of least resistance. They tell themselves that they'll figure out quality standards during development. They assume that evaluation criteria can be defined when they have something to evaluate. They convince themselves that the traditional format is good enough and that the additional specifications are nice-to-have extras, not core requirements.

This illusion persists until reality intervenes. The team builds something, shows it to stakeholders, and discovers they all have different expectations about what acceptable quality means. The conversation that should have happened before development starts instead happens months later when changing direction is expensive and demoralizing. The team learns the hard way that AI-specific requirements are not optional extras. They're the foundation without which no AI project can succeed.

Some teams recognize the gap and try to patch it without changing the underlying format. They add a section to their traditional PRD titled "quality requirements" and write something like "the system should have high accuracy and low latency." This cosmetic addition doesn't solve the problem because it's still vague and unmeasurable. High accuracy compared to what? Measured how? On what data? With what acceptable variance? Low latency under what load conditions? What percentile targets? The traditional format doesn't prompt these specifications, so teams don't provide them.

## The Fundamental Mismatch

The fundamental mismatch between traditional PRDs and AI products is philosophical, not just technical. Traditional software development treats requirements as contracts: if you build exactly what's specified, you've succeeded. AI product development treats requirements as hypotheses: we believe this approach will achieve these outcomes, and we'll measure to verify.

A traditional requirement says "when the user clicks submit, validate the email format." You can verify this deterministically. An AI requirement says "when the user submits a support question, provide an answer that resolves their issue at least 85 percent of the time as measured by user satisfaction." You cannot verify this deterministically. You need evaluation data, measurement methodology, statistical significance thresholds, and ongoing monitoring because the quality is probabilistic and changes over time.

Traditional requirements assume you can specify complete behavior upfront. AI requirements acknowledge that you cannot know all the ways users will interact with the system or all the failure modes that will emerge until you observe real usage. Traditional requirements treat edge cases as exceptions to enumerate. AI requirements treat edge cases as a continuous distribution to manage with policies and fallback mechanisms.

This philosophical mismatch means you cannot simply add a few sections to a traditional PRD and call it adapted for AI. You need a different framework that embraces uncertainty, specifies quality as distributions, defines measurement methodologies, and acknowledges iteration as core to the development process.

## Moving Beyond the Traditional Format

The path forward is not to abandon structure or discipline in requirements. It's to adopt a structure designed for probabilistic systems. The format should prompt the specifications that AI products actually need: measurable outcomes with defined quality targets, evaluation criteria and measurement methodologies, uncertainty handling and fallback policies, cost and latency constraints, example-driven behavior specs, and staged iteration plans with clear quality gates.

This new format requires more upfront work than a traditional PRD. You cannot write it in an afternoon. You need to involve domain experts to define quality standards, engineering to estimate cost and latency constraints, product to align on business metrics, and stakeholders to agree on acceptable accuracy ranges. This work feels slow when you're eager to start building. But it's fast compared to the months of rework that result from skipping it.

The teams that succeed with AI products are the teams that do this boring work upfront. They define what good looks like before building. They specify how they'll measure whether they achieved good. They align all stakeholders on acceptable quality ranges. They document known unknowns and plans for learning. They create requirements that match the reality of probabilistic systems instead of forcing AI into a deterministic framework.

The next step is understanding what to put in that new format and how to write requirements that actually work for AI products.
