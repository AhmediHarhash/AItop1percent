# Chapter 3.4 — Tier 3: High-Risk — Financial, Medical, Legal, Safety-Critical

Tier 3 is where AI products make decisions that affect people's health, money, legal rights, or physical safety. The difference between Tier 2 and Tier 3 isn't just a higher quality bar — it's a fundamentally different relationship between the system and the consequences of its outputs.

---

### What Tier 3 Looks Like

Tier 3 products operate in domains where a wrong answer can cause measurable, significant harm to individuals or organizations:

- **Medical AI.** Diagnostic assistance, treatment recommendations, drug interaction checking, triage systems. A wrong diagnosis can delay treatment. A missed drug interaction can cause harm.
- **Financial AI.** Credit scoring, fraud detection, trading systems, insurance underwriting, loan approval. A wrong credit decision denies someone a mortgage. A missed fraud signal costs the business real money.
- **Legal AI.** Contract analysis, legal research, compliance checking, case outcome prediction. Wrong legal advice can result in lawsuits, regulatory penalties, or contractual breaches.
- **Safety-critical systems.** Autonomous vehicles, industrial automation, infrastructure monitoring. Errors can cause physical harm or death.
- **HR and employment AI.** Resume screening, performance evaluation, promotion recommendations. Biased or incorrect decisions affect people's careers and livelihoods.

---

### What Changes at Tier 3

**Human oversight is mandatory, not optional.** At Tier 3, the AI assists human decision-makers — it doesn't replace them. A medical AI suggests a diagnosis; a doctor confirms it. A financial AI flags a transaction; an analyst reviews it. The human-in-the-loop isn't a nice-to-have; it's a legal and ethical requirement.

**Explainability becomes a requirement.** "The AI said no" isn't an acceptable answer when you're denying someone a loan or recommending a medical treatment. Tier 3 products need to explain their reasoning in terms that domain experts and, in some cases, end users can understand. This isn't just good practice — in many jurisdictions, it's legally required.

**Bias testing is non-negotiable.** Tier 3 decisions affect different demographic groups differently. A credit scoring system that performs worse for certain ethnic groups is both unethical and illegal. Bias audits across protected categories (race, gender, age, disability, religion) are mandatory, not aspirational.

**Audit trails are essential.** Every decision, every input, every output, every model version, every prompt — all of it needs to be logged and retrievable. When a regulator asks "why did your system make this decision six months ago?" you need to be able to answer with evidence, not guesses.

**Validation against domain standards.** Medical AI needs clinical validation studies. Financial AI needs backtesting against historical data. Legal AI needs review by qualified attorneys. The validation methodology depends on the domain, but the requirement is universal: you need domain-appropriate evidence that the system works.

---

### The Tier 3 Quality Bar

The quality bar for Tier 3 is: **as good as or better than a competent human professional, with transparent limitations.**

This means:
- **Accuracy above 95%** for the core decision, validated against domain-expert ground truth
- **Calibrated confidence scores** that accurately reflect the system's certainty
- **Explicit uncertainty handling** — the system must know when it doesn't know and escalate appropriately
- **Bias metrics** within acceptable thresholds across all protected categories
- **Full audit trail** for every decision
- **Documented limitations** — clear statements about what the system can and cannot do

What you need at Tier 3:
- Comprehensive eval sets (1,000+ examples) reviewed by domain experts
- Continuous monitoring with real-time alerting
- Mandatory human review for all outputs (or at minimum, for outputs below a confidence threshold)
- Regular bias audits (quarterly or more frequent)
- Clinical, financial, or legal validation studies as appropriate
- Incident response playbooks specific to the domain
- Legal review of the system's scope and limitations
- Insurance and liability coverage

---

### The Tier 3 Mistake

The most dangerous mistake at Tier 3 is building a Tier 2 system and deploying it in a Tier 3 context. This happens when teams build a "general-purpose" AI tool and then someone uses it for medical advice, legal analysis, or financial decisions without upgrading the evaluation, monitoring, and safety infrastructure.

If your product could be used for Tier 3 decisions — even if you didn't intend it — you need either Tier 3 safeguards or hard guardrails preventing Tier 3 use.

*Next: Tier 4 — where regulation defines the rules and non-compliance isn't a risk, it's a certainty.*
