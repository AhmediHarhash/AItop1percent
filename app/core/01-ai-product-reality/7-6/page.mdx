# 7.6 â€” Go/No-Go Criteria: When to Proceed, Pivot, or Stop

In late 2024, a healthcare software company spent seven months building an AI tool to generate patient discharge summaries. The team built a beautiful interface, integrated with their EMR system, created a comprehensive evaluation dataset, and ran their first real evaluation. The model scored 62% on their accuracy criteria. The product manager wanted to ship it anyway. The team had promised the feature to customers. The CEO had announced it at a conference. Marketing had already written the blog post. Sales had included it in three pending deals. The engineering lead said no. They'd agreed before building that the go threshold was 85% accuracy for a patient-facing document. At 62%, the product wasn't safe to ship. The PM argued that they could add a disclaimer, launch it as a beta, and improve it over time. The engineering lead asked what would happen if a doctor missed a critical detail because the AI summary was incomplete or wrong. The room went quiet. They killed the project. Not forever, but for the current approach. They'd spent seven months, but shipping a 62% accurate tool for patient care would have been professional negligence. Three months later, they pivoted to a different use case: generating first-draft summaries for internal care coordination, not patient-facing documents. The quality bar for that use case was lower. The risk was lower. The 62% accuracy was enough to be useful as a draft. They shipped it. It worked. Within six months, they improved it to 81% accuracy and expanded the use case.

You've built a prototype. You've defined success criteria. You've run your first evaluation. You have a baseline quality number. Now comes the hardest decision in AI product development: should you keep going? Most teams skip this decision entirely. They built something, so they ship it. The sunk cost fallacy takes over. The team has invested weeks or months. Stakeholders are expecting the feature. Backing out now feels like failure. But a disciplined team defines explicit criteria for proceeding before they're emotionally invested in the outcome. They decide in advance what quality threshold means go, what threshold means conditional go, and what threshold means stop or pivot. Then they follow the criteria, even when it's uncomfortable.

## The Go/No-Go Framework

The framework has three thresholds, and you define all three before you evaluate. Not after you see the score. Before. If you define the thresholds after you know the results, you'll rationalize. You'll move the goalposts. You'll convince yourself that 70% is actually fine even though you said you needed 85%. Define the thresholds first. Write them down. Share them with the team and with stakeholders. Then run your evaluation and follow the decision framework.

The **go threshold** is the quality level where the product is good enough to put in front of real users, even in a limited launch. "If our eval score is above X%, we proceed to MVP development." This doesn't mean the product is perfect. It means the product is useful enough that the value it provides outweighs the cost of its errors. For an internal tool that generates first drafts, the go threshold might be 70%. The tool makes mistakes, but users can easily catch them and fix them, and even an imperfect draft saves time. For a customer-facing feature that provides financial advice, the go threshold might be 95%. Errors have real consequences, and users trust the output, so the bar is high.

The **conditional go threshold** is the range where the product isn't good enough to launch broadly, but it might be good enough to launch with constraints. "If our eval score is between X% and Y%, we proceed but only with specific mitigations in place." Maybe you launch with human review on every output. Maybe you limit the product to a narrow use case where quality is high enough, and defer the broader use case until you improve. Maybe you add a prominent disclaimer that the output is AI-generated and must be verified. Maybe you launch to a small group of expert users who understand the limitations. The conditional go range is where you proceed cautiously, with guardrails, rather than proceeding confidently at full scale.

The **no-go threshold** is the quality level below which you do not proceed with the current approach. "If our eval score is below Y%, we stop or pivot." This doesn't mean you abandon the product idea entirely. It means the current technical approach is not working, and you need to change something fundamental before proceeding. Maybe you need a different model. Maybe you need a different prompt architecture. Maybe you need to redefine the task. Maybe you need to collect more training data or build a fine-tuned model instead of using a general-purpose model. Maybe the product idea itself is not viable with current AI capabilities, and you need to wait for better models or pivot to a different product.

The no-go threshold is the hardest threshold to enforce because it means admitting that the work you've done so far hasn't produced a shippable product. But enforcing it is what separates professional teams from teams that ship products they shouldn't. Shipping a product that doesn't meet your quality threshold is not courage. It's negligence.

## Setting the Thresholds for Your Product

The right thresholds depend on your risk tier, your product type, and the consequences of errors. There are no universal numbers, but there are reasonable starting points based on how much trust users place in your output and what happens when the output is wrong.

For **Tier 1 products**, which are internal tools or draft generators where users can easily verify and correct outputs, the go threshold is typically 70% to 75%. The conditional go range is 55% to 70%. The no-go threshold is below 55%. These products are low-risk. Users expect to review the output and make edits. An imperfect AI is still useful because it provides a starting point that's better than a blank page. A 70% accurate draft saves time even if you have to fix 30% of it.

For **Tier 2 products**, which are customer-facing features where the AI output is visible to users but not directly used for high-stakes decisions, the go threshold is typically 85% to 88%. The conditional go range is 70% to 85%. The no-go threshold is below 70%. These products carry moderate risk. Users will trust the output more than they trust an internal tool, so the quality bar is higher. Errors are visible and hurt your brand, but they don't cause financial or safety harm. A chatbot that answers product questions, a summary tool for news articles, or a content recommendation engine fall into this category.

For **Tier 3 products**, which inform high-stakes decisions but don't automate them, the go threshold is typically 92% to 95%. The conditional go range is 85% to 92%. The no-go threshold is below 85%. These products carry high risk. Users rely on the output to make decisions that have real consequences. A contract analysis tool used by legal teams, a medical coding assistant used by billing departments, or a fraud detection system that flags transactions for review fall into this category. Errors are costly, but humans are still in the loop to catch them.

For **Tier 4 products**, which operate in regulated domains or automate decisions without human oversight, the go threshold is 95% or higher. The conditional go range is 90% to 95%. The no-go threshold is below 90%. These products carry the highest risk. Errors can cause regulatory violations, legal liability, financial loss, or harm to individuals. An AI that makes credit decisions, an AI that approves insurance claims, or an AI that generates clinical documentation without review falls into this category. The bar is high because the stakes are high.

These numbers are starting points, not rules. Adjust them based on your specific domain, your users' tolerance for errors, the severity of error consequences, and your ability to mitigate errors with guardrails or human review. A 70% accurate tool might be great if the 30% of errors are obvious and easy to fix. The same 70% might be unacceptable if the errors are subtle and hard to detect. Context matters.

## Reading the Signals: Proceed, Pivot, or Stop

Sometimes the evaluation score is clear. You hit 88% on an 85% threshold, so you proceed. You hit 54% on a 70% threshold, so you stop. But often the score lands in the conditional range, or the score is above the threshold on some criteria and below it on others, or the quantitative score doesn't tell the whole story. In those cases, you need to read the qualitative signals to decide whether to proceed, pivot, or stop.

One signal is **performance variance by input type**. Maybe your overall score is 78%, which is in the conditional range. But when you break it down, you discover that the model scores 94% on simple queries and 42% on complex queries. This is a pivot signal. The AI is good enough for the simple use case but not for the complex use case. You can proceed by launching with the simple use case first and deferring the complex use case until you improve the model or the prompt. You've learned something valuable: where the AI works and where it doesn't.

Another signal is **the model being good at a different task than you planned**. You built a contract summarizer, but when you analyze the outputs, you realize the model is better at extracting specific clauses than summarizing entire contracts. The summaries are vague and miss key details, but the extraction is precise and reliable. This is a pivot signal. Change the product. Instead of a summarizer, build an extraction tool. Ship the thing the AI is actually good at, not the thing you wanted it to be good at.

Another signal is **user feedback diverging from your eval scores**. You score 82% on your evaluation dataset, which is in the conditional range. You launch to a small beta group with disclaimers and monitoring. Users love it. They report that it's saving them hours per week. The errors are obvious and easy to fix, and the value of the correct outputs far outweighs the cost of fixing the incorrect ones. This is a proceed signal. Your evaluation was conservative, which is good, but real-world value is what matters. Proceed to broader launch, but keep monitoring quality in production.

The opposite can also happen. You score 87% on your eval, which is above your go threshold. You launch to a beta group. Users complain that the outputs are often subtly wrong in ways that are hard to detect and time-consuming to verify. Your evaluation dataset missed a critical dimension of quality, or the errors that happen in the 13% failure rate are more costly than you anticipated. This is a stop or pivot signal. Your eval score was misleading. Go back and refine your evaluation to capture the quality dimension you're missing, then re-evaluate whether the product is actually ready.

Another signal is **economic viability**. The AI works. The quality is above your go threshold. But when you calculate the cost per query at production scale, the economics don't work. You're spending 80 cents per query, and you can't charge users more than 20 cents. This is a pivot signal, not a stop signal. The AI works, but the current architecture is too expensive. Pivot to a more efficient approach. Use a cheaper model. Add caching to reduce redundant queries. Use model routing to send simple queries to a fast, cheap model and complex queries to an expensive model. Rethink your pricing model. The product is viable, but the current implementation is not.

## Making the Decision Without Ego

The go/no-go decision should be made by the team, using the pre-defined criteria, based on data. It should not be made by the executive who promised the feature at a conference. It should not be made by the engineer who spent three months building the prototype and is emotionally invested in shipping it. It should not be made by the salesperson who told a customer it's coming next quarter. All of those people have conflicts of interest. Their judgment is compromised by sunk costs, ego, or external commitments.

The decision should be made by the product manager, the engineering lead, and the ML lead together, in a room, looking at the evaluation results and the pre-defined thresholds. If the score is above the go threshold, proceed. If it's in the conditional range, discuss what constraints or mitigations make it safe to proceed. If it's below the no-go threshold, stop or pivot. Follow the criteria. If you don't follow the criteria, there was no point in defining them.

When you have to make a no-go decision, communicate it clearly and without defensiveness. "We evaluated the prototype against our pre-defined success criteria. The model scored 64% on accuracy, and our threshold for proceeding was 85%. We're not proceeding with the current approach. We're exploring three options: switching to a different model, redefining the task to focus on the use case where quality is higher, or pausing this project until better models are available. We'll update you in two weeks with a recommendation." This is not failure. This is discipline.

The sunk cost trap is real. You've spent two months or six months building something. The team is tired. Stakeholders are impatient. The temptation is to ship what you have, even if it doesn't meet your criteria, because stopping feels like wasting the time you've invested. But sunk costs are sunk. The time is gone whether you ship or not. The only question is whether shipping this product creates value or creates harm. If the product doesn't meet your quality threshold, shipping it creates harm. It damages your brand, frustrates users, generates support costs, and in some cases creates legal or safety risk. The disciplined decision is to stop.

Stopping is not the same as failing. Stopping is learning. You learned that the current approach doesn't work. You learned where the AI is strong and where it's weak. You learned what quality threshold is necessary for your use case. You learned how to evaluate AI products before shipping them. All of that knowledge makes your next project better. The teams that ship products they shouldn't are the teams that never learn, because they never stop long enough to ask whether they should have kept going.

## When to Revisit a Stopped Project

A no-go decision is not permanent. AI capabilities improve quickly. A product that scored 64% in 2024 might score 89% in 2025 with a newer model and no changes to your prompt or architecture. A product that was economically unviable at 80 cents per query in 2024 might become viable in 2025 when model prices drop to 15 cents per query. A product that was blocked by regulation in 2024 might become legal in 2025 when the regulation updates.

When you stop a project, document why you stopped. Write down the evaluation score, the threshold you needed, the specific failure modes you observed, and the conditions under which you'd revisit the project. "We need a model that scores at least 85% on medical coding accuracy, or we need the cost per query to drop below 30 cents, or we need regulatory clarity on whether AI-generated billing codes are permissible." Then set a calendar reminder to revisit the project every six months. Check whether any of the conditions have changed. If they have, re-evaluate. If they haven't, keep waiting.

Some of the best AI products launched in 2025 were projects that were stopped in 2023 or 2024 and restarted when better models became available. The teams that stopped those projects weren't wrong to stop them. They were right to wait. The teams that shipped the 64% accurate products in 2023 were wrong, and many of them are still dealing with the reputational and technical debt from that decision.

The discipline of defining go/no-go criteria before you evaluate, following those criteria even when it's uncomfortable, and stopping or pivoting when the data says stop is what separates professional AI product teams from teams that ship things they shouldn't. Your criteria are your integrity. Follow them.

Next, you'll learn how to build the minimum viable AI product: what to include, what to cut, and how to ship something useful quickly.
