# Chapter 6.8 — Case Studies: Products That Survived Model Transitions

Theory is useful. Examples are better. Let's look at how the market dynamics we've discussed played out for real products — some that adapted and thrived, and some that didn't.

---

### Case Study 1: The Wrapper That Became a Platform

**The product:** A startup launched in 2023 as a "better ChatGPT for customer support." They wrapped GPT-3.5 in a clean UI, added conversation memory, and sold to small businesses. They got traction.

**The crisis:** ChatGPT launched its own Teams product. Then Intercom, Zendesk, and Freshdesk all added AI copilots to their existing platforms. The startup's "better chat UI" was suddenly a feature in every competitor's product.

**What saved them:** They'd been collecting customer support conversations for 18 months. They built domain-specific evaluation frameworks. They fine-tuned models on real support interactions. They integrated deeply into their customers' ticketing systems, knowledge bases, and escalation workflows.

**The result:** They pivoted from "AI chatbot" to "AI-native customer support platform." The competitors' bolt-on AI features couldn't match the depth of their domain-specific system. They survived because their moat wasn't the model — it was the data, the evals, and the integration.

**The lesson:** Products that start as wrappers need to evolve into platforms. The window is short — usually 12-18 months before the platforms catch up.

---

### Case Study 2: The Fine-Tuned Model That Became a Liability

**The product:** A legal tech company spent eight months fine-tuning GPT-3.5 for contract analysis. They achieved excellent accuracy on their benchmark. They launched to enterprise customers.

**The crisis:** GPT-4 launched six months later and outperformed their fine-tuned GPT-3.5 on contract analysis out of the box — without any fine-tuning. Their eight months of work was suddenly a disadvantage because their architecture was tightly coupled to the fine-tuned model.

**What went wrong:** They had hardcoded model-specific assumptions throughout their pipeline. Their prompts relied on GPT-3.5 behavioral quirks. Their evaluation suite tested against GPT-3.5 outputs, not against ground-truth contract interpretations. Migrating to GPT-4 required essentially rebuilding.

**The lesson:** Fine-tuning is valuable, but architecture matters more. If they'd built a model-agnostic architecture with ground-truth evaluations, they could have swapped to GPT-4 in weeks instead of months.

---

### Case Study 3: The Multi-Provider Strategy That Paid Off

**The product:** A healthcare AI company built their diagnostic support tool to work with multiple model providers from day one. Primary: Anthropic's Claude. Secondary: GPT-4. Tertiary: a self-hosted open-source model.

**The crisis:** During a major provider outage, their primary model was unavailable for four hours.

**What saved them:** Automatic failover routed requests to the secondary provider within 30 seconds. They'd run their eval suite on all three providers monthly, so they knew the quality delta. The secondary provider's accuracy was 3% lower on their benchmark — acceptable for the duration of an outage. Their users barely noticed.

**The lesson:** Provider resilience isn't a nice-to-have for healthcare AI — it's a patient safety requirement. But the approach applies to any product where downtime has real consequences.

---

### Case Study 4: The Product That Didn't Survive

**The product:** A startup built an AI writing assistant focused on marketing copy. Nice UI, good prompts, growing user base.

**The failure:** They had no proprietary data, no domain-specific evaluations, no deep workflow integration, and no unique capabilities beyond prompt engineering. When Claude and ChatGPT improved their native writing capabilities and every competitor offered similar features, users had no reason to pay for a separate tool.

**What they missed:** They treated their product as finished after launch. No feedback loops, no data flywheel, no continuous evaluation, no differentiation investment. They were a feature, not a product, and the market treated them accordingly.

**The lesson:** In AI, standing still is moving backward. The models improve. The competitors improve. If you're not compounding your advantages — through data, evaluations, and product depth — you're losing them.

---

### The Pattern

The products that survive have three things in common: they build moats beyond the model, they architect for change, and they invest in evaluation as a core competency. The products that don't survive treat the model as the product and the launch as the finish line.

---

*That wraps up Chapter 6. You now understand the market you're building into — the dynamics, the risks, and what separates products that last from products that don't. In Chapter 7, we'll get practical: how to go from an idea to your first evaluation, the step that separates thinkers from builders.*
