# Chapter 5.5 — User Tolerance for Errors: By Product Type

Not all errors are equal, and not all users react to errors the same way. A customer support chatbot that gets 1 out of 10 answers wrong might be fine. A medical AI that gets 1 out of 10 diagnoses wrong is a lawsuit. Understanding your users' error tolerance — and designing for it — is one of the most important scoping decisions you'll make.

---

### The Error Tolerance Spectrum

**Very high tolerance (errors are acceptable):**
- Code completion tools: developers expect some bad suggestions and quickly dismiss them
- Email draft generators: users edit every output anyway
- Creative writing assistants: "wrong" is subjective, and users treat outputs as starting points
- Internal search: employees can verify results themselves

Acceptable error rate: 15-30% of outputs may need correction or rejection.

**Moderate tolerance (errors are annoying but manageable):**
- Customer support chatbots: wrong answers frustrate users but can be corrected through escalation
- Product recommendations: irrelevant suggestions waste time but don't cause harm
- Meeting summarizers: missing or wrong points are caught by attendees
- Content personalization: irrelevant content is scrolled past

Acceptable error rate: 5-15% of outputs may be wrong, but fallback paths exist.

**Low tolerance (errors are costly):**
- Financial analysis tools: wrong numbers lead to bad decisions
- Legal research assistants: wrong citations waste expensive lawyer time
- Hiring screening tools: wrong assessments affect people's careers
- Contract analysis: missed clauses create business risk

Acceptable error rate: 2-5% of outputs may be wrong, with mandatory human review.

**Near-zero tolerance (errors are harmful):**
- Medical diagnostic support: wrong diagnoses affect patient health
- Autonomous vehicle decisions: wrong actions endanger lives
- Drug interaction checkers: missed interactions can be fatal
- Safety-critical industrial controls: errors cause physical damage

Acceptable error rate: under 1%, with comprehensive human oversight.

---

### How Error Tolerance Shapes Your Product

**The error cost equation.** For every product, calculate: (probability of error) times (cost per error) equals (expected error cost). If expected error cost is tolerable, ship. If not, either reduce the probability (better model, better evaluation) or reduce the cost (add human review, limit the blast radius).

Example: A customer support bot handles 10,000 queries/day with a 10% error rate. Each error causes a support escalation costing $15 in agent time. Expected daily error cost: $15,000. Is that acceptable? Compare against the cost of human agents handling all 10,000 queries: $150,000/day. Even with errors, the bot saves $135,000/day — if the error rate stays at 10%.

**Design for the tolerance, not the ideal.** If your users tolerate 10% errors, don't spend six months getting from 90% to 98%. Ship at 90%, monitor, and improve iteratively. The marginal cost of each percentage point of accuracy increases exponentially, and the first version that's "good enough" teaches you more than months of offline optimization.

**Different user segments have different tolerances.** Power users tolerate more errors because they can identify and correct them. New users tolerate fewer errors because they don't know when the AI is wrong. Enterprise buyers tolerate fewer errors because they're liable to their own customers. Segment your user base and design for the least tolerant segment that represents a meaningful portion of your business.

---

### The Trust Decay Problem

Error tolerance isn't static — it decays with exposure. A user might tolerate the first wrong answer ("AI isn't perfect, I get it"). By the fifth wrong answer, they've stopped trusting the system. By the tenth, they've stopped using it.

This means your effective error tolerance is lower than any single-interaction measurement suggests. A 90% accuracy rate doesn't mean users are happy 90% of the time. It means that over 20 interactions, every user has experienced 2 failures — and some users have experienced 5 or more.

Design for accumulated trust, not individual accuracy. A system that's 88% accurate but gracefully acknowledges uncertainty ("I'm not sure about this — would you like me to connect you with a specialist?") builds more trust than one that's 92% accurate but confidently wrong the other 8%.

---

*Next: fallback design — because the question isn't whether your AI will fail, it's what happens when it does.*
