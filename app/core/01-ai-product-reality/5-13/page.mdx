# Chapter 5.13 — Explainability: When and How to Open the Black Box

"Why did the AI say that?" This is the question users, regulators, and your own team will ask. How you answer it — or whether you can answer it at all — depends on decisions you make during the scoping phase, not after launch.

---

### When Explainability Is Required

**Legally required.** The EU AI Act requires transparency for high-risk AI systems. GDPR's Article 22 gives individuals the right to "meaningful information about the logic involved" in automated decisions that significantly affect them. The US Equal Credit Opportunity Act requires creditors to explain adverse decisions. If your product makes decisions about people, explainability may be a legal requirement.

**Commercially required.** Enterprise buyers in regulated industries (banking, insurance, healthcare) will ask: "Can you explain how the AI reached this conclusion?" If your answer is "no, it's a black box," you lose the deal. Explainability is a sales requirement before it's a technical requirement.

**Trust required.** Users trust AI more when they understand its reasoning. A medical AI that says "you might have condition X because your symptoms Y and Z are commonly associated with it" is more trustworthy than one that just says "you might have condition X." Even in low-risk products, explainability improves the user experience.

**Debugging required.** When the AI gives a wrong answer, your team needs to understand why. Without explainability, debugging is guesswork. With it, you can trace the reasoning, identify the failure point, and fix the root cause.

---

### The Explainability Spectrum

Not every product needs the same level of explainability. Design for the level your use case requires:

**Level 0: No explanation.** The AI gives an output with no reasoning. Acceptable for: creative writing, brainstorming, code suggestions where the user can evaluate the output themselves.

**Level 1: Confidence indication.** The AI indicates how sure it is. "I'm fairly confident" vs "I'm not sure about this." Acceptable for: internal tools, search suggestions, recommendations.

**Level 2: Source attribution.** The AI shows where its answer came from. "Based on document X, section Y..." Acceptable for: RAG systems, research tools, knowledge bases.

**Level 3: Reasoning trace.** The AI shows its step-by-step reasoning. "I considered factors A, B, and C. Factor A suggests X, but factors B and C suggest Y, so I recommend Y." Acceptable for: decision support tools, analysis products, advisory systems.

**Level 4: Counterfactual explanation.** The AI explains what would need to change for a different outcome. "Your application was declined because your credit score is 620. If it were above 680, the decision would likely be different." Required for: credit decisions, insurance underwriting, hiring decisions.

---

### Technical Approaches to Explainability

**Chain-of-thought prompting.** Ask the model to "think step by step" and show its reasoning. This is the simplest approach and works well for Level 3 explanations. The caveat: chain-of-thought reasoning doesn't always reflect the model's actual decision process — it's a post-hoc rationalization that may or may not be accurate.

**Retrieval attribution.** For RAG systems, show the user which documents were retrieved and which parts of those documents informed the answer. This provides Level 2 explainability and is relatively straightforward to implement — it's an architecture feature, not a model feature.

**Feature importance.** For classification tasks, identify which input features most influenced the decision. "The email was classified as urgent because it contained the words 'deadline' and 'ASAP' and was from a VIP customer." This works for structured inputs and provides Level 3-4 explainability.

**Structured output with reasoning.** Instead of generating a single answer, generate a structured response that includes: the answer, the confidence, the key factors, and any caveats. This gives you Level 3 explainability as a byproduct of your output design.

---

### Scoping Explainability

During the requirements phase, determine:

1. **What level of explainability does our use case require?** Match to the spectrum above.
2. **Is explainability legally required?** Check the regulations that apply to your product and geography.
3. **Is explainability commercially required?** Will customers demand it?
4. **What technical approach will we use?** Choose and architect for it from the start.
5. **How will we validate explanations?** An explanation that's wrong is worse than no explanation. You need to evaluate the quality of explanations, not just the quality of answers.

Retrofitting explainability is expensive. Building for it from the start is incremental. Make the decision now.

---

*That wraps up Chapter 5. You now have a complete scoping and requirements framework for AI products — from PRDs that work, to success criteria, tradeoffs, error tolerance, fallbacks, data needs, compliance, threat modeling, SLOs, unit economics, and explainability. In Chapter 6, we'll look at the market you're building into — because even the best product fails if the market isn't ready or the competition is insurmountable.*
