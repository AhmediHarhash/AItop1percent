# 4.3 — ML Engineers vs AI Engineers vs Software Engineers

In June 2025, a financial services company hired two engineers for their new AI-powered fraud detection product. Both had "AI Engineer" in their job titles. One had a PhD in machine learning, published research in neural network architectures, and spent five years at a research lab training large-scale models. The other had three years of experience building production applications on top of GPT-4 and Claude, with deep expertise in prompt engineering, RAG architectures, and evaluation frameworks. The company put them both on the same team and expected them to collaborate seamlessly.

Within a month, the tension was obvious. The PhD wanted to train a custom model from scratch using the company's proprietary fraud data, arguing that a fine-tuned model would outperform generic foundation models for this specialized use case. The production-focused engineer wanted to build a prompt-based system using Claude with structured outputs, arguing that this would ship in weeks instead of months and deliver sufficient accuracy for the initial version. Both were right within their mental models. Both were wrong about the other's approach being inferior. The project stalled for seven weeks while leadership tried to mediate, eventually splitting the team into two parallel efforts that duplicated work and created integration headaches.

The root problem was that these two people were not the same kind of engineer, despite having similar titles. Their skills, mental models, and instincts were fundamentally different. One was an ML engineer. The other was an AI engineer. These are distinct roles, and treating them as interchangeable creates expensive mistakes. Add software engineers to the mix, and you have three engineering roles that companies routinely confuse, mis-hire for, and mis-assign work to.

## ML Engineers: Model Training and Research

The ML engineer's core skill is building and training models. They understand model architectures deeply: transformer layers, attention mechanisms, loss functions, optimization algorithms. They work with PyTorch or TensorFlow or JAX. They think in terms of training loops, gradient descent, hyperparameter tuning, and model evaluation in the research sense—precision, recall, F1 scores, AUC-ROC curves.

They manage training infrastructure: GPUs, distributed training, data pipelines that feed training jobs, experiment tracking systems. They design training procedures: what data to train on, how to structure it, how long to train, when to stop, how to prevent overfitting. They debug model behavior at the architectural level: why is the loss not decreasing, why is the model memorizing training data, why is it performing poorly on out-of-distribution examples.

When you need them: You're training or fine-tuning custom models. You have proprietary data and a clear hypothesis that a model trained on your specific data will outperform general-purpose foundation models. You're building specialized classifiers, embedding models, or domain-specific models where off-the-shelf solutions don't exist or don't perform well enough. You're doing research-adjacent work that requires deep model expertise: experimenting with novel architectures, optimizing training efficiency, or pushing the boundary of what's possible in your domain.

You also need ML engineers when you're working at sufficient scale that even small model improvements translate to large business impact. If you're processing millions of queries per day, and a custom model can reduce error rates by 2% or reduce inference costs by 15%, the ROI on having an ML engineer train and optimize that model is clear.

When you don't need them: You're building products on top of foundation model APIs—GPT-5, Claude Opus 4.5, Gemini 2.0. Your product is primarily prompt engineering, orchestration, and application logic. You don't have training data, or the data you have isn't sufficient to train a model that would outperform prompted APIs. You're at the prototype or early production stage where speed matters more than squeezing every point of performance improvement.

Most companies in 2026 don't need ML engineers in the early stages. They need them later, when they have product-market fit, real usage data, and clear evidence that a custom model would provide meaningful advantage. Hiring ML engineers too early is expensive, because ML engineering talent is scarce and commands high salaries, and because ML engineers working on problems that don't require custom models get bored and leave.

## AI Engineers: Application-Layer AI Integration

The AI engineer role emerged and scaled rapidly in 2024 through 2026 as foundation model APIs became the dominant way to build AI products. The AI engineer's core skill is building products that use AI models. They understand prompt engineering deeply: how to structure prompts for different tasks, how to use few-shot examples, how to handle edge cases, how to optimize for quality and cost simultaneously.

They understand RAG architectures: how to chunk documents, how to generate and store embeddings, how to retrieve relevant context, how to format retrieved information into prompts, how to handle cases where retrieval fails or returns irrelevant results. They understand agent frameworks: how to give models tools, how to structure multi-step reasoning, how to handle errors and retries, how to orchestrate multiple model calls into coherent workflows.

They build evaluation infrastructure, and this is one of the most important parts of the role. They design evaluation frameworks, create and maintain evaluation datasets, write automated scoring functions, build dashboards that track quality metrics over time, and run regression tests before shipping changes. They understand that evaluation is not a one-time task but an ongoing practice that needs to scale with the product.

They understand the operational aspects of running AI in production: how to manage API rate limits, how to handle failures gracefully, how to optimize prompts for latency and cost, how to cache effectively, how to monitor quality in production, how to route between different models based on task requirements or cost constraints.

They work with APIs, orchestration libraries, evaluation tools, and vector databases. They write code, but their code is often more about glue and logic than about training or optimizing models. They're building the layer between the foundation model and the user-facing product.

When you need them: You're building products on top of foundation model APIs, which describes most AI product companies in 2026. You need prompt engineering, RAG implementation, agent design, and evaluation infrastructure. You're integrating AI into existing software products. You're at any stage from prototype to scale, because these skills are needed throughout the product lifecycle.

AI engineers are often the first AI-focused hire for teams building on APIs. They can move quickly, ship prototypes, validate product ideas, and then scale those prototypes into production systems with proper evaluation and monitoring. They bridge the gap between the foundation model's raw capabilities and the specific behavior your product needs.

When you don't need them: You're doing fundamental model research with no near-term product application. Your work is primarily traditional software with a thin AI layer—maybe a single API call to generate text or classify sentiment—that a software engineer can handle without specialized AI knowledge.

## Software Engineers: Infrastructure and Reliability

The software engineer builds the application that the AI sits inside. They own APIs, databases, authentication, frontend, deployment pipelines, monitoring, security, and all the infrastructure that makes the AI accessible, reliable, and safe. They handle the non-AI parts of the product, which in most AI products is still the majority of the system.

They ensure the product works even when the AI doesn't. They implement fallback paths: what happens when the model is unavailable, when it returns an error, when it's taking too long. They implement graceful degradation: how to degrade functionality when the AI is slow or unreliable without breaking the entire user experience. They implement retry logic, circuit breakers, and error handling.

They build the user interface, the data storage layer, the integrations with other systems. They manage deployment: CI/CD pipelines, staging environments, production rollouts, rollback procedures. They implement security: input validation, rate limiting, abuse prevention, authentication, authorization. They monitor system health: uptime, latency, error rates, infrastructure costs.

In an AI product, software engineers are essential, and their work is often underestimated. The AI is the differentiation, but the application is what makes the AI usable. Bad infrastructure, high latency, poor error handling, security vulnerabilities, and unreliable deployments will kill an AI product as quickly as bad model quality will.

When you need them: Always. Every AI product needs software engineering. Even if your product is entirely AI-driven, you need someone to build the API that serves it, the frontend that users interact with, the backend that manages state and data, and the infrastructure that keeps everything running reliably.

Software engineers who work on AI products don't need to understand model training, but they do need to understand how to integrate with AI systems. They need to know how to handle non-deterministic behavior, how to design for cases where the AI is slow or unavailable, how to monitor and debug issues that span traditional software and AI components, and how to think about cost in a system where compute is expensive and usage-dependent.

## The Overlap and the Boundaries

In practice, the best AI product teams have engineers who span these categories. An AI engineer who can also build solid application infrastructure and doesn't need to hand off every API design or database schema to a software engineer. A software engineer who understands enough about prompting and evaluation to make good design decisions about how to structure the AI's inputs and outputs. An ML engineer who understands the application context well enough to optimize for the right metrics.

Rigid role boundaries create handoff problems. If the AI engineer designs a prompt that requires a specific input format, but the software engineer building the API doesn't understand why that format matters, you get integration bugs. If the software engineer implements error handling that swallows useful error messages from the AI, and the AI engineer doesn't see those errors, you can't debug quality issues effectively. If the ML engineer trains a model without understanding how it will be used in the application, you get a model that's optimized for the wrong metrics.

The solution is not to eliminate specialization—specialization is necessary because the skills are deep and distinct—but to ensure enough overlap that people can communicate effectively and understand each other's constraints. The AI engineer should know enough about software engineering to design systems that are buildable. The software engineer should know enough about AI to design infrastructure that supports AI's unique requirements. The ML engineer should know enough about product to ensure their models solve real problems.

## The Staffing Decision by Stage

At the prototype or MVP stage, you typically need an AI engineer and a software engineer. You're building on APIs, not training models. You need prompt engineering, basic evaluation, and solid application development to make the AI accessible and usable. An ML engineer is usually overkill unless your use case genuinely requires custom training from day one, which is rare. Most early-stage teams that hire ML engineers end up underutilizing them because there's not enough model work to justify the role.

At the production stage, as you scale from prototype to real users, you add an evaluation specialist or a dedicated AI engineer who focuses on evaluation while another focuses on feature development. Evaluation becomes a full-time job as your product grows. The person building the evaluation framework, maintaining the evaluation dataset, running quality reviews, and analyzing failure modes shouldn't also be designing prompts, building features, and responding to production incidents. Splitting the AI engineering work between feature development and evaluation quality improves both.

At the scale and differentiation stage, you add an ML engineer. This happens when you have enough data, enough usage, and enough evidence that a custom or fine-tuned model would outperform prompted APIs by a margin that justifies the investment. The ML engineer starts exploring custom models, fine-tuning, or other optimization approaches that go beyond prompt engineering. But this should be driven by clear evidence, not by assumption. Many teams hire ML engineers prematurely because they assume "real AI companies train models" when in reality most successful AI products in 2026 are built entirely on API calls to foundation models.

## The Hiring Trap

The most common hiring mistake in 2026 is hiring ML engineers when you need AI engineers. Companies see "AI product" and think "we need ML expertise," so they hire PhDs with deep research backgrounds who are brilliant at training models but have never built a production application on an API, never designed a RAG system, never written evaluation frameworks, and don't have intuition for the operational challenges of running AI in production.

The result is over-engineered model infrastructure when the real need was good prompt engineering, solid evaluation, and reliable application code. The ML engineer spends weeks setting up training pipelines and experimenting with fine-tuning when a well-designed prompt would have shipped in days and performed adequately. Or the ML engineer builds a custom model that performs slightly better than the API-based baseline but requires ongoing maintenance, monitoring, and retraining, adding operational overhead that outweighs the performance gain.

This wastes money and time. ML engineering talent is expensive and scarce. If you hire an ML engineer and don't have model work for them to do, they get bored and leave, or they create model work that doesn't need to exist because they want to use their skills. Both outcomes are bad.

The reverse mistake is less common but still happens: hiring software engineers and expecting them to handle AI integration without AI-specific skills. The software engineer can integrate the API call, but they don't know how to design effective prompts, build evaluation frameworks, or debug why the AI is producing bad outputs. The product ships with shallow AI integration that doesn't realize the model's full potential because nobody on the team knows how to unlock it.

## Hiring for What You Need Now

Hire for what you need now, not for what you imagine you might need later. If you're building on APIs, hire AI engineers. If you're building application infrastructure, hire software engineers. If you have clear evidence that custom models would provide meaningful advantage and you have the data and scale to support model training, hire ML engineers.

Plan for what you'll need later, but don't hire prematurely. It's easier to add an ML engineer when you have product-market fit and real data than it is to keep an ML engineer engaged when you're still figuring out whether the product works at all.

Be honest about which stage you're at. Most teams in 2026 are at the API-based, prompt-engineering, evaluation-focused stage. If that's you, hire AI engineers who are great at prompt design, RAG systems, evaluation, and production AI operations. Don't hire ML researchers unless you have research problems. Don't hire software engineers who've never worked with non-deterministic systems and expect them to figure it out.

The teams that staff correctly ship faster, spend less, and avoid the organizational friction of having the wrong people in the wrong roles. The teams that staff incorrectly spend months on work that doesn't need to be done, or they ship products without the evaluation and quality infrastructure that makes AI reliable.

Once you have the right people in place, the next structural question is who owns evaluation, because evaluation ownership determines whether your quality process actually works or becomes a bureaucratic formality that nobody trusts.
