# 4.7 — Building vs Hiring: Team Composition at Each Stage

In mid-2025, a well-funded AI startup burned through eighteen months of runway in nine months. They had raised $4 million in seed funding and immediately hired a team of twelve: three ML engineers specializing in model optimization, two data scientists for experimental research, a VP of Engineering, a product designer, two full-stack engineers, a DevOps specialist, a data engineer, and a PM. The team was impressive on paper. The investors were thrilled.

The problem was that the company had no users. They had a hypothesis about automating contract review for legal teams, but they had not validated product-market fit. They had not spoken to enough lawyers. They had not built a prototype that anyone was willing to pay for. What they had was a team optimized for scale when they needed a team optimized for discovery. The ML engineers spent months debating model architectures when the company needed someone to talk to customers and figure out if the idea was worth building at all. By the time they realized they had the wrong team for their stage, they had burned $1.8 million on payroll and had nothing to show for it except technical infrastructure no one was using.

The failure was not technical. It was a team composition failure. They staffed for the future instead of the present. They hired for the company they hoped to become instead of the company they were. In AI products, this mistake is lethal because the path from idea to scale is longer and more uncertain than in traditional software. You cannot afford to hire specialists before you have earned the right to need them.

## The Stage-Based Hiring Framework

AI product teams evolve through four distinct stages, and each stage demands a different composition. The mistake most teams make is hiring linearly — adding headcount as budget allows — instead of hiring strategically based on what the current stage actually requires. The right hire at the wrong stage is still the wrong hire.

At the prototype stage, you need one to three people who can do everything. You need generalists who can write prompts, build evals, ship code, and talk to users. You do not need specialists. You do not need a VP of anything. You need people who can move fast across the entire stack without needing coordination overhead.

At the MVP stage, you add depth in the areas that are becoming bottlenecks. If the application is brittle, you add a software engineer who can harden it. If the problem definition is vague, you add a PM who can sharpen it. If domain knowledge is blocking progress, you bring in a domain expert as a contractor or advisor. You are still a small team — three to six people — but you are adding targeted capability in the areas where generalists are no longer sufficient.

At the production stage, you need operational rigor. You add an evaluation specialist because eval is now a dedicated function, not something the prompt engineer does on the side. You add monitoring and reliability engineering because uptime matters. If you are building a Tier 3 or Tier 4 system, you add legal and compliance resources because risk is real. The team is six to fifteen people, and you are starting to specialize.

At the scale stage, you split into sub-teams. You have a model team, an application team, an evaluation team, and a platform team. You hire ML engineers if you are fine-tuning or training models. You hire data engineers to manage training and eval data pipelines. You hire a trust and safety lead if your product has user-generated content or safety-critical applications. The coordination overhead increases, but so does your capability. You are no longer a scrappy startup. You are an organization.

## The Prototype Stage: Generalists Only

The prototype stage is about speed and learning. You have an idea, a thesis about what AI might be able to do for a specific use case, and you need to find out if that thesis is correct. You do not need a large team. You need one exceptional person or a very small team of two to three who can move without friction.

The most valuable profile at this stage is the full-stack AI engineer. This is someone who can write effective prompts, design a basic evaluation framework, build an API, deploy it, and iterate based on user feedback. They are not deep specialists in any one area, but they are competent across the entire stack. In 2026, this profile is rare and expensive, but it is also the highest-leverage hire you can make at this stage. One full-stack AI engineer can do the work of three specialists without the coordination cost.

You do not need an ML engineer at this stage unless your use case genuinely requires custom model training, which is rare. Most AI products in 2026 are built on frontier models accessed via API. Prompt engineering, eval design, and application logic are the bottlenecks, not model training. Hiring an ML engineer to optimize a model you do not own yet is premature.

You do not need a data scientist. Data scientists are valuable when you have data to analyze and hypotheses to test at scale. At the prototype stage, you have neither. You have user interviews and a handful of test cases. A data scientist will be bored and underutilized.

You do not need a QA specialist. Quality at this stage is about whether the product works for the core use case, not whether it handles every edge case gracefully. The person building the product is also the person testing it. Dedicated QA is a luxury you cannot afford and do not need.

You do not need domain experts on staff. You need access to domain expertise, which is different. Bring in domain experts as contractors, advisors, or beta testers. Have them review your eval set. Have them validate your output quality. Pay them for a few hours a week, not a full-time salary. If you hire a domain expert full-time before you have product-market fit, they will spend most of their time waiting for you to build something worth reviewing.

The right team at this stage is one or two AI engineers, possibly one software engineer if the AI engineer is not strong on application development, and regular access to domain expertise through contractors. That is it. Anything more is overhead.

## The MVP Stage: Adding Depth Where It Matters

Once you have a working prototype and early users who are willing to use it despite its rough edges, you move to the MVP stage. The goal here is to validate product-market fit and harden the product enough that users can rely on it. You are no longer optimizing for speed alone. You are optimizing for reliability and repeatability.

The first hire at this stage is often a PM. Someone needs to own the problem definition, manage the backlog, talk to users, synthesize feedback, and decide what gets built next. At the prototype stage, the AI engineer can wear this hat. At the MVP stage, the product complexity and the user feedback volume make this untenable. You need a dedicated PM who understands AI well enough to make informed tradeoffs about quality, cost, and capability.

The second hire is usually a software engineer focused on hardening the application. The prototype was functional but fragile. The MVP needs to be reliable. It needs error handling, logging, monitoring, and graceful degradation. It needs to handle edge cases that the prototype ignored. A software engineer with production experience can build this robustness without slowing down iteration speed.

If domain expertise is critical to your use case — and it usually is — you formalize your relationship with a domain expert. This might mean converting an advisor to part-time staff, or it might mean setting up a regular weekly review cadence with a contractor. The domain expert reviews output quality, validates eval cases, and helps the team understand what good looks like in the specific domain you are targeting.

You still do not need an ML engineer unless your use case has proven that prompt engineering and frontier models are insufficient. Most teams at this stage discover that the bottleneck is not the model. It is the eval framework, the application logic, or the problem definition. Adding an ML engineer before you have exhausted the capabilities of existing models is premature optimization.

You still do not need a compliance officer unless you are entering a regulated market or handling sensitive data. If you are building for healthcare, finance, or government, bring in a compliance consultant to review your architecture and data handling. If you are building a general-purpose tool, defer this hire until production.

The MVP team is typically three to six people: one or two AI engineers, one or two software engineers, a PM, and access to domain expertise. This team can ship a reliable product, gather user feedback, iterate quickly, and validate whether the product has enough value to scale.

## The Production Stage: Specialization and Operations

When you have paying customers and a product that works reliably enough that people depend on it, you enter the production stage. The stakes are higher. Quality regressions affect real users. Downtime costs money and trust. You need operational rigor.

The most critical hire at this stage is an evaluation specialist. Evaluation is no longer something the AI engineer does in their spare time. It is a dedicated function. Someone needs to own the eval framework, maintain the eval set, run evals on every change, track quality metrics over time, and surface regressions before they reach production. This person is often an AI engineer who specializes in eval infrastructure, or it might be someone with a QA background who has learned AI evaluation. Either way, this role is non-negotiable at production scale.

You add software engineers focused on reliability, monitoring, and scaling. The application needs to handle higher traffic, recover gracefully from failures, and provide visibility into what is happening under the hood. You need observability tooling, incident response processes, and on-call rotations. This is traditional DevOps and SRE work, but applied to AI systems where debugging is harder because behavior is probabilistic.

If you are building a Tier 3 or Tier 4 system — anything that affects user trust, safety, or regulatory compliance — you bring in legal and compliance resources. This might be a full-time hire, or it might be a fractional legal advisor who reviews your risk posture, data handling, and incident response processes. The cost of getting this wrong at production scale is existential.

If domain expertise is central to your product quality, you may convert your domain advisor to full-time staff. A contract review tool needs a lawyer on the team. A medical triage system needs a clinician. A financial planning assistant needs a CFP. The domain expert is no longer just reviewing outputs. They are contributing to product design, eval criteria, and quality thresholds.

You still might not need an ML engineer. If you are using frontier models via API and prompt engineering is getting you the quality you need, there is no reason to hire someone to fine-tune or train models. ML engineers are expensive and specialized. Only hire them when you have data, infrastructure, and a proven need for custom models. Many successful AI products at production scale never hire a dedicated ML engineer because they do not need one.

The production team is typically six to fifteen people, with clear functional ownership. Someone owns eval. Someone owns reliability. Someone owns product. Someone owns domain quality. The team is no longer a generalist collective. It is a set of specialists coordinating to deliver a reliable product.

## The Scale Stage: Teams Within Teams

At scale, you are no longer one team. You are multiple teams, each with a distinct focus. The model team works on fine-tuning, evaluation, and model performance. The application team builds features, improves UX, and integrates AI into the broader product. The evaluation team maintains the eval framework, runs experiments, and tracks quality. The platform team builds shared infrastructure: prompt management, observability, cost tracking, and deployment pipelines.

This is the stage where you finally hire ML engineers, assuming your use case justifies it. You have enough data to train or fine-tune models. You have enough traffic to justify the engineering cost of custom model development. You have proven that frontier models alone are not sufficient. Now you need someone who can train, evaluate, and optimize models at scale.

You hire data engineers to build and maintain data pipelines for training data, eval data, and production logs. AI at scale generates massive amounts of data, and managing that data is a full-time job. Data engineers build the infrastructure that makes it possible to retrain models, refresh eval sets, and analyze production behavior.

You hire platform engineers who build internal tooling for the rest of the AI organization. They build prompt versioning systems, A/B testing frameworks, cost attribution dashboards, and deployment automation. Platform engineering at this stage is what enables the rest of the organization to move quickly without reinventing infrastructure.

If your product has user-generated content, moderation workflows, or safety-critical applications, you hire a trust and safety lead. This person owns content policy, moderation processes, escalation workflows, and safety evaluations. They work closely with legal, product, and engineering to ensure the product does not cause harm at scale.

For Tier 4 systems — anything in healthcare, finance, or other heavily regulated domains — you may need a dedicated compliance team. This is not a single contractor. It is a function with full-time staff who understand the regulatory landscape, manage audits, maintain documentation, and ensure the product meets legal requirements.

The scale-stage team is fifteen or more people, often organized into sub-teams with clear charters. Coordination overhead is real, but you have earned the right to this complexity because you have a product that works, users who depend on it, and revenue that justifies the investment.

## The Contractor Strategy: Buy Before You Build

Not everything needs a full-time hire. Many capabilities are needed intermittently or at low intensity, and hiring full-time for those needs is wasteful. Use contractors and advisors strategically.

Domain expertise during prototype and MVP stages is almost always better as contract work. You need a few hours a week to review outputs, validate eval cases, and answer questions. You do not need someone sitting in your office full-time. Hire a domain expert as an advisor, pay them for weekly review sessions, and convert them to full-time only when the work becomes continuous and core to product quality.

Security audits are project-based work. You do not need a full-time security engineer during early stages. Hire a security contractor to review your architecture before launch, audit your data handling, and run penetration tests. Do this quarterly or before major releases. Pay for the work when you need it, not year-round.

Compliance reviews are the same. Before entering a regulated market, hire a compliance consultant who specializes in that domain. Have them review your data handling, your terms of service, your incident response plan, and your documentation. Get their signoff before launch. You can convert this to a full-time hire later if regulatory complexity justifies it.

Red team testing before major launches is contract work. Hire AI safety researchers or adversarial testers to try to break your product. Pay them to find failure modes, safety violations, and edge cases you missed. Use their findings to improve the product. Do not hire them full-time unless you are building a product where adversarial robustness is a continuous concern.

Specialized ML work — one-time fine-tuning, model evaluation, or experiments — can often be done by contractors. If you need to fine-tune a model once to see if it improves quality, hire an ML contractor for a three-month project. If the experiment works and you need ongoing ML capacity, convert the role to full-time. If it does not work, you saved yourself the cost of a bad hire.

The principle is simple: hire contractors when the work is intermittent, specialized, or exploratory. Hire full-time when the work is continuous, core to the product, and requires deep context. The right time to convert a contractor to full-time is when they are working every week and the work is bottlenecking progress. The wrong time is when you hope the work will become important someday.

## The Hiring Priority Matrix

Different stages have different hiring priorities. The most critical hire at the prototype stage is useless at the scale stage, and vice versa. Here is how to prioritize.

At the prototype stage, the most critical hire is a full-stack AI engineer. This person unblocks everything. The second priority is domain expertise, but as contract work, not a full-time hire. Defer everything else: ML engineers, QA, legal, compliance, data engineers, platform engineers. You do not need them yet.

At the MVP stage, the most critical hires are a PM and a software engineer. Someone needs to own the product, and someone needs to harden the application. The second priority is formalizing domain expertise, either as part-time staff or a committed contractor. Defer ML engineering unless you have proven that prompt engineering is insufficient. Defer compliance unless you are entering a regulated market.

At the production stage, the most critical hire is an evaluation specialist. Quality is now a dedicated function. The second priority is domain expertise, potentially converting to full-time if domain quality is central to your value proposition. The third priority is legal or compliance resources if you are Tier 3 or higher. You might still defer ML engineering if frontier models are sufficient.

At the scale stage, the most critical hires depend on your bottlenecks. If you need custom models, hire ML engineers. If you need shared infrastructure, hire platform engineers. If you need trust and safety, hire a T&S lead. There is no single answer because scale-stage teams are diverse and specialized.

The meta-principle is this: hire for the bottleneck you have today, not the bottleneck you fear you will have tomorrow. AI products evolve quickly. The team that got you here will not get you there, but the team you need next year will be wrong if you hire them this year. Build the team for the stage you are at, and add capability as you earn the right to need it.

