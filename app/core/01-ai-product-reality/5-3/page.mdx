# Chapter 5.3 — Defining Success Criteria Before You Build

If I could give one piece of advice to every AI product team, it would be this: define how you'll know the product is working before you write the first prompt.

It sounds obvious. It almost never happens. Teams start building because building is exciting. Defining success criteria is boring paperwork. But the teams that do this boring paperwork before building ship products. The teams that skip it build things that nobody can agree are working.

---

### Why Up-Front Success Criteria Matter

**They prevent the "is it good enough?" debate.** Without pre-defined criteria, every release becomes an argument. The PM thinks it's ready. The ML engineer thinks it needs more work. The CEO thinks it's amazing because they saw a cherry-picked demo. With pre-defined criteria, the conversation changes: "We said we'd ship at 85% accuracy. We're at 82%. We're not there yet. Here's the plan."

**They align the team.** When everyone agrees on what success looks like before building, they make consistent decisions during building. The engineer optimizes for the right metrics. The PM makes tradeoff decisions with the right framework. The domain expert evaluates against the right standard.

**They create accountability.** Pre-defined success criteria make it clear whether the project succeeded or failed. That clarity is uncomfortable — which is exactly why it works. Nobody can hide behind "it depends" when the criteria are written down.

**They enable iteration.** "V1: 80% accuracy. V2: 90%." This tells the team what to aim for now and what to aim for next. Without it, teams either aim for perfection (and never ship) or aim for nothing (and ship garbage).

---

### How to Define Success Criteria

**Step 1: Identify your primary metric.**
What's the single most important indicator of success? For a customer support chatbot, it might be "resolution rate without human escalation." For a document extraction tool, it might be "field-level accuracy." For a search product, it might be "percentage of queries where the correct document appears in the top 3 results."

Choose one primary metric. You'll have secondary metrics too, but one metric should be the North Star.

**Step 2: Set a specific target.**
"Improve quality" is not a success criterion. "Achieve 85% resolution rate" is. The target should be:
- Specific: a number, not a direction
- Measurable: you can calculate it from data
- Achievable: based on what's technically possible with current models
- Relevant: it reflects real business value
- Bounded: for V1, not forever

How to pick the number: look at human performance as an anchor. If human agents resolve 92% of queries, targeting 85% for AI is ambitious but reasonable. If you don't know human performance, measure it before you set the AI target.

**Step 3: Define secondary metrics and constraints.**
Your primary metric might be accuracy, but you also care about:
- Latency (responses under 3 seconds)
- Cost (under $0.05 per query)
- Safety (zero harmful outputs in 10,000 queries)
- User satisfaction (CSAT above 4.0/5.0)

List these as constraints: "we must achieve the primary metric while keeping these secondary metrics within these bounds."

**Step 4: Define failure criteria.**
Success criteria tell you when to ship. Failure criteria tell you when to stop. "If after 4 weeks of iteration we can't exceed 70% accuracy, we'll reassess the approach." This prevents zombie projects that limp along without ever meeting the bar.

**Step 5: Get sign-off.**
Write the criteria down. Share them with the PM, ML lead, engineering lead, and any domain experts. Get explicit agreement. "Do we all agree that if we achieve X while maintaining Y and Z, this is ready to ship?" If there's disagreement, resolve it now — not at launch.

---

### The Criteria Hierarchy

Not all criteria are equal. Structure them in tiers:

**Must-have (blocking):** If these aren't met, we don't ship. Period.
Example: accuracy above 80%, zero safety violations, latency under 5 seconds.

**Should-have (target):** We want these, and we'll work toward them, but they won't block V1.
Example: accuracy above 90%, CSAT above 4.2, cost under $0.03 per query.

**Nice-to-have (aspirational):** The ideal state we're working toward over multiple versions.
Example: accuracy above 95%, human-parity on our eval set, cost under $0.01 per query.

This hierarchy prevents perfectionism (the must-haves are achievable) while still pushing the team toward excellence (the aspirational targets are ambitious).

---

*Next: the eternal triangle — latency, cost, and quality — and how to navigate the tradeoffs.*
