# 5.3 â€” Defining Success Criteria Before You Build

In late 2024, a legal technology company spent nine months building an AI system to analyze contracts for compliance risks. The engineering team delivered a technically sophisticated solution using the latest models and retrieval techniques. The product manager thought it looked promising based on demos. The CEO thought it was ready to sell. The legal experts who would actually use it said the quality was nowhere near sufficient for their work. The system flagged some genuine risks but also generated dozens of false positives per contract, missed subtle compliance issues that experienced lawyers would catch, and used legal terminology imprecisely in ways that created liability concerns. Nobody could agree whether the product was ready to launch because nobody had agreed before building what ready meant.

The team spent another four months arguing about quality standards, rebuilding evaluation frameworks, and iterating toward a moving target that different stakeholders defined differently. The project eventually shipped, but it took 13 months instead of the planned six, morale was destroyed by constant rework, and the final product satisfied nobody because it represented a compromise between incompatible quality expectations rather than a clear vision of success. The root cause was not technical failure. It was the absence of defined, agreed-upon success criteria before development started. The team built first and defined success later, which meant they built the wrong thing and spent months trying to retrofit it into something acceptable.

This pattern repeats across the industry. Teams start building because building is exciting and feels like progress. Defining success criteria is boring administrative work that slows momentum. But skipping this step guarantees expensive rework, interpersonal conflict, and products that don't meet real needs. The teams that do this boring work upfront ship products that succeed. The teams that skip it ship products late or not at all.

## Why Up-Front Success Criteria Are Not Optional

Defining success criteria before building serves four critical functions that cannot be replicated by any other practice. First, it prevents the perpetual "is it good enough?" debate. Without predefined criteria, every milestone becomes an argument. Engineering thinks the system is ready because it meets technical specifications. Product thinks it needs more work because user testing revealed issues. Domain experts think it's fundamentally flawed because it doesn't match how they think about the problem. The CEO thinks it's brilliant because they saw a carefully selected demo. Nobody is wrong because nobody is working from the same definition of right.

With predefined criteria, the conversation changes completely. The team said before building that the system would ship when it achieved 87 percent accuracy on the agreed evaluation set, with latency under two seconds and fewer than three critical errors per 1,000 requests. Current measurements show 84 percent accuracy, 1.8 seconds latency, and four critical errors per 1,000 requests. The system is not ready to ship because it doesn't meet the criteria. The conversation is about the data, not about opinions. Either the team continues iterating toward the criteria, or they revisit the criteria and decide whether the original targets were realistic, but everyone is discussing the same objective reality.

Second, predefined success criteria align the entire team toward the same goal. When an engineer faces a tradeoff decision between optimizing for speed or accuracy, they know which to prioritize because the success criteria specified acceptable ranges for both. When a product manager decides what feedback to incorporate from user testing, they know whether to prioritize reducing error rates or improving edge case handling based on which dimension the success criteria emphasize. When a domain expert reviews system outputs, they evaluate against the defined quality standard rather than an idealized vision of perfection. Alignment eliminates wasted work and ensures everyone's effort contributes toward the same outcome.

Third, predefined success criteria create accountability. When you write down what success looks like before building, you create a record of expectations against which actual results can be measured. This is uncomfortable, which is precisely why it works. Teams cannot hide behind "we did our best" or "it's good enough for now" or "it depends on how you look at it" when the criteria are explicit and the measurements are objective. The project either met its goals or it didn't. That clarity forces honest conversations about whether the approach is working, whether the criteria were realistic, and whether to continue or pivot.

Fourth, predefined success criteria enable staged iteration without perfectionism. A team can define success as "version one achieves 80 percent accuracy suitable for beta launch, version two achieves 88 percent accuracy suitable for general availability, version three achieves 94 percent accuracy suitable for enterprise customers." These staged criteria set realistic expectations and create a path forward. The team doesn't aim for perfection before shipping, but they also don't ship inadequate quality and claim victory. They ship when they meet the defined criteria for the current stage and continue improving toward the next stage.

## The Three-Tier Success Framework

Success criteria for AI products fall into three categories: business metrics that measure whether the product delivers value, quality metrics that measure whether the AI performs adequately, and operational metrics that measure whether the system can run sustainably in production. All three tiers matter. A system that delivers business value but costs more to operate than it generates in revenue is not successful. A system that achieves perfect quality metrics on test data but degrades in production is not successful. A system that runs efficiently but doesn't solve the user's problem is not successful.

Business metrics connect the AI to actual value creation. For a customer support chatbot, business metrics might include resolution rate without human escalation, average time to resolution, customer satisfaction scores, and reduction in support ticket volume. For a document processing system, business metrics might include processing time savings compared to manual work, error reduction compared to previous methods, and user adoption rate. For a content recommendation system, business metrics might include click-through rate, time on site, and conversion rate.

The critical requirement for business metrics is that they measure outcomes that matter to the business, not just AI system performance. Accuracy is not a business metric. Time saved is. Model confidence scores are not a business metric. User satisfaction is. Engineering teams often want to optimize metrics they can directly control like model performance on evaluation sets. Business metrics force the team to optimize for what actually matters: whether the product creates value for users and the business.

Quality metrics measure AI performance on defined tasks. For a classification system, quality metrics might include precision, recall, F1 score, and confusion matrix analysis across different categories. For a generation system, quality metrics might include factual accuracy, relevance, coherence, and safety measured through human evaluation. For a retrieval system, quality metrics might include recall at various rank positions, mean reciprocal rank, and relevance scores.

Quality metrics must be measured against representative evaluation data, not cherry-picked examples. The evaluation set should cover the distribution of inputs the system will encounter in production, including common cases, edge cases, and adversarial cases. The criteria should specify both the target performance level and the evaluation methodology: what data is used, how it's sampled, how outputs are scored, and who does the scoring.

Operational metrics measure whether the system can sustainably run in production. Operational metrics include latency under realistic load, cost per request or per user, error rates and failure modes, system uptime and reliability, and scalability characteristics. A system might achieve excellent business and quality metrics in testing but fail operationally if it cannot handle production load, costs too much to run at scale, or fails in ways that require constant manual intervention.

Operational metrics should be measured under realistic conditions, not synthetic benchmarks. Latency should be measured with real data under expected concurrent load, not with a single query to an idle system. Cost should be calculated including all components: model inference, data processing, storage, and any supporting services. Error rates should include not just model errors but system errors: API timeouts, database failures, malformed inputs that crash processing pipelines.

## How to Set Specific, Measurable Targets

Vague success criteria like "good accuracy" or "fast response times" are not success criteria. They're aspirations that provide no guidance for decision-making. Useful success criteria specify exact targets with explicit measurement methodologies. The process for setting these targets involves understanding current baseline performance, researching comparable benchmarks, and negotiating realistic targets with stakeholders.

Start by measuring baseline performance: where are you today before any AI intervention? For a customer support chatbot replacing email support, baseline metrics might show that human agents currently resolve 91 percent of common questions within an average of 18 minutes with a customer satisfaction score of 4.1 out of 5.0. These baselines anchor your targets. An AI system that performs worse than the current baseline is not an improvement regardless of technical sophistication.

Research comparable systems to understand what's achievable. If competitors or similar products achieve 85 percent automation rates for customer support, that gives you a reference point. If published research shows state-of-the-art document extraction at 93 percent field-level accuracy, that tells you whether your target of 97 percent is realistic or fantastical. Benchmarks prevent both underambition and impossible expectations.

Negotiate targets through structured conversations with all stakeholders. The business leader wants to know what improvement justifies the investment. The domain expert wants to know what quality level is professionally acceptable. The engineer wants to know what's technically achievable with available resources and time. The finance team wants to know what cost structure is sustainable. These perspectives are often in tension, and the success criteria must balance them.

A structured negotiation might go like this. Engineering says: "Based on initial prototyping with GPT-4, we believe we can achieve 82 to 86 percent accuracy on contract risk detection within three months." The domain expert says: "Anything below 88 percent accuracy will generate too many false positives for lawyers to trust the system." Finance says: "At current model pricing, each contract analysis costs approximately 1.20 dollars. Our business model assumes 0.60 dollars per contract." Product says: "Our research shows that lawyers will accept some false positives if the system saves them significant time, but they will abandon the system entirely if it misses critical risks."

This conversation surfaces the constraints. The team cannot ship at 82 percent accuracy because domain experts won't accept it. They cannot ship with current cost structure because it's economically unviable. They cannot miss critical risks because that destroys user trust entirely. The negotiation yields success criteria: "Achieve 88 percent overall accuracy with zero misses on critical contract risks defined in the risk taxonomy. Reduce cost per contract to 0.65 dollars through prompt optimization and model selection. Latency must be under 30 seconds per contract to deliver time savings compared to manual review."

These criteria are specific, measurable, and represent negotiated agreement among stakeholders with different priorities. They're also achievable based on engineering's prototyping, though they require work to optimize both accuracy and cost from the initial baseline.

## Defining the Measurement Methodology

Success criteria are only as good as the methodology used to measure them. Two teams measuring "accuracy" might get completely different results if they're using different evaluation sets, different definitions of correct, or different sampling approaches. The success criteria must specify not just the target but exactly how achievement of that target will be verified.

For accuracy metrics, specify the evaluation dataset: its size, how it was created, how it's sampled from the production distribution, and how it's maintained over time. A complete specification might be: "Accuracy measured on an evaluation set of 2,000 contract analysis tasks created by sampling contracts from our production pipeline over the past 18 months, stratified by contract type (NDA, vendor agreement, employment agreement, license, partnership) to match production distribution. Ground truth labels provided by senior legal reviewers with at least five years of experience. Evaluation set is refreshed quarterly to capture evolving contract patterns."

This specification ensures everyone is measuring accuracy against the same data using the same standards. It also makes the measurement repeatable: a new engineer joining the team six months later can measure accuracy using the same methodology and get comparable results.

For subjective quality metrics like relevance or helpfulness, specify the evaluation rubric and the rater qualifications. A complete specification might be: "Helpfulness measured by three independent raters who are experienced customer support agents, using a 5-point rubric where 1 equals actively unhelpful or incorrect, 3 equals partially helpful but incomplete, and 5 equals fully resolves the customer's question with clear, accurate information. Raters evaluate 500 randomly sampled conversations per week. Inter-rater agreement must exceed 0.75 Cohen's kappa. Final score is the median rating across raters."

This specification defines who evaluates, using what rubric, with what quality controls for consistency, and how scores are aggregated. It transforms a subjective judgment into a structured, repeatable measurement.

For operational metrics like latency, specify the measurement conditions: load characteristics, input data distribution, percentile targets, and measurement instrumentation. A complete specification might be: "Latency measured as time from API request receipt to complete response delivery, excluding network transit time. Measured under simulated production load of 200 concurrent requests with input documents ranging from 1 to 75 pages matching the production document length distribution. Target: 95th percentile latency under 2.8 seconds, 99th percentile under 4.5 seconds, measured over rolling 7-day windows."

This specification ensures latency is measured under realistic conditions, uses percentiles to account for variability, and defines observation windows to distinguish sustained performance from brief anomalies.

The measurement methodology should also specify who has authority to verify that criteria are met. For business metrics, product leadership might verify. For quality metrics, domain experts might verify. For operational metrics, engineering might verify with oversight from finance on cost metrics. Specifying verification authority prevents arguments about whether standards are met and ensures appropriate expertise reviews each dimension.

## Failure Criteria and Pivot Points

Success criteria tell you when to ship. Failure criteria tell you when to stop and reassess whether your current approach can work. Without failure criteria, projects continue indefinitely making marginal improvements toward targets they'll never reach, wasting resources and morale. Failure criteria create decision points where the team must honestly evaluate whether to continue the current approach, pivot to a different strategy, or cancel the project.

A failure criterion might specify: "If after eight weeks of evaluation-driven iteration we have not achieved at least 78 percent accuracy, we will pause development for a technical review to assess whether the current model and architecture can reach the target 88 percent accuracy, or whether we need additional training data, a different model, or a fundamentally different approach."

This criterion creates a decision point. It doesn't mean the project is canceled if accuracy is 76 percent after eight weeks. It means the team stops iterating and asks hard questions. Is the gap from 76 percent to 88 percent achievable through prompt engineering and prompt refinement, or does it require fundamentally different capabilities? Are we seeing steady improvement that suggests we'll reach the target with more time, or are we plateaued? Do we have evidence that 88 percent is achievable with this approach, or are we hoping?

Failure criteria should cover multiple dimensions. For cost: "If we cannot reduce inference cost below 0.80 dollars per request after exploring prompt optimization, model selection, and caching strategies, we will reassess whether the business model can support higher costs or whether the feature set must be reduced to lower costs." For latency: "If 95th percentile latency remains above 4.0 seconds after performance optimization work, we will evaluate whether users will accept higher latency or whether we need to redesign the approach to achieve target latency."

Failure criteria also prevent teams from shipping inadequate quality by changing the definition of success. The criterion might specify: "The success criteria defined in this document are the requirements for general availability launch. They cannot be changed to justify earlier launch without explicit approval from product leadership, legal, and domain expert stakeholders. If these criteria prove unachievable with available resources and time, the appropriate response is to delay launch or cancel the project, not to lower the bar."

This rigidity might seem inflexible, but it prevents the common failure mode where teams realize they can't meet original criteria and retroactively declare lower quality acceptable. If the original criteria were wrong, that's a learning, and they should be changed through the same stakeholder negotiation process that created them. But they shouldn't be quietly lowered to ship something inadequate.

## Getting Stakeholder Sign-Off

Success criteria only work if everyone who has authority over the project agrees to them before development starts. This agreement cannot be informal or assumed. It must be explicit, documented, and signed off by specific individuals who can be held accountable if they later claim different expectations.

The sign-off process should involve presenting the complete success criteria document to all stakeholders in a review meeting. The document should include: the business metrics and targets, the quality metrics and measurement methodology, the operational metrics and constraints, the staged iteration plan showing version targets, and the failure criteria defining decision points. Each stakeholder should review the criteria relevant to their domain and confirm they agree these criteria define success.

Product leadership signs off that the business metrics represent real value and the targets justify the investment. Domain experts sign off that the quality targets represent professionally acceptable work and the measurement methodology is sound. Engineering leadership signs off that the targets are technically achievable with available resources and time. Finance signs off that the cost structure is economically viable. Legal and compliance sign off that the safety and risk criteria adequately protect the company and users.

This sign-off is not a formality. It's a commitment. When the team later delivers a system that meets the criteria, these stakeholders have agreed that the system is ready to ship. They cannot retroactively claim the criteria were insufficient or that they expected something different. The sign-off creates accountability and eliminates moving goalposts.

The success criteria document should be a living document that evolves, but changes require the same sign-off process as the original. If user research reveals the initial quality target was too low, the team can propose raising it, but that proposal must be reviewed and signed off by the same stakeholders. Changes cannot be unilateral. This process prevents criteria from being changed arbitrarily while allowing legitimate updates based on learning.

## What Happens When You Skip This Step

Teams that skip defining success criteria before building pay predictable costs. Development proceeds without clear direction, and engineers optimize toward their own intuitions about quality rather than shared criteria. This produces a system that matches engineering's standards but not necessarily what users need or stakeholders expected. Months of work are invested before anyone realizes the disconnect.

The launch decision becomes subjective and political. Without objective criteria, the decision to ship or continue iterating depends on who has more organizational power, not on whether the product is actually ready. Product managers feel pressured to ship before the product is ready to hit arbitrary deadlines. Engineers resist shipping because they see quality issues that others dismiss. Domain experts veto launches based on concerns that others consider edge cases. Nobody is wrong because there's no objective standard of right.

Stakeholder trust erodes because expectations are misaligned. The CEO expected a system that automates 70 percent of support tickets and sees 52 percent automation as failure. The engineering team expected 52 percent would be celebrated as an achievement given the difficulty of the problem. The domain experts expected zero critical errors and are horrified by the three critical errors found in testing. These misalignments create conflict that damages relationships and makes future collaboration harder.

The product ships with unclear quality, and users bear the cost. Without defined success criteria, teams tend to ship when they run out of time or patience, not when the product is actually ready. Users encounter errors that the team knew about but deemed acceptable without checking whether users would agree. The product launches to poor reviews or low adoption, and the team is surprised because they thought it was good enough.

All of these failures trace back to the same root cause: building before defining what you're building toward. The solution is not complicated. Write down what success looks like in specific, measurable terms. Get all stakeholders to agree those criteria represent success. Measure progress against those criteria. Ship when criteria are met. Reassess when failure criteria indicate the approach isn't working. This discipline feels like overhead when you're eager to start building. It's actually the only reliable path to building something that succeeds.

The next step is understanding how to navigate the inevitable tradeoffs between quality, cost, and speed that define every AI product development process.
