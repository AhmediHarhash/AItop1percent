# Chapter 1.8 — When Not to Use AI

This might be the most important subchapter in the entire book, because the best AI product decision you can make is sometimes not building one.

I know that's not what you want to hear. You bought a book about AI engineering. You're excited about the technology. Your board is asking for an AI strategy. But the teams that consistently win are the ones who know when AI is the wrong tool — and choose something simpler, cheaper, and more reliable instead.

---

### The Signs AI Is the Wrong Choice

**1. You can solve it with rules.**
If the problem can be solved with clear if/then logic, a lookup table, a decision tree, or a simple algorithm — do that. Rules are deterministic, testable, explainable, and free. AI is probabilistic, expensive to evaluate, hard to explain, and costs money on every query.

Example: routing customer support tickets by keyword and category. You could build an LLM classifier. Or you could build a rules engine that routes based on subject line keywords and form fields. The rules engine is faster, cheaper, more predictable, and easier to debug. Save the LLM for the cases where rules genuinely can't handle the complexity.

**2. The error cost is too high and you can't add human review.**
Some tasks have error costs so severe that even a 2% failure rate is unacceptable, and you can't put a human in the loop to catch mistakes. Automated medication dosing. Financial trading decisions. Safety-critical control systems. If wrong outputs cause irreversible harm and there's no human checkpoint, deterministic systems are safer.

This doesn't mean AI can never touch these domains — it means AI should assist humans in these domains, not replace them.

**3. You don't have the data to evaluate it.**
If you can't measure whether the AI is working, you can't safely deploy it. And measuring AI quality requires labeled data, ground truth, or at minimum a clear rubric for human reviewers. If you don't have these and can't create them, you're flying blind — and flying blind with a probabilistic system is asking for trouble.

**4. The cost doesn't make sense.**
Sometimes the math just doesn't work. If solving the problem with AI costs $5 per interaction and the value of each interaction to your business is $2, no amount of clever engineering changes the unit economics. Before you build, model the cost. If the cost per resolved task exceeds the value per resolved task, walk away.

**5. Users don't trust AI for this task.**
In some domains, users actively distrust AI — and for good reason. Legal advice, mental health support, medical diagnosis, financial planning. You can build the most accurate AI system in the world, but if users won't trust it enough to act on its outputs, it doesn't matter. Understanding user trust boundaries is as important as understanding technical capabilities.

**6. The problem is actually a process problem, not a technology problem.**
Sometimes teams want AI because their current process is broken — slow, inconsistent, error-prone. But the fix isn't AI. The fix is fixing the process: better training, clearer guidelines, simpler workflows. Adding AI to a broken process gives you a broken process that's harder to debug.

---

### The "AI Might Help Later" Category

Some problems aren't ready for AI today but might be tomorrow. These are worth parking, not killing:

- You don't have enough data yet, but you're collecting it
- The models aren't good enough yet, but they're improving rapidly
- The cost is too high now, but inference prices are dropping every quarter
- The regulatory framework isn't clear, but EU AI Act guidelines are coming

For these, set a reminder to re-evaluate in six months. Don't invest in building today, but don't close the door.

---

### How to Have This Conversation

When someone proposes an AI solution, ask these three questions:

1. **What's the simplest non-AI solution?** If one exists and it's good enough, use it.
2. **What's the cost of being wrong?** If it's very high and you can't add human review, think twice.
3. **Can we measure success?** If we can't tell whether the AI is working, we can't safely ship it.

These questions aren't anti-AI. They're pro-quality. The best AI teams are ruthlessly selective about where they apply the technology, because that focus is what lets them do it well.

---

*Next: the pilot-to-production gap — the graveyard where promising AI pilots go to die.*
