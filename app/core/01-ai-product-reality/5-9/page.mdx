# 5.9 — The Pre-Mortem: Predicting Failure Before You Ship

In October 2024, a fintech company was three days from launching an AI-powered investment advice chatbot. The team was confident. The model had passed all evaluation tests. The latency was under 500 milliseconds. The compliance review had cleared. On the Thursday before Monday's launch, the Head of Engineering suggested running a pre-mortem — a structured exercise where the team imagines the product has failed catastrophically and works backward to identify what went wrong. The PM resisted. "We've tested everything. We're ready. Let's not invent problems." The engineering lead insisted. They spent two hours in a conference room. Twenty minutes into the exercise, the data engineer said: "Our evaluation dataset doesn't include any examples where users ask about multiple accounts in one question, and our biggest customers all have multiple accounts." They tested it. The chatbot failed 40 percent of multi-account queries, giving advice based on only one account while ignoring the others. The pattern was invisible in their evaluation set because every test case was single-account. They delayed launch by two weeks, rebuilt the evaluation dataset, and fixed the handling of complex queries. The two-hour pre-mortem saved them from a launch disaster that would have affected their largest and most valuable customers.

A post-mortem is what you do after something goes wrong. You analyze the failure, identify root causes, and implement fixes. A pre-mortem is what you do before launch — you deliberately imagine that your product has failed spectacularly, and you work backward to identify all the ways it could happen. Pre-mortems are valuable for all software. For AI products, they're essential. AI failures are less predictable than traditional software failures, they often emerge only at scale, and the consequences are often worse because users trust AI outputs in ways they don't trust traditional software outputs.

## Why Pre-Mortems Work When Other Planning Exercises Don't

Most planning exercises ask teams to think about risks and mitigation strategies in the abstract. "What could go wrong?" The answers are generic: the model could have bugs, users might not like it, it might be too slow. These abstract risks don't generate useful preparation. Pre-mortems work differently. They ask the team to imagine a specific future where the product has definitively failed, and they ask for concrete explanations of what happened. The specificity forces deeper thinking.

Research on prospective hindsight shows that imagining an event has already occurred increases the ability to identify reasons for that event by 30 percent or more. When you ask "what could go wrong," people generate possibilities. When you ask "it has gone wrong, what happened," people generate explanations. Explanations are more concrete, more detailed, and more useful. A possibility is "the model might hallucinate." An explanation is "the model hallucinated because our evaluation dataset didn't include questions about products we launched after we created the dataset, so the model filled in details from older products and users made purchase decisions based on wrong information."

Pre-mortems also create psychological safety for dissent. In most teams approaching launch, there's social pressure to be positive and confident. Raising concerns feels like being negative or blocking progress. The pre-mortem explicitly asks everyone to imagine failure, which removes the stigma from identifying problems. The most junior person on the team can raise a concern about data quality without appearing to question the senior ML engineer's work, because the exercise is designed for everyone to surface problems. The format creates permission to be critical.

## How to Run an AI Product Pre-Mortem

The mechanics are simple. The discipline is hard. You need two hours of uninterrupted time with everyone who's been involved in building the product. That means Product, Engineering, ML, Data, and anyone from Design, Legal, Compliance, or Operations who has context. If your product has domain experts involved, include them. Everyone needs to be in the same room or on the same video call. No multitasking. No laptops open for "just checking something quickly." The exercise requires focus.

**Step one** is setting the scene. The facilitator — usually the PM or Engineering Lead — says: "It's six months from now. We launched this product. It failed catastrophically. It's in the news. Customers are furious. Our executives are demanding explanations. Some of us are in meetings defending what went wrong. The product is either shut down or severely restricted. What happened?" The framing needs to be vivid and specific. Not "imagine some problems." Imagine a disaster. The severity of the imagined failure determines the depth of the analysis.

**Step two** is silent brainstorming. Give everyone ten minutes to write down every failure scenario they can imagine. They should write independently, not discuss. The goal is to generate scenarios without groupthink or anchoring on the first idea someone mentions. They should write specific scenarios with concrete details: what failed, how it manifested, who was affected, what the consequences were. "The model was inaccurate" is too vague. "The model recommended the wrong medication dosage for elderly patients because our evaluation dataset skewed young, and three patients were hospitalized before we caught it" is specific enough to be useful.

**Step three** is sharing and clustering. Go around the room. Each person shares their scenarios. Someone takes notes on a whiteboard or shared document. As patterns emerge, cluster similar scenarios together. "Model degraded after provider API update" and "Model behavior changed when we switched from GPT-5 to GPT-5.1" are the same underlying failure mode: dependency on external model behavior. You'll typically end up with 15 to 25 distinct failure scenarios after clustering. If you have fewer than ten, your team isn't thinking broadly enough. If you have more than 30, some of your clusters are too granular and should be combined.

**Step four** is prioritization. For each scenario, the team estimates likelihood — high, medium, or low — and impact — high, medium, or low. Don't spend time debating precise probabilities. Rough estimates are sufficient. The goal is to identify the scenarios that are both likely and impactful, because those are the ones that need immediate mitigation. High-likelihood, high-impact scenarios get addressed before launch. High-impact, low-likelihood scenarios get monitoring plans so you can detect them quickly if they occur. Low-impact scenarios get acknowledged and deprioritized.

**Step five** is mitigation. For each high-priority scenario, ask: what can we do right now to prevent this, or reduce its likelihood, or reduce its impact if it happens? Assign an owner and a deadline. Some mitigations are features you need to build. Some are tests you need to run. Some are monitoring you need to add. Some are documentation you need to write so the on-call engineer knows what to do if the scenario occurs. The output of this step is a concrete list of action items that will make your launch safer.

## The AI-Specific Failure Scenarios That Emerge in Every Pre-Mortem

After facilitating pre-mortems for dozens of AI product teams, certain categories of failure scenarios appear consistently. These are the patterns that every AI team should consider, regardless of what they're building.

**Model failure scenarios** are the most obvious category. The model hallucinates confidently, and users trust the hallucination. The model gives correct information that's harmful in context — correct medication dosing for the wrong patient population, correct legal precedent from the wrong jurisdiction, correct investment advice for the wrong risk tolerance. The model degrades silently after an upstream provider updates their model, and nobody notices for weeks because the degradation is gradual. The model behaves differently for different demographic groups, producing better results for some users than others, and the evaluation set didn't catch it because it wasn't stratified by demographics.

**Data failure scenarios** reflect the reality that most AI failures are data failures, not model failures. The evaluation dataset doesn't represent real production inputs, so the accuracy numbers are misleading. The knowledge base contains outdated information, and the AI retrieves and presents it as current. Customer data leaks into model outputs because logging captured more than it should have, or because the model memorized training data, or because the RAG system retrieved documents it shouldn't have access to. The team discovers PII in the training or evaluation data after launch, creating a privacy incident.

**Scale failure scenarios** are the ones that only manifest under production load. Costs explode when traffic volume is 10x or 100x what you tested at. Latency degrades under load because your infrastructure was sized for average traffic, not peak traffic. Rate limits from your model provider throttle your requests, causing user-facing errors. A viral event sends 100x normal traffic, and your system falls over. These scenarios are predictable but frequently overlooked because teams test at small scale and assume production scale will be "similar but more."

**User behavior failure scenarios** reflect the reality that real users don't behave like test users. Users trust the AI too much and make bad decisions based on incorrect outputs. Users find adversarial prompts that make the AI say things you never intended — offensive content, confidential information, nonsense that looks plausible. Users use the product for purposes you didn't design for, and it fails in ways you didn't anticipate. A bad AI interaction gets screenshot and shared on social media, going viral and creating a PR crisis. These scenarios are hard to predict because user creativity is unbounded, but the category is predictable: users will do unexpected things.

**Operational failure scenarios** are about your team's ability to respond when something goes wrong. Nobody notices quality degradation for two weeks because your monitoring doesn't effectively measure quality, only latency and error rates. The on-call engineer gets paged for an AI quality issue and doesn't know how to diagnose it because AI debugging is different from traditional software debugging. A prompt change gets deployed without proper testing and causes regressions, but you don't have tooling to quickly roll back prompts the way you can roll back code. The escalation path to human reviewers breaks under load, so users get stuck with AI responses even when they need human help.

**Regulatory and compliance failure scenarios** are especially important for regulated industries. You expand into a new market and discover you're non-compliant with regulations you didn't know existed. A regulation you thought didn't apply to you turns out to apply, and you're not in compliance. A customer's legal or compliance team audits your product and finds data handling practices that violate their policies or their regulatory obligations. A regulator asks for audit evidence you don't have because you didn't document your evaluation, validation, and monitoring processes.

**Team and organizational failure scenarios** reflect human and process risks. The one person who deeply understands the evaluation framework leaves the company, and nobody else knows how to maintain it. ML and Product can't agree on quality thresholds, and the debate delays launch or results in shipping a product that doesn't meet one team's standards. Stakeholders override the team's quality assessment for business reasons — "we need to launch for the board meeting" — and the team ships knowing the product isn't ready. The team doesn't have capacity to respond to post-launch issues because everyone moved on to the next project immediately after launch.

## The Pre-Mortem Outputs You Need Before Launch

The pre-mortem exercise produces three critical artifacts that need to be documented and shared before launch. These aren't bureaucratic paperwork. These are operational tools that your team will use when things go wrong.

**The risk register** is a document that lists every identified failure scenario, its assessed likelihood and impact, the mitigation strategy, and the owner. For risks you've mitigated, it documents what you did. For risks you've accepted, it documents why you accepted them and what the trigger would be for revisiting that decision. For risks you're monitoring, it documents what metrics you're watching and what thresholds would indicate the risk is materializing. The risk register is a living document that gets updated as you learn more, but it needs to exist before launch so everyone knows what risks you're tracking.

**The monitoring plan** documents what you're watching for after launch. For each major failure scenario, what metric or signal would indicate it's happening? Who's responsible for watching that metric? What threshold triggers investigation versus immediate action? How do you detect model degradation, data quality issues, cost spikes, latency problems, user dissatisfaction, or regulatory concerns? Most teams launch with infrastructure monitoring — CPU, memory, error rates, latency — but without AI-specific monitoring. The pre-mortem should generate a list of AI-specific things to monitor: output quality metrics, user feedback patterns, cost per query, retrieval accuracy for RAG systems, and domain-specific measures that matter for your product.

**The decision criteria document** answers the question: what would make us roll back, pause, or shut down? If you wait until you're in a crisis to have this conversation, the debate will be heated and pressured. Sales will want to keep going. Engineering will want to pause and fix. Legal will have opinions about liability. The CEO will have opinions about the board. By documenting decision criteria in advance, you pre-make the decision. If output quality drops below X, we roll back. If user complaints exceed Y, we pause and investigate. If we detect a privacy leak, we shut down immediately. These criteria need to be specific and measurable, and they need to be agreed upon by leadership before launch so there's no ambiguity when the situation occurs.

## Why Most Teams Skip Pre-Mortems and Why That's a Mistake

Pre-mortems feel pessimistic. Teams approaching launch are excited and optimistic. You've built something that works. You've passed tests. You've gotten approvals. The energy is "let's ship this" not "let's imagine how it fails." Suggesting a pre-mortem can feel like questioning the team's work or expressing lack of confidence. PMs worry it will demoralize the team. Engineering leads worry it will delay launch. Executives worry it will create risk aversion.

These concerns are backwards. Pre-mortems don't create risk aversion. They create informed risk-taking. The team that runs a pre-mortem isn't more hesitant to launch. They're more confident because they've thought through the risks and either mitigated them or prepared to respond to them. They launch knowing what they're watching for. The team that skips the pre-mortem launches blind, and when something goes wrong — and something will go wrong — they're improvising their response under pressure.

The time investment is trivial relative to the value. Two hours before launch versus 20 hours responding to an incident you didn't prepare for. Two hours versus the customer trust you lose when users encounter a failure you could have predicted. Two hours versus the revenue impact of a launch delay caused by a problem you discover too late to fix easily. The teams that ship reliable AI products aren't lucky. They're prepared. They've thought through failure modes systematically, and they've built systems that either prevent those failures or detect and respond to them quickly.

## The Relationship Between Pre-Mortems and Evaluation Strategy

A well-run pre-mortem reveals gaps in your evaluation strategy. The failure scenarios your team identifies should map to test cases in your evaluation dataset. If you identify "the model might give advice that's correct in general but wrong for our specific user context" as a failure scenario, your evaluation set needs test cases that check for context-awareness. If you identify "the model might degrade when the provider updates their model" as a risk, you need a process for re-running evaluations when upstream changes occur.

The most valuable outcome of a pre-mortem is often the realization that your testing has blind spots. The fintech company at the beginning of this chapter discovered through their pre-mortem that they had no test cases for multi-account queries. That realization led to a two-week delay, but the alternative was launching with a systematic blind spot that would have affected their most important customers. The delay was a feature, not a bug. The pre-mortem's purpose is to find problems while they're still cheap to fix.

Some teams run pre-mortems, identify gaps, and then launch anyway because of timeline pressure. This is a mistake. If the pre-mortem identifies a high-likelihood, high-impact risk that you haven't mitigated, you're not ready to launch. The purpose of the exercise is not to document risks you'll ignore. The purpose is to identify risks so you can address them. If leadership decides to launch despite unmitigated risks identified in the pre-mortem, that decision needs to be explicit and documented. "We're launching knowing that X is a risk because of Y business justification." That way, when X happens, nobody's surprised, and the response plan exists.

## Pre-Mortems as Cultural Practice

The best AI teams run pre-mortems not just before launches but before major changes. Before switching model providers. Before deploying a significant prompt update. Before expanding into a new use case or market. Before making architectural changes to the evaluation pipeline. The pre-mortem format becomes a standard part of the planning process, not a special event reserved for launches.

This practice creates a culture where identifying risks is valued, not punished. Where the most useful team member is not the one who's most optimistic but the one who thinks most clearly about what could go wrong. Where "what if this fails" is not a sign of negativity but a sign of professionalism. AI products fail in surprising ways, and the teams that ship reliable AI products are the ones who are systematically unsurprised. They've thought through the failure modes. They've tested for them. They've built monitoring to detect them. And when something does go wrong — and it will — they have a plan.

Pre-mortems are not about pessimism. They're about realism. Optimists believe everything will work. Pessimists believe everything will fail. Realists believe some things will work and some things will fail, and they prepare for both. The best AI teams aren't optimists or pessimists. They're realists who plan for success and prepare for failure.

---

*Next: AI threat modeling — because in 2026, your AI is an attack surface.*
