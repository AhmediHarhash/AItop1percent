# Chapter 5.9 — The Pre-Mortem: Predicting Failure Before You Ship

A post-mortem is what you do after something goes wrong. A pre-mortem is what you do before — deliberately imagining everything that could go wrong, so you can prevent it.

Every AI product should run a pre-mortem before launch. It takes two hours. It prevents incidents that cost weeks.

---

### How to Run a Pre-Mortem

**Step 1: Gather the team.** PM, ML engineer, software engineer, domain expert (if available), and anyone else involved in the product. Everyone in one room (or one call).

**Step 2: Set the scene.** "It's six months from now. Our AI product has failed spectacularly. It's in the news. Customers are furious. Leadership is asking what went wrong. What happened?"

**Step 3: Brainstorm failures.** Give everyone 10 minutes to silently write down every failure scenario they can imagine. Not just technical failures — business failures, trust failures, operational failures, regulatory failures.

**Step 4: Share and cluster.** Go around the room. Each person shares their scenarios. Cluster similar ones. You'll typically end up with 15-25 distinct failure scenarios.

**Step 5: Prioritize.** For each scenario, rate: likelihood (high/medium/low) and impact (high/medium/low). Focus on the high-likelihood and high-impact scenarios.

**Step 6: Mitigate.** For each high-priority scenario, define: what can we do now to prevent it or reduce its impact? Assign owners and deadlines.

---

### The AI Pre-Mortem Failure Categories

Here are the categories that consistently come up in AI product pre-mortems:

**Model failures.**
- The model hallucinates confidently and users believe it
- The model gives correct but harmful advice (e.g., correct medication dose for the wrong patient type)
- The model degrades after a provider update we didn't test for
- The model behaves differently for different demographic groups

**Data failures.**
- Our evaluation data doesn't represent real production inputs
- Our knowledge base contains outdated or incorrect information
- Customer data leaks into model outputs
- We discover PII in our training or eval data

**Scale failures.**
- Cost explodes when we hit real traffic volume
- Latency degrades under load
- Rate limits from the model provider throttle our service
- A viral event sends 100x normal traffic

**User behavior failures.**
- Users trust the AI too much and make bad decisions based on wrong outputs
- Users find ways to make the AI say things we never intended
- Users use the product for purposes we didn't design for
- A bad AI interaction goes viral on social media

**Operational failures.**
- Nobody notices quality degradation for two weeks
- The on-call engineer doesn't know how to roll back a prompt change
- The escalation path to human agents breaks during peak hours
- The monitoring system itself has a bug and misses an alert

**Regulatory failures.**
- We enter a new market and discover we're non-compliant
- A regulation we didn't know about applies to our product
- A customer's legal team finds something in our data handling that violates their requirements
- We're required to produce audit evidence we don't have

**Team failures.**
- The one person who understands the eval framework leaves
- ML and PM can't agree on quality thresholds and the debate delays launch
- Stakeholders override the team's quality assessment for business reasons

---

### The Pre-Mortem Output

Your pre-mortem should produce a document with three sections:

**Known risks.** The scenarios we've identified, their likelihood and impact, and our mitigation plan.

**Monitoring commitments.** What we're watching for, how we'll detect each failure mode, and who's responsible.

**Decision criteria.** What would trigger a rollback, a pause, or a shutdown? Define these before they're needed so the decision is pre-made, not debated during a crisis.

---

### Why Most Teams Skip This

Pre-mortems feel pessimistic. Teams are excited to launch. Nobody wants to spend two hours imagining failure. But the two hours you spend in a pre-mortem save you the twenty hours you'd spend in a real incident — plus the customer trust, revenue, and reputation you'd lose.

The best AI teams aren't optimists or pessimists. They're realists. They plan for success and prepare for failure.

*Next: AI threat modeling — because in 2026, your AI is an attack surface.*
