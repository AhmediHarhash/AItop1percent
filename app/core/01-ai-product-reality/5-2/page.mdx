# Chapter 5.2 — Writing AI Product Requirements That Work

Now that we've covered why traditional PRDs fail, let's build requirements that actually work. The goal isn't documentation for documentation's sake — it's a shared understanding between everyone on the team about what we're building, what "good" means, and where the boundaries are.

---

### Start with the User's Job-to-Be-Done

Before you write a single requirement, answer: what is the user trying to accomplish, and how will they know the AI helped?

Bad requirement: "Build an AI chatbot for customer support."
Good requirement: "Help customers resolve return and exchange questions without waiting for a human agent. Success: 70% of return/exchange queries are resolved by the AI without human escalation, with 90% accuracy on policy information."

The difference: the good requirement tells the team exactly what success looks like. The bad requirement tells them what to build but not how to know if it's working.

---

### The Behavior Specification Framework

For each AI behavior your product needs, specify four things:

**1. The intent.** What is the user trying to do?
Example: "The user wants to know if they can return a product purchased 45 days ago."

**2. The ideal response.** What should the AI do in the best case?
Example: "Accurately state the return policy (30-day window), inform the user they're outside the window, and offer alternative options (exchange, store credit)."

**3. The minimum acceptable response.** What's the lowest quality response that's still acceptable?
Example: "Correctly states the 30-day return window. May not offer alternatives but doesn't give incorrect information."

**4. The unacceptable response.** What must the AI never do?
Example: "Never tell the user they can return a product outside the return window. Never make up a policy that doesn't exist. Never be rude or dismissive."

This four-part spec is more useful than a hundred-page PRD because it gives the ML team a clear target, a minimum bar, and hard boundaries. It also directly translates into evaluation criteria.

---

### Writing Requirements for Uncertainty

AI products have a category of behavior that traditional products don't: uncertain behavior. The system doesn't know the answer, isn't confident, or encounters an input it wasn't designed for.

You need to specify what happens in these cases:

**Low confidence behavior.** "When the system's confidence is below 70%, it should say: 'I'm not sure about that. Let me connect you with a team member who can help.' It should NOT attempt to answer with low confidence."

**Out-of-scope behavior.** "When the user asks about topics outside our product domain (e.g., general knowledge, personal advice), the system should politely redirect: 'I can help with questions about [product/service]. For other questions, I'd recommend [alternative].' It should NOT attempt to answer out-of-scope questions."

**Error behavior.** "When the system encounters a technical error (API timeout, malformed response), it should say: 'I'm having trouble right now. Please try again in a moment, or contact us at [support channel].' It should NOT show technical error messages to the user."

These uncertainty requirements are often the difference between a product that feels polished and one that feels broken.

---

### Requirements for Edge Cases

You can't enumerate every edge case, but you can categorize them:

**Length extremes.** What happens with very short inputs ("hi") and very long inputs (10,000 words)?

**Language and format.** What languages do you support? What about mixed-language input? What about emoji, special characters, or code?

**Adversarial input.** What happens when someone tries to jailbreak, prompt inject, or abuse the system?

**Sensitive topics.** How should the system handle questions about self-harm, illegal activities, or medical emergencies?

**Multi-turn complexity.** What happens in very long conversations? When the user contradicts themselves? When they change the topic?

For each category, define the policy. You don't need to enumerate every specific case — you need to define the principle that guides the system's behavior.

---

### The Living Document

AI requirements aren't finished when you ship V1. They evolve:

- New failure modes discovered in production get added as unacceptable behaviors
- User feedback reveals new intents that need to be covered
- Quality metrics reveal areas where the minimum bar needs to be raised
- Regulatory changes impose new constraints

Review and update your requirements monthly. A requirements document that's six months old and hasn't been updated isn't a requirements document — it's historical fiction.

*Next: defining success criteria before you build — the single most impactful practice in AI product development.*
