# 5.2 â€” Writing AI Product Requirements That Work

In mid-2024, a financial services company launched an internal tool that used AI to generate first-draft responses to customer complaints. The product manager had written a detailed requirements document that specified the input format, the output structure, the user interface, and the workflow integration. Engineering delivered everything in the spec on time. The tool technically worked. But when the compliance team reviewed it before broader rollout, they found the AI was occasionally inventing account details, citing policies that didn't exist, and making commitments the company couldn't honor. The PRD had described what information the AI should include in responses but never specified how accurate that information needed to be, what the system should do when information was unavailable, or what outputs were legally unacceptable. The project was paused for three months while the team retrofitted evaluation frameworks, safety filters, and accuracy requirements they should have defined before development started.

The problem wasn't that the original requirements were wrong. The problem was that they were incomplete in ways specific to AI products. They described desired features but not quality standards. They specified happy-path behavior but not uncertainty handling or failure modes. They treated the AI as a deterministic component when it was fundamentally probabilistic. Writing requirements that work for AI means acknowledging this probabilistic nature upfront and building the spec around it.

## Start with Measurable Outcomes, Not Features

Traditional requirements often start with features: "The system will have a chatbot that answers customer questions." This describes what you're building but not why it matters or how you'll know it works. For AI products, this feature-first approach leads to building systems that technically implement the spec but don't deliver value because nobody defined what value meant.

Start instead with the outcome you're trying to achieve and how you'll measure it. For a customer support chatbot, the outcome-based requirement might be: "Reduce average time to resolution for common customer questions about returns, shipping, and account access by 40 percent, while maintaining customer satisfaction scores above 4.2 out of 5.0. Enable 65 percent of these common questions to be fully resolved without human agent involvement."

This requirement defines success in measurable business terms. The team knows exactly what they're optimizing for: faster resolution, maintained satisfaction, and reduced need for human escalation. When they make tradeoff decisions during development, they have clear guidance. A response that's faster but reduces satisfaction below 4.2 doesn't meet requirements. A response that increases resolution rate to 70 percent but only for uncommon questions doesn't meet requirements. The outcome specification creates clarity.

Outcome-based requirements also force necessary conversations upfront. How do you measure time to resolution? How do you calculate customer satisfaction? What counts as a common question? What counts as fully resolved? These questions surface ambiguities that feature-based requirements hide. The team has to align on definitions before building, which prevents the alignment failures that sink AI projects.

The outcome requirement should specify the primary metric, the target value, and the constraints within which that target must be achieved. For a document extraction tool, this might be: "Achieve 94 percent field-level accuracy on invoices, purchase orders, and receipts, with processing time under two seconds per document and cost under 0.08 dollars per document. Field-level accuracy is measured as the percentage of individual fields extracted with values matching ground truth in both content and data type."

This requirement combines the primary metric (field-level accuracy at 94 percent), operational constraints (time and cost), and explicit definition of the measurement methodology. Engineering can build toward this target. Finance can evaluate whether the cost structure is viable. Product can assess whether 94 percent accuracy is enough for the intended use case. Everyone is aligned because the requirement is specific and measurable.

## Define Quality as a Spectrum, Not a Binary

AI quality is not pass-fail. It exists on a spectrum from perfect to unacceptable, with many gradations in between. Your requirements need to define where on that spectrum different quality levels fall and what system behavior corresponds to each level.

For a content moderation AI, quality levels might be specified as: "Excellent performance: detects 96 percent or more of policy-violating content with false positive rate below 2 percent. Acceptable performance: detects 91 percent or more of policy-violating content with false positive rate below 5 percent. Unacceptable performance: detects less than 91 percent of policy-violating content or false positive rate exceeds 5 percent."

This spectrum does three things. First, it defines the minimum acceptable quality bar: below 91 percent detection or above 5 percent false positives, the system doesn't ship. Second, it defines the aspirational target: 96 percent detection with 2 percent false positives is the goal, but the team can ship at the acceptable level and iterate toward excellent. Third, it acknowledges that quality has multiple dimensions: both detection rate and false positive rate matter, and you cannot optimize one at the expense of the other beyond defined thresholds.

Quality spectrum requirements prevent two common pathologies. They prevent perfectionism: teams don't refuse to ship because they haven't hit 99 percent accuracy when 91 percent is acceptable for the use case. They also prevent complacency: teams don't ship at 85 percent accuracy and call it done when 91 percent is the defined minimum. The spectrum creates clear guardrails.

For each quality dimension that matters to your product, specify the measurement methodology, the acceptable range, and the target range. Quality dimensions might include accuracy, precision, recall, latency, cost per request, user satisfaction, safety violation rate, or task completion rate. The specific dimensions depend on your product, but the requirement structure is the same: define what you're measuring, how you're measuring it, what's acceptable, and what's excellent.

## Specify Behavior for Uncertainty and Edge Cases

AI systems encounter three categories of challenging inputs: low confidence situations where the system is uncertain about the correct output, out-of-scope situations where the input is outside the system's intended domain, and adversarial situations where the user is actively trying to break or abuse the system. Traditional PRDs don't specify behavior for these categories because traditional software doesn't have uncertain states. AI requirements must.

For uncertainty, specify the confidence threshold and the corresponding behavior. A content generation AI might have requirements like: "When the system's internal confidence score for a generated response is below 0.75, do not show the response to the user. Instead, display: We're not confident we can provide a good answer to this query. Would you like to rephrase your question or contact our support team? When confidence is between 0.75 and 0.85, show the response with a header: Here's what we found, though we're not entirely certain this fully answers your question."

This requirement defines two confidence thresholds and specifies exact behavior for each range. Engineering knows what to implement. Users get appropriate signals about output reliability. Product knows what user experience to expect. The specification removes ambiguity about a category of behavior that's critical to user trust but absent from traditional requirements.

For out-of-scope inputs, specify both detection criteria and response behavior. A customer support AI might require: "When the user's query is not related to product features, account management, billing, or technical support (the defined scope), the system should respond: I'm designed to help with questions about your account and our product. For other topics, I'd recommend consulting appropriate resources. The system should not attempt to answer general knowledge questions, provide personal advice, or discuss topics unrelated to our product domain."

This requirement defines the system's scope boundaries and the behavior when inputs fall outside those boundaries. It prevents scope creep where the AI attempts to handle requests it wasn't designed for. It also prevents user frustration when the AI refuses to help without explanation. The defined response acknowledges the limitation politely while redirecting the user.

For adversarial inputs, specify detection mechanisms and defensive responses. A chatbot might require: "When the system detects attempts to manipulate its behavior through prompt injection, jailbreaking, or other adversarial techniques, it should terminate the conversation and log the attempt for review. Detection mechanisms include pattern matching for common jailbreak phrases, anomaly detection for unusual conversation structures, and content filters for requests to ignore previous instructions or adopt different personas."

This requirement acknowledges that users will attempt to abuse the system and defines how the system should respond. It specifies both the detection approach and the action to take. It also implies an evaluation requirement: the team needs to test the system against known adversarial techniques to verify the defenses work.

## Use Examples to Define Expected Behavior

Prose descriptions of AI behavior are inherently ambiguous. Two people can read the same requirement and imagine completely different system outputs. Examples eliminate this ambiguity. For every major behavior your AI needs to exhibit, provide concrete input-output pairs that illustrate expected behavior.

A requirement for a summarization tool should include examples like: "Input: A 3,000-word research article about climate change impacts on agriculture. Expected output: A 200-word summary that identifies the main finding, the methodology, the geographic scope, and the key implications for policymakers, written in plain language accessible to non-experts. The summary should not include speculation beyond what's in the original article and should attribute findings to the researchers."

This example communicates more about desired behavior than paragraphs of prose ever could. It shows the expected compression ratio, the content structure, the tone and language level, and the boundaries around attribution and speculation. An engineer reading this example understands what good looks like. A domain expert can verify whether the example matches their expectations. A QA engineer can use it as a test case.

Provide 20 to 30 representative examples covering the range of inputs your system will encounter. Include examples of excellent outputs, minimally acceptable outputs, and unacceptable outputs. For each example, annotate why it's excellent, acceptable, or unacceptable. These annotations teach the team your quality standards more effectively than abstract quality definitions.

Examples should also cover edge cases and boundary conditions. Include examples of very short inputs, very long inputs, ambiguous inputs, multi-part inputs, inputs in unexpected formats, and inputs that are adjacent to but outside your defined scope. For each, specify the expected behavior. These examples become your edge case test suite and ensure the team thinks through unusual scenarios before encountering them in production.

The example set is not static documentation. As you discover new failure modes in testing or production, add examples covering those scenarios to the requirements. As user patterns shift, update examples to reflect new common input types. The example set is a living specification of system behavior that evolves with your understanding of the problem space.

## Specify Constraints as Hard Requirements

Cost, latency, data availability, and compliance constraints are not nice-to-have features. They're hard requirements that determine system viability. If your AI product costs 5 dollars per request to operate but your business model assumes 0.20 dollars per request, the product doesn't work regardless of quality. If latency is eight seconds but users abandon after three seconds, the product doesn't work. If the system requires proprietary training data you don't have legal rights to use, the product doesn't work. These constraints belong in the requirements document as blocking criteria.

Specify cost constraints as maximum acceptable cost per request, per user, or per time period, depending on your business model. For a customer support chatbot, this might be: "Total cost of AI inference, including model API calls and any supplementary services, must not exceed 0.12 dollars per conversation. A conversation is defined as a single customer session from initiation to resolution or handoff to human agent."

This requirement forces the team to design within economic constraints from the start. They know they cannot use an approach that costs 0.50 dollars per conversation, even if it achieves higher quality. They must find techniques that hit quality targets within the cost budget. This prevents the common failure mode where engineering optimizes purely for quality, builds something economically nonviable, and discovers late in development that the approach cannot scale.

Specify latency constraints as maximum acceptable response time under defined load conditions. For a document processing API, this might be: "95th percentile response time must be under 2.5 seconds for documents up to 50 pages when processing up to 100 concurrent requests. 99th percentile response time must be under 4.0 seconds under the same conditions."

This requirement defines both the target latency and the load conditions under which it must be met. It uses percentiles rather than averages to prevent the team from hitting the target in common cases while allowing unacceptable latency for outliers. It acknowledges that latency varies with input characteristics (document length) and system load, and specifies acceptable variation.

Specify data requirements as explicit prerequisites. For a recommendation system, this might be: "The system requires a minimum of 10,000 historical user-item interaction events for training, with interactions spanning at least 12 months to capture seasonal patterns. Interaction data must include timestamps, user identifiers, item identifiers, and interaction type (view, purchase, return). All data must be collected in compliance with GDPR consent requirements."

This requirement defines what data is needed, in what volume, with what characteristics, and with what legal constraints. It makes data availability a prerequisite for development. If the data doesn't exist or isn't legally usable, the team knows before building that they need to pause and solve the data problem first.

## Define Iteration Milestones and Exit Criteria

AI products improve through iteration. Your requirements should acknowledge this by defining quality targets for each version and exit criteria for deciding when to move from one version to the next. This prevents both premature shipping of inadequate systems and endless iteration toward unachievable perfection.

A version milestone requirement might specify: "Version 1.0 targets 82 percent accuracy on the defined evaluation set, with zero critical safety failures and latency under three seconds. This version is sufficient for beta launch to internal users. Version 2.0 targets 89 percent accuracy with the same safety and latency requirements. This version is sufficient for general availability to all customers. Version 3.0 targets 94 percent accuracy and latency under two seconds. This version is sufficient for expansion to premium tier customers and regulated use cases."

This staged requirement sets clear expectations for what each version achieves and when each version is appropriate to deploy. It prevents arguments about whether the system is ready to ship by defining ready for different contexts. It also creates a concrete improvement path: the team knows that getting from 82 percent to 89 percent accuracy is the primary goal for the version 2.0 development cycle.

Exit criteria should also include failure conditions: thresholds that indicate the current approach is not working and the team should reassess rather than continuing to iterate. A failure exit criterion might be: "If after six weeks of prompt engineering and evaluation-driven iteration, accuracy remains below 75 percent, pause development and reassess whether the current model and approach can achieve the required quality, or whether a different architecture or additional training data is needed."

This criterion prevents zombie projects that limp along indefinitely without meeting minimum standards. It forces the team to confront when an approach isn't working and make a decision: pivot to a different technique, invest in additional resources like training data, or cancel the project rather than continuing to invest in a failing approach.

## Make Requirements Auditable and Measurable

Every requirement you write should be verifiable through measurement or testing. Vague requirements like "the system should provide helpful responses" are not requirements because you cannot objectively determine whether they're met. Specific requirements like "the system should achieve a helpfulness score of 4.2 or higher on a 5-point scale as rated by domain experts reviewing a random sample of 200 responses" are requirements because you can measure whether the system meets them.

For each requirement, specify the measurement methodology. How will you calculate the metric? What data will you use? Who will evaluate it? What sample size is needed for statistical validity? A requirement without a measurement methodology is an aspiration, not a requirement. The measurement methodology should be detailed enough that two different people following it would produce the same result.

For subjective quality dimensions like helpfulness, tone, or professionalism, specify the evaluation rubric. What criteria do evaluators use to assign scores? What anchor examples define different score levels? How do you ensure consistency across multiple evaluators? Subjective evaluation is valid for AI products, but it must be structured and consistent. The requirement should specify the structure.

Requirements should also specify who has authority to verify them. For accuracy requirements, domain experts might be the authority. For latency requirements, engineering might verify through performance testing. For cost requirements, finance might verify through actual usage data. Specifying the verification authority prevents disputes about whether requirements are met and ensures the right expertise evaluates each dimension.

The goal is a requirements document where every claim can be tested, every target can be measured, and every decision about whether to ship can be based on objective data rather than subjective opinion. This level of specificity requires more work upfront than writing vague traditional requirements. But it eliminates weeks of confusion and argument during development and makes the launch decision straightforward rather than contentious.

AI products fail most often not from technical impossibility but from misalignment between what was built and what was actually needed. Writing requirements that work for AI means being explicit about quality distributions, uncertainty handling, constraints, measurement methodologies, and iteration plans. It means replacing the illusion of deterministic behavior with honest acknowledgment of probabilistic reality. The teams that do this work upfront build products that ship. The teams that skip it build products that linger in perpetual beta or launch to user disappointment.

The next step is translating these requirements into specific success criteria that the entire team agrees on before any development work begins.
