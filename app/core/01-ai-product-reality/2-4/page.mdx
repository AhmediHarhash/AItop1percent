# 2.4 â€” Extraction and Structuring Products

In November 2024, a mid-sized insurance company launched what seemed like a straightforward AI project. They had been manually processing 15,000 insurance claims per month, with paralegals spending an average of twelve minutes per document extracting key information: policy numbers, claim amounts, dates of loss, injury descriptions, and supporting evidence references. The math was simple. Twelve minutes times 15,000 claims times a loaded hourly rate of seventy-eight dollars meant they were spending nearly a quarter million dollars per month on pure data extraction. An AI vendor promised to automate 95% of the work at a fraction of the cost. The pilot went beautifully. On a test set of 500 carefully selected claims, the system achieved 97% accuracy. The company signed a contract, integrated the system into their production workflow, and redirected most of their paralegal team to higher-value work. Three months later, they discovered the system had been systematically misreading dollar amounts on claims that used certain PDF formats, underestimating losses by an average of 30% on approximately 8% of all claims processed. The financial exposure exceeded two million dollars. The reputational damage was worse.

The root cause was not a bug in the model. The root cause was a fundamental misunderstanding of what extraction products are and how they fail. Extraction looks like a solved problem because it works beautifully on clean, consistent inputs in controlled tests. But production data is never clean and never consistent. Document formats drift over time as vendors update their systems. Schema requirements evolve as business processes change. Edge cases multiply faster than anyone can catalog them. And unlike text generation where quality degrades gradually, extraction products have a binary failure mode. Either you get the right value or you don't. There is no partial credit for extracting one point four million when the actual value was one point seven million.

## What Extraction Products Actually Do

Extraction products solve a problem that exists in every organization that handles documents, communications, or unstructured data sources. You have information trapped in formats that humans can read but systems cannot process. Contracts contain critical dates and obligations buried in forty pages of legal language. Invoices list line items in tables that vary by vendor. Medical records describe patient histories in narrative form that needs to become structured clinical data. Customer support emails contain issues, products mentioned, sentiment, and implicit requests that need to route to the right team with the right priority.

Extraction products pull specific information out of this unstructured mess and convert it into structured data that downstream systems can use. Entity extraction identifies and classifies specific pieces of information scattered throughout a document. Document parsing understands the structure and layout of a document so you can extract not just text but meaning from how that text is organized. Summarization condenses long-form content into essential points while preserving the information that matters for your specific use case. Structured output generation takes free-form text and maps it to a predefined schema with typed fields, validation rules, and format requirements.

The value proposition is obvious. Humans are terrible at repetitive extraction tasks. We get bored, we make mistakes, we slow down as the day progresses. A paralegal who processes claims for six hours straight will make more errors in hour six than in hour one. An AI system processes the fifteen thousandth claim with exactly the same attention and accuracy as the first. When extraction works well, it eliminates thousands of hours of mind-numbing manual labor and converts unstructured data into the structured information that powers dashboards, analytics, compliance reporting, and automated workflows.

The challenge is that extraction products face unique constraints that other AI archetypes do not. Correctness is not subjective. If a contract specifies a termination date of March 15, 2027, and your system extracts March 15, 2026, you are not approximately right. You are wrong. Unlike text generation where quality exists on a spectrum and users can tolerate imperfection, extraction products are often used in contexts where wrong answers create concrete financial, legal, or operational consequences. You cannot tell a judge that your contract analysis system was 94% accurate so the incorrect termination date should be overlooked.

## The Accuracy Ceiling Problem

Extraction products hit an accuracy ceiling that is maddeningly difficult to push through. You start with a model that gets 85% of extractions right on your test set. You add more training examples and get to 90%. You fine-tune on domain-specific data and reach 94%. You add validation rules and prompt engineering improvements and reach 96%. And then you get stuck. The remaining 4% of errors are not random noise. They are systematic failures on specific document types, specific edge cases, specific formatting quirks that your training data did not cover and your validation logic did not anticipate.

A healthcare company building a system to extract clinical data from patient notes discovered this ceiling the hard way. Their system performed beautifully on notes from their largest hospital network, achieving 96% accuracy on a test set of ten thousand notes. But when they expanded to smaller regional clinics, accuracy dropped to 87%. The difference was not the medical content. The difference was that smaller clinics used different electronic health record systems that formatted notes differently, abbreviated drug names inconsistently, and used local conventions for recording measurements. The model had learned patterns specific to one format and failed when those patterns changed.

Pushing through the accuracy ceiling requires understanding that the last 4% of errors are qualitatively different from the first 15%. Early errors are low-hanging fruit caused by insufficient model capacity, inadequate prompts, or obvious training data gaps. Late-stage errors are edge cases. Documents that use non-standard formatting. Text that contains ambiguous references or conflicting information. Schemas that require domain knowledge to populate correctly. These errors do not disappear with more data or better models. They disappear when you change your product strategy.

Some organizations solve the accuracy ceiling problem by adding a human review layer for low-confidence extractions. The system processes everything but flags cases where confidence scores fall below a threshold, and humans review only the flagged subset. This works when the percentage of flagged cases is small enough that human review is economically viable and when you can reliably detect low-confidence cases before they cause damage. Other organizations solve it by narrowing scope. Instead of extracting from any invoice format, they extract only from invoices that match a pre-approved list of vendor templates. This reduces accuracy pressure by eliminating the long tail of edge cases, but it also reduces the value of automation.

The accuracy ceiling is not a technical problem that better models will solve. It is a product design problem. You must decide what accuracy level your use case actually requires, what you will do with cases that fall below that threshold, and how much complexity you are willing to accept to handle edge cases. A system that achieves 96% accuracy with automated handling and 4% human review might deliver more value than a system that achieves 98% accuracy but requires three times the engineering effort to maintain.

## Schema Drift and the Maintenance Burden

Extraction products fail in production not because the model stops working but because the world changes and the product does not adapt. Schema drift is the silent killer of extraction systems. The schema you defined six months ago when you launched the product is not the schema your organization needs today. Business requirements evolve. Compliance rules change. New document types enter the workflow. Fields that were optional become required. Validation rules that made sense in January cause false rejections in June.

A financial services company built an extraction system to process loan applications. The schema included fields for applicant name, income, employment history, credit score, and requested loan amount. Six months after launch, new regulations required capturing additional information about the source of the down payment. The business team updated their manual processes immediately, but nobody told the AI team. For two months, the extraction system continued processing applications using the old schema, and the missing down payment information caused compliance violations that were discovered only during an audit.

Schema drift happens because extraction products sit at the boundary between unstructured inputs and structured systems. The upstream world of documents and free-form text is constantly changing. The downstream world of databases and business logic is also changing. Your extraction product is caught in the middle, trying to map a moving target to another moving target. If you design the product as a one-time integration, it will break. You need to design it as a continuously maintained system with version control for schemas, automated alerts when extraction patterns change, and a process for updating the model when new requirements emerge.

Document format drift is equally destructive. The vendor who supplies 40% of your invoices updates their billing system and changes the layout of line item tables. The hospital network you process records from migrates to a new electronic health record platform that uses different section headers. The law firm you extract contracts from starts using a new template that embeds signature blocks in a different location. Each of these changes is invisible to humans who read the documents, but they break extraction systems that rely on positional cues, keyword matching, or layout assumptions.

You discover format drift the same way the insurance company discovered their PDF problem. Slowly, quietly, and too late. Error rates creep up but not enough to trigger alarms. Individual failures look like random noise until someone aggregates months of data and realizes that errors cluster around specific vendors, specific time periods, or specific document subtypes. By the time you identify the problem, you have processed thousands of documents incorrectly, and the downstream consequences are already in motion.

The defense against drift is continuous monitoring. You cannot evaluate an extraction product once at launch and assume it will maintain that performance forever. You need automated metrics that track accuracy over time, broken down by document type, vendor, and schema field. You need alerting that fires when accuracy drops below thresholds or when the distribution of extracted values changes in unexpected ways. You need a process for investigating drift, determining whether it reflects real-world changes or model degradation, and deploying updates before errors accumulate into crises.

## Nested Structures and Domain-Specific Formats

The simplest extraction products handle flat schemas. Extract a name, a date, a dollar amount. The fields are independent, the validation rules are simple, and errors are isolated. Production extraction products are never this simple. Real-world data has nested structures. A contract contains multiple parties, each with a name, address, and role. An invoice contains line items, each with a description, quantity, unit price, and total. A medical record contains a patient history with multiple diagnoses, each with onset dates, treatment plans, and outcome notes.

Nested structures multiply the failure modes. You might correctly extract that a contract has three parties but incorrectly assign which obligations belong to which party. You might extract all the line items from an invoice but misalign quantities with descriptions. You might capture every diagnosis in a patient record but lose the temporal relationships that indicate which diagnosis came first and which treatments were attempted for each condition. Flat errors are obvious. Nested errors are subtle and often undetectable until a human reads the structured output carefully and compares it against the source document.

Domain-specific formats add another layer of complexity. Legal documents use conventions and language that differ fundamentally from medical records, which differ from financial statements, which differ from customer support emails. A system trained to extract entities from legal contracts will fail catastrophically on radiology reports because the two domains use different vocabularies, different structures, and different implicit knowledge. Domain specificity means you cannot build one general-purpose extraction system. You need specialized models, specialized prompts, specialized validation rules, and specialized evaluation datasets for each domain you operate in.

A legal technology company discovered this when they tried to expand their contract analysis product from employment agreements to commercial leases. Both are legal contracts. Both contain parties, obligations, dates, and termination clauses. But the language, structure, and key provisions are completely different. Employment agreements focus on compensation, responsibilities, confidentiality, and non-compete terms. Commercial leases focus on premises descriptions, rent schedules, maintenance obligations, and renewal options. The same extraction model produced 91% accuracy on employment agreements and 67% accuracy on leases. They needed separate models, separate schemas, and separate evaluation processes.

Domain specificity also affects how you measure quality. In some domains, exact match is the only acceptable standard. If a financial statement lists assets of $47.3 million, extracting $47 million or $48 million is not close enough. In other domains, semantic equivalence matters more than exact strings. If a patient record says "the patient experienced acute myocardial infarction" and your system extracts "heart attack," you have captured the meaning even though the wording differs. Your evaluation strategy must match the domain requirements, and those requirements vary wildly across use cases.

## When Extraction Fails Spectacularly

Extraction products fail most spectacularly on exactly the documents that matter most. Legal documents that determine contractual obligations worth millions of dollars. Medical records that inform treatment decisions for critically ill patients. Financial statements that trigger regulatory reporting requirements. These documents are long, complex, filled with domain-specific terminology, and formatted inconsistently. They are also the documents where errors have the highest cost.

Legal document extraction fails on ambiguity. Contracts are written by lawyers who are paid to introduce carefully crafted ambiguity that favors their clients. A termination clause might say "either party may terminate with ninety days notice, except in cases where material breach has occurred, in which case termination may be immediate upon written notice, provided that the breaching party has failed to cure within thirty days of receiving notice of breach." Extracting the termination notice period from this clause requires understanding conditional logic, implicit references, and legal concepts like material breach and cure periods. A model that extracts "ninety days" is technically correct but dangerously incomplete.

Medical record extraction fails on abbreviations and context dependence. Clinical notes use hundreds of abbreviations, many of which have multiple meanings depending on context. "MS" might mean multiple sclerosis, mitral stenosis, or morphine sulfate. "PT" might mean patient, physical therapy, or prothrombin time. A model that extracts abbreviations without resolving them to full terms will produce a structured output that is technically accurate but clinically meaningless. A model that resolves abbreviations incorrectly will produce outputs that are dangerously wrong.

Financial statement extraction fails on calculation dependencies and cross-references. A balance sheet contains values that must reconcile with each other. Assets must equal liabilities plus equity. Line items in one section must match totals in another section. If your extraction system pulls numbers independently without validating these relationships, you will produce outputs that are internally inconsistent and useless for financial analysis. If you try to enforce validation rules, you will discover that many real-world financial statements contain errors, inconsistencies, or non-standard presentations that break your rules, and deciding whether to reject the document or relax the validation becomes a judgment call that your model cannot make.

The common thread in these failures is that extraction is not just pattern matching. It is interpretation. To extract correctly, you need to understand not just what the text says but what it means in context, how different pieces of information relate to each other, and which details matter for the downstream use case. Models are getting better at this kind of reasoning, but they are not yet reliable enough to handle high-stakes extractions without human oversight.

## Evaluation of Extraction Products

Extraction evaluation seems straightforward because you have ground truth. You know what the correct values are. You can compare the model's output to the correct answer and measure accuracy. But extraction evaluation is far more nuanced than binary right-or-wrong comparisons.

Exact match evaluation works for fields where there is only one correct representation. Policy numbers, product codes, ISBN numbers, and other identifiers must match character-for-character. But exact match fails for fields where multiple representations are valid. A date can be written as "January 15, 2026," "15 Jan 2026," "01/15/2026," or "2026-01-15." All are correct. An exact match evaluator will penalize three of these variants even though they convey identical information. You need normalization rules that convert different representations to a canonical form before comparing.

Fuzzy match evaluation handles cases where minor variations are acceptable. Names can be written with or without middle initials. Addresses can abbreviate "Street" as "St" or spell it out. Descriptions can use synonyms or paraphrase the source text. A fuzzy match evaluator uses string similarity metrics or semantic similarity models to determine whether the extraction is close enough to count as correct. But "close enough" is a judgment call. Is "John Smith" close enough to "John A. Smith"? Is "123 Main Street" close enough to "123 Main St, Suite 200"? You need to define fuzzy match thresholds that match your use case requirements, and those thresholds will differ across fields and domains.

Semantic match evaluation applies to fields where the meaning matters more than the exact wording. If a contract says "the agreement shall remain in effect for a period of three years from the date of execution" and your system extracts "three-year term," you have captured the semantic content even though the language differs. Semantic evaluation requires a model to compare meanings, which introduces a second layer of AI into your evaluation pipeline and creates opportunities for evaluation errors to mask extraction errors.

Coverage is as important as accuracy. An extraction system that achieves 98% precision but only 60% recall is finding the right values when it finds them, but it is missing 40% of the entities that should be extracted. Missing data is often worse than wrong data because wrong data is visible and triggers error handling, while missing data is silent and creates gaps that nobody notices until the consequences appear. You need to measure both precision and recall and decide which matters more for your use case. In some contexts, false positives are expensive. In others, false negatives are the real risk.

Schema compliance is a first-class evaluation metric for production extraction systems. It does not matter if your system extracts the right values if it puts them in the wrong fields, uses the wrong data types, or violates required field constraints. A downstream system expecting a date in ISO format will break if you provide a date as free text. A database expecting a foreign key reference will break if you provide a string. Schema compliance must be 100%, and it must be enforced through validation before outputs leave the extraction pipeline.

## The Business Case for Extraction Products

Extraction products have the clearest return on investment of any AI archetype because the value is easy to quantify and the alternative is obvious. If you are currently paying humans to extract data from documents, you can calculate exactly how much time AI saves, how much cost it eliminates, and how quickly the investment pays back. A paralegal who processes twenty claims per day at a loaded cost of two hundred fifty dollars per day is spending twelve dollars and fifty cents per claim on pure labor. If AI processes the same claim for fifty cents and requires human review on 10% of cases, your cost per claim drops to three dollars. The ROI is immediate and measurable.

But the business case depends entirely on the accuracy number and the cost of errors. If your AI achieves 96% accuracy and the 4% error rate requires full human review of flagged cases, your actual labor savings are not 90%. They are 90% minus the cost of reviewing the 10% of low-confidence cases minus the cost of fixing the errors that slip through. If reviewing flagged cases takes 80% as long as processing the document from scratch, and if 4% of cases produce errors that require rework, your effective labor savings might be closer to 60%. That is still a good business case, but it is not the 90% that the vendor promised.

The cost of errors varies wildly by domain. In low-stakes extraction like categorizing customer feedback, an error means one data point in your analysis is wrong. The impact is negligible. In high-stakes extraction like processing insurance claims, an error might mean paying out the wrong amount, denying a valid claim, or violating a regulatory requirement. The cost of a single error can exceed the cost savings from automating thousands of correct extractions. You need to factor error costs into your ROI model, and you need to be honest about whether your accuracy threshold is adequate for your risk tolerance.

Extraction products also create value by enabling workflows that were previously impossible. When data extraction is a manual bottleneck, you cannot scale to process millions of documents. You cannot provide real-time responses. You cannot run analytics that require structured data from unstructured sources. AI extraction removes the bottleneck and unlocks new capabilities. A legal team that can analyze ten thousand contracts in hours instead of months can identify risks, negotiate better terms, and move faster than competitors. A healthcare system that can extract structured data from clinical notes in real time can trigger alerts, populate decision support tools, and improve patient outcomes. The value is not just cost savings. It is speed, scale, and capabilities that were not feasible before.

## Building Extraction Products That Last

Extraction products are infrastructure. They sit in critical paths between data sources and business systems. When they break, downstream processes fail. Building extraction products that last requires treating them as maintained systems, not one-time integrations.

You need version control for schemas. When business requirements change and new fields are added or validation rules are updated, you need to track which version of the schema applies to which documents and which time periods. You need backward compatibility so that old documents can still be processed, and you need migration paths so that data extracted under old schemas can be converted to new formats.

You need continuous evaluation with real production data. Test set accuracy at launch tells you nothing about accuracy six months later after document formats have drifted, vendors have updated their systems, and business processes have evolved. You need ongoing measurement of precision, recall, schema compliance, and error distributions, with automated alerts when performance degrades.

You need human-in-the-loop workflows for handling edge cases and low-confidence extractions. The goal is not to eliminate humans. The goal is to focus human effort on the cases where judgment is required and automate the cases where it is not. A well-designed extraction product makes humans more productive, not obsolete.

You need clear ownership and maintenance responsibilities. Extraction systems break when nobody is responsible for monitoring them, investigating performance degradations, or deploying updates. You need a team that owns the system, a process for handling issues, and a roadmap for evolving the product as requirements change.

## The Hidden Costs of Poor Extraction

The costs of extraction failures extend beyond the immediate error. A financial services firm discovered this when their contract extraction system systematically misread penalty clauses in vendor agreements. The extraction errors were not caught during review because the structured output looked plausible. Over eighteen months, the company entered into contracts with penalty terms they did not understand, and when vendors invoked those penalties, the financial exposure exceeded eight million dollars. The cost was not just the penalties. It was the legal fees to dispute them, the management time spent investigating how the errors occurred, and the reputational damage when the story became public.

Poor extraction quality creates downstream data quality problems that compound over time. When your extraction system feeds a data warehouse that powers business intelligence dashboards, every extraction error becomes a data quality issue. Analysts make decisions based on incorrect data. Executives present metrics to the board that are wrong. Compliance reports submitted to regulators contain inaccuracies. The errors propagate through your organization, creating a web of consequences that are difficult to trace back to the source.

Extraction errors also create operational inefficiencies that are harder to measure than direct financial costs. When an extraction system requires human review of 20% of cases due to low confidence scores, you have not automated 80% of the work. You have created a workflow where humans must context-switch between reviewing flagged cases and handling other responsibilities. The cognitive overhead of switching contexts, the time spent loading the original document and comparing it to the extracted output, and the decision fatigue from evaluating hundreds of edge cases all reduce productivity in ways that do not show up in simple time-saved calculations.

The most insidious cost is the erosion of trust. When users discover that the extraction system makes errors, they stop trusting it entirely. They start manually checking every extraction, which defeats the purpose of automation. Or worse, they stop checking, accept the errors as inevitable, and build processes around the assumption that extracted data is unreliable. Once trust is lost, it is nearly impossible to regain, even after you fix the underlying issues.

## Extraction as a Foundation for Compound AI Systems

Extraction products rarely exist in isolation. They are components in larger systems where extracted data feeds into downstream AI products, analytics workflows, or business processes. A customer support system extracts issue type and sentiment from tickets, then routes them to the right team using classification logic. A legal research tool extracts key clauses from contracts, then compares them across documents using similarity models. A medical records system extracts diagnoses and medications, then uses that data to populate decision support tools that recommend treatments.

When extraction is a foundation layer, its error rate compounds. If your extraction system has 95% accuracy and the downstream classification system has 95% accuracy, your end-to-end accuracy is not 95%. It is 90%. Each layer multiplies the error rate of the previous layer. A system with three layers each at 95% accuracy has an end-to-end accuracy of 86%. This compounding effect means that extraction products used as foundation layers need higher accuracy targets than standalone extraction products. A 95% accurate extraction system might be acceptable when humans review the output. It is not acceptable when the output feeds into automated systems that make decisions without human oversight.

The compounding error problem is why organizations building compound AI systems invest heavily in extraction quality. They treat extraction as a bottleneck that limits the performance of everything downstream. Improving extraction accuracy from 95% to 98% might seem like a marginal gain, but when that extraction feeds into five downstream systems, the improvement cascades through the entire pipeline and delivers disproportionate value.

## The Extraction Product Maturity Curve

Extraction products follow a predictable maturity curve. The first version achieves acceptable accuracy on the happy path cases that dominate your test set. You deploy to production, celebrate the initial success, and assume the hard work is done. Then reality sets in. Edge cases appear that you never anticipated. Document formats change. Business requirements evolve. Accuracy on production data diverges from accuracy on your test set.

The second phase is firefighting. You fix the most obvious errors by adding special case handling, expanding validation rules, and collecting more training data for problem areas. Accuracy improves, but the system becomes more complex and brittle. Each fix addresses a specific failure mode but introduces new failure modes you have not yet discovered. You are playing whack-a-mole with errors, and the pace of new error discovery exceeds the pace of fixes.

The third phase is systematic improvement. You invest in proper evaluation infrastructure that measures accuracy on representative production data, broken down by document type and schema field. You build monitoring that detects drift before it causes crises. You establish processes for updating the model when requirements change. You design the product architecture to handle edge cases gracefully rather than treating them as special cases. This is the phase where extraction products mature from prototypes into production infrastructure.

Most organizations never reach the third phase. They get stuck in firefighting mode, where each new error requires manual intervention and where the backlog of known issues grows faster than the team can address them. The extraction product becomes a maintenance burden rather than a value generator. The organizations that reach maturity are the ones that treat extraction as a continuous improvement problem, invest in the infrastructure to support that improvement, and resist the temptation to declare victory after the first deployment.

## Why Extraction Looks Easier Than It Is

Extraction products suffer from the demo-to-production gap more than any other AI archetype. Building a demo that works on clean test cases is trivial with modern models. You can build a working prototype in hours. The prototype extracts entities from well-formatted documents with 95% accuracy, and stakeholders are impressed. But production is different. Documents are messy. Formats are inconsistent. Edge cases are everywhere. Achieving 95% accuracy on diverse production data is exponentially harder than achieving it on curated test cases.

The gap exists because extraction performance is highly sensitive to input distribution. A model trained on contracts from US law firms will fail on contracts from UK solicitors because the language and conventions differ. A model trained on invoices from large enterprises will fail on invoices from small businesses because the formatting differs. A model trained on medical records from one hospital system will fail on records from another system because the abbreviations and templates differ. You cannot test extraction quality with a small, homogeneous dataset and extrapolate to production. You must test on data that represents the full diversity of inputs you will encounter in production, and building that test set is hard.

The gap also exists because extraction requirements are stricter than other AI tasks. A chatbot that is 90% helpful is useful. An extraction system that is 90% accurate is often unusable because the 10% error rate creates downstream chaos. Users expect extraction to be right, not mostly right. The bar for acceptable quality is higher, and reaching that bar requires more effort, more data, more validation, and more iteration than stakeholders expect based on the prototype.

## The Strategic Value of Extraction Excellence

Organizations that excel at extraction gain compounding advantages. When your extraction quality is consistently high, you can automate workflows that competitors cannot. You can process higher volumes with lower error rates. You can expand into adjacent use cases faster because you have reusable infrastructure, proven evaluation methods, and accumulated domain expertise.

Extraction excellence also creates data moats. The structured data you extract from unstructured sources becomes a proprietary asset. A legal firm that has extracted and structured data from ten thousand contracts has insights that competitors who process contracts manually do not. A healthcare system that has extracted structured clinical data from millions of patient encounters can train better predictive models, identify patterns that improve care, and operate more efficiently than systems that leave clinical data trapped in narrative notes.

The strategic value extends beyond the immediate use case. Extraction products that work well become platforms that enable other AI products. Once you can reliably extract entities from documents, you can build classification products that route based on those entities, generation products that summarize based on those entities, and analytics products that aggregate based on those entities. Extraction is foundational infrastructure that unlocks value across your entire AI portfolio.

The organizations that treat extraction as a core competency rather than a commodity feature are the ones that build sustainable competitive advantages. They invest in evaluation infrastructure that catches errors before they reach production. They build monitoring systems that detect drift and trigger updates. They develop domain expertise that informs better schema design, validation rules, and error handling. They create feedback loops where production failures improve future versions. This continuous improvement cycle is what separates extraction products that last from extraction products that fail.

Extraction products are powerful, practical, and valuable. They also fail in subtle, persistent, and expensive ways when you treat them as solved problems instead of maintained systems. The organizations that succeed with extraction are the ones that invest in evaluation, monitoring, and continuous improvement, and that design their products to handle errors gracefully instead of pretending errors will not occur.

*Next, we examine agent products, where AI does not just extract information but takes actions in the real world, and where the stakes and the failure modes multiply dramatically.*
