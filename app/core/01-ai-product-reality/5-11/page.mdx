# Chapter 5.11 — SLOs for AI Products (Quality, Latency, Cost & Safety Commitments)

Service Level Objectives for traditional software are well understood: 99.9% uptime, P99 latency under 200ms, error rate below 0.1%. AI products need SLOs too — but the dimensions are different. Uptime isn't enough when your system is "up" but giving bad answers.

SLOs translate AI chaos into commitments that executives, customers, and your own team can hold you to.

---

### Why AI Products Need SLOs

**Without SLOs, quality is a feeling.** "The AI seems to be working well." This is how most AI teams describe their product's health. It's useless. SLOs replace feelings with numbers.

**SLOs create accountability.** When you commit to "90% answer accuracy measured on a daily sample of production traffic," you're committing to measure, monitor, and maintain that level. The commitment forces the infrastructure.

**SLOs align the team.** When everyone knows the targets, they make consistent decisions. The engineer asks "will this change affect our quality SLO?" The PM asks "are we hitting our cost SLO?" The conversation is productive because it's grounded in shared metrics.

**SLOs communicate to customers.** Enterprise customers want commitments, not promises. An SLA backed by SLOs ("we guarantee 88% answer accuracy as measured by our evaluation framework") is more credible than "our AI is really good."

---

### The Four SLO Dimensions for AI

**1. Quality SLO.**
What it measures: the accuracy, helpfulness, or correctness of AI outputs.
How to measure: automated evaluation on a random sample of production traffic (1-5% sampled daily), scored by LLM-as-judge, rule-based checks, or periodic human review.
Example: "85% of customer support responses are rated 'acceptable or better' by our automated quality scorer, measured daily."

Setting the target: start with your current performance, add a small buffer, and commit. If your system currently scores 87%, set the SLO at 83%. SLOs should be achievable — they represent the minimum you commit to, not your aspiration.

**2. Latency SLO.**
What it measures: how long users wait for a response.
How to measure: P50, P95, and P99 latency percentiles.
Example: "P95 response latency is under 3 seconds."

Why percentiles matter: average latency is misleading. An average of 1.5 seconds might mean 90% of users wait 1 second and 10% wait 6 seconds. P95 and P99 capture the experience of your worst-served users.

**3. Cost SLO.**
What it measures: the cost per query, per user, or per resolved task.
How to measure: total AI spend (model costs + infrastructure + human review) divided by query volume.
Example: "Cost per resolved customer query stays below $0.50."

Why this matters: AI costs are variable and can spike without warning. A cost SLO forces proactive monitoring and prevents budget surprises. It also creates a framework for evaluating cost-saving changes: "this cheaper model saves 40% but would it violate our quality SLO?"

**4. Safety SLO.**
What it measures: the rate of harmful, policy-violating, or inappropriate outputs.
How to measure: automated safety classifiers on 100% of production traffic, plus human review of flagged content.
Example: "Harmful or policy-violating outputs occur in fewer than 0.1% of interactions."

The safety SLO is often the tightest because the cost of violation is the highest. A single safety failure can generate more damage than a thousand quality failures.

---

### SLO Error Budgets

An SLO of 90% quality means you have a 10% error budget. That 10% isn't a problem — it's the budget you spend on pushing boundaries, shipping fast, and learning from production.

When you're well within your error budget: ship faster, experiment more, try new models.
When you're close to exhausting your error budget: slow down, focus on reliability, fix the top failure modes.
When you've exhausted your error budget: stop shipping new changes, investigate, and restore quality.

Error budgets transform the quality conversation from "we need to be perfect" to "we need to stay within our budget." This is psychologically healthier and operationally more effective.

---

### Getting Started

Don't try to set all four SLOs on day one. Start with the one that matters most for your product:
- Customer-facing chatbot? Start with quality SLO.
- Real-time voice? Start with latency SLO.
- High-volume internal tool? Start with cost SLO.
- Any Tier 3+ product? Start with safety SLO.

Add the others as your monitoring infrastructure matures. One SLO you actually measure and act on is worth more than four SLOs nobody tracks.

---

*Next: unit economics — because if the math doesn't work, nothing else matters.*
