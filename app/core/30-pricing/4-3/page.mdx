# 30.4.3 — Feature Gating for AI Products: What Goes in Free, What Goes Behind the Wall

Every feature in an AI product has a cost-to-serve. This is the fact that separates AI feature gating from every other kind of software packaging decision. In traditional SaaS, gating a feature behind a paid tier was primarily a marketing decision — you withheld something to create perceived value, and the cost of delivering it when the customer upgraded was zero. In AI products, gating is a margin decision first and a marketing decision second. The feature you put in the free tier burns compute on every invocation. The feature you put in the mid-tier runs on a model that costs real money per call. The feature you put behind the enterprise wall might trigger agent workflows that cost two to five dollars per execution. Where you draw the gate line determines your cost structure, your adoption curve, and your margin profile simultaneously. Get it right and you build a product that hooks users cheaply and monetizes them profitably. Get it wrong and you either starve adoption by gating too aggressively or bleed margin by giving away too much.

## The Cost-Value Matrix

The framework for AI feature gating is a two-dimensional assessment. One axis is cost-to-serve: how much does this feature cost per invocation? The other axis is perceived value: how much does the user care about this feature when deciding to adopt, continue using, or upgrade?

Features that are low cost and high value are your free tier anchors. They hook users without draining your infrastructure budget. Features that are high cost and high value are your paid tier differentiators. They are the capabilities users will pay for because the value is obvious. Features that are high cost and low perceived value are candidates for elimination — they cost you money and do not drive upgrade decisions. Features that are low cost and low perceived value can be sprinkled across tiers as filler without economic impact, though they rarely move the needle on adoption or expansion.

This sounds straightforward in theory. In practice, most AI companies get it wrong because they assess perceived value based on what the product team thinks is impressive rather than what users actually need in their daily workflow. A creative image generation feature using a frontier model might cost $0.08 per invocation and wow users in a demo — but if the product is a business document platform, the feature that drives daily adoption is the $0.003 summarization feature that saves thirty minutes per document. The expensive showpiece goes behind the wall. The cheap workhorse goes in free. Many companies reverse this, gate the workhorse to drive upgrades, and discover that users never get hooked enough to reach the upgrade conversation.

## What Goes in Free: The Hook Layer

The free tier of an AI product serves exactly one strategic purpose: to create habitual usage that makes the product difficult to abandon. Every feature in the free tier must earn its place by driving one of two outcomes — daily active usage or data network effects that make the product smarter over time. If a free feature does neither, it is consuming compute for no strategic return.

The features that belong in the free tier share three characteristics. First, they are cheap to serve. The per-invocation cost should be under $0.01, ideally under $0.005. At this cost level, even aggressive free-tier usage does not create a margin problem. You can afford millions of monthly invocations because each one is measured in fractions of a cent. Second, they are immediately useful. The user should experience value within the first session, without configuration, without training, without reading documentation. The free feature is the product's handshake — it must be compelling in seconds. Third, they create patterns. The user should reach for the feature repeatedly, ideally daily. A feature used once a month does not build the habit loop that makes the product sticky.

GitHub Copilot's free tier illustrates this precisely. It includes two thousand code completions per month — the core feature that every developer reaches for dozens of times per day. The cost per completion on a small, fast model is minimal. The perceived value is enormous because it eliminates the friction of typing boilerplate code. A developer who uses free Copilot for two weeks has formed a muscle memory that makes coding without it feel slow. That muscle memory is the hook. The fifty monthly premium requests included in the free tier serve a different purpose — they give the user a taste of the more expensive capabilities without enough volume to satisfy daily usage, creating natural expansion pressure toward the $10 per month Pro tier.

The free tier mistake that kills AI products is including expensive features to impress rather than cheap features to hook. A document analysis startup that offers free access to its frontier-model-powered deep analysis feature will attract users who burn through $2 per query without converting, because the user came for the powerful feature, got their answer, and left. The same startup offering free access to its quick-summary feature — powered by a small, fast, cheap model — hooks users into a daily workflow of summarizing documents as they arrive. The quick summary is not impressive. It is habitual. And habitual usage converts to paid subscriptions at three to five times the rate of impressive-but-occasional usage.

## What Goes in the Mid-Tier: The Daily Workflow Layer

The mid-tier — whether you call it Professional, Pro, Business, or Plus — should contain the features that support the user's core daily workflow at a level of quality and volume that justifies the price. This is the tier where most of your revenue will concentrate, where most of your customers will land, and where your margin math must work hardest.

Mid-tier features share a different set of characteristics from free-tier features. They are moderate in cost — per-invocation costs between $0.01 and $0.15 are typical. They require more sophisticated models or longer context windows than free-tier features, but they do not require frontier reasoning capabilities or multi-step agent workflows. They are the features that transform a user from someone who occasionally uses the product into someone who depends on it.

For an AI writing assistant, the mid-tier might include advanced editing suggestions, tone adjustment across full documents, and grammar correction in multiple languages — features that run on a capable but not frontier model, that cost $0.02 to $0.08 per invocation, and that a professional writer reaches for on every piece of content. For an AI customer support tool, the mid-tier might include ticket classification, response drafting, and sentiment analysis — features that cost $0.03 to $0.10 per interaction and that a support team manager uses for every shift. Writer, the enterprise AI writing platform, prices its mid-tier at $18 per user per month, packaging exactly this kind of daily-workflow AI into a seat that covers the compute cost with margin to spare.

The critical design principle for mid-tier gating is to include enough AI capability that the user cannot imagine going back to the free tier, while holding back the capabilities that serve advanced or specialized use cases. The mid-tier user should feel fully served for their regular workflow. They should only feel the pull toward the enterprise tier when their needs expand beyond regular work — when they need custom model behavior, when they need compliance features, when they need capabilities that serve the organization rather than the individual.

## What Goes Behind the Enterprise Wall: The Margin Protection Layer

The enterprise tier serves a dual purpose that most packaging guides miss. Yes, it captures willingness to pay from large organizations. But in AI products, it also functions as a **margin protection layer** — a gate that keeps the most expensive capabilities behind pricing that is explicitly designed to cover their cost.

Enterprise-tier features are the ones that cost real money to deliver and serve organizational rather than individual needs. Custom fine-tuned models that cost thousands of dollars to train and pennies per inference but require dedicated infrastructure. Agent-based workflows where a single execution might involve four to eight model calls, tool use, and multi-step reasoning at a total cost of $2 to $8. Compliance features like data residency, audit logging, and encryption-at-rest that require dedicated infrastructure. Admin capabilities like role-based access control, usage analytics per team, and integration with enterprise identity providers. Priority access to frontier models with guaranteed latency SLAs that require reserved capacity.

The margin protection function of the enterprise tier works because enterprise pricing is negotiated, not listed. When a mid-tier customer pays $79 per user per month, that price must cover the average cost-to-serve across all mid-tier customers — including the heavy users. When an enterprise customer negotiates a contract, you can price the specific capabilities they need based on their specific projected usage. A law firm that needs access to a custom-tuned legal reasoning model for a hundred lawyers gets a price that reflects the actual cost of maintaining that model and serving that query volume. A consulting firm that needs agent-based workflows processing five thousand documents per month gets a price that reflects the actual cost of those agent executions. The enterprise tier lets you escape the averaging problem that plagues flat-rate tiers.

Harvey operates almost entirely at this enterprise layer. At approximately $1,200 per lawyer per month with twelve-month commitments and minimum seat counts around twenty, they price their product to cover the significant cost of running frontier legal reasoning models while sustaining healthy margins. That price reflects the genuine cost-to-serve — long-context reasoning across complex legal documents is expensive — and positions the product not as a technology expense but as a substitute for associate labor that costs five to ten times more per hour. The enterprise wall is not a barrier. It is a value statement: this capability is serious enough to warrant serious pricing.

## The Gating Gradient: Smooth Transitions, Not Hard Walls

The worst feature gating feels like a locked door. The user clicks a button, gets a message telling them to upgrade, and feels penalized for using the product. The best feature gating feels like a natural horizon — the user can see what is available on the next tier, understands why it costs more, and can make a rational decision about whether the additional value justifies the additional cost.

Designing a smooth gating gradient means giving users controlled exposure to gated features rather than completely hiding them. There are three proven approaches.

The first is the trial allowance. Give every user a small number of invocations of paid features per month — enough to experience the capability, not enough to satisfy their need. GitHub Copilot does this with fifty premium requests in the free tier. The user sees the quality difference between standard completions and premium model responses. They understand what they are paying for if they upgrade. The trial allowance converts at two to three times the rate of features the user has never experienced.

The second is the degraded preview. Show the user what the paid feature would produce, but at reduced quality or with limitations. An AI document analysis tool might offer the free tier a summary generated by a basic model in three sentences. The paid tier generates a comprehensive analysis with citations, key findings, and recommended actions using an advanced model. The free user sees the three-sentence summary and, beneath it, a preview of the first paragraph of what the full analysis would contain. They know exactly what the upgrade delivers. The degraded preview converts at about one and a half times the rate of fully hidden features.

The third is the outcome-based tease. Show the user the outcome metrics of the paid feature without showing the feature itself. "Our advanced model achieves ninety-two percent accuracy on documents like this. Your current plan uses the standard model at seventy-eight percent accuracy." The user does not need to experience the feature to understand the value gap. The number does the selling. This approach works particularly well for accuracy-sensitive use cases like legal research, medical coding, and financial analysis, where the difference between seventy-eight and ninety-two percent accuracy is not a marginal improvement — it is the difference between useful and unreliable.

## The Migration Path: Gating Changes Without Customer Revolt

Feature gates are not permanent. As model costs decline, features that were expensive to serve become cheap. As your product evolves, new expensive features replace old ones. The agent workflow that cost $3 per execution in 2025 might cost $0.40 in 2026 after model improvements and inference optimization. That feature should move from your enterprise tier to your mid-tier, freeing the enterprise tier to gate the new, more expensive capabilities you have since developed.

But moving features between tiers is a sensitive operation. Moving a feature down — from enterprise to mid-tier or from mid-tier to free — is almost always well received. Customers on the tier that lost the exclusive feel mildly annoyed but rarely churn over it. Customers on the tier that gained the feature feel pleasantly surprised. The net effect is positive.

Moving a feature up — from free to mid-tier or from mid-tier to enterprise — is dangerous. Customers who were using the feature for free now have to pay for it. The perception is that you took something away, even if you added other features to compensate. The most common result is vocal backlash, negative reviews, and a spike in churn among the affected cohort.

The safe approach to upward migration is the grandfather clause. Existing users who were using the feature on their current tier continue to have access to it. New users are subject to the new gating. This creates a temporary inconsistency — some free-tier users have the feature, new free-tier users do not — but the alternative is a customer revolt that costs more in churn and reputation damage than the inconsistency costs in complexity.

The other safe approach is the swap. Instead of removing a feature from a tier, replace it with a feature of comparable perceived value that costs less to serve. The user does not lose a feature — they get a different one. The net perceived value stays constant. Your cost-to-serve decreases. This requires having a pipeline of new features in development, which most AI companies do. The key is timing the swap so the replacement feature is ready before the expensive feature needs to move.

## Gating Across Model Quality Tiers

One AI-specific gating dimension deserves dedicated attention: gating by model quality rather than by feature. This is the approach where every tier has access to the same features, but each tier runs those features on a different model. The free tier uses a small, fast, cheap model. The mid-tier uses a capable, mid-range model. The enterprise tier uses the frontier model with the highest accuracy and reasoning capability.

This approach has gained significant traction in 2025-2026 because it solves two problems simultaneously. First, it eliminates the feature-hiding problem entirely — no user sees a grayed-out button or a locked feature. Everyone can do everything. The difference is in the quality of the output. Second, it creates a natural quality gradient that users can experience firsthand. A user on the free tier who generates a document summary gets a decent result. A user on the mid-tier gets a noticeably better result. The enterprise user gets the best available. The quality difference does the selling.

GitHub Copilot's 2025-2026 tier structure uses exactly this pattern. Every tier provides code completions and chat capabilities. The Free tier uses a standard model. The Pro tier at $10 per month provides access to better models. The Pro Plus tier at $39 per month unlocks all available models — including Claude Opus 4, OpenAI o3, and Gemini 2.5 Pro — with fifteen hundred premium requests per month. The features are the same. The models are different. The quality is visibly different. And the cost-to-serve at each tier aligns with the pricing because the model cost is the primary variable.

The risk of model-quality gating is that cheaper models improve faster than premium models, compressing the perceived quality gap between tiers. The small model that was noticeably worse than the frontier model in 2024 may be nearly as good by 2026. If the gap closes enough, the mid-tier user sees no reason to upgrade. You can mitigate this risk by continually upgrading the premium tier to the latest frontier model, maintaining the quality gap even as the baseline improves. The quality gap is not static. It is maintained through continuous investment in access to the best available models.

## The Gating Decision Framework

When you are unsure whether a feature should be free, mid-tier, or enterprise, run it through five questions. What is the per-invocation cost? If under $0.005, it can safely be free. If between $0.01 and $0.15, it belongs in the mid-tier. If above $0.20, it belongs in enterprise or behind usage-based pricing. How frequently will users invoke it? Daily-use features should be available at or below the tier where users first discover them. Weekly or occasional features can be gated higher because the usage volume does not create cost exposure. Does this feature drive initial adoption? If yes, it must be in the free tier regardless of cost — but if it is expensive, you limit the volume. Does this feature drive daily retention? If yes, it must be in the tier where the majority of paying customers land. Is this feature organizationally complex to support? Compliance, custom models, and dedicated infrastructure belong behind the enterprise wall because the cost of supporting them includes human effort, not just compute.

These five questions do not eliminate judgment. But they eliminate the most common mistake — gating based on what the product team thinks is premium rather than what the cost structure and user behavior data say should be gated. The best packaging teams run this framework quarterly, reassess every feature's cost-to-serve as model prices change, and adjust gates based on real data rather than intuition.

Feature gating defines what each tier contains. But the most strategic packaging question in 2026 is not what goes behind the wall — it is what happens when you offer a free tier where every user costs real money. The next subchapter tackles the free tier dilemma: when giving the product away for free is a growth strategy, and when it is a path to bankruptcy.