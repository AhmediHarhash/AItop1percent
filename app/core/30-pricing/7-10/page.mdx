# 30.7.10 — The Usage Volatility Problem: Why AI Revenue Is Harder to Predict Than SaaS

In late 2025, a Series B AI company selling a contract analysis product to mid-market legal departments projected $18 million in annual recurring revenue for the following year. The projection was grounded in what looked like solid data: 340 active enterprise accounts, average monthly usage growing twelve percent quarter over quarter, and a healthy pipeline of new deals. The finance team modeled a base case of $18 million, a bear case of $15.5 million, and a bull case of $21 million. The board approved the plan. Headcount was hired against the base case. Infrastructure capacity was provisioned. Sales compensation was structured around the $18 million target. Nine months later, actual annualized revenue was tracking at $12.8 million — thirty percent below the base case and below even the bear case. The company had not lost customers. Its churn rate was actually lower than projected. What happened was something that no one on the finance team had modeled: three simultaneous forces compressed usage across the customer base in ways that a traditional SaaS forecasting model could not predict.

The first force was prompt optimization. As customers became more sophisticated with the product, they learned to write more precise queries that returned better results on the first attempt. Average queries per contract review dropped from 8.3 to 4.9 over six months — a forty-one percent reduction in usage per unit of work. The customers were happier. They were getting better results with fewer queries. And every improvement in their technique reduced the revenue the company earned from them.

The second force was competitive pressure. A well-funded competitor launched a similar product at sixty percent of the price, targeting the same mid-market legal segment. The competitor's product was not better — in blind evaluations, the incumbent won on quality in seven out of ten categories. But five of the 340 accounts switched entirely, and another forty-two began running parallel evaluations, which split their usage between two products. The usage from those forty-two accounts dropped by roughly half overnight, not because they churned but because they were testing an alternative.

The third force was champion departure. The internal advocate who had driven adoption at twenty-eight of the company's largest accounts left their respective organizations over a nine-month period — normal turnover in a hot job market. In twenty of those twenty-eight accounts, usage dropped by more than sixty percent within three months of the champion's departure. No new champion emerged because the product was embedded in the departing person's workflow, not in the team's workflow. When the person left, the usage went with them.

None of these forces showed up in a traditional churn analysis. The accounts were still active. The contracts were still signed. The logos were still on the customer list. But the revenue had evaporated from the inside, silently and simultaneously, in a way that a per-seat SaaS business would never experience because a seat license generates the same revenue whether the user logs in once or a thousand times.

## The Structural Difference Between SaaS Revenue and AI Revenue

This is the **Usage Volatility Problem**, and it is the single most important difference between AI revenue and SaaS revenue. Traditional SaaS revenue is deterministic at the contract level. A customer who signs a $120,000 annual contract for one hundred seats will generate $120,000 in revenue that year regardless of whether those seats are used heavily, lightly, or not at all. The revenue is locked at the point of sale. Usage affects renewal probability but not current-period revenue.

AI revenue with usage-based or hybrid pricing models is stochastic. The revenue generated by a customer in any given period is a function of how much they use the product, which is a function of adoption depth, workflow integration, user sophistication, competitive dynamics, and internal organizational factors that the vendor cannot observe directly and cannot control. A customer who generated $15,000 in usage last month might generate $22,000 this month because they onboarded a new team, or $8,000 because their power user went on vacation, or $3,000 because they figured out how to get the same output with fewer queries. The variance at the individual customer level is enormous, and while it partially averages out across a large customer base, it never fully stabilizes because the underlying forces that drive usage are correlated — a model provider price cut affects all customers simultaneously, a new competitor enters the market for everyone at once, and macroeconomic tightening makes every customer scrutinize every AI expenditure at the same time.

## The Three Sources of Usage Volatility

Usage volatility in AI products comes from three distinct sources, each with different dynamics and different management strategies. Understanding which source is driving your volatility is essential because the wrong intervention for the wrong source makes the problem worse, not better.

The first source is **behavioral efficiency** — customers learning to get more value from fewer interactions. This is the most counterintuitive form of volatility because it is driven by product success, not product failure. When your AI product works well, users learn to use it more efficiently. They write better prompts. They structure their inputs more precisely. They stop asking redundant follow-up questions because the first response is good enough. Every improvement in product quality and every improvement in user skill reduces the number of interactions required to accomplish a given task, which reduces usage-based revenue.

Behavioral efficiency is a slow-moving force. It does not hit suddenly. It compounds over months as users climb the learning curve. A new customer might average twelve queries per task in their first month. By month six, they average seven. By month twelve, they average four. If your pricing is per-query, you have just experienced a sixty-seven percent revenue decline per task from your most engaged, most satisfied customer. The customer is delighted. Your finance team is confused by the revenue shortfall. And the traditional SaaS playbook — which assumes that happier customers generate more revenue, not less — does not apply.

Managing behavioral efficiency requires pricing structures that decouple revenue from interaction count. Per-seat pricing eliminates the problem entirely because revenue is fixed regardless of efficiency gains. Per-outcome pricing aligns your revenue with the value delivered rather than the effort required. Tiered pricing with capacity bands can dampen the effect if the tiers are wide enough that efficiency gains do not push customers into lower tiers. The worst pricing structure for behavioral efficiency is pure per-query or per-token pricing, where every improvement in your product and every improvement in your user's skill directly reduces your revenue.

## The Second Source: Market and Competitive Disruption

The second source of usage volatility is **market disruption** — external events that shift usage patterns across your entire customer base simultaneously. Competitive entry is the most common form. When a credible competitor launches a product in your category at a lower price point, usage from your existing customers does not necessarily drop because they churn. It drops because they experiment. Enterprise customers are sophisticated enough to run parallel evaluations, split their workloads across multiple providers, and use competitive entry as leverage to negotiate lower pricing or better terms. During the evaluation period — which can last three to six months — your usage from those accounts drops by twenty to fifty percent. Some of that usage returns when the evaluation concludes in your favor. Some does not.

Model provider pricing changes create a similar dynamic. When a foundation model provider cuts pricing dramatically — as happened repeatedly through 2024 and 2025 — some of your customers reconsider whether they need your product at all. If your product is a layer on top of an API, and the API just became seventy percent cheaper, the customer's build-versus-buy calculus shifts. Even customers who ultimately decide to stay with your product may reduce their usage while they evaluate alternatives, creating a temporary but real revenue dip.

Macroeconomic shifts affect AI usage more than SaaS usage because AI spending is often discretionary and newer in the enterprise budget. When a company faces budget pressure, the hundred-seat CRM license is the last thing they cut because it is embedded in mission-critical workflows that have existed for years. The AI contract analysis tool they adopted eight months ago is easier to scale back — not cancel entirely, but reduce from five teams to two, or from unlimited usage to a capped plan. This selective scaling back is invisible in traditional churn metrics but devastating to usage-based revenue.

Managing market disruption requires a combination of contractual protection and product strategy. Minimum spend commitments protect your revenue floor when customers experiment with competitors. Multi-year contracts with annual minimums give you time to prove value before the customer can reduce usage. On the product side, deep workflow integration makes your product harder to replace even when a cheaper alternative exists, because the switching cost is measured in disruption, not dollars.

## The Third Source: Organizational and Champion Risk

The third source of usage volatility is **organizational dynamics** — changes within the customer's company that affect who uses your product and how much they use it. Champion departure is the most acute form. AI products in 2026 are still heavily dependent on internal advocates — the product manager, the director of operations, the legal tech lead who found your product, championed the purchase, drove the integration, and trained their team to use it. When that person leaves, usage often collapses. Not because the product stopped working, but because no one else in the organization has the knowledge, the motivation, or the authority to maintain the adoption level.

Team reorganizations create similar disruption. When a department is restructured, reporting lines change, budgets shift, and tools that were approved under one manager come under scrutiny from a new one. If the new manager has different priorities, different tool preferences, or simply does not understand what the AI product does, usage declines as the product loses its organizational sponsorship.

Budget reallocation is the third form of organizational disruption. Even when the champion stays and the team is intact, a shift in departmental priorities can redirect the budget that funded AI tool usage toward other initiatives. The contract may still be in place, but the team is told to reduce their usage to stay within a newly tightened budget. They comply by using the product less frequently, reserving it for the highest-value tasks and reverting to manual processes for everything else.

Managing organizational risk requires building what you might call **institutional embedding** — making your product part of the organization's workflow rather than one person's workflow. Products that are embedded in shared systems, that generate outputs consumed by multiple teams, that create data dependencies that span departments — these products survive champion departure because removing them would disrupt processes that extend beyond any single person. Products that live in one person's browser tabs do not survive that person's exit.

The operational response is proactive multi-threading within each account. Your customer success team should maintain relationships with at least three contacts per account — the champion, their manager, and a peer in an adjacent function. When the champion leaves, you already have relationships that can sustain the account through the transition. When a reorganization happens, you have visibility into the new structure and can re-engage before usage declines become permanent.

## The Forecasting Implications

Usage volatility means that AI revenue forecasting cannot rely on the simple models that work for SaaS. The SaaS forecasting model is straightforward: take your beginning ARR, subtract expected churn, add expected expansion, add expected new business. Each component is relatively predictable because seat-based revenue does not fluctuate within a contract period.

AI revenue forecasting requires a different approach. Instead of forecasting at the contract level, you need to forecast at the usage-behavior level, which means building models that account for efficiency gains, competitive dynamics, and organizational changes. The most effective AI revenue forecasting models use three layers.

The first layer is **cohort-based usage modeling**. Instead of treating all customers as a single pool, segment them into cohorts based on tenure, size, and usage maturity. New customers in their first six months show different usage trajectories than customers in months twelve through eighteen. Mature customers in year two show different patterns than either. Build separate usage models for each cohort, with assumptions that reflect the lifecycle dynamics specific to that stage.

The second layer is **efficiency decay curves**. For each cohort, model the rate at which per-task usage declines as users become more efficient. If your data shows that customers reduce their per-task query volume by eight percent per quarter on average, build that decay into your forecast. The decay rate is not constant — it accelerates during the first year and then flattens as users reach a baseline efficiency level. Calibrate the curve to your actual customer data, not to assumptions.

The third layer is **scenario-based disruption modeling**. This is where you account for the events that cannot be predicted from historical usage data: competitive entries, provider price changes, macroeconomic shifts, and organizational disruptions. Instead of trying to predict these events, model their impact and probability. If there is a thirty percent chance that a major competitor enters your segment in the next twelve months, and the historical impact of competitive entry is a fifteen percent usage reduction across affected accounts, then your expected impact is a 4.5 percent usage reduction across your base. Build this into your bear case, not your base case, but have the analysis ready.

## Revenue Stabilization Strategies

The companies that manage usage volatility most effectively deploy a combination of pricing structure and product strategy. On the pricing side, the most powerful stabilization mechanism is the hybrid model: a base subscription fee that provides revenue predictability, combined with a usage component that captures expansion. The base fee should cover your cost-to-serve plus a margin floor for each customer, so that even if usage drops to zero, you are not losing money on the account. The usage component sits on top, providing upside when customers use more and declining when they use less, but without threatening the viability of the account.

Minimum spend commitments are the second stabilization mechanism. Even with usage-based pricing, a contractual minimum spend — typically set at seventy to eighty percent of the customer's projected usage — creates a revenue floor that your forecast can rely on. The customer accepts the minimum because it comes with a volume discount: they get a lower per-unit rate in exchange for the commitment. You accept a lower rate because the predictability allows you to plan infrastructure, staff, and investment with confidence.

On the product side, the most effective stabilization strategy is expanding the surface area of usage within each account. If your product serves one use case for one team, usage is concentrated and fragile. If your product serves three use cases across five teams, usage is distributed and resilient. The loss of one champion in one team might reduce usage by fifteen percent instead of sixty percent because four other teams are still active. Product roadmaps for AI companies should explicitly include expansion vectors that increase per-account usage breadth, not just per-user usage depth.

## The Reporting Discipline

Usage volatility makes quarterly reporting more complex and more important. Your board, your investors, and your leadership team need to see not just the revenue number but the composition of that number — how much came from base subscriptions, how much from usage, how much from minimum-spend true-ups. They need to see usage trends at the cohort level, not just the aggregate level. And they need to see the leading indicators that predict future volatility: champion turnover rates, competitive evaluation signals, efficiency trends, and per-account usage concentration metrics.

The companies that report these metrics proactively build trust with their boards and investors. The companies that report only aggregate revenue and then explain a miss after the fact lose trust, and lost trust in a volatile-revenue business is extraordinarily difficult to rebuild. A board that understands usage volatility and sees the management team actively measuring and managing it has patience. A board that is surprised by a revenue miss has no patience at all.

Revenue volatility is a structural feature of AI businesses, not a temporary growing pain. It does not go away with scale, though it becomes more manageable as your customer base diversifies and your pricing structures mature. The discipline of understanding it, measuring it, and building stabilization mechanisms into your pricing and product strategy is what separates AI companies that can forecast reliably from those that lurch from surprise to surprise every quarter. That discipline extends beyond the finance team and into the operational machinery of how you set prices, adjust them, and execute changes across your customer base — the pricing operations infrastructure that is the subject of the next chapter.
