# 30.1.5 — The LLMflation Paradox: Costs Drop 10x Per Year and Spending Keeps Rising

The pricing team at an enterprise AI company stares at two charts pinned side by side on the conference room whiteboard. The left chart shows the cost of GPT-4-level inference over time — a line falling so steeply it looks like a cliff. March 2023: $30 per million input tokens. Late 2024: $2.50 per million tokens for equivalent quality. Mid-2025: under $1 per million tokens for models that match or exceed that original capability. A 96 percent cost reduction in roughly two years. The right chart shows the company's total AI inference spending over the same period. It is going up. Not gently. Sharply. Monthly spend has tripled in eighteen months, even though the per-unit cost of every individual inference call has dropped by an order of magnitude. The head of product stares at both charts and says what everyone in the room is thinking: "How are we spending more money on something that keeps getting cheaper?"

This is the **LLMflation Paradox**, and it is the single most important force shaping AI pricing strategy in 2026. The term comes from Andreessen Horowitz's research documenting that the cost of LLM inference is declining at roughly 10x per year for equivalent performance — a rate of deflation faster than the decline of compute costs during the PC revolution and faster than the decline of bandwidth costs during the dotcom era. And yet, total enterprise spending on AI inference keeps rising, because companies use the savings from cheaper models to tackle harder problems, deploy more use cases, process more volume, and reach for capabilities that require the most expensive frontier models. The cost per unit falls. The number of units explodes. The bill goes up.

## The Deflation Engine

Understanding the mechanism behind cost deflation is essential if you are going to build a pricing strategy that survives it. Three forces drive the decline simultaneously, and they compound.

The first is **model commoditization**. When OpenAI launched GPT-4 in March 2023, it was the only model at that capability level, and it priced accordingly. By mid-2025, a dozen models deliver GPT-4-equivalent performance or better: Claude Sonnet 4.5, Gemini 2.5 Flash, Llama 4 Scout, DeepSeek V3.2, Mistral Large 3, and others. Open-source models gained market share from roughly 25 percent in January 2025 to 45 percent by August 2025, driven by increasingly capable releases that match proprietary quality at a fraction of the cost. Competition among providers has turned GPT-4-class inference into a commodity. Commodities compete on price. Prices fall.

The second is **hardware efficiency**. Each generation of inference-optimized hardware — NVIDIA's Blackwell architecture, Google's TPU v6, AMD's MI350, and the growing fleet of custom ASICs from hyperscalers — delivers more inferences per watt and per dollar than the previous generation. When the same model runs on newer hardware, the cost of serving it drops even without any change to the model itself. This is the classic Moore's Law dynamic, but applied to inference throughput rather than transistor density. The trajectory is consistent: roughly a 2x improvement in inference cost-efficiency per hardware generation, with new generations arriving every twelve to eighteen months.

The third is **algorithmic optimization**. Techniques like speculative decoding, KV-cache compression, mixture-of-experts architectures, quantization from FP16 to FP8 and below, and continuous batching improvements mean that the same model on the same hardware serves requests more cheaply with each round of engineering optimization. Distillation allows smaller, cheaper models to absorb the capabilities of larger, more expensive ones. The result is a compounding effect: better hardware running more efficient models using smarter serving techniques. Each factor contributes a 2x to 5x improvement independently. Together, they produce the 10x annual deflation that Andreessen Horowitz documented.

## Why Spending Rises Anyway

If inference costs are falling at 10x per year, you might expect total spending to fall too. It does not. Total enterprise AI inference spending rose sharply through 2025, and industry analysts project continued growth through 2026 and beyond. Gartner projects that spending on inference will overtake training workloads in AI-optimized infrastructure by 2026, rising to over 65 percent of all AI compute spending by 2029. This is not a contradiction. It is the economic principle of **induced demand** — the same force that ensures building wider highways does not reduce traffic congestion.

When calling GPT-4 cost $30 per million input tokens, every use case had a serious cost conversation. Product managers ran spreadsheets calculating the cost per customer query, and many use cases were killed on cost alone. A customer service chatbot processing 50,000 queries per day at $30 per million tokens generated a monthly inference bill of roughly $45,000 just for input tokens — and that was before output tokens, which cost twice as much. At that price, only high-value, high-volume use cases could justify deployment.

When equivalent capability costs $1 per million tokens, the calculus changes completely. That same 50,000-query chatbot now costs roughly $1,500 per month for input tokens. Use cases that were killed on cost become viable. The product team that rejected an AI-powered document summarizer because it cost $0.12 per document can now deploy it at $0.004 per document. The marketing team that couldn't justify AI-generated product descriptions at $0.08 each can now run them at less than a penny. The engineering team that used AI only for code review can now use it for code generation, test generation, documentation, and dependency analysis. Each cost reduction opens a new tier of use cases. Each new tier of use cases adds volume. Volume times a lower price can — and empirically does — exceed the original total spend.

But the real spending driver is not just more use cases at the old quality level. It is harder use cases at higher quality levels. The frontier models of 2026 — GPT-5 Pro, Claude Opus 4.6, Gemini 3 Deep Think — cost dramatically more per token than their commodity-class equivalents. GPT-5 Pro output tokens price at $120 per million tokens. These models exist because some problems require reasoning depth, multi-step analysis, and precision that commodity models cannot deliver. And companies that have already deployed commodity AI across their easy use cases now want to tackle the hard problems: complex legal analysis, multi-variable financial modeling, scientific research synthesis, autonomous agent workflows with dozens of chained reasoning steps. These hard problems consume frontier models at frontier prices. The average cost per token falls. The average complexity per task rises. Total spending goes up.

## The Jevons Paradox of AI

Economists have a name for this phenomenon. In 1865, William Stanley Jevons observed that improvements in steam engine efficiency did not reduce total coal consumption — they increased it, because more efficient engines made steam power economical for applications that previously couldn't justify it. The same dynamic plays out in AI inference. Cheaper tokens do not reduce total token spending. They increase it, because cheaper tokens make AI economical for applications that previously couldn't justify the cost.

The implications for pricing strategy are profound and uncomfortable. If you build your pricing model on today's cost structure, you are building on a foundation that will be 90 percent cheaper in twelve months. A price that gives you 65 percent gross margin today will give you 93 percent gross margin in a year if costs fall 10x and your price stays constant. That sounds wonderful until you realize that your competitor — the one who launched six months after you — built their pricing on the lower cost structure and is offering equivalent capability at 40 percent of your price. Your 93 percent margin lasts exactly as long as it takes for a competitor to commoditize your pricing tier.

But the reverse is equally dangerous. If you price aggressively today, anticipating future cost reductions, you may run negative margins for the months or quarters before costs actually fall. A startup that priced its AI writing assistant at $0.005 per generated paragraph in early 2025, anticipating that inference costs would drop to make that profitable, burned through $1.8 million in excess infrastructure costs over seven months before cost reductions caught up. They survived. Many don't. Pricing ahead of the cost curve requires capital reserves and investor patience that most companies lack.

## The Pricing Half-Life

The concept that matters most for practical pricing strategy is what we call the **pricing half-life** — the time it takes for a competitive price point to become uncompetitive due to cost deflation and competitive response. In traditional SaaS, pricing half-lives are measured in years. A per-seat price set in 2023 was typically still competitive in 2025. In AI, pricing half-lives are measured in months.

Consider the trajectory. In January 2024, a competitive price for AI-powered document summarization was roughly $0.10 per page. By January 2025, new entrants using cheaper models offered equivalent quality at $0.03 per page. By January 2026, companies using Llama 4 variants running on optimized inference infrastructure offer the same capability at $0.008 per page. The pricing half-life of that $0.10 price point was approximately eight months. Any company that set its price in January 2024 and did not revisit it was three times more expensive than new entrants by the end of the year, and twelve times more expensive by the following year.

This does not mean you should drop your prices every quarter. It means you need a pricing architecture that anticipates deflation. The companies navigating this well in 2026 share several practices. They separate their price from their cost by anchoring pricing on value delivered rather than cost incurred. They build contractual mechanisms — annual rate reviews, volume-based tier adjustments, technology improvement clauses — that allow prices to decline gracefully without requiring painful renegotiations. They invest the savings from cost deflation into higher-capability features that justify premium pricing on the next tier. And they monitor competitive price points monthly, not annually, because a competitor can undercut you in the time it takes to schedule a quarterly business review.

## Building for the Deflation Future

The strategic response to the LLMflation Paradox is not to fight deflation. It is to build your pricing model so that deflation works for you rather than against you. Three principles guide this.

First, price on outcomes or value, not on tokens or compute. If your price is tied to the cost of a token, your price is on a 10x annual decline trajectory whether you like it or not. If your price is tied to the value of a resolved customer ticket, the price holds even as the cost to deliver it drops — and your margin expands. The companies with the strongest margin trajectories in 2026 are the ones who decoupled their pricing from their cost-of-goods early.

Second, reinvest deflation savings into capability expansion. When your inference cost drops by 80 percent, you have a choice. You can capture that as margin. You can pass it to the customer as a price reduction. Or you can invest it into serving harder use cases, adding more steps to your AI workflow, improving quality, and building features that justify a higher value tier. The best operators do all three in deliberate proportions — roughly 30 percent captured as margin, 30 percent passed to the customer to maintain competitive positioning, and 40 percent reinvested into capability that justifies premium pricing on the next tier. These proportions vary by market and competitive intensity, but the discipline of splitting the savings intentionally, rather than letting competitive pressure dictate the split, is what separates companies that build durable margin from companies that race to the bottom.

Third, build tiered pricing that maps to model capability, not model cost. Offer a standard tier that uses commodity-class models at commodity prices for routine tasks. Offer a premium tier that uses frontier models for complex, high-stakes work. As today's frontier models become tomorrow's commodity models — and they always do, on roughly a twelve-month cycle — your standard tier gets better without costing you more, and your premium tier shifts to whatever the new frontier is. Your customer sees continuous improvement. Your margin profile stays healthy because each tier is priced against the value it delivers, not the model it uses.

## The Paradox Is the Opportunity

The LLMflation Paradox seems like a threat to pricing stability, and it is — for companies that price on cost. For companies that price on value, it is the greatest margin expansion opportunity in the history of software. Every year, the cost of delivering a given AI capability drops by an order of magnitude. If your pricing captures value, your cost-to-serve falls while your price holds. Your gross margin improves by 10 to 20 percentage points per year without any change to your pricing. No traditional SaaS business has ever experienced margin expansion at this rate.

But capturing this opportunity requires understanding something else: what this section covers and what it does not. The mechanics of cost reduction — token optimization, caching strategies, batching techniques, model routing for cost efficiency — live in Section 24. The strategy of balancing cost reduction against quality maintenance lives in Section 25. This section, Section 30, focuses on the revenue side: how to translate your AI capabilities into pricing, packaging, and unit economics that build a durable business. Understanding where that boundary falls is the necessary orientation before going deeper.
