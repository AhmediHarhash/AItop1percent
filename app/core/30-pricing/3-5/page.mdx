# 30.3.5 — Outcome-Based Pricing: Charging for Results, Not Effort

The only honest way to price AI is to charge when it works.

That sentence is provocative, and it is meant to be. Outcome-based pricing is not the only honest way to price AI, but it is the model that makes the value exchange undeniable. When you charge per resolution, per completed analysis, per qualified lead, per processed claim, the customer's ROI calculation becomes trivially simple: the AI costs less than the human alternative, or it does not. There is no ambiguity about seat utilization. No debates about whether credits were consumed efficiently. No procurement negotiation where the buyer questions whether the platform fee is justified. The customer pays when AI achieves the result they hired it to achieve, and pays nothing when it does not.

This model is not theoretical. Sierra, the enterprise customer experience company founded by Bret Taylor, reached $100 million in annual recurring revenue in just twenty-one months — one of the fastest revenue ramps in enterprise software history. Sacra estimates they crossed $150 million in ARR by January 2026. Their pricing model is outcome-based: customers pay per resolution, not per seat, not per conversation, not per API call. The result they charge for is the result the customer actually wants — a resolved customer issue. Intercom's Fin AI agent operates on the same principle at a different price point: $0.99 per resolution. Fin now resolves over one million customer issues per week, and the product has grown from $1 million to over $100 million in ARR on that single, simple charge metric.

These are not niche experiments. They are proof that outcome-based pricing can scale to nine figures. But scaling requires getting three things right that most companies underestimate: defining the outcome with surgical precision, managing the margin variance between simple and complex outcomes, and accepting the fundamental asymmetry that you absorb cost when AI fails.

## The Alignment That Makes the Model Work

Outcome-based pricing works because it solves the alignment problem that plagues every other model. In per-seat pricing, the vendor is incentivized to sell more seats regardless of whether those seats generate value. In consumption pricing, the vendor is incentivized to maximize token usage regardless of whether that usage produces useful output. In both models, there is a gap between what the vendor optimizes for — revenue — and what the customer optimizes for — results.

Outcome-based pricing closes that gap. The vendor only earns revenue when the customer gets a result. This creates a powerful structural incentive for the vendor to invest in product quality, because every failed outcome is lost revenue. If your AI agent attempts to resolve a customer ticket and fails — the customer escalates, the issue remains unresolved, the interaction ends without resolution — you earn nothing. You consumed compute, ran retrieval, executed model calls, and paid for all of it. The cost is yours. The revenue is zero. This dynamic forces vendors to treat product quality not as a differentiator but as a survival mechanism. You cannot be casually mediocre when your revenue depends on every interaction succeeding.

The customer sees this incentive alignment and trusts it. When a vendor says "you only pay when we deliver," the procurement conversation shifts from "justify your price" to "prove your resolution rate." The buyer does not need to evaluate whether seats are utilized or credits are consumed fairly. They need to evaluate one number: what percentage of interactions result in a billable outcome, and is the per-outcome price lower than the alternative. For a customer support use case, the alternative is usually a human agent costing between $6 and $15 per resolved ticket when you account for salary, benefits, training, management overhead, and the portion of time spent on non-resolution activities. An AI resolution at $0.99 is not a marginal savings. It is an order-of-magnitude reduction in cost-per-resolution that makes the purchase decision nearly automatic.

## Defining the Outcome: The Hardest Part of the Simplest Model

The simplicity of outcome-based pricing is real — for the customer. For the vendor, the complexity hides behind the definition of the outcome itself. What counts as a resolution? When does a conversation become a billable event? How do you handle the edge cases that inevitably emerge when millions of interactions hit an outcome definition designed for the typical case?

Intercom's definition illustrates both the precision required and the trade-offs involved. A **resolution** in Intercom's model is a conversation where Fin delivers an answer and the customer either explicitly confirms the issue is solved or leaves the conversation without further response. That second condition — the customer simply stops replying — counts as an "assumed resolution." This is pragmatic. Most customers who get a helpful answer do not come back to click a satisfaction button. They move on. If you only counted explicit confirmations, your billable resolution rate would be a fraction of the actual resolution rate, and your pricing model would undercount the value you deliver.

But assumed resolutions create a gray area. A customer might stop replying because they got their answer. They might also stop replying because they gave up and called the phone number instead. They might stop replying because they found the answer elsewhere. They might stop replying because they got frustrated and churned. All of these scenarios look identical in the data — the customer stopped responding — but only the first one is a genuine resolution. The vendor has a financial incentive to count all of them as resolutions, because each one generates $0.99 in revenue. The customer has a financial incentive to scrutinize the definition, because they are paying for outcomes that may not be outcomes at all.

This tension is not a flaw in Intercom's model. It is inherent to every outcome-based pricing system. The outcome must be defined precisely enough that both parties agree on what they are trading, but practically enough that the system can classify outcomes at scale without human judgment on every interaction. Too loose a definition and the customer feels cheated. Too strict a definition and the vendor's revenue underrepresents the value delivered. The definition is the contract, and getting it wrong poisons the model regardless of how good the underlying AI is.

## Five Principles for Defining Billable Outcomes

The companies that have successfully deployed outcome-based pricing at scale share five principles in their outcome definitions that are worth studying.

First, the outcome must be **observable without human judgment**. If determining whether the outcome occurred requires a human to review the interaction and render a verdict, the model does not scale. Intercom uses customer behavior — reply or no reply within a time window — as the observable signal. Sierra uses resolution status in the customer's ticketing or CRM system. A legal document analysis product might define the outcome as "analysis delivered with all required fields populated and no flags for human review." The observable must be automated, unambiguous, and available in real time.

Second, the outcome must be **meaningful to the customer's business**. A resolved ticket matters. A generated response does not, unless it actually resolves something. This distinction separates outcome pricing from consumption pricing wearing a different label. If your "outcome" is "the AI produced an output," you have consumption pricing with extra steps. If your outcome is "the AI achieved the result the customer hired it to achieve," you have genuine outcome pricing. The test is whether the customer can point to the outcome and say: "That thing saved me money, time, or risk." If they cannot, your outcome is not an outcome.

Third, the outcome must have a **clear boundary in time**. When does the outcome period end? If a customer asks a support question and the AI responds, but the customer comes back two days later with a follow-up, is that the same outcome or a new one? Intercom uses a twenty-four-hour inactivity window: if the customer does not respond within twenty-four hours, the conversation is closed and the resolution is counted. A new message starts a new conversation and potentially a new billable resolution. This is a business decision, not a technical one. Shorter windows mean more billable resolutions and higher revenue but potentially lower customer satisfaction if follow-ups are treated as new charges. Longer windows mean fewer billable resolutions but a more generous definition that builds customer trust.

Fourth, the outcome must be **auditable by the customer**. Customers paying per outcome need to be able to review what they were charged for. This means providing a log of every billable event with enough context for the customer to evaluate whether the outcome actually occurred. A customer who sees 10,000 resolutions on their invoice should be able to drill into any individual resolution, see the conversation, see the AI's response, and verify that the interaction was genuinely resolved. If the invoice is a black box, trust erodes. If the invoice is transparent, the customer becomes an ally in improving the outcome definition — they flag the edge cases, suggest refinements, and help you calibrate the model because their money depends on accuracy.

Fifth, the outcome must be **economically viable to produce across the full distribution of complexity**. This is the principle that separates aspiration from sustainable business, and it leads directly to the hardest problem in outcome-based pricing: margin variance.

## The Customer Support Case: Why the Numbers Work

Customer support is the proving ground for outcome-based pricing because the economics are transparent and the comparison is direct. A human support agent in the United States costs between $45,000 and $65,000 per year in base salary. Add benefits, payroll taxes, management overhead, workspace costs, training, and the portion of time agents spend in meetings, breaks, and non-ticket work, and the fully loaded cost per productive agent is between $75,000 and $110,000 per year. An agent handling thirty to forty tickets per day resolves roughly seven thousand to nine thousand tickets per year. That puts the cost-per-human-resolution at roughly $8 to $15, depending on the company's labor market and ticket complexity.

An AI resolution at $0.99 is seven to fifteen times cheaper. Even if the AI only resolves sixty percent of incoming tickets and the remaining forty percent still require human agents, the blended cost drops dramatically. A company handling 100,000 tickets per month with sixty percent AI resolution and forty percent human resolution spends roughly $59,400 on AI resolutions and $160,000 on human agent costs for the remaining tickets, for a blended cost of $219,400. Without AI, the same 100,000 tickets at $10 per human resolution would cost $1,000,000. The savings are $780,600 per month, or $9.4 million per year. The AI vendor captures $712,800 per year in resolution fees. The customer captures $8.7 million in net savings. Both sides win by a large margin.

These numbers explain why outcome-based pricing adoption is accelerating in customer support faster than in any other AI use case. The comparison is concrete. The math is simple. The ROI is undeniable. A CFO does not need a slide deck to understand that $0.99 is less than $10.

## Beyond Customer Support: Where Outcome Pricing Extends

Customer support is the first beachhead, but outcome-based pricing is expanding into domains where the outcome can be defined with similar precision. AI-powered legal document review charges per analyzed contract or per identified risk clause. AI-driven recruitment tools charge per qualified candidate surfaced or per completed screening. AI code review products are beginning to experiment with per-reviewed-pull-request pricing. AI sales tools charge per meeting booked or per qualified lead generated.

Each of these extensions follows the same structural requirement: the outcome must be observable, meaningful, time-bounded, auditable, and economically viable. The domains where outcome pricing struggles are the ones where the outcome is diffuse or subjective. An AI writing assistant that helps someone draft better emails creates real value, but "better email" is not an observable, auditable outcome. An AI analytics tool that surfaces insights from data creates real value, but "useful insight" is not a billable event with a clear boundary. These products default to consumption or seat pricing not because outcome pricing would not be better for alignment, but because the outcome cannot be defined precisely enough to bill against.

The strategic question for every AI company in 2026 is whether their product's value can be expressed as a discrete, observable outcome. If it can, outcome-based pricing offers the most powerful alignment model available. If it cannot today, the question becomes whether you can redesign your product's interface or workflow to create discrete outcome events that become the charge metric. Some companies are discovering that the effort to define billable outcomes forces them to sharpen their product's value proposition in ways that benefit the entire business — because if you cannot define what your product achieves, pricing is the least of your problems.

## The Vendor Psychology Shift

Adopting outcome-based pricing requires a psychological shift that many founding teams underestimate. In every other pricing model, the vendor earns revenue as a function of inputs: seats provisioned, tokens consumed, platform fees collected. Revenue is predictable because it is disconnected from product performance. A seat-priced product earns the same revenue whether the customer uses it once a day or never. A consumption-priced product earns revenue whether the AI output is useful or garbage.

Outcome-based pricing strips away that cushion. Revenue is a direct function of product quality. If your AI gets worse, your revenue declines — not next quarter when the customer churns, but this month when fewer interactions produce billable outcomes. If your AI gets better, your revenue increases — not through a pricing change, but through organic growth in billable outcomes as the product resolves more interactions successfully. This creates a feedback loop that is both powerful and terrifying: engineering investment in quality has an immediate, measurable revenue impact, and engineering neglect has an immediate, measurable revenue cost.

This feedback loop changes how the entire company operates. Product teams prioritize reliability over features, because a feature that improves resolution rate by two percentage points generates more revenue than a feature that looks impressive in a demo but does not affect outcomes. Engineering teams treat regressions as revenue events, not just quality events, because a model update that drops resolution rate from sixty-five percent to sixty percent represents a seven-and-a-half percent revenue decline. Sales teams sell with confidence because they are not asking customers to trust a projection — they are offering a guarantee backed by a pricing model that only charges when value is delivered.

The companies that thrive under outcome-based pricing are the ones that embrace this exposure. They build their entire operating model around outcome measurement, outcome improvement, and outcome reliability. The companies that struggle are the ones that adopt the pricing model for its sales appeal without internalizing the operational discipline it demands.

But aligning incentives is only half the equation. The other half is economics — specifically, what happens to your margins when some outcomes cost three cents to produce and others cost three dollars. That margin variance is the subject of the next subchapter, and it is where most outcome-based pricing models either mature into sustainable businesses or collapse under their own cost structure.