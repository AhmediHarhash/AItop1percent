# 30.6.6 — The Batch, Cache, and Route Trifecta: Cost Levers That Change Your Pricing Math

Three cost levers exist that can reduce your per-query AI cost by fifty to ninety percent. Not five percent. Not ten percent. Fifty to ninety. These are not theoretical optimizations or speculative improvements — they are production-proven techniques that the most cost-efficient AI companies in 2026 use as standard infrastructure. If you are pricing your AI product based on your unoptimized cost-to-serve, you are either overcharging your customers or destroying your own margins, depending on which direction your pricing error points. The trifecta of batch processing, prompt caching, and model routing does not just reduce costs. It fundamentally changes the math that your pricing model sits on.

Section 24 covers the technical implementation of each lever in detail. This subchapter is not about how to build them. It is about what they mean for your pricing strategy — how each lever changes your cost structure, when you should reflect those savings in customer pricing, and how to model the financial impact on your margins.

## Batch Processing: The Fifty Percent Discount You Are Probably Ignoring

**Batch processing** is the simplest of the three levers. Instead of sending each query to the model as it arrives in real time, you collect queries into groups and submit them together during off-peak hours. Most major API providers offer a flat fifty percent discount on batch requests because batch processing lets them schedule workloads during periods of low demand, improving their GPU utilization. OpenAI's Batch API, Anthropic's batch endpoints, and Google's batch inference all offer this discount structure in 2026, typically with a completion window of up to twenty-four hours.

The constraint is obvious: batch processing only works for workloads that do not require an immediate response. A customer-facing chatbot cannot batch its replies — the user is waiting. But an enormous number of AI workloads are not real-time. Nightly content moderation sweeps. Weekly report generation. Daily customer sentiment analysis across support tickets. Batch classification of incoming documents. Periodic summarization of meeting transcripts. Any workflow where the result is consumed hours or days after the input is generated is a batch candidate.

The pricing implication is significant. If forty percent of your product's query volume is batch-eligible, and batch processing cuts that portion's cost in half, your blended cost-to-serve drops by twenty percent. That twenty percent can flow to your margin, to your customer's price, or to some combination of both. But here is the mistake most teams make: they batch their workloads to save on infrastructure costs but never update their pricing model to reflect the new cost structure. They continue pricing as if every query costs the real-time rate, which means their margins quietly expand by twenty percent. That sounds good until a competitor who is also batching passes those savings to their customers and undercuts your price.

The strategic question is not whether to batch. It is whether your product architecture separates batch-eligible workloads from real-time workloads cleanly enough to capture the discount systematically. If your system sends every request through the same real-time pipeline, you are paying the premium rate for queries that could have waited. The architecture decision is a pricing decision.

For products where a large portion of the workload is batch-eligible, consider pricing that explicitly distinguishes between real-time and batch tiers. A customer who needs instant responses pays the real-time rate. A customer who can wait until morning for their analysis results pays a lower rate that reflects the batch discount you receive from your provider. This tiered approach is honest about the cost structure and gives the customer a clear trade-off: speed versus price.

## Prompt Caching: The Lever Most Teams Discover Too Late

**Prompt caching** exploits a pattern that exists in virtually every AI product but that most teams never measure: repetition in the prompt. Every request to your AI system includes a prompt — the system instructions, the context, the formatting requirements, the examples — plus the user's actual input. In most products, the prompt is largely identical across requests. The system instructions are the same for every query. The few-shot examples are the same. The formatting rules are the same. Only the user's input and perhaps some retrieved context changes from request to request.

Without caching, your model provider processes the full prompt on every request, and you pay full price for every token — including the tokens that are identical to the previous request. With caching, the provider recognizes that the prefix of your prompt has not changed since the last request and reuses its internal computation from the previous call. Anthropic's prefix caching, for example, charges only ten percent of the standard input token rate for cached tokens. The discount is ninety percent on the cached portion of the prompt.

The math is dramatic. Consider a product where each request includes a 3,000-token system prompt and the user's input averages 500 tokens. Without caching, you pay for 3,500 input tokens per request. With caching, you pay full price for the 500 user-specific tokens and ten percent of the price for the 3,000 cached tokens. If the standard rate is three dollars per million input tokens, the uncached cost is $0.0105 per request. The cached cost is $0.0015 for the user tokens plus $0.0009 for the cached tokens — $0.0024 per request. That is a seventy-seven percent reduction in input token costs on every single request after the first.

The reduction gets more dramatic as your system prompt gets longer. Products that use long context windows — those that stuff retrieval results, conversation history, or extensive instructions into the prompt — benefit most from caching because the cached portion is a larger share of the total token count. A RAG system that retrieves five thousand tokens of context, prepends a two-thousand-token system prompt, and then appends a five-hundred-token user query sees caching reduce input costs by more than eighty percent once the system prompt is cached.

The pricing implication of caching is that your cost-to-serve the second query is dramatically lower than your cost-to-serve the first query. If you are pricing based on average cost across all queries, you are underpricing the first query and overpricing every subsequent query. This matters most for products with high session volumes — where users send many sequential requests in a single session — because the cache hit rate is highest when requests come in rapid succession. A customer who sends fifty queries in an hour might have a ninety-five percent cache hit rate. A customer who sends one query per day has no meaningful caching benefit because the cache expires between sessions.

This usage pattern creates a counterintuitive pricing dynamic: your heaviest users are your cheapest to serve on a per-query basis, because they generate the highest cache hit rates. If your pricing model charges a flat per-query rate, your highest-volume customers are your most profitable — not because they pay more, but because they cost less. Understanding this dynamic lets you offer volume discounts that actually improve your margin, which is the rare pricing structure where both you and the customer win.

## Model Routing: The Lever That Changes Everything

**Model routing** is the most powerful cost lever because it addresses the fundamental inefficiency in most AI products: using the same expensive model for every request, regardless of difficulty. In 2026, the cost difference between the cheapest and most expensive models from a single provider can be ten to twenty-five times. GPT-5-nano costs a fraction of GPT-5. Claude Haiku 4.5 costs a fraction of Claude Opus 4.6. Gemini 3 Flash costs a fraction of Gemini 3 Deep Think. If you route every request to the expensive model, you are paying frontier prices for tasks that a model costing one-twentieth as much could handle just as well.

Model routing works by classifying each incoming request by complexity and directing it to the cheapest model that can handle it at your quality threshold. A simple factual question, a straightforward classification, or a standard template-based response does not need your most capable model. A complex multi-step reasoning task, an ambiguous query requiring nuanced judgment, or a high-stakes response where accuracy is critical does. The router sits between your application and your model providers, making a split-second decision about which model handles each request.

The cost impact is substantial. In a typical AI product, the distribution of query complexity follows a pattern where sixty to seventy-five percent of queries are simple enough for the cheapest model tier, fifteen to twenty-five percent require a mid-tier model, and five to fifteen percent genuinely need the frontier model. If your frontier model costs fifteen dollars per million output tokens and your cheapest model costs seventy-five cents per million output tokens, routing seventy percent of traffic to the cheap model reduces your blended cost by more than sixty percent compared to sending everything to the frontier model.

Section 24 covers the technical implementation of routing — how to build the classifier, how to set quality thresholds, how to handle edge cases where the router sends a query to a model that cannot handle it. Here, the focus is on what routing means for pricing.

Routing creates a **variable cost structure** where your per-query cost depends on the complexity distribution of each customer's usage. A customer whose workload is eighty percent simple queries has a blended cost-to-serve that is dramatically lower than a customer whose workload is fifty percent complex queries. If you charge both customers the same flat per-query rate, you are cross-subsidizing — the simple-workload customer subsidizes the expensive-workload customer.

This cross-subsidy is a pricing risk. The simple-workload customer is overpaying relative to your cost-to-serve, which makes them vulnerable to a competitor who charges less because they price based on actual complexity distribution. The complex-workload customer is underpaying, which drags down your gross margin. As your customer base grows, the mix of simple-to-complex workloads across customers creates margin volatility that becomes harder to predict and manage.

The two responses to this are complexity-adjusted pricing and blended pricing with margin buffer. **Complexity-adjusted pricing** charges different rates for different query types — a lower rate for simple queries, a higher rate for complex ones. This is the most honest approach and the most operationally challenging, because you need to classify each query's complexity and communicate that classification transparently to the customer. **Blended pricing with margin buffer** charges a single per-query rate that assumes a worst-case complexity distribution — say, fifty percent frontier model usage — and captures the upside when the actual distribution is more favorable. Blended pricing is simpler but leaves money on the table when you have simple-heavy customers, and it overcharges them relative to a competitor who uses complexity-adjusted rates.

## The Trifecta in Combination

The three levers compound. Batch processing cuts batch-eligible costs by fifty percent. Prompt caching cuts input token costs by sixty to ninety percent on cached portions. Model routing cuts blended model costs by forty to seventy percent. Applied together to a workload where forty percent is batch-eligible, eighty percent of input tokens are cacheable, and seventy percent of queries route to the cheapest model, the total cost reduction can exceed eighty percent compared to an unoptimized baseline.

Consider a concrete example. An AI product processes two million queries per month at an unoptimized cost of $0.03 per query — a monthly spend of $60,000. After optimization: 800,000 batch-eligible queries drop from $0.03 to $0.015 each, saving $12,000. Prompt caching across all queries reduces input token costs by seventy-five percent on the cached portion, saving an additional $9,000. Model routing sends 1.4 million simple queries to a model costing one-fifteenth as much, saving roughly $22,000. The optimized monthly cost is approximately $17,000 — a seventy-two percent reduction from the unoptimized baseline.

That $43,000 per month in savings — over $500,000 per year — is the gap between your unoptimized cost structure and your optimized cost structure. Your pricing strategy determines who captures that gap.

## How the Trifecta Changes Your Pricing Decisions

The trifecta creates a decision that every AI product company must make: do you price based on your unoptimized cost, your optimized cost, or something in between?

Pricing based on unoptimized cost means your margins are enormous once you deploy the optimizations, but your price is higher than it needs to be. You are vulnerable to a competitor who optimizes and passes the savings through. Pricing based on optimized cost means your price is lower and more competitive, but your margin is razor-thin and any increase in costs — a provider price hike, a shift toward more complex queries, a cache miss rate increase — could push you into negative margin territory.

The right answer is to price based on your optimized cost with a reasonable margin buffer, and to treat any additional optimization gains as margin improvement rather than price reduction. Set your price assuming you will achieve the trifecta optimizations. Build in a margin that accounts for the fact that optimization levels fluctuate — cache hit rates are not perfectly stable, routing accuracy is not one hundred percent, and batch eligibility varies by customer. Then, as your optimizations improve over time, your margins improve with them. You do not lower prices every time your engineering team finds another five percent of cost savings. You bank those savings as margin improvement until competitive pressure forces you to adjust.

This approach gives you a competitive price — because it reflects your actual cost structure, not the inflated unoptimized cost. It gives you healthy margins — because the buffer accounts for variability. And it gives you a margin expansion path — because engineering improvements flow to your bottom line, not immediately to your customer's price.

The question of when and how to pass cost savings to customers versus capturing them as margin is not a simple one. It depends on competitive dynamics, customer expectations, and your current margin position. That decision framework is the subject of the next subchapter.
