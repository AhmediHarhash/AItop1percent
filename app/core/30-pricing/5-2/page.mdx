# 30.5.2 — SLA Design for AI Products: Promising Quality When Quality Varies

In early 2025, a healthcare technology company signed an enterprise contract with a major hospital network for an AI-powered clinical documentation assistant. The sales team, eager to close a seven-figure deal, agreed to an SLA that promised "ninety-nine percent accuracy on clinical note generation." The language felt reasonable — it was modeled on uptime SLAs that the company's legal team had used for years. The product was performing well in demos. Internal benchmarks showed accuracy above ninety-five percent across test sets. Ninety-nine percent seemed ambitious but achievable. Within four months, the hospital's quality team flagged the AI for producing subtly incorrect medication dosages in three percent of generated notes, misattributing symptoms to the wrong patient in one point five percent of multi-patient summaries, and using outdated clinical terminology in roughly two percent of oncology-related documentation. By any reasonable standard, the product was performing well — better than many human scribes on most metrics. But the contract did not say "better than human scribes." It said "ninety-nine percent accuracy." The hospital's legal team sent a formal breach notice. The healthcare company spent the next six months in a remediation process that consumed $380,000 in engineering time, required two model upgrades, and nearly cost them the account. The product was never the problem. The SLA was.

This story repeats across the AI industry because teams design SLAs using a framework built for a fundamentally different kind of product. Traditional SaaS SLAs promise that the system will be available. AI SLAs must also promise that the system will be good. And "good" in a probabilistic system is a much harder thing to define, measure, and guarantee than "available."

## Why Traditional SLAs Break for AI

The traditional SLA framework has one primary metric: **uptime**. The system will be available ninety-nine point nine percent of the time, measured on a monthly basis. If availability falls below the threshold, the customer receives a service credit — typically five to fifteen percent of the monthly fee for each tier below the committed level. This framework works because availability is binary. The system is either responding to requests or it is not. Measurement is unambiguous. Breaches are obvious. Credits are calculated automatically.

AI products need this availability layer, but they also need something that traditional software never required: a quality layer. The clinical documentation assistant in the opening story was available one hundred percent of the time during the period in question. The infrastructure never went down. Every request received a response within the latency threshold. By the traditional SLA framework, the product was performing flawlessly. The problem was that the responses, while prompt and consistent, were sometimes wrong. An AI product that is always available but produces inaccurate outputs at unacceptable rates is not meeting its obligations — but a traditional SLA has no mechanism to capture that failure.

The core challenge is that quality in AI systems is inherently statistical. You cannot guarantee that a language model will never hallucinate, any more than you can guarantee that a weather forecast will never be wrong. What you can guarantee is that the model will hallucinate within defined bounds, that accuracy will exceed defined thresholds over defined measurement windows, and that when quality degrades, there are detection mechanisms and remediation processes in place. The SLA must be designed around this statistical reality rather than around the deterministic framework inherited from traditional software.

## The Three-Layer SLA

The most effective AI SLA structure uses three layers, each governing a different dimension of product performance. Together they provide comprehensive coverage without creating impossible-to-meet commitments.

**Layer One is the Availability SLA.** This is the familiar uptime commitment. The AI system will be available and responsive for a defined percentage of each measurement period. For most enterprise AI products, the threshold is ninety-nine point five to ninety-nine point nine percent monthly availability. This layer covers infrastructure reliability: servers are running, APIs are responding, the system is accepting and processing requests. The measurement is straightforward — monitoring tools track whether the system responds to health checks and whether API calls receive responses within the timeout window. Service credits for availability breaches follow the standard SaaS model. This layer is necessary but not sufficient. It tells the customer the system is on. It says nothing about whether the system is performing well.

**Layer Two is the Performance SLA.** This covers latency and throughput — how fast the system responds and how many concurrent requests it handles. AI products have latency characteristics that differ from traditional SaaS because inference time varies with input length, model complexity, and query type. A simple classification request might return in 200 milliseconds. A complex document analysis with a large context window might take four to eight seconds. An agent workflow with multiple model calls and tool use might take fifteen to thirty seconds. The Performance SLA must define latency commitments at the percentile level rather than as simple averages. The SLA might commit to P50 latency of under two seconds, P95 latency of under five seconds, and P99 latency of under twelve seconds for a document analysis use case. Percentile-based commitments are more honest than average-based commitments because they acknowledge the tail — the small percentage of requests that take significantly longer than typical — while still providing meaningful guarantees for the vast majority of interactions.

**Layer Three is the Quality SLA.** This is the layer that has no precedent in traditional software and where most AI contracts either overcommit or undercommit. The Quality SLA defines the accuracy, reliability, and correctness of the AI system's outputs — the dimension that matters most to the customer and is hardest to guarantee. Getting this layer right is the difference between a contract that builds trust and a contract that creates disputes.

## Designing the Quality SLA

The Quality SLA must answer four questions with precision. What are we measuring? How are we measuring it? Over what time window? And what happens when the measurement falls below the threshold?

The first question — what to measure — depends on the use case. A customer service AI might measure resolution accuracy, which is the percentage of AI-resolved tickets that the customer later marks as resolved correctly. A document analysis AI might measure extraction accuracy, which is the percentage of data fields extracted correctly from a document set. A clinical documentation AI might measure clinical accuracy, which is the percentage of generated notes that pass a structured quality audit by a clinical reviewer. The critical principle is that the quality metric must be specific to the use case, measurable by both parties, and defined precisely enough that a third-party auditor could reproduce the measurement.

Do not use generic metrics like "overall accuracy" or "AI quality score" in the SLA. These are too vague to be enforceable and too broad to be meaningful. The customer service AI that achieves ninety-two percent resolution accuracy but only seventy percent accuracy on billing disputes has a quality profile that the single number obscures. If billing disputes are the customer's primary use case, the aggregate metric hides the failure that matters most. Quality SLAs should be scoped to the specific tasks and domains that the customer contracted for, with separate thresholds for each if the performance profile varies significantly across task types.

The second question — how to measure — requires a jointly defined evaluation methodology. Who reviews the outputs? What constitutes a correct versus incorrect output? How are edge cases handled? The strongest approach is a **shared evaluation framework** where both vendor and customer agree on the evaluation rubric before the contract is signed. The rubric defines the rating scale, provides calibration examples for each rating level, and specifies the sample size and sampling methodology for periodic quality audits. Some enterprises use random sampling of one to five percent of AI outputs, reviewed by trained evaluators on a monthly cadence. Others use automated evaluation pipelines that flag outputs matching known failure patterns. The best contracts specify both a primary evaluation method and a fallback dispute resolution method — typically a joint review panel — for cases where the parties disagree on whether a specific output meets the quality threshold.

The third question — measurement window — is more consequential than most teams realize. A quality threshold measured daily is a fundamentally different commitment than the same threshold measured quarterly. Daily measurement is volatile: a single bad batch of inputs, a temporary data pipeline issue, or an unusual distribution of edge-case queries can push the daily measurement below threshold even when the system is performing well on average. Quarterly measurement is stable but slow: a genuine quality degradation might persist for weeks before it triggers an SLA breach, exposing the customer to prolonged periods of substandard performance without contractual remedy.

The recommended approach is a **rolling window** — typically fourteen to thirty days — that smooths daily volatility while still detecting sustained degradation within a reasonable timeframe. The SLA might state: "Accuracy as measured by the agreed evaluation methodology will meet or exceed the threshold on a rolling thirty-day basis, measured weekly." This gives the customer assurance that quality is consistently monitored while giving the vendor protection against statistical noise in short measurement windows.

## What Happens When Quality Falls Below Threshold

The fourth question — remediation — is where most AI contracts are weakest. Traditional SaaS contracts handle availability breaches with service credits: if uptime drops below threshold, the customer gets a percentage of the monthly fee back. This works for availability because the breach is clear-cut and the credit is simple to calculate. Quality breaches are more nuanced, and the remediation must match that nuance.

The best AI contracts use a **tiered remediation structure**. The first tier is a notification and investigation period. When quality falls below the SLA threshold in a measurement window, the vendor is obligated to acknowledge the breach, investigate the root cause, and provide a remediation plan within a defined timeframe — typically five to ten business days. This is not a penalty. It is an operational response that acknowledges the issue and demonstrates that the vendor is treating quality as seriously as the customer does.

The second tier activates if quality remains below threshold for a sustained period — typically two to three consecutive measurement windows. At this point, service credits apply. The credit structure for quality SLAs should be more generous than for availability SLAs because quality failures are harder for the customer to detect and can cause downstream damage before they are caught. A five to ten percent monthly credit for each measurement period in breach is typical for quality SLAs, compared to two to five percent for availability SLAs.

The third tier activates if quality remains below threshold for an extended period — typically four to six consecutive measurement windows. At this point, the customer gains the right to terminate the contract without penalty. This is the ultimate backstop, and it protects both parties: the customer is not locked into a product that is not meeting its quality commitments, and the vendor has a clear timeline and a structured escalation path rather than facing an angry customer with no contractual remedy. The termination right is not something you hope to use. It is something whose existence prevents the scenario that would trigger it — because the vendor knows the clock is ticking and the customer knows there is a defined path to resolution.

## The Hallucination Rate Ceiling

One quality metric deserves special attention because it is unique to AI and because getting it wrong creates disproportionate risk: the **hallucination rate ceiling**. A hallucination rate ceiling defines the maximum acceptable percentage of outputs that contain fabricated, unsupported, or factually incorrect content as determined by the agreed evaluation methodology.

Setting the right ceiling depends on the use case and the consequences of a hallucination. For customer service applications, a hallucination rate ceiling of three to five percent may be acceptable because human agents review escalated cases and the cost of an incorrect response is typically a poor customer experience, not physical or financial harm. For legal document analysis, the ceiling should be below two percent because hallucinated legal citations or misinterpreted contract clauses can lead to material business decisions based on incorrect information. For clinical or financial applications where incorrect outputs can cause direct harm, the ceiling should be below one percent and must be paired with mandatory human review processes defined in the contract.

The key principle is that the hallucination rate ceiling must be achievable with current model capabilities while still being meaningfully protective for the customer. Industry data from 2025 shows that well-optimized retrieval-augmented generation systems achieve hallucination rates between one and three percent on domain-specific tasks, while general-purpose models without retrieval augmentation can hallucinate at five to fifteen percent rates depending on the domain and query complexity. Promising a ceiling below what your system can reliably achieve is not ambition — it is a future breach waiting to happen.

## Common SLA Design Mistakes

Three mistakes appear repeatedly in enterprise AI contracts, and each one creates problems that are expensive to fix after the contract is signed.

The first is promising quality on tasks the customer has not yet defined. The contract might say "ninety-five percent accuracy on all document analysis tasks" — but the customer later starts using the AI for a document type that was not in the training data or the evaluation set. Quality drops. The customer invokes the SLA. You argue the new document type was not covered. They argue the SLA says "all document analysis tasks." The dispute consumes months and poisons the relationship. The fix is to define the covered scope precisely in the SLA — list the document types, query categories, or task domains that the quality commitment applies to, and specify a process for adding new scopes that includes evaluation, threshold setting, and mutual agreement.

The second mistake is measuring quality without the customer's agreement on methodology. You run your internal evaluation and report ninety-three percent accuracy. The customer runs their own evaluation and reports eighty-one percent accuracy. Both numbers are correct — they just used different evaluation rubrics, different sample sets, and different definitions of "correct." The dispute is not about quality. It is about measurement. The fix is to define the evaluation methodology in the contract with enough specificity that both parties will arrive at the same number when measuring the same set of outputs.

The third mistake is treating quality SLAs as static. The quality capabilities of AI systems improve rapidly. A hallucination rate ceiling that was challenging to meet in 2024 may be easily achievable with 2026 models. If the SLA thresholds never tighten, the customer is paying premium prices for quality commitments that have become trivially easy for the vendor to meet. The best contracts include a **quality ratchet** — an annual review of quality thresholds where thresholds are adjusted upward to reflect improvements in the state of the art. This is good for the customer because they get better quality over time. It is good for the vendor because it demonstrates continuous improvement and justifies the ongoing relationship. The ratchet should be negotiated, not automatic — a three-to-five percentage point annual improvement in accuracy thresholds is typical, but the specific adjustment should reflect the pace of improvement in the relevant domain.

## SLAs as Trust Architecture

The SLA is not just a legal backstop. It is a trust architecture. When you define quality metrics precisely, commit to measurable thresholds, and structure a fair remediation process, you are telling the customer something important: you stand behind your product's quality, you have the operational maturity to measure and maintain it, and you have a plan for what happens when things go wrong. This confidence closes deals. Vague quality promises — "best-in-class AI" or "industry-leading accuracy" — signal the opposite: that you have not thought through quality measurement, that you do not track the metrics closely enough to commit to them, or that you know the numbers and are afraid to put them in writing.

The enterprise buyer in 2026 has been burned enough by AI hype to demand specifics. The vendor who provides those specifics with confidence — backed by a well-designed three-layer SLA — wins the deal over the competitor who hedges, deflects, and asks the customer to "trust us." Quality SLAs are where trust gets codified.

With quality, performance, and availability guaranteed in the SLA, the next question is revenue predictability: how do you structure volume commitments and minimum spend clauses that protect your economics without constraining the customer's flexibility?
