# 30.6.9 — The Race to Zero: Why AI Commoditization Destroys Weak Pricing Models

In early 2025, a mid-stage AI startup serving the commercial real estate industry had what looked like a defensible business. Their product analyzed property listings, lease agreements, and market comparables using a fine-tuned model running on a major provider's API. They priced at roughly three times their API cost — a markup that covered engineering, support, sales, and left them with a healthy fifty-eight percent gross margin. Customers paid $1,800 per seat per month. The API cost per seat was approximately $600. The math was clean, the growth was strong, and the board deck told a story of expanding margins at scale. Eleven months later, the provider dropped pricing on the model family they depended on by sixty percent. A competitor who had launched six months behind them adopted the cheaper model, matched their feature set with eighty percent fidelity, and priced at $700 per seat. The startup's customers — commercial real estate firms with thin margins and aggressive procurement teams — started asking pointed questions. Not about quality, not about features. About price. Within two quarters, the startup had cut pricing to $1,100 per seat to stop the bleeding, watched their gross margin collapse to thirty-one percent, and burned through a quarter of their remaining runway repricing every contract in their pipeline. Their business model had not failed because of bad execution. It had failed because the floor dropped out from under their cost basis, and their pricing had been anchored to a floor that was always falling.

This is **The Race to Zero** — the commoditization spiral where declining AI inference costs compress the margins of every company whose pricing is tethered to the raw cost of compute. It is the single most important structural force shaping AI business economics in 2026, and the companies that do not understand it will price themselves into irrelevance.

## The Mathematics of Falling Costs

The decline in AI inference costs is not gradual. It is exponential, and the rate is accelerating. Research from Epoch AI tracking inference prices across equivalent quality levels found that the cost to match a given performance benchmark falls by roughly ten to fifty times per year, with a median of approximately fifty times per year. Data from the second half of 2024 through 2026 shows the fastest trends accelerating further, with median declines reaching two hundred times per year when measured against fixed performance thresholds. To put this in concrete terms: if it cost you $1 per thousand queries to achieve a certain quality level in January 2025, you could achieve that same quality level for two cents per thousand queries by January 2026. And by January 2027, the cost might be a fraction of a cent.

These declines come from three simultaneous forces. First, algorithmic improvements make models more efficient at inference time — techniques like speculative decoding, quantization, and architecture optimizations reduce the compute required per token without reducing quality. Second, hardware improvements deliver more inference compute per dollar as NVIDIA, AMD, and custom silicon from Google and Amazon increase throughput per chip. Third, competitive pressure among providers drives aggressive pricing — OpenAI, Anthropic, Google, Meta, Mistral, and DeepSeek are all racing to capture market share in a winner-takes-most market, and pricing is one of the most visible competitive levers. When DeepSeek launched models matching proprietary performance at a fraction of the price in early 2025, it forced every major provider to accelerate their pricing timeline. The result is a market where the price of intelligence falls faster than the price of almost any other input in the history of technology.

## Why Cost-Anchored Pricing Fails

The traditional SaaS model taught companies to price based on value delivered, not cost incurred. But many AI companies — especially those that launched quickly during the 2023-2024 gold rush — priced based on a multiple of their API costs because it was the simplest path to a margin structure that investors would accept. Three times cost. Five times cost. Some benchmarked against the API pricing page and added a percentage. This worked when API costs were high enough that the absolute dollar margin was meaningful. It fails catastrophically when those costs crater.

The failure mechanism is straightforward. Your price is a multiple of your cost. Your cost drops by eighty percent. If you maintain your price, your gross margin improves dramatically — but only until a competitor adjusts their price to reflect the new cost floor. Once that competitor drops their price, your customers see the gap between what they pay you and what the compute actually costs, and the value conversation becomes a cost conversation. You can hold pricing for one quarter, maybe two, if your product is differentiated enough. But if your primary differentiation is "we put a nice interface on top of an API," the hold is temporary. The competitor does not need to match your quality. They need to be close enough at a price low enough that the savings justify the quality gap.

This is Layer 4 of the AI Pricing Stack — the **Competitive Anchor**. Your pricing ceiling is not set by the value you deliver. It is set by the cheapest acceptable alternative your customer can find, and in a commoditizing market, that alternative gets cheaper every quarter. The companies that anchored pricing to API cost multiples are now trapped: they can cut prices to match the falling floor, which destroys margin, or they can hold prices above the falling floor, which invites competition. Neither option is survivable long-term without a fundamental shift in what you are selling.

## The Commoditization Spiral

The Race to Zero creates a specific pattern of competitive destruction that repeats across every market segment where AI is the primary product.

Phase one is **margin compression**. The first wave of cost declines reduces your input costs, which initially improves margins. But competitors see the same cost declines and lower their prices. The market price drops toward the new cost floor, and margins return to where they started — or worse, because the price war attracts new entrants who are willing to operate at lower margins to gain share.

Phase two is **feature convergence**. As model capabilities improve and costs drop, the baseline level of quality available at low cost rises rapidly. Features that differentiated premium products twelve months ago become table stakes. The summarization quality that justified your premium in 2024 is now available from any model at one-tenth the cost. The analysis accuracy that set you apart is matched by a competitor using a newer, cheaper model with better default performance. Your differentiation window shortens with every model generation.

Phase three is **buyer sophistication**. As the market matures, buyers learn to benchmark. Enterprise procurement teams build internal cost models that track API pricing. They know what a query costs at the API level, and they use that knowledge to negotiate your price down toward the compute cost. The information asymmetry that allowed early AI companies to price with comfortable margins evaporates as buyers gain visibility into the underlying cost structure.

Phase four is **consolidation or death**. Companies without structural differentiation beyond the model layer are forced to consolidate, pivot, or shut down. Industry data shows that forty-two percent of companies abandoned most of their AI initiatives in 2025, up from seventeen percent in 2024. The MIT Sloan Management Review reported that roughly five percent of AI pilot programs achieve rapid revenue acceleration, while the vast majority stall. The Race to Zero is a significant contributor to this mortality — not because the technology fails, but because the business model does.

## What Survives Commoditization

The companies that survive and thrive despite the Race to Zero share a common trait: they sell something that cannot be replicated by switching to a cheaper model. They have decoupled their pricing from compute cost by building value that exists independently of which model runs underneath.

**Domain expertise** survives commoditization because deep knowledge of a specific industry cannot be downloaded from an API. Harvey has built a legal AI business valued at $11 billion in 2026 not because it uses a better model than its competitors — it uses the same foundation models available to everyone — but because it has spent years building domain-specific capabilities that generic AI cannot match. Its custom legal embeddings, trained on over twenty billion tokens of legal text, deliver twenty-five percent fewer irrelevant search results than off-the-shelf alternatives. Its exclusive integration with LexisNexis, announced in mid-2025, gives it access to one of the two essential proprietary US legal research libraries. A competitor cannot replicate this by calling a cheaper API. The domain expertise is the product. The model is the engine.

**Workflow integration** survives commoditization because switching costs are measured in disruption, not dollars. Glean has built an enterprise search and knowledge management product that sits inside the workflows of hundreds of enterprise customers. The product is deeply integrated with email systems, document repositories, internal wikis, project management tools, and communication platforms. The AI capabilities are powered by models that could be replaced with cheaper alternatives, but the workflow integration — the connections, the permissions, the indexing, the contextual understanding of each organization's information architecture — cannot be replaced by switching models. The customer's cost of switching away from Glean is not the cost of finding a cheaper AI. It is the cost of re-integrating every knowledge source, retraining every user, and rebuilding every custom workflow.

**Proprietary data** survives commoditization because data that is exclusive to your product creates output quality that no competitor can match regardless of which model they use. Bloomberg built its terminal business on proprietary financial data that no competitor could access. Bloomberg's AI capabilities leverage this same data advantage — their models are trained on and retrieve from datasets that simply do not exist in any other product. A competitor using a cheaper, better model with public data still produces inferior outputs for the specific financial analysis tasks that Bloomberg's customers depend on.

**Outcome-based pricing** survives commoditization because it decouples your revenue from compute cost entirely. When you charge per successful outcome — per qualified lead generated, per contract clause reviewed, per compliance violation detected — the falling cost of inference is your friend, not your enemy. Your price stays the same because the value of the outcome stays the same, while your cost-to-deliver that outcome drops with every model generation and every provider price cut. Your margins expand as costs fall instead of compressing. The challenge of outcome-based pricing — defining outcomes, measuring them, handling edge cases — is real, but the structural advantage against commoditization is enormous.

## The Strategic Response: Decoupling Price from Compute

If your current pricing is anchored to API costs, you need a migration plan. The migration does not happen overnight, but the direction must be deliberate and the timeline must be measured in quarters, not years, because the Race to Zero does not pause while you figure out your strategy.

The first step is auditing your current price-to-cost relationship. For every product tier and every customer segment, calculate what percentage of your price is justified by compute cost versus what percentage is justified by non-compute value — domain expertise, workflow integration, support, compliance, proprietary data, speed of iteration. If compute cost justifies more than forty percent of your current price, you are dangerously exposed to the Race to Zero. Every ten-times reduction in API costs erodes forty percent of your pricing justification.

The second step is investing in non-compute value creation. This means different things for different businesses. For vertical AI companies, it means deepening domain expertise — more training data, more domain-specific evaluation, more partnerships with industry-specific data providers, more regulatory compliance capabilities. For horizontal AI companies, it means deepening workflow integration — more connectors, more customization, more embedded workflows that make your product the operating system for a specific job function. For platform AI companies, it means building developer tooling, ecosystem, and distribution that competitors cannot replicate by offering cheaper compute alone.

The third step is repricing around value, not cost. This is the pricing migration itself. You move from per-token or per-query pricing to per-seat, per-outcome, or per-workflow pricing that reflects the value you deliver independently of the compute that powers it. The transition is delicate because existing customers have anchored expectations, but the alternative — continuing to price on a cost basis that erodes every quarter — is a guaranteed path to margin destruction.

## The Commoditization Calendar

The Race to Zero does not operate on a fixed schedule, but its pattern is predictable enough to plan around. In any given market segment, the clock starts when the first credible AI product launches. From that moment, the commoditization timeline follows a roughly predictable arc.

In the first twelve to eighteen months, the pioneer enjoys high margins because competitors have not yet appeared and buyers lack the sophistication to benchmark costs. This is the window where three-times-cost and five-times-cost pricing works and nobody questions it.

In months eighteen through thirty-six, competitors arrive using the same or newer models at lower prices. Buyers begin benchmarking. The pioneer's margin compresses from premium to competitive, typically losing fifteen to twenty-five margin points.

In months thirty-six through forty-eight, the market bifurcates. Products with structural differentiation — domain expertise, proprietary data, deep integration — maintain margins by selling value that transcends compute. Products without structural differentiation compete on price and consolidate. Multiple players in the segment either merge, pivot to a niche, or fail.

Beyond month forty-eight, the surviving companies have either become infrastructure — commodity providers competing on cost at thin margins — or have established defensible positions based on non-compute value. There is almost no middle ground. The Race to Zero does not leave room for "pretty good" margins on undifferentiated products.

## Turning Commoditization into Advantage

The Race to Zero is catastrophic for companies that fight it. It is a gift for companies that ride it. When your costs fall faster than your competitors can adjust, every cost decline makes your product more profitable — if your pricing is decoupled from the cost. When inference costs drop by ten times, your cost-to-serve a $500-per-month customer drops from $150 to $15. Your gross margin goes from seventy percent to ninety-seven percent. That is not a margin improvement. That is a structural transformation of your business economics.

The companies that will dominate AI markets in 2027 and beyond are the ones that treat the Race to Zero not as a threat to be managed but as a tailwind to be captured. They invest in the non-compute value that justifies stable pricing. They price on outcomes and workflows, not tokens and queries. They let declining costs flow directly to the bottom line. And they use the margin expansion to invest in more domain expertise, more integration, more data — building the moat wider at exactly the rate the commodity floor falls.

The Race to Zero determines whether your pricing model survives the next two years. But there is another competitive force that operates on a shorter timeline and with more immediate impact: the enterprise buyer's internal calculation of whether to build your product themselves using raw API access and an internal team. That build-versus-buy benchmark is the most direct competitive anchor on your pricing, and understanding it is the subject of the next subchapter.