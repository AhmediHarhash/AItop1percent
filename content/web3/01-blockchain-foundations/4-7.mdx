Scope: Web3

# 4.7 â€” Data Availability: The Hidden Bottleneck

Most builders assume the expensive part of running a rollup is execution â€” computing state transitions, running smart contract logic, processing user transactions. They are wrong. Execution on a modern L2 is fast and cheap. The sequencer runs it on a single server, and the marginal cost of one more transaction is negligible. The expensive part, the part that dominates your L2's operating cost and ultimately determines what your users pay, is making transaction data available to the rest of the network so anyone can verify that execution was correct.

This is counterintuitive. When you think about scaling, you think about speed â€” how many operations per second, how fast the sequencer processes a batch. But the bottleneck is not speed. The bottleneck is data. Specifically, the cost of publishing transaction data to a place where anyone, at any time, can download it, reconstruct the full L2 state, and independently verify that the sequencer did not cheat. That is what **data availability** means, and until you understand it, your mental model of rollup economics is incomplete.

## What Data Availability Actually Means

Data availability answers a deceptively simple question: can anyone who wants to verify the chain actually access the data they need to do so?

When a rollup sequencer processes a batch of transactions and posts the resulting state root to Ethereum, that state root alone is not enough. It tells the L1 what the new state is â€” which accounts hold which balances â€” but it does not tell anyone how that state was reached. Without the underlying transaction data, nobody can independently re-execute the batch and confirm that the state root is correct. The sequencer says "the new balance of account X is 500 tokens." Data availability is what lets a verifier say "I can see the transactions, I re-ran them myself, and yes, the balance should be 500 tokens."

This matters because the entire security model of rollups depends on verification. Optimistic rollups rely on challengers who re-execute batches to detect fraud. If the transaction data is not available, nobody can challenge a fraudulent batch, and the fraud proof mechanism becomes theater. ZK rollups generate validity proofs, but even with mathematical proofs, users still need the transaction data to reconstruct their own state and generate withdrawal proofs. Without data availability, you cannot exit the rollup independently. Your funds depend on the sequencer remaining cooperative.

A rollup without data availability is not a rollup. It is a server with a blockchain-sounding name. The data is what makes the system trustless. Everything else is optimism and branding.

## Why Data Is the Dominant Cost

Before Ethereum's Dencun upgrade in March 2024, rollups posted their transaction data as **calldata** â€” regular transaction input data stored permanently on Ethereum L1. Calldata is expensive because it is stored by every Ethereum node forever. Every byte of calldata costs gas, and gas costs money. Before Dencun, the data posting cost accounted for the majority of a rollup's operating expense â€” often 80 to 90 percent of total L1 fees paid by the rollup operator.

The numbers were stark. Major rollups like Arbitrum and Optimism were collectively spending more than 10,000 dollars per day on calldata alone. That cost was passed through to users as transaction fees. The execution itself was nearly free. The proving or fraud detection mechanisms were cheap. What users were actually paying for, in almost every L2 transaction fee, was the cost of storing data on Ethereum.

This is why data availability is the hidden bottleneck. It is hidden because users see a single transaction fee and assume it pays for computation. In reality, the vast majority of that fee pays for data storage on L1. Reduce the cost of data, and you reduce the cost of everything.

## EIP-4844 and the Blob Revolution

The Ethereum community recognized the data cost problem and addressed it with **EIP-4844**, implemented in the Dencun upgrade of March 2024. EIP-4844 introduced a new transaction type called a **blob-carrying transaction**. Instead of cramming rollup data into calldata, rollups can now attach data in **blobs** â€” large binary objects that are cheaper to store because they are not permanently accessible to smart contracts and are pruned from the network after a limited retention period of roughly 18 days.

The key insight behind blobs is that rollup data does not need the same permanence guarantees as regular Ethereum state. A smart contract balance needs to be accessible forever. Rollup transaction data only needs to be available long enough for verifiers to check it and for users to construct withdrawal proofs. After that, the data has served its purpose. By separating rollup data from permanent state and giving it a shorter shelf life, Ethereum created a cheaper data market without compromising security.

The impact was immediate and dramatic. Rollup transaction fees dropped by roughly 90 percent after Dencun. The blob fee market operates independently from Ethereum's regular gas market, with its own base fee that adjusts based on blob demand. When blob space is underutilized, the base fee drops to near zero. When demand exceeds the target, the fee rises exponentially to ration scarce space.

The Pectra upgrade in May 2025 doubled blob capacity, raising the target from 3 blobs per block to 6 and the maximum from 6 to 9. By early 2026, Ethereum increased the target again to 14 blobs with a maximum of 21, giving rollups substantially more room to post data. Even so, rollup demand has not yet consistently saturated the expanded capacity â€” blob utilization averages around 4 to 6 blobs per block as of early 2026, which keeps the blob base fee near its floor. The result is that posting data to Ethereum is currently extremely cheap, sometimes less than a hundredth of a cent per transaction.

## The Fixed-Size Problem

Blobs have a fixed size of 128 kilobytes each. A rollup cannot post half a blob. It pays for the full 128 kilobytes regardless of how much data it actually needs to include. For large rollups like Arbitrum or Base, which generate enough transaction volume to fill multiple blobs per batch, the fixed size is not an issue. Their batches are large enough to use the space efficiently.

For smaller rollups â€” chains with lower transaction volume â€” the fixed blob size creates a dilemma. If your batch of transactions only fills 30 percent of a blob, you are paying for 70 percent empty space. You can wait longer between batches to accumulate more transactions and fill the blob more efficiently, but waiting longer means your users experience higher latency between submitting a transaction and having it posted to L1. You can submit partially filled blobs for faster posting, but you pay a premium per transaction.

This small-rollup dilemma has spawned research into blob sharing â€” protocols that allow multiple small rollups to bundle their data into a single blob, splitting the cost. The approach is promising but adds coordination complexity. As of early 2026, most small rollups either accept the inefficiency or extend their batch intervals.

## Dedicated Data Availability Layers

Ethereum blobs solved the data cost problem for rollups that can afford to post to Ethereum and want Ethereum-grade security for their data. But a class of dedicated DA layers has emerged that offer an alternative: cheaper data availability at the cost of different security assumptions.

**Celestia** launched its mainnet in October 2023 as the first purpose-built data availability network. Celestia does one thing: it accepts data blobs, makes them available for a configurable period, and provides cryptographic guarantees that the data was published. It does not execute transactions. It does not settle state transitions. It is a pure DA layer. Rollups that use Celestia post their transaction data to Celestia instead of Ethereum, and they pay Celestia's data fee instead of Ethereum's blob fee. Celestia's fees are typically lower than Ethereum's because Celestia's architecture is optimized specifically for data throughput, with a throughput capacity designed to scale with the number of light nodes sampling the network.

**EigenDA** takes a different approach, leveraging Ethereum's own validator set through the EigenLayer restaking protocol. Validators who have staked ETH on Ethereum can opt into EigenDA, providing data availability services secured by the same economic stake that secures Ethereum itself. By early 2025, EigenDA had accumulated over 4 million ETH in restaked security â€” billions of dollars backing the data availability guarantees. EigenDA offers both reserved bandwidth and on-demand tiers, letting rollups choose between predictable pricing and pay-as-you-go flexibility.

**Avail**, which spun out of the Polygon ecosystem, offers a general-purpose DA layer with a focus on developer ergonomics and transparent fee structures. Avail uses a validity-proof-based architecture and supports Data Availability Sampling natively, positioning itself as a middle ground between Celestia's independent security model and EigenDA's Ethereum-aligned approach.

The trade-off between these options is straightforward. Ethereum blobs offer the highest security â€” your data availability is backed by the full Ethereum validator set and the full Ethereum economic security. Dedicated DA layers offer lower cost but rely on their own validator sets and security assumptions. A rollup using Celestia for DA inherits Celestia's security, not Ethereum's. If Celestia's validator set is compromised or withholds data, the rollup's safety guarantees degrade, even if its settlement still happens on Ethereum.

## Data Availability Sampling: How Verification Scales

The naive approach to data availability is simple: every node downloads every blob and checks that it is complete. This works when data volumes are small, but it does not scale. If every node must download every blob, the bandwidth requirements grow linearly with throughput. Double the data, double the bandwidth every node needs. You hit the same wall as monolithic blockchains.

**Data Availability Sampling**, or DAS, breaks through this wall with a probabilistic approach. Instead of downloading all the data, each light node downloads a small random sample â€” a few tiny pieces of a blob â€” and checks that those pieces are available. The mathematical trick relies on **erasure coding**, the same technique used in RAID storage and satellite communications. The original data is expanded with redundant information using polynomial interpolation, so that the full dataset can be reconstructed from any sufficiently large subset.

Here is why this matters. If a block producer withholds even a small portion of the original data, the erasure coding ensures that roughly half of the expanded data becomes unrecoverable. When dozens or hundreds of light nodes each sample random pieces, the probability that all of them happen to sample only the available pieces â€” missing the gap â€” drops exponentially toward zero. With enough light nodes sampling independently, the network reaches mathematical certainty that the data was fully published, even though no single node downloaded the complete dataset.

The implication is profound. With DAS, adding more light nodes to the network increases the data availability guarantee without increasing the per-node bandwidth requirement. The system scales by adding more participants, not by demanding more from each participant. This is the opposite of the monolithic scaling pattern, where adding throughput demands more from every node.

Ethereum's Fusaka upgrade, planned for late 2025 or early 2026, introduces PeerDAS â€” a peer-to-peer implementation of Data Availability Sampling that allows validators to verify data availability by sampling chunks from their peers. PeerDAS is projected to reduce full-node storage requirements by roughly 80 percent while maintaining the same data availability guarantees. Celestia has supported DAS since its mainnet launch and uses it as the foundation of its scaling model.

## The DA Decision Framework

If you are building a rollup or choosing which rollup to build on, the data availability layer is one of the most consequential decisions in your architecture. It determines your cost floor, your security model, and your relationship to the Ethereum ecosystem.

Choose Ethereum blobs if your application requires maximum security and you are willing to pay for it. Ethereum's DA is backed by the same validator set that secures trillions of dollars in assets. For DeFi protocols handling significant value, for institutional applications where regulatory scrutiny demands the strongest possible guarantees, Ethereum blobs are the conservative choice. The cost is higher than dedicated DA layers, but the security margin is unmatched.

Choose a dedicated DA layer if your application is cost-sensitive and operates at high data volume. Gaming platforms, social applications, and high-frequency trading systems generate enormous transaction volumes where the per-transaction DA cost is the primary determinant of user fees. A gaming rollup processing millions of moves per day can save tens of thousands of dollars monthly by posting data to Celestia or EigenDA instead of Ethereum. The security trade-off is real â€” you depend on the DA layer's own validator set â€” but for many applications, the cost savings justify the different trust model.

Consider the hybrid approach for applications that need both security and cost efficiency. Some rollups post critical settlement data â€” state roots and withdrawal proofs â€” to Ethereum while posting bulk transaction data to a cheaper DA layer. This gives you Ethereum's security for the operations that need it most while reducing costs for the high-volume data that does not require the same guarantees.

## How DA Costs Shape L2 Economics

Data availability is not a technical detail. It is the single largest variable in your L2's unit economics. When Ethereum blob fees are near their floor â€” as they have been for much of late 2025 and early 2026 â€” L2 transaction fees can reach fractions of a cent. When blob demand exceeds the target and the exponential fee adjustment kicks in, those same fees can spike by orders of magnitude. Your users experience this as unpredictable transaction costs, and unpredictable costs drive users away.

Understanding DA economics means understanding that your L2's fee structure is ultimately a passthrough of the underlying DA market. You can optimize execution, compress data more aggressively, batch more efficiently â€” and you should do all of these things. But the floor of your transaction fee is set by the cost of making data available. That cost is set by a market you do not control, governed by demand from every other rollup sharing the same DA layer.

This is why data availability is not just a technical problem. It is an economic problem, a competitive problem, and increasingly a strategic problem. The rollups that manage DA costs most effectively â€” through compression, through DA layer selection, through intelligent batching â€” will offer the lowest fees and attract the most users. The rollups that treat DA as an afterthought will find that their cost structure makes them uncompetitive.

The modular blockchain thesis, which the next subchapter explores in full, is ultimately a thesis about specialization. Execution, consensus, data availability, and settlement are four different jobs. The chains that try to do all four at once hit scaling walls. The stacks that let each layer specialize in one job scale further, cheaper, and faster. Data availability is where that thesis became real â€” where the abstract idea of modularity turned into concrete cost savings that reshaped the entire L2 landscape.
