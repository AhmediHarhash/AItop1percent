Scope: Web3

# 4.4 â€” Scalability: TPS Is the Wrong Metric

Every chain's marketing page leads with a TPS number. That number is a lie. Not because the chains fabricate it â€” most can technically hit their claimed throughput under controlled conditions â€” but because transactions per second tells you almost nothing about whether a blockchain can handle your application at production scale, at production cost, with production reliability.

TPS is the blockchain equivalent of a car manufacturer quoting top speed. A car that hits 250 kilometers per hour on a test track in ideal conditions but overheats in city traffic, takes 14 seconds to brake from highway speed, and costs 40 dollars per kilometer in fuel is not a fast car. It is a marketing number attached to an impractical machine. The metrics that matter for daily driving â€” acceleration, braking distance, fuel efficiency, reliability â€” are the ones the brochure buries.

Scalability works the same way. The number on the brochure is the least useful metric. The numbers that determine whether your application survives contact with real users are harder to find, harder to compare, and almost never appear in a project's headline statistics.

## Why TPS Misleads

The first problem with TPS is that not all transactions are equal. A simple token transfer â€” moving ETH from one address to another â€” is computationally trivial. It updates two balances. A complex DeFi swap that routes through three liquidity pools, checks oracle prices, calculates slippage, and emits events is orders of magnitude more expensive in computation. When Solana reports 65,000 theoretical TPS or when a newer chain claims 100,000, the question you should ask is: transactions of what complexity?

Most TPS benchmarks use the simplest possible transaction type. Some chains count validator vote messages â€” internal consensus bookkeeping that has nothing to do with user activity â€” as transactions. Solana's reported on-chain TPS frequently includes vote transactions, which account for roughly two-thirds of all recorded throughput. Strip those out, and Solana's real user-initiated throughput sits between 1,000 and 1,050 TPS under typical conditions. That is still impressive. But it is a different number than 65,000, and it tells a different story.

The second problem is the gap between theoretical and sustained throughput. Theoretical TPS measures what the network achieves in a short burst under ideal conditions â€” few competing transactions, no state bloat, no network congestion. Sustained throughput measures what the network delivers hour after hour, day after day, under real load with real users doing real things. Solana hit 107,000 TPS in a 2025 stress test using lightweight no-operation calls. That peak lasted moments. Under sustained real-world usage, it delivers roughly one to four thousand TPS. The gap between burst and sustained performance is not a flaw in Solana specifically â€” it is a universal property of distributed systems that marketing materials consistently obscure.

## The Metrics That Actually Matter

If TPS is the wrong headline number, what should you measure instead?

**Time to finality** is the interval between when you submit a transaction and when the network guarantees it will never be reversed. This is the metric that determines user experience in any application involving money, ownership, or state changes that matter. Bitcoin's time to finality is roughly 60 minutes â€” six confirmations at ten minutes each â€” because its probabilistic consensus means any individual block could theoretically be reorganized. Ethereum reaches finality in approximately 13 minutes through its Casper FFG mechanism, which requires two full epochs of validator attestations. Solana achieves finality in roughly 2.5 seconds. Sei Network hits sub-400-millisecond finality through its Twin-Turbo consensus.

These finality numbers tell you something TPS never can: how long your user waits before their action is irreversible. A chain that processes 100,000 TPS but takes 60 minutes to finalize is worse for a payment application than a chain that processes 1,000 TPS with sub-second finality. The user does not care how many other transactions the network can handle. They care how long they stand at the checkout counter.

**Cost per transaction** measures what a user actually pays to execute an operation. This is where the trilemma bites hardest, because cost is a direct function of how much competition exists for block space. Ethereum L1 transaction costs swing from under a dollar during quiet periods to fifty or a hundred dollars during demand spikes. The cost is not set by the protocol â€” it is set by an auction in which users compete for limited block space. A chain that offers high TPS but charges five dollars per transaction is less scalable, in practical terms, than a chain with lower TPS and sub-cent fees.

**Throughput under load** separates production-ready chains from testnet demos. What happens when transaction volume doubles? When it quadruples? Does the chain maintain consistent block times? Do fees spike linearly or exponentially? Does the network experience outages? Solana experienced multiple extended outages between 2022 and 2024 during periods of extreme demand, where the network halted entirely because the volume of incoming transactions overwhelmed the validator infrastructure. Those outages tell you more about scalability than any TPS number because they reveal the breaking point â€” the moment where theoretical capacity meets reality and reality wins.

## Block Size, Block Time, and the Engineering Constraints

The raw throughput of any blockchain is a function of two variables: how much data fits in a block and how frequently blocks are produced. Multiply block size by block frequency, and you get theoretical bandwidth. Everything else is engineering within those constraints.

**Block time** is the interval between consecutive blocks. Bitcoin produces a block every ten minutes. Ethereum produces a block every twelve seconds. Solana produces blocks roughly every 400 milliseconds. Shorter block times mean faster transaction inclusion â€” your transaction gets into a block sooner â€” but they impose stricter requirements on network propagation. If blocks are produced faster than they can propagate across the global validator set, validators start building on different chain tips, and the network forks. This is why you cannot simply set block time to one millisecond and declare victory. The block time must be longer than the time it takes for block data to reach the majority of validators, and that propagation time is bounded by physics â€” the speed of light across fiber optic cables spanning continents.

**Block size** determines how many transactions fit in each block. Larger blocks mean more throughput per block, but larger blocks also take longer to propagate, longer to validate, and more storage to maintain. Ethereum's current gas limit of approximately 36 million gas per block â€” with planned increases to 100 million and eventually 200 million gas through the Glamsterdam upgrade in mid-2026 â€” reflects the tension between throughput and the ability of ordinary hardware to keep up. If you double the block size, you double the throughput, but you also double the bandwidth, storage, and computation required of every full node. Nodes that cannot keep up drop off the network. When nodes drop off, decentralization decreases. The trilemma, again.

This is why simply increasing block size or decreasing block time is not a scalability strategy. It is a decentralization trade. Every monolithic chain that achieves high TPS does so by raising the hardware requirements for validators, which reduces the number of people who can afford to run one, which concentrates the network in fewer hands.

## The Monolithic Scaling Wall

A **monolithic blockchain** handles all four core functions â€” execution, consensus, data availability, and settlement â€” in a single integrated system. Every validator executes every transaction, participates in consensus on every block, stores all transaction data, and settles all state transitions. This architecture is simple and elegant, but it hits a fundamental wall: the system's capacity is limited by the weakest validator's resources.

If you want 10,000 validators to participate, you need the 10,000th-slowest machine to keep up with every block. The more you demand from validators â€” bigger blocks, faster block times, heavier computation â€” the more expensive it becomes to run a validator. At some point, the hardware requirements cross a threshold where only professional data centers can participate. You have not solved the scalability problem. You have solved it for entities that can afford a 64-core server with 512 gigabytes of RAM and a 10-gigabit internet connection.

This is the wall Ethereum hit. By 2023, it was clear that scaling Ethereum by simply increasing L1 throughput would either require raising hardware requirements beyond what hobbyist validators could afford or making compromises on block propagation that risked centralizing the network. The Ethereum community made a deliberate choice: instead of scaling the monolith, decompose the problem.

## The Modular Alternative

The **modular blockchain thesis** starts from a simple observation: execution, consensus, data availability, and settlement have different scaling characteristics. Execution benefits from parallelism and specialized hardware. Consensus benefits from a large, diverse validator set. Data availability benefits from erasure coding and sampling. Settlement benefits from maximal security.

If these four functions have different scaling requirements, why force them into a single system? Why not let each function be handled by a layer optimized for that specific job?

This is the architectural insight that drives the modern Ethereum roadmap and the broader modular ecosystem. Instead of one chain doing everything, you get a stack: L2 rollups handle execution, Ethereum L1 handles settlement and consensus, and dedicated data availability layers handle the storage and distribution of transaction data. Each layer scales independently. The execution layer can get faster without requiring consensus to speed up. The data availability layer can get cheaper without requiring execution to change.

By early 2026, data from L2Beat and ecosystem trackers shows modular architectures leading in both TVL growth and developer activity. The total throughput across Ethereum L2s â€” Arbitrum, Base, Optimism, zkSync, StarkNet, Scroll, and dozens of smaller rollups â€” exceeds Ethereum L1 by a factor of a hundred or more. No single L2 needed to solve the scalability trilemma alone. Each L2 optimizes its own tradeoff profile, and the aggregate system delivers throughput that no monolithic chain can match without centralizing.

## Measuring Scalability Honestly

When you evaluate a chain or a layer for your application, ignore the TPS headline. Instead, ask five questions.

First, what is the time to finality for my transaction type? Not for a simple transfer, but for the specific contract interaction my users will perform. Test it. Measure it. The number on the website and the number in production are often different.

Second, what does a transaction cost during peak demand? Not during quiet Sunday mornings, but during the moments when everyone is using the chain at once. Check historical fee data during past demand spikes. If fees went from one cent to fifty dollars during the last major NFT mint or token launch, that volatility is part of the scalability profile.

Third, what is the sustained throughput under realistic load? Not burst throughput, not testnet throughput, not throughput with synthetic transactions. Actual sustained throughput with real smart contract interactions over a 24-hour period.

Fourth, what happens at the breaking point? Has the chain ever halted, degraded, or become unusable during peak load? How long did recovery take? What was the root cause? A chain that has never been stress-tested under genuine adversarial load is an unknown quantity, not a safe bet.

Fifth, what are the hardware requirements for running a full node? Can you verify the chain on consumer hardware, or do you need a data center? The answer tells you how decentralized the chain will remain as throughput increases â€” and therefore how durable its security and censorship resistance properties are.

These five questions give you a real scalability profile. TPS gives you a marketing slide.

## Why This Matters for Builders

Scalability is not an abstract property. It determines your cost structure, your user experience, and your architectural options. A chain that is cheap and fast during development can become expensive and slow when your user base grows. A chain that offers high headline throughput but centralizes under load may not provide the censorship resistance your application relies on. A chain that scales beautifully but finalizes slowly may not work for the real-time interactions your product requires.

The trilemma teaches you that scalability cannot be evaluated in isolation. Every throughput gain comes at a cost to either decentralization or security â€” or it comes from moving computation off the base layer entirely. That last option â€” moving execution off-chain while inheriting base layer security â€” is not a compromise. It is the dominant scaling strategy in 2026, and it is the subject of the next subchapter. Layer 2 networks do not try to make L1 faster. They make L1 irrelevant for most computation, and that changes everything.
