Scope: Web3

# 3.10 â€” Building Your Trust Audit

Before you build anything, map every trust assumption. Write them down. All of them. Not the ones that are convenient to acknowledge â€” all of them. The exercise will take an hour. Skipping it can cost you everything.

Every system you build on a blockchain inherits trust assumptions from every layer of the stack. Some of these assumptions are explicit â€” you chose Ethereum, you chose Chainlink, you chose a multi-sig. Others are implicit â€” you did not think about sequencer liveness, or you assumed the bridge you use will always be solvent, or you never tested what happens when the oracle goes stale. The implicit assumptions are the ones that kill projects. Not because they are necessarily wrong, but because no one evaluated them, stress-tested them, or built contingencies for their failure.

**The Trust Audit** is a structured framework for identifying, evaluating, and documenting every trust dependency in your system. It is not a one-time exercise. It is a living document that evolves as your system changes, as the protocols you depend on upgrade, and as the threat landscape shifts. A Trust Audit written in January is partially stale by June. A Trust Audit never written is a liability from day one.

## Why You Need a Framework

Most teams understand trust in the abstract. They know that blockchains involve trust tradeoffs. They nod when someone mentions oracle risk or bridge risk. But understanding trust abstractly and mapping it concretely are entirely different activities, and only the concrete version protects you.

The problem with abstract understanding is that it does not force prioritization. If you vaguely know that "oracles are a risk" and "bridges are a risk" and "key management is a risk," you treat them all with equal concern, which in practice means you treat none of them with sufficient concern. A Trust Audit forces you to ask: which of these risks, if realized, would cause the most damage? Which ones have the highest probability of occurring? Which ones have mitigations available, and which ones do you simply accept?

In late 2024, a cross-chain lending protocol lost over 12 million dollars because it relied on a bridge whose security model degraded after a validator set change that no one on the lending team was monitoring. They knew bridges were risky. They had discussed it in a team meeting months earlier. But no one had documented the specific trust assumptions they were making about that specific bridge, and no one had assigned responsibility for monitoring changes to its security model. The knowledge existed. The discipline did not.

A Trust Audit converts ambient awareness into accountable documentation. It takes the vague feeling of "we should probably look into that" and turns it into a line item with an owner, a severity rating, and a review date.

## The Trust Stack: Layer by Layer

The Trust Audit walks through your system's Trust Stack systematically. For each layer, you answer four questions: who are you trusting, what could go wrong, how much damage would a failure cause, and what mitigations exist or are needed.

We covered each of these layers in the preceding subchapters. Now we bring them together into a single audit structure.

## Layer 1: Protocol and Consensus Trust

This is the foundation. You are trusting that the blockchain you build on will continue producing blocks, finalizing state, and enforcing its protocol rules correctly.

**Who you are trusting.** The validator set of your chosen chain. On Ethereum, this is over one million validators collectively staking over 34 million ETH. On Solana, it is fewer than a thousand validators with concentrated stake. On a Cosmos chain, it might be 100 to 175 validators. The size, distribution, and incentive alignment of this validator set determines the baseline security of everything you build.

**What could go wrong.** A consensus failure â€” validators disagreeing on the canonical chain â€” could cause a chain split or a reorg that reverts your transactions. A liveness failure â€” validators stopping block production â€” could freeze your protocol. A censorship attack â€” validators colluding to exclude specific transactions â€” could prevent your users from interacting with your contracts.

**How much damage.** If the base layer fails, everything built on it fails. This is the highest-severity layer by definition. A consensus failure on Ethereum would affect hundreds of billions of dollars in DeFi TVL simultaneously. On a smaller chain, the absolute dollar amount is lower but the probability might be higher.

**What mitigations exist.** You cannot mitigate base-layer risk from within your application. Your mitigation is your chain selection. Choose a chain with a large, diverse, well-incentivized validator set and a track record of sustained uptime. If you build on a newer or smaller chain, acknowledge the elevated risk and design your protocol accordingly â€” perhaps with lower TVL caps, emergency pause mechanisms, or cross-chain escape hatches.

## Layer 2: Cryptographic Trust

You are trusting that the cryptographic primitives your chain uses â€” hash functions, digital signature schemes, encryption algorithms â€” are secure against current and near-future attacks.

**Who you are trusting.** Decades of cryptographic research, the specific implementations used by your chain's client software, and the assumption that quantum computing will not break elliptic curve cryptography within your system's operational lifetime.

**What could go wrong.** A fundamental break in a cryptographic primitive â€” such as a practical attack against the secp256k1 elliptic curve used by Ethereum and Bitcoin â€” would allow an attacker to forge signatures and steal funds from any address. A bug in a specific implementation (a flawed random number generator, a side-channel vulnerability in a signing library) could compromise a subset of users without breaking the underlying math.

**How much damage.** A break in the core cryptography would be existential â€” not just for your protocol, but for the entire chain. An implementation bug would be severe but localized.

**What mitigations exist.** Cryptographic risk is largely accepted as a baseline assumption. The practical mitigation is staying informed about post-quantum cryptographic research and ensuring your system can migrate to quantum-resistant algorithms when the industry moves in that direction. Ethereum's long-term roadmap includes provisions for post-quantum cryptography, but no timeline has been committed. For implementation bugs, using well-audited, widely-used cryptographic libraries rather than custom implementations reduces risk.

## Layer 3: Execution Environment Trust

If you build on an L2 rollup, a sidechain, or an appchain rather than the L1 directly, you are introducing an additional trust layer around how your transactions are executed and how their validity is communicated back to the base layer.

**Who you are trusting.** On an optimistic rollup like Arbitrum or Optimism, you are trusting that at least one honest verifier will submit a fraud proof within the challenge window if the sequencer posts invalid state. On a ZK rollup like zkSync or StarkNet, you are trusting the correctness of the zero-knowledge proof system and its implementation. On a sidechain, you are trusting a separate, usually smaller validator set.

**What could go wrong.** Sequencer centralization â€” most L2s run a single sequencer that orders transactions â€” means a single entity can censor transactions, extract MEV, or go offline entirely. On optimistic rollups, the challenge window (typically seven days) creates a period during which invalid state could be posted and not yet challenged. On ZK rollups, bugs in the proof system or the prover could allow invalid state transitions to be accepted by the L1.

**How much damage.** A sequencer outage halts your application until the sequencer recovers or an alternative submission mechanism activates. A successful invalid state attack on a rollup could drain all assets secured by the rollup's bridge contract.

**What mitigations exist.** Check whether your L2 has a credible sequencer decentralization roadmap. Verify that fraud proof or validity proof mechanisms are live and have been tested â€” some rollups launched with training wheels that bypass the proof mechanism. Understand the escape hatch: can users force-include transactions on L1 if the sequencer is unresponsive? Document the specific sequencer operator and their uptime history.

## Layer 4: Bridge Trust

If your system moves assets or data between chains, every bridge is a trust dependency.

**Who you are trusting.** The bridge's validator set, its smart contract code, its liquidity providers, and its operational team. Different bridges have radically different security models. A bridge secured by a 4-of-7 multi-sig is trusting seven people. A bridge secured by zero-knowledge proofs of the source chain's consensus is trusting mathematics. Most bridges fall somewhere in between.

**What could go wrong.** A bridge hack â€” the most common catastrophic failure in Web3 history. The Ronin bridge lost 625 million dollars. The Wormhole bridge lost 320 million dollars. The Nomad bridge lost 190 million dollars. These are not edge cases. They are defining events in the industry.

**How much damage.** Total loss of all assets that transited through the bridge, plus reputational damage and potential legal liability. Bridge failures are typically total, not partial.

**What mitigations exist.** Minimize the number of bridges you depend on. Prefer bridges with higher security guarantees â€” those using light client verification or zero-knowledge proofs rather than small multi-sigs. Set maximum exposure limits on any single bridge. Monitor bridge health metrics: validator set changes, contract upgrades, unusual transaction patterns. Have a documented response plan for a bridge compromise.

## Layer 5: Oracle Trust

If your system consumes any off-chain data â€” prices, event outcomes, real-world state â€” your oracle is a trust dependency.

**Who you are trusting.** The oracle network's node operators, their data sources, the aggregation mechanism, and the update frequency.

**What could go wrong.** Oracle manipulation (an attacker corrupts the data), oracle failure (the oracle stops updating), or oracle staleness (the data is accurate but too old for your use case). Any of these can trigger incorrect liquidations, unfair settlements, or protocol-draining exploits.

**How much damage.** Proportional to the economic actions your protocol takes based on oracle data. A lending protocol with 500 million dollars in TVL that liquidates positions based on oracle prices has 500 million dollars of oracle-dependent risk.

**What mitigations exist.** Use established, decentralized oracle networks with multiple independent nodes and diverse data sources. Implement staleness checks â€” revert if the oracle data is older than your acceptable threshold. Implement deviation circuit breakers â€” pause if the oracle reports a price change larger than a reasonable threshold in a single update. Consider using multiple oracle sources and requiring agreement. Monitor oracle health proactively, not reactively.

## Layer 6: Governance Trust

If your protocol can be upgraded, if parameters can be changed, if contracts can be paused â€” someone has that power. That someone is a trust dependency.

**Who you are trusting.** The governance mechanism: a DAO token vote, a multi-sig of core contributors, a time-locked admin key, or some combination. The specific humans or entities who hold governance power.

**What could go wrong.** A governance attack â€” someone acquires enough voting power to pass a malicious proposal. A compromised admin key â€” the multi-sig signers' keys are stolen. A rug pull â€” the team uses admin privileges to drain the protocol. A governance failure â€” the DAO cannot reach quorum to address a critical issue.

**How much damage.** If governance has the power to upgrade contracts arbitrarily, the maximum damage is total â€” a malicious upgrade can drain everything. If governance power is limited by time locks and scope restrictions, the damage is bounded.

**What mitigations exist.** Time locks on all governance actions â€” typically 24 to 72 hours between proposal approval and execution, allowing users to exit if they disagree. Scope-limited governance â€” separating the power to change protocol parameters (fee rates, collateral ratios) from the power to upgrade contract code. Multi-sig requirements for critical actions. Transparent governance processes with on-chain voting. Regular reviews of who holds governance power and whether the distribution has changed.

## Layer 7: Key Management Trust

How users and administrators hold their keys is a trust layer that most protocol teams overlook because they think of it as "the user's problem." It is not. It is your system's problem.

**Who you are trusting.** Your users, to secure their own keys. Your team, to secure admin keys. Your custody provider, if you use one. Your key management infrastructure, if you deploy smart accounts or MPC wallets.

**What could go wrong.** User key loss â€” funds become permanently inaccessible. User key theft â€” phishing, malware, social engineering. Admin key compromise â€” an attacker gains control of protocol admin functions. Custody provider failure â€” the provider loses key shares, gets hacked, or goes out of business.

**How much damage.** User key loss or theft affects individual users. Admin key compromise can affect the entire protocol.

**What mitigations exist.** For admin keys: multi-sig with geographic distribution, hardware wallet requirements for signers, time locks on sensitive operations, rotation schedules. For user keys: offering smart account wallets with social recovery, integrating MPC-based onboarding that does not expose seed phrases, providing clear educational materials. For custody providers: due diligence on provider security practices, key share backup strategies, contractual guarantees.

## Scoring and Prioritizing

A raw list of trust dependencies is useful but not actionable until you prioritize. For each dependency, assign two ratings.

**Severity: what happens if this fails?** Rate on a scale from one to five. One means minor inconvenience â€” a temporary price discrepancy or a delayed transaction. Five means catastrophic â€” total loss of protocol funds, regulatory action, or permanent reputational damage.

**Likelihood: how probable is this failure?** Rate on a scale from one to five. One means near-impossible â€” a break in SHA-256, a coordinated compromise of Ethereum's entire validator set. Five means has happened before and will happen again â€” oracle staleness, sequencer downtime, user key loss.

Multiply severity by likelihood to get a priority score. A dependency with severity 5 and likelihood 4 (score 20) needs active mitigation today. A dependency with severity 2 and likelihood 1 (score 2) can be acknowledged and accepted.

The highest-priority items are your action list. For each one, document the specific mitigation: what you are doing about it, who owns it, and when it will be reviewed next.

## The Living Document

A Trust Audit is not a deliverable you complete and file away. It is a living document because the trust landscape changes continuously.

Protocol upgrades change the security model. When Ethereum shipped Pectra, every protocol on Ethereum inherited new capabilities and new assumptions. When an L2 decentralizes its sequencer, the trust model of every application on that L2 shifts. When Chainlink adds or removes nodes from a price feed, the oracle trust model changes.

Your own system evolves. You add new features that introduce new dependencies. You integrate a new bridge. You migrate from a multi-sig to a DAO. You launch on a new chain. Each change updates the Trust Stack, and each update requires re-evaluating the audit.

The threat landscape evolves. New attack vectors emerge. Flash loan attacks were unknown before 2020. Oracle manipulation became a dominant attack vector in 2022-2023. Bridge exploits dominated 2022. MEV extraction matured through 2023-2024. Whatever the dominant attack vector of 2027 is, it probably exists in embryonic form today. Regular re-audits â€” quarterly at minimum, after every major dependency change â€” keep the document current.

Assign ownership. Every trust dependency in the audit should have a named owner â€” a specific person responsible for monitoring it and raising an alert if conditions change. "The team" is not an owner. "Alice, the infrastructure lead" is an owner. Without named ownership, the audit is a list of good intentions that no one acts on.

## What the Audit Does Not Do

The Trust Audit does not eliminate risk. It makes risk visible, quantified, and owned. It ensures that when a failure occurs â€” and failures will occur â€” the team's response is "we identified this risk, we have a mitigation plan, and here is who is executing it" rather than "we did not think about that."

The audit also does not tell you whether your system is "safe." No blockchain system is safe in an absolute sense. Every system is a collection of bets â€” bets on cryptographic hardness, bets on validator honesty, bets on oracle accuracy, bets on user behavior, bets on regulatory stability. The Trust Audit makes those bets explicit. It turns unconscious assumptions into conscious decisions. And conscious decisions, even imperfect ones, are categorically better than unconscious assumptions.

The teams that suffer catastrophic failures in Web3 are rarely the ones who took the wrong risks. They are the ones who did not know which risks they were taking. The Trust Audit is how you avoid being one of them.

## Putting It Into Practice

Start with a blank document and seven sections â€” one for each layer of the Trust Stack. For each section, answer the four questions: who are you trusting, what could go wrong, how bad would it be, and what are you doing about it. Be specific. "We trust Chainlink" is not useful. "We trust Chainlink's ETH/USD price feed, which is served by 21 nodes with a 0.5 percent deviation threshold and a 3600-second heartbeat, and we check staleness on every read with a maximum age of 3700 seconds" is useful.

When you finish, review the document with your team. The disagreements are the most valuable part. If your smart contract developer thinks oracle staleness is a severity 3 and your DeFi researcher thinks it is a severity 5, that conversation needs to happen before the oracle goes stale in production, not after.

Update the document every time you ship a major change, integrate a new dependency, or learn about a new attack vector in your dependency chain. Keep it in your team's shared workspace, not buried in someone's notes. Refer to it in architecture reviews, security audits, and incident post-mortems.

The Trust Audit will not make your system trustless. Nothing will, because trustless systems do not exist. But it will make your system's trust requirements explicit, measurable, and manageable. In a space where billions of dollars have been lost to unexamined assumptions, that clarity is worth more than any smart contract optimization.

The next chapter turns from trust to tradeoffs â€” the blockchain trilemma and the engineering of the choices every chain must make between security, decentralization, and scalability.
